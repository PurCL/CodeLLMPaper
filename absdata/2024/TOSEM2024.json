{
    "Understanding Developers Well-Being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic": {
        "type": "article",
        "key": "10.1145/3638244",
        "author": "Russo, Daniel and Hanel, Paul H. P. and van Berkel, Niels",
        "title": "Understanding Developers Well-Being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3638244",
        "doi": "10.1145/3638244",
        "abstract": "The COVID-19 pandemic has brought significant and enduring shifts in various aspects of life, including increased flexibility in work arrangements. In a longitudinal study, spanning 24 months with six measurement points from April 2020 to April 2022, we explore changes in well-being, productivity, social contacts, and needs of software engineers during this time. Our findings indicate systematic changes in various variables. For example, well-being and quality of social contacts increased while emotional loneliness decreased as lockdown measures were relaxed. Conversely, people\u2019s boredom and productivity remained stable. Furthermore, a preliminary investigation into the future of work at the end of the pandemic revealed a consensus among developers for a preference of hybrid work arrangements. We also discovered that prior job changes and low job satisfaction were consistently linked to intentions to change jobs if current work conditions do not meet developers\u2019 needs. This highlights the need for software organizations to adapt to various work arrangements to remain competitive employers. Building upon our findings and the existing literature, we introduce the Integrated Job Demands-Resources and Self-Determination (IJARS) Model as a comprehensive framework to explain the well-being and productivity of software engineers during the COVID-19 pandemic.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "57",
        "numpages": "44",
        "keywords": "COVID-19, longitudinal study, well-being, future of work, IJARS model"
    },
    "Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health": {
        "type": "article",
        "key": "10.1145/3630252",
        "author": "Lustosa, Andre and Menzies, Tim",
        "title": "Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3630252",
        "doi": "10.1145/3630252",
        "abstract": "When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g., the number of closed pull requests in 12 months time). The training data for this task may be very small (e.g., 5 years of data, collected every month means just 60 rows of training data). The models generated from such tiny datasets can make many prediction errors.Those errors can be tamed by a landscape analysis that selects better learner control parameters. Our niSNEAK tool (a)&nbsp;clusters the data to find the general landscape of the hyperparameters, then (b)&nbsp;explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g., FLASH, HYPEROPT, OPTUNA).The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as C = number of commits, I = number of closed issues, and R = number of closed pull requests, niSNEAK\u2019s 12-month prediction errors are {I=0\\%, R=33\\%&nbsp;C=47\\%}, whereas other methods have far larger errors of {I=61\\%,R=119\\%&nbsp;C=149\\%}. We conjecture that niSNEAK works so well since it finds the most informative regions of the hyperparameters, then jumps to those regions. Other methods (that do not reflect over the landscape) can waste time exploring less informative options.Based on the preceding, we recommend landscape analytics (e.g., niSNEAK) especially when learning from very small datasets. This article only explores the application of niSNEAK to project health. That said, we see nothing in principle that prevents the application of this technique to a wider range of problems.To assist other researchers in repeating, improving, or even refuting our results, all our scripts and data are available on GitHub at https://github.com/zxcv123456qwe/niSneak.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "58",
        "numpages": "22",
        "keywords": "Hyperparameter tuning, software health, indepedent variable clustering"
    },
    "The Lost World: Characterizing and Detecting Undiscovered Test Smells": {
        "type": "article",
        "key": "10.1145/3631973",
        "author": "Yang, Yanming and Hu, Xing and Xia, Xin and Yang, Xiaohu",
        "title": "The Lost World: Characterizing and Detecting Undiscovered Test Smells",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631973",
        "doi": "10.1145/3631973",
        "abstract": "Test smell refers to poor programming and design practices in testing and widely spreads throughout software projects. Considering test smells have negative impacts on the comprehension and maintenance of test code and even make code-under-test more defect-prone, it thus has great importance in mining, detecting, and refactoring them. Since Deursen et&nbsp;al. introduced the definition of \u201ctest smell\u201d, several studies worked on discovering new test smells from test specifications and software practitioners\u2019 experience. Indeed, many bad testing practices are \u201cobserved\u201d by software developers during creating test scripts rather than through academic research and are widely discussed in the software engineering community (e.g., Stack Overflow)&nbsp;[70, 94]. However, no prior studies explored new bad testing practices from software practitioners\u2019 discussions, formally defined them as new test smell types, and analyzed their characteristics, which plays a bad role for developers in knowing these bad practices and avoiding using them during test code development. Therefore, we pick up those challenges and act by working on systematic methods to explore new test smell types from one of the most mainstream developers\u2019 Q&amp;A platforms, i.e., Stack Overflow. We further investigate the harmfulness of new test smells and analyze possible solutions for eliminating them. We find that some test smells make it hard for developers to fix failed test cases and trace their failing reasons. To exacerbate matters, we have identified two types of test smells that pose a risk to the accuracy of test cases. Next, we develop a detector to detect test smells from software. The detector is composed of six detection methods for different smell types. These detection methods are both wrapped with a set of syntactic rules based on the code patterns extracted from different test smells and developers\u2019 code styles. We manually construct a test smell dataset from seven popular Java projects and evaluate the effectiveness of our detector on it. The experimental results show that our detector achieves high performance in precision, recall, and F1 score. Then, we utilize our detector to detect smells from 919 real-world Java projects to explore whether the six test smells are prevalent in practice. We observe that these test smells are widely spread in 722 out of 919 Java projects, which demonstrates that they are prevalent in real-world projects. Finally, to validate the usefulness of test smells in practice, we submit 56 issue reports to 53 real-world projects with different smells. Our issue reports achieve 76.4\\% acceptance by conducting sentiment analysis on developers\u2019 replies. These evaluations confirm the effectiveness of our detector and the prevalence and practicality of new test smell types on real-world projects.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "59",
        "numpages": "32",
        "keywords": "Test smell, mining software repositories, test smell detection, empirical study"
    },
    "How Important Are Good Method Names in Neural Code Generation? A Model Robustness Perspective": {
        "type": "article",
        "key": "10.1145/3630010",
        "author": "Yang, Guang and Zhou, Yu and Yang, Wenhua and Yue, Tao and Chen, Xiang and Chen, Taolue",
        "title": "How Important Are Good Method Names in Neural Code Generation? A Model Robustness Perspective",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3630010",
        "doi": "10.1145/3630010",
        "abstract": "Pre-trained code generation models (PCGMs) have been widely applied in neural code generation, which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this article, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs from a model robustness perspective. Specifically, we propose a novel approach, named neuRAl coDe generAtor Robustifier (RADAR). RADAR consists of two components: RADAR-Attack and RADAR-Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR-Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR-Attack can reduce the CodeBLEU of generated code by 19.72\\% to 38.74\\% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task and reduce the Pass@1 of generated code by 32.28\\% to 44.42\\% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR-Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "60",
        "numpages": "35",
        "keywords": "Code generation, adversarial examples, robustness, passive defense, pre-trained model"
    },
    "A Post-training Framework for Improving the Performance of Deep Learning Models via Model Transformation": {
        "type": "article",
        "key": "10.1145/3630011",
        "author": "Jiang, Jiajun and Yang, Junjie and Zhang, Yingyi and Wang, Zan and You, Hanmo and Chen, Junjie",
        "title": "A Post-training Framework for Improving the Performance of Deep Learning Models via Model Transformation",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3630011",
        "doi": "10.1145/3630011",
        "abstract": "Deep learning (DL) techniques have attracted much attention in recent years and have been applied to many application scenarios. To improve the performance of DL models regarding different properties, many approaches have been proposed in the past decades, such as improving the robustness and fairness of DL models to meet the requirements for practical use. Among existing approaches, post-training is an effective method that has been widely adopted in practice due to its high efficiency and good performance. Nevertheless, its performance is still limited due to the incompleteness of training data. Additionally, existing approaches are always specifically designed for certain tasks, such as improving model robustness, which cannot be used for other purposes.In this article, we aim to fill this gap and propose an effective and general post-training framework, which can be adapted to improve the model performance from different aspects. Specifically, it incorporates a novel model transformation technique that transforms a classification model into an isomorphic regression model for fine-tuning, which can effectively overcome the problem of incomplete training data by forcing the model to strengthen the memory of crucial input features and thus improve the model performance eventually. To evaluate the performance of our framework, we have adapted it to two emerging tasks for improving DL models, i.e., robustness and fairness improvement, and conducted extensive studies by comparing it with state-of-the-art approaches. The experimental results demonstrate that our framework is indeed general, as it is effective in both tasks. Specifically, in the task of robustness improvement, our approach Dare has achieved the best results on 61.1\\% cases (vs. 11.1\\% cases achieved by baselines). In the task of fairness improvement, our approach FMT can effectively improve the fairness without sacrificing the accuracy of the models.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "61",
        "numpages": "41",
        "keywords": "Deep neural network, delta debugging, model robustness, model fairness"
    },
    "Poison Attack and Poison Detection on Deep Source Code Processing Models": {
        "type": "article",
        "key": "10.1145/3630008",
        "author": "Li \u2642, Jia and Li, Zhuo and Zhang, Huangzhao and Li, Ge and Jin, Zhi and Hu, Xing and Xia, Xin",
        "title": "Poison Attack and Poison Detection on Deep Source Code Processing Models",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3630008",
        "doi": "10.1145/3630008",
        "abstract": "In the software engineering (SE) community, deep learning (DL) has recently been applied to many source code processing tasks, achieving state-of-the-art results. Due to the poor interpretability of DL models, their security vulnerabilities require scrutiny. Recently, researchers have identified an emergent security threat to DL models, namely, poison attacks. The attackers aim to inject insidious backdoors into DL models by poisoning the training data with poison samples. The backdoors mean that poisoned models work normally with clean inputs but produce targeted erroneous results with inputs embedded with specific triggers. By using triggers to activate backdoors, attackers can manipulate poisoned models in security-related scenarios (e.g., defect detection) and lead to severe consequences. To verify the vulnerability of deep source code processing models to poison attacks, we present a poison attack approach for source code named CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable and functionality-preserving poison samples and effectively attack deep source code processing models by poisoning the training data with poison samples. To defend against poison attacks, we further propose an effective poison detection approach named CodeDetector. CodeDetector can automatically identify poison samples in the training data. We apply CodePoisoner and CodeDetector to six deep source code processing models, including defect detection, clone detection, and code repair models. The results show that \u2776 CodePoisoner conducts successful poison attacks with a high attack success rate (average: 98.3\\%, maximum: 100\\%). It validates that existing deep source code processing models have a strong vulnerability to poison attacks. \u2777 CodeDetector effectively defends against multiple poison attack approaches by detecting (maximum: 100\\%) poison samples in the training data. We hope this work can help SE researchers and practitioners notice poison attacks and inspire the design of more advanced defense techniques.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "62",
        "numpages": "31",
        "keywords": "Poison attack, poison detection, source code processing, deep learning"
    },
    "How Are Multilingual Systems Constructed: Characterizing Language Use and Selection in Open-Source Multilingual Software": {
        "type": "article",
        "key": "10.1145/3631967",
        "author": "Li, Wen and Marino, Austin and Yang, Haoran and Meng, Na and Li, Li and Cai, Haipeng",
        "title": "How Are Multilingual Systems Constructed: Characterizing Language Use and Selection in Open-Source Multilingual Software",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631967",
        "doi": "10.1145/3631967",
        "abstract": "For many years now, modern software is known to be developed in multiple languages (hence termed as multilingual or multi-language software). Yet, to date, we still only have very limited knowledge about how multilingual software systems are constructed. For instance, it is not yet really clear how different languages are used, selected together, and why they have been so in multilingual software development. Given the fact that using multiple languages in a single software project has become a norm, understanding language use and selection (i.e., language profile) as a basic element of the multilingual construction in contemporary software engineering is an essential first step.In this article, we set out to fill this gap with a large-scale characterization study on language use and selection in open-source multilingual software. We start with presenting an updated overview of language use in 7,113 GitHub projects spanning the 5 past years by characterizing overall statistics of language profiles, followed by a deeper look into the functionality relevance/justification of language selection in these projects through association rule mining. We proceed with an evolutionary characterization of 1,000 GitHub projects for each of the 10 past years to provide a longitudinal view of how language use and selection have changed over the years, as well as how the association between functionality and language selection has been evolving.Among many other findings, our study revealed a growing trend of using three to five languages in one multilingual software project and the noticeable stableness of top language selections. We found a non-trivial association between language selection and certain functionality domains, which was less stable than that with individual languages over time. In a historical context, we also have observed major shifts in these characteristics of multilingual systems both in contrast to earlier peer studies and along the evolutionary timeline. Our findings offer essential knowledge on the multilingual construction in modern software development. Based on our results, we also provide insights and actionable suggestions for both researchers and developers of multilingual systems.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "63",
        "numpages": "46",
        "keywords": "Multilingual software, language use, language selection, language profile, functionality relevance, evolutionary characterization, association mining"
    },
    "Safety of Perception Systems for Automated Driving: A Case Study on Apollo": {
        "type": "article",
        "key": "10.1145/3631969",
        "author": "Kochanthara, Sangeeth and Singh, Tajinder and Forrai, Alexandru and Cleophas, Loek",
        "title": "Safety of Perception Systems for Automated Driving: A Case Study on Apollo",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631969",
        "doi": "10.1145/3631969",
        "abstract": "The automotive industry is now known for its software-intensive and safety-critical nature. The industry is on a path to the holy grail of completely automating driving, starting from relatively simple operational areas like highways. One of the most challenging, evolving, and essential parts of automated driving is the software that enables understanding of surroundings and the vehicle\u2019s own as well as surrounding objects\u2019 relative position, otherwise known as the perception system. Current generation perception systems are formed by a combination of traditional software and machine learning-related software. With automated driving systems transitioning from research to production, it is imperative to assess their safety.We assess the safety of Apollo, the most popular open-source automotive software, at the design level for its use on a Dutch highway. We identified 58 safety requirements, 38 of which are found to be fulfilled at the design level. We observe that all requirements relating to traditional software are fulfilled, while most requirements specific to machine learning systems are not. This study unveils issues that need immediate attention; and directions for future research to make automated driving safe.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "64",
        "numpages": "28",
        "keywords": "Automated driving, self-driving, functional safety, safety of intended functionality, ISO 26262, ISO 21448, safety requirements, requirement elicitation, architecture assessment, perception system, machine learning, artificial intelligence"
    },
    "Improving Automated Program Repair with Domain Adaptation": {
        "type": "article",
        "key": "10.1145/3631972",
        "author": "Zirak, Armin and Hemmati, Hadi",
        "title": "Improving Automated Program Repair with Domain Adaptation",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631972",
        "doi": "10.1145/3631972",
        "abstract": "Automated Program Repair (APR) is defined as the process of fixing a bug/defect in the source code, by an automated tool. APR tools have recently experienced promising results by leveraging state-of-the-art Neural Language Processing (NLP) techniques. APR tools such as TFix and CodeXGLUE that combine text-to-text transformers with software-specific techniques are outperforming alternatives, these days. However, in most APR studies, the train and test sets are chosen from the same set of projects (i.e., when APR fixes a bug in the test set from project A, the model has already seen example fixed bugs from project A in the training set). In the real world, however, APR models are meant to be generalizable to new and different projects. Therefore, there is a potential threat that reported APR models with high effectiveness perform poorly when the characteristics of the new project or its bugs are different than the training set\u2019s (\u201cDomain Shift\u201d).In this study, we first define the problem of domain shift in automated program repair. Next, we measure the potential damage of domain shift on two recent APR models (TFix and CodeXGLUE). Based on this observation, we then propose a domain adaptation framework that can adapt an APR model for a given target project. We conduct an empirical study with three domain adaptation methods FullFineTuning, TuningWithLightWeightAdapterLayers, and CurriculumLearning and two APR models on 2,672 bugs from 12 projects.The results show that our proposed framework on average can improve the effectiveness of TFix by 13.05\\% and CodeXGLUE by 48.78\\%, in terms of \u201cExact Match\u201d. Through experiments, we also show that the framework provides high efficiency and reliability (in terms of \u201cExposure Bias\u201d). Using synthetic data to domain adapt TFix and CodeXGLUE on the projects with no data (Zero-shot learning), also results in an average improvement of 5.76\\% and 17.62\\% for TFix and CodeXGLUE, respectively.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "65",
        "numpages": "43",
        "keywords": "Automated program repair, deep learning, neural machine translation, transformers, CodeBERT, domain adaptation"
    },
    "Octopus: Scaling Value-Flow Analysis via Parallel Collection of Realizable Path Conditions": {
        "type": "article",
        "key": "10.1145/3632743",
        "author": "Tang, Wensheng and Dong, Dejun and Li, Shijie and Wang, Chengpeng and Yao, Peisen and Zhou, Jinguo and Zhang, Charles",
        "title": "Octopus: Scaling Value-Flow Analysis via Parallel Collection of Realizable Path Conditions",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3632743",
        "doi": "10.1145/3632743",
        "abstract": "Value-flow analysis is a fundamental technique in program analysis, benefiting various clients, such as memory corruption detection and taint analysis. However, existing efforts suffer from the low potential speedup that leads to a deficiency in scalability. In this work, we present a parallel algorithm Octopus to collect path conditions for realizable paths efficiently. Octopus builds on the realizability decomposition to collect the intraprocedural path conditions of different functions simultaneously on-demand and obtain realizable path conditions by concatenation, which achieves a high potential speedup in parallelization. We implement Octopus as a tool and evaluate it over 15 real-world programs. The experiment shows that Octopus significantly outperforms the state-of-the-art algorithms. Particularly, it detects NULL-pointer-dereference bugs for the project llvm with 6.3 MLoC within 6.9 minutes under the 40-thread setting. We also state and prove several theorems to demonstrate the soundness, completeness, and high potential speedup of Octopus. Our empirical and theoretical results demonstrate the great potential of Octopus in supporting various program analysis clients. The implementation has officially deployed at Ant Group, scaling the nightly code scan for massive FinTech applications.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "66",
        "numpages": "33",
        "keywords": "Value-flow analysis, parallel computation"
    },
    "Deep Is Better? An Empirical Comparison of Information Retrieval and Deep Learning Approaches to Code Summarization": {
        "type": "article",
        "key": "10.1145/3631975",
        "author": "Zhu, Tingwei and Li, Zhong and Pan, Minxue and Shi, Chaoxuan and Zhang, Tian and Pei, Yu and Li, Xuandong",
        "title": "Deep Is Better? An Empirical Comparison of Information Retrieval and Deep Learning Approaches to Code Summarization",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631975",
        "doi": "10.1145/3631975",
        "abstract": "Code summarization aims to generate short functional descriptions for source code to facilitate code comprehension. While Information Retrieval (IR) approaches that leverage similar code snippets and corresponding summaries have led the early research, Deep Learning (DL) approaches that use neural models to capture statistical properties between code and summaries are now mainstream. Although some preliminary studies suggest that IR approaches are more effective in some cases, it is currently unclear how effective the existing approaches can be in general, where and why IR/DL approaches perform better, and whether the integration of IR and DL can achieve better performance. Consequently, there is an urgent need for a comprehensive study of the IR and DL code summarization approaches to provide guidance for future development in this area. This article presents the first large-scale empirical study of 18 IR, DL, and hybrid code summarization approaches on five benchmark datasets. We extensively compare different types of approaches using automatic metrics, we conduct quantitative and qualitative analyses of where and why IR and DL approaches perform better, respectively, and we also study hybrid approaches for assessing the effectiveness of integrating IR and DL. The study shows that the performance of IR approaches should not be underestimated, that while DL models perform better in predicting tokens from method signatures and capturing structural similarities in code, simple IR approaches tend to perform better in the presence of code with high similarity or long reference summaries, and that existing hybrid approaches do not perform as well as individual approaches in their respective areas of strength. Based on our findings, we discuss future research directions for better code summarization.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "67",
        "numpages": "37",
        "keywords": "Code summarization, deep learning, information retrieval"
    },
    "Attack as Detection: Using Adversarial Attack Methods to Detect Abnormal Examples": {
        "type": "article",
        "key": "10.1145/3631977",
        "author": "Zhao, Zhe and Chen, Guangke and Liu, Tong and Li, Taishan and Song, Fu and Wang, Jingyi and Sun, Jun",
        "title": "Attack as Detection: Using Adversarial Attack Methods to Detect Abnormal Examples",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631977",
        "doi": "10.1145/3631977",
        "abstract": "As a new programming paradigm, deep learning (DL) has achieved impressive performance in areas such as image processing and speech recognition, and has expanded its application to solve many real-world problems. However, neural networks and DL are normally black-box systems; even worse, DL-based software are vulnerable to threats from abnormal examples, such as adversarial and backdoored examples constructed by attackers with malicious intentions as well as unintentionally mislabeled samples. Therefore, it is important and urgent to detect such abnormal examples. Although various detection approaches have been proposed respectively addressing some specific types of abnormal examples, they suffer from some limitations; until today, this problem is still of considerable interest. In this work, we first propose a novel characterization to distinguish abnormal examples from normal ones based on the observation that abnormal examples have significantly different (adversarial) robustness from normal ones. We systemically analyze those three different types of abnormal samples in terms of robustness and find that they have different characteristics from normal ones. As robustness measurement is computationally expensive and hence can be challenging to scale to large networks, we then propose to effectively and efficiently measure robustness of an input sample using the cost of adversarially attacking the input, which was originally proposed to test robustness of neural networks against adversarial examples. Next, we propose a novel detection method, named attack as detection (A2D for short), which uses the cost of adversarially attacking an input instead of robustness to check if it is abnormal. Our detection method is generic, and various adversarial attack methods could be leveraged. Extensive experiments show that A2D is more effective than recent promising approaches that were proposed to detect only one specific type of abnormal examples. We also thoroughly discuss possible adaptive attack methods to our adversarial example detection method and show that A2D is still effective in defending carefully designed adaptive adversarial attack methods\u2014for example, the attack success rate drops to 0\\% on CIFAR10.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "68",
        "numpages": "45",
        "keywords": "Deep learning, neural networks, detection, adversarial examples, backdoored samples, mislabeled samples"
    },
    "Representation Learning for Stack Overflow Posts: How Far Are We?": {
        "type": "article",
        "key": "10.1145/3635711",
        "author": "He, Junda and Zhou, Xin and Xu, Bowen and Zhang, Ting and Kim, Kisub and Yang, Zhou and Thung, Ferdian and Irsan, Ivana Clairine and Lo, David",
        "title": "Representation Learning for Stack Overflow Posts: How Far Are We?",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3635711",
        "doi": "10.1145/3635711",
        "abstract": "The tremendous success of Stack Overflow has accumulated an extensive corpus of software engineering knowledge, thus motivating researchers to propose various solutions for analyzing its content. The performance of such solutions hinges significantly on the selection of representation models for Stack Overflow posts. As the volume of literature on Stack Overflow continues to burgeon, it highlights the need for a powerful Stack Overflow post representation model and drives researchers\u2019 interest in developing specialized representation models that can adeptly capture the intricacies of Stack Overflow posts. The state-of-the-art (SOTA) Stack Overflow post representation models are Post2Vec and BERTOverflow, which are built upon neural networks such as convolutional neural network and transformer architecture (e.g., BERT). Despite their promising results, these representation methods have not been evaluated in the same experimental setting. To fill the research gap, we first empirically compare the performance of the representation models designed specifically for Stack Overflow posts (Post2Vec and BERTOverflow) in a wide range of related tasks (i.e., tag recommendation, relatedness prediction, and API recommendation). The results show that Post2Vec cannot further improve the SOTA techniques of the considered downstream tasks, and BERTOverflow shows surprisingly poor performance. To find more suitable representation models for the posts, we further explore a diverse set of transformer-based models, including (1) general domain language models (RoBERTa, Longformer, and GPT2) and (2) language models built with software engineering related textual artifacts (CodeBERT, GraphCodeBERT, seBERT, CodeT5, PLBart, and CodeGen). This exploration shows that models like CodeBERT and RoBERTa are suitable for representing Stack Overflow posts. However, it also illustrates the \u201cNo Silver Bullet\u201d concept, as none of the models consistently wins against all the others. Inspired by the findings, we propose SOBERT, which employs a simple yet effective strategy to improve the representation models of Stack Overflow posts by continuing the pre-training phase with the textual artifact from Stack Overflow. The overall experimental results demonstrate that SOBERT can consistently outperform the considered models and increase the SOTA performance significantly for all the downstream tasks.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "69",
        "numpages": "24",
        "keywords": "Stack Overflow, transformers, pre-trained models"
    },
    "Reusing Convolutional Neural Network Models through Modularization and Composition": {
        "type": "article",
        "key": "10.1145/3632744",
        "author": "Qi, Binhang and Sun, Hailong and Zhang, Hongyu and Gao, Xiang",
        "title": "Reusing Convolutional Neural Network Models through Modularization and Composition",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3632744",
        "doi": "10.1145/3632744",
        "abstract": "With the widespread success of deep learning technologies, many trained deep neural network (DNN) models are now publicly available. However, directly reusing the public DNN models for new tasks often fails due to mismatching functionality or performance. Inspired by the notion of modularization and composition in software reuse, we investigate the possibility of improving the reusability of DNN models in a more fine-grained manner. Specifically, we propose two modularization approaches named CNNSplitter and GradSplitter, which can decompose a trained convolutional neural network (CNN) model for N-class classification into N small reusable modules. Each module recognizes one of the N classes and contains a part of the convolution kernels of the trained CNN model. Then, the resulting modules can be reused to patch existing CNN models or build new CNN models through composition. The main difference between CNNSplitter and GradSplitter lies in their search methods: the former relies on a genetic algorithm to explore search space, while the latter utilizes a gradient-based search method. Our experiments with three representative CNNs on three widely used public datasets demonstrate the effectiveness of the proposed approaches. Compared with CNNSplitter, GradSplitter incurs less accuracy loss, produces much smaller modules (19.88\\% fewer kernels), and achieves better results on patching weak models. In particular, experiments on GradSplitter show that (1) by patching weak models, the average improvement in terms of precision, recall, and F1-score is 17.13\\%, 4.95\\%, and 11.47\\%, respectively, and (2) for a new task, compared with the models trained from scratch, reusing modules achieves similar accuracy (the average loss of accuracy is only 2.46\\%) without a costly training process. Our approaches provide a viable solution to the rapid development and improvement of CNN models.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "70",
        "numpages": "39",
        "keywords": "Model reuse, convolutional neural network, CNN modularization, module composition"
    },
    "SourcererJBF: A Java Build Framework For Large-Scale Compilation": {
        "type": "article",
        "key": "10.1145/3635710",
        "author": "Misu, Md Rakib Hossain and Achar, Rohan and Lopes, Cristina V.",
        "title": "SourcererJBF: A Java Build Framework For Large-Scale Compilation",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3635710",
        "doi": "10.1145/3635710",
        "abstract": "Researchers and tool developers working on dynamic analysis, software testing, automated program repair, verification, and validation, need large compiled, compilable, and executable code corpora to test their ideas. The publicly available corpora are relatively small, and/or non-compilable, and/or non-executable. Developing a compiled code corpus is a laborious activity demanding significant manual effort and human intervention. To facilitate large-scale program analysis research, we develop SourcererJBF, a Java Build Framework that can automatically build a large Java code corpus without project-specific instructions and human intervention. To generate a compiled code corpus, SourcererJBF creates an offline knowledge base by collecting external dependencies from the project directories and existing build scripts (if available). It constructs indices of those collected external dependencies that enable a fast search for resolving dependencies during the project compilation. As the output of the large-scale compilation, it produces JAigantic, a compilable Java corpus containing compiled projects, their bytecode, dependencies, normalized build script, and build command. We evaluated SourcererJBF\u2019s effectiveness, correctness, performance, and scalability in a large collection of Java projects. Our experimental results demonstrate that SourcererJBF is significantly effective and scalable in building large Java code corpus. Besides, it substantiates reasonable performance and correctness similar to projects\u2019 existing build systems.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "71",
        "numpages": "35",
        "keywords": "Automated builds, build systems, Java, program analysis"
    },
    "PTM-APIRec: Leveraging Pre-trained Models of Source Code in API Recommendation": {
        "type": "article",
        "key": "10.1145/3632745",
        "author": "Li, Zhihao and Li, Chuanyi and Tang, Ze and Huang, Wanhong and Ge, Jidong and Luo, Bin and Ng, Vincent and Wang, Ting and Hu, Yucheng and Zhang, Xiaopeng",
        "title": "PTM-APIRec: Leveraging Pre-trained Models of Source Code in API Recommendation",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3632745",
        "doi": "10.1145/3632745",
        "abstract": "Recommending APIs is a practical and essential feature of IDEs. Improving the accuracy of API recommendations is an effective way to improve coding efficiency. With the success of deep learning in software engineering, the state-of-the-art (SOTA) performance of API recommendation is also achieved by deep-learning-based approaches. However, existing SOTAs either only consider the API sequences in the code snippets or rely on complex operations for extracting hand-crafted features, all of which have potential risks in under-encoding the input code snippets and further resulting in sub-optimal recommendation performance. To this end, this article proposes to utilize the code understanding ability of existing general code Pre-Training Models to fully encode the input code snippet to improve the accuracy of API Recommendation, namely, PTM-APIRec. To ensure that the code semantics of the input are fully understood and the API recommended actually exists, we use separate vocabularies for the input code snippet and the APIs to be predicted. The experimental results on the JDK and Android datasets show that PTM-APIRec surpasses existing approaches. Besides, an effective way to improve the performance of PTM-APIRec is to enhance the pre-trained model with more pre-training data (which is easier to obtain than API recommendation datasets).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "72",
        "numpages": "30",
        "keywords": "API recommendation, Code Pre-training, Code Completion"
    },
    "Testing of Deep Reinforcement Learning Agents with Surrogate Models": {
        "type": "article",
        "key": "10.1145/3631970",
        "author": "Biagiola, Matteo and Tonella, Paolo",
        "title": "Testing of Deep Reinforcement Learning Agents with Surrogate Models",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631970",
        "doi": "10.1145/3631970",
        "abstract": "Deep Reinforcement Learning (DRL) has received a lot of attention from the research community in recent years. As the technology moves away from game playing to practical contexts, such as autonomous vehicles and robotics, it is crucial to evaluate the quality of DRL agents.In this article, we propose a search-based approach to test such agents. Our approach, implemented in a tool called Indago, trains a classifier on failure and non-failure environment (i.e., pass) configurations resulting from the DRL training process. The classifier is used at testing time as a surrogate model for the DRL agent execution in the environment, predicting the extent to which a given environment configuration induces a failure of the DRL agent under test. The failure prediction acts as a fitness function, guiding the generation towards failure environment configurations, while saving computation time by deferring the execution of the DRL agent in the environment to those configurations that are more likely to expose failures.Experimental results show that our search-based approach finds 50\\% more failures of the DRL agent than state-of-the-art techniques. Moreover, such failures are, on average, 78\\% more diverse; similarly, the behaviors of the DRL agent induced by failure configurations are 74\\% more diverse.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "73",
        "numpages": "33",
        "keywords": "Software testing, reinforcement learning"
    },
    "Causality-driven Testing of Autonomous Driving Systems": {
        "type": "article",
        "key": "10.1145/3635709",
        "author": "Giamattei, Luca and Guerriero, Antonio and Pietrantuono, Roberto and Russo, Stefano",
        "title": "Causality-driven Testing of Autonomous Driving Systems",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3635709",
        "doi": "10.1145/3635709",
        "abstract": "Testing Autonomous Driving Systems (ADS) is essential for safe development of self-driving cars. For thorough and realistic testing, ADS are usually embedded in a simulator and tested in interaction with the simulated environment. However, their high complexity and the multiple safety requirements lead to costly and ineffective testing. Recent techniques exploit many-objective strategies and ML to efficiently search the huge input space. Despite the indubitable advances, the need for smartening the search keep being pressing. This article presents CART (CAusal-Reasoning-driven Testing), a new technique that formulates testing as a causal reasoning task. Learning causation, unlike correlation, allows assessing the effect of actively changing an input on the output, net of possible confounding variables. CART first infers the causal relations between test inputs and outputs, then looks for promising tests by querying the learnt model. Only tests suggested by the model are run on the simulator. An extensive empirical evaluation, using Pylot as ADS and CARLA as simulator, compares CART with state-of-the-art algorithms used recently on ADS. CART shows a significant gain in exposing more safety violations and does so more efficiently. More broadly, the work opens to a wider exploitation of causal learning beside (or on top of) ML for testing-related tasks.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "74",
        "numpages": "35",
        "keywords": "Self-driving cars, autonomous vehicles, AI testing, search-based software testing, causal reasoning"
    },
    "An Extractive-and-Abstractive Framework for Source Code Summarization": {
        "type": "article",
        "key": "10.1145/3632742",
        "author": "Sun, Weisong and Fang, Chunrong and Chen, Yuchen and Zhang, Quanjun and Tao, Guanhong and You, Yudu and Han, Tingxu and Ge, Yifei and Hu, Yuling and Luo, Bin and Chen, Zhenyu",
        "title": "An Extractive-and-Abstractive Framework for Source Code Summarization",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3632742",
        "doi": "10.1145/3632742",
        "abstract": "(Source) Code summarization aims to automatically generate summaries/comments for given code snippets in the form of natural language. Such summaries play a key role in helping developers understand and maintain source code. Existing code summarization techniques can be categorized into extractive methods and abstractive methods. The extractive methods extract a subset of important statements and keywords from the code snippet using retrieval techniques and generate a summary that preserves factual details in important statements and keywords. However, such a subset may miss identifier or entity naming, and consequently, the naturalness of the generated summary is usually poor. The abstractive methods can generate human-written-like summaries leveraging encoder-decoder models. However, the generated summaries often miss important factual details.To generate human-written-like summaries with preserved factual details, we propose a novel extractive-and-abstractive framework. The extractive module in the framework performs the task of extractive code summarization, which takes in the code snippet and predicts important statements containing key factual details. The abstractive module in the framework performs the task of abstractive code summarization, which takes in the code snippet and important statements in parallel and generates a succinct and human-written-like natural language summary. We evaluate the effectiveness of our technique, called EACS, by conducting extensive experiments on three datasets involving six programming languages. Experimental results show that EACS significantly outperforms state-of-the-art techniques for all three widely used metrics, including BLEU, METEOR, and ROUGH-L. In addition, the human evaluation demonstrates that the summaries generated by EACS have higher naturalness and informativeness and are more relevant to given code snippets.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "75",
        "numpages": "39",
        "keywords": "Code summarization, extractive code summarization, abstractive code summarization program comprehension"
    },
    "Algorithm&nbsp;Selection for Software Verification Using Graph Neural Networks": {
        "type": "article",
        "key": "10.1145/3637225",
        "author": "Leeson, Will and Dwyer, Matthew B.",
        "title": "Algorithm&nbsp;Selection for Software Verification Using Graph Neural Networks",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3637225",
        "doi": "10.1145/3637225",
        "abstract": "The field of software verification has produced a wide array of algorithmic techniques that can prove a variety of properties of a given program. It has been demonstrated that the performance of these techniques can vary up to 4 orders of magnitude on the same verification problem. Even for verification experts, it is difficult to decide which tool will perform best on a given problem. For general users, deciding the best tool for their verification problem is effectively impossible. In this work, we present Graves, a selection strategy based on graph neural networks (GNNs). Graves generates a graph representation of a program from which a GNN predicts a score for a verifier that indicates its performance on the program. We evaluate Graves on a set of 10 verification tools and over 8,000 verification problems and find that it improves the state-of-the-art in verification algorithm selection by 12\\%, or 8 percentage points. Further, it is able to verify 9\\% more problems than any existing verifier on our test set. Through a qualitative study on model interpretability, we find strong evidence that the Graves model learns to base its predictions on factors that relate to the unique features of the algorithmic techniques.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "76",
        "numpages": "36",
        "keywords": "Algorithm&nbsp;selection, graph neural networks"
    },
    "Learning-based Relaxation of Completeness Requirements for Data Entry Forms": {
        "type": "article",
        "key": "10.1145/3635708",
        "author": "Belgacem, Hichem and Li, Xiaochen and Bianculli, Domenico and Briand, Lionel",
        "title": "Learning-based Relaxation of Completeness Requirements for Data Entry Forms",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3635708",
        "doi": "10.1145/3635708",
        "abstract": "Data entry forms use completeness requirements to specify the fields that are required or optional to fill for collecting necessary information from different types of users. However, because of the evolving nature of software, some required fields may not be applicable for certain types of users anymore. Nevertheless, they may still be incorrectly marked as required in the form; we call such fields obsolete required fields. Since obsolete required fields usually have \u201cnot-null\u201d validation checks before submitting the form, users have to enter meaningless values in such fields to complete the form submission. These meaningless values threaten the quality of the filled data and could negatively affect stakeholders or learning-based tools that use the data. To avoid users filling meaningless values, existing techniques usually rely on manually written rules to identify the obsolete required fields and relax their completeness requirements. However, these techniques are ineffective and costly.In this article, we propose LACQUER, a learning-based automated approach for relaxing the completeness requirements of data entry forms. LACQUER builds Bayesian Network models to automatically learn conditions under which users had to fill meaningless values. To improve its learning ability, LACQUER identifies the cases where a required field is only applicable for a small group of users and uses SMOTE, an oversampling technique, to generate more instances on such fields for effectively mining dependencies on them. During the data entry session, LACQUER predicts the completeness requirement of a target based on the already filled fields and their conditional dependencies in the trained model.Our experimental results show that LACQUER can accurately relax the completeness requirements of required fields in data entry forms with precision values ranging between 0.76 and 0.90 on different datasets. LACQUER can prevent users from filling 20\\% to 64\\% of meaningless values, with negative predictive values (i.e., the ability to correctly predict a field as \u201coptional\u201d) between 0.72 and 0.91. Furthermore, LACQUER is efficient; it takes at most 839 ms to predict the completeness requirement of an instance.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "77",
        "numpages": "32",
        "keywords": "Form filling, data entry forms, completeness requirements relaxation, machine learning, software data quality, user interfaces"
    },
    "Vision Transformer Inspired Automated Vulnerability Repair": {
        "type": "article",
        "key": "10.1145/3632746",
        "author": "Fu, Michael and Nguyen, Van and Tantithamthavorn, Chakkrit and Phung, Dinh and Le, Trung",
        "title": "Vision Transformer Inspired Automated Vulnerability Repair",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3632746",
        "doi": "10.1145/3632746",
        "abstract": "Recently, automated vulnerability repair approaches have been widely adopted to combat increasing software security issues. In particular, transformer-based encoder-decoder models achieve competitive results. Whereas vulnerable programs may only consist of a few vulnerable code areas that need repair, existing AVR approaches lack a mechanism guiding their model to pay more attention to vulnerable code areas during repair generation. In this article, we propose a novel vulnerability repair framework inspired by the Vision Transformer based approaches for object detection in the computer vision domain. Similar to the object queries used to locate objects in object detection in computer vision, we introduce and leverage vulnerability queries (VQs) to locate vulnerable code areas and then suggest their repairs. In particular, we leverage the cross-attention mechanism to achieve the cross-match between VQs and their corresponding vulnerable code areas. To strengthen our cross-match and generate more accurate vulnerability repairs, we propose to learn a novel vulnerability mask (VM) and integrate it into decoders\u2019 cross-attention, which makes our VQs pay more attention to vulnerable code areas during repair generation. In addition, we incorporate our VM into encoders\u2019 self-attention to learn embeddings that emphasize the vulnerable areas of a program. Through an extensive evaluation using the real-world 5,417 vulnerabilities, our approach outperforms all of the automated vulnerability repair baseline methods by 2.68\\% to 32.33\\%. Additionally, our analysis of the cross-attention map of our approach confirms the design rationale of our VM and its effectiveness. Finally, our survey study with 71 software practitioners highlights the significance and usefulness of AI-generated vulnerability repairs in the realm of software security. The training code and pre-trained models are available at https://github.com/awsm-research/VQM.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "78",
        "numpages": "29",
        "keywords": "Software security, automated vulnerability repair"
    },
    "Compositional Verification of First-Order Masking Countermeasures against Power Side-Channel Attacks": {
        "type": "article",
        "key": "10.1145/3635707",
        "author": "Gao, Pengfei and Song, Fu and Chen, Taolue",
        "title": "Compositional Verification of First-Order Masking Countermeasures against Power Side-Channel Attacks",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3635707",
        "doi": "10.1145/3635707",
        "abstract": "Power side-channel attacks allow an adversary to efficiently and effectively steal secret information (e.g., keys) by exploiting the correlation between secret data and runtime power consumption, hence posing a serious threat to software security, particularly cryptographic implementations. Masking is a commonly used countermeasure against such attacks, which breaks the statistical dependence between secret data and side-channel leaks via randomization. In a nutshell, a variable is represented by a vector of shares armed with random variables, called masking encoding, on which cryptographic computations are performed. While compositional verification for the security of masked cryptographic implementations has received much attention because of its high efficiency, existing compositional approaches either use implicitly fixed pre-conditions that may not be fulfilled by state-of-the-art efficient implementations, or require user-provided hard-coded pre-conditions that are time consuming and highly non-trivial, even for an expert. In this article, we tackle the compositional verification problem of first-order masking countermeasures, where first-order means that the adversary is allowed to access only one intermediate computation result. Following the literature, we consider countermeasures given as gadgets, which are special procedures whose inputs are masking encodings of variables. We introduce a new security notion parameterized by an explicit pre-condition for each gadget, as well as composition rules for reasoning about masking countermeasures against power side-channel attacks. We propose accompanying efficient algorithms to automatically infer proper pre-conditions, based on which our new compositional approach can efficiently and automatically prove security for masked implementations. We implement our approaches as a tool MaskCV and conduct experiments on publicly available masked cryptographic implementations including 10 different full AES implementations. The experimental results confirm the effectiveness and efficiency of our approach.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "79",
        "numpages": "38",
        "keywords": "Formal verification, compositional verification, cryptographic programs, side-channel attacks, masking countermeasures"
    },
    "Ethics in the Age of AI: An Analysis of AI Practitioners\u2019 Awareness and Challenges": {
        "type": "article",
        "key": "10.1145/3635715",
        "author": "Pant, Aastha and Hoda, Rashina and Spiegler, Simone V. and Tantithamthavorn, Chakkrit and Turhan, Burak",
        "title": "Ethics in the Age of AI: An Analysis of AI Practitioners\u2019 Awareness and Challenges",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3635715",
        "doi": "10.1145/3635715",
        "abstract": "Ethics in AI has become a debated topic of public and expert discourse in recent years. But what do people who build AI\u2014AI practitioners\u2014have to say about their understanding of AI ethics and the challenges associated with incorporating it into the AI-based systems they develop? Understanding AI practitioners\u2019 views on AI ethics is important as they are the ones closest to the AI systems and can bring about changes and improvements. We conducted a survey aimed at understanding AI practitioners\u2019 awareness of AI ethics and their challenges in incorporating ethics. Based on 100 AI practitioners\u2019 responses, our findings indicate that the majority of AI practitioners had a reasonable familiarity with the concept of AI ethics, primarily due to workplace rules and policies. Privacy protection and security was the ethical principle that the majority of them were aware of. Formal education/training was considered somewhat helpful in preparing practitioners to incorporate AI ethics. The challenges that AI practitioners faced in the development of ethical AI-based systems included (i) general challenges, (ii) technology-related challenges, and (iii) human-related challenges. We also identified areas needing further investigation and provided recommendations to assist AI practitioners and companies in incorporating ethics into AI development.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "80",
        "numpages": "35",
        "keywords": "AI ethics, AI practitioners, awareness, challenges, survey"
    },
    "Early Validation and Verification of System Behaviour in Model-based Systems Engineering: A Systematic Literature Review": {
        "type": "article",
        "key": "10.1145/3631976",
        "author": "Cederbladh, Johan and Cicchetti, Antonio and Suryadevara, Jagadish",
        "title": "Early Validation and Verification of System Behaviour in Model-based Systems Engineering: A Systematic Literature Review",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631976",
        "doi": "10.1145/3631976",
        "abstract": "In the Systems Engineering (SE) domain there has been a paradigm shift from document-based to model-based system development artefacts; in fact, new methodologies are emerging to meet the increasing complexity of current systems and the corresponding growing need of digital workflows. In this regard, Model-Based Systems Engineering (MBSE) is considered as a key enabler by many central players of the SE community. MBSE has reached an adequate level of maturity, and there exist documented success stories in its adoption in industry. In particular, one significant benefit of utilising MBSE when compared to the traditional manual and document-centric workflows is that models are available from early phases of systems development; these enable a multitude of analyses prior any implementation effort together with other relevant capabilities, like the automation of development tasks. Nonetheless, it is noticeable there is a lack of a common understanding for how formal analyses for the verification and validation (V&amp;V) of systems behaviour, specifically in the early phases of development, could be placed in an MBSE setting.In this article, we report on the planning, execution, and results of a systematic literature review regarding the early V&amp;V of systems behaviour in the context of model-based systems engineering. The review aims to provide a structured representation of the state of the art with respect to motivations, proposed solutions, and limitations. From an initial set of potentially relevant 701 peer-reviewed publications we selected 149 primary studies, which we analysed according to a rigorous data extraction, analysis, and synthesis process. Based on our results, early V&amp;V has usually the goal of checking the quality of a system design to avoid discovering flaws when parts are being concretely realised; SysML is a de facto standard for describing the system under study, while the solutions for the analyses tend to be varied; also V&amp;V analyses tend to target varied properties with a slight predominance of functional concerns, and following the variation mentioned so far the proposed solutions are largely context specific; the proposed approaches are usually presented without explicit limitations, while when limitations are discussed, readiness of the solutions, handling of analyses simplifications/assumptions, and languages/tools integration are among the most frequently mentioned issues.Based on the survey results and the standard SE practices, we discuss how the current state-of-the-art MBSE supports early V&amp;V of systems behaviour with a special focus on industrial adoption and identify relevant challenges to be researched further.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "81",
        "numpages": "67",
        "keywords": "MBSE, validation, verification, system behaviour, systematic literature review"
    },
    "Understanding Developers Well-being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic\u2014RCR Report": {
        "type": "article",
        "key": "10.1145/3640338",
        "author": "Russo, Daniel and Hanel, Paul H. P. and van Berkel, Niels",
        "title": "Understanding Developers Well-being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic\u2014RCR Report",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3640338",
        "doi": "10.1145/3640338",
        "abstract": "The artifact accompanying the paper \u201cUnderstanding Developers Well-Being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic\u201d provides a comprehensive set of tools, data, and scripts that were utilized in the longitudinal study. Spanning 24 months, from April 2020 to April 2022, the study delves into the shifts in well-being, productivity, social contacts, needs, and several other variables of software engineers during the COVID-19 pandemic. The artifact facilitates the reproduction of the study\u2019s findings, offering a deeper insight into the systematic changes observed in various variables, such as well-being, quality of social contacts, and emotional loneliness. By providing access to the evidence-generating mechanisms and the generated data, the artifact ensures transparency and reproducibility and allows researchers to use our rich dataset to test their own research question. This Replicated Computational Results report aims to detail the contents of the artifact, its relevance to the main paper, and guidelines for its effective utilization.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "82",
        "numpages": "4",
        "keywords": "COVID-19, longitudinal study, well-being, future of work, IJARS Model"
    },
    "DRIVE: Dockerfile Rule Mining and Violation Detection": {
        "type": "article",
        "key": "10.1145/3617173",
        "author": "Zhou, Yu and Zhan, Weilin and Li, Zi and Han, Tingting and Chen, Taolue and Gall, Harald",
        "title": "DRIVE: Dockerfile Rule Mining and Violation Detection",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3617173",
        "doi": "10.1145/3617173",
        "abstract": "A Dockerfile defines a set of instructions to build Docker images, which can then be instantiated to support containerized applications. Recent studies have revealed a considerable amount of quality issues with Dockerfiles. In this article, we propose a novel approach, Dockerfiles Rule mIning and Violation dEtection (DRIVE), to mine implicit rules and detect potential violations of such rules in Dockerfiles. DRIVE first parses Dockerfiles and transforms them to an intermediate representation. It then leverages an efficient sequential pattern mining algorithm to extract potential patterns. With heuristic-based reduction and moderate human intervention, potential rules are identified, which can then be utilized to detect potential violations of Dockerfiles. DRIVE identifies 34 semantic rules and 19 syntactic rules including 9 new semantic rules that have not been reported elsewhere. Extensive experiments on real-world Dockerfiles demonstrate the efficacy of our approach.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "30",
        "numpages": "23",
        "keywords": "violation detection, configuration files, pattern mining, dockerfile, Docker"
    },
    "FQN Inference in Partial Code by Prompt-tuned Language Model of Code": {
        "type": "article",
        "key": "10.1145/3617174",
        "author": "Huang, Qing and Yuan, Zhiqiang and Xing, Zhenchang and Peng, Xin and Xu, Xiwei and Lu, Qinghua",
        "title": "FQN Inference in Partial Code by Prompt-tuned Language Model of Code",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3617174",
        "doi": "10.1145/3617174",
        "abstract": "Partial code usually involves non-fully-qualified type names (non-FQNs) and undeclared receiving objects. Resolving the FQNs of these non-FQN types and undeclared receiving objects (referred to as type inference) is the prerequisite to effective search and reuse of partial code. Existing dictionary-lookup based methods build a symbolic knowledge base of API names and code contexts, which involve significant compilation overhead and are sensitive to unseen API names and code context variations. In this article, we propose using a prompt-tuned code masked language model (MLM) as a neural knowledge base for type inference, called POME, which is lightweight and has minimal requirements on code compilation. Unlike the existing symbol name and context matching for type inference, POME infers the FQNs syntax and usage knowledge encapsulated in prompt-tuned code MLM through a colze-style fill-in-blank strategy. POME is integrated as a plug-in into web and integrated development environments (IDE) to assist developers in inferring FQNs in the real world. We systematically evaluate POME on a large amount of source code from GitHub and Stack Overflow, and explore its generalization and hybrid capability. The results validate the effectiveness of the POME design and its applicability for partial code type inference, and they can be easily extended to different programming languages (PL). POME can also be used to generate a PL-hybrid type inference model for providing a one-for-all solution. As the first of its kind, our neural type inference method opens the door to many innovative ways of using partial code.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "31",
        "numpages": "32",
        "keywords": "neural knowledge base, code masked language model, fully qualified names, Type inference"
    },
    "Probabilistic Safe WCET Estimation for Weakly Hard Real-time Systems at Design Stages": {
        "type": "article",
        "key": "10.1145/3617176",
        "author": "Lee, Jaekwon and Shin, Seung Yeob and Briand, Lionel C. and Nejati, Shiva",
        "title": "Probabilistic Safe WCET Estimation for Weakly Hard Real-time Systems at Design Stages",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3617176",
        "doi": "10.1145/3617176",
        "abstract": "Weakly hard real-time systems can, to some degree, tolerate deadline misses, but their schedulability still needs to be analyzed to ensure their quality of service. Such analysis usually occurs at early design stages to provide implementation guidelines to engineers so they can make better design decisions. Estimating worst-case execution times (WCET) is a key input to schedulability analysis. However, early on during system design, estimating WCET values is challenging, and engineers usually determine them as plausible ranges based on their domain knowledge. Our approach aims at finding restricted, safe WCET sub-ranges given a set of ranges initially estimated by experts in the context of weakly hard real-time systems. To this end, we leverage (1)&nbsp;multi-objective search aiming at maximizing the violation of weakly hard constraints to find worst-case scheduling scenarios and (2)&nbsp;polynomial logistic regression to infer safe WCET ranges with a probabilistic interpretation. We evaluated our approach by applying it to an industrial system in the satellite domain and several realistic synthetic systems. The results indicate that our approach significantly outperforms a baseline relying on random search without learning and estimates safe WCET ranges with a high degree of confidence in practical time (&lt; 23 h).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "32",
        "numpages": "34",
        "keywords": "meta-heuristic search, weakly hard real-time systems, Worst-case execution time"
    },
    "Acrobats and Safety Nets: Problematizing Large-Scale Agile Software Development": {
        "type": "article",
        "key": "10.1145/3617169",
        "author": "Rolland, Knut H. and Fitzgerald, Brian and Dings\\o{}yr, Torgeir and Stol, Klaas-Jan",
        "title": "Acrobats and Safety Nets: Problematizing Large-Scale Agile Software Development",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3617169",
        "doi": "10.1145/3617169",
        "abstract": "Agile development methods have become a standard in the software industry, including in large-scale projects. These methods share a set of underlying assumptions that distinguish them from more traditional plan-driven approaches. In this article, we adopt Alvesson and Sandberg's problematization approach to challenge three key assumptions that are prevalent in the large-scale agile literature: (1) agile and plan-driven methods are mutually exclusive; (2) self-managing and hierarchically organized teams are mutually exclusive; and (3) agile methods can scale through simple linear composition. Using a longitudinal case study of large-scale agile development, we describe a series of trigger events and episodes whereby the agile approach was tailored to address the needs of the large-scale development context, which was very much at odds with these fundamental assumptions. We develop a set of new underlying assumptions which suggest that agile and plan-driven practices are mutually enabling and necessary for coordination and scaling in large-scale agile projects. We develop nine propositions for large-scale agile projects based on these new alternative underlying assumptions. Finally, we summarize our theoretical contribution in a generic process model of continuously adjusting agile and plan-driven practices in order to accommodate process challenges in large-scale agile projects.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "33",
        "numpages": "45",
        "keywords": "multiteam project management, requirements engineering, software architecture, case study, literature review, assumptions, problematization, Large-scale agile"
    },
    "A Closer Look at the Security Risks in the Rust Ecosystem": {
        "type": "article",
        "key": "10.1145/3624738",
        "author": "Zheng, Xiaoye and Wan, Zhiyuan and Zhang, Yun and Chang, Rui and Lo, David",
        "title": "A Closer Look at the Security Risks in the Rust Ecosystem",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3624738",
        "doi": "10.1145/3624738",
        "abstract": "Rust is an emerging programming language designed for the development of systems software. To facilitate the reuse of Rust code, crates.io, as a central package registry of the Rust ecosystem, hosts thousands of third-party Rust packages. The openness of crates.io enables the growth of the Rust ecosystem but comes with security risks by severe security advisories. Although Rust guarantees a software program to be safe via programming language features and strict compile-time checking, the unsafe keyword in Rust allows developers to bypass compiler safety checks for certain regions of code. Prior studies empirically investigate the memory safety and concurrency bugs in the Rust ecosystem, as well as the usage of unsafe keywords in practice. Nonetheless, the literature lacks a systematic investigation of the security risks in the Rust ecosystem. In this article, we perform a comprehensive investigation into the security risks present in the Rust ecosystem, asking \u201cwhat are the characteristics of the vulnerabilities, what are the characteristics of the vulnerable packages, and how are the vulnerabilities fixed in practice?\u201d. To facilitate the study, we first compile a dataset of 433 vulnerabilities, 300 vulnerable code repositories, and 218 vulnerability fix commits in the Rust ecosystem, spanning over 7 years. With the dataset, we characterize the types, life spans, and evolution of the disclosed vulnerabilities. We then characterize the popularity, categorization, and vulnerability density of the vulnerable Rust packages, as well as their versions and code regions affected by the disclosed vulnerabilities. Finally, we characterize the complexity of vulnerability fixes and localities of corresponding code changes, and inspect how practitioners fix vulnerabilities in Rust packages with various localities. We find that memory safety and concurrency issues account for nearly two thirds of the vulnerabilities in the Rust ecosystem. It takes over 2 years for the vulnerabilities to become publicly disclosed, and one-third of the vulnerabilities have no fixes committed before their disclosure. In terms of vulnerability density, we observe a continuous upward trend at the package level over time, but a decreasing trend at the code level since August 2020. In the vulnerable Rust packages, the vulnerable code tends to be localized at the file level, and contains statistically significantly more unsafe functions and blocks than the rest of the code. More popular packages tend to have more vulnerabilities, while the less popular packages suffer from vulnerabilities for more versions. The vulnerability fix commits tend to be localized to a limited number of lines of code. Developers tend to address vulnerable safe functions by adding safe functions or lines to them, vulnerable unsafe blocks by removing them, and vulnerable unsafe functions by modifying unsafe trait implementations. Based on our findings, we discuss implications, provide recommendations for software practitioners, and outline directions for future research.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "34",
        "numpages": "30",
        "keywords": "empirical study, vulnerability, security risks, ecosystem, Rust"
    },
    "Stress Testing Control Loops in Cyber-physical Systems": {
        "type": "article",
        "key": "10.1145/3624742",
        "author": "Mandrioli, Claudio and Shin, Seung Yeob and Maggio, Martina and Bianculli, Domenico and Briand, Lionel",
        "title": "Stress Testing Control Loops in Cyber-physical Systems",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3624742",
        "doi": "10.1145/3624742",
        "abstract": "Cyber-physical Systems (CPSs) are often safety-critical and deployed in uncertain environments. Identifying scenarios where CPSs do not comply with requirements is fundamental but difficult due to the multidisciplinary nature of CPSs. We investigate the testing of control-based CPSs, where control and software engineers develop the software collaboratively. Control engineers make design assumptions during system development to leverage control theory and obtain guarantees on CPS behaviour. In the implemented system, however, such assumptions are not always satisfied, and their falsification can lead the loss of guarantees. We define stress testing of control-based CPSs as generating tests to falsify such design assumptions. We highlight different types of assumptions, focusing on the use of linearised physics models. To generate stress tests falsifying such assumptions, we leverage control theory to qualitatively characterise the input space of a control-based CPS. We propose a novel test parametrisation for control-based CPSs and use it with the input space characterisation to develop a stress testing approach. We evaluate our approach on three case study systems, including a drone, a continuous-current motor (in five configurations), and an aircraft. Our results show the effectiveness of the proposed testing approach in falsifying the design assumptions and highlighting the causes of assumption violations.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "35",
        "numpages": "58",
        "keywords": "control theory, software testing, Cyber-physical systems"
    },
    "Understanding the Helpfulness of Stale Bot for Pull-Based Development: An Empirical Study of 20 Large Open-Source Projects": {
        "type": "article",
        "key": "10.1145/3624739",
        "author": "Khatoonabadi, Sayedhassan and Costa, Diego Elias and Mujahid, Suhaib and Shihab, Emad",
        "title": "Understanding the Helpfulness of Stale Bot for Pull-Based Development: An Empirical Study of 20 Large Open-Source Projects",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3624739",
        "doi": "10.1145/3624739",
        "abstract": "Pull Requests (PRs) that are neither progressed nor resolved clutter the list of PRs, making it difficult for the maintainers to manage and prioritize unresolved PRs. To automatically track, follow up, and close such inactive PRs, Stale bot was introduced by GitHub. Despite its increasing adoption, there are ongoing debates on whether using Stale bot alleviates or exacerbates the problem of inactive PRs. To better understand if and how Stale bot helps projects in their pull-based development workflow, we perform an empirical study of 20 large and popular open source projects. We find that Stale bot can help deal with a backlog of unresolved PRs, as the projects closed more PRs within the first few months of adoption. Moreover, Stale bot can help improve the efficiency of the PR review process as the projects reviewed PRs that ended up merged and resolved PRs that ended up closed faster after the adoption. However, Stale bot can also negatively affect the contributors, as the projects experienced a considerable decrease in their number of active contributors after the adoption. Therefore, relying solely on Stale bot to deal with inactive PRs may lead to decreased community engagement and an increased probability of contributor abandonment.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "36",
        "numpages": "43",
        "keywords": "open source software, social coding platforms, modern code review, pull-based development, pull request abandonment, Software development bots"
    },
    "Characterizing and Detecting WebAssembly Runtime Bugs": {
        "type": "article",
        "key": "10.1145/3624743",
        "author": "Zhang, Yixuan and Cao, Shangtong and Wang, Haoyu and Chen, Zhenpeng and Luo, Xiapu and Mu, Dongliang and Ma, Yun and Huang, Gang and Liu, Xuanzhe",
        "title": "Characterizing and Detecting WebAssembly Runtime Bugs",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3624743",
        "doi": "10.1145/3624743",
        "abstract": "WebAssembly (abbreviated WASM) has emerged as a promising language of the Web and also been used for a wide spectrum of software applications such as mobile applications and desktop applications. These applications, named WASM applications, commonly run in WASM runtimes. Bugs in WASM runtimes are frequently reported by developers and cause the crash of WASM applications. However, these bugs have not been well studied. To fill in the knowledge gap, we present a systematic study to characterize and detect bugs in WASM runtimes. We first harvest a dataset of 311 real-world bugs from hundreds of related posts on GitHub. Based on the collected high-quality bug reports, we distill 31 bug categories of WASM runtimes and summarize their common fix strategies. Furthermore, we develop a pattern-based bug detection framework to automatically detect bugs in WASM runtimes. We apply the detection framework to seven popular WASM runtimes and successfully uncover 60 bugs that have never been reported previously, among which 13 have been confirmed and 9 have been fixed by runtime developers.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "37",
        "numpages": "29",
        "keywords": "WebAssembly runtime, WebAssembly"
    },
    "LoGenText-Plus: Improving Neural Machine Translation Based Logging Texts Generation with Syntactic Templates": {
        "type": "article",
        "key": "10.1145/3624740",
        "author": "Ding, Zishuo and Tang, Yiming and Cheng, Xiaoyu and Li, Heng and Shang, Weiyi",
        "title": "LoGenText-Plus: Improving Neural Machine Translation Based Logging Texts Generation with Syntactic Templates",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3624740",
        "doi": "10.1145/3624740",
        "abstract": "Developers insert logging statements in the source code to collect important runtime information about software systems. The textual descriptions in logging statements (i.e., logging texts) are printed during system executions and exposed to multiple stakeholders including developers, operators, users, and regulatory authorities. Writing proper logging texts is an important but often challenging task for developers. Prior studies find that developers spend significant efforts modifying their logging texts. However, despite extensive research on automated logging suggestions, research on suggesting logging texts rarely exists. To fill this knowledge gap, we first propose LoGenText (initially reported in our conference paper), an automated approach that uses neural machine translation (NMT) models to generate logging texts by translating the related source code into short textual descriptions. LoGenText takes the preceding source code of a logging text as the input and considers other context information, such as the location of the logging statement, to automatically generate the logging text. LoGenText\u2019s evaluation on 10 open source projects indicates that the approach is promising for automatic logging text generation and significantly outperforms the state-of-the-art approach. Furthermore, we extend LoGenText to LoGenText-Plus by incorporating the syntactic templates of the logging texts. Different from LoGenText, LoGenText-Plus decomposes the logging text generation process into two stages. LoGenText-Plus first adopts an NMT model to generate the syntactic template of the target logging text. Then LoGenText-Plus feeds the source code and the generated template as the input to another NMT model for logging text generation. We also evaluate LoGenText-Plus on the same 10 projects and observe that it outperforms LoGenText on 9 of them. According to a human evaluation from developers\u2019 perspectives, the logging texts generated by LoGenText-Plus have a higher quality than those generated by LoGenText and the prior baseline approach. By manually examining the generated logging texts, we then identify five aspects that can serve as guidance for writing or generating good logging texts. Our work is an important step toward the automated generation of logging statements, which can potentially save developers\u2019 efforts and improve the quality of software logging. Our findings shed light on research opportunities that leverage advances in NMT techniques for automated generation and suggestion of logging statements.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "38",
        "numpages": "45",
        "keywords": "neural machine translation, logging text, Software logging"
    },
    "ALL: Supporting Experiential Accessibility Education and Inclusive Software Development": {
        "type": "article",
        "key": "10.1145/3625292",
        "author": "Shi, Weishi and Moses, Heather and Yu, Qi and Malachowsky, Samuel and Krutz, Daniel E.",
        "title": "ALL: Supporting Experiential Accessibility Education and Inclusive Software Development",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3625292",
        "doi": "10.1145/3625292",
        "abstract": "Creating accessible software is imperative for making software inclusive for all users.Unfortunately, the topic of accessibility is frequently excluded from computing education, leading to scenarios where students are unaware of either how to develop accessible software or see the need to create it. To address this challenge, we have created a set of educational labs that are systematically designed to not only inform students about fundamental topics in producing accessible software but also demonstrate its importance. Over the previous year, these labs were included in several Computer Science 2 offerings at the Rochester Institute of Technology, comprising a total of 500&nbsp;student participants. This article discusses instructional observations from these offerings, some of which include the following: (i) many of the research findings from previous efforts remain true with the larger, more diverse evaluation; (ii) our created material and format reduced students\u2019 belief that creating accessible software was difficult in relation to the baseline,; (iii) we observed that our created material and format benefited student opinion that creating accessible software is important, and (iv) computing majors may not be uniformly impacted by experiential educational accessibility material. The educational labs are publicly available on the project website (https://all.rit.edu).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "39",
        "numpages": "30",
        "keywords": "computing accessibility, computing education, Accessibility education"
    },
    "Search-Based Software Testing Driven by Automatically Generated and Manually Defined Fitness Functions": {
        "type": "article",
        "key": "10.1145/3624745",
        "author": "Formica, Federico and Fan, Tony and Menghi, Claudio",
        "title": "Search-Based Software Testing Driven by Automatically Generated and Manually Defined Fitness Functions",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3624745",
        "doi": "10.1145/3624745",
        "abstract": "Search-based software testing (SBST) typically relies on fitness functions to guide the search exploration toward software failures. There are two main techniques to define fitness functions: (a)&nbsp;automated fitness function computation from the specification of the system requirements, and (b)&nbsp;manual fitness function design. Both techniques have advantages. The former uses information from the system requirements to guide the search toward portions of the input domain more likely to contain failures. The latter uses the engineers\u2019 domain knowledge.We propose ATheNA, a novel SBST framework that combines fitness functions automatically generated from requirements specifications and those manually defined by engineers. We design and implement ATheNA-S, an instance of ATheNA that targets Simulink\u00ae models. We evaluate ATheNA-S by considering a large set of models from different domains. Our results show that ATheNA-S generates more failure-revealing test cases than existing baseline tools and that the difference between the runtime performance of ATheNA-S and the baseline tools is not statistically significant. We also assess whether ATheNA-S could generate failure-revealing test cases when applied to two representative case studies: one from the automotive domain and one from the medical domain. Our results show that ATheNA-S successfully revealed a requirement violation in our case studies.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "40",
        "numpages": "37",
        "keywords": "CPS, fitness functions, falsification, Testing"
    },
    "Variable-based Fault Localization via Enhanced Decision Tree": {
        "type": "article",
        "key": "10.1145/3624741",
        "author": "Jiang, Jiajun and Wang, Yumeng and Chen, Junjie and Lv, Delin and Liu, Mengjiao",
        "title": "Variable-based Fault Localization via Enhanced Decision Tree",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3624741",
        "doi": "10.1145/3624741",
        "abstract": "Fault localization, aiming at localizing the root cause of the bug under repair, has been a longstanding research topic. Although many approaches have been proposed in past decades, most of the existing studies work at coarse-grained statement or method levels with very limited insights about how to repair the bug (granularity problem), but few studies target the finer-grained fault localization. In this article, we target the granularity problem and propose a novel finer-grained variable-level fault localization technique. Specifically, the basic idea of our approach is that fault-relevant variables may exhibit different values in failed and passed test runs, and variables that have higher discrimination ability have a larger possibility to be the root causes of the failure. Based on this, we propose a program-dependency-enhanced decision tree model to boost the identification of fault-relevant variables via discriminating failed and passed test cases based on the variable values. To evaluate the effectiveness of our approach, we have implemented it in a tool called VarDT and conducted an extensive study over the Defects4J benchmark. The results show that VarDT outperforms the state-of-the-art fault localization approaches with at least 268.4\\% improvement in terms of bugs located at Top-1, and the average improvement is 351.3\\%. Besides, to investigate whether our finer-grained fault localization result can further improve the effectiveness of downstream APR techniques, we have adapted VarDT to the application of patch filtering, where we use the variables located by VarDT to filter incorrect patches. The results denote that VarDT outperforms the state-of-the-art PATCH-SIM and BATS by filtering 14.8\\% and 181.8\\% more incorrect patches, respectively, demonstrating the effectiveness of our approach. It also provides a new way of thinking for improving automatic program repair techniques.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "41",
        "numpages": "32",
        "keywords": "decision tree, program debugging, Fault localization"
    },
    "Hierarchical Distribution-aware Testing of Deep Learning": {
        "type": "article",
        "key": "10.1145/3625290",
        "author": "Huang, Wei and Zhao, Xingyu and Banks, Alec and Cox, Victoria and Huang, Xiaowei",
        "title": "Hierarchical Distribution-aware Testing of Deep Learning",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3625290",
        "doi": "10.1145/3625290",
        "abstract": "With its growing use in safety/security-critical applications, Deep Learning (DL) has raised increasing concerns regarding its dependability. In particular, DL has a notorious problem of lacking robustness. Input added with adversarial perturbations, i.e., Adversarial Examples (AEs), are easily mispredicted by the DL model. Despite recent efforts made in detecting AEs via state-of-the-art attack and testing methods, they are normally input distribution\u2013agnostic and/or disregard the perceptual quality of adversarial perturbations. Consequently, the detected AEs are irrelevant inputs in the application context or noticeably unrealistic to humans. This may lead to a limited effect on improving the DL model\u2019s dependability, as the testing budget is likely to be wasted on detecting AEs that are encountered very rarely in its real-life operations. In this article, we propose a new robustness testing approach for detecting AEs that considers both the feature-level distribution and the pixel-level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. First, we select test seeds based on the density of feature-level distribution and the vulnerability of adversarial robustness. The vulnerability of test seeds is indicated by the auxiliary information, which are highly correlated with local robustness. Given a test seed, we then develop a novel genetic algorithm\u2013based local test case generation method, in which two fitness functions work alternatively to control the perceptual quality of detected AEs. Finally, extensive experiments confirm that our holistic approach considering hierarchical distributions is superior to the state-of-the-arts that either disregard any input distribution or only consider a single (non-hierarchical) distribution, in terms of not only detecting imperceptible AEs but also improving the overall robustness of the DL model under testing.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "42",
        "numpages": "35",
        "keywords": "safe AI, robustness growth, distribution-aware testing, natural perturbations, adversarial examples detection, Deep learning robustness"
    },
    "Learning to Detect Memory-related Vulnerabilities": {
        "type": "article",
        "key": "10.1145/3624744",
        "author": "Cao, Sicong and Sun, Xiaobing and Bo, Lili and Wu, Rongxin and Li, Bin and Wu, Xiaoxue and Tao, Chuanqi and Zhang, Tao and Liu, Wei",
        "title": "Learning to Detect Memory-related Vulnerabilities",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3624744",
        "doi": "10.1145/3624744",
        "abstract": "Memory-related vulnerabilities can result in performance degradation or even program crashes, constituting severe threats to the security of modern software. Despite the promising results of deep learning (DL)-based vulnerability detectors, there exist three main limitations: (1) rich contextual program semantics related to vulnerabilities have not yet been fully modeled; (2) multi-granularity vulnerability features in hierarchical code structure are still hard to be captured; and (3) heterogeneous flow information is not well utilized. To address these limitations, in this article, we propose a novel DL-based approach, called MVD+, to detect memory-related vulnerabilities at the statement-level. Specifically, it conducts both intraprocedural and interprocedural analysis to model vulnerability features, and adopts a hierarchical representation learning strategy, which performs syntax-aware neural embedding within statements and captures structured context information across statements based on a novel Flow-Sensitive Graph Neural Networks, to learn both syntactic and semantic features of vulnerable code. To demonstrate the performance, we conducted extensive experiments against eight state-of-the-art DL-based approaches as well as five well-known static analyzers on our constructed dataset with 6,879 vulnerabilities in 12 popular C/C++ applications. The experimental results confirmed that MVD+ can significantly outperform current state-of-the-art baselines and make a great trade-off between effectiveness and efficiency.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "43",
        "numpages": "35",
        "keywords": "flow analysis, graph neural networks, abstract syntax tree, Statement-level vulnerability detection"
    },
    "Poracle: Testing Patches under Preservation Conditions to Combat the Overfitting Problem of Program Repair": {
        "type": "article",
        "key": "10.1145/3625293",
        "author": "Ismayilzada, Elkhan and Rahman, Md Mazba Ur and Kim, Dongsun and Yi, Jooyong",
        "title": "Poracle: Testing Patches under Preservation Conditions to Combat the Overfitting Problem of Program Repair",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3625293",
        "doi": "10.1145/3625293",
        "abstract": "To date, the users of test-driven program repair tools suffer from the overfitting problem; a generated patch may pass all available tests without being correct. In the existing work, users are treated as merely passive consumers of the tests. However, what if they are willing to modify the test to better assess the patches obtained from a repair tool? In this work, we propose a novel semi-automatic patch-classification methodology named Poracle. Our key contributions are three-fold. First, we design a novel lightweight specification method that reuses the existing test. Specifically, the users extend the existing failing test with a preservation condition\u2014the condition under which the patched and pre-patched versions should produce the same output. Second, we develop a fuzzer that performs differential fuzzing with a test containing a preservation condition. Once we find an input that satisfies a specified preservation condition but produces different outputs between the patched and pre-patched versions, we classify the patch as incorrect with high confidence. We show that our approach is more effective than the four state-of-the-art patch classification approaches. Last, we show through a user study that the users find our semi-automatic patch assessment method more effective and preferable than the manual assessment.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "44",
        "numpages": "39",
        "keywords": "preservation condition, patch classification, patch validation, overfitting problem, Automated program repair"
    },
    "CLFuzz: Vulnerability Detection of Cryptographic Algorithm&nbsp;Implementation via Semantic-aware Fuzzing": {
        "type": "article",
        "key": "10.1145/3628160",
        "author": "Zhou, Yuanhang and Ma, Fuchen and Chen, Yuanliang and Ren, Meng and Jiang, Yu",
        "title": "CLFuzz: Vulnerability Detection of Cryptographic Algorithm&nbsp;Implementation via Semantic-aware Fuzzing",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3628160",
        "doi": "10.1145/3628160",
        "abstract": "Cryptography is a core component of many security applications, and flaws hidden in its implementation will affect the functional integrity or, more severely, pose threats to data security. Hence, guaranteeing the correctness of the implementation is important. However, the semantic characteristics (e.g., diverse input data and complex functional transformation) challenge those traditional program validation techniques (e.g., static analysis and dynamic fuzzing). In this article, we propose CLFuzz, a semantic-aware fuzzer for the vulnerability detection of cryptographic algorithm implementation. CLFuzz first extracts the semantic information of targeted algorithms including their cryptographic-specific constraints and function signatures. Based on them, CLFuzz generates high-quality input data adaptively to trigger error-prone situations efficiently. Furthermore, CLFuzz applies innovative logical cross-check that strengthens the logical bug detection ability. We evaluate CLFuzz on the widely used implementations of 54 cryptographic algorithms. It outperforms state-of-the-art cryptographic fuzzing tools. For example, compared with Cryptofuzz, it achieves a coverage speedup of 3.4\\texttimes{} and increases the final coverage by 14.4\\%. Furthermore, CLFuzz has detected 12 previously unknown implementation bugs in 8 cryptographic algorithms (e.g., CMAC in OpenSSL and Message Digest in SymCrypt), most of which are security-critical and have been successfully collected in the national vulnerability database (7 in NVD/CNVD) and is awarded by the Microsoft bounty program (2 for $1,000).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "45",
        "numpages": "28",
        "keywords": "implementation bug, fuzzing, Cryptographic algorithm"
    },
    "Automated Test Suite Generation for Software Product Lines Based on Quality-Diversity Optimization": {
        "type": "article",
        "key": "10.1145/3628158",
        "author": "Xiang, Yi and Huang, Han and Li, Sizhe and Li, Miqing and Luo, Chuan and Yang, Xiaowei",
        "title": "Automated Test Suite Generation for Software Product Lines Based on Quality-Diversity Optimization",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3628158",
        "doi": "10.1145/3628158",
        "abstract": "A Software Product Line (SPL) is a set of software products that are built from a variability model. Real-world SPLs typically involve a vast number of valid products, making it impossible to individually test each of them. This arises the need for automated test suite generation, which was previously modeled as either a single-objective or a multi-objective optimization problem considering only objective functions. This article provides a completely different mathematical model by exploiting the benefits of Quality-Diversity (QD) optimization that is composed of not only an objective function (e.g., t-wise coverage or test suite diversity) but also a user-defined behavior space (e.g., the space with test suite size as its dimension). We argue that the new model is more suitable and generic than the two alternatives because it provides at a time a large set of diverse (measured in the behavior space) and high-performing solutions that can ease the decision-making process. We apply MAP-Elites, one of the most popular QD algorithms, to solve the model. The results of the evaluation, on both realistic and artificial SPLs, are promising, with MAP-Elites significantly and substantially outperforming both single- and multi-objective approaches, and also several state-of-the-art SPL testing tools. In summary, this article provides a new and promising perspective on the test suite generation for SPLs.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "46",
        "numpages": "52",
        "keywords": "Quality-Diversity (QD) optimization, automated test suite generation, Software Product Line"
    },
    "Automated Mapping of Adaptive App GUIs from Phones to TVs": {
        "type": "article",
        "key": "10.1145/3631968",
        "author": "Hu, Han and Dong, Ruiqi and Grundy, John and Nguyen, Thai Minh and Liu, Huaxiao and Chen, Chunyang",
        "title": "Automated Mapping of Adaptive App GUIs from Phones to TVs",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631968",
        "doi": "10.1145/3631968",
        "abstract": "With the increasing interconnection of smart devices, users often desire to adopt the same app on quite different devices for identical tasks, such as watching the same movies on both their smartphones and TVs. However, the significant differences in screen size, aspect ratio, and interaction styles make it challenging to adapt Graphical User Interfaces (GUIs) across these devices. Although there are millions of apps available on Google Play, only a few thousand are designed to support smart TV displays. Existing techniques to map a mobile app GUI to a TV either adopt a responsive design, which struggles to bridge the substantial gap between phone and TV, or use mirror apps for improved video display, which requires hardware support and extra engineering efforts. Instead of developing another app for supporting TVs, we propose a semi-automated approach to generate corresponding adaptive TV GUIs, given the phone GUIs as the input. Based on our empirical study of GUI pairs for TVs and phones in existing apps, we synthesize a list of rules for grouping and classifying phone GUIs, converting them to TV GUIs, and generating dynamic TV layouts and source code for the TV display. Our tool is not only beneficial to developers but also to GUI designers, who can further customize the generated GUIs for their TV app development. An evaluation and user study demonstrate the accuracy of our generated GUIs and the usefulness of our tool.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "47",
        "numpages": "31",
        "keywords": "adaptive GUI, cross-screen, Graphic user interface"
    },
    "KAPE: kNN-based Performance Testing for Deep Code Search": {
        "type": "article",
        "key": "10.1145/3624735",
        "author": "Guo, Yuejun and Hu, Qiang and Xie, Xiaofei and Cordy, Maxime and Papadakis, Mike and Le Traon, Yves",
        "title": "KAPE: kNN-based Performance Testing for Deep Code Search",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3624735",
        "doi": "10.1145/3624735",
        "abstract": "Code search is a common yet important activity of software developers. An efficient code search model can largely facilitate the development process and improve the programming quality. Given the superb performance of learning the contextual representations, deep learning models, especially pre-trained language models, have been widely explored for the code search task. However, studies mainly focus on proposing new architectures for ever-better performance on designed test sets but ignore the performance on unseen test data where only natural language queries are available. The same problem in other domains, e.g., CV and NLP, is usually solved by test input selection that uses a subset of the unseen set to reduce the labeling effort. However, approaches from other domains are not directly applicable and still require labeling effort. In this article, we propose the kNN-based performance testing (KAPE) to efficiently solve the problem without manually matching code snippets to test queries. The main idea is to use semantically similar training data to perform the evaluation. Extensive experiments on six programming language datasets, three state-of-the-art pre-trained models, and seven baseline methods demonstrate that KAPE can effectively assess the model performance (e.g., CodeBERT achieves MRR 0.5795 on JavaScript) with a slight difference (e.g., 0.0261).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "48",
        "numpages": "24",
        "keywords": "test selection, deep learning testing, software testing, Deep code search"
    },
    "Aspect-level Information Discrepancies across Heterogeneous Vulnerability Reports: Severity, Types and Detection Methods": {
        "type": "article",
        "key": "10.1145/3624734",
        "author": "Sun, Jiamou and Xing, Zhenchang and Xia, Xin and Lu, Qinghua and Xu, Xiwei and Zhu, Liming",
        "title": "Aspect-level Information Discrepancies across Heterogeneous Vulnerability Reports: Severity, Types and Detection Methods",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3624734",
        "doi": "10.1145/3624734",
        "abstract": "Vulnerable third-party libraries pose significant threats to software applications that reuse these libraries. At an industry scale of reuse, manual analysis of third-party library vulnerabilities can be easily overwhelmed by the sheer number of vulnerabilities continually collected from diverse sources for thousands of reused libraries. Our study of four large-scale, actively maintained vulnerability databases (NVD, IBM X-Force, ExploitDB, and Openwall) reveals the wide presence of information discrepancies, in terms of seven vulnerability aspects, i.e., product, version, component, vulnerability type, root cause, attack vector, and impact, between the reports for the same vulnerability from heterogeneous sources. It would be beneficial to integrate and cross-validate multi-source vulnerability information, but it demands automatic aspect extraction and aspect discrepancy detection. In this work, we experimented with a wide range of NLP methods to extract named entities (e.g., product) and free-form phrases (e.g., root cause) from textual vulnerability reports and to detect semantically different aspect mentions between the reports. Our experiments confirm the feasibility of applying NLP methods to automate aspect-level vulnerability analysis and identify the need for domain customization of general NLP methods. Based on our findings, we propose a discrepancy-aware, aspect-level vulnerability knowledge graph and a KG-based web portal that integrates diversified vulnerability key aspect information from heterogeneous vulnerability databases. Our conducted user study proves the usefulness of our web portal. Our study opens the door to new types of vulnerability integration and management, such as vulnerability portraits of a product and explainable prediction of silent vulnerabilities.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "49",
        "numpages": "38",
        "keywords": "hetergeneous vulnerability reports, information discrepancy, Vulnerability key aspect"
    },
    "Generation-based Differential Fuzzing for Deep Learning Libraries": {
        "type": "article",
        "key": "10.1145/3628159",
        "author": "Liu, Jiawei and Huang, Yuheng and Wang, Zhijie and Ma, Lei and Fang, Chunrong and Gu, Mingzheng and Zhang, Xufan and Chen, Zhenyu",
        "title": "Generation-based Differential Fuzzing for Deep Learning Libraries",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3628159",
        "doi": "10.1145/3628159",
        "abstract": "Deep learning (DL) libraries have become the key component in developing and deploying DL-based software nowadays. With the growing popularity of applying DL models in both academia and industry across various domains, any bugs inherent in the DL libraries can potentially cause unexpected server outcomes. As such, there is an urgent demand for improving the software quality of DL libraries. Although there are some existing approaches specifically designed for testing DL libraries, their focus is usually limited to one specific domain, such as computer vision (CV). It is still not very clear how the existing approaches perform in detecting bugs of different DL libraries regarding different task domains and to what extent. To bridge this gap, we first conduct an empirical study on four representative and state-of-the-art DL library testing approaches. Our empirical study results reveal that it is hard for existing approaches to generalize to other task domains. We also find that the test inputs generated by these approaches usually lack diversity, with only a few types of bugs. What is worse, the false-positive rate of existing approaches is also high (up to 58\\%). To address these issues, we propose a guided differential fuzzing approach based on generation, namely, Gandalf. To generate testing inputs across diverse task domains effectively, Gandalf adopts the context-free grammar to ensure validity and utilizes a Deep Q-Network to maximize the diversity. Gandalf also includes 15 metamorphic relations to make it possible for the generated test cases to generalize across different DL libraries. Such a design can decrease the false positives because of the semantic difference for different APIs. We evaluate the effectiveness of Gandalf on nine versions of three representative DL libraries, covering 309 operators from computer vision, natural language processing, and automated speech recognition. The evaluation results demonstrate that Gandalf can effectively and efficiently generate diverse test inputs. Meanwhile, Gandalf successfully detects five categories of bugs with only 3.1\\% false-positive rates. We report all 49 new unique bugs found during the evaluation to the DL libraries\u2019 developers, and most of these bugs have been confirmed. Details about our empirical study and evaluation results are available on our project website.1",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "50",
        "numpages": "28",
        "keywords": "generation-based fuzzing, deep learning libraries, Software testing"
    },
    "The Good, the Bad, and the Missing: Neural Code Generation for Machine Learning Tasks": {
        "type": "article",
        "key": "10.1145/3630009",
        "author": "Shin, Jiho and Wei, Moshi and Wang, Junjie and Shi, Lin and Wang, Song",
        "title": "The Good, the Bad, and the Missing: Neural Code Generation for Machine Learning Tasks",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3630009",
        "doi": "10.1145/3630009",
        "abstract": "Machine learning (ML) has been increasingly used in a variety of domains, while solving ML programming tasks poses unique challenges due to the fundamental difference in the nature and the construct of general programming tasks, especially for developers who do not have ML backgrounds. Automatic code generation that produces a code snippet from a natural language description can be a promising technique to accelerate ML programming tasks. In recent years, although many deep learning-based neural code generation models have been proposed with high accuracy, the fact that most of them are mainly evaluated on general programming tasks calls into question their effectiveness and usefulness in ML programming tasks. In this article, we set out to investigate the effectiveness of existing neural code generation models on ML programming tasks. For our analysis, we select six state-of-the-art neural code generation models and evaluate their performance on four widely used ML libraries, with newly created 83K pairs of natural-language described ML programming tasks. Our empirical study reveals some good, bad, and missing aspects of neural code generation models on ML tasks, with a few major ones listed below. (Good) Neural code generation models perform significantly better on ML tasks than on non-ML tasks with an average difference of 10.6 points in BLEU-4 scores. (Bad) More than 80\\% of the generated code is semantically incorrect. (Bad) Code generation models do not have significance in improving developers\u2019 completion time. (Good) The generated code can help developers write correct code by providing developers with clues for using correct APIs. (Missing) The observation from our user study reveals the missing aspects of code generation for ML tasks, e.g., decomposing code generation for divide-and-conquer into API sequence identification and API usage generation.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "51",
        "numpages": "24",
        "keywords": "empirical analysis, machine learning tasks, Neural code generation"
    },
    "LibAM: An Area Matching Framework for Detecting Third-Party Libraries in Binaries": {
        "type": "article",
        "key": "10.1145/3625294",
        "author": "Li, Siyuan and Wang, Yongpan and Dong, Chaopeng and Yang, Shouguo and Li, Hong and Sun, Hao and Lang, Zhe and Chen, Zuxin and Wang, Weijie and Zhu, Hongsong and Sun, Limin",
        "title": "LibAM: An Area Matching Framework for Detecting Third-Party Libraries in Binaries",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3625294",
        "doi": "10.1145/3625294",
        "abstract": "Third-party libraries (TPLs) are extensively utilized by developers to expedite the software development process and incorporate external functionalities. Nevertheless, insecure TPL reuse can lead to significant security risks. Existing methods, which involve extracting strings or conducting function matching, are employed to determine the presence of TPL code in the target binary. However, these methods often yield unsatisfactory results due to the recurrence of strings and the presence of numerous similar non-homologous functions. Furthermore, the variation in C/C++ binaries across different optimization options and architectures exacerbates the problem. Additionally, existing approaches struggle to identify specific pieces of reused code in the target binary, complicating the detection of complex reuse relationships and impeding downstream tasks. And, we call this issue the poor interpretability of TPL detection results.In this article, we observe that TPL reuse typically involves not just isolated functions but also areas encompassing several adjacent functions on the Function Call Graph (FCG). We introduce LibAM, a novel Area Matching framework that connects isolated functions into function areas on FCG and detects TPLs by comparing the similarity of these function areas, significantly mitigating the impact of different optimization options and architectures. Furthermore, LibAM is the first approach capable of detecting the exact reuse areas on FCG and offering substantial benefits for downstream tasks. To validate our approach, we compile the first TPL detection dataset for C/C++ binaries across various optimization options and architectures. Experimental results demonstrate that LibAM outperforms all existing TPL detection methods and provides interpretable evidence for TPL detection results by identifying exact reuse areas. We also evaluate LibAM\u2019s scalability on large-scale, real-world binaries in IoT firmware and generate a list of potential vulnerabilities for these devices. Our experiments indicate that the Area Matching framework performs exceptionally well in the TPL detection task and holds promise for other binary similarity analysis tasks. Last but not least, by analyzing the detection results of IoT firmware, we make several interesting findings, for instance, different target binaries always tend to reuse the same code area of TPL. The datasets and source code used in this article are available at .",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "52",
        "numpages": "35",
        "keywords": "software component analysis, third-party library detection, Static binary analysis"
    },
    "FormatFuzzer: Effective Fuzzing of Binary File Formats": {
        "type": "article",
        "key": "10.1145/3628157",
        "author": "Dutra, Rafael and Gopinath, Rahul and Zeller, Andreas",
        "title": "FormatFuzzer: Effective Fuzzing of Binary File Formats",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3628157",
        "doi": "10.1145/3628157",
        "abstract": "Effective fuzzing of programs that process structured binary inputs, such as multimedia files, is a challenging task, since those programs expect a very specific input format. Existing fuzzers, however, are mostly format-agnostic, which makes them versatile, but also ineffective when a specific format is required. We present FormatFuzzer, a generator for format-specific fuzzers. FormatFuzzer takes as input a binary template (a format specification used by the 010&nbsp;Editor) and compiles it into C++ code that acts as parser, mutator, and highly efficient generator of inputs conforming to the rules of the language. The resulting format-specific fuzzer can be used as a standalone producer or mutator in black-box settings, where no guidance from the program is available. In addition, by providing mutable decision seeds, it can be easily integrated with arbitrary format-agnostic fuzzers such as AFL to make them format-aware. In our evaluation on complex formats such as MP4 or ZIP, FormatFuzzer showed to be a highly effective producer of valid inputs that also detected previously unknown memory errors in ffmpeg and timidity.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "53",
        "numpages": "29",
        "keywords": "generator-based fuzzing, parser generators, grammars, binary files, file format specifications, Structure-aware fuzzing"
    },
    "Survey of Code Search Based on Deep Learning": {
        "type": "article",
        "key": "10.1145/3628161",
        "author": "Xie, Yutao and Lin, Jiayi and Dong, Hande and Zhang, Lei and Wu, Zhonghai",
        "title": "Survey of Code Search Based on Deep Learning",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3628161",
        "doi": "10.1145/3628161",
        "abstract": "Code writing is repetitive and predictable, inspiring us to develop various code intelligence techniques. This survey focuses on code search, that is, to retrieve code that matches a given natural language query by effectively capturing the semantic similarity between the query and code. Deep learning, being able to extract complex semantics information, has achieved great success in this field. Recently, various deep learning methods, such as graph neural networks and pretraining models, have been applied to code search with significant progress. Deep learning is now the leading paradigm for code search. In this survey, we provide a comprehensive overview of deep learning-based code search. We review the existing deep learning-based code search framework that maps query/code to vectors and measures their similarity. Furthermore, we propose a new taxonomy to illustrate the state-of-the-art deep learning-based code search in a three-step process: query semantics modeling, code semantics modeling, and matching modeling, which involves the deep learning model training. Finally, we suggest potential avenues for future research in this promising field.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "54",
        "numpages": "42",
        "keywords": "pre-training, deep learning, natural language processing, code understanding, Code search"
    },
    "A Survey of Learning-based Automated Program Repair": {
        "type": "article",
        "key": "10.1145/3631974",
        "author": "Zhang, Quanjun and Fang, Chunrong and Ma, Yuxiang and Sun, Weisong and Chen, Zhenyu",
        "title": "A Survey of Learning-based Automated Program Repair",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631974",
        "doi": "10.1145/3631974",
        "abstract": "Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance.In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: .",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "55",
        "numpages": "69",
        "keywords": "AI and software engineering, neural machine translation, deep learning, Automatic program repair"
    },
    "Keeper: Automated Testing and Fixing of Machine Learning Software": {
        "type": "article",
        "key": "10.1145/3672451",
        "author": "Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yuhan and Hoffmann, Henry and Maire, Michael and Lu, Shan",
        "title": "Keeper: Automated Testing and Fixing of Machine Learning Software",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672451",
        "doi": "10.1145/3672451",
        "abstract": "The increasing number of software applications incorporating machine learning (ML) solutions has led to the need for testing techniques. However, testing ML software requires tremendous human effort to design realistic and relevant test inputs and to judge software output correctness according to human common sense. Even when misbehavior is exposed, it is often unclear whether the defect is inside ML API or the surrounding code and how to fix the implementation. This article tackles these challenges by proposing Keeper, an automated testing and fixing tool for ML software. The core idea of Keeper is designing pseudo-inverse functions that semantically reverse the corresponding ML task in an empirical way and proxy common human judgment of real-world data. It incorporates these functions into a symbolic execution engine to generate tests. Keeper also detects code smells that degrade software performance. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used to alleviate the misbehavior.Our evaluation on a variety of applications shows that Keeper greatly improves branch coverage, while identifying 74 previously unknown failures and 19 code smells from 56 out of 104 applications. Our user studies show that 78\\% of end-users and 95\\% of developers agree with Keeper\u2019s detection and fixing results.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "167",
        "numpages": "33",
        "keywords": "Software testing, machine learning, machine learning API"
    },
    "Technical Debt Monitoring Decision Making with Skin in the Game": {
        "type": "article",
        "key": "10.1145/3664805",
        "author": "Fungprasertkul, Suwichak and Bahsoon, Rami and Kazman, Rick",
        "title": "Technical Debt Monitoring Decision Making with Skin in the Game",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664805",
        "doi": "10.1145/3664805",
        "abstract": "Technical Debt Management (TDM) can suffer from unpredictability, communication gaps and the inaccessibility of relevant information, which hamper the effectiveness of its decision making. These issues can stem from division among decision-makers which takes root in unfair consequences of decisions among different decision-makers. One mitigation route is Skin in the Game thinking, which enforces transparency, fairness and shared responsibility during collective decision-making under uncertainty. This article illustrates characteristics which require Skin in the Game thinking in Technical Debt (TD) identification, measurement, prioritisation and monitoring. We point out crucial problems in TD monitoring rooted in asymmetric information and asymmetric payoff between different factions of decision-makers. A systematic TD monitoring method is presented to mitigate the said problems. The method leverages Replicator Dynamics and Behavioural Learning. The method supports decision-makers with automated TD monitoring decisions; it informs decision-makers when human interventions are required. Two publicly available industrial projects with a non-trivial number of TD and timestamps are utilised to evaluate the application of our method. Mann\u2013Whitney U hypothesis tests are conducted on samples of decisions from our method and the baseline. The statistical evidence indicates that our method can produce cost-effective and contextual TD monitoring decisions.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "168",
        "numpages": "27",
        "keywords": "Technical debt, software management, software maintenance, monitoring"
    },
    "Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities": {
        "type": "article",
        "key": "10.1145/3664606",
        "author": "Ma, Wei and Liu, Shangqing and Zhao, Mengjie and Xie, Xiaofei and Wang, Wenhang and Hu, Qiang and Zhang, Jie and Liu, Yang",
        "title": "Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664606",
        "doi": "10.1145/3664606",
        "abstract": "Code models have made significant advancements in code intelligence by encoding knowledge about programming languages. While previous studies have explored the capabilities of these models in learning code syntax, there has been limited investigation on their ability to understand code semantics. Additionally, existing analyses assume that the number of edges between nodes at the abstract syntax tree&nbsp;(AST) is related to syntax distance, and also often require transforming the high-dimensional space of deep learning models to a low-dimensional one, which may introduce inaccuracies. To study how code models represent code syntax and semantics, we conduct a comprehensive analysis of seven code models, including four representative code pre-trained models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) and three large language models (LLMs) (StarCoder, CodeLlama and CodeT5+). We design four probing tasks to assess the models\u2019 capacities in learning both code syntax and semantics. These probing tasks reconstruct code syntax and semantics structures (AST, control dependence graph (CDG), data dependence graph (DDG), and control flow graph (CFG)) in the representation space. These structures are core concepts for code understanding. We also investigate the syntax token role in each token representation and the long dependency between the code tokens. Additionally, we analyze the distribution of attention weights related to code semantic structures. Through extensive analysis, our findings highlight the strengths and limitations of different code models in learning code syntax and semantics. The results demonstrate that these models excel in learning code syntax, successfully capturing the syntax relationships between tokens and the syntax roles of individual tokens. However, their performance in encoding code semantics varies. CodeT5 and CodeBERT demonstrate proficiency in capturing control and data dependencies, whereas UnixCoder shows weaker performance in this aspect. We do not observe LLMs generally performing much better than pre-trained models. The shallow layers of LLMs perform better than their deep layers. The investigation of attention weights reveals that different attention heads play distinct roles in encoding code semantics. Our research findings emphasize the need for further enhancements in code models to better learn code semantics. This study contributes to the understanding of code models\u2019 abilities in syntax and semantics analysis. Our findings provide guidance for future improvements in code models, facilitating their effective application in various code-related tasks.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "169",
        "numpages": "29",
        "keywords": "Code model analysis, syntax and semantic encoding"
    },
    "Understanding Vulnerability Inducing Commits of the Linux Kernel": {
        "type": "article",
        "key": "10.1145/3672452",
        "author": "Jiang, Muhui and Jiang, Jinan and Wu, Tao and Ma, Zuchao and Luo, Xiapu and Zhou, Yajin",
        "title": "Understanding Vulnerability Inducing Commits of the Linux Kernel",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672452",
        "doi": "10.1145/3672452",
        "abstract": "The Linux kernel is popular and well-maintained. Over the past decade, around 860 thousand commits were merged with hundreds of vulnerabilities (i.e., 223 on average) disclosed every year, taking the total lines of code to 35.1 million in 2022. Many algorithms have been proposed to detect the vulnerabilities, but few studied how they were induced. To fill this gap, we conduct the first empirical study on the Kernel Vulnerability Inducing Commits (KVIC), the commits that induced vulnerabilities in the Linux kernel. We utilized six different methods on identifying the Kernel Vulnerability Fixing Commits (KVFCs), the commits that fix vulnerabilities in the Linux kernel, and proposed the other four different methods for identifying KVICs by using the identified KVFCs as a bridge. In total, we constructed the first dataset of KVICs with 1,240 KVICs for 1,335 CVEs. We conducted a thorough analysis on the characteristics, purposes, and involved human factors of the KVICs and obtained many interesting findings and insights. For example, KVICs usually have limited reviewers and can still be induced by experienced authors or maintainers. Based on these insights, we proposed several suggestions to the Linux community to help mitigate the induction of KVICs.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "170",
        "numpages": "28",
        "keywords": "KVIC, Linux kernel, vulnerability induction"
    },
    "Code to Qed, the Project Manager's Guide to Proof Engineering": {
        "type": "article",
        "key": "10.1145/3664807",
        "author": "Dejon, Nicolas and Gaber, Chrystel and Grimaud, Gilles and Jomaa, Narjes",
        "title": "Code to Qed, the Project Manager's Guide to Proof Engineering",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664807",
        "doi": "10.1145/3664807",
        "abstract": "Despite growing efforts and encouraging successes in recent decades, fully formally verified projects are still rare in the industrial landscape. The industry often lacks the tools and methodologies to efficiently scale the proof development process. In this work, we give a comprehensible overview of the proof development process for proof developers and project managers. The goal is to support proof developers by rationalizing the proof development process, which currently relies heavily on their intuition and expertise, and by facilitating communication with the management line. To this end, we concentrate on the aspect of proof manufacturing and highlight the most significant sources of proof effort. We propose means to mitigate the latter through proof practices (proof structuring, proof strategies, and proof planning), proof metrics, and tools. Our approach is project-agnostic, independent of specific proof expertise, and computed estimations do not assume prior similar developments. We evaluate our guidelines using a separation kernel undergoing formal verification, driving the proof process in an optimised way. Feedback from a project manager unfamiliar with proof development confirms the benefits of detailed planning of the proof development steps, clear progress communication to the hierarchy line, and alignment with established practices in the software industry.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "171",
        "numpages": "50",
        "keywords": "Proof development, proof strategy, proof metrics, industrial development, project management"
    },
    "Paving a Path for a Combined Family of Feature Toggle and Configuration Option Research": {
        "type": "article",
        "key": "10.1145/3672555",
        "author": "Mahdavi-Hezaveh, Rezvan and Fatima, Sameeha and Williams, Laurie",
        "title": "Paving a Path for a Combined Family of Feature Toggle and Configuration Option Research",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672555",
        "doi": "10.1145/3672555",
        "abstract": "Feature toggles and configuration options are techniques to include or exclude functionality in software. The research contributions to these two techniques have most often been focused on either one of them. However, focusing on the similarities of these two techniques and the use of a common terminology may enable a combined family of research on software configuration (a term we use to encompass both techniques) and prevent duplication of effort. The goal of this study is to aid researchers in conducting a family of research on software configuration by extending an existing model of software configuration that provides a common terminology for feature toggles and configuration options in research studies. We started with Siegmund et al.\u2019s Model of Software Configuration (MSC), which was developed based on configuration option-related resources. We extend the MSC by qualitative analysis of feature toggle-related resources. From our analysis, we proposed MSCv2 and evaluated it through its application on publications and an industrial system. Our results indicate researchers studying the same system may provide different definitions of software configuration in publications, similar research questions may be answered repeatedly because of a lack of a clear definition of software configuration, and having an MSC may enable generalized research on this family of research.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "172",
        "numpages": "27",
        "keywords": "Feature toggle, configuration option, software configuration, software engineering"
    },
    "Synthesis and Verification of Mission Plans for Multiple Autonomous Agents under Complex Road Conditions": {
        "type": "article",
        "key": "10.1145/3672445",
        "author": "Gu, Rong and Baranov, Eduard and Ameri, Afshin and Seceleanu, Cristina and Enoiu, Eduard Paul and C\\\"{u}r\\\"{u}kl\\\"{u}, Baran and Legay, Axel and Lundqvist, Kristina",
        "title": "Synthesis and Verification of Mission Plans for Multiple Autonomous Agents under Complex Road Conditions",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672445",
        "doi": "10.1145/3672445",
        "abstract": "Mission planning for multi-agent autonomous systems aims to generate feasible and optimal mission plans that satisfy given requirements. In this article, we propose a tool-supported mission-planning methodology that combines (i) a path-planning algorithm for synthesizing path plans that are safe in environments with complex road conditions, and (ii) a task-scheduling method for synthesizing task plans that schedule the tasks in the right and fastest order, taking into account the planned paths. The task-scheduling method is based on model checking, which provides means of automatically generating task execution orders that satisfy the requirements and ensure the correctness and efficiency of the plans by construction. We implement our approach in a tool named MALTA, which offers a user-friendly GUI for configuring mission requirements, a module for path planning, an integration with the model checker UPPAAL, and functions for automatic generation of formal models, and parsing of the execution traces of models. Experiments with the tool demonstrate its applicability and performance in various configurations of an industrial case study of an autonomous quarry. We also show the adaptability of our tool by employing it in a special case of an industrial case study.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "173",
        "numpages": "46",
        "keywords": "Mission-plan synthesis, autonomous agents, path planning, task scheduling, UPPAAL"
    },
    "Meta-Learning for Multi-Family Android Malware Classification": {
        "type": "article",
        "key": "10.1145/3664806",
        "author": "Li, Yao and Yuan, Dawei and Zhang, Tao and Cai, Haipeng and Lo, David and Gao, Cuiyun and Luo, Xiapu and Jiang, He",
        "title": "Meta-Learning for Multi-Family Android Malware Classification",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664806",
        "doi": "10.1145/3664806",
        "abstract": "With the emergence of smartphones, Android has become a widely used mobile operating system. However, it is vulnerable when encountering various types of attacks. Every day, new malware threatens the security of users\u2019 devices and private data. Many methods have been proposed to classify malicious applications, utilizing static or dynamic analysis for classification. However, previous methods still suffer from unsatisfactory performance due to two challenges. First, they are unable to address the imbalanced data distribution problem, leading to poor performance for malware families with few members. Second, they are unable to address the zero-day malware (zero-day malware refers to malicious applications that exploit unknown vulnerabilities) classification problem. In this article, we introduce an innovative meta-learning approach for multi-family Android malware classification named Meta-MAMC, which uses meta-learning technology to learn meta-knowledge (i.e., the similarities and differences among different malware families) of few-family samples and combines new sampling algorithms to solve the above challenges. Meta-MAMC integrates (i) the meta-knowledge contained within the dataset to guide models in learning to identify unknown malware; and (ii) more accurate and diverse tasks based on novel sampling strategies, as well as directly adapting meta-learning to a new few-sample and zero-sample task to classify families. We have evaluated Meta-MAMC on two popular datasets and a corpus of real-world Android applications. The results demonstrate its efficacy in accurately classifying malicious applications belonging to certain malware families, even achieving 100\\% classification in some families.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "174",
        "numpages": "27",
        "keywords": "Android, malware family, meta-learning, classification"
    },
    "Effective, Platform-Independent GUI Testing via Image Embedding and Reinforcement Learning": {
        "type": "article",
        "key": "10.1145/3674728",
        "author": "Yu, Shengcheng and Fang, Chunrong and Li, Xin and Ling, Yuchen and Chen, Zhenyu and Su, Zhendong",
        "title": "Effective, Platform-Independent GUI Testing via Image Embedding and Reinforcement Learning",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3674728",
        "doi": "10.1145/3674728",
        "abstract": "Software applications (apps) have been playing an increasingly important role in various aspects of society. In particular, mobile apps and web apps are the most prevalent among all applications and are widely used in various industries as well as in people\u2019s daily lives. To help ensure mobile and web app quality, many approaches have been introduced to improve app GUI testing via automated exploration, including random testing, model-based testing, learning-based testing, and so on. Despite the extensive effort, existing approaches are still limited in reaching high code coverage, constructing high-quality models, and being generally applicable. Reinforcement learning-based approaches, as a group of representative and advanced approaches for automated GUI exploration testing, are faced with difficult challenges, including effective app state abstraction, reward function design, and so on. Moreover, they heavily depend on the specific execution platforms (i.e., Android or Web), thus leading to poor generalizability and being unable to adapt to different platforms.This work specifically tackles these challenges based on the high-level observation that apps from distinct platforms share commonalities in GUI design. Indeed, we propose PIRLTest, an effective platform-independent approach for app testing. Specifically, PIRLTest utilizes computer vision and reinforcement learning techniques in a novel, synergistic manner for automated testing. It extracts the GUI widgets from GUI pages and characterizes the corresponding GUI layouts, embedding the GUI pages as states. The app GUI state combines the macroscopic perspective (app GUI layout) and the microscopic perspective (app GUI widget) and attaches the critical semantic information from GUI images. This enables PIRLTest to be platform-independent and makes the testing approach generally applicable on different platforms. PIRLTest explores apps with the guidance of a curiosity-driven strategy, which uses a Q-network to estimate the values of specific state-action pairs to encourage more exploration in uncovered pages without platform dependency. The exploration will be assigned with rewards for all actions, which are designed considering both the app GUI states and the concrete widgets, to help the framework explore more uncovered pages. We conduct an empirical study on 20 mobile apps and 5 web apps, and the results show that PIRLTest is zero-cost when being adapted to different platforms, and can perform better than the baselines, covering 6.3\u201341.4\\% more code on mobile apps and 1.5\u201351.1\\% more code on web apps. PIRLTest is capable of detecting 128 unique bugs on mobile and web apps, including 100 bugs that cannot be detected by the baselines.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "175",
        "numpages": "27",
        "keywords": "Software testing, platform-independent testing, reinforcement learning, GUI image understanding"
    },
    "Automated Testing Linguistic Capabilities of NLP Models": {
        "type": "article",
        "key": "10.1145/3672455",
        "author": "Lee, Jaeseong and Chen, Simin and Mordahl, Austin and Liu, Cong and Yang, Wei and Wei, Shiyi",
        "title": "Automated Testing Linguistic Capabilities of NLP Models",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672455",
        "doi": "10.1145/3672455",
        "abstract": "Natural language processing (NLP) has gained widespread adoption in the development of real-world applications. However, the black-box nature of neural networks in NLP applications poses a challenge when evaluating their performance, let alone ensuring it. Recent research has proposed testing techniques to enhance the trustworthiness of NLP-based applications. However, most existing works use a single, aggregated metric (i.e., accuracy) which is difficult for users to assess NLP model performance on fine-grained aspects, such as LCs. To address this limitation, we present ALiCT, an automated testing technique for validating NLP applications based on their LCs. ALiCT takes user-specified LCs as inputs and produces diverse test suite with test oracles for each of given LC. We evaluate ALiCT on two widely adopted NLP tasks, sentiment analysis and hate speech detection, in terms of diversity, effectiveness, and consistency. Using Self-BLEU and syntactic diversity metrics, our findings reveal that ALiCT generates test cases that are 190\\% and 2213\\% more diverse in semantics and syntax, respectively, compared to those produced by state-of-the-art techniques. In addition, ALiCT is capable of producing a larger number of NLP model failures in 22 out of 25 LCs over the two NLP applications.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "176",
        "numpages": "33",
        "keywords": "Software testing, LC, sentiment analysis, hate speech detection"
    },
    "A Systematic Mapping Study Exploring Quantification Approaches to Code, Design, and Architecture Technical Debt": {
        "type": "article",
        "key": "10.1145/3675393",
        "author": "Perera, Judith and Tempero, Ewan and Tu, Yu-Cheng and Blincoe, Kelly",
        "title": "A Systematic Mapping Study Exploring Quantification Approaches to Code, Design, and Architecture Technical Debt",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3675393",
        "doi": "10.1145/3675393",
        "abstract": "To effectively manage Technical Debt (TD), we need reliable means to quantify it. We conducted a Systematic Mapping Study (SMS) where we identified 39 quantification approaches for Code, Design, and Architecture TD. We analyzed concepts and metrics discussed in these quantification approaches by classifying the quantification approaches based on a set of abstract TD Quantification (TDQ) concepts and their high-level themes, process/time, cost, benefit, probability, and priority, which we developed during our SMS. This helped identify gaps in the literature and to propose future research directions. Among the abstract TDQ concepts discussed in the different quantification approaches, TD item, TD remediation cost, TD interest, and Benefit of remediating TD were the most frequently discussed concepts. They were also supported by some form of measurement. However, some TDQ concepts were poorly examined, for example, the benefit of taking TD. It was evident that cost concepts were more frequently quantified among the approaches, while benefit concepts were not. Most of the approaches focused on remediating TD in retrospect rather than quantifying TD to strategically use it during software development. This raises the question of whether existing approaches reliably quantify TD and suggests the need to further explore TDQ.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "177",
        "numpages": "44",
        "keywords": "Technical debt management, technical debt quantification, technical debt measurement, software quality"
    },
    "A Comprehensive View on TD Prevention Practices and Reasons for Not Preventing It": {
        "type": "article",
        "key": "10.1145/3674727",
        "author": "Freire, S\\'{a}vio and Pacheco, Alexia and Rios, Nicolli and P\\'{e}rez, Boris and Castellanos, Camilo and Correal, Dar\\'{\\i}o and Rama\\v{c}, Robert and Mandi\\'{c}, Vladimir and Tau\\v{s}an, Neboj\\v{s}a and L\\'{o}pez, Gustavo and Mendon\\c{c}a, Manoel and Falessi, Davide and Izurieta, Clemente and Seaman, Carolyn and Sp\\'{\\i}nola, Rodrigo",
        "title": "A Comprehensive View on TD Prevention Practices and Reasons for Not Preventing It",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3674727",
        "doi": "10.1145/3674727",
        "abstract": "Context. Technical debt (TD) prevention allows software practitioners to apply practices to avoid potential TD items in their projects. Aims. To uncover and prioritize, from the point of view of software practitioners, the practices that could be used to avoid TD items, the relations between these practices and the causes of TD, and the practice avoidance reasons (PARs) that could explain the failure to prevent TD. Method. We analyze data collected from six replications of a global industrial family of surveys on TD, totaling 653 answers. We also conducted a follow up survey to understand the importance level of analyzed data. Results. Most practitioners indicated that TD could be prevented, revealing 89 prevention practices and 23 PARs for explaining the failure to prevent TD. The article identifies statistically significant relationships between preventive practices and certain causes of TD. Further, it prioritizes the list of practices, PARs, and relationships regarding their level of importance for TD prevention based on the opinion of software practitioners. Conclusion. This work organizes TD prevention practices and PARs in a conceptual map and the relationships between practices and causes of TD in a Sankey diagram to help the visualization of the body of knowledge reported in this study.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "178",
        "numpages": "44",
        "keywords": "Technical debt, technical debt prevention, technical debt management, InsighTD project"
    },
    "Harmonising Contributions: Exploring Diversity in Software Engineering through CQA Mining on Stack Overflow": {
        "type": "article",
        "key": "10.1145/3672453",
        "author": "Zolduoarrati, Elijah and Licorish, Sherlock A. and Stanger, Nigel",
        "title": "Harmonising Contributions: Exploring Diversity in Software Engineering through CQA Mining on Stack Overflow",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672453",
        "doi": "10.1145/3672453",
        "abstract": "The need for collective intelligence in technology means that online Q&amp;A platforms, such as Stack Overflow and Reddit, have become invaluable in building the global knowledge ecosystem. Despite literature demonstrating a prevalence of inclusion and contribution disparities in online communities, studies investigating the underlying reasons behind such fluctuations remain scarce. The current study examines Stack Overflow users\u2019 contribution profiles, both in isolation and relative to various diversity metrics, including GDP and access to electricity. This study also examines whether such profiles propagate to the city and state levels, supplemented by granular data such as per capita income and education, before validating quantitative findings using content analysis. We selected 143 countries and compared the profiles of their respective users to assess implicit diversity-related complications that impact how users contribute. Results show that countries with high GDP, prominent R&amp;D presence, less wealth inequality and sufficient access to infrastructure tend to have more users, regardless of their development status. Similarly, cities and states where technology is more prevalent (e.g., San Francisco and New York) have more users who tend to contribute more often. Qualitative analysis reveals distinct communication styles based on users\u2019 locations. Urban users exhibited assertive, solution-oriented behaviour, actively sharing information. Conversely, rural users engaged through inquiries and discussions, incorporating personal anecdotes, gratitude and conciliatory language. Findings from this study may benefit scholars and practitioners, allowing them to develop sustainable mechanisms to bridge the inclusion and diversity gaps.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "179",
        "numpages": "54",
        "keywords": "Contribution, diversity, online Q&amp;A, software engineering"
    },
    "When Automated Program Repair Meets Regression Testing\u2014An Extensive Study on Two Million Patches": {
        "type": "article",
        "key": "10.1145/3672450",
        "author": "Lou, Yiling and Yang, Jun and Benton, Samuel and Hao, Dan and Tan, Lin and Chen, Zhenpeng and Zhang, Lu and Zhang, Lingming",
        "title": "When Automated Program Repair Meets Regression Testing\u2014An Extensive Study on Two Million Patches",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672450",
        "doi": "10.1145/3672450",
        "abstract": "In recent years, Automated Program Repair (APR) has been extensively studied in academia and even drawn wide attention from the industry. However, APR techniques can be extremely time consuming since (1) a large number of patches can be generated for a given bug, and (2) each patch needs to be executed on the original tests to ensure its correctness. In the literature, various techniques (e.g., based on learning, mining, and constraint solving) have been proposed/studied to reduce the number of patches. Intuitively, every patch can be treated as a software revision during regression testing; thus, traditional Regression Test Selection (RTS) techniques can be leveraged to only execute the tests affected by each patch (as the other tests would keep the same outcomes) to further reduce patch execution time. However, few APR systems actually adopt RTS and there is still a lack of systematic studies demonstrating the benefits of RTS and the impact of different RTS strategies on APR. To this end, this article presents the first extensive study of widely used RTS techniques at different levels (i.e., class/method/statement levels) for 12 state-of-the-art APR systems on over 2M patches. Our study reveals various practical guidelines for bridging the gap between APR and regression testing, including: (1) the number of patches widely used for measuring APR efficiency can incur skewed conclusions, and the use of inconsistent RTS configurations can further skew the conclusions; (2) all studied RTS techniques can substantially improve APR efficiency and should be considered in future APR work; (3) method- and statement-level RTS outperform class-level RTS substantially and should be preferred; (4) RTS techniques can substantially outperform state-of-the-art test prioritization techniques for APR, and combining them can further improve APR efficiency; and (5) traditional Regression Test Prioritization (RTP) widely studied in regression testing performs even better than APR-specific test prioritization when combined with most RTS techniques. Furthermore, we also present the detailed impact of different patch categories and patch validation strategies on our findings.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "180",
        "numpages": "23",
        "keywords": "Test selection, program repair, patch validation"
    },
    "An Empirical Study on the Characteristics of Database Access Bugs in Java Applications": {
        "type": "article",
        "key": "10.1145/3672449",
        "author": "Liu, Wei and Mondal, Shouvick and Chen, Tse-Hsun (Peter)",
        "title": "An Empirical Study on the Characteristics of Database Access Bugs in Java Applications",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672449",
        "doi": "10.1145/3672449",
        "abstract": "Database-backed applications rely on the database access code to interact with the underlying database management systems (DBMSs). Although many prior studies aim at database access issues like SQL anti-patterns or SQL code smells, there is a lack of study of database access bugs during the maintenance of database-backed applications. In this paper, we empirically investigate 423 database access bugs collected from seven large-scale Java open-source applications that use relational DBMSs (e.g., MySQL or PostgreSQL). We study the characteristics (e.g., occurrence and root causes) of the bugs by manually examining the bug reports and commit histories. We find that the number of reported database and non-database access bugs share a similar trend but their modified files in bug fixing commits are different. Additionally, we generalize categories of the root causes of database access bugs, containing five main categories (SQL queries, Schema, API, Configuration, and SQL query result) and 25 unique root causes. We find that the bugs pertaining to SQL queries, Schema, and API cover 84.2\\% of database access bugs across all studied applications. In particular, SQL queries bug (54\\%) and API bug (38.7\\%) are the most frequent issues when using JDBC and Hibernate, respectively. Finally, we provide a discussion on the implications of our findings for developers and researchers.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "181",
        "numpages": "25",
        "keywords": "Bugs, database access, empirical study, SQL, object-relational mapping"
    },
    "Self-Planning Code Generation with Large Language Models": {
        "type": "article",
        "key": "10.1145/3672456",
        "author": "Jiang, Xue and Dong, Yihong and Wang, Lecheng and Fang, Zheng and Shang, Qiwei and Li, Ge and Jin, Zhi and Jiao, Wenpin",
        "title": "Self-Planning Code Generation with Large Language Models",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672456",
        "doi": "10.1145/3672456",
        "abstract": "Although large language models (LLMs) have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule solution steps prior to implementation. To this end, we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem-solving. This paper proposes a self-planning code generation approach with large language models, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, LLM plans out concise solution steps from the intent combined with few-shot prompting. Subsequently, in the implementation phase, the model generates code step by step, guided by the preceding solution steps. We conduct extensive experiments on various code-generation benchmarks across multiple programming languages. Experimental results show that self-planning code generation achieves a relative improvement of up to 25.4\\% in Pass@1 compared to direct code generation, and up to 11.9\\% compared to Chain-of-Thought of code generation. Moreover, our self-planning approach also enhances the quality of the generated code with respect to correctness, readability, and robustness, as assessed by humans.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "182",
        "numpages": "30",
        "keywords": "Code Generation, Large language models, Planning"
    },
    "Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Dataflows to Generate Dataflow Graphs in Dynamically Typed Code": {
        "type": "article",
        "key": "10.1145/3672458",
        "author": "Huang, Qing and Luo, Zhiwen and Xing, Zhenchang and Zeng, Jinshan and Chen, Jieshan and Xu, Xiwei and Chen, Yong",
        "title": "Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Dataflows to Generate Dataflow Graphs in Dynamically Typed Code",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672458",
        "doi": "10.1145/3672458",
        "abstract": "Dataflow graphs (DFGs) capture definitions (defs) and uses across program blocks, which is a fundamental program representation for program analysis, testing and maintenance. However, dynamically typed programming languages like Python present implicit dataflow issues that make it challenging to determine def-use flow information at compile time. Static analysis methods like Soot and WALA are inadequate for handling these issues, and manually enumerating comprehensive heuristic rules is impractical. Large pre-trained language models (LLMs) offer a potential solution, as they have powerful language understanding and pattern matching abilities, allowing them to predict implicit dataflow by analyzing code context and relationships between variables, functions, and statements in code. We propose leveraging LLMs\u2019 in-context learning ability to learn implicit rules and patterns from code representation and contextual information to solve implicit dataflow problems. To further enhance the accuracy of LLMs, we design a five-step chain of thought (CoT) and break it down into an Artificial Intelligence (AI) chain, with each step corresponding to a separate AI unit to generate accurate DFGs for Python code. Our approach\u2019s performance is thoroughly assessed, demonstrating the effectiveness of each AI unit in the AI Chain. Compared to static analysis, our method achieves 82\\% higher def coverage and 58\\% higher use coverage in DFG generation on implicit dataflow. We also prove the indispensability of each unit in the AI Chain. Overall, our approach offers a promising direction for building software engineering tools by utilizing foundation models, eliminating significant engineering and maintenance effort, but focusing on identifying problems for AI to solve.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "183",
        "numpages": "29",
        "keywords": "Dataflow graph, AI chain, Large Language Models"
    },
    "On the Model Update Strategies for Supervised Learning in AIOps Solutions": {
        "type": "article",
        "key": "10.1145/3664599",
        "author": "Lyu, Yingzhe and Li, Heng and Jiang, Zhen Ming (Jack) and Hassan, Ahmed E.",
        "title": "On the Model Update Strategies for Supervised Learning in AIOps Solutions",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664599",
        "doi": "10.1145/3664599",
        "abstract": "AIOps (Artificial Intelligence for IT Operations) solutions leverage the massive data produced during the operation of large-scale systems and machine learning models to assist software engineers in their system operations. As operation data produced in the field are constantly evolving due to factors such as the changing operational environment and user base, the models in AIOps solutions need to be constantly maintained after deployment. While prior works focus on innovative modeling techniques to improve the performance of AIOps models before releasing them into the field, when and how to update AIOps models remain an under-investigated topic. In this work, we performed a case study on three large-scale public operation data: two trace datasets from the cloud computing platforms of Google and Alibaba and one disk stats dataset from the BackBlaze cloud storage data center. We empirically assessed five different types of model update strategies for supervised learning regarding their performance, updating cost, and stability. We observed that active model update strategies (e.g., periodical retraining, concept drift guided retraining, time-based model ensembles, and online learning) achieve better and more stable performance than a stationary model. Particularly, applying sophisticated model update strategies (e.g., concept drift detection, time-based ensembles, and online learning) could provide better performance, efficiency, and stability than simply retraining AIOps models periodically. In addition, we observed that, although some update strategies (e.g., time-based ensemble and online learning) can save model training time, they significantly sacrifice model testing time, which could hinder their applications in AIOps solutions where the operation data arrive at high pace and volume and where immediate inferences are required. Our findings highlight that practitioners should consider the evolution of operation data and actively maintain AIOps models over time. Our observations can also guide researchers and practitioners in investigating more efficient and effective model update strategies that fit in the context of AIOps.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "184",
        "numpages": "38",
        "keywords": "AIOps, machine learning engineering, failure prediction, concept drift, model maintenance"
    },
    "Graphuzz: Data-driven Seed Scheduling for Coverage-guided Greybox Fuzzing": {
        "type": "article",
        "key": "10.1145/3664603",
        "author": "Xu, Hang and Chen, Liheng and Gan, Shuitao and Zhang, Chao and Li, Zheming and Ji, Jiangan and Chen, Baojian and Hu, Fan",
        "title": "Graphuzz: Data-driven Seed Scheduling for Coverage-guided Greybox Fuzzing",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664603",
        "doi": "10.1145/3664603",
        "abstract": "Seed scheduling is a critical step of greybox fuzzing, which assigns different weights to seed test cases during seed selection, and significantly impacts the efficiency of fuzzing. Existing seed scheduling strategies rely on manually designed models to estimate the potentials of seeds and determine their weights, which fails to capture the rich information of a seed and its execution and thus the estimation of seeds\u2019 potentials is not optimal. In this article, we introduce a new seed scheduling solution, Graphuzz, for coverage-guided greybox fuzzing, which utilizes deep learning models to estimate the potentials of seeds and works in a data-driven way. Specifically, we propose an extended control flow graph called e-CFG to represent the control-flow and data-flow features of a seed's execution, which is suitable for graph neural networks (GNN) to process and estimate seeds\u2019 potential. We evaluate each seed's code coverage increment and use it as the label to train the GNN model. Further, we propose a self-attention mechanism to enhance the GNN model so that it can capture overlooked features. We have implemented a prototype of Graphuzz based on the baseline fuzzer AFLplusplus. The evaluation results show that our model can estimate the potential of seeds and has the robust capability to generalize to different targets. Furthermore, the evaluation using 12 benchmarks from FuzzBench shows that Graphuzz outperforms AFLplusplus and the state-of-the-art seed scheduling solution K-Scheduler and other coverage-guided fuzzers in terms of code coverage, and the evaluation using 8 benchmarks from Magma shows that Graphuzz outperforms the baseline fuzzer AFLplusplus and SOTA solutions in terms of bug detection.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "185",
        "numpages": "36",
        "keywords": "Fuzzing, seed scheduling, graph neural network"
    },
    "LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models": {
        "type": "article",
        "key": "10.1145/3664812",
        "author": "Feng, Xiaoning and Han, Xiaohong and Chen, Simin and Yang, Wei",
        "title": "LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664812",
        "doi": "10.1145/3664812",
        "abstract": "Large Language Models (LLMs) have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of LLMs, which is of paramount importance due to often vast generation demands and real-time requirements, has surprisingly received little attention. In this article, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art LLMs. By analyzing the working mechanism and implementation of 20,543 public-accessible LLMs, we observe a fundamental property in LLMs that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of LLMs instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime-generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that LLMs would have to go through enough iterations to satisfy the pre-configured threshold. We present LLMEffiChecker, which can work under both white-box setting and black-box setting. In the white-box scenario, LLMEffiChecker develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level. In the black-box scenario, LLMEffiChecker employs a causal inference-based approach to find critical tokens and similarly applies three levels of imperceptible perturbation to them. Both the white-box and black-box settings effectively delay the appearance of EOS, compelling these inputs to reach the naturally unreachable threshold. To demonstrate the effectiveness of LLMEffiChecker, we conduct a systematic evaluation on nine publicly available LLMs: Google T5, AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT, and Salesforce CodeGen. Experimental results show that LLMEffiChecker can increase on average LLMs\u2019 response latency and energy consumption by 325\\% to 3,244\\% and 344\\% to 3,616\\%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by LLMEffiChecker significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "186",
        "numpages": "38",
        "keywords": "Machine learning, software testing, large language model"
    },
    "A Formal Explainer for Just-In-Time Defect Predictions": {
        "type": "article",
        "key": "10.1145/3664809",
        "author": "Yu, Jinqiang and Fu, Michael and Ignatiev, Alexey and Tantithamthavorn, Chakkrit and Stuckey, Peter",
        "title": "A Formal Explainer for Just-In-Time Defect Predictions",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664809",
        "doi": "10.1145/3664809",
        "abstract": "Just-in-Tim e (JIT) defect prediction has been proposed to help teams prioritize the limited resources on the most risky commits (or pull requests), yet it remains largely a black box, whose predictions are not explainable or actionable to practitioners. Thus, prior studies have applied various model-agnostic techniques to explain the predictions of JIT models. Yet, explanations generated from existing model-agnostic techniques are still not formally sound, robust, and actionable. In this article, we propose FoX, a Formal eXplainer for JIT Defect Prediction, which builds on formal reasoning about the behavior of JIT defect prediction models and hence is able to provide provably correct explanations, which are additionally guaranteed to be minimal. Our experimental results show that FoX &nbsp;is able to efficiently generate provably correct, robust, and actionable explanations, while existing model-agnostic techniques cannot. Our survey study with 54 software practitioners provides valuable insights into the usefulness and trustworthiness of our FoX &nbsp;approach; 86\\% of participants agreed that our approach is useful, while 74\\% of participants found it trustworthy. Thus, this article serves as an important stepping stone towards trustable explanations for JIT models to help domain experts and practitioners better understand why a commit is predicted as defective and what to do to mitigate the risk.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "187",
        "numpages": "31",
        "keywords": "Explainable AI for SE, Just-In-Time Defect Prediction, Formal Explainability, Software Quality"
    },
    "Neuron Sensitivity-Guided Test Case Selection": {
        "type": "article",
        "key": "10.1145/3672454",
        "author": "Huang, Dong and Bu, Qingwen and Fu, Yichao and Qing, Yuhao and Xie, Xiaofei and Chen, Junjie and Cui, Heming",
        "title": "Neuron Sensitivity-Guided Test Case Selection",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672454",
        "doi": "10.1145/3672454",
        "abstract": "Deep neural networks (DNNs) have been widely deployed in software to address various tasks (e.g., autonomous driving, medical diagnosis). However, they can also produce incorrect behaviors that result in financial losses and even threaten human safety. To reveal and repair incorrect behaviors in DNNs, developers often collect rich, unlabeled datasets from the natural world and label them to test DNN models. However, properly labeling a large number of datasets is a highly expensive and time-consuming task.To address the above-mentioned problem, we propose neuron sensitivity-guided test case selection (NSS), which can reduce the labeling time by selecting valuable test cases from unlabeled datasets. NSS leverages the information of the internal neuron induced by the test cases to select valuable test cases, which have high confidence in causing the model to behave incorrectly. We evaluated NSS with four widely used datasets and four well-designed DNN models compared to the state-of-the-art (SOTA) baseline methods. The results show that NSS performs well in assessing the probability of failure triggering in test cases and in the improvement capabilities of the model. Specifically, compared to the baseline approaches, NSS achieves a higher fault detection rate (e.g., when selecting 5\\% of the test cases from the unlabeled dataset in the MNIST and LeNet1 experiment, NSS can obtain an 81.8\\% fault detection rate, which is a 20\\% increase compared with SOTA baseline strategies).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "188",
        "numpages": "32",
        "keywords": "Deep learning testing, neuron sensitivity, model interpretation"
    },
    "Self-Collaboration Code Generation via ChatGPT": {
        "type": "article",
        "key": "10.1145/3672459",
        "author": "Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge",
        "title": "Self-Collaboration Code Generation via ChatGPT",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672459",
        "doi": "10.1145/3672459",
        "abstract": "Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct \u201cexperts,\u201d each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other\u2019s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development\u2019s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9\u201347.1\\% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "189",
        "numpages": "38",
        "keywords": "Code generation, large language models, multi-agent collaboration, software development"
    },
    "Can Coverage Criteria Guide Failure Discovery for Image Classifiers? An Empirical Study": {
        "type": "article",
        "key": "10.1145/3672446",
        "author": "Wang, Zhiyu and Xu, Sihan and Fan, Lingling and Cai, Xiangrui and Li, Linyu and Liu, Zheli",
        "title": "Can Coverage Criteria Guide Failure Discovery for Image Classifiers? An Empirical Study",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672446",
        "doi": "10.1145/3672446",
        "abstract": "Quality assurance of deep neural networks (DNNs) is crucial for the deployment of DNN-based software, especially in mission- and safety-critical tasks. Inspired by structural white-box testing in traditional software, many test criteria have been proposed to test DNNs, i.e., to exhibit erroneous behaviors by activating new test units that have not been covered, such as new neurons, values, and decision paths. Many studies have been done to evaluate the effectiveness of DNN test coverage criteria. However, existing empirical studies mainly focused on measuring the effectiveness of DNN test criteria for improving the adversarial robustness of DNNs, while ignoring the correctness property when testing DNNs. To fill in this gap, we conduct a comprehensive study on 11 structural coverage criteria, 6 widely-used image datasets, and 9 popular DNNs. We investigate the effectiveness of DNN coverage criteria over natural inputs from four aspects: (1) the correlation between test coverage and test diversity; (2) the effects of criteria parameters and target DNNs; (3) the effectiveness to prioritize in-distribution natural inputs that lead to erroneous behaviors; and (4) the capability to detect out-of-distribution natural samples. Our findings include: (1) For measuring the diversity, coverage criteria considering the relationship between different neurons are more effective than coverage criteria that treat each neuron independently. For instance, the neuron-path criteria (i.e., SNPC and ANPC) show high correlation with test diversity, which is promising to measure test diversity for DNNs. (2) The hyper-parameters have a big influence on the effectiveness of criteria, especially those relevant to the granularity of test criteria. Meanwhile, the computational complexity is one of the important issues to be considered when designing deep learning test coverage criteria, especially for large-scale models. (3) Test criteria related to data distribution (i.e., LSA and DSA, SNAC, and NBC) can be used to prioritize both in-distribution natural faults and out-of-distribution inputs. Furthermore, for OOD detection, the boundary metrics (i.e., SNAC and NBC) are also effective indicators with lower computational costs and higher detection efficiency compared with LSA and DSA. These findings motivate follow-up research on scalable test coverage criteria that improve the correctness of DNNs.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "190",
        "numpages": "28",
        "keywords": "Deep learning testing, test coverage criteria"
    },
    "FunFuzz: A Function-Oriented Fuzzer for Smart Contract Vulnerability Detection with High Effectiveness and Efficiency": {
        "type": "article",
        "key": "10.1145/3674725",
        "author": "Ye, Mingxi and Nan, Yuhong and Dai, Hong-Ning and Yang, Shuo and Luo, Xiapu and Zheng, Zibin",
        "title": "FunFuzz: A Function-Oriented Fuzzer for Smart Contract Vulnerability Detection with High Effectiveness and Efficiency",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3674725",
        "doi": "10.1145/3674725",
        "abstract": "With the increasing popularity of Decentralized Applications (DApps) in blockchain, securing smart contracts has been a long-term, high-priority subject in the domain. Among the various research directions for vulnerability detection, fuzzing has received extensive attention because of its high effectiveness. However, with the increasing complexity of smart contracts, existing fuzzers may waste substantial time exploring locations irrelevant to smart contract vulnerabilities. In this article, we present FunFuzz, a function-oriented fuzzer, which is dedicatedly tailored for detecting smart contract vulnerability with high effectiveness and efficiency. The key observation in our research is that most smart contract vulnerabilities exist in specific functions rather than randomly distributed in all program code like other traditional software. To this end, unlike traditional fuzzers which mainly target code coverage, FunFuzz identifies risky functions while pruning non-risky ones in smart contracts. In this way, it significantly narrows down the exploration scope during the fuzzing process. In addition, FunFuzz employs three unique strategies to direct itself toward effectively discovering vulnerabilities specific to smart contracts (e.g., reentrancy, block dependency, and gasless send). Extensive experiments on 170 real-world contracts demonstrate that FunFuzz outperforms state-of-the-art fuzzers in terms of effectiveness and efficiency.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "191",
        "numpages": "20",
        "keywords": "Fuzz testing, smart contract, blockchain, vulnerability detection"
    },
    "Bitmap-Based Security Monitoring for Deeply Embedded Systems": {
        "type": "article",
        "key": "10.1145/3672460",
        "author": "Peng, Anni and Fang, Dongliang and Guan, Le and Kouwe, Erik van der and Li, Yin and Wang, Wenwen and Sun, Limin and Zhang, Yuqing",
        "title": "Bitmap-Based Security Monitoring for Deeply Embedded Systems",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672460",
        "doi": "10.1145/3672460",
        "abstract": "Deeply embedded systems powered by microcontrollers are becoming popular with the emergence of Internet-of-Things (IoT) technology. However, these devices primarily run C/C ({+}{+})  code and are susceptible to memory bugs, which can potentially lead to both control data attacks and non-control data attacks. Existing defense mechanisms (such as control-flow integrity (CFI), dataflow integrity (DFI) and write integrity testing (WIT), etc.) consume a massive amount of resources, making them less practical in real products. To make it lightweight, we design a bitmap-based allowlist mechanism to unify the storage of the runtime data for protecting both control data and non-control data. The memory requirements are constant and small, regardless of the number of deployed defense mechanisms. We store the allowlist in the TrustZone to ensure its integrity and confidentiality. Meanwhile, we perform an offline analysis to detect potential collisions and make corresponding adjustments when it happens. We have implemented our idea on an ARM Cortex-M-based development board. Our evaluation results show a substantial reduction in memory consumption when deploying the proposed CFI and DFI mechanisms, without compromising runtime performance. Specifically, our prototype enforces CFI and DFI at a cost of just 2.09\\% performance overhead and 32.56\\% memory overhead on average.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "192",
        "numpages": "31",
        "keywords": "CFI, DFI, microcontroller, TEE"
    },
    "A Tale of Two Comprehensions? Analyzing Student Programmer Attention during Code Summarization": {
        "type": "article",
        "key": "10.1145/3664808",
        "author": "Karas, Zachary and Bansal, Aakash and Zhang, Yifan and Li, Toby and McMillan, Collin and Huang, Yu",
        "title": "A Tale of Two Comprehensions? Analyzing Student Programmer Attention during Code Summarization",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664808",
        "doi": "10.1145/3664808",
        "abstract": "Code summarization is the task of creating short, natural language descriptions of source code. It is an important part of code comprehension and a powerful method of documentation. Previous work has made progress in identifying where programmers focus in code as they write their own summaries (i.e., Writing). However, there is currently a gap in studying programmers\u2019 attention as they read code with pre-written summaries (i.e., Reading). As a result, it is currently unknown how these two forms of code comprehension compare: Reading and Writing. Also, there is a limited understanding of programmer attention with respect to program semantics. We address these shortcomings with a human eye-tracking study (n = 27) comparing Reading and Writing. We examined programmers\u2019 attention with respect to fine-grained program semantics, including their attention sequences (i.e., scan paths). We find distinctions in programmer attention across the comprehension tasks, similarities in reading patterns between them, and differences mediated by demographic factors. This can help guide code comprehension in both computer science education and automated code summarization. Furthermore, we mapped programmers\u2019 gaze data onto the Abstract Syntax Tree to explore another representation of human attention. We find that visual behavior on this structure is not always consistent with that on source code.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "193",
        "numpages": "37",
        "keywords": "Cognitive science, code summarization, eye-tracking, expertise, code comprehension"
    },
    "IDE4ICDS: A Human-Centric and Model-Driven Proposal to Improve the Digitization of Clinical Practice Guideline": {
        "type": "article",
        "key": "10.1145/3674732",
        "author": "Parra-Calder\\'{o}n, Carlos and Garc\\'{\\i}a-Garc\\'{\\i}a, Juli\\'{a}n Alberto and Ramos-Cueli, Juan Manuel and Alvarez-Romero, Celia and Rom\\'{a}n-Villar\\'{a}n, Esther and Mart\\'{\\i}nez-Garc\\'{\\i}a, Alicia and Escalona, Mar\\'{\\i}a Jos\\'{e",
        "title": "IDE4ICDS: A Human-Centric and Model-Driven Proposal to Improve the Digitization of Clinical Practice Guideline",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3674732",
        "doi": "10.1145/3674732",
        "abstract": "Clinical practice guidelines (CPGs) are a formalization of specific clinical knowledge that states the best evidence-based clinical practices for treating pathologies. However, CPGs are limited because they are usually expressed as text. This gives rise to a certain level of ambiguity, subjective interpretation of the actions to be performed, and variability in clinical practice by different health professionals facing the same circumstances. The inherent complexity of CPGs is also a challenge for software engineers designing, developing, and maintaining software systems and clinical decision support system to manage and digitize them. This challenge stems from the need to evolve CPGs and design software systems capable of allowing their evolution. This paper proposes a model-driven, human-centric and tool-supported framework (called IDE4ICDS) for improving digitisation of CPG in practical environments. This framework is designed from a human-centric perspective to be used by mixed teams of clinicians and software engineers. It was also validated with the type 2 diabetes mellitus CPG in the Andalusian Public Health System (Spain) involving 89 patients and obtaining a kappa-based analysis. The recommendations were acceptable (0.61\u20130.80) with a total kappa index of 0.701, leading to the conclusion that the proposal provided appropriate recommendations for each patient.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "194",
        "numpages": "38",
        "keywords": "Clinical practice guideline, human-centric software design, model-driven engineering paradigm, computer-interpretable clinical guidelines, knowledge representation"
    },
    "Risky Dynamic Typing-related Practices in Python: An Empirical Study": {
        "type": "article",
        "key": "10.1145/3649593",
        "author": "Chen, Zhifei and Chen, Lin and Yang, Yibiao and Feng, Qiong and Li, Xuansong and Song, Wei",
        "title": "Risky Dynamic Typing-related Practices in Python: An Empirical Study",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649593",
        "doi": "10.1145/3649593",
        "abstract": "Python\u2019s dynamic typing nature provides developers with powerful programming abstractions. However, many type-related bugs are accumulated in code bases of Python due to the misuse of dynamic typing. The goal of this article is to aid in the understanding of developers\u2019 high-risk practices toward dynamic typing and the early detection of type-related bugs. We first formulate the rules of six types of risky dynamic typing-related practices (type smells for short) in Python. We then develop a rule-based tool named RUPOR, which builds an accurate type base to detect type smells. Our evaluation shows that RUPOR outperforms the existing type smell detection techniques (including the Large Language Models\u2013based approaches, Mypy, and PYDYPE) on a benchmark of 900 Python methods. Based on RUPOR, we conduct an empirical study on 25 real-world projects. We find that type smells are significantly related to the occurrence of post-release faults. The fault-proneness prediction model built with type smell features slightly outperforms the model built without them. We also summarize the common patterns, including inserting type check to fix type smell bugs. These findings provide valuable insights for preventing and fixing type-related bugs in the programs written in dynamic-typed languages.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "140",
        "numpages": "35",
        "keywords": "Dynamic typing, Python, empirical study, bug fixing"
    },
    "Data Complexity: A New Perspective for Analyzing the Difficulty of Defect Prediction Tasks": {
        "type": "article",
        "key": "10.1145/3649596",
        "author": "Wan, Xiaohui and Zheng, Zheng and Qin, Fangyun and Lu, Xuhui",
        "title": "Data Complexity: A New Perspective for Analyzing the Difficulty of Defect Prediction Tasks",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649596",
        "doi": "10.1145/3649596",
        "abstract": "Defect prediction is crucial for software quality assurance and has been extensively researched over recent decades. However, prior studies rarely focus on data complexity in defect prediction tasks, and even less on understanding the difficulties of these tasks from the perspective of data complexity. In this article, we conduct an empirical study to estimate the hardness of over 33,000 instances, employing a set of measures to characterize the inherent difficulty of instances and the characteristics of defect datasets. Our findings indicate that: (1) instance hardness in both classes displays a right-skewed distribution, with the defective class exhibiting a more scattered distribution; (2) class overlap is the primary factor influencing instance hardness and can be characterized through feature, structural, and instance-level overlap; (3) no universal preprocessing technique is applicable to all datasets, and it may not consistently reduce data complexity, fortunately, dataset complexity measures can help identify suitable techniques for specific datasets; (4)&nbsp;integrating data complexity information into the learning process can enhance an algorithm\u2019s learning capacity. In summary, this empirical study highlights the crucial role of data complexity in defect prediction tasks, and provides a novel perspective for advancing research in defect prediction techniques.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "141",
        "numpages": "45",
        "keywords": "Defect prediction, machine learning, data complexity, instance hardness"
    },
    "Advanced White-Box Heuristics for Search-Based Fuzzing of REST APIs": {
        "type": "article",
        "key": "10.1145/3652157",
        "author": "Arcuri, Andrea and Zhang, Man and Galeotti, Juan",
        "title": "Advanced White-Box Heuristics for Search-Based Fuzzing of REST APIs",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3652157",
        "doi": "10.1145/3652157",
        "abstract": "Due to its importance and widespread use in industry, automated testing of REST APIs has attracted major interest from the research community in the last few years. However, most of the work in the literature has been focused on black-box fuzzing. Although existing fuzzers have been used to automatically find many faults in existing APIs, there are still several open research challenges that hinder the achievement of better results (e.g., in terms of code coverage and fault finding). For example, under-specified schemas are a major issue for black-box fuzzers. Currently, EvoMaster is the only existing tool that supports white-box fuzzing of REST APIs. In this paper, we provide a series of novel white-box heuristics, including for example how to deal with under-specified constrains in API schemas, as well as under-specified schemas in SQL databases. Our novel techniques are implemented as an extension to our open-source, search-based fuzzer EvoMaster. An empirical study on 14 APIs from the EMB corpus, plus one industrial API, shows clear improvements of the results in some of these APIs.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "142",
        "numpages": "36",
        "keywords": "SBST, fuzzing, REST, Web API, OpenAPI, schema, SQL"
    },
    "Help Them Understand: Testing and Improving Voice User Interfaces": {
        "type": "article",
        "key": "10.1145/3654438",
        "author": "Guglielmi, Emanuela and Rosa, Giovanni and Scalabrino, Simone and Bavota, Gabriele and Oliveto, Rocco",
        "title": "Help Them Understand: Testing and Improving Voice User Interfaces",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3654438",
        "doi": "10.1145/3654438",
        "abstract": "Voice-based virtual assistants are becoming increasingly popular. Such systems provide frameworks to developers for building custom apps. End-users can interact with such apps through a Voice User Interface (VUI), which allows the user to use natural language commands to perform actions. Testing such apps is not trivial: The same command can be expressed in different semantically equivalent ways. In this article, we introduce VUI-UPSET, an approach that adapts chatbot-testing approaches to VUI-testing. We conducted an empirical study to understand how VUI-UPSET compares to two state-of-the-art approaches (i.e., a chatbot testing technique and ChatGPT) in terms of (i) correctness of the generated paraphrases, and (ii) capability of revealing bugs. To this aim, we analyzed 14,898 generated paraphrases for 40 Alexa Skills. Our results show that VUI-UPSET generates more bug-revealing paraphrases than the two baselines with, however, ChatGPT being the approach generating the highest percentage of correct paraphrases. We also tried to use the generated paraphrases to improve the skills. We tried to include in the voice interaction models of the skills (i) only the bug-revealing paraphrases, (ii) all the valid paraphrases. We observed that including only bug-revealing paraphrases is sometimes not sufficient to make all the tests pass.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "143",
        "numpages": "33",
        "keywords": "Voice user interfaces, software testing, NLP;"
    },
    "On the Impact of Lower Recall and Precision in Defect Prediction for Guiding Search-based Software Testing": {
        "type": "article",
        "key": "10.1145/3655022",
        "author": "Perera, Anjana and Turhan, Burak and Aleti, Aldeida and B\\\"{o}hme, Marcel",
        "title": "On the Impact of Lower Recall and Precision in Defect Prediction for Guiding Search-based Software Testing",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3655022",
        "doi": "10.1145/3655022",
        "abstract": "Defect predictors, static bug detectors, and humans inspecting the code can propose locations in the program that are more likely to be buggy before they are discovered through testing. Automated test generators such as search-based software testing (SBST) techniques can use this information to direct their search for test cases to likely buggy code, thus speeding up the process of detecting existing bugs in those locations. Often the predictions given by these tools or humans are imprecise, which can misguide the SBST technique and may deteriorate its performance. In this article, we study the impact of imprecision in defect prediction on the bug detection effectiveness of SBST.Our study finds that the recall of the defect predictor, i.e., the proportion of correctly identified buggy code, has a significant impact on bug detection effectiveness of SBST with a large effect size. More precisely, the SBST technique detects 7.5 fewer bugs on average (out of 420 bugs) for every 5\\% decrements of the recall. However, the effect of precision, a measure for false alarms, is not of meaningful practical significance, as indicated by a very small effect size.In the context of combining defect prediction and SBST, our recommendation is to increase the recall of defect predictors as a primary objective and precision as a secondary objective. In our experiments, we find that 75\\% precision is as good as 100\\% precision. To account for the imprecision of defect predictors, in particular low recall values, SBST techniques should be designed to search for test cases that also cover the predicted non-buggy parts of the program, while prioritising the parts that have been predicted as buggy.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "144",
        "numpages": "27",
        "keywords": "Search-based software testing, automated test generation, defect prediction"
    },
    "On Estimating the Feasible Solution Space of Multi-objective Testing Resource Allocation": {
        "type": "article",
        "key": "10.1145/3654444",
        "author": "Zhang, Guofu and Li, Lei and Su, Zhaopin and Yue, Feng and Chen, Yang and Li, Miqing and Yao, Xin",
        "title": "On Estimating the Feasible Solution Space of Multi-objective Testing Resource Allocation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3654444",
        "doi": "10.1145/3654444",
        "abstract": "The multi-objective testing resource allocation problem (MOTRAP) is concerned on how to reasonably plan the testing time of software testers to save the cost and improve the reliability as much as possible. The feasible solution space of a MOTRAP is determined by its variables (i.e., the time invested in each component) and constraints (e.g., the pre-specified reliability, cost, or time). Although a variety of state-of-the-art constrained multi-objective optimisers can be used to find individual solutions in this space, their search remains inefficient and expensive due to the fact that this space is very tiny compared to the large search space. The decision maker may often suffer a prolonged but unsuccessful search that fails to return a feasible solution. In this work, we first formulate a heavily constrained MOTRAP on the basis of an architecture-based model, in which reliability, cost, and time are optimised under the pre-specified multiple constraints on reliability, cost, and time. Then, to estimate the feasible solution space of this specific MOTRAP, we develop theoretical and algorithmic approaches to deduce new tighter lower and upper bounds on variables from constraints. Importantly, our approach can help the decision maker identify whether their constraint settings are practicable, and meanwhile, the derived bounds can just enclose the tiny feasible solution space and help off-the-shelf constrained multi-objective optimisers make the search within the feasible solution space as much as possible. Additionally, to further make good use of these bounds, we propose a generalised bound constraint handling method that can be readily employed by constrained multi-objective optimisers to pull infeasible solutions back into the estimated space with theoretical guarantee. Finally, we evaluate our approach on application and empirical cases. Experimental results reveal that our approach significantly enhances the efficiency, effectiveness, and robustness of off-the-shelf constrained multi-objective optimisers and state-of-the-art bound constraint handling methods at finding high-quality solutions for the decision maker. These improvements may help the decision maker take the stress out of setting constraints and selecting constrained multi-objective optimisers and facilitate the testing planning more efficiently and effectively.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "145",
        "numpages": "41",
        "keywords": "Multi-objective multi-constraint testing resource allocation, constrained multi-objective optimisers, estimation of feasible solution space, lower and upper bounds, bound constraint handling"
    },
    "Early and Realistic Exploitability Prediction of Just-Disclosed Software Vulnerabilities: How Reliable Can It Be?": {
        "type": "article",
        "key": "10.1145/3654443",
        "author": "Iannone, Emanuele and Sellitto, Giulia and Iaccarino, Emanuele and Ferrucci, Filomena and De Lucia, Andrea and Palomba, Fabio",
        "title": "Early and Realistic Exploitability Prediction of Just-Disclosed Software Vulnerabilities: How Reliable Can It Be?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3654443",
        "doi": "10.1145/3654443",
        "abstract": "With the rate of discovered and disclosed vulnerabilities escalating, researchers have been experimenting with machine learning to predict whether a vulnerability will be exploited. Existing solutions leverage information unavailable when a CVE is created, making them unsuitable just after the disclosure. This paper experiments with early exploitability prediction models driven exclusively by the initial CVE record, i.e., the original description and the linked online discussions. Leveraging NVD and Exploit Database, we evaluate 72 prediction models trained using six traditional machine learning classifiers, four feature representation schemas, and three data balancing algorithms. We also experiment with five pre-trained large language models (LLMs). The models leverage seven different corpora made by combining three data sources, i.e., CVE description, Security Focus, and BugTraq. The models are evaluated in a realistic, time-aware fashion by removing the training and test instances that cannot be labeled \u201cneutral\u201d&nbsp;with sufficient confidence. The validation reveals that CVE descriptions and Security Focus discussions are the best data to train on. Pre-trained LLMs do not show the expected performance, requiring further pre-training in the security domain. We distill new research directions, identify possible room for improvement, and envision automated systems assisting security experts in assessing the exploitability.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "146",
        "numpages": "41",
        "keywords": "Exploitability prediction, software vulnerabilities, machine learning, text mining, natural language processing"
    },
    "Testing Multi-Subroutine Quantum Programs: From Unit Testing to Integration Testing": {
        "type": "article",
        "key": "10.1145/3656339",
        "author": "Long, Peixun and Zhao, Jianjun",
        "title": "Testing Multi-Subroutine Quantum Programs: From Unit Testing to Integration Testing",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3656339",
        "doi": "10.1145/3656339",
        "abstract": "Quantum computing has emerged as a promising field with the potential to revolutionize various domains by harnessing the principles of quantum mechanics. As quantum hardware and algorithms continue to advance, developing high-quality quantum software has become crucial. However, testing quantum programs poses unique challenges due to the distinctive characteristics of quantum systems and the complexity of multi-subroutine programs. This article addresses the specific testing requirements of multi-subroutine quantum programs. We begin by investigating critical properties by surveying existing quantum libraries and providing insights into the challenges of testing these programs. Building upon this understanding, we focus on testing criteria and techniques based on the whole testing process perspective, spanning from unit testing to integration testing. We delve into various aspects, including IO analysis, quantum relation checking, structural testing, behavior testing, integration of subroutine pairs, and test case generation. We also introduce novel testing principles and criteria to guide the testing process. We conduct comprehensive testing on typical quantum subroutines, including diverse mutants and randomized inputs, to evaluate our proposed approach. The analysis of failures provides valuable insights into the effectiveness of our testing methodology. Additionally, we present case studies on representative multi-subroutine quantum programs, demonstrating the practical application and effectiveness of our proposed testing principles and criteria.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "147",
        "numpages": "61",
        "keywords": "Quantum computing, software testing, unit testing, integration testing"
    },
    "MTL-TRANSFER: Leveraging Multi-task Learning and Transferred Knowledge for Improving Fault Localization and Program Repair": {
        "type": "article",
        "key": "10.1145/3654441",
        "author": "Wang, Xu and Yu, Hongwei and Meng, Xiangxin and Cao, Hongliang and Zhang, Hongyu and Sun, Hailong and Liu, Xudong and Hu, Chunming",
        "title": "MTL-TRANSFER: Leveraging Multi-task Learning and Transferred Knowledge for Improving Fault Localization and Program Repair",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3654441",
        "doi": "10.1145/3654441",
        "abstract": "Fault localization (FL) and automated program repair (APR) are two main tasks of automatic software debugging. Compared with traditional methods, deep learning-based approaches have been demonstrated to achieve better performance in FL and APR tasks. However, the existing deep learning-based FL methods ignore the deep semantic features or only consider simple code representations. And for APR tasks, existing template-based APR methods are weak in selecting the correct fix templates for more effective program repair, which are also not able to synthesize patches via the embedded end-to-end code modification knowledge obtained by training models on large-scale bug-fix code pairs. Moreover, in most of FL and APR methods, the model designs and training phases are performed separately, leading to ineffective sharing of updated parameters and extracted knowledge during the training process. This limitation hinders the further improvement in the performance of FL and APR tasks. To solve the above problems, we propose a novel approach called MTL-TRANSFER, which leverages a multi-task learning strategy to extract deep semantic features and transferred knowledge from different perspectives. First, we construct a large-scale open-source bug datasets and implement 11 multi-task learning models for bug detection and patch generation sub-tasks on 11 commonly used bug types, as well as one multi-classifier to learn the relevant semantics for the subsequent fix template selection task. Second, an MLP-based ranking model is leveraged to fuse spectrum-based, mutation-based and semantic-based features to generate a sorted list of suspicious statements. Third, we combine the patches generated by the neural patch generation sub-task from the multi-task learning strategy with the optimized fix template selecting order gained from the multi-classifier mentioned above. Finally, the more accurate FL results, the optimized fix template selecting order, and the expanded patch candidates are combined together to further enhance the overall performance of APR tasks. Our extensive experiments on widely-used benchmark Defects4J show that MTL-TRANSFER outperforms all baselines in FL and APR tasks, proving the effectiveness of our approach. Compared with our previously proposed FL method TRANSFER-FL (which is also the state-of-the-art statement-level FL method), MTL-TRANSFER increases the faults hit by 8/11/12 on Top-1/3/5 metrics (92/159/183 in total). And on APR tasks, the number of successfully repaired bugs of MTL-TRANSFER under the perfect localization setting reaches 75, which is 8 more than our previous APR method TRANSFER-PR. Furthermore, another experiment to simulate the actual repair scenarios shows that MTL-TRANSFER can successfully repair 15 and 9 more bugs (56 in total) compared with TBar and TRANSFER, which demonstrates the effectiveness of the combination of our optimized FL and APR components.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "148",
        "numpages": "31",
        "keywords": "Fault localization, automated program repair, deep neural networks, transfer learning, multi-task learning, neural machine translation"
    },
    "On the Way to SBOMs: Investigating Design Issues and Solutions in Practice": {
        "type": "article",
        "key": "10.1145/3654442",
        "author": "Bi, Tingting and Xia, Boming and Xing, Zhenchang and Lu, Qinghua and Zhu, Liming",
        "title": "On the Way to SBOMs: Investigating Design Issues and Solutions in Practice",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3654442",
        "doi": "10.1145/3654442",
        "abstract": "The increase of software supply chain threats has underscored the necessity for robust security mechanisms, among which the Software Bill of Materials (SBOM) stands out as a promising solution. SBOMs, by providing a machine-readable inventory of software composition details, play a crucial role in enhancing transparency and traceability within software supply chains. This empirical study delves into the practical challenges and solutions associated with the adoption of SBOMs through an analysis of 4,786 GitHub discussions across 510 SBOM-related projects. Through repository mining and analysis, this research delineates key topics, challenges, and solutions intrinsic to the effective utilization of SBOMs. Furthermore, we shed light on commonly used tools and frameworks for SBOM generation, exploring their respective strengths and limitations. This study underscores a set of findings, for example, there are four phases of the SBOM life cycle, and each phase has a set of SBOM development activities and issues; in addition, this study emphasizes the role SBOM play in ensuring resilient software development practices and the imperative of their widespread adoption and integration to bolster supply chain security. The insights of our study provide vital input for future work and practical advancements in this topic.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "149",
        "numpages": "25",
        "keywords": "Software supply chain, software bill of materials, SBOM, empirical study, mining software repository"
    },
    "MR-Scout: Automated Synthesis of Metamorphic Relations from Existing Test Cases": {
        "type": "article",
        "key": "10.1145/3656340",
        "author": "Xu, Congying and Terragni, Valerio and Zhu, Hengcheng and Wu, Jiarong and Cheung, Shing-Chi",
        "title": "MR-Scout: Automated Synthesis of Metamorphic Relations from Existing Test Cases",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3656340",
        "doi": "10.1145/3656340",
        "abstract": "Metamorphic Testing (MT) alleviates the oracle problem by defining oracles based on metamorphic relations (MRs) that govern multiple related inputs and their outputs. However, designing MRs is challenging, as it requires domain-specific knowledge. This hinders the widespread adoption of MT. We observe that developer-written test cases can embed domain knowledge that encodes MRs. Such encoded MRs could be synthesized for testing not only their original programs but also other programs that share similar functionalities.In this article, we propose MR-Scout to automatically synthesize MRs from test cases in open-source software (OSS) projects. MR-Scout first discovers MR-encoded test cases (MTCs), and then synthesizes the encoded MRs into parameterized methods (called codified MRs), and filters out MRs that demonstrate poor quality for new test case generation. MR-Scout discovered over 11,000 MTCs from 701 OSS projects. Experimental results show that over 97\\% of codified MRs are of high quality for automated test case generation, demonstrating the practical applicability of MR-Scout. Furthermore, codified-MRs-based tests effectively enhance the test adequacy of programs with developer-written tests, leading to 13.52\\% and 9.42\\% increases in line coverage and mutation score, respectively. Our qualitative study shows that 55.76\\% to 76.92\\% of codified MRs are easily comprehensible for developers.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "150",
        "numpages": "28",
        "keywords": "Software testing, metamorphic testing, metamorphic relation, automated test case generation"
    },
    "Replication in Requirements Engineering: The NLP for RE Case": {
        "type": "article",
        "key": "10.1145/3658669",
        "author": "Abualhaija, Sallam and Aydemir, F. Basak and Dalpiaz, Fabiano and Dell'Anna, Davide and Ferrari, Alessio and Franch, Xavier and Fucci, Davide",
        "title": "Replication in Requirements Engineering: The NLP for RE Case",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3658669",
        "doi": "10.1145/3658669",
        "abstract": "Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Despite its empirical vocation, RE research has given limited attention to replication of NLP for RE studies. Replication is hampered by several factors, including the context specificity of the studies, the heterogeneity of the tasks involving NLP, the tasks\u2019 inherent hairiness, and, in turn, the heterogeneous reporting structure. To address these issues, we propose a new artifact, referred to as ID-Card, whose goal is to provide a structured summary of research papers emphasizing replication-relevant information. We construct the ID-Card through a structured, iterative process based on design science. In this article: (i) we report on hands-on experiences of replication; (ii) we review the state-of-the-art and extract replication-relevant information: (iii) we identify, through focus groups, challenges across two typical dimensions of replication: data annotation and tool reconstruction; and (iv) we present the concept and structure of the ID-Card to mitigate the identified challenges. This study aims to create awareness of replication in NLP for RE. We propose an ID-Card that is intended to foster study replication but can also be used in other contexts, e.g., for educational purposes.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "151",
        "numpages": "33",
        "keywords": "Requirements Engineering (RE), Natural Language Processing (NLP), replication, tool reconstruction, annotation, ID card"
    },
    "Focused Test Generation for Autonomous Driving Systems": {
        "type": "article",
        "key": "10.1145/3664605",
        "author": "Zohdinasab, Tahereh and Riccio, Vincenzo and Tonella, Paolo",
        "title": "Focused Test Generation for Autonomous Driving Systems",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664605",
        "doi": "10.1145/3664605",
        "abstract": "Testing Autonomous Driving Systems (ADSs) is crucial to ensure their reliability when navigating complex environments. ADSs may exhibit unexpected behaviours when presented, during operation, with driving scenarios containing features inadequately represented in the training dataset. To address this shift from development to operation, developers must acquire new data with the newly observed features. This data can be then utilised to fine tune the ADS, so as to reach the desired level of reliability in performing driving tasks. However, the resource-intensive nature of testing ADSs requires efficient methodologies for generating targeted and diverse tests.In this work, we introduce a novel approach, DeepAtash-LR, that incorporates a surrogate model into the focused test generation process. This integration significantly improves focused testing effectiveness and applicability in resource-intensive scenarios. Experimental results show that the integration of the surrogate model is fundamental to the success of DeepAtash-LR. Our approach was able to generate an average of up to 60\\texttimes{} more targeted, failure-inducing inputs compared to the baseline approach. Moreover, the inputs generated by DeepAtash-LR were useful to significantly improve the quality of the original ADS through fine tuning.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "152",
        "numpages": "32",
        "keywords": "Software testing, deep learning, search based software engineering, autonomous driving systems"
    },
    "Supporting Emotional Intelligence, Productivity and Team Goals while Handling Software Requirements Changes": {
        "type": "article",
        "key": "10.1145/3664600",
        "author": "Madampe, Kashumi and Hoda, Rashina and Grundy, John",
        "title": "Supporting Emotional Intelligence, Productivity and Team Goals while Handling Software Requirements Changes",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664600",
        "doi": "10.1145/3664600",
        "abstract": "Background: Research shows that emotional intelligence (EI) should be used alongside cognitive intelligence during requirements change (RC) handling in Software Engineering (SE), especially in agile settings. Objective: We wanted to study the role of EI in-depth during RC handling. Method: We conducted a mixed-methods study (an interview study followed by a survey study) with 124 software practitioners. Findings: We found the causal condition, intervening condition and causes lead to key direct consequences of regulating own emotions, managing relationships, and extended consequences of sustaining productivity, setting and sustaining team goals. We found several strategies of supporting EI during RC handling. Further, we found strong correlations between six strategies and one being aware of own emotions, regulating own emotions, sustaining team productivity, and setting and sustaining team goals. Conclusion: Empathising with others and tracking commitments and decisions as a team are key strategies that have strong correlations between managing emotions, between sustaining team productivity, and between setting and sustaining team goals. To the best of our knowledge, the framework we present in this paper is the first theoretical framework on EI in SE research. We provide recommendations for software practitioners to consider during RC handling.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "153",
        "numpages": "38",
        "keywords": "Emotions, emotional intelligence, affects, requirements, changes, human factors, software engineering, software teams, socio-technical grounded theory, agile, well-being, workplace awareness, productivity, team goals"
    },
    "Automatic Repair of Quantum Programs via Unitary Operation": {
        "type": "article",
        "key": "10.1145/3664604",
        "author": "Li, Yuechen and Pei, Hanyu and Huang, Linzhi and Yin, Beibei and Cai, Kai-Yuan",
        "title": "Automatic Repair of Quantum Programs via Unitary Operation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664604",
        "doi": "10.1145/3664604",
        "abstract": "With the continuous advancement of quantum computing (QC), the demand for high-quality quantum programs (QPs) is growing. To avoid program failure, in software engineering, the technology of automatic program repair (APR) employs appropriate patches to remove potential bugs without the intervention of a human. However, the method tailored for repairing defective QPs is still absent. This article proposes, to the best of our knowledge, a new APR method named UnitAR that can repair QPs via unitary operation automatically. Based on the characteristics of superposition and entanglement in QC, the article constructs an algebraic model and adopts a generate-and-validate approach for the repair procedure. Furthermore, the article presents two schemes that can respectively promote the efficiency of generating patches and guarantee the effectiveness of applying patches. For the purpose of evaluating the proposed method, the article selects 29 mutated versions as well as five real-world buggy programs as the objects and introduces two traditional APR approaches GenProg and TBar as baselines. According to the experiments, UnitAR can fix 23 buggy programs, and this method demonstrates the highest efficiency and effectiveness among three APR approaches. Besides, the experimental results further manifest the crucial roles of two constituents involved in the framework of UnitAR.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "154",
        "numpages": "43",
        "keywords": "Quantum computing, automatic program repair, quantum software engineering, unitary operation, software cybernetics, S-ADA"
    },
    "Testing Updated Apps by Adapting Learned Models": {
        "type": "article",
        "key": "10.1145/3664601",
        "author": "Ngo, Chanh Duc and Pastore, Fabrizio and Briand, Lionel",
        "title": "Testing Updated Apps by Adapting Learned Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664601",
        "doi": "10.1145/3664601",
        "abstract": "Although App updates are frequent and software engineers would like to verify updated features only, automated testing techniques verify entire Apps and are thus wasting resources.We present Continuous Adaptation of Learned Models (CALM), an automated App testing approach that efficiently test App updates by adapting App models learned when automatically testing previous App versions. CALM focuses on functional testing. Since functional correctness can be mainly verified through the visual inspection of App screens, CALM minimizes the number of App screens to be visualized by software testers while maximizing the percentage of updated methods and instructions exercised.Our empirical evaluation shows that CALM exercises a significantly higher proportion of updated methods and instructions than six state-of-the-art approaches, for the same maximum number of App screens to be visually inspected. Further, in common update scenarios, where only a small fraction of methods are updated, CALM is even quicker to outperform all competing approaches in a more significant way.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "155",
        "numpages": "40",
        "keywords": "Model reuse, android testing, regression testing, update testing, model-based testing"
    },
    "Fairness Testing of Machine Translation Systems": {
        "type": "article",
        "key": "10.1145/3664608",
        "author": "Sun, Zeyu and Chen, Zhenpeng and Zhang, Jie and Hao, Dan",
        "title": "Fairness Testing of Machine Translation Systems",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664608",
        "doi": "10.1145/3664608",
        "abstract": "Machine translation is integral to international communication and extensively employed in diverse human-related applications. Despite remarkable progress, fairness issues persist within current machine translation systems. In this article, we propose FairMT, an automated fairness testing approach tailored for machine translation systems. FairMT operates on the assumption that translations of semantically similar sentences, containing protected attributes from distinct demographic groups, should maintain comparable meanings. It comprises three key steps: (1) test input generation, producing inputs covering various demographic groups; (2) test oracle generation, identifying potential unfair translations based on semantic similarity measurements; and (3) regression, discerning genuine fairness issues from those caused by low-quality translation. Leveraging FairMT, we conduct an empirical study on three leading machine translation systems\u2013Google Translate, T5, and Transformer. Our investigation uncovers up to 832, 1,984, and 2,627 unfair translations across the three systems, respectively. Intriguingly, we observe that fair translations tend to exhibit superior translation performance, challenging the conventional wisdom of a fairness-performance tradeoff prevalent in the fairness literature.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "156",
        "numpages": "27",
        "keywords": "Fairness testing, metamorphic testing, machine translation, protected attributes"
    },
    "Mobile Application Online Cross-Project Just-in-Time Software Defect Prediction Framework": {
        "type": "article",
        "key": "10.1145/3664607",
        "author": "Jiang, Siyu and He, Zhenhang and Chen, Yuwen and Zhang, Mingrong and Ma, Le",
        "title": "Mobile Application Online Cross-Project Just-in-Time Software Defect Prediction Framework",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664607",
        "doi": "10.1145/3664607",
        "abstract": "As mobile applications evolve rapidly, their fast iterative update nature leads to an increase in software defects. Just-In-Time Software Defect Prediction (JIT-SDP) offers immediate feedback on code changes. For new applications without historical data, researchers have proposed Cross-Project JIT-SDP (CP JIT-SDP). Existing CP JIT-SDP approaches are designed for offline scenarios where target data is available in advance. However, target data in real-world applications usually arrives online in a streaming manner, making online CP JIT-SDP face cross-project distribution differences and target project data concept drift challenges in online scenarios. These challenges often co-exist during application development, and their interactions cause model performance to degrade. To address these issues, we propose an online CP JIT-SDP framework called COTL. Specifically, COTL consists of two stages: offline and online. In the offline stage, the cross-domain structure preserving projection algorithm is used to reduce the cross-project distribution differences. In the online stage, target data arrives sequentially over time. By reducing the differences in marginal and conditional distributions between offline and online data for target project, concept drift is mitigated and classifier weights are updated online. Experimental results on 15 mobile application benchmark datasets show that COTL outperforms 13 benchmark methods on four performance metrics.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "157",
        "numpages": "31",
        "keywords": "Online transfer learning, mobile applications bug prediction, cross-project just-in-time software defect prediction, concept drift"
    },
    "DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks": {
        "type": "article",
        "key": "10.1145/3644388",
        "author": "Aghababaeyan, Zohreh and Abdellatif, Manel and Dadkhah, Mahboubeh and Briand, Lionel",
        "title": "DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3644388",
        "doi": "10.1145/3644388",
        "abstract": "Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. In particular, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this article, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault-revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by selecting diverse mispredicted inputs. The experimental results conducted on four widely used datasets and five DNN models show that in terms of fault-revealing ability, (1) white-box, coverage-based approaches fare poorly, (2) DeepGD outperforms existing black-box test selection approaches in terms of fault detection, and (3) DeepGD also leads to better guidance for DNN model retraining when using selected inputs to augment the training set.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "158",
        "numpages": "29",
        "keywords": "Deep neural network, test case selection, DNN fault detection, multi-objective optimization, deep learning model evaluation, uncertainty metrics, diversity, model retraining guidance"
    },
    "Reducing the Impact of Time Evolution on Source Code Authorship Attribution via Domain Adaptation": {
        "type": "article",
        "key": "10.1145/3652151",
        "author": "Li, Zhen and Zhao, Shasha and Chen, Chen and Chen, Qian",
        "title": "Reducing the Impact of Time Evolution on Source Code Authorship Attribution via Domain Adaptation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3652151",
        "doi": "10.1145/3652151",
        "abstract": "Source code authorship attribution is an important problem in practical applications such as plagiarism detection, software forensics, and copyright disputes. Recent studies show that existing methods for source code authorship attribution can be significantly affected by time evolution, leading to a decrease in attribution accuracy year by year. To alleviate the problem of Deep Learning (DL)-based source code authorship attribution degrading in accuracy due to time evolution, we propose a new framework called Time Domain Adaptation (TimeDA) by adding new feature extractors to the original DL-based code attribution framework that enhances the learning ability of the original model on source domain features without requiring new or more source data. Moreover, we employ a centroid-based pseudo-labeling strategy using neighborhood clustering entropy for adaptive learning to improve the robustness of DL-based code authorship attribution. Experimental results show that TimeDA can significantly enhance the robustness of DL-based source code authorship attribution to time evolution, with an average improvement of 8.7\\% on the Java dataset and 5.2\\% on the C++ dataset. In addition, our TimeDA benefits from employing the centroid-based pseudo-labeling strategy, which significantly reduced the model training time by 87.3\\% compared to traditional unsupervised domain adaptive methods.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "159",
        "numpages": "27",
        "keywords": "Authorship attribution, source code, time evolution, deep learning, domain adaptation"
    },
    "Do Code Summarization Models Process Too Much Information? Function Signature May Be All That Is Needed": {
        "type": "article",
        "key": "10.1145/3652156",
        "author": "Ding, Xi and Peng, Rui and Chen, Xiangping and Huang, Yuan and Bian, Jing and Zheng, Zibin",
        "title": "Do Code Summarization Models Process Too Much Information? Function Signature May Be All That Is Needed",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3652156",
        "doi": "10.1145/3652156",
        "abstract": "With the fast development of large software projects, automatic code summarization techniques, which summarize the main functionalities of a piece of code using natural languages as comments, play essential roles in helping developers understand and maintain large software projects. Many research efforts have been devoted to building automatic code summarization approaches. Typical code summarization approaches are based on deep learning models. They transform the task into a sequence-to-sequence task, which inputs source code and outputs summarizations in natural languages. All code summarization models impose different input size limits, such as 50 to 10,000, for the input source code. However, how the input size limit affects the performance of code summarization models still remains under-explored. In this article, we first conduct an empirical study to investigate the impacts of different input size limits on the quality of generated code comments. To our surprise, experiments on multiple models and datasets reveal that setting a low input size limit, such as 20, does not necessarily reduce the quality of generated comments.Based on this finding, we further propose to use function signatures instead of full source code to summarize the main functionalities first and then input the function signatures into code summarization models. Experiments and statistical results show that inputs with signatures are, on average, more than 2 percentage points better than inputs without signatures and thus demonstrate the effectiveness of involving function signatures in code summarization. We also invite programmers to do a questionnaire to evaluate the quality of code summaries generated by two inputs with different truncation levels. The results show that function signatures generate, on average, 9.2\\% more high-quality comments than full code.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "160",
        "numpages": "35",
        "keywords": "Code summarization, function signature, empirical study"
    },
    "BatFix: Repairing language model-based transpilation": {
        "type": "article",
        "key": "10.1145/3658668",
        "author": "Ramos, Daniel and Lynce, In\\^{e}s and Manquinho, Vasco and Martins, Ruben and Le Goues, Claire",
        "title": "BatFix: Repairing language model-based transpilation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3658668",
        "doi": "10.1145/3658668",
        "abstract": "To keep up with changes in requirements, frameworks, and coding practices, software organizations might need to migrate code from one language to another. Source-to-source migration, or transpilation, is often a complex, manual process. Transpilation requires expertise both in the source and target language, making it highly laborious and costly. Languages models for code generation and transpilation are becoming increasingly popular. However, despite capturing code-structure well, code generated by language models is often spurious and contains subtle problems. We propose BatFix, a novel approach that augments language models for transpilation by leveraging program repair and synthesis to fix the code generated by these models. BatFix takes as input both the original program, the target program generated by the machine translation model, and a set of test cases and outputs a repaired program that passes all test cases. Experimental results show that our approach is agnostic to language models and programming languages. BatFix can locate bugs spawning multiple lines and synthesize patches for syntax and semantic bugs for programs migrated from Java to C++ and Python to C++ from multiple language models, including, OpenAI\u2019s Codex.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "161",
        "numpages": "29",
        "keywords": "Program analysis, automated refactoring, machine learning, transpilation"
    },
    "Deep Domain Adaptation With Max-Margin Principle for Cross-Project Imbalanced Software Vulnerability Detection": {
        "type": "article",
        "key": "10.1145/3664602",
        "author": "Nguyen, Van and Le, Trung and Tantithamthavorn, Chakkrit and Grundy, John and Phung, Dinh",
        "title": "Deep Domain Adaptation With Max-Margin Principle for Cross-Project Imbalanced Software Vulnerability Detection",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664602",
        "doi": "10.1145/3664602",
        "abstract": "Software vulnerabilities (SVs) have become a common, serious, and crucial concern due to the ubiquity of computer software. Many AI-based approaches have been proposed to solve the software vulnerability detection (SVD) problem to ensure the security and integrity of software applications (in both the development and testing phases). However, there are still two open and significant issues for SVD in terms of (i) learning automatic representations to improve the predictive performance of SVD, and (ii) tackling the scarcity of labeled vulnerability datasets that conventionally need laborious labeling effort by experts. In this paper, we propose a novel approach to tackle these two crucial issues. We first exploit the automatic representation learning with deep domain adaptation for SVD. We then propose a novel cross-domain kernel classifier leveraging the max-margin principle to significantly improve the transfer learning process of SVs from imbalanced labeled into imbalanced unlabeled projects. Our approach is the first work that leverages solid body theories of the max-margin principle, kernel methods, and bridging the gap between source and target domains for imbalanced domain adaptation (DA) applied in cross-project SVD. The experimental results on real-world software datasets show the superiority of our proposed method over state-of-the-art baselines. In short, our method obtains a higher performance on F1-measure, one of the most important measures in SVD, from 1.83\\% to 6.25\\% compared to the second highest method in the used datasets.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "162",
        "numpages": "34",
        "keywords": "Software security, automated cross-project vulnerability detection"
    },
    "Enhancing GUI Exploration Coverage of Android Apps with Deep Link-Integrated Monkey": {
        "type": "article",
        "key": "10.1145/3664810",
        "author": "Hu, Han and Wang, Han and Dong, Ruiqi and Chen, Xiao and Chen, Chunyang",
        "title": "Enhancing GUI Exploration Coverage of Android Apps with Deep Link-Integrated Monkey",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664810",
        "doi": "10.1145/3664810",
        "abstract": "Mobile apps are ubiquitous in our daily lives for supporting different tasks such as reading and chatting. Despite the availability of many GUI testing tools, app testers still struggle with low testing code coverage due to tools frequently getting stuck in loops or overlooking activities with concealed entries. This results in a significant amount of testing time being spent on redundant and repetitive exploration of a few GUI pages. To address this, we utilize Android\u2019s deep links, which assist in triggering Android intents to lead users to specific pages and introduce a deep link-enhanced exploration method. This approach, integrated into the testing tool Monkey, gives rise to Delm (Deep Link-enhanced Monkey). Delm oversees the dynamic exploration process, guiding the tool out of meaningless testing loops to unexplored GUI pages. We provide a rigorous activity context mock-up approach for triggering existing Android intents to discover more activities with hidden entrances. We conduct experiments to evaluate Delm\u2019s effectiveness on activity context mock-up, activity coverage, method coverage, and crash detection. The findings reveal that Delm can mock up more complex activity contexts and significantly outperform state-of-the-art baselines with 27.2\\% activity coverage, 21.13\\% method coverage, and 23.81\\% crash detection.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "163",
        "numpages": "31",
        "keywords": "GUI, android GUI testing, android app exploration, app analysis"
    },
    "The IDEA of Us: An Identity-Aware Architecture for Autonomous Systems": {
        "type": "article",
        "key": "10.1145/3654439",
        "author": "Gavidia-Calderon, Carlos and Kordoni, Anastasia and Bennaceur, Amel and Levine, Mark and Nuseibeh, Bashar",
        "title": "The IDEA of Us: An Identity-Aware Architecture for Autonomous Systems",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3654439",
        "doi": "10.1145/3654439",
        "abstract": "Autonomous systems, such as drones and rescue robots, are increasingly used during emergencies. They deliver services and provide situational awareness that facilitate emergency management and response. To do so, they need to interact and cooperate with humans in their environment. Human behaviour is uncertain and complex, so it can be difficult to reason about it formally. In this article, we propose IDEA: an adaptive software architecture that enables cooperation between humans and autonomous systems, by leveraging the social identity approach. This approach establishes that group membership drives human behaviour. Identity and group membership are crucial during emergencies, as they influence cooperation among survivors. IDEA systems infer the social identity of surrounding humans, thereby establishing their group membership. By reasoning about groups, we limit the number of cooperation strategies the system needs to explore. IDEA systems select a strategy from the equilibrium analysis of game-theoretic models that represent interactions between group members and the IDEA system. We demonstrate our approach using a search-and-rescue scenario, in which an IDEA rescue robot optimises evacuation by collaborating with survivors. Using an empirically validated agent-based model, we show that the deployment of the IDEA system can reduce median evacuation time by 13.6\\%.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "164",
        "numpages": "38",
        "keywords": "Autonomous systems, game theory, social identity, agent-based modelling"
    },
    "What Makes a Good TODO Comment?": {
        "type": "article",
        "key": "10.1145/3664811",
        "author": "Wang, Haoye and Gao, Zhipeng and Bi, Tingting and Grundy, John and Wang, Xinyu and Wu, Minghui and Yang, Xiaohu",
        "title": "What Makes a Good TODO Comment?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664811",
        "doi": "10.1145/3664811",
        "abstract": "Software development is a collaborative process that involves various interactions among individuals and teams. TODO comments in source code play a critical role in managing and coordinating diverse tasks during this process. However, this study finds that a large proportion of open-source project TODO comments are left unresolved or take a long time to be resolved. About 46.7\\% of TODO comments in open-source repositories are of low-quality (e.g., TODOs that are ambiguous, lack information, or are useless to developers). This highlights the need for better TODO practices. In this study, we investigate four aspects regarding the quality of TODO comments in open-source projects: (1) the prevalence of low-quality TODO comments; (2) the key characteristics of high-quality TODO comments; (3) how are TODO comments of different quality managed in practice; and (4) the feasibility of automatically assessing TODO comment quality. Examining 2,863 TODO comments from Top100 GitHub Java repositories, we propose criteria to identify high-quality TODO comments and provide insights into their optimal composition. We discuss the lifecycle of TODO comments with varying quality. To assist developers, we construct deep learning-based methods that show promising performance in identifying the quality of TODO comments, potentially enhancing development efficiency and code quality.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "165",
        "numpages": "30",
        "keywords": "Documentation, comment quality, comment lifecycle"
    },
    "A Survey of Source Code Search: A 3-Dimensional Perspective": {
        "type": "article",
        "key": "10.1145/3656341",
        "author": "Sun, Weisong and Fang, Chunrong and Ge, Yifei and Hu, Yuling and Chen, Yuchen and Zhang, Quanjun and Ge, Xiuting and Liu, Yang and Chen, Zhenyu",
        "title": "A Survey of Source Code Search: A 3-Dimensional Perspective",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3656341",
        "doi": "10.1145/3656341",
        "abstract": "(Source) code search is widely concerned by software engineering researchers because it can improve the productivity and quality of software development. Given a functionality requirement usually described in a natural language sentence, a code search system can retrieve code snippets that satisfy the requirement from a large-scale code corpus, e.g., GitHub. To realize effective and efficient code search, many techniques have been proposed successively. These techniques improve code search performance mainly by optimizing three core components, including query understanding component, code understanding component, and query-code matching component. In this article, we provide a 3-dimensional perspective survey for code search. Specifically, we categorize existing code search studies into query-end optimization techniques, code-end optimization techniques, and match-end optimization techniques according to the specific components they optimize. These optimization techniques are proposed to enhance the performance of specific components, and thus the overall performance of code search. Considering that each end can be optimized independently and contributes to the code search performance, we treat each end as a dimension. Therefore, this survey is 3-dimensional in nature, and it provides a comprehensive summary of each dimension in detail. To understand the research trends of the three dimensions in existing code search studies, we systematically review 68 relevant literatures. Different from existing code search surveys that only focus on the query end or code end or introduce various aspects shallowly (including codebase, evaluation metrics, modeling technique, etc.), our survey provides a more nuanced analysis and review of the evolution and development of the underlying techniques used in the three ends. Based on a systematic review and summary of existing work, we outline several open challenges and opportunities at the three ends that remain to be addressed in future work.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "166",
        "numpages": "51",
        "keywords": "Source code search, deep learning, query-end optimization, code-end optimization, match-end optimization"
    },
    "Communicating Study Design Trade-offs in Software Engineering": {
        "type": "article",
        "key": "10.1145/3649598",
        "author": "Robillard, Martin P. and Arya, Deeksha M. and Ernst, Neil A. and Guo, Jin L. C. and Lamothe, Maxime and Nassif, Mathieu and Novielli, Nicole and Serebrenik, Alexander and Steinmacher, Igor and Stol, Klaas-Jan",
        "title": "Communicating Study Design Trade-offs in Software Engineering",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649598",
        "doi": "10.1145/3649598",
        "abstract": "Reflecting on the limitations of a study is a crucial part of the research process. In software engineering studies, this reflection is typically conveyed through discussions of study limitations or threats to validity. In current practice, such discussions seldom provide sufficient insight to understand the rationale for decisions taken before and during the study, and their implications. We revisit the practice of discussing study limitations and threats to validity and identify its weaknesses. We propose to refocus this practice of self-reflection to a discussion centered on the notion of trade-offs. We argue that documenting trade-offs allows researchers to clarify how the benefits of their study design decisions outweigh the costs of possible alternatives. We present guidelines for reporting trade-offs in a way that promotes a fair and dispassionate assessment of researchers\u2019 work.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "112",
        "numpages": "10",
        "keywords": "Empirical software engineering, threats to validity, empirical study design, metascience, research validity, research design trade-offs"
    },
    "Learning Failure-Inducing Models for Testing Software-Defined Networks": {
        "type": "article",
        "key": "10.1145/3641541",
        "author": "Ollando, Rapha\\\"{e}l and Shin, Seung Yeob and Briand, Lionel C.",
        "title": "Learning Failure-Inducing Models for Testing Software-Defined Networks",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3641541",
        "doi": "10.1145/3641541",
        "abstract": "Software-defined networks (SDN) enable flexible and effective communication systems that are managed by centralized software controllers. However, such a controller can undermine the underlying communication network of an SDN-based system and thus must be carefully tested. When an SDN-based system fails, in order to address such a failure, engineers need to precisely understand the conditions under which it occurs. In this article, we introduce a machine learning-guided fuzzing method, named FuzzSDN, aiming at both (1)&nbsp;generating effective test data leading to failures in SDN-based systems and (2)&nbsp;learning accurate failure-inducing models that characterize conditions under which such system fails. To our knowledge, no existing work simultaneously addresses these two objectives for SDNs. We evaluate FuzzSDN by applying it to systems controlled by two open-source SDN controllers. Furthermore, we compare FuzzSDN with two state-of-the-art methods for fuzzing SDNs and two baselines for learning failure-inducing models. Our results show that (1)&nbsp;compared to the state-of-the-art methods, FuzzSDN generates at least 12 times more failures, within the same time budget, with a controller that is fairly robust to fuzzing and (2)&nbsp;our failure-inducing models have, on average, a precision of 98\\% and a recall of 86\\%, significantly outperforming the baselines.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "113",
        "numpages": "25",
        "keywords": "Software-defined networks, software testing, fuzzing, machine learning"
    },
    "sGuard+: Machine Learning Guided Rule-Based Automated Vulnerability Repair on Smart Contracts": {
        "type": "article",
        "key": "10.1145/3641846",
        "author": "Gao, Cuifeng and Yang, Wenzhang and Ye, Jiaming and Xue, Yinxing and Sun, Jun",
        "title": "sGuard+: Machine Learning Guided Rule-Based Automated Vulnerability Repair on Smart Contracts",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3641846",
        "doi": "10.1145/3641846",
        "abstract": "Smart contracts are becoming appealing targets for hackers because of the vast amount of cryptocurrencies under their control. Asset loss due to the exploitation of smart contract codes has increased significantly in recent years. To guarantee that smart contracts are vulnerability-free, there are many works to detect the vulnerabilities of smart contracts, but only a few vulnerability repair works have been proposed. Repairing smart contract vulnerabilities at the source code level is attractive as it is transparent to users, whereas existing repair tools, such as SCRepair and sGuard, suffer from many limitations: (1) ignoring the code of vulnerability prevention; (2) possibly applying the repair to the wrong statements and changing the original business logic of smart contracts; and (3) showing poor performance in terms of time and gas overhead.In this work, we propose machine learning guided rule-based automated vulnerability repair on smart contracts to improve the effectiveness and efficiency of sGuard. To address the limitations mentioned above, we design the features that characterize both the symptoms of vulnerabilities and the methods of vulnerability prevention to learn various vulnerability patterns and reduce false positives. Additionally, a fine-grained localization algorithm is designed by traversing the nodes of the abstract syntax tree, and we refine and extend the repair rules of sGuard to preserve the original business logic of smart contracts and support new vulnerability types. Our tool, named sGuard+, reduces time overhead based on machine learning models, and reduces gas overhead by fewer code changes and precise patching.In our experiment, we collect a publicly available vulnerability dataset from CVE, SWC, and SmartBugs Curated as a ground truth for evaluations. Overall, sGuard+ repairs more vulnerabilities with less time and gas overhead than state-of-the-art tools. Furthermore, we reproduce about 9,000 historical transactions for regression testing. It is shown that sGuard+ has no impact on the original business logic of smart contracts.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "114",
        "numpages": "55",
        "keywords": "Vulnerability repair, smart contract, machine learning"
    },
    "Try with Simpler - An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection": {
        "type": "article",
        "key": "10.1145/3644386",
        "author": "Yang, Lin and Chen, Junjie and Gao, Shutao and Gong, Zhihao and Zhang, Hongyu and Kang, Yue and Li, Huaan",
        "title": "Try with Simpler - An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3644386",
        "doi": "10.1145/3644386",
        "abstract": "With the rapid development of deep learning (DL), the recent trend of log-based anomaly detection focuses on extracting semantic information from log events (i.e., templates of log messages) and designing more advanced DL models for anomaly detection. Indeed, the effectiveness of log-based anomaly detection can be improved, but these DL-based techniques further suffer from the limitations of more heavy dependency on training data (such as data quality or data labels) and higher costs in time and resources due to the complexity and scale of DL models, which hinder their practical use. On the contrary, the techniques based on traditional machine learning or data mining algorithms are less dependent on training data and more efficient but produce worse effectiveness than DL-based techniques, which is mainly caused by the problem of unseen log events (some log events in incoming log messages are unseen in training data) confirmed by our motivating study. Intuitively, if we can improve the effectiveness of traditional techniques to be comparable with advanced DL-based techniques, then log-based anomaly detection can be more practical. Indeed, an existing study in the other area (i.e., linking questions posted on Stack Overflow) has pointed out that traditional techniques with some optimizations can indeed achieve comparable effectiveness with the state-of-the-art DL-based technique, indicating the feasibility of enhancing traditional log-based anomaly detection techniques to some degree.Inspired by the idea of \u201ctry-with-simpler,\u201d we conducted the first empirical study to explore the potential of improving traditional techniques for more practical log-based anomaly detection. In this work, we optimized the traditional unsupervised PCA (Principal Component Analysis) technique by incorporating a lightweight semantic-based log representation in it, called SemPCA, and conducted an extensive study to investigate the potential of SemPCA for more practical log-based anomaly detection. By comparing seven log-based anomaly detection techniques (including four DL-based techniques, two traditional techniques, and SemPCA) on both public and industrial datasets, our results show that SemPCA achieves comparable effectiveness as advanced supervised/semi-supervised DL-based techniques while being much more stable under insufficient training data and more efficient, demonstrating that the traditional technique can still excel after small but useful adaptation.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "115",
        "numpages": "27",
        "keywords": "Anomaly detection, log analysis, deep learning, machine learning, empirical study"
    },
    "Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues": {
        "type": "article",
        "key": "10.1145/3643674",
        "author": "Liu, Yue and Le-Cong, Thanh and Widyasari, Ratnadira and Tantithamthavorn, Chakkrit and Li, Li and Le, Xuan-Bach D. and Lo, David",
        "title": "Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3643674",
        "doi": "10.1145/3643674",
        "abstract": "Since its introduction in November 2022, ChatGPT has rapidly gained popularity due to its remarkable ability in language understanding and human-like responses. ChatGPT, based on GPT-3.5 architecture, has shown great promise for revolutionizing various research fields, including code generation. However, the reliability and quality of code generated by ChatGPT remain unexplored, raising concerns about potential risks associated with the widespread use of ChatGPT-driven code generation.In this article, we systematically study the quality of 4,066 ChatGPT-generated programs of code implemented in two popular programming languages, i.e., Java and Python, for 2,033 programming tasks. The goal of this work is threefold. First, we analyze the correctness of ChatGPT on code generation tasks and uncover the factors that influence its effectiveness, including task difficulty, programming language, time that tasks are introduced, and program size. Second, we identify and characterize potential issues with the quality of ChatGPT-generated code. Last, we provide insights into how these issues can be mitigated. Experiments highlight that out of 4,066 programs generated by ChatGPT, 2,756 programs are deemed correct, 1,082 programs provide wrong outputs, and 177 programs contain compilation or runtime errors. Additionally, we further analyze other characteristics of the generated code through static analysis tools, such as code style and maintainability, and find that 1,930 ChatGPT-generated code snippets suffer from maintainability issues. Subsequently, we investigate ChatGPT\u2019s self-repairing ability and its interaction with static analysis tools to fix the errors uncovered in the previous step. Experiments suggest that ChatGPT can partially address these challenges, improving code quality by more than 20\\%, but there are still limitations and opportunities for improvement. Overall, our study provides valuable insights into the current limitations of ChatGPT and offers a roadmap for future research and development efforts to enhance the code generation capabilities of artificial intelligence models such as ChatGPT.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "116",
        "numpages": "26",
        "keywords": "Automated code generation, ChatGPT, code analysis"
    },
    "An Empirical Analysis of Issue Templates Usage in Large-Scale Projects on GitHub": {
        "type": "article",
        "key": "10.1145/3643673",
        "author": "S\\\"{u}l\\\"{u}n, Emre and Sa\\c{c}ak\\c{c}\\i{}, Metehan and T\\\"{u}z\\\"{u}n, Eray",
        "title": "An Empirical Analysis of Issue Templates Usage in Large-Scale Projects on GitHub",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3643673",
        "doi": "10.1145/3643673",
        "abstract": "GitHub Issues is a widely used issue tracking tool in open-source software projects. Originally designed with broad flexibility, its lack of standardization led to incomplete issue reports, impeding software development and maintenance efficiency. To counteract this, GitHub introduced issue templates in 2016, which rapidly became popular. Our study assesses the current use and evolution of these templates in large-scale open-source projects and their impact on issue tracking metrics, including resolution time, number of reopens, and number of issue comments. Employing a comprehensive analysis of 350 templates from 100 projects, we also evaluated over 1.9 million issues for template conformity and impact. Additionally, we solicited insights from open-source software maintainers through a survey. Our findings highlight issue templates\u2019 extensive usage in 99 of the 100 surveyed projects, with a growing preference for YAML-based templates, a more structured template variant. Projects with a template exhibited markedly reduced resolution time (381.02 days to 103.18&nbsp;days) and reduced issue comment count (4.95 to 4.32) compared to those without. The use of YAML-based templates further significantly decreased resolution time, the number of reopenings, and the discussion extent. Thus, our research underscores issue templates\u2019 positive impact on large-scale open-source projects, offering recommendations for improved effectiveness.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "117",
        "numpages": "28",
        "keywords": "Issue templates, issue forms, issue tracking, GitHub issues, bug tracking, empirical study"
    },
    "Enumerating Valid Non-Alpha-Equivalent Programs for Interpreter Testing": {
        "type": "article",
        "key": "10.1145/3647994",
        "author": "Xia, Xinmeng and Feng, Yang and Shi, Qingkai and Jones, James A. and Zhang, Xiangyu and Xu, Baowen",
        "title": "Enumerating Valid Non-Alpha-Equivalent Programs for Interpreter Testing",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3647994",
        "doi": "10.1145/3647994",
        "abstract": "Skeletal program enumeration (SPE) can generate a great number of test programs for validating the correctness of compilers or interpreters. The classic SPE generates programs by exhaustively enumerating all possible variable usage patterns into a given syntactic structure. Even though it is capable of producing many test programs, the exhaustive enumeration strategy generates a large number of invalid programs, which may waste plenty of testing time and resources. To address the problem, this article proposes a tree-based SPE technique. Compared to the state-of-the-art, the key merit of the tree-based approach is that it allows us to take the dependency information into consideration when producing test programs and, thus, make it possible to (1) directly generate non-equivalent programs and (2) apply dominance relations to eliminate invalid test programs that have undefined variables. Hence, our approach significantly saves the cost of the na\\\"{\\i}ve SPE approach. We have implemented our approach into an automated testing tool, IFuzzer, and applied it to test eight different implementations of Python interpreters, including CPython, PyPy, IronPython, Jython, RustPython, GPython, Pyston, and Codon. In three months of fuzzing, IFuzzer detected 142 bugs, of which 87 have been confirmed to be previously unknown bugs, of which 34 have been fixed. Compared to the state-of-the-art SPE techniques, IFuzzer takes only 61.0\\% of the time cost given the same number of testing seeds and improves 5.3\\% source code function coverage in the same time budget of testing.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "118",
        "numpages": "31",
        "keywords": "Interpreter testing, fuzz testing, program enumeration"
    },
    "Dynamic Transitive Closure-based Static Analysis through the Lens of Quantum Search": {
        "type": "article",
        "key": "10.1145/3644389",
        "author": "Ren, Jiawei and Sui, Yulei and Cheng, Xiao and Feng, Yuan and Zhao, Jianjun",
        "title": "Dynamic Transitive Closure-based Static Analysis through the Lens of Quantum Search",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3644389",
        "doi": "10.1145/3644389",
        "abstract": "Many existing static analysis algorithms suffer from cubic bottlenecks because of the need to compute a dynamic transitive closure (DTC). For the first time, this article studies the quantum speedups on searching subtasks in DTC-based static analysis algorithms using quantum search (e.g., Grover\u2019s algorithm). We first introduce our oracle implementation in Grover\u2019s algorithm for DTC-based static analysis and illustrate our quantum search subroutine. Then, we take two typical DTC-based analysis algorithms: context-free-language reachability and set constraint-based analysis, and show that our quantum approach can reduce the time complexity of these two algorithms to truly subcubic ( (O(N^2sqrt {N}{it polylog}(N))) ), yielding better results than the upper bound (O(N3/log N)) of existing classical algorithms. Finally, we conducted a classical simulation of Grover\u2019s search to validate our theoretical approach, due to the current quantum hardware limitation of lacking a practical, large-scale, noise-free quantum machine. We evaluated the correctness and efficiency of our approach using IBM Qiskit on nine open-source projects and randomly generated edge-labeled graphs/constraints. The results demonstrate the effectiveness of our approach and shed light on the promising direction of applying quantum algorithms to address the general challenges in static analysis.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "119",
        "numpages": "29",
        "keywords": "CFL-reachability, set constraint-based analysis, grover\u2019s search"
    },
    "Non-Autoregressive Line-Level Code Completion": {
        "type": "article",
        "key": "10.1145/3649594",
        "author": "Liu, Fang and Fu, Zhiyi and Li, Ge and Jin, Zhi and Liu, Hui and Hao, Yiyang and Zhang, Li",
        "title": "Non-Autoregressive Line-Level Code Completion",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649594",
        "doi": "10.1145/3649594",
        "abstract": "Software developers frequently use code completion tools to accelerate software development by suggesting the following code elements. Researchers usually employ AutoRegressive (AR) decoders to complete code sequences in a left-to-right, token-by-token fashion. To improve the accuracy and efficiency of code completion, we argue that tokens within a code statement have the potential to be predicted concurrently. In this article, we first conduct an empirical study to analyze the dependency among the target tokens in line-level code completion. The results suggest that it is potentially practical to generate all statement tokens in parallel. To this end, we introduce SANAR, a simple and effective syntax-aware non-autoregressive model for line-level code completion. To further improve the quality of the generated code, we propose an adaptive and syntax-aware sampling strategy to boost the model\u2019s performance. The experimental results obtained from two widely used datasets indicate that our model outperforms state-of-the-art code completion approaches of similar model size by a considerable margin, and is faster than these models with up to 9\\texttimes{} speed-up. Moreover, the extensive results additionally demonstrate that the enhancements achieved by SANAR become even more pronounced with larger model sizes, highlighting their significance.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "120",
        "numpages": "34",
        "keywords": "Code completion, neural networks, non-autoregressive generation"
    },
    "Precisely Extracting Complex Variable Values from Android Apps": {
        "type": "article",
        "key": "10.1145/3649591",
        "author": "Miltenberger, Marc and Arzt, Steven",
        "title": "Precisely Extracting Complex Variable Values from Android Apps",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649591",
        "doi": "10.1145/3649591",
        "abstract": "Millions of users nowadays rely on their smartphones to process sensitive data through apps from various vendors and sources. Therefore, it is vital to assess these apps for security vulnerabilities and privacy violations. Information such as to which server an app connects through which protocol, and which algorithm it applies for encryption, are usually encoded as variable values and arguments of API calls. However, extracting these values from an app is not trivial. The source code of an app is usually not available, and manual reverse engineering is cumbersome with binary sizes in the tens of megabytes. Current automated tools, however, cannot retrieve values that are computed at runtime through complex transformations.In this article, we present ValDroid, a novel static analysis tool for automatically extracting the set of possible values for a given variable at a given statement in the Dalvik byte code of an Android app. We evaluate ValDroid against existing approaches (JSA, Violist, DroidRA, Harvester, BlueSeal, StringHound, IC3, and COAL) on benchmarks and 794 real-world apps. ValDroid greatly outperforms existing tools. It provides an average F1 score of more than 90\\%, while only requiring 0.1 s per value on average. For many data types including Network Connections and Dynamic Code Loading, its recall is more than twice the recall of the best existing approaches.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "121",
        "numpages": "56",
        "keywords": "Static analysis, Android, security, value analysis, mobile"
    },
    "DinoDroid: Testing Android Apps Using Deep Q-Networks": {
        "type": "article",
        "key": "10.1145/3652150",
        "author": "Zhao, Yu and Harrison, Brent and Yu, Tingting",
        "title": "DinoDroid: Testing Android Apps Using Deep Q-Networks",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3652150",
        "doi": "10.1145/3652150",
        "abstract": "The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers need to guarantee the quality of mobile apps before it is released to the market. There have been many approaches using different strategies to test the GUI of mobile apps. However, they still need improvement due to their limited effectiveness. In this article, we propose DinoDroid, an approach based on deep Q-networks to automate testing of Android apps. DinoDroid learns a behavior model from a set of existing apps and the learned model can be used to explore and generate tests for new apps. DinoDroid is able to capture the fine-grained details of GUI events (e.g., the content of GUI widgets) and use them as features that are fed into deep neural network, which acts as the agent to guide app exploration. DinoDroid automatically adapts the learned model during the exploration without the need of any modeling strategies or pre-defined rules. We conduct experiments on 64 open-source Android apps. The results showed that DinoDroid outperforms existing Android testing tools in terms of code coverage and bug detection.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "122",
        "numpages": "24",
        "keywords": "Mobile testing, deep q-networks, reinforcement learning"
    },
    "Generating Python Type Annotations from Type Inference: How Far Are We?": {
        "type": "article",
        "key": "10.1145/3652153",
        "author": "Guo, Yimeng and Chen, Zhifei and Chen, Lin and Xu, Wenjie and Li, Yanhui and Zhou, Yuming and Xu, Baowen",
        "title": "Generating Python Type Annotations from Type Inference: How Far Are We?",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3652153",
        "doi": "10.1145/3652153",
        "abstract": "In recent years, dynamic languages such as Python have become popular due to their flexibility and productivity. The lack of static typing makes programs face the challenges of fixing type errors, early bug detection, and code understanding. To alleviate these issues, PEP 484 introduced optional type annotations for Python in 2014, but unfortunately, a large number of programs are still not annotated by developers. Annotation generation tools can utilize type inference techniques. However, several important aspects of type annotation generation are overlooked by existing works, such as in-depth effectiveness analysis, potential improvement exploration, and practicality evaluation. And it is unclear how far we have been and how far we can go.In this paper, we set out to comprehensively investigate the effectiveness of type inference tools for generating type annotations, applying three categories of state-of-the-art tools on a carefully-cleaned dataset. First, we use a comprehensive set of metrics and categories, finding that existing tools have different effectiveness and cannot achieve both high accuracy and high coverage. Then, we summarize six patterns to present the limitations in type annotation generation. Next, we implement a simple but effective tool to demonstrate that existing tools can be improved in practice. Finally, we conduct a controlled experiment showing that existing tools can reduce the time spent annotating types and determine more precise types, but cannot reduce subjective difficulty. Our findings point out the limitations and improvement directions in type annotation generation, which can inspire future work.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "123",
        "numpages": "38",
        "keywords": "Type annotations, type inference, Python, empirical study"
    },
    "Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains": {
        "type": "article",
        "key": "10.1145/3638247",
        "author": "Cheng, Yu and Chen, Jieshan and Huang, Qing and Xing, Zhenchang and Xu, Xiwei and Lu, Qinghua",
        "title": "Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3638247",
        "doi": "10.1145/3638247",
        "abstract": "The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e., prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we systematically review, summarise, refine and extend the concept of AI chain by incorporating the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematize AI chain engineering methodology. We also develop a no-code integrated development environment, , which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "124",
        "numpages": "24",
        "keywords": "AI chain engineering, visual programming, large language models, No/Low code, SE for AI"
    },
    "Machine Translation Testing via Syntactic Tree Pruning": {
        "type": "article",
        "key": "10.1145/3640329",
        "author": "Zhang, Quanjun and Zhai, Juan and Fang, Chunrong and Liu, Jiawei and Sun, Weisong and Hu, Haichuan and Wang, Qingyu",
        "title": "Machine Translation Testing via Syntactic Tree Pruning",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3640329",
        "doi": "10.1145/3640329",
        "abstract": "Machine translation systems have been widely adopted in our daily life, making life easier and more convenient. Unfortunately, erroneous translations may result in severe consequences, such as financial losses. This requires to improve the accuracy and the reliability of machine translation systems. However, it is challenging to test machine translation systems because of the complexity and intractability of the underlying neural models. To tackle these challenges, we propose a novel metamorphic testing approach by syntactic tree pruning (STP) to validate machine translation systems. Our key insight is that a pruned sentence should have similar crucial semantics compared with the original sentence. Specifically, STP (1) proposes a core semantics-preserving pruning strategy by basic sentence structures and dependency relations on the level of syntactic tree representation, (2) generates source sentence pairs based on the metamorphic relation, and (3) reports suspicious issues whose translations break the consistency property by a bag-of-words model. We further evaluate STP on two state-of-the-art machine translation systems (i.e., Google Translate and Bing Microsoft Translator) with 1,200 source sentences as inputs. The results show that STP accurately finds 5,073 unique erroneous translations in Google Translate and 5,100 unique erroneous translations in Bing Microsoft Translator (400\\% more than state-of-the-art techniques), with 64.5\\% and 65.4\\% precision, respectively. The reported erroneous translations vary in types and more than 90\\% of them are not found by state-of-the-art techniques. There are 9,393 erroneous translations unique to STP, which is 711.9\\% more than state-of-the-art techniques. Moreover, STP is quite effective in detecting translation errors for the original sentences with a recall reaching 74.0\\%, improving state-of-the-art techniques by 55.1\\% on average.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "125",
        "numpages": "39",
        "keywords": "Software testing, machine translation, metamorphic testing"
    },
    "On the Reliability and Explainability of Language Models for Program Generation": {
        "type": "article",
        "key": "10.1145/3641540",
        "author": "Liu, Yue and Tantithamthavorn, Chakkrit and Liu, Yonghui and Li, Li",
        "title": "On the Reliability and Explainability of Language Models for Program Generation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3641540",
        "doi": "10.1145/3641540",
        "abstract": "Recent studies have adopted pre-trained language models, such as CodeT5 and CodeGPT, for automated program generation tasks like code generation, repair, and translation. Numerous language model based approaches have been proposed and evaluated on various benchmark datasets, demonstrating promising performance. However, there is still uncertainty about the reliability of these models, particularly their realistic ability to consistently transform code sequences. This raises a question: are these techniques sufficiently trustworthy for automated program generation? Consequently, further research is needed to understand model logic and assess reliability and explainability. To bridge these research gaps, we conduct a thorough empirical study of eight popular language models on five representative datasets to determine the capabilities and limitations of automated program generation approaches. We further employ advanced explainable AI approaches to highlight the tokens that significantly contribute to the code transformation. We discover that state-of-the-art approaches suffer from inappropriate performance evaluation stemming from severe data duplication, causing overoptimistic results. Our explainability analysis reveals that, in various experimental scenarios, language models can recognize code grammar and structural information, but they exhibit limited robustness to changes in input sequences. Overall, more rigorous evaluation approaches and benchmarks are critical to enhance the reliability and explainability of automated program generation moving forward. Our findings provide important guidelines for this goal.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "126",
        "numpages": "26",
        "keywords": "Automated program generation, empirical analysis, explainable AI"
    },
    "Beyond Fidelity: Explaining Vulnerability Localization of Learning-Based Detectors": {
        "type": "article",
        "key": "10.1145/3641543",
        "author": "Cheng, Baijun and Zhao, Shengming and Wang, Kailong and Wang, Meizhen and Bai, Guangdong and Feng, Ruitao and Guo, Yao and Ma, Lei and Wang, Haoyu",
        "title": "Beyond Fidelity: Explaining Vulnerability Localization of Learning-Based Detectors",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3641543",
        "doi": "10.1145/3641543",
        "abstract": "Vulnerability detectors based on deep learning&nbsp;(DL) models have proven their effectiveness in recent years. However, the shroud of opacity surrounding the decision-making process of these detectors makes it difficult for security analysts to comprehend. To address this, various explanation approaches have been proposed to explain the predictions by highlighting important features, which have been demonstrated effective in domains such as computer vision and natural language processing. Unfortunately, there is still a lack of in-depth evaluation of vulnerability-critical features, such as fine-grained vulnerability-related code lines, learned and understood by these explanation approaches. In this study, we first evaluate the performance of ten explanation approaches for vulnerability detectors based on graph and sequence representations, measured by two quantitative metrics including fidelity and vulnerability line coverage rate. Our results show that fidelity alone is insufficent for evaluating these approaches, as fidelity incurs significant fluctuations across different datasets and detectors. We subsequently check the precision of the vulnerability-related code lines reported by the explanation approaches, and find poor accuracy in this task among all of them. This can be attributed to the inefficiency of explainers in selecting important features and the presence of irrelevant artifacts learned by DL-based detectors.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "127",
        "numpages": "33",
        "keywords": "Vulnerability Detection, Explanation Approaches, Fidelity, Coverage Rate"
    },
    "RAPID: Zero-Shot Domain Adaptation for Code Search with Pre-Trained Models": {
        "type": "article",
        "key": "10.1145/3641542",
        "author": "Fan, Guodong and Chen, Shizhan and Gao, Cuiyun and Xiao, Jianmao and Zhang, Tao and Feng, Zhiyong",
        "title": "RAPID: Zero-Shot Domain Adaptation for Code Search with Pre-Trained Models",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3641542",
        "doi": "10.1145/3641542",
        "abstract": "Code search, which refers to the process of identifying the most relevant code snippets for a given natural language query, plays a crucial role in software maintenance. However, current approaches heavily rely on labeled data for training, which results in performance decreases when confronted with cross-domain scenarios including domain- or project-specific situations. This decline can be attributed to their limited ability to effectively capture the semantics associated with such scenarios. To tackle the aforementioned problem, we propose a zeRo-shot domAin adaPtion with pre-traIned moDels framework for code search named RAPID. The framework first generates synthetic data by pseudo labeling, then trains the CodeBERT with sampled synthetic data. To avoid the influence of noisy synthetic data and enhance the model performance, we propose a mixture sampling strategy to obtain hard negative samples during training. Specifically, the mixture sampling strategy considers both relevancy and diversity to select the data that are hard to be distinguished by the models. To validate the effectiveness of our approach in zero-shot settings, we conduct extensive experiments and find that RAPID outperforms the CoCoSoDa and UniXcoder model by an average of 15.7\\% and 10\\%, respectively, as measured by the MRR metric. When trained on full data, our approach results in an average improvement of 7.5\\% under the MRR metric using CodeBERT. We observe that as the model\u2019s performance in zero-shot tasks improves, the impact of hard negatives diminishes. Our observation also indicates that fine-tuning CodeT5 for generating pseudo labels can enhance the performance of the code search model, and using only 100-shot samples can yield comparable results to the supervised baseline. Furthermore, we evaluate the effectiveness of RAPID in real-world code search tasks in three GitHub projects through both human and automated assessments. Our findings reveal RAPID exhibits superior performance, e.g., an average improvement of 18\\% under the MRR metric over the top-performing model.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "128",
        "numpages": "35",
        "keywords": "Code search, zero-shot learning, software maintenance, pre-trained models, domain adaption"
    },
    "Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks": {
        "type": "article",
        "key": "10.1145/3644387",
        "author": "Liu, Jiaxiang and Xing, Yunhan and Shi, Xiaomu and Song, Fu and Xu, Zhiwu and Ming, Zhong",
        "title": "Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3644387",
        "doi": "10.1145/3644387",
        "abstract": "As a new programming paradigm, deep neural networks (DNNs) have been increasingly deployed in practice, but the lack of robustness hinders their applications in safety-critical domains. While there are techniques for verifying DNNs with formal guarantees, they are limited in scalability and accuracy. In this article, we present a novel counterexample-guided abstraction refinement (CEGAR) approach for scalable and exact verification of DNNs. Specifically, we propose a novel abstraction to break down the size of DNNs by over-approximation. The result of verifying the abstract DNN is conclusive if no spurious counterexample is reported. To eliminate each spurious counterexample introduced by abstraction, we propose a novel counterexample-guided refinement that refines the abstract DNN to exclude the spurious counterexample while still over-approximating the original one, leading to a sound, complete yet efficient CEGAR approach. Our approach is orthogonal to and can be integrated with many existing verification techniques. For demonstration, we implement our approach using two promising tools, Marabou and Planet, as the underlying verification engines, and evaluate on widely used benchmarks for three datasets ACAS, Xu, MNIST, and CIFAR-10. The results show that our approach can boost their performance by solving more problems in the same time limit, reducing on average 13.4\\%\u201386.3\\% verification time of Marabou on almost all the verification tasks, and reducing on average 8.3\\%\u201378.0\\% verification time of Planet on all the verification tasks. Compared to the most relevant CEGAR-based approach, our approach is 11.6\u201326.6 times faster.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "129",
        "numpages": "35",
        "keywords": "Neural networks, abstraction, refinement, CEGAR, formal verification, robustness"
    },
    "Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches": {
        "type": "article",
        "key": "10.1145/3643671",
        "author": "Attaoui, Mohammed Oualid and Fahmy, Hazem and Pastore, Fabrizio and Briand, Lionel",
        "title": "Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3643671",
        "doi": "10.1145/3643671",
        "abstract": "The adoption of deep neural networks (DNNs) in safety-critical contexts is often prevented by the lack of effective means to explain their results, especially when they are erroneous. In our previous work, we proposed a white-box approach (HUDD) and a black-box approach (SAFE) to automatically characterize DNN failures. They both identify clusters of similar images from a potentially large set of images leading to DNN failures. However, the analysis pipelines for HUDD and SAFE were instantiated in specific ways according to common practices, deferring the analysis of other pipelines to future work.In this article, we report on an empirical evaluation of 99 different pipelines for root cause analysis of DNN failures. They combine transfer learning, autoencoders, heatmaps of neuron relevance, dimensionality reduction techniques, and different clustering algorithms. Our results show that the best pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters almost exclusively capturing images of the same failure scenario, thus facilitating root cause analysis. Further, it generates distinct clusters for each root cause of failure, thus enabling engineers to detect all the unsafe scenarios. Interestingly, these results hold even for failure scenarios that are only observed in a small percentage of the failing images.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "130",
        "numpages": "48",
        "keywords": "DNN explanation, DNN functional safety analysis, DNN debugging, clustering, transfer learning"
    },
    "Analyzing and Detecting Information Types of Developer Live Chat Threads": {
        "type": "article",
        "key": "10.1145/3643677",
        "author": "Shang, Xiuwei and Zhang, Shuai and Zhang, Yitong and Guo, Shikai and Li, Yulong and Chen, Rong and Li, Hui and Li, Xiaochen and Jiang, He",
        "title": "Analyzing and Detecting Information Types of Developer Live Chat Threads",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3643677",
        "doi": "10.1145/3643677",
        "abstract": "Online chatrooms serve as vital platforms for information exchange among software developers. With multiple developers engaged in rapid communication and diverse conversation topics, the resulting chat messages often manifest complexity and lack structure. To enhance the efficiency of extracting information from chat threads, automatic mining techniques are introduced for thread classification. However, previous approaches still grapple with unsatisfactory classification accuracy due to two primary challenges that they struggle to adequately capture long-distance dependencies within chat threads and address the issue of category imbalance in labeled datasets. To surmount these challenges, we present a topic classification approach for chat information types named EAEChat. Specifically, EAEChat comprises three core components: the text feature encoding component captures contextual text features using a multi-head self-attention mechanism-based text feature encoder, and a siamese network is employed to mitigate overfitting caused by limited data; the data augmentation component expands a small number of categories in the training dataset using a technique tailored to developer chat messages, effectively tackling the challenge of imbalanced category distribution; the non-text feature encoding component employs a feature fusion model to integrate deep text features with manually extracted non-text features. Evaluation across three real-world projects demonstrates that EAEChat, respectively, achieves an average precision, recall, and F1-score of 0.653, 0.651, and 0.644, and it marks a significant 7.60\\% improvement over the state-of-the-art approaches. These findings confirm the effectiveness of our method in proficiently classifying developer chat messages in online chatrooms.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "131",
        "numpages": "32",
        "keywords": "Developer chatroom, information type classification, data augmentation, deep learning"
    },
    "Test Input Prioritization for 3D Point Clouds": {
        "type": "article",
        "key": "10.1145/3643676",
        "author": "Li, Yinghua and Dang, Xueqi and Ma, Lei and Klein, Jacques and Le Traon, Yves and Bissyand\\'{e}, Tegawend\\'{e} F.",
        "title": "Test Input Prioritization for 3D Point Clouds",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3643676",
        "doi": "10.1145/3643676",
        "abstract": "3D point cloud applications have become increasingly prevalent in diverse domains, showcasing their efficacy in various software systems. However, testing such applications presents unique challenges due to the high-dimensional nature of 3D point cloud data and the vast number of possible test cases. Test input prioritization has emerged as a promising approach to enhance testing efficiency by prioritizing potentially misclassified test cases during the early stages of the testing process. Consequently, this enables the early labeling of critical inputs, leading to a reduction in the overall labeling cost. However, applying existing prioritization methods to 3D point cloud data is constrained by several factors: (1) inadequate consideration of crucial spatial information, and (2) susceptibility to noises inherent in 3D point cloud data. In this article, we propose PCPrior, the first test prioritization approach specifically designed for 3D point cloud test cases. The fundamental concept behind PCPrior is that test inputs closer to the decision boundary of the model are more likely to be predicted incorrectly. To capture the spatial relationship between a point cloud test and the decision boundary, we propose transforming each test (a point cloud) into a low-dimensional feature vector, toward indirectly revealing the underlying proximity between a test and the decision boundary. To achieve this, we carefully design a group of feature generation strategies, and for each test input, we generate four distinct types of features, namely spatial features, mutation features, prediction features, and uncertainty features. Through a concatenation of the four feature types, PCPrior assembles a final feature vector for each test. Subsequently, a ranking model is employed to estimate the probability of misclassification for each test based on its feature vector. Finally, PCPrior ranks all tests based on their misclassification probabilities. We conducted an extensive study based on 165 subjects to evaluate the performance of PCPrior, encompassing both natural and noisy datasets. The results demonstrate that PCPrior outperforms all of the compared test prioritization approaches, with an average improvement of 10.99\\% to 66.94\\% on natural datasets and 16.62\\% to 53\\% on noisy datasets.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "132",
        "numpages": "44",
        "keywords": "Test input prioritization, deep neural network, learning to rank, labeling"
    },
    "KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation": {
        "type": "article",
        "key": "10.1145/3643675",
        "author": "Tao, Wei and Zhou, Yucheng and Wang, Yanlin and Zhang, Hongyu and Wang, Haofen and Zhang, Wenqiang",
        "title": "KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3643675",
        "doi": "10.1145/3643675",
        "abstract": "Commit messages are natural language descriptions of code changes, which are important for software evolution such as code understanding and maintenance. However, previous methods are trained on the entire dataset without considering the fact that a portion of commit messages adhere to good practice (i.e., good-practice commits), while the rest do not. On the basis of our empirical study, we discover that training on good-practice commits significantly contributes to the commit message generation. Motivated by this finding, we propose a novel knowledge-aware denoising learning method called KADEL. Considering that good-practice commits constitute only a small proportion of the dataset, we align the remaining training samples with these good-practice commits. To achieve this, we propose a model that learns the commit knowledge by training on good-practice commits. This knowledge model enables supplementing more information for training samples that do not conform to good practice. However, since the supplementary information may contain noise or prediction errors, we propose a dynamic denoising training method. This method composes a distribution-aware confidence function and a dynamic distribution list, which enhances the effectiveness of the training process. Experimental results on the whole MCMD dataset demonstrate that our method overall achieves state-of-the-art performance compared with previous methods.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "133",
        "numpages": "32",
        "keywords": "Commit message generation, knowledge introducing, denoising training"
    },
    "Automated Mapping of Vulnerability Advisories onto their Fix Commits in Open Source Repositories": {
        "type": "article",
        "key": "10.1145/3649590",
        "author": "Hommersom, Daan and Sabetta, Antonino and Coppola, Bonaventura and Nucci, Dario Di and Tamburri, Damian A.",
        "title": "Automated Mapping of Vulnerability Advisories onto their Fix Commits in Open Source Repositories",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649590",
        "doi": "10.1145/3649590",
        "abstract": "The lack of comprehensive sources of accurate vulnerability data represents a critical obstacle to studying and understanding software vulnerabilities (and their corrections). In this article, we present an approach that combines heuristics stemming from practical experience and machine-learning (ML)\u2014specifically, natural language processing (NLP)\u2014to address this problem. Our method consists of three phases. First, we construct an advisory record object containing key information about a vulnerability that is extracted from an advisory, such as those found in the National Vulnerability Database (NVD). These advisories are expressed in natural language. Second, using heuristics, a subset of candidate fix commits is obtained from the source code repository of the affected project, by filtering out commits that can be identified as unrelated to the vulnerability at hand. Finally, for each of the remaining candidate commits, our method builds a numerical feature vector reflecting the characteristics of the commit that are relevant to predicting its match with the advisory at hand. Based on the values of these feature vectors, our method produces a ranked list of candidate fixing commits. The score attributed by the ML model to each feature is kept visible to the users, allowing them to easily interpret the predictions.We implemented our approach and we evaluated it on an open data set, built by manual curation, that comprises 2,391 known fix commits corresponding to 1,248 public vulnerability advisories. When considering the top-10 commits in the ranked results, our implementation could successfully identify at least one fix commit for up to 84.03\\% of the vulnerabilities (with a fix commit on the first position for 65.06\\% of the vulnerabilities). Our evaluation shows that our method can reduce considerably the manual effort needed to search open-source software (OSS) repositories for the commits that fix known vulnerabilities.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "134",
        "numpages": "28",
        "keywords": "Open source software, software security, common vulnerabilities and exposures (CVE), National Vulnerability Database (NVD), mining software repositories, code-level vulnerability data, machine learning applied to software security"
    },
    "Navigating the Complexity of Generative AI Adoption in Software Engineering": {
        "type": "article",
        "key": "10.1145/3652154",
        "author": "Russo, Daniel",
        "title": "Navigating the Complexity of Generative AI Adoption in Software Engineering",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3652154",
        "doi": "10.1145/3652154",
        "abstract": "This article explores the adoption of Generative Artificial Intelligence (AI) tools within the domain of software engineering, focusing on the influencing factors at the individual, technological, and social levels. We applied a convergent mixed-methods approach to offer a comprehensive understanding of AI adoption dynamics. We initially conducted a questionnaire survey with 100 software engineers, drawing upon the Technology Acceptance Model, the Diffusion of Innovation Theory, and the Social Cognitive Theory as guiding theoretical frameworks. Employing the Gioia methodology, we derived a theoretical model of AI adoption in software engineering: the Human-AI Collaboration and Adaptation Framework. This model was then validated using Partial Least Squares\u2013Structural Equation Modeling based on data from 183 software engineers. Findings indicate that at this early stage of AI integration, the compatibility of AI tools within existing development workflows predominantly drives their adoption, challenging conventional technology acceptance theories. The impact of perceived usefulness, social factors, and personal innovativeness seems less pronounced than expected. The study provides crucial insights for future AI tool design and offers a framework for developing effective organizational implementation strategies.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "135",
        "numpages": "50",
        "keywords": "Generative AI, large language models, technology adaption, empirical software engineering"
    },
    "Lessons Learned from Developing a Sustainability Awareness Framework for Software Engineering Using Design Science": {
        "type": "article",
        "key": "10.1145/3649597",
        "author": "Betz, Stefanie and Penzenstadler, Birgit and Duboc, Leticia and Chitchyan, Ruzanna and Kocak, Sedef Akinli and Brooks, Ian and Oyedeji, Shola and Porras, Jari and Seyff, Norbert and Venters, Colin C.",
        "title": "Lessons Learned from Developing a Sustainability Awareness Framework for Software Engineering Using Design Science",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649597",
        "doi": "10.1145/3649597",
        "abstract": "To foster a sustainable society within a sustainable environment, we must dramatically reshape our work and consumption activities, most of which are facilitated through software. Yet, most software engineers hardly consider the effects on the sustainability of the IT products and services they deliver. This issue is exacerbated by a lack of methods and tools for this purpose. Despite the practical need for methods and tools that explicitly support consideration of the effects that IT products and services have on the sustainability of their intended environments, such methods and tools remain largely unavailable. Thus, urgent research is needed to understand how to design such tools for the IT community properly. In this article, we describe our experience using design science to create the Sustainability Awareness Framework (SusAF), which supports software engineers in anticipating and mitigating the potential sustainability effects during system development. More specifically, we identify and present the challenges faced during this process. The challenges that we have faced and addressed in the development of the SusAF are likely to be relevant to others who aim to create methods and tools to integrate sustainability analysis into their IT products and services development. Thus, the lessons learned in SusAF development are shared for the benefit of researchers and other professionals who design tools for that end.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "136",
        "numpages": "39",
        "keywords": "IT products, IT services, sustainability analysis"
    },
    "Fairness Testing: A Comprehensive Survey and Analysis of Trends": {
        "type": "article",
        "key": "10.1145/3652155",
        "author": "Chen, Zhenpeng and Zhang, Jie M. and Hort, Max and Harman, Mark and Sarro, Federica",
        "title": "Fairness Testing: A Comprehensive Survey and Analysis of Trends",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3652155",
        "doi": "10.1145/3652155",
        "abstract": "Unfair behaviors of Machine Learning (ML) software have garnered increasing attention and concern among software engineers. To tackle this issue, extensive research has been dedicated to conducting fairness testing of ML software, and this article offers a comprehensive survey of existing studies in this field. We collect 100 papers and organize them based on the testing workflow (i.e., how to test) and testing components (i.e., what to test). Furthermore, we analyze the research focus, trends, and promising directions in the realm of fairness testing. We also identify widely adopted datasets and open-source tools for fairness testing.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "137",
        "numpages": "59",
        "keywords": "Machine learning, fairness testing, survey, analysis, trends"
    },
    "Fine-grained Coverage-based Fuzzing": {
        "type": "article",
        "key": "10.1145/3587158",
        "author": "Wu, Wei-Cheng and Nongpoh, Bernard and Nour, Marwan and Marcozzi, Micha\\\"{e}l and Bardin, S\\'{e}bastien and Hauser, Christophe",
        "title": "Fine-grained Coverage-based Fuzzing",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3587158",
        "doi": "10.1145/3587158",
        "abstract": "Fuzzing is a popular software testing method that discovers bugs by massively feeding target applications with automatically generated inputs. Many state-of-the-art fuzzers use branch coverage as a feedback metric to guide the fuzzing process. The fuzzer retains inputs for further mutation only if branch coverage is increased. However, branch coverage only provides a shallow sampling of program behaviors and hence may discard interesting inputs to mutate. This work aims to take advantage of the large body of research in defining finer-grained code coverage metrics (such as control-flow, data-flow, or mutation coverage) and to evaluate how fuzzing performance is impacted when using these metrics to select interesting inputs for mutation. We propose to make branch coverage-based fuzzers support most fine-grained coverage metrics out of the box (i.e., without changing fuzzer internals). We achieve this by making the test objectives defined by these metrics (such as conditions to activate or mutants to kill) explicit as new branches in the target program. Fuzzing such a modified target is then equivalent to fuzzing the original target, but the fuzzer will also retain inputs covering the additional metric objectives for mutation. In addition, all the fuzzer mechanisms to penetrate hard-to-cover branches will help in covering the additional metric objectives. We use this approach to evaluate the impact of supporting two fine-grained coverage metrics (multiple condition coverage and weak mutation) over the performance of two state-of-the-art fuzzers (AFL++ and QSYM) with the standard LAVA-M and MAGMA benchmarks. This evaluation suggests that our mechanism for runtime fuzzer guidance, where the fuzzed code is instrumented with additional branches, is effective and could be leveraged to encode guidance from human users or static analyzers. Our results also show that the impact of fine-grained metrics over fuzzing performance is hard to predict before fuzzing and most of the time either neutral or negative. As a consequence, we do not recommend using them to guide fuzzers, except maybe in some possibly favorable circumstances yet to be investigated, like for limited parts of the code or to complement classical fuzzing campaigns.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "138",
        "numpages": "41",
        "keywords": "Fuzzing, code coverage criteria, mutation testing"
    },
    "Fine-grained Coverage-based Fuzzing - RCR Report": {
        "type": "article",
        "key": "10.1145/3649592",
        "author": "Wu, Wei-Cheng and Nongpoh, Bernard and Nour, Marwan and Marcozzi, Micha\\\"{e}l and Bardin, S\\'{e}bastien and Hauser, Christophe",
        "title": "Fine-grained Coverage-based Fuzzing - RCR Report",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649592",
        "doi": "10.1145/3649592",
        "abstract": "This is the RCR report of the artifact for the article \u201cFine-grained Coverage-based Fuzzing.\u201d This report contains scripts and pre-build binary programs to reproduce the results presented in the main article. The artifact is released on Zenodo with DOI: 10.5281/zenodo.7275184. We claim the artifact to be available, functional, and reusable. The technology skills needed to review the artifact are knowing how to use Linux/Unix terminal and a basic understanding of Docker.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "139",
        "numpages": "4",
        "keywords": "Fuzzing, code coverage criteria, mutation testing"
    },
    "EASE: An Effort-aware Extension of Unsupervised Key Class Identification Approaches": {
        "type": "article",
        "key": "10.1145/3635714",
        "author": "Pan, Weifeng and Kessentini, Marouane and Ming, Hua and Yang, Zijiang",
        "title": "EASE: An Effort-aware Extension of Unsupervised Key Class Identification Approaches",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3635714",
        "doi": "10.1145/3635714",
        "abstract": "Key class identification approaches aim at identifying the most important classes to help developers, especially newcomers, start the software comprehension process. So far, many supervised and unsupervised approaches have been proposed; however, they have not considered the effort to comprehend classes. In this article, we identify the challenge of \u201ceffort-aware key class identification\u201d; to partially tackle it, we propose an approach, EASE, which is implemented through a modification to existing unsupervised key class identification approaches to take into consideration the effort to comprehend classes. First, EASE chooses a set of network metrics that has a wide range of applications in the existing unsupervised approaches and also possesses good discriminatory power. Second, EASE normalizes the network metric values of classes to quantify the probability of any class to be a key class and utilizes Cognitive Complexity to estimate the effort required to comprehend classes. Third, EASE proposes a metric, RKCP, to measure the relative key-class proneness of classes and further uses it to sort classes in descending order. Finally, an effort threshold is utilized, and the top-ranked classes within the threshold are identified as the cost-effective key classes. Empirical results on a set of 18 software systems show that (i) the proposed effort-aware variants perform significantly better in almost all (\u224898.33\\%) the cases, (ii) they are superior to most of the baseline approaches with only several exceptions, and (iii) they are scalable to large-scale software systems. Based on these findings, we suggest that (i) we should resort to effort-aware key class identification techniques in budget-limited scenarios; and (ii) when using different techniques, we should carefully choose the weighting mechanism to obtain the best performance.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "84",
        "numpages": "43",
        "keywords": "Key classes, network metrics, complex networks, static analysis, program comprehension"
    },
    "PACE: A Program Analysis Framework for Continuous Performance Prediction": {
        "type": "article",
        "key": "10.1145/3637230",
        "author": "Biringa, Chidera and Kul, G\\\"{o}khan",
        "title": "PACE: A Program Analysis Framework for Continuous Performance Prediction",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3637230",
        "doi": "10.1145/3637230",
        "abstract": "Software development teams establish elaborate continuous integration pipelines containing automated test cases to accelerate the development process of software. Automated tests help to verify the correctness of code modifications decreasing the response time to changing requirements. However, when the software teams do not track the performance impact of pending modifications, they may need to spend considerable time refactoring existing code. This article presents PACE, a program analysis framework that provides continuous feedback on the performance impact of pending code updates. We design performance microbenchmarks by mapping the execution time of functional test cases given a code update. We map microbenchmarks to code stylometry features and feed them to predictors for performance predictions. Our experiments achieved significant performance in predicting code performance, outperforming current state-of-the-art by 75\\% on neural-represented code stylometry features.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "85",
        "numpages": "23",
        "keywords": "Current Code State, Code Stylometry Features, Microbenchmarking"
    },
    "Assessing Effectiveness of Test Suites: What Do We Know and What Should We Do?": {
        "type": "article",
        "key": "10.1145/3635713",
        "author": "Zhang, Peng and Wang, Yang and Liu, Xutong and Lu, Zeyu and Yang, Yibiao and Li, Yanhui and Chen, Lin and Wang, Ziyuan and Sun, Chang-Ai and Yu, Xiao and Zhou, Yuming",
        "title": "Assessing Effectiveness of Test Suites: What Do We Know and What Should We Do?",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3635713",
        "doi": "10.1145/3635713",
        "abstract": "Background. Software testing is a critical activity for ensuring the quality and reliability of software systems. To evaluate the effectiveness of different test suites, researchers have developed a variety of metrics. Problem. However, comparing these metrics is challenging due to the lack of a standardized evaluation framework including comprehensive factors. As a result, researchers often focus on single factors (e.g., size), which finally leads to different or even contradictory conclusions. After comparing dozens of pieces of work in detail, we have found two main problems most troubling to our community: (1) researchers tend to oversimplify the description of the ground truth they use, and (2) data involving real defects is not suitable for analysis using traditional statistical indicators. Objective. We aim at scrutinizing the whole process of comparing test suites for our community. Method. To hit this aim, we propose a framework ASSENT (evAluating teSt Suite EffectiveNess meTrics) to guide the follow-up research for evaluating a test suite effectiveness metric. ASSENT consists of three fundamental components: ground truth, benchmark test suites, and agreement indicator. Its functioning is as follows: first, users clarify the ground truth for determining the real order in effectiveness among test suites. Second, users generate a set of benchmark test suites and derive their ground truth order in effectiveness. Third, users use the metric to derive the order in effectiveness for the same test suites. Finally, users calculate the agreement indicator between the two orders derived by two metrics. Result. With ASSENT, we are able to compare the accuracy of different test suite effectiveness metrics. We apply ASSENT to evaluate representative test suite effectiveness metrics, including mutation score and code coverage metrics. Our results show that, based on the real faults, mutation score, and subsuming mutation score are the best metrics to quantify test suite effectiveness. Meanwhile, by using mutants instead of real faults, test effectiveness will be overestimated by more than 20\\% in values. Conclusion. We recommend that the standardized evaluation framework ASSENT should be used for evaluating and comparing test effectiveness metrics in the future work.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "86",
        "numpages": "32",
        "keywords": "Test suite effectiveness, coverage testing, mutation testing, order preservation, statistical indicators"
    },
    "Using Voice and Biofeedback to Predict User Engagement during Product Feedback Interviews": {
        "type": "article",
        "key": "10.1145/3635712",
        "author": "Ferrari, Alessio and Huichapa, Thaide and Spoletini, Paola and Novielli, Nicole and Fucci, Davide and Girardi, Daniela",
        "title": "Using Voice and Biofeedback to Predict User Engagement during Product Feedback Interviews",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3635712",
        "doi": "10.1145/3635712",
        "abstract": "Capturing users\u2019 engagement is crucial for gathering feedback about the features of a software product. In a market-driven context, current approaches to collecting and analyzing users\u2019 feedback are based on techniques leveraging information extracted from product reviews and social media. These approaches are hardly applicable in contexts where online feedback is limited, as for the majority of apps, and software in general. In such cases, companies need to resort to face-to-face interviews to get feedback on their products. In this article, we propose to utilize biometric data, in terms of physiological and voice features, to complement product feedback interviews with information about the engagement of the user on product-relevant topics. We evaluate our approach by interviewing users while gathering their physiological data (i.e., biofeedback) using an Empatica E4 wristband, and capturing their voice through the default audio-recorder of a common laptop. Our results show that we can predict users\u2019 engagement by training supervised machine learning algorithms on biofeedback and voice data, and that voice features alone can be sufficiently effective. The best configurations evaluated achieve an average F1 \u223c 70\\% in terms of classification performance, and use voice features only. This work is one of the first studies in requirements engineering in which biometrics are used to identify emotions. Furthermore, this is one of the first studies in software engineering that considers voice analysis. The usage of voice features can be particularly helpful for emotion-aware feedback collection in remote communication, either performed by human analysts or voice-based chatbots, and can also be exploited to support the analysis of meetings in software engineering research.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "87",
        "numpages": "36",
        "keywords": "Software engineering, requirements engineering, emotion detection, voice analysis, speech analysis, biofeedback analysis, affective requirements engineering"
    },
    "RE Methods for Virtual Reality Software Product Development: A Mapping Study": {
        "type": "article",
        "key": "10.1145/3649595",
        "author": "Karre, Sai Anirudh and Reddy, Y. Raghu and Mittal, Raghav",
        "title": "RE Methods for Virtual Reality Software Product Development: A Mapping Study",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649595",
        "doi": "10.1145/3649595",
        "abstract": "Software practitioners use various methods in Requirements Engineering (RE) to elicit, analyze, and specify the requirements of enterprise products. The methods impact the final product characteristics and influence product delivery. Ad-hoc usage of the methods by software practitioners can lead to inconsistency and ambiguity in the product. With the notable rise in enterprise products, games, and so forth across various domains, Virtual Reality (VR) has become an essential technology for the future. The methods adopted for RE for developing VR products requires a detailed study. This article presents a mapping study on RE methods prescribed and used for developing VR applications including requirements elicitation, requirements analysis, and requirements specification. Our study provides insights into the use of such methods in the VR community and suggests using specific RE methods in various fields of interest. We also discuss future directions in RE for VR products.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "88",
        "numpages": "31",
        "keywords": "Software requirements, requirements elicitation, virtual reality, industrial practices"
    },
    "A Smart Status Based Monitoring Algorithm&nbsp;for the Dynamic Analysis of Memory Safety": {
        "type": "article",
        "key": "10.1145/3637227",
        "author": "Chen, Zhe and Yan, Rui and Ma, Yingzi and Sui, Yulei and Xue, Jingling",
        "title": "A Smart Status Based Monitoring Algorithm&nbsp;for the Dynamic Analysis of Memory Safety",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3637227",
        "doi": "10.1145/3637227",
        "abstract": "C is a dominant programming language for implementing system and low-level embedded software. Unfortunately, the unsafe nature of its low-level control of memory often leads to memory errors. Dynamic analysis has been widely used to detect memory errors at runtime. However, existing monitoring algorithms for dynamic analysis are not yet satisfactory, as they cannot deterministically and completely detect some types of errors, such as segment confusion errors, sub-object overflows, use-after-frees and memory leaks.We propose a new monitoring algorithm, namely Smatus, short for smart status, that improves memory safety by performing comprehensive dynamic analysis. The key innovation is to maintain at runtime a small status node for each memory object. A status node records the status value and reference count of an object, where the status value denotes the liveness and segment type of this object, and the reference count tracks the number of pointer variables pointing to this object. Smatus maintains at runtime a pointer metadata for each pointer variable, to record not only the base and bound of a pointer\u2019s referent but also the address of the referent\u2019s status node. All the pointers pointing to the same referent share the same status node in their pointer metadata. A status node is smart in the sense that it is automatically deleted when it becomes useless (indicated by its reference count reaching zero). To the best of our knowledge, Smatus represents the most comprehensive approach of its kind.We have evaluated Smatus by using a large set of programs including the NIST Software Assurance Reference Dataset, MSBench, MiBench, SPEC and stress testing benchmarks. In terms of effectiveness (detecting different types of memory errors), Smatus outperforms state-of-the-art tools, Google\u2019s AddressSanitizer, SoftBoundCETS and Valgrind, as it is capable of detecting more errors. In terms of performance (the time and memory overheads), Smatus outperforms SoftBoundCETS and Valgrind in terms of both lower time and memory overheads incurred, and is on par with AddressSanitizer in terms of the time and memory overhead tradeoff made (with much lower memory overheads incurred).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "89",
        "numpages": "47",
        "keywords": "Software quality, software testing, dynamic analysis, bug finding, memory errors, monitoring algorithm"
    },
    "Measuring and Clustering Heterogeneous Chatbot Designs": {
        "type": "article",
        "key": "10.1145/3637228",
        "author": "Ca\\~{n}izares, Pablo C. and L\\'{o}pez-Morales, Jose Mar\\'{\\i}a and P\\'{e}rez-Soler, Sara and Guerra, Esther and de Lara, Juan",
        "title": "Measuring and Clustering Heterogeneous Chatbot Designs",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3637228",
        "doi": "10.1145/3637228",
        "abstract": "Conversational agents, or chatbots, have become popular to access all kind of software services. They provide an intuitive natural language interface for interaction, available from a wide range of channels including social networks, web pages, intelligent speakers or cars. In response to this demand, many chatbot development platforms and tools have emerged. However, they typically lack support to statically measure properties of the chatbots being built, as indicators of their size, complexity, quality or usability. Similarly, there are hardly any mechanisms to compare and cluster chatbots developed with heterogeneous technologies.To overcome this limitation, we propose a suite of 21 metrics for chatbot designs, as well as two clustering methods that help in grouping chatbots along their conversation topics and design features. Both the metrics and the clustering methods are defined on a neutral chatbot design language, becoming independent of the implementation platform. We provide automatic translations of chatbots defined on some major platforms into this neutral notation to perform the measurement and clustering. The approach is supported by our tool Asymob, which we have used to evaluate the metrics and the clustering methods over a set of 259 Dialogflow and Rasa chatbots from open-source repositories. The results open the door to incorporating the metrics within chatbot development processes for the early detection of quality issues, and to exploit clustering to organise large collections of chatbots into significant groups to ease chatbot comprehension, search and comparison.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "90",
        "numpages": "43",
        "keywords": "Chatbot design, metrics, clustering, quality assurance, model-driven engineering"
    },
    "Building Domain-Specific Machine Learning Workflows: A Conceptual Framework for the State of the Practice": {
        "type": "article",
        "key": "10.1145/3638243",
        "author": "Oakes, Bentley James and Famelis, Michalis and Sahraoui, Houari",
        "title": "Building Domain-Specific Machine Learning Workflows: A Conceptual Framework for the State of the Practice",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3638243",
        "doi": "10.1145/3638243",
        "abstract": "Domain experts are increasingly employing machine learning to solve their domain-specific problems. This article presents to software engineering researchers the six key challenges that a domain expert faces in addressing their problem with a computational workflow, and the underlying executable implementation. These challenges arise out of our conceptual framework which presents the \u201croute\u201d of transformations that a domain expert may choose to take while developing their solution.To ground our conceptual framework in the state of the practice, this article discusses a selection of available textual and graphical workflow systems and their support for the transformations described in our framework. Example studies from the literature in various domains are also examined to highlight the tools used by the domain experts as well as a classification of the domain specificity and machine learning usage of their problem, workflow, and implementation.The state of the practice informs our discussion of the six key challenges, where we identify which challenges and transformations are not sufficiently addressed by available tools. We also suggest possible research directions for software engineering researchers to increase the automation of these tools and disseminate best-practice techniques between software engineering and various scientific domains.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "91",
        "numpages": "50",
        "keywords": "Computational workflow, workflow composition, domain experts, machine learning, machine learning pipelines, software engineering framework"
    },
    "Exploring Semantic Redundancy using Backdoor Triggers: A Complementary Insight into the Challenges Facing DNN-based Software Vulnerability Detection": {
        "type": "article",
        "key": "10.1145/3640333",
        "author": "Shao, Changjie and Li, Gaolei and Wu, Jun and Zheng, Xi",
        "title": "Exploring Semantic Redundancy using Backdoor Triggers: A Complementary Insight into the Challenges Facing DNN-based Software Vulnerability Detection",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3640333",
        "doi": "10.1145/3640333",
        "abstract": "To detect software vulnerabilities with better performance, deep neural networks (DNNs) have received extensive attention recently. However, these vulnerability detection DNN models trained with code representations are vulnerable to specific perturbations on code representations. This motivates us to rethink the bane of software vulnerability detection and find function-agnostic features during code representation which we name as semantic redundant features. This paper first identifies a tight correlation between function-agnostic triggers and semantic redundant feature space (where the redundant features reside) in these DNN models. For correlation identification, we propose a novel Backdoor-based Semantic Redundancy Exploration (BSemRE) framework. In BSemRE, the sensitivity of the trained models to function-agnostic triggers is observed to verify the existence of semantic redundancy in various code representations. Specifically, acting as the typical manifestations of semantic redundancy, naming conventions, ternary operators and identically-true conditions are exploited to generate function-agnostic triggers. Extensive comparative experiments on 1,613,823 samples of eight representative vulnerability datasets and state-of-the-art code representation techniques and vulnerability detection models demonstrate that the existence of semantic redundancy determines the upper trustworthiness limit of DNN-based software vulnerability detection. To the best of our knowledge, this is the first work exploring the bane of software vulnerability detection using backdoor triggers.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "92",
        "numpages": "28",
        "keywords": "Software vulnerability detection, deep neural networks, backdoor triggers, semantic redundancy, function-agnostic"
    },
    "Test Generation Strategies for Building Failure Models and Explaining Spurious Failures": {
        "type": "article",
        "key": "10.1145/3638246",
        "author": "Jodat, Baharin A. and Chandar, Abhishek and Nejati, Shiva and Sabetzadeh, Mehrdad",
        "title": "Test Generation Strategies for Building Failure Models and Explaining Spurious Failures",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3638246",
        "doi": "10.1145/3638246",
        "abstract": "Test inputs fail not only when the system under test is faulty but also when the inputs are invalid or unrealistic. Failures resulting from invalid or unrealistic test inputs are spurious. Avoiding spurious failures improves the effectiveness of testing in exercising the main functions of a system, particularly for compute-intensive (CI) systems where a single test execution takes significant time. In this article, we propose to build failure models for inferring interpretable rules on test inputs that cause spurious failures. We examine two alternative strategies for building failure models: (1)&nbsp;machine learning (ML)-guided test generation and (2)&nbsp;surrogate-assisted test generation. ML-guided test generation infers boundary regions that separate passing and failing test inputs and samples test inputs from those regions. Surrogate-assisted test generation relies on surrogate models to predict labels for test inputs instead of exercising all the inputs. We propose a novel surrogate-assisted algorithm that uses multiple surrogate models simultaneously, and dynamically selects the prediction from the most accurate model. We empirically evaluate the accuracy of failure models inferred based on surrogate-assisted and ML-guided test generation algorithms. Using case studies from the domains of cyber-physical systems and networks, we show that our proposed surrogate-assisted approach generates failure models with an average accuracy of 83\\%, significantly outperforming ML-guided test generation and two baselines. Further, our approach learns failure-inducing rules that identify genuine spurious failures as validated against domain knowledge.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "93",
        "numpages": "32",
        "keywords": "Search-based testing, machine learning, surrogate models, failure models, test-input validity, and spurious failures"
    },
    "Identifying and Explaining Safety-critical Scenarios for Autonomous Vehicles via Key Features": {
        "type": "article",
        "key": "10.1145/3640335",
        "author": "Neelofar, Neelofar and Aleti, Aldeida",
        "title": "Identifying and Explaining Safety-critical Scenarios for Autonomous Vehicles via Key Features",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3640335",
        "doi": "10.1145/3640335",
        "abstract": "Ensuring the safety of autonomous vehicles (AVs) is of utmost importance, and testing them in simulated environments is a safer option than conducting in-field operational tests. However, generating an exhaustive test suite to identify critical test scenarios is computationally expensive, as the representation of each test is complex and contains various dynamic and static features, such as the AV under test, road participants (vehicles, pedestrians, and static obstacles), environmental factors (weather and light), and the road\u2019s structural features (lanes, turns, road speed, etc.). In this article, we present a systematic technique that uses Instance Space Analysis (ISA) to identify the significant features of test scenarios that affect their ability to reveal the unsafe behaviour of AVs. ISA identifies the features that best differentiate safety-critical scenarios from normal driving and visualises the impact of these features on test scenario outcomes (safe/unsafe) in two dimensions. This visualisation helps to identify untested regions of the instance space and provides an indicator of the quality of the test suite in terms of the percentage of feature space covered by testing. To test the predictive ability of the identified features, we train five Machine Learning classifiers to classify test scenarios as safe or unsafe. The high precision, recall, and F1 scores indicate that our proposed approach is effective in predicting the outcome of a test scenario without executing it and can be used for test generation, selection, and prioritisation.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "94",
        "numpages": "32",
        "keywords": "Testing autonomous vehicles, feature-impact analysis, instance space analysis, search-based software testing"
    },
    "Rigorous Assessment of Model Inference Accuracy using Language Cardinality": {
        "type": "article",
        "key": "10.1145/3640332",
        "author": "Clun, Donato and Shin, Donghwan and Filieri, Antonio and Bianculli, Domenico",
        "title": "Rigorous Assessment of Model Inference Accuracy using Language Cardinality",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3640332",
        "doi": "10.1145/3640332",
        "abstract": "Models such as finite state automata are widely used to abstract the behavior of software systems by capturing the sequences of events observable during their execution. Nevertheless, models rarely exist in practice and, when they do, get easily outdated; moreover, manually building and maintaining models is costly and error-prone. As a result, a variety of model inference methods that automatically construct models from execution traces have been proposed to address these issues.However, performing a systematic and reliable accuracy assessment of inferred models remains an open problem. Even when a reference model is given, most existing model accuracy assessment methods may return misleading and biased results. This is mainly due to their reliance on statistical estimators over a finite number of randomly generated traces, introducing avoidable uncertainty about the estimation and being sensitive to the parameters of the random trace generative process.This article addresses this problem by developing a systematic approach based on analytic combinatorics that minimizes bias and uncertainty in model accuracy assessment by replacing statistical estimation with deterministic accuracy measures. We experimentally demonstrate the consistency and applicability of our approach by assessing the accuracy of models inferred by state-of-the-art inference tools against reference models from established specification mining benchmarks.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "95",
        "numpages": "39",
        "keywords": "Model inference, specification mining, process mining, model assessment, formal specifications, machine learning, software engineering, behavioral comparison, conformance checking, precision, recall"
    },
    "ARCTURUS: Full Coverage Binary Similarity Analysis with Reachability-guided Emulation": {
        "type": "article",
        "key": "10.1145/3640337",
        "author": "Zhou, Anshunkang and Hu, Yikun and Xu, Xiangzhe and Zhang, Charles",
        "title": "ARCTURUS: Full Coverage Binary Similarity Analysis with Reachability-guided Emulation",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3640337",
        "doi": "10.1145/3640337",
        "abstract": "Binary code similarity analysis is extremely useful, since it provides rich information about an unknown binary, such as revealing its functionality and identifying reused libraries. Robust binary similarity analysis is challenging, as heavy compiler optimizations can make semantically similar binaries have gigantic syntactic differences. Unfortunately, existing semantic-based methods still suffer from either incomplete coverage or low accuracy.In this article, we propose ARCTURUS, a new technique that can achieve high code coverage and high accuracy simultaneously by manipulating program execution under the guidance of code reachability. Our key insight is that the compiler must preserve program semantics&nbsp;(e.g., dependences between code fragments) during compilation; therefore, the code reachability, which implies the interdependence between code, is invariant across code transformations. Based on the above insight, our key idea is to leverage the stability of code reachability to manipulate the program execution such that deep code logic can also be covered in a consistent way. Experimental results show that ARCTURUS achieves an average precision of 87.8\\% with 100\\% block coverage, outperforming compared methods by 38.4\\%, on average. ARCTURUS takes only 0.15 second to process one function, on average, indicating that it is efficient for practical use.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "96",
        "numpages": "31",
        "keywords": "Binary code similarity, emulation, reverse engineering, program analysis"
    },
    "Characterizing Deep Learning Package Supply Chains in PyPI: Domains, Clusters, and Disengagement": {
        "type": "article",
        "key": "10.1145/3640336",
        "author": "Gao, Kai and He, Runzhi and Xie, Bing and Zhou, Minghui",
        "title": "Characterizing Deep Learning Package Supply Chains in PyPI: Domains, Clusters, and Disengagement",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3640336",
        "doi": "10.1145/3640336",
        "abstract": "Deep learning (DL) frameworks have become the cornerstone of the rapidly developing DL field. Through installation dependencies specified in the distribution metadata, numerous packages directly or transitively depend on DL frameworks, layer after layer, forming DL package supply chains (SCs), which are critical for DL frameworks to remain competitive. However, vital knowledge on how to nurture and sustain DL package SCs is still lacking. Achieving this knowledge may help DL frameworks formulate effective measures to strengthen their SCs to remain competitive and shed light on dependency issues and practices in the DL SC for researchers and practitioners. In this paper, we explore the domains, clusters, and disengagement of packages in two representative PyPI DL package SCs to bridge this knowledge gap. We analyze the metadata of nearly six million PyPI package distributions and construct version-sensitive SCs for two popular DL frameworks: TensorFlow and PyTorch. We find that popular packages (measured by the number of monthly downloads) in the two SCs cover 34 domains belonging to eight categories. Applications, Infrastructure, and Sciences categories account for over 85\\% of popular packages in either SC and TensorFlow and PyTorch SC have developed specializations on Infrastructure and Applications packages, respectively. We employ the Leiden community detection algorithm and detect 131 and 100 clusters in the two SCs. The clusters mainly exhibit four shapes: Arrow, Star, Tree, and Forest with increasing dependency complexity. Most clusters are Arrow or Star, while Tree and Forest clusters account for most packages (Tensorflow SC: 70.7\\%, PyTorch SC: 92.9\\%). We identify three groups of reasons why packages disengage from the SC (i.e., remove the DL framework and its dependents from their installation dependencies): dependency issues, functional improvements, and ease of installation. The most common reason in TensorFlow SC is dependency incompatibility and in PyTorch SC is to simplify functionalities and reduce installation size. Our study provides rich implications for DL framework vendors, researchers, and practitioners on the maintenance and dependency management practices of PyPI DL SCs.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "97",
        "numpages": "27",
        "keywords": "Software supply chain, PyPI ecosystem, deep learning, software structure and evolution"
    },
    "Method-level Bug Prediction: Problems and Promises": {
        "type": "article",
        "key": "10.1145/3640331",
        "author": "Chowdhury, Shaiful and Uddin, Gias and Hemmati, Hadi and Holmes, Reid",
        "title": "Method-level Bug Prediction: Problems and Promises",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3640331",
        "doi": "10.1145/3640331",
        "abstract": "Fixing software bugs can be colossally expensive, especially if they are discovered in the later phases of the software development life cycle. As such, bug prediction has been a classic problem for the research community. As of now, the Google Scholar site generates \u223c113,000 hits if searched with the \u201cbug prediction\u201d phrase. Despite this staggering effort by the research community, bug prediction research is criticized for not being decisively adopted in practice. A significant problem of the existing research is the granularity level (i.e., class/file level) at which bug prediction is historically studied. Practitioners find it difficult and time-consuming to locate bugs at the class/file level granularity. Consequently, method-level bug prediction has become popular in the past decade. We ask, are these method-level bug prediction models ready for industry use? Unfortunately, the answer is no. The reported high accuracies of these models dwindle significantly if we evaluate them in different realistic time-sensitive contexts. It may seem hopeless at first, but, encouragingly, we show that future method-level bug prediction can be improved significantly. In general, we show how to reliably evaluate future method-level bug prediction models and how to improve them by focusing on four different improvement avenues: building noise-free bug data, addressing concept drift, selecting similar training projects, and developing a mixture of models. Our findings are based on three publicly available method-level bug datasets and a newly built bug dataset of 774,051 Java methods originating from 49 open-source software projects.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "98",
        "numpages": "31",
        "keywords": "Method-level bug prediction, code metrics, maintenance, McCabe, code complexity"
    },
    "Industry Practices for Challenging Autonomous Driving Systems with Critical Scenarios": {
        "type": "article",
        "key": "10.1145/3640334",
        "author": "Song, Qunying and Engstr\\\"{o}m, Emelie and Runeson, Per",
        "title": "Industry Practices for Challenging Autonomous Driving Systems with Critical Scenarios",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3640334",
        "doi": "10.1145/3640334",
        "abstract": "Testing autonomous driving systems for safety and reliability is essential, yet complex. A primary challenge is identifying relevant test scenarios, especially the critical ones that may expose hazards or harm to autonomous vehicles and other road users. Although numerous approaches and tools for critical scenario identification are proposed, the industry practices for selection, implementation, and evaluation of approaches, are not well understood. Therefore, we aim at exploring practical aspects of how autonomous driving systems are tested, particularly the identification and use of critical scenarios. We interviewed 13 practitioners from 7 companies in autonomous driving in Sweden. We used thematic modeling to analyse and synthesize the interview data. As a result, we present 9 themes of practices and 4 themes of challenges related to critical scenarios. Our analysis indicates there is little joint effort in the industry, despite every approach has its own limitations, and tools and platforms are lacking. To that end, we recommend the industry and academia combine different approaches, collaborate among different stakeholders, and continuously learn the field. The contributions of our study are exploration and synthesis of industry practices and related challenges for critical scenario identification and testing, and potential increase of industry relevance for future studies.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "99",
        "numpages": "35",
        "keywords": "Critical scenario identification, testing, autonomous driving systems, industry practices, challenges, interview"
    },
    "Compiler Autotuning through Multiple-phase Learning": {
        "type": "article",
        "key": "10.1145/3640330",
        "author": "Zhu, Mingxuan and Hao, Dan and Chen, Junjie",
        "title": "Compiler Autotuning through Multiple-phase Learning",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3640330",
        "doi": "10.1145/3640330",
        "abstract": "Widely used compilers like GCC and LLVM usually have hundreds of optimizations controlled by optimization flags, which are enabled or disabled during compilation to improve the runtime performance (e.g., small execution time) of the compiler program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to manually tune compiler optimization flags. In the literature, a number of autotuning techniques have been proposed, which tune optimization flags for a compiled program by comparing its actual runtime performance with different optimization flag combinations. Due to the huge search space and heavy actual runtime cost, these techniques suffer from the widely recognized efficiency problem. To reduce the heavy runtime cost, in this article we propose a lightweight learning approach that uses a small number of actual runtime performance data to predict the runtime performance of a compiled program with various optimization flag combinations. Furthermore, to reduce the search space, we design a novel particle swarm algorithm that tunes compiler optimization flags with the prediction model. To evaluate the performance of the proposed approach, CompTuner, we conduct an extensive experimental study on two popular C compilers, GCC and LLVM, with two widely used benchmarks, cBench and PolyBench. The experimental results show that CompTuner&nbsp;significantly outperforms the six compared techniques, including the state-of-the-art technique BOCA.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "100",
        "numpages": "38",
        "keywords": "Compiler, compiler autotuning, multiple phase learning, particle swarm optimization"
    },
    "Bug Analysis in Jupyter Notebook Projects: An Empirical Study": {
        "type": "article",
        "key": "10.1145/3641539",
        "author": "De Santana, Taijara Loiola and Neto, Paulo Anselmo Da Mota Silveira and De Almeida, Eduardo Santana and Ahmed, Iftekhar",
        "title": "Bug Analysis in Jupyter Notebook Projects: An Empirical Study",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3641539",
        "doi": "10.1145/3641539",
        "abstract": "Computational notebooks, such as Jupyter, have been widely adopted by data scientists to write code for analyzing and visualizing data. Despite their growing adoption and popularity, few studies have been found to understand Jupyter development challenges from the practitioners\u2019 point of view. This article presents a systematic study of bugs and challenges that Jupyter practitioners face through a large-scale empirical investigation. We mined 14,740 commits from 105 GitHub open source projects with Jupyter Notebook code. Next, we analyzed 30,416 StackOverflow posts, which gave us insights into bugs that practitioners face when developing Jupyter Notebook projects. Next, we conducted 19 interviews with data scientists to uncover more details about Jupyter bugs and to gain insight into Jupyter developers\u2019 challenges. Finally, to validate the study results and proposed taxonomy, we conducted a survey with 91 data scientists. We highlight bug categories, their root causes, and the challenges that Jupyter practitioners face.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "101",
        "numpages": "34",
        "keywords": "Jupyter Notebooks, bugs, interviews, mining software repositories (MSR), StackOverflow, empirical study"
    },
    "Mapping APIs in Dynamic-typed Programs by Leveraging Transfer Learning": {
        "type": "article",
        "key": "10.1145/3641848",
        "author": "Huang, Zhenfei and Chen, Junjie and Jiang, Jiajun and Liang, Yihua and You, Hanmo and Li, Fengjie",
        "title": "Mapping APIs in Dynamic-typed Programs by Leveraging Transfer Learning",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3641848",
        "doi": "10.1145/3641848",
        "abstract": "Application Programming Interface (API) migration is a common task for adapting software across different programming languages and platforms, where manually constructing the mapping relations between APIs is indeed time-consuming and error-prone. To facilitate this process, many automated API mapping approaches have been proposed. However, existing approaches were mainly designed and evaluated for mapping APIs of statically-typed languages, while their performance on dynamically-typed languages remains unexplored.In this article, we conduct the first extensive study to explore existing API mapping approaches\u2019 performance for mapping APIs in dynamically-typed languages, for which we have manually constructed a high-quality dataset. According to the empirical results, we have summarized several insights. In particular, the source code implementations of APIs can significantly improve the effectiveness of API mapping. However, due to the confidentiality policy, they may not be available in practice. To overcome this, we propose a novel API mapping approach, named Matl, which leverages the transfer learning technique to learn the semantic embeddings of source code implementations from large-scale open-source repositories and then transfers the learned model to facilitate the mapping of APIs. In this way, Matl can produce more accurate API embedding of its functionality for more effective mapping without knowing the source code of the APIs. To evaluate the performance of Matl, we have conducted an extensive study by comparing Matl with state-of-the-art approaches. The results demonstrate that Matl is indeed effective as it improves the state-of-the-art approach by at least 18.36\\% for mapping APIs of dynamically-typed language and by 30.77\\% for mapping APIs of the statically-typed language.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "102",
        "numpages": "29",
        "keywords": "API mapping, program transformation, transfer learning"
    },
    "Deceiving Humans and Machines Alike: Search-based Test Input Generation for DNNs Using Variational Autoencoders": {
        "type": "article",
        "key": "10.1145/3635706",
        "author": "Kang, Sungmin and Feldt, Robert and Yoo, Shin",
        "title": "Deceiving Humans and Machines Alike: Search-based Test Input Generation for DNNs Using Variational Autoencoders",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3635706",
        "doi": "10.1145/3635706",
        "abstract": "Due to the rapid adoption of Deep Neural Networks (DNNs) into larger software systems, testing of DNN-based systems has received much attention recently. While many different test adequacy criteria have been suggested, we lack effective test input generation techniques. Inputs such as images of real-world objects and scenes are not only expensive to collect but also difficult to randomly sample. Consequently, current testing techniques for DNNs tend to apply small local perturbations to existing inputs to generate new inputs. We propose SINVAD (Search-based Input space Navigation using Variational AutoencoDers), a way to sample from, and navigate over, a space of realistic inputs that resembles the true distribution in the training data. Our input space is constructed using Variational Autoencoders (VAEs), and navigated through their latent vector space. Our analysis shows that the VAE-based input space is well-aligned with human perception of what constitutes realistic inputs. Further, we show that this space can be effectively searched to achieve various testing scenarios, such as boundary testing of two different DNNs or analyzing class labels that are difficult for the given DNN to distinguish. Guidelines on how to design VAE architectures are presented as well. Our results have the potential to open the field to meaningful exploration through the space of highly structured images.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "103",
        "numpages": "24",
        "keywords": "Test data generation, deep neural network, search-based software engineering"
    },
    "Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep Learning Projects": {
        "type": "article",
        "key": "10.1145/3638245",
        "author": "Wang, Han and Yu, Sijia and Chen, Chunyang and Turhan, Burak and Zhu, Xiaodong",
        "title": "Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep Learning Projects",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3638245",
        "doi": "10.1145/3638245",
        "abstract": "Deep Learning (DL) models have rapidly advanced, focusing on achieving high performance through testing model accuracy and robustness. However, it is unclear whether DL projects, as software systems, are tested thoroughly or functionally correct when there is a need to treat and test them like other software systems. Therefore, we empirically study the unit tests in open-source DL projects, analyzing 9,129 projects from GitHub. We find that: (1) unit tested DL projects have positive correlation with the open-source project metrics and have a higher acceptance rate of pull requests; (2) 68\\% of the sampled DL projects are not unit tested at all; (3) the layer and utilities (utils) of DL models have the most unit tests. Based on these findings and previous research outcomes, we built a mapping taxonomy between unit tests and faults in DL projects. We discuss the implications of our findings for developers and researchers and highlight the need for unit testing in open-source DL projects to ensure their reliability and stability. The study contributes to this community by raising awareness of the importance of unit testing in DL projects and encouraging further research in this area.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "104",
        "numpages": "22",
        "keywords": "Deep learning, unit testing"
    },
    "Estimating Uncertainty in Labeled Changes by SZZ Tools on Just-In-Time Defect Prediction": {
        "type": "article",
        "key": "10.1145/3637226",
        "author": "Guo, Shikai and Li, Dongmin and Huang, Lin and Lv, Sijia and Chen, Rong and Li, Hui and Li, Xiaochen and Jiang, He",
        "title": "Estimating Uncertainty in Labeled Changes by SZZ Tools on Just-In-Time Defect Prediction",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3637226",
        "doi": "10.1145/3637226",
        "abstract": "The aim of Just-In-Time (JIT) defect prediction is to predict software changes that are prone to defects in a project in a timely manner, thereby improving the efficiency of software development and ensuring software quality. Identifying changes that introduce bugs is a critical task in just-in-time defect prediction, and researchers have introduced the SZZ approach and its variants to label these changes. However, it has been shown that different SZZ algorithms introduce noise to the dataset to a certain extent, which may reduce the predictive performance of the model. To address this limitation, we propose the Confident Learning Imbalance (CLI) model. The model identifies and excludes samples whose labels may be corrupted by estimating the joint distribution of noisy labels and true labels, and mitigates the impact of noisy data on the performance of the prediction model. The CLI consists of two components: identifying noisy data (Confident Learning Component) and generating a predicted probability matrix for imbalanced data (Imbalanced Data Probabilistic Prediction Component). The IDPP component generates precise predicted probabilities for each instance in the training set, while the CL component uses the generated predicted probability matrix and noise labels to clean up the noise and build a classification model. We evaluate the performance of our model through extensive experiments on a total of 126,526 changes from ten Apache open source projects, and the results show that our model outperforms the baseline methods.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "105",
        "numpages": "25",
        "keywords": "Just-in-time defect prediction, SZZ tools, confident learning, imbalance"
    },
    "Smart Contract Code Repair Recommendation based on Reinforcement Learning and Multi-metric Optimization": {
        "type": "article",
        "key": "10.1145/3637229",
        "author": "Guo, Hanyang and Chen, Yingye and Chen, Xiangping and Huang, Yuan and Zheng, Zibin",
        "title": "Smart Contract Code Repair Recommendation based on Reinforcement Learning and Multi-metric Optimization",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3637229",
        "doi": "10.1145/3637229",
        "abstract": "A smart contract is a kind of code deployed on the blockchain that executes automatically once an event triggers a clause in the contract. Since smart contracts involve businesses such as asset transfer, they are more vulnerable to attacks, so it is crucial to ensure the security of smart contracts. Because a smart contract cannot be tampered with once deployed on the blockchain, for smart contract developers, it is necessary to fix vulnerabilities before deployment. Compared with many vulnerability detection tools for smart contracts, the amount of automatic fix approaches for smart contracts is relatively limited. These approaches mainly use defined pattern-based methods or heuristic search algorithms for vulnerability repairs. In this article, we propose RLRep, a reinforcement learning-based approach to provide smart contract repair recommendations for smart contract developers automatically. This approach adopts an agent to provide repair action suggestions based on the vulnerable smart contract without any supervision, which can solve the problem of missing labeled data in machine learning-based repair methods. We evaluate our approach on a dataset containing 853 smart contract programs (programming language: Solidity) with different kinds of vulnerabilities. We split them into training and test sets. The result shows that our approach can provide 54.97\\% correct repair recommendations for smart contracts.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "106",
        "numpages": "31",
        "keywords": "Repair recommendation, smart contract"
    },
    "Mitigating Debugger-based Attacks to Java Applications with Self-debugging": {
        "type": "article",
        "key": "10.1145/3631971",
        "author": "Pizzolotto, Davide and Berlato, Stefano and Ceccato, Mariano",
        "title": "Mitigating Debugger-based Attacks to Java Applications with Self-debugging",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631971",
        "doi": "10.1145/3631971",
        "abstract": "Java bytecode is a quite high-level language and, as such, it is fairly easy to analyze and decompile with malicious intents, e.g., to tamper with code and skip license checks. Code obfuscation was a first attempt to mitigate malicious reverse-engineering based on static analysis. However, obfuscated code can still be dynamically analyzed with standard debuggers to perform step-wise execution and to inspect (or change) memory content at important execution points, e.g., to alter the verdict of license validity checks. Although some approaches have been proposed to mitigate debugger-based attacks, they are only applicable to binary compiled code and none address the challenge of protecting Java bytecode.In this article, we propose a novel approach to protect Java bytecode from malicious debugging. Our approach is based on automated program transformation to manipulate Java bytecode and split it into two binary processes that debug each other (i.e., a self-debugging solution). In fact, when the debugging interface is already engaged, an additional malicious debugger cannot attach. To be resilient against typical attacks, our approach adopts a series of technical solutions, e.g., an encoded channel is shared by the two processes to avoid leaking information, an authentication protocol is established to avoid Man-in-the-middle attacks, and the computation is spread between the two processes to prevent the attacker to replace or terminate either of them.We test our solution on 18 real-world Java applications, showing that our approach can effectively block the most common debugging tasks (either with the Java debugger or the GNU debugger) while preserving the functional correctness of the protected programs. While the final decision on when to activate this protection is still up to the developers, the observed performance overhead was acceptable for common desktop application domains.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "107",
        "numpages": "38",
        "keywords": "Anti-debugging, maliciuos reverse engineering, tampering attacks, man at the end attacks"
    },
    "Battling against Protocol Fuzzing: Protecting Networked Embedded Devices from Dynamic Fuzzers": {
        "type": "article",
        "key": "10.1145/3641847",
        "author": "Liu, Puzhuo and Zheng, Yaowen and Sun, Chengnian and Li, Hong and Li, Zhi and Sun, Limin",
        "title": "Battling against Protocol Fuzzing: Protecting Networked Embedded Devices from Dynamic Fuzzers",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3641847",
        "doi": "10.1145/3641847",
        "abstract": "Networked Embedded Devices (NEDs) are increasingly targeted by cyberattacks, mainly due to their widespread use in our daily lives. Vulnerabilities in NEDs are the root causes of these cyberattacks. Although deployed NEDs go through thorough code audits, there can still be considerable exploitable vulnerabilities. Existing mitigation measures like code encryption and obfuscation adopted by vendors can resist static analysis on deployed NEDs, but are ineffective against protocol fuzzing. Attackers can easily apply protocol fuzzing to discover vulnerabilities and compromise deployed NEDs. Unfortunately, prior anti-fuzzing techniques are impractical as they significantly slow down NEDs, hampering NED availability.To address this issue, we propose Armor\u2014the first anti-fuzzing technique specifically designed for NEDs. First, we design three adversarial primitives\u2013delay, fake coverage, and forged exception\u2013to break the fundamental mechanisms on which fuzzing relies to effectively find vulnerabilities. Second, based on our observation that inputs from normal users consistent with the protocol specification and certain program paths are rarely executed with normal inputs, we design static and dynamic strategies to decide whether to activate the adversarial primitives. Extensive evaluations show that Armor incurs negligible time overhead and effectively reduces the code coverage (e.g., line coverage by 22\\%-61\\%) for fuzzing, significantly outperforming the state of the art.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "108",
        "numpages": "26",
        "keywords": "Internet of Things, protocol fuzzing, anti-fuzzing"
    },
    "Enablers and Barriers of Empathy in Software Developer and User Interactions: A Mixed Methods Case Study": {
        "type": "article",
        "key": "10.1145/3641849",
        "author": "Gunatilake, Hashini and Grundy, John and Hoda, Rashina and Mueller, Ingo",
        "title": "Enablers and Barriers of Empathy in Software Developer and User Interactions: A Mixed Methods Case Study",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3641849",
        "doi": "10.1145/3641849",
        "abstract": "Software engineering (SE) requires developers to collaborate with stakeholders, and understanding their emotions and perspectives is often vital. Empathy is a concept characterising a person\u2019s ability to understand and share the feelings of another. However, empathy continues to be an under-researched human aspect in SE. We studied how empathy is practised between developers and end users using a mixed methods case study. We used an empathy test, observations, and interviews to collect data and socio-technical grounded theory and descriptive statistics to analyse data. We identified the nature of awareness required to trigger empathy and enablers of empathy. We discovered barriers to empathy and a set of potential strategies to overcome these barriers. We report insights on emerging relationships and present a set of recommendations and potential future works on empathy and SE for software practitioners and SE researchers.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "109",
        "numpages": "41",
        "keywords": "Empathy, human aspects, software engineering, awareness, enablers, barriers, software developers, end users"
    },
    "Understanding Real-Time Collaborative Programming: A Study of Visual Studio Live Share": {
        "type": "article",
        "key": "10.1145/3643672",
        "author": "Tan, Xin and Lv, Xinyue and Jiang, Jing and Zhang, Li",
        "title": "Understanding Real-Time Collaborative Programming: A Study of Visual Studio Live Share",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3643672",
        "doi": "10.1145/3643672",
        "abstract": "Real-time collaborative programming (RCP) entails developers working simultaneously, regardless of their geographic locations. RCP differs from traditional asynchronous online programming methods, such as Git or SVN, where developers work independently and update the codebase at separate times. Although various real-time code collaboration tools (e.g., Visual Studio Live Share, Code with Me, and Replit) have kept emerging in recent years, none of the existing studies explicitly focus on a deep understanding of the processes or experiences associated with RCP. To this end, we combine interviews and an e-mail survey with the users of Visual Studio Live Share, aiming to understand (i) the scenarios, (ii) the requirements, and (iii) the challenges when developers participate in RCP. We find that developers participate in RCP in 18 different scenarios belonging to six categories, e.g., pair programming, group debugging, and code review. However, existing users\u2019 attitudes toward the usefulness of the current RCP tools in these scenarios were significantly more negative than the expectations of potential users. As for the requirements, the most critical category is live editing, followed by the need for sharing terminals to enable hosts and guests to run commands and see the results, as well as focusing and following, which involves \u201cfollowing\u201d the host\u2019s edit location and \u201cfocusing\u201d the guests\u2019 attention on the host with a notification. Under these categories, we identify 17 requirements, but most of them are not well supported by current tools. In terms of challenges, we identify 19 challenges belonging to seven categories. The most severe category of challenges is lagging followed by permissions and conflicts. The above findings indicate that the current RCP tools and even collaborative environment need to be improved greatly and urgently. Based on these findings, we discuss the recommendations for different stakeholders, including practitioners, tool designers, and researchers.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "110",
        "numpages": "28",
        "keywords": "Real-time collaboration, software development, online collaboration, synchronous collaboration"
    },
    "Test Optimization in DNN Testing: A Survey": {
        "type": "article",
        "key": "10.1145/3643678",
        "author": "Hu, Qiang and Guo, Yuejun and Xie, Xiaofei and Cordy, Maxime and Ma, Lei and Papadakis, Mike and Le Traon, Yves",
        "title": "Test Optimization in DNN Testing: A Survey",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3643678",
        "doi": "10.1145/3643678",
        "abstract": "This article presents a comprehensive survey on test optimization in deep neural network&nbsp;(DNN) testing. Here, test optimization refers to testing with low data labeling effort. We analyzed 90 papers, including 43 from the software engineering (SE) community, 32 from the machine learning (ML) community, and 15 from other communities. Our study: (i) unifies the problems as well as terminologies associated with low-labeling cost testing, (ii) compares the distinct focal points of SE and ML communities, and (iii) reveals the pitfalls in existing literature. Furthermore, we highlight the research opportunities in this domain.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "apr",
        "articleno": "111",
        "numpages": "42",
        "keywords": "Test optimization, DNN testing, low-labeling cost"
    }
}