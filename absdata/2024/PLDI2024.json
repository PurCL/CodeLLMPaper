{
    "Input-Relational Verification of Deep Neural Networks": {
        "type": "software",
        "key": "10.5281/zenodo.10807316",
        "author": "Banerjee, Debangshu and Xu, Changming and Singh, Gagandeep",
        "title": "Input-Relational Verification of Deep Neural Networks",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10807316",
        "abstract": "    <p>We consider the verification of input-relational properties defined over deep neural networks (DNNs) such as robustness against universal adversarial perturbations, monotonicity, etc. Precise verification of these properties requires reasoning about multiple executions of the same DNN. We introduce a novel concept of difference tracking to compute the difference between the outputs of two executions of the same DNN at all layers. We design a new abstract domain, DiffPoly for efficient difference tracking that can scale large DNNs. DiffPoly is equipped with custom abstract transformers for common activation functions (ReLU, Tanh, Sigmoid, etc.) and affine layers and can create precise linear cross-execution constraints. We implement a input-relational verifier for DNNs called RaVeN which uses DiffPoly and linear program formulations to handle a wide range of input-relational properties. Our experimental results on challenging benchmarks show that by leveraging precise linear constraints defined over multiple executions of the DNN, RaVeN gains substantial precision over baselines on a wide range of datasets, networks, and input-relational properties.</p>",
        "keywords": "Abstract Interpretation, Deep Learning, Relational Verification"
    },
    "Modular Hardware Design of Pipelined Circuits with Hazards": {
        "type": "software",
        "key": "10.5281/zenodo.10906305",
        "author": "Jang, Minseong and Rhee, Jungin and Lee, Woojin and Zhao, Shuangshuang and Kang, Jeehoon",
        "title": "Modular Hardware Design of Pipelined Circuits with Hazards",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10906305",
        "abstract": "    <p>This is the <strong>Modular Hardware Design of Pipelined Circuits with Hazards</strong> paper artifact submitted for evaluation of the 45th ACM SIGPLAN conference on Programming Language Design and Implementation (PLDI`24).</p><p>It contains two files:</p><ul><li><code>hazardflow-artifact-pldi2024.zip</code>: Repository of the artifacts. Follow the README inside to reproduce the results.</li><li><code>artifact_evaluation_latest.tar.gz</code>: Docker image to run the CPU experiments.</li></ul>",
        "keywords": "Functional Hardware Description, HazardFlow, PLDI24"
    },
    "Verified Extraction from Coq to OCaml": {
        "type": "article",
        "key": "10.1145/3656379",
        "author": "Forster, Yannick and Sozeau, Matthieu and Tabareau, Nicolas",
        "title": "Verified Extraction from Coq to OCaml",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656379",
        "doi": "10.1145/3656379",
        "abstract": "One of the central claims of fame of the Coq proof assistant is extraction, i.e. the ability to obtain efficient programs in industrial programming languages such as OCaml, Haskell, or Scheme from programs written in Coq\u2019s expressive dependent type theory. Extraction is of great practical usefulness, used crucially e.g. in the CompCert project. However, for such executables obtained by extraction, the extraction process is part of the trusted code base (TCB), as are Coq\u2019s kernel and the compiler used to compile the extracted code. The extraction process contains intricate semantic transformation of programs that rely on subtle operational features of both the source and target language. Its code has also evolved since the last theoretical exposition in the seminal PhD thesis of Pierre Letouzey. Furthermore, while the exact correctness statements for the execution of extracted code are described clearly in academic literature, the interoperability with unverified code has never been investigated formally, and yet is used in virtually every project relying on extraction. In this paper, we describe the development of a novel extraction pipeline from Coq to OCaml, implemented and verified in Coq itself, with a clear correctness theorem and guarantees for safe interoperability. We build our work on the MetaCoq project, which aims at decreasing the TCB of Coq\u2019s kernel by re-implementing it in Coq itself and proving it correct w.r.t. a formal specification of Coq\u2019s type theory in Coq. Since OCaml does not have a formal specification, we make use of the project specifying the semantics of the intermediate language of the OCaml compiler. Our work fills some gaps in the literature and highlights important differences between the operational semantics of Coq programs and their extraction. In particular, we focus on the guarantees that can be provided for interoperability with unverified code, and prove that extracted programs of first-order data type are correct and can safely interoperate, whereas for higher-order programs already simple interoperations can lead to incorrect behaviour and even outright segfaults.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "149",
        "numpages": "24",
        "keywords": "Coq, extraction, functional programming, verified compilation"
    },
    "Artifact for \"Verified Extraction from Coq to OCaml": {
        "type": "software",
        "key": "10.5281/zenodo.10975363",
        "author": "Forster, Yannick and Sozeau, Matthieu and Tabareau, Nicolas",
        "title": "Artifact for \"Verified Extraction from Coq to OCaml",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10975363",
        "abstract": "    <p>The Coq code corresponding to the development described in the paper, and benchmarks corresponding to section 6.</p>",
        "keywords": "Coq, extraction, functional programming, verified compilation"
    },
    "Robust Resource Bounds with Static Analysis and Bayesian Inference": {
        "type": "article",
        "key": "10.1145/3656380",
        "author": "Pham, Long and Saad, Feras A. and Hoffmann, Jan",
        "title": "Robust Resource Bounds with Static Analysis and Bayesian Inference",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656380",
        "doi": "10.1145/3656380",
        "abstract": "There are two approaches to automatically deriving symbolic worst-case resource bounds for programs: static analysis of the source code and data-driven analysis of cost measurements obtained by running the program. Static resource analysis is usually sound but incomplete. Data-driven analysis can always return a result, but its lack of robustness often leads to unsound results. This paper presents the design, implementation, and empirical evaluation of hybrid resource bound analyses that tightly integrate static analysis and data-driven analysis. The static analysis part builds on automatic amortized resource analysis (AARA), a state-of-the-art type-based resource analysis method that performs cost bound inference using linear optimization. The data-driven part is rooted in novel Bayesian modeling and inference techniques that improve upon previous data-driven analysis methods by reporting an entire probability distribution over likely resource cost bounds. A key innovation is a new type inference system called Hybrid AARA that coherently integrates Bayesian inference into conventional AARA, combining the strengths of both approaches. Hybrid AARA is proven to be statistically sound under standard assumptions on the runtime cost data. An experimental evaluation on a challenging set of benchmarks shows that Hybrid AARA (i) effectively mitigates the incompleteness of purely static resource analysis; and (ii) is more accurate and robust than purely data-driven resource analysis.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "150",
        "numpages": "26",
        "keywords": "Bayesian inference, data-driven analysis, hybrid analysis, resource analysis, static analysis, worst-case costs"
    },
    "Hybrid Resource-Aware ML": {
        "type": "software",
        "key": "10.5281/zenodo.10937074",
        "author": "Pham, Long and Saad, Feras A. and Hoffmann, Jan",
        "title": "Hybrid Resource-Aware ML",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10937074",
        "abstract": "    <p>Hybrid Resource-Aware ML (Hybrid RaML) is a program analysis tool that takes in an OCaml program and infers its polynomial cost bound using the technique Hybrid Automatic Amortized Resource Analysis (AARA). It integrates data-driven resource analysis (specifically linear programming and Bayesian inference) and static resource analysis (specifically the conventional AARA). Hybrid RaML is wrapped inside a Docker image, and it comes with (i) a guide README.pdf describing how to use run the software and (ii) a paper paper.pdf describing Hybrid AARA.</p>",
        "keywords": "Bayesian inference, data-driven analysis, hybrid analysis, program analysis, resource analysis, static analysis, type systems, worst-case costs"
    },
    "Recursive Program Synthesis using Paramorphisms": {
        "type": "article",
        "key": "10.1145/3656381",
        "author": "Hong, Qiantan and Aiken, Alex",
        "title": "Recursive Program Synthesis using Paramorphisms",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656381",
        "doi": "10.1145/3656381",
        "abstract": "We show that synthesizing recursive functional programs using a class of primitive recursive combinators is both simpler and solves more benchmarks from the literature than previously proposed approaches. Our method synthesizes paramorphisms, a class of programs that includes the most common recursive programming patterns on algebraic data types. The crux of our approach is to split the synthesis problem into two parts: a multi-hole template that fixes the recursive structure, and a search for non-recursive program fragments to fill the template holes.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "151",
        "numpages": "24",
        "keywords": "Examples, Program Synthesis, Recursion Schemes, Stochastic Synthesis"
    },
    "A Tensor Compiler with Automatic Data Packing for Simple and Efficient Fully Homomorphic Encryption": {
        "type": "article",
        "key": "10.1145/3656382",
        "author": "Krastev, Aleksandar and Samardzic, Nikola and Langowski, Simon and Devadas, Srinivas and Sanchez, Daniel",
        "title": "A Tensor Compiler with Automatic Data Packing for Simple and Efficient Fully Homomorphic Encryption",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656382",
        "doi": "10.1145/3656382",
        "abstract": "Fully Homomorphic Encryption (FHE) enables computing on encrypted data, letting clients securely offload computation to untrusted servers. While enticing, FHE has two key challenges that limit its applicability: it has high performance overheads (10,000\\texttimes{} over unencrypted computation) and it is extremely hard to program. Recent hardware accelerators and algorithmic improvements have reduced FHE\u2019s overheads and enabled large applications to run under FHE. These large applications exacerbate FHE\u2019s programmability challenges.     Writing FHE programs directly is hard because FHE schemes expose a restrictive, low-level interface that prevents abstraction and composition. Specifically, FHE requires packing encrypted data into large vectors (tens of thousands of elements long), FHE provides limited operations on these vectors, and values have noise that grows with each operation, which creates unintuitive performance tradeoffs. As a result, translating large applications, like neural networks, into efficient FHE circuits takes substantial tedious work.     We address FHE\u2019s programmability challenges with the Fhelipe FHE compiler. Fhelipe exposes a simple, numpy-style tensor programming interface, and compiles high-level tensor programs into efficient FHE circuits. Fhelipe\u2019s key contribution is automatic data packing, which chooses data layouts for tensors and packs them into ciphertexts to maximize performance. Our novel framework considers a wide range of layouts and optimizes them analytically. This lets compile large FHE programs efficiently, unlike prior FHE compilers, which either use inefficient layouts or do not scale beyond tiny programs.     We evaluate on both a state-of-the-art FHE accelerator and a CPU. is the first compiler that matches or exceeds the performance of large hand-optimized FHE applications, like deep neural networks, and outperforms a state-of-the-art FHE compiler by gmean 18.5. At the same time, dramatically simplifies programming, reducing code size by 10\u201348.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "152",
        "numpages": "25",
        "keywords": "automatic bootstrapping, fully homomorphic encryption, tensors"
    },
    "Concurrent Immediate Reference Counting": {
        "type": "article",
        "key": "10.1145/3656383",
        "author": "Jung, Jaehwang and Kim, Jeonghyeon and Parkinson, Matthew J. and Kang, Jeehoon",
        "title": "Concurrent Immediate Reference Counting",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656383",
        "doi": "10.1145/3656383",
        "abstract": "Memory management for optimistic concurrency in unmanaged programming languages is challenging. Safe memory reclamation (SMR) algorithms help address this, but they are difficult to use correctly. Automatic reference counting provides a simpler interface, but it has been less efficient than SMR algorithms. Recently, there has been a push to apply the optimizations used in garbage collectors for managed languages to elide reference count updates from local references. Notably, Fast Reference Counter, OrcGC, and Concurrent Deferred Reference Counting use SMR algorithms to protect local references by deferring decrements or reclamation. While they show a significant performance improvement, their use of deferral may result in growing memory usage due to slow reclamation of linked structures, and suboptimal performance in update-heavy workloads. We present Concurrent Immediate Reference Counting (CIRC), a new combination of SMR algorithms with reference counting. CIRC employs deferral like other modern methods, but it avoids their problems with novel algorithms for (1) immediately reclaiming linked structures recursively by tracking the reachability of each object, and (2) applying decrements immediately and deferring only the reclamation. Our experiments show that CIRC\u2019s memory usage does not grow over time and is only slightly higher than the underlying SMR. Moreover, CIRC further narrows the performance gap between the underlying SMR, positioning it as a promising solution to safe automatic memory management for highly concurrent data structures in unmanaged languages.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "153",
        "numpages": "24",
        "keywords": "automatic memory reclamation, concurrent data structures, reference counting"
    },
    "Artifact for \"Concurrent Immediate Reference Counting": {
        "type": "software",
        "key": "10.5281/zenodo.10806736",
        "author": "Jung, Jaehwang and Kim, Jeonghyeon and Parkinson, Matthew J. and Kang, Jeehoon",
        "title": "Artifact for \"Concurrent Immediate Reference Counting",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10806736",
        "abstract": "    <p>This is the artifact for PLDI 2024 paper: \u201cConcurrent Immediate Reference Counting\u201d.</p><p>This artifact comprises the following files:</p><ul><li><code>circ-benchmark.zip</code>: This archive mainly contains a benchmark suite used to produce the results presented in the paper. Additionally, it contains:<ul><li><code>README.md</code>: instructions on how to reproduce the benchmark results, and</li><li><code>paper-results</code>: generated result files that are included in the paper.</li></ul></li><li><code>circ-docker.tar.gz</code>: This file is a pre-built Docker image for conveniently running the benchmark.</li></ul><p>Refer to the README.md in the attached file for more information on this artifact.</p>",
        "keywords": "automatic memory reclamation, concurrent data structures, reference counting"
    },
    "A Proof Recipe for Linearizability in Relaxed Memory Separation Logic": {
        "type": "article",
        "key": "10.1145/3656384",
        "author": "Park, Sunho and Kim, Jaewoo and Mulder, Ike and Jung, Jaehwang and Lee, Janggun and Krebbers, Robbert and Kang, Jeehoon",
        "title": "A Proof Recipe for Linearizability in Relaxed Memory Separation Logic",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656384",
        "doi": "10.1145/3656384",
        "abstract": "Linearizability is the de facto standard for correctness of concurrent objects\u2014it essentially says that all the object\u2019s operations behave as if they were atomic. There have been a number of recent advances in developing increasingly strong linearizability specifications for relaxed memory consistency (RMC), but scalable proof methods for these specifications do not exist due to the challenges arising from out-of-order executions (requiring event reordering) and selected synchronization (requiring tracking of view transfers). We propose a proof recipe for the linearizable history specifications by Dang et al. in the Iris-based iRC11 concurrent separation logic in Coq. Key to our proof recipe is the notion of object modification order (OMO), which generalizes the modification order of the C11 memory model to an object-local setting. Using OMO we minimize the conditions that need to be proved for event reordering. To enable proof reuse for concurrent libraries that are built on top of others, OMO provides the novel notion of a commit-with relation that connects the linearization points of the lower and upper libraries. Using our recipe, we verify the linearizability of the Michael\u2013Scott queue, the elimination stack, and Folly\u2019s MPMC queue in RMC for the first time; and verify stronger specifications of a spinlock and atomic reference counting in RMC than prior work.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "154",
        "numpages": "24",
        "keywords": "automation, linearizability, relaxed memory, separation logic"
    },
    "Artifact for \"A Proof Recipe for Linearizability in Relaxed Memory Separation Logic\", PLDI 2024": {
        "type": "software",
        "key": "10.5281/zenodo.10933398",
        "author": "Park, Sunho and Kim, Jaewoo and Mulder, Ike and Jung, Jaehwang and Lee, Janggun and Krebbers, Robbert and Kang, Jeehoon",
        "title": "Artifact for \"A Proof Recipe for Linearizability in Relaxed Memory Separation Logic\", PLDI 2024",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10933398",
        "abstract": "    <p>This is the formalization for the paper \u201cA Proof Recipe for Linearizability in Relaxed Memory Separation Logic\u201d, written in Coq, along with a Docker image file (<code>artifact.tar.gz</code>) that contains a compiled version of the project and all dependencies installed.</p><p>Detailed instructions and explanations are written in the README.md inside <code>pldi24-36-artifact.zip</code>.</p>",
        "keywords": "automation, linearizability, relaxed memory, separation logic"
    },
    "Diffy: Data-Driven Bug Finding for Configurations": {
        "type": "article",
        "key": "10.1145/3656385",
        "author": "Kakarla, Siva Kesava Reddy and Yan, Francis Y. and Beckett, Ryan",
        "title": "Diffy: Data-Driven Bug Finding for Configurations",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656385",
        "doi": "10.1145/3656385",
        "abstract": "Configuration errors remain a major cause of system failures and service outages. One promising approach to identify configuration errors automatically is to learn common usage patterns (and anti-patterns) using data-driven methods. However, existing data-driven learning approaches analyze only simple configurations (e.g., those with no hierarchical structure), identify only simple types of issues (e.g., type errors), or require extensive domain-specific tuning. In this paper, we present Diffy, the first push-button configuration analyzer that detects likely bugs in structured configurations. From example configurations, Diffy learns a common template, with \"holes\" that capture their variation. It then applies unsupervised learning to identify anomalous template parameters as likely bugs. We evaluate Diffy on a large cloud provider's wide-area network, an operational 5G network testbed, and MySQL configurations, demonstrating its versatility, performance, and accuracy. During Diffy's development, it caught and prevented a bug in a configuration timer value that had previously caused an outage for the cloud provider.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "155",
        "numpages": "24",
        "keywords": "anomaly detection, configuration bug finding, template synthesis"
    },
    "Source code for article \"Diffy: Data-Driven Bug Finding for Configurations": {
        "type": "software",
        "key": "10.5281/zenodo.10740687",
        "author": "Kakarla, Siva Kesava Reddy and Yan, Francis Y. and Beckett, Ryan",
        "title": "Source code for article \"Diffy: Data-Driven Bug Finding for Configurations",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10740687",
        "abstract": "    <p>Diffy is a push-button configuration analyzer that detects potential bugs in JSON configurations. It learns a common template from a set of similar configurations and uses unsupervised learning to identify anomalous template parameters as likely bugs.</p><p>We have used F# and C# as our programming languages and have provided instructions in our README on how to operate the tool.</p><p>In the paper, we used three datasets: - A large cloud provider\u2019s wide-area network configurations - Operational configurations from a 5GvRAN network testbed - MySQL configurations</p><p>We have uploaded the 5GvRAN configurations and MySQL configurations to the repository under the examples directory. The majority of the evaluation section in the paper is based on the WAN configurations, which unfortunately, we cannot release publicly.</p>",
        "keywords": "anomaly detection, configuration bug finding, template synthesis"
    },
    "Boosting Compiler Testing by Injecting Real-World Code": {
        "type": "article",
        "key": "10.1145/3656386",
        "author": "Li, Shaohua and Theodoridis, Theodoros and Su, Zhendong",
        "title": "Boosting Compiler Testing by Injecting Real-World Code",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656386",
        "doi": "10.1145/3656386",
        "abstract": "We introduce a novel approach for testing optimizing compilers with code from real-world applications. The main idea is to construct well-formed programs by fusing multiple code snippets from various real-world projects. The key insight is backed by the fact that the large volume of real-world code exercises rich syntactical and semantic language features, which current engineering-intensive approaches like random program generators are hard to fully support. To construct well-formed programs from real-world code, our approach works by (1) extracting real-world code at the granularity of function, (2) injecting function calls into seed programs, and (3) leveraging dynamic execution information to maintain the semantics and build complex data dependencies between injected functions and the seed program. With this idea, our approach complements the existing generators by boosting their expressiveness via fusing real-world code in a semantics-preserving way.  We implement our idea in a tool, Creal, to test C compilers. In a nine-month testing period, we have reported 132 bugs to GCC and LLVM, two of the most popular and well-tested C compilers.  At the time of writing, 121 of them have been confirmed as unknown bugs, and 101 of them have been fixed. Most of these bugs were miscompilations, and many were recognized as long-latent and critical. Our evaluation results evidently demonstrate the significant advantage of using real-world code to stress-test compilers. We believe this idea will benefit the general compiler testing direction and will be directly applicable to other compilers.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "156",
        "numpages": "23",
        "keywords": "Compiler testing, compilers, miscompilation, reliability, testing"
    },
    "Artifact for PLDI'2024 paper \"Boosting Compiler Testing by Injecting Real-world Code": {
        "type": "software",
        "key": "10.5281/zenodo.10951313",
        "author": "Li, Shaohua and Theodoridis, Theodoros and Su, Zhendong",
        "title": "Artifact for PLDI'2024 paper \"Boosting Compiler Testing by Injecting Real-world Code",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10951313",
        "abstract": "    <p>This is the artifact for the PLDI\u20192024 paper \u201cBoosting Compiler Testing by Injecting Real-World Code\u201d. Please first untar the package and then refer to the README.pdf file for detailed instructions.</p>",
        "keywords": "Compiler testing, compilers, miscompilation, reliability, testing"
    },
    "SMT Theory Arbitrage: Approximating Unbounded Constraints using Bounded Theories": {
        "type": "article",
        "key": "10.1145/3656387",
        "author": "Mikek, Benjamin and Zhang, Qirun",
        "title": "SMT Theory Arbitrage: Approximating Unbounded Constraints using Bounded Theories",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656387",
        "doi": "10.1145/3656387",
        "abstract": "SMT solvers are foundational tools for reasoning about constraints in practical problems both within and outside program analysis. Faster SMT solving improves the performance of practical tools and expands the set of tractable problems. Existing approaches to improving solver performance either focus on general algorithms applied below the level of individual theories, or focus on optimizations within a single theory. Unbounded constraints in which the number of possible variable values is infinite, such as real numbers and integers, pose a particularly difficult challenge for solvers. Bounded constraints in which the set of possible values is finite, such as bitvectors and floating-point numbers, on the other hand, are decidable and have been the subject of extensive performance improvement efforts. This paper introduces a theory arbitrage: we transform unbounded constraints, which are often expensive to solve, into bounded constraints, which are typically cheaper to solve. By converting unbounded problems into bounded ones, theory arbitrage takes advantage of better performance on bounded constraints and unlocks optimization techniques that only apply to bounded theories. The transformation is achieved by harnessing a novel abstract interpretation strategy to infer bounds. The bounded transformed constraint is then an underapproximation of the semantics of the unbounded original. We realize our method for the theories of integers and real numbers with a practical tool (STAUB). Our evaluation demonstrates that theory arbitrage alone can speed up individual constraints by orders of magnitude and achieve up to a 1.4\\texttimes{} speedup on average across nonlinear integer benchmarks. Furthermore, it enables the use of the recent compiler optimization-based technique SLOT for unbounded SMT theories, unlocking a further speedup of up to 3\\texttimes{}. Finally, we incorporate STAUB into a practical termination proving tool and observe an overall 9\\% improvement in performance.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "157",
        "numpages": "26",
        "keywords": "SMT, abstract interpretation, bound inference, constraint solving"
    },
    "STAUB: SMT Theory Arbitrage from Unbounded to Bounded": {
        "type": "software",
        "key": "10.5281/zenodo.10895770",
        "author": "Mikek, Benjamin and Zhang, Qirun",
        "title": "STAUB: SMT Theory Arbitrage from Unbounded to Bounded",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10895770",
        "abstract": "    <p>Implementation of STAUB in C++ and experimental data reflecting STAUB\u2019s effectiveness in speeding up SMT solving for unbounded constraints.</p>",
        "keywords": "abstract interpretation, constraint solving, SMT"
    },
    "Compilation of Qubit Circuits to Optimized Qutrit Circuits": {
        "type": "article",
        "key": "10.1145/3656388",
        "author": "Sharma, Ritvik and Achour, Sara",
        "title": "Compilation of Qubit Circuits to Optimized Qutrit Circuits",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656388",
        "doi": "10.1145/3656388",
        "abstract": "Quantum computers are a revolutionary class of computational platforms that are capable of solving computationally hard problems. However, today\u2019s quantum hardware is subject to noise and decoherence issues that together limit the scale and complexity of the quantum circuits that can be implemented. Recently, practitioners have developed qutrit-based quantum hardware platforms that compute over 0, 1, and 2 states, and have presented circuit depth reduction techniques using qutrits\u2019 higher energy 2 states to temporarily store information. However, thus far, such quantum circuits that use higher order states for temporary storage need to be manually crafted by hardware designers. We present , an optimizing compiler for qutrit circuits that implement qubit computations.  deploys a qutrit circuit decomposition algorithm and a rewrite engine to construct and optimize qutrit circuits. We evaluate  against hand-optimized qutrit circuits and qubit circuits, and find  delivers up to 65\\% depth improvement over manual qutrit implementations, and 43-75\\% depth improvement over qubit circuits. We also perform a fidelity analysis and find -optimized qutrit circuits deliver up to 8.9\\texttimes{} higher fidelity circuits than their manually implemented counterparts.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "158",
        "numpages": "24",
        "keywords": "Quantum computing, Qutrits, Rewriting Tools, Synthesis"
    },
    "DARE Qutrit Compiler": {
        "type": "software",
        "key": "10.5281/zenodo.10807175",
        "author": "Sharma, Ritvik and Achour, Sara",
        "title": "DARE Qutrit Compiler",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10807175",
        "abstract": "    <p>This contains all code required to regenerate results of Compilation of Qubit Circuits to Optimized Qutrit Circuits and use the DARE compiler.</p>",
        "keywords": "Quantum computing, Qutrits, Rewriting Tools, Synthesis"
    },
    "Optimistic Stack Allocation and Dynamic Heapification for Managed Runtimes": {
        "type": "software",
        "key": "10.5281/zenodo.10804712",
        "author": "Anand, Aditya and Adithya, Solai and Rustagi, Swapnil and Seth, Priyam and Sundaresan, Vijay and Maier, Daryl and Nandivada, V. Krishna and Thakur, Manas",
        "title": "Optimistic Stack Allocation and Dynamic Heapification for Managed Runtimes",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10804712",
        "abstract": "    <p>Artifact for PLDI 2024 paper title \u201cOptimistic Stack Allocation and Dynamic Heapification for Managed Runtimes\u201d.</p>",
        "keywords": "Escape analysis, Managed runtimes, Stack allocation"
    },
    "A Verified Compiler for a Functional Tensor Language": {
        "type": "software",
        "key": "10.5281/zenodo.10932109",
        "author": "Liu, Amanda and Bernstein, Gilbert and Chlipala, Adam and Ragan-Kelley, Jonathan",
        "title": "A Verified Compiler for a Functional Tensor Language",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10932109",
        "abstract": "    <p>Virtual machine for AEC PLDI 2024. This contains the source for the ATL language, its verified rewrite framework, and the proof of correctness for its lowering algorithm embedded and implemented in Coq.</p>",
        "keywords": "array programming, formal verification, functional programming, tensors, type systems"
    },
    "IsoPredict: Dynamic Predictive Analysis for Detecting Unserializable Behaviors in Weakly Isolated Data Store Applications": {
        "type": "article",
        "key": "10.1145/3656391",
        "author": "Geng, Chujun and Blanas, Spyros and Bond, Michael D. and Wang, Yang",
        "title": "IsoPredict: Dynamic Predictive Analysis for Detecting Unserializable Behaviors in Weakly Isolated Data Store Applications",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656391",
        "doi": "10.1145/3656391",
        "abstract": "Distributed data stores typically provide weak isolation levels, which are efficient but can lead to unserializable behaviors, which are hard for programmers to understand and often result in errors. This paper presents the first dynamic predictive analysis for data store applications under weak isolation levels, called IsoPredict. Given an observed serializable execution of a data store application, IsoPredict generates and solves SMT constraints to find an unserializable execution that is a feasible execution of the application. IsoPredict introduces novel techniques to handle divergent application behavior; to solve mutually recursive sets of constraints; and to balance coverage, precision, and performance. An evaluation shows IsoPredict finds unserializable behaviors in four data store benchmarks, and that more than 99\\% of its predicted executions are feasible.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "161",
        "numpages": "25",
        "keywords": "data stores, dynamic predictive analysis, transactions, weak isolation levels"
    },
    "Reproduction Package for 'IsoPredict: Dynamic Predictive Analysis for Detecting Unserializable Behaviors in Weakly Isolated Data Store Applications'": {
        "type": "software",
        "key": "10.5281/zenodo.10802748",
        "author": "Geng, Chujun and Blanas, Spyros and Bond, Michael D. and Wang, Yang",
        "title": "Reproduction Package for 'IsoPredict: Dynamic Predictive Analysis for Detecting Unserializable Behaviors in Weakly Isolated Data Store Applications'",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10802748",
        "abstract": "    <p>The artifact contains IsoPredict and its benchmarks. IsoPredict was written in Python and benchmarks were written in Java and Rust. The benchmarks generate traces that will be analyzed by IsoPredict. IsoPredict will perform both predictive analysis and validation. Everything will be provided as a docker container image. We recommend running them on a Linux machine with at least 16GB of RAM.</p>",
        "keywords": "concurrency, database isolation levels, Dynamic predictive analysis, software debugging"
    },
    "Compiling with Abstract Interpretation": {
        "type": "article",
        "key": "10.1145/3656392",
        "author": "Lesbre, Dorian and Lemerre, Matthieu",
        "title": "Compiling with Abstract Interpretation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656392",
        "doi": "10.1145/3656392",
        "abstract": "Rewriting and static analyses are mutually beneficial techniques: program  transformations change the intensional aspects of the program, and can thus  improve analysis precision, while some efficient transformations are enabled by specific  knowledge of some program invariants. Despite the strong interaction between  these techniques, they are usually considered distinct. In this paper, we  demonstrate that we can turn abstract interpreters into compilers, using a  simple free algebra over the standard signature of abstract domains. Functor  domains correspond to compiler passes, for which soundness is translated to a  proof of forward simulation, and completeness to backward simulation.  We achieve translation to SSA using an abstract domain with a non-standard  SSA signature. Incorporating such an SSA translation to an abstract  interpreter improves its precision; in particular we show that an  SSA-based non-relational domain is always more precise than a standard  non-relational domain for similar time and memory complexity.  Moreover, such a domain allows recovering from precision losses that occur when analyzing low-level  machine code instead of source code. These results help implement analyses or  compilation passes where symbolic and semantic methods simultaneously refine  each other, and improves precision when compared to doing the passes in sequence.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "162",
        "numpages": "26",
        "keywords": "Abstract Interpretation, Compilation, Static Analysis"
    },
    "Artifact for paper \"Compiling with abstract interpretation": {
        "type": "software",
        "key": "10.5281/zenodo.10895582",
        "author": "Lesbre, Dorian and Lemerre, Matthieu",
        "title": "Artifact for paper \"Compiling with abstract interpretation",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10895582",
        "abstract": "    <p>This is an artifact for the paper Compiling with Abstract Interpretation, submitted at PLDI 2024. It contains two abstract interpreters:</p><pre><code>TAI, very simple frama-c plugin that closely follows the paper definitions, but only supports a small subset of C (only integer variables, macro, and non-recursive function calls). It is used to demonstrate our technique but isn't very time or memory efficient.It's a frama-c plugin but only uses frama-c as a C parser, so no knowledge of frama-c is required to understand it.Codex: a much larger abstract interpretation library. It supports every aspect of C (as a frama-c plugin) and a number of binary formats (as a binsec plugin). This library implements many techniques and domains beyond the scope of the paper, but some ideas from our paper such as translation to SSA as an abstract interpretation pass have made it into its codebase.</code></pre>",
        "keywords": "Abstract Interpretation, Compilation, Frama-C, OCaml, SSA"
    },
    "Associated Effects: Flexible Abstractions for Effectful Programming": {
        "type": "article",
        "key": "10.1145/3656393",
        "author": "Lutze, Matthew and Madsen, Magnus",
        "title": "Associated Effects: Flexible Abstractions for Effectful Programming",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656393",
        "doi": "10.1145/3656393",
        "abstract": "We present associated effects, a programming language feature that enables type classes to abstract over the effects of their function signatures, allowing each type class instance to specify its concrete effects.  Associated effects significantly increase the flexibility and expressive power of a programming language that combines a type and effect system with type classes. In particular, associated effects allow us to (i) abstract over total and partial functions, where partial functions may throw exceptions",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "163",
        "numpages": "23",
        "keywords": "ad-hoc polymorphism, associated effects, associated types, effect systems, generic programming, type classes, type functions"
    },
    "Associated Effects (Artifact)": {
        "type": "software",
        "key": "10.5281/zenodo.10932590",
        "author": "Lutze, Matthew and Madsen, Magnus",
        "title": "Associated Effects (Artifact)",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10932590",
        "abstract": "    <p>This artifact contains a Flix compiler, modified to support associated effects, as detailed in the paper. The artifact includes example files from the standard library, accessible in a QEMU virtual machine, in order to allow browsing files using Flix\u2019s VSCode extension.</p>",
        "keywords": "ad-hoc polymorphism, associated effects, associated types, effect systems, generic programming, type classes, type functions"
    },
    "Efficient Static Vulnerability Analysis for JavaScript with Multiversion Dependency Graphs": {
        "type": "article",
        "key": "10.1145/3656394",
        "author": "Ferreira, Mafalda and Monteiro, Miguel and Brito, Tiago and Coimbra, Miguel E. and Santos, Nuno and Jia, Limin and Santos, Jos\\'{e} Fragoso",
        "title": "Efficient Static Vulnerability Analysis for JavaScript with Multiversion Dependency Graphs",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656394",
        "doi": "10.1145/3656394",
        "abstract": "While static analysis tools that rely on Code Property Graphs (CPGs) to detect security vulnerabilities have proven effective, deciding how much information to include in the graphs remains a challenge. Including less information can lead to a more scalable analysis but at the cost of reduced effectiveness in identifying vulnerability patterns, potentially resulting in classification errors. Conversely, more information in the graph allows for a more effective analysis but may affect scalability. For example, scalability issues have been recently highlighted in ODGen, the state-of-the-art CPG-based tool for detecting Node.js vulnerabilities.  This paper examines a new point in the design space of CPGs for JavaScript vulnerability detection. We introduce the Multiversion Dependency Graph (MDG), a novel graph-based data structure that captures the state evolution of objects and their properties during program execution. Compared to the graphs used by ODGen, MDGs are significantly simpler without losing key information needed for vulnerability detection. We implemented Graph.js, a new MDG-based static vulnerability scanner specialized in analyzing npm packages and detecting taint-style and prototype pollution vulnerabilities. Our evaluation shows that Graph.js outperforms ODGen by significantly reducing both the false negatives and the analysis time. Additionally, we have identified 49 previously undiscovered vulnerabilities in npm packages.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "164",
        "numpages": "25",
        "keywords": "JavaScript, Static Analysis, Vulnerability Detection"
    },
    "Artifact for paper \"Efficient Static Vulnerability Analysis for JavaScript with Multiversion Dependency Graphs": {
        "type": "software",
        "key": "10.5281/zenodo.10936488",
        "author": "Ferreira, Mafalda and Monteiro, Miguel and Brito, Tiago and Coimbra, Miguel E. and Santos, Nuno and Jia, Limin and Santos, Jos\\'{e} Fragoso",
        "title": "Artifact for paper \"Efficient Static Vulnerability Analysis for JavaScript with Multiversion Dependency Graphs",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10936488",
        "abstract": "    <p>This artifact evaluates Graph.js, a novel static vulnerability detection tool for Node.js applications, that detects taint-style and prototype pollution vulnerabilities. The repository includes all source code, reference datasets and instructions on how to build and run the experiments. These experiments result in the tables and plots presented in the paper, which can be used to validate the results.</p>",
        "keywords": "JavaScript, Static Analysis, Vulnerability Detection"
    },
    "Floating-Point TVPI Abstract Domain": {
        "type": "article",
        "key": "10.1145/3656395",
        "author": "Rivera, Joao and Franchetti, Franz and P\\\"{u}schel, Markus",
        "title": "Floating-Point TVPI Abstract Domain",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656395",
        "doi": "10.1145/3656395",
        "abstract": "Floating-point arithmetic is natively supported in hardware and the preferred choice when implementing numerical software in scientific or engineering applications. However, such programs are notoriously hard to analyze due to round-off errors and the frequent use of elementary functions such as log, arctan, or sqrt.    In this work, we present the Two Variables per Inequality Floating-Point (TVPI-FP) domain, a numerical and constraint-based abstract domain designed for the analysis of floating-point programs. TVPI-FP supports all features of real-world floating-point programs including conditional branches, loops, and elementary functions and it is efficient asymptotically and in practice. Thus it overcomes limitations of prior tools that often are restricted to straight-line programs or require the use of expensive solvers. The key idea is the consistent use of interval arithmetic in inequalities and an associated redesign of all operators. Our extensive experiments show that TVPI-FP is often orders of magnitudes faster than more expressive tools at competitive, or better precision while also providing broader support for realistic programs with loops and conditionals.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "165",
        "numpages": "25",
        "keywords": "abstract interpretation, numerical program analysis"
    },
    "Artifact: Floating-Point TVPI Abstract Domain": {
        "type": "software",
        "key": "10.5281/zenodo.10806719",
        "author": "Rivera, Joao and Franchetti, Franz and P\\\"{u}schel, Markus",
        "title": "Artifact: Floating-Point TVPI Abstract Domain",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10806719",
        "abstract": "    <p>Artifact for the paper \u201cFloating-Point TVPI Abstract Domain\u201d at PLDI 2024. The artifact comes in the form of a virtual machine running Ubuntu 20.04. It contains the full source code of TVPI-FP, and benchmarks and scripts for reproducing main experiments.</p>",
        "keywords": "abstract interpretation, numerical program analysis"
    },
    "NetBlocks: Staging Layouts for High-Performance Custom Host Network Stacks": {
        "type": "article",
        "key": "10.1145/3656396",
        "author": "Brahmakshatriya, Ajay and Rinard, Chris and Ghobadi, Manya and Amarasinghe, Saman",
        "title": "NetBlocks: Staging Layouts for High-Performance Custom Host Network Stacks",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656396",
        "doi": "10.1145/3656396",
        "abstract": "Modern network applications and environments, ranging from data centers and IoT devices to AR/VR headsets and underwater robotics, present diverse requirements that cannot be satisfied by the all or-nothing approach of TCP and UDP protocols. Network researchers and engineers need to create highly tailored protocols targeting individual problem domains. Existing library-based approaches either fall short on the flexibility in features or offer them at a significant performance overhead. To address this challenge, we present NetBlocks, a domain-specific language, and compiler for designing ad-hoc protocols and generating their highly optimized host network stack implementations. NetBlocks DSL input allows users to configure protocols by selecting and customizing features. Unlike other DSL compilers, NetBlocks also allows network researchers to extend the system and add more features easily without any prior compiler knowledge. Our design and implementation employ a high-performance Aspect-Oriented Programming framework written with the staging framework BuildIt. We also introduce a novel Layout Customization Layer that allows \"staging packet layouts\" alongside the implementation, which is critical for getting the best performance out of the protocol when possible, while allowing the practitioners to maintain compatibility with existing protocol layers where needed. Our evaluations on three applications ranging across deployments in data centers and underwater acoustic networks demonstrate a trade-off between performance (both latency and throughput) and selected features allowing the user to only pay-for what-they-use.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "166",
        "numpages": "25",
        "keywords": "compilers, layouts, network-protocols"
    },
    "Replication package for the PLDI 2024 paper: NetBlocks: Staging Layouts for High-Performance Custom Host Network Stacks": {
        "type": "software",
        "key": "10.5281/zenodo.11099781",
        "author": "Brahmakshatriya, Ajay and Rinard, Chris and Ghobadi, Manya and Amarasinghe, Saman",
        "title": "Replication package for the PLDI 2024 paper: NetBlocks: Staging Layouts for High-Performance Custom Host Network Stacks",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.11099781",
        "abstract": "    <p>Replication package for the PLDI 2024 paper: NetBlocks: Staging Layouts for High-Performance Custom Host Network Stacks</p>",
        "keywords": "compilers, layouts, network-protocols"
    },
    "The T-Complexity Costs of Error Correction for Control Flow in Quantum Computation": {
        "type": "software",
        "key": "10.5281/zenodo.10729070",
        "author": "Yuan, Charles and Carbin, Michael",
        "title": "The T-Complexity Costs of Error Correction for Control Flow in Quantum Computation",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10729070",
        "abstract": "    <p>The artifact contains the sources for the Spire compiler, the benchmark programs and circuits used in the paper, and the evaluation package.</p>",
        "keywords": "quantum compilers, quantum programming languages"
    },
    "The Functional Essence of Imperative Binary Search Trees": {
        "type": "article",
        "key": "10.1145/3656398",
        "author": "Lorenzen, Anton and Leijen, Daan and Swierstra, Wouter and Lindley, Sam",
        "title": "The Functional Essence of Imperative Binary Search Trees",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656398",
        "doi": "10.1145/3656398",
        "abstract": "Algorithms on restructuring binary search trees are typically  presented in imperative pseudocode. Understandably so, as their  performance relies on in-place execution, rather than the repeated  allocation of fresh nodes in memory. Unfortunately, these imperative  algorithms are notoriously difficult to verify as their loop  invariants must relate the unfinished tree fragments being  rebalanced. This paper presents several novel functional algorithms  for accessing and inserting elements in a restructuring binary search  tree that are as fast as their imperative counterparts; yet the  correctness of these functional algorithms is established using a  simple inductive argument. For each data structure, move-to-root",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "168",
        "numpages": "25",
        "keywords": "FBIP, FIP, Splay Trees, Tail Recursion Modulo Cons, Zip Trees, Zippers"
    },
    "The Functional Essence of Imperative Binary Search Trees (Artifact)": {
        "type": "software",
        "key": "10.5281/zenodo.10790231",
        "author": "Lorenzen, Anton and Leijen, Daan and Swierstra, Wouter and Lindley, Sam",
        "title": "The Functional Essence of Imperative Binary Search Trees (Artifact)",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10790231",
        "abstract": "    <p>Our artifact contains implementations of the binary search tree algorithms discussed in the paper in Koka, C, OCaml and Haskell as well as a benchmarking setup to reproduce our numbers. We also include the AddressC proofs of all lemmas in the paper.</p>",
        "keywords": "FBIP, FIP, Splay Trees, Tail Recursion Modulo Cons, Zip Trees, Zippers"
    },
    "Compositional Semantics for Shared-Variable Concurrency": {
        "type": "article",
        "key": "10.1145/3656399",
        "author": "Svyatlovskiy, Mikhail and Mermelstein, Shai and Lahav, Ori",
        "title": "Compositional Semantics for Shared-Variable Concurrency",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656399",
        "doi": "10.1145/3656399",
        "abstract": "We revisit the fundamental problem of defining a compositional semantics for a concurrent programming language under sequentially consistent memory with the aim of equating the denotations of pieces of code if and only if these pieces induce the same behavior under all program contexts. While the denotational semantics presented by Brookes [Information and Computation 127, 2 (1996)] has been considered a definitive solution, we observe that Brookes's full abstraction result crucially relies on the availability of an impractical whole-memory atomic read-modify-write instruction. In contrast, we consider a language with standard primitives, which apply to a single variable. For that language, we propose an alternative denotational semantics based on traces that track program write actions together with the writes expected from the environment, and equipped with several closure operators to achieve necessary abstraction. We establish the adequacy of the semantics, and demonstrate full abstraction for the case that the analyzed code segment is loop-free. Furthermore, we show that by including a whole-memory atomic read in the language, one obtains full abstraction for programs with loops. To gain confidence, our results are fully mechanized in Coq.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "169",
        "numpages": "24",
        "keywords": "Compiler Optimizations, Concurrency, Denotational Semantics, Shared-Memory"
    },
    "Coq Mechanization for \"Compositional Semantics for Shared-Variable Concurrency\" (PLDI 2024)": {
        "type": "software",
        "key": "10.5281/zenodo.10925596",
        "author": "Svyatlovskiy, Mikhail and Mermelstein, Shai and Lahav, Ori",
        "title": "Coq Mechanization for \"Compositional Semantics for Shared-Variable Concurrency\" (PLDI 2024)",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10925596",
        "abstract": "    <p>Coq mechanization for the paper \u201cCompositional Semantics for Shared-Variable Concurrency\u201d (PLDI 2024)</p>",
        "keywords": "Compiler Optimizations, Concurrency, Denotational Semantics, Shared-Memory"
    },
    "Falcon: A Fused Approach to Path-Sensitive Sparse Data Dependence Analysis": {
        "type": "article",
        "key": "10.1145/3656400",
        "author": "Yao, Peisen and Zhou, Jinguo and Xiao, Xiao and Shi, Qingkai and Wu, Rongxin and Zhang, Charles",
        "title": "Falcon: A Fused Approach to Path-Sensitive Sparse Data Dependence Analysis",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656400",
        "doi": "10.1145/3656400",
        "abstract": "This paper presents a scalable path- and context-sensitive data dependence analysis. The key is to address the aliasing-path-explosion problem when enforcing a path-sensitive memory model. Specifically, our approach decomposes the computational efforts of disjunctive reasoning into 1) a context- and semi-path-sensitive analysis that concisely summarizes data dependence as the symbolic and storeless value-flow graphs, and 2) a demand-driven phase that resolves transitive data dependence over the graphs, piggybacking the computation of fully path-sensitive pointer information with the resolution of data dependence of interest. We have applied the approach to two clients, namely thin slicing and value-flow bug finding. Using a suite of 16 C/C++ programs ranging from 13 KLoC to 8 MLoC, we compare our techniques against a diverse group of state-of-the-art analyses, illustrating the significant precision and scalability advantages of our approach.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "170",
        "numpages": "26",
        "keywords": "data dependence analysis, path-sensitive analysis"
    },
    "Allo: A Programming Model for Composable Accelerator Design": {
        "type": "software",
        "key": "10.5281/zenodo.10961342",
        "author": "Chen, Hongzheng and Zhang, Niansong and Xiang, Shaojie and Zeng, Zhichen and Dai, Mengjia and Zhang, Zhiru",
        "title": "Allo: A Programming Model for Composable Accelerator Design",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10961342",
        "abstract": "    <p>This artifact contains scripts for setting up environments and reproducing results presented in the PLDI 2024 paper entitled \u201cAllo: A Programming Model for Composable Accelerator Design\u201d. Please refer to our github repo for instructions on how to install and run the artifact. https://github.com/cornell-zhang/allo-pldi24-artifact</p>",
        "keywords": "accelerator design language, compiler optimization, Hardware accelerators, schedule language"
    },
    "VESTA: Power Modeling with Language Runtime Events": {
        "type": "article",
        "key": "10.1145/3656402",
        "author": "Raskind, Joseph and Babakol, Timur and Mahmoud, Khaled and Liu, Yu David",
        "title": "VESTA: Power Modeling with Language Runtime Events",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656402",
        "doi": "10.1145/3656402",
        "abstract": "Power modeling is an essential building block for computer systems in support of energy optimization, energy profiling, and energy-aware application development. We introduce VESTA, a novel approach to modeling the power consumption of applications with one key insight: language runtime events are often correlated with a sustained level of power consumption. When compared with the established approach of power modeling based on hardware performance counters (HPCs), VESTA has the benefit of solely requiring application-scoped information and enabling a higher level of explainability, while achieving comparable or even higher precision. Through experiments performed on 37 real-world applications on the Java Virtual Machine (JVM), we find the power model built by VESTA is capable of predicting energy consumption with a mean absolute percentage error of 1.56\\%, while the monitoring of language runtime events incurs small performance and energy overhead.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "172",
        "numpages": "26",
        "keywords": "BPF, Java virtual machines, language runtimes, power modeling"
    },
    "Reproduction Artifact for VESTA": {
        "type": "software",
        "key": "10.5281/zenodo.10965986",
        "author": "Raskind, Joseph and Babakol, Timur and Mahmoud, Khaled and Liu, Yu David",
        "title": "Reproduction Artifact for VESTA",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10965986",
        "abstract": "    <p>An artifact that reproduces the VESTA model</p>",
        "keywords": "BPF, Java virtual machines, language runtimes, power modeling"
    },
    "Mechanised Hypersafety Proofs about Structured Data": {
        "type": "article",
        "key": "10.1145/3656403",
        "author": "Gladshtein, Vladimir and Zhao, Qiyuan and Ahrens, Willow and Amarasinghe, Saman and Sergey, Ilya",
        "title": "Mechanised Hypersafety Proofs about Structured Data",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656403",
        "doi": "10.1145/3656403",
        "abstract": "Arrays are a fundamental abstraction to represent collections of data. It is often possible to exploit structural properties of the data stored in an array (e.g., repetition or sparsity) to develop a specialised representation optimised for space efficiency. Formally reasoning about correctness of manipulations with such structured data is challenging, as they are often composed of multiple loops with non-trivial invariants.   In this work, we observe that specifications for structured data manipulations can be phrased as hypersafety properties, i.e., predicates that relate traces of k programs. To turn this observation into an effective verification methodology, we developed the Logic for Graceful Tensor Manipulation (LGTM), a new Hoare-style relational separation logic for specifying and verifying computations over structured data. The key enabling idea of LGTM is that of parametrised hypersafety specifications that allow the number k of the program components to depend on the program variables. We implemented LGTM as a foundational embedding into Coq, mechanising its rules, meta-theory, and the proof of soundness. Furthermore, we developed a library of domain-specific tactics that automate computer-aided hypersafety reasoning, resulting in pleasantly short proof scripts that enjoy a high degree of reuse. We argue for the effectiveness of relational reasoning about structured data in LGTM by specifying and mechanically proving correctness of 13 case studies including computations on compressed arrays and efficient operations over multiple kinds of sparse tensors.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "173",
        "numpages": "24",
        "keywords": "mechanised proofs, relational logic, sparse data structures"
    },
    "LGTM: the Logic for Graceful Tensor Manipulation": {
        "type": "software",
        "key": "10.5281/zenodo.10951930",
        "author": "Gladshtein, Vladimir and Zhao, Qiyuan and Ahrens, Willow and Amarasinghe, Saman and Sergey, Ilya",
        "title": "LGTM: the Logic for Graceful Tensor Manipulation",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10951930",
        "abstract": "    <p>This is the research artefact for the paper Mechanised Hypersafety Proofs about Structured Data to appear in the proceedings of the 45th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2024).</p>",
        "keywords": "coq, mechanised proofs, relational logic, sparse data structures"
    },
    "Refined Input, Degraded Output: The Counterintuitive World of Compiler Behavior": {
        "type": "article",
        "key": "10.1145/3656404",
        "author": "Theodoridis, Theodoros and Su, Zhendong",
        "title": "Refined Input, Degraded Output: The Counterintuitive World of Compiler Behavior",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656404",
        "doi": "10.1145/3656404",
        "abstract": "To optimize a program, a compiler needs precise information about it. Significant effort is dedicated to improving the ability of compilers to analyze programs, with the expectation that more information results in better optimization. But this assumption does not always hold: due to unexpected interactions between compiler components and phase ordering issues, sometimes more information leads to worse optimization. This can lead to wasted research and engineering effort whenever compilers cannot efficiently leverage additional information. In this work, we systematically examine the extent to which additional information can be detrimental to compilers. We consider two types of information: dead code, i.e., whether a program location is unreachable, and value ranges, i.e., the possible values a variable can take at a specific program location. Given a seed program, we refine it with additional information and check whether this degrades the output. Based on this approach, we develop a fully automated and effective testing method for identifying such issues, and through an extensive evaluation and analysis, we quantify their existence and prevalence in widely used compilers. In particular, we have reported 59 cases in GCC and LLVM, of which 55 have been confirmed or fixed so far, highlighting the practical relevance and value of our findings. This work\u2019s fresh perspective opens up a new direction in understanding and improving compilers.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "174",
        "numpages": "21",
        "keywords": "Automated Compiler Testing, Missed Compiler Optimizations"
    },
    "PLDI 2024 Artifact for \"Refined Input, Degraded Output: The Counterintuitive World of Compiler Behavior": {
        "type": "software",
        "key": "10.5281/zenodo.10808465",
        "author": "Theodoridis, Theodoros and Su, Zhendong",
        "title": "PLDI 2024 Artifact for \"Refined Input, Degraded Output: The Counterintuitive World of Compiler Behavior",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10808465",
        "abstract": "    <p>Ths artifact consists of a docker image with instructions, the dataset, and the code necessary to reproduce the evaluation of Refined Input, Degraded Output: The Counterintuitive World of Compiler Behavior PLDI 2024.</p>",
        "keywords": "automated compiler testing, missed compiler optimizations"
    },
    "Jacdac: Service-Based Prototyping of Embedded Systems": {
        "type": "article",
        "key": "10.1145/3656405",
        "author": "Ball, Thomas and de Halleux, Peli and Devine, James and Hodges, Steve and Moskal, Micha\\l",
        "title": "Jacdac: Service-Based Prototyping of Embedded Systems",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656405",
        "doi": "10.1145/3656405",
        "abstract": "The traditional approach to programming embedded systems is monolithic: firmware on a microcontroller contains both application code and the drivers needed to communicate with sensors and actuators, using low-level protocols such as I2C, SPI, and RS232. In comparison, software development for the cloud has moved to a service-based development and operation paradigm: a service provides a discrete unit of functionality that can be accessed remotely by an application, or other service, but is independently managed and updated.        We propose, design, implement, and evaluate a service-based approach to prototyping embedded systems called Jacdac. Jacdac defines a service specification language, designed especially for embedded systems, along with a host of specifications for a variety of sensors and actuators. With Jacdac, each sensor/actuator in a system is paired with a low-cost microcontroller that advertises the services that represent the functionality of the underlying hardware over an efficient and low-cost single-wire bus protocol. A separate microcontroller executes the user's application program, which is a client of the Jacdac services on the bus.        Our evaluation shows that Jacdac supports a service-based abstraction for sensors/actuators at low cost and reasonable performance, with many benefits for prototyping: ease of use via the automated discovery of devices and their capabilities, substitution of same-service devices for each other, as well as high-level programming, monitoring, and debugging. We also report on the experience of bringing Jacdac to commercial availability via third-party manufacturers.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "175",
        "numpages": "24",
        "keywords": "embedded systems, microcontrollers, plug-and-play, services"
    },
    "Jacdac: Service-based Prototyping of Embedded Systems (PLDI 2024 Artifact Evaluation)": {
        "type": "software",
        "key": "10.5281/zenodo.10892762",
        "author": "Ball, Thomas and de Halleux, Peli and Devine, James and Hodges, Steve and Moskal, Micha\\l",
        "title": "Jacdac: Service-based Prototyping of Embedded Systems (PLDI 2024 Artifact Evaluation)",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10892762",
        "abstract": "    <p>This artifact allows others to reproduce and explore the results seen in \u201cJacdac: Service-based Prototyping of Embedded Systems\u201d. The artifact contains a prebuilt docker image and the Dockerfile source used to produce the prebuilt docker image. Evaluators should follow the README contained in this artifact for complete instruction.</p>",
        "keywords": "embedded systems, microcontrollers, plug-and-play, services"
    },
    "Don\u2019t Write, but Return: Replacing Output Parameters with Algebraic Data Types in C-to-Rust Translation": {
        "type": "article",
        "key": "10.1145/3656406",
        "author": "Hong, Jaemin and Ryu, Sukyoung",
        "title": "Don\u2019t Write, but Return: Replacing Output Parameters with Algebraic Data Types in C-to-Rust Translation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656406",
        "doi": "10.1145/3656406",
        "abstract": "Translating legacy system programs from C to Rust is a promising way to enhance their reliability. To alleviate the burden of manual translation, automatic C-to-Rust translation is desirable. However, existing translators fail to generate Rust code fully utilizing Rust\u2019s language features, including algebraic data types. In this work, we focus on tuples and Option/Result types, an important subset of algebraic data types. They are used as functions\u2019 return types to represent those returning multiple values and those that may fail. Due to the absence of these types, C programs use output parameters, i.e., pointer-type parameters for producing outputs, to implement such functions. As output parameters make code less readable and more error-prone, their use is discouraged in Rust. To address this problem, this paper presents a technique for removing output parameters during C-to-Rust translation. This involves three steps: (1) syntactically translating C code to Rust using an existing translator; (2) analyzing the Rust code to extract information related to output parameters; and (3) transforming the Rust code using the analysis result. The second step poses several challenges, including the identification and classification of output parameters. To overcome these challenges, we propose a static analysis based on abstract interpretation, complemented by the notion of abstract read/write sets, which approximate the sets of read/written pointers, and two sensitivities: write set sensitivity and nullity sensitivity. Our evaluation shows that the proposed technique is (1) scalable, with the analysis and transformation of 190k LOC within 213 seconds, (2) useful, with the detection of 1,670 output parameters across 55 real-world C programs, and (3) mostly correct, with 25 out of 26 programs passing their test suites after the transformation.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "176",
        "numpages": "25",
        "keywords": "Algebraic Data Type, Automatic Translation, C, Output Parameter, Rust"
    },
    "Don't Write, but Return: Replacing Output Parameters with Algebraic Data Types in C-to-Rust Translation (Artifact)": {
        "type": "software",
        "key": "10.5281/zenodo.10795858",
        "author": "Hong, Jaemin and Ryu, Sukyoung",
        "title": "Don't Write, but Return: Replacing Output Parameters with Algebraic Data Types in C-to-Rust Translation (Artifact)",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10795858",
        "abstract": "    <p>This artifact is for the paper Don\u2019t Write, but Return: Replacing Output Parameters with Algebraic Data Types in C-to-Rust Translation. It introduces static analysis that identifies output parameters and program transformation that removes these parameters, enhancing automatic C-to-Rust translation. The tool, Nopcrat, which embodies the proposed method, is developed in Rust. Our evaluation dataset comprises 35 real-world C programs. You need the capability to run Docker containers, as the artifact is provided via a Docker image. To replicate the study\u2019s results, a computer with at least 32 GB of RAM is necessary.</p><p>Nopcrat translates C code to Rust while replacing output parameters with Rust\u2019s algebraic data types. It consists of four components: a modified version of the C2Rust translator, Extern2use, a static analyzer, and a code transformer. The translator translates C code to Rust. Extern2use replaces extern declarations in C2Rust-generated code with use. The analyzer analyzes the Rust code to identify output parameters and stores the information in a JSON file. The transformer removes output parameters in the Rust code using the analysis results.</p><p>The artifact supports the claims made in Section 5 of the paper by allowing the reproduction of the experimental results.</p>",
        "keywords": "Algebraic Data Type, Automatic Translation, C, Output Parameter, Rust"
    },
    "Quantitative Robustness for Vulnerability Assessment": {
        "type": "software",
        "key": "10.5281/zenodo.10953315",
        "author": "Girol, Guillaume and Lacombe, Guilhem and Bardin, S\\'{e}bastien",
        "title": "Quantitative Robustness for Vulnerability Assessment",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10953315",
        "abstract": "    <p>Most software analysis techniques focus on bug reachability. However, this approach is not ideal for security evaluation as it does not take into account the difficulty of triggering said bugs. The recently introduced notion of robust reachability tackles this issue by distinguishing between bugs that can be reached independently from uncontrolled inputs, from those that cannot. Yet, this qualitative notion is too strong in practice as it cannot distinguish mostly replicable bugs from truly unrealistic ones.</p><p>In this work we propose a more flexible quantitative version of robust reachability together with a dedicated form of symbolic execution, in order to automatically measure the difficulty of triggering bugs. This quantitative robust symbolic execution (QRSE) relies on a variant of model counting, called functional E-MAJSAT, which allows to account for the asymmetry between attacker-controlled and uncontrolled variables. While this specific model counting problem has been studied in AI research fields such as Bayesian networks, knowledge representation and probabilistic planning, its use within the context of formal verification presents a new set of challenges. We show the applicability of our solutions through security-oriented case studies, including real-world vulnerabilities such as CVE-2019-20839 from libvncserver.</p>",
        "keywords": "Security, Static Analysis, Verification (automated)"
    },
    "Automated Verification of Fundamental Algebraic Laws": {
        "type": "software",
        "key": "10.5281/zenodo.10949342",
        "author": "Zakhour, George and Weisenburger, Pascal and Salvaneschi, Guido",
        "title": "Automated Verification of Fundamental Algebraic Laws",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10949342",
        "abstract": "    <p>Propel \u2013 Automated Verification of Fundamental Algebraic Laws</p><p>Artifact for the paper #174 \u201cAutomated Verification of Fundamental Algebraic Laws\u201d</p><h2 id=\"claims-addressed-by-this-artifact\">CLAIMS ADDRESSED BY THIS ARTIFACT</h2><p>This artifact addresses the following claims made in the paper:</p><ul><li><p>Propel is implemented in Scala for proving a subset of Scala (cf.&nbsp;\u201cUSING PROPEL AS A SCALA DSL\u201d) translated into an intermediate representation (cf.&nbsp;\u201cUSING PROPEL STANDALONE (OUTSIDE OF SCALA)\u201d).</p></li><li><p>Propel outperforms cvc5, vawpire, Zeno, HipSpec, and CycleQ as claimed in Table 1 Section 4 (cf. \u201cRUNNING THE BENCHMARKS\u201d) on 142 algebraic properties.</p></li><li><p>The implementation of Propel is in Scala 3 and is about 10 K lines long throughout 46 Scala files (cf.&nbsp;\u201cSTRUCTURE OF THE PROPEL SOURCE CODE\u201d).</p></li></ul><h2 id=\"getting-started\">GETTING STARTED</h2><h3 id=\"building-and-loading-the-docker-image\">BUILDING AND LOADING THE DOCKER IMAGE</h3><p>We provide you with <code>propel.tar.xz</code>, which is a pre-built container image that contains all necessary programs. To load, run the following command:</p><pre><code>$ docker load &lt; propel.tar.xz</code></pre><p>Further, we also provide the option to build the contain anew. To build, run the following command which takes between 10 and 20 minutes:</p><pre><code>$ docker build -t propel .</code></pre><p>Rebuilding the image may not work on Apple M1 machines because of incomplete emulation of system calls (specifically the inotify kernel subsystem). Hence, we recommend rebuilding the image on a platform fully supported by Docker, like x86-64 systems.</p><h3 id=\"checking-if-the-container-and-the-relevant-programs-run-correctly\">CHECKING IF THE CONTAINER AND THE RELEVANT PROGRAMS RUN CORRECTLY</h3><p>We provide a script that runs fast checks on Propel and the other provers (HipSpec, Zeno, CycleQ, cvc5, Vampire) used in the evaluation.</p><p>The check verifies commutativity of natural number addition \u2013 a task which all programs are able to prove correct quickly. The following command runs the check:</p><pre><code>$ docker run -it --rm propel /check_image/check</code></pre><p>If you see in green the line \u201cCheck Done\u201d at the end, the container is behaving as expected.</p><p>The check will show the provers\u2019 output, which should look similar to the following (shortened) excerpt:</p><pre><code>Checking Zeno[...]Searching for proofs... Proved \"CommutativityAddition.prop_add_comm\u202f: add x y = add y x\"[...]Checking HipSpec[...]Proved:    add m n == add n m    add m (add n o) == add n (add m o)    prop_add_comm {- add x y == add y x -}Checking cvc5\"comm nat_add2p\"unsatChecking Vampire[...]\\% Termination reason: Refutation[...]Checking CycleQ[...]Attempting to prove: prop_nat_add1_rightidSuccess!Checking Propel\u2714 Check successful.Check Done</code></pre><p>Note that cvc5 and Vampire report <code>unsat</code> or <code>Refutation</code>, respectively. This is because properties are verified by SMT solvers by finding a counterexample for their negation.</p><h2 id=\"step-by-step-instructions\">STEP-BY-STEP INSTRUCTIONS</h2><h3 id=\"compiling-propel\">COMPILING PROPEL</h3><p>The provided container already contains a binary executable of Propel.</p><p>To compile Propel to Java bytecode yourself, run the following command:</p><pre><code>$ docker run -it --rm propel bash -c 'cd /propel; sbt clean compile'</code></pre><p>To compile Propel to a native binary yourself, run the following command:</p><pre><code>$ docker run -it --rm propel bash -c 'cd /propel; sbt clean nativeLink'</code></pre><p>Compiling Propel, to bytecode or to a native executable, may not work inside the Docker container on Apple M1 machines for the reasons mentioned earlier.</p><p>The resulting binary is at <code>/propel/.native/target/scala-3.3.0/propel</code>. The <code>propel</code> executable in the PATH is already symlinked to that binary file. Hence, by default, you can just run <code>propel</code>.</p><h3 id=\"testing-propel\">TESTING PROPEL</h3><p>To run the tests in Propel, execute:</p><pre><code>$ docker run -it --rm propel bash -c 'cd /propel &amp;\\&amp; sbt test'</code></pre><p>Running the Propel tests may not work inside the Docker container on Apple M1 machines for the reasons mentioned earlier.</p><p>Note that running all unit tests can take several minutes. The output should look similar to the following (shortened) excerpt:</p><pre><code>[info] SuccessfulPropertyChecks:[info] - nat_add2p[info] - nat_add3p[info] - nat_mult2p[info] - bv_add[...][info] FailingPropertyChecks:[info] - nat_add2p_acc\u202f!!! IGNORED\u202f!!![info] - nat_add3p_acc\u202f!!! IGNORED\u202f!!![...][info] Total number of tests run: 49[info] Suites: completed 2, aborted 0[info] Tests: succeeded 49, failed 0, canceled 0, ignored 15, pending 0</code></pre><p>The <code>SuccessfulPropertyChecks</code> contain the examples for which Propel can verify all properties. The <code>FailingPropertyChecks</code> contain the examples for which Propel is unable to verify all properties, hence their unit tests are disabled (<code>IGNORED</code>).</p><h3 id=\"running-the-benchmarks\">RUNNING THE BENCHMARKS</h3><p>The benchmarks in Table 1 on pages 15, 16 and 16 can be re-executed with the container. The number of the properties that (1) could be proven, (2) could not be proven and (3) timed out should match the content of tables and the figures. The given time may differ depending on the system where the benchmarks are run. Due to the timeout of one minute, not only the amount of seconds can differ but also the type of the result. It could be the case that the benchmark succeeds or fails in less than 60s on one setup but takes more than 60s on a different setup, in which case it would time out.</p><p>To execute the benchmarks on HipSpec, run:</p><pre><code>$ docker run -it --rm propel /benchmarks/hipspec/run</code></pre><p>To execute the benchmarks on Zeno, run:</p><pre><code>$ docker run -it --rm propel /benchmarks/zeno/run</code></pre><p>To execute the benchmarks on CycleQ, run:</p><pre><code>$ docker run -it --rm propel /benchmarks/cycleq/run</code></pre><p>To execute the benchmarks on cvc5, run:</p><pre><code>$ docker run -it --rm propel /benchmarks/cvc5/run</code></pre><p>To execute the benchmarks on Vampire, run:</p><pre><code>$ docker run -it --rm propel /benchmarks/vampire/run</code></pre><p>To execute the benchmarks on Propel, run:</p><pre><code>$ docker run -it --rm propel /benchmarks/propel/run</code></pre><p>To execute the benchmarks on Propel without inequalities, run:</p><pre><code>$ docker run -it --rm propel /benchmarks/propel/run_no_ineq</code></pre><p>To execute the benchmarks that count the explored auxilliary lemmas, run:</p><pre><code>$ docker run -it --rm propel /benchmarks/propel/run_count_lemmas</code></pre><p>The results in Table 1 on pages 15, 16 and 16 correspond to one line from the output of each previous command.</p><h3 id=\"using-propel-as-a-scala-dsl\">USING PROPEL AS A SCALA DSL</h3><p>Propel as described in Section 2.1 is a DSL in Scala. To experiment with the DSL, we invite you take a look into <code>/propel/src/test/scala/propel/ScalaExamplesNat.scala</code>, <code>/propel/src/test/scala/propel/ScalaExamplesNum.scala</code> and <code>/propel/src/test/scala/propel/ScalaExamplesList.scala</code> inside the container.</p><p>As an example, you can execute the following commands to run a shell, explore the files and recompile the project:</p><pre><code>$ docker run -it --rm propel bash                               # open a shell$ nano /propel/src/test/scala/propel/ScalaExamplesList.scala    # open the file# edit and save the file$ cd /propel &amp;\\&amp; sbt Test/compile                                # recompile</code></pre><p>Compiling the examples may not work inside the Docker container on Apple M1 machines for the reasons mentioned earlier.</p><p>You may define your own function using the following syntax:</p><pre><code>def myFunction = prop[(FunctionProperties)\u202f:= (T1, T1) =&gt;: T2] {(x, y) =&gt; body}// ordef myRecursiveFunction = prop.rec[(FunctionProperties)\u202f:= (T1, T1) =&gt;: T2] {myRecursiveFunction =&gt; (x, y) =&gt; body}</code></pre><p>Here, <code>myFunction</code> is the name of the function, <code>FunctionProperties</code> is a list of function properties the function has (separated by <code>&amp;</code>), <code>T1</code> is the type of the arguments of the binary function, <code>T2</code> is the return type of the function, <code>x</code> and <code>y</code> are the names of the function arguments, and <code>body</code> is the function body.</p><p>The function properties are chosen from the following list: <code>Comm</code>, <code>Assoc</code>, <code>Idem</code>, <code>Sel</code>, <code>Refl</code>, <code>Antisym</code>, <code>Trans</code>, <code>Conn</code>, and <code>Sym</code>. Their semantics is defined in Section 2.2.</p><p>If Propel is able to prove the properties that the function is annotated with, then compilation succeeds. If the properties cannot be proven, then a compilation error indicates which property could not be proven</p><p>We hope that the integration into Scala makes the artifact easily usable by other researchers, either (1) by directly using the DSL to check algebraic and relational properties of their programs or (2) by building on Propel\u2019s verification engine. To facilitate the latter, the implementation of Propel\u2019s Scala DSL (<code>propel.dsl</code> package) is separated from the verification mechanism (<code>propel.evaluator</code> package), which researchers can adopt independently of the Scala integration (an overview of the package structure is in the last section).</p><h4 id=\"example\">Example</h4><p>You can create a file in <code>/propel/src/test/scala/propel</code> with the following preamble:</p><pre><code>package propelimport propel.dsl.scala.*</code></pre><p>You can copy the <code>add1</code> example from the paper (Listing 1):</p><pre><code>enum \u2115:  case Z  case S(pred: \u2115)def add1 = prop.rec[(Comm \\&amp; Assoc)\u202f:= (\u2115, \u2115) =&gt;: \u2115]: add1 =&gt;  case (\u2115.Z, y) =&gt; y  case (\u2115.S(x), y) =&gt; \u2115.S(add1(x, y))</code></pre><p>To define custom properties to be proven, you can add a <code>props</code> clause to the definition of <code>add1</code>. For instance, the following example proves the left and right identity laws:</p><pre><code>def add1 = prop.rec[(Comm \\&amp; Assoc)\u202f:= (\u2115, \u2115) =&gt;: \u2115]: add1 =&gt;  props(    (x: \u2115) =&gt; add1(\u2115.Z, x) =:= x, // left identity    (x: \u2115) =&gt; add1(x, \u2115.Z) =:= x, // right identity  ):    case (\u2115.Z, y) =&gt; y    case (\u2115.S(x), y) =&gt; \u2115.S(add1(x, y))</code></pre><p>You can execute <code>sbt Test/compile</code> to check all the annotated properties.</p><h3 id=\"using-propel-standalone-outside-of-scala\">USING PROPEL STANDALONE (OUTSIDE OF SCALA)</h3><p>Propel can be directly reused as a verification tool in other projects (without the Scala and JVM dependency) through the <code>propel</code> binary. The binary consumes ASTs of Propel\u2019s calculus in an S-expression-based syntax.</p><p>Our Scala implementation of the full surface language also follows the approach of translating Scala programs to terms in the calculus and passing them to the verification mechanism. A similar approach can be adopted by other tools that use Propel. Note that the AST is a bit more low-level then the Scala implementation and the calculus presented in the paper. In particular, the properties that are captured in the type of a function need to be propagated to the call sites of the function, i.e., function calls are syntactically annotated with the properties that should hold for them. The concrete format is described in the FORMAT.md file.</p><p>We provide all benchmarks in this format in the <code>/benchmarks/propel</code> directory. For example, the <code>nat_add1_comm.propel</code> is a direct translation of the <code>add1</code> function of Listing 1. This file can be checked by running:</p><pre><code>propel -f /benchmarks/propel/nat_add1_comm.propel</code></pre><p>Additional information about the proof attempts can be shown using the <code>-d</code> and <code>-r</code> flags.</p><h3 id=\"structure-of-the-propel-source-code\">STRUCTURE OF THE PROPEL SOURCE CODE</h3><p>Propel is organized into the following packages:</p><ul><li><code>ast</code>: Abstract syntax tree definitions for the verifier</li><li><code>dsl</code>: Scala DSL</li><li><code>evaluator</code>: Rewrite engine (used by an implementation of the calculus\u2019 dynamic semantics and by the verifier)</li><li><code>evaluator.properties</code>: Verifier for algebraic and relational properties (call <code>evaluator.properties.check</code> on an <code>ast.Term</code> to verify properties)</li><li><code>parser</code>: Parser for Propel\u2019s serialization format (as used by the benchmarks)</li><li><code>printing</code>: Pretty-printer for Propel ASTs</li><li><code>typer</code>: Standard type checker (not checking algebraic and relational properties)</li><li><code>util</code>: Small, useful definitions</li></ul>",
        "keywords": "Algebraic Properties, Type Systems, Verification"
    },
    "GenSQL: A Probabilistic Programming System for Querying Generative Models of Database Tables": {
        "type": "article",
        "key": "10.1145/3656409",
        "author": "Huot, Mathieu and Ghavami, Matin and Lew, Alexander K. and Schaechtle, Ulrich and Freer, Cameron E. and Shelby, Zane and Rinard, Martin C. and Saad, Feras A. and Mansinghka, Vikash K.",
        "title": "GenSQL: A Probabilistic Programming System for Querying Generative Models of Database Tables",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656409",
        "doi": "10.1145/3656409",
        "abstract": "This article presents GenSQL, a probabilistic programming system for querying probabilistic generative models of database tables. By augmenting SQL with only a few key primitives for querying probabilistic models, GenSQL enables complex Bayesian inference workflows to be concisely implemented. GenSQL\u2019s query planner rests on a unified programmatic interface for interacting with probabilistic models of tabular data, which makes it possible to use models written in a variety of probabilistic programming languages that are tailored to specific workflows. Probabilistic models may be automatically learned via probabilistic program synthesis, hand-designed, or a combination of both. GenSQL is formalized using a novel type system and denotational semantics, which together enable us to establish proofs that precisely characterize its soundness guarantees. We evaluate our system on two case real-world studies\u2014an anomaly detection in clinical trials and conditional synthetic data generation for a virtual wet lab\u2014and show that GenSQL more accurately captures the complexity of the data as compared to common baselines. We also show that the declarative syntax in GenSQL is more concise and less error-prone as compared to several alternatives. Finally, GenSQL delivers a 1.7-6.8x speedup compared to its closest competitor on a representative benchmark set and runs in comparable time to hand-written code, in part due to its reusable optimizations and code specialization.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "179",
        "numpages": "26",
        "keywords": "AutoML, Bayesian data analysis, generative modeling, probabilistic programming, query language, semantics and correctness"
    },
    "PLDI artifact evaluation for \"GenSQL: A Probabilistic Programming System for Querying Generative Models of Database Tables": {
        "type": "software",
        "key": "10.5281/zenodo.10949799",
        "author": "Huot, Mathieu and Ghavami, Matin and Lew, Alexander K. and Schaechtle, Ulrich and Freer, Cameron E. and Shelby, Zane and Rinard, Martin C. and Saad, Feras A. and Mansinghka, Vikash K.",
        "title": "PLDI artifact evaluation for \"GenSQL: A Probabilistic Programming System for Querying Generative Models of Database Tables",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10949799",
        "abstract": "    <p>This tarball provides evaluators with all means necessary to fully reproduce the results we show in the paper. We recommend users to check out the official GitHub repositories for GenSQL at https://github.com/OpenGen/GenSQL.query.</p>",
        "keywords": "Experimental evaluation"
    },
    "Daedalus: Safer Document Parsing": {
        "type": "article",
        "key": "10.1145/3656410",
        "author": "Diatchki, Iavor S. and Dodds, Mike and Goldstein, Harrison and Harris, Bill and Holland, David A. and Razet, Benoit and Schlesinger, Cole and Winwood, Simon",
        "title": "Daedalus: Safer Document Parsing",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656410",
        "doi": "10.1145/3656410",
        "abstract": "Despite decades of contributions to the theoretical foundations of parsing and the many tools available to aid    in parser development, many security attacks in the wild still exploit parsers. The issues are myriad\u2014flaws    in memory management in contexts lacking memory safety, flaws in syntactic or semantic validation of    input, and misinterpretation of hundred-page-plus standards documents. It remains challenging to build and    maintain parsers for common, mature data formats.    In response to these challenges, we present Daedalus, a new domain-specific language (DSL) and toolchain    for writing safe parsers. Daedalus is built around functional-style parser combinators, which suit the rich data    dependencies often found in complex data formats. It adds domain-specific constructs for stream manipulation",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "180",
        "numpages": "25",
        "keywords": "Format definition languages, NITF, PDF, binary data formats"
    },
    "Daedalus": {
        "type": "software",
        "key": "10.5281/zenodo.10966813",
        "author": "Diatchki, Iavor S. and Dodds, Mike and Goldstein, Harrison and Harris, Bill and Holland, David A. and Razet, Benoit and Schlesinger, Cole and Winwood, Simon",
        "title": "Daedalus",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10966813",
        "abstract": "    <p>Daedalus artifact submitted to PLDI</p>",
        "keywords": "parser-generator, parsing"
    },
    "Descend: A Safe GPU Systems Programming Language": {
        "type": "article",
        "key": "10.1145/3656411",
        "author": "K\\\"{o}pcke, Bastian and Gorlatch, Sergei and Steuwer, Michel",
        "title": "Descend: A Safe GPU Systems Programming Language",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656411",
        "doi": "10.1145/3656411",
        "abstract": "Graphics Processing Units (GPU) offer tremendous computational power by following a throughput oriented paradigm where many thousand computational units operate in parallel. Programming such massively parallel hardware is challenging. Programmers must correctly and efficiently coordinate thousands of threads and their accesses to various shared memory spaces. Existing mainstream GPU programming languages, such as CUDA and OpenCL, are based on C/C++ inheriting their fundamentally unsafe ways to access memory via raw pointers. This facilitates easy to make, but hard to detect bugs, such as data races and deadlocks. In this paper, we present Descend: a safe GPU programming language. In contrast to prior safe high-level GPU programming approaches, Descend is an imperative GPU systems programming language in the spirit of Rust, enforcing safe CPU and GPU memory management in the type system by tracking Ownership and Lifetimes. Descend introduces a new holistic GPU programming model where computations are hierarchically scheduled over the GPU\u2019s execution resources: grid, blocks, warps, and threads. Descend\u2019s extended Borrow checking ensures that execution resources safely access memory regions without data races. For this, we introduced views describing safe parallel access patterns of memory regions, as well as atomic variables. For memory accesses that can\u2019t be checked by our type system, users can annotate limited code sections as unsafe. We discuss the memory safety guarantees offered by Descend and evaluate our implementation using multiple benchmarks, demonstrating that Descend is capable of expressing real-world GPU programs showing competitive performance compared to manually written CUDA programs lacking Descend\u2019s safety guarantees.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "181",
        "numpages": "24",
        "keywords": "GPU programming, language design, memory safety, type systems"
    },
    "Bit Blasting Probabilistic Programs": {
        "type": "article",
        "key": "10.1145/3656412",
        "author": "Garg, Poorva and Holtzen, Steven and Van den Broeck, Guy and Millstein, Todd",
        "title": "Bit Blasting Probabilistic Programs",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656412",
        "doi": "10.1145/3656412",
        "abstract": "Probabilistic programming languages (PPLs) are an expressive means for creating and reasoning about probabilistic models. Unfortunately hybrid probabilistic programs that involve both continuous and discrete structures are not well supported by today\u2019s PPLs. In this paper we develop a new approximate inference algorithm for hybrid probabilistic programs that first discretizes the continuous distributions and then performs discrete inference on the resulting program. The key novelty is a form of discretization that we call bit blasting, which uses a binary representation of numbers such that a domain of 2b discretized points can be succinctly represented as a discrete probabilistic program over poly(b) Boolean random variables. Surprisingly, we prove that many common continuous distributions can be bit blasted in a manner that incurs no loss of accuracy over an explicit discretization and supports efficient probabilistic inference. We have built a probabilistic programming system for hybrid programs called HyBit, which employs bit blasting followed by discrete probabilistic inference. We empirically demonstrate the benefits of our approach over existing sampling-based and symbolic inference approaches.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "182",
        "numpages": "24",
        "keywords": "bit blasting, discretization, probabilistic inference"
    },
    "Reproduction Package for Article \"Bit Blasting Probabilistic Programs": {
        "type": "software",
        "key": "10.5281/zenodo.10901544",
        "author": "Garg, Poorva and Holtzen, Steven and Van den Broeck, Guy and Millstein, Todd",
        "title": "Reproduction Package for Article \"Bit Blasting Probabilistic Programs",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10901544",
        "abstract": "    <p>The artifact consists of implementation of HyBit, a probabilistic programming system based on bit blasting. It scales inference for hybrid probabilistic programs with respect to the discrete structure. The artifact also consists of all the necessary scripts to reproduce the experiment in the article \u201cBit Blasting Probabilistic Programs\u201d.</p>",
        "keywords": "bit blasting, discretization, probabilistic inference, probabilistic programming system"
    },
    "Quiver: Guided Abductive Inference of Separation Logic Specifications in Coq": {
        "type": "article",
        "key": "10.1145/3656413",
        "author": "Spies, Simon and G\\\"{a}her, Lennard and Sammler, Michael and Dreyer, Derek",
        "title": "Quiver: Guided Abductive Inference of Separation Logic Specifications in Coq",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656413",
        "doi": "10.1145/3656413",
        "abstract": "Over the past two decades, there has been a great deal of progress on verification of full functional correctness of programs using separation logic, sometimes even producing \u201cfoundational\u201d proofs in proof assistants like Coq.  Unfortunately, even though existing approaches to this problem provide significant support for automated verification, they still incur a significant specification overhead: the user must supply the specification against which the program is verified, and the specification may be long, complex, or tedious to formulate.   In this paper, we introduce Quiver, the first technique for inferring functional correctness specifications in separation logic while simultaneously verifying foundationally that they are correct.  To guide Quiver towards the final specification, we take hints from the user in the form of a specification sketch, and then complete the sketch using inference.  To do so, Quiver introduces a new abductive deductive verification technique, which integrates ideas from abductive inference (for specification inference) together with deductive separation logic automation (for foundational verification).  The result is that users have to provide some guidance, but significantly less than with traditional deductive verification techniques based on separation logic.  We have evaluated Quiver on a range of case studies, including code from popular open-source libraries.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "183",
        "numpages": "25",
        "keywords": "Coq, Iris, abduction, functional correctness, specification inference"
    },
    "Artifact and Appendix for Quiver: Guided Abductive Inference of Separation Logic Specifications in Coq": {
        "type": "software",
        "key": "10.5281/zenodo.10940320",
        "author": "Spies, Simon and G\\\"{a}her, Lennard and Sammler, Michael and Dreyer, Derek",
        "title": "Artifact and Appendix for Quiver: Guided Abductive Inference of Separation Logic Specifications in Coq",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10940320",
        "abstract": "    <p>This is the artifact for \u201cQuiver: Guided Abductive Inference of Separation Logic Specifications in Coq\u201d, submitted to PLDI 2024. It consists of a Coq implementation of a new specification inference technique in separation logic introduced in the paper.</p>",
        "keywords": "abduction, Coq, functional correctness, Iris, specification inference"
    },
    "Program Analysis for Adaptive Data Analysis": {
        "type": "article",
        "key": "10.1145/3656414",
        "author": "Liu, Jiawen and Qu, Weihao and Gaboardi, Marco and Garg, Deepak and Ullman, Jonathan",
        "title": "Program Analysis for Adaptive Data Analysis",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656414",
        "doi": "10.1145/3656414",
        "abstract": "Data analyses are usually designed to identify some property of the population from which the data are drawn, generalizing beyond the specific data sample. For this reason, data analyses are often designed in a way that guarantees that they produce a low generalization error.  That is, they are designed so that the result of a data analysis run on a sample data does not differ too much from the result one would achieve by running the analysis over the entire population.   An adaptive data analysis can be seen as a process composed by multiple queries interrogating some data, where the choice of which query to run next may rely on the results of previous queries.  The generalization error of each individual query/analysis can be controlled by using an array of well-established statistical techniques.  However, when queries are arbitrarily composed, the different errors can propagate through the chain of different queries and bring to a high generalization error.  To address this issue, data analysts are designing several techniques that not only guarantee bounds on the generalization errors of single queries, but that also guarantee bounds on the generalization error of the composed analyses.  The choice of which of these techniques to use, often depends on the chain of queries that an adaptive data analysis can generate.   In this work, we consider adaptive data analyses implemented as while-like programs and we design a program analysis which can help with identifying which technique to use to control their generalization errors.  More specifically, we formalize the intuitive notion of adaptivity as a quantitative property of programs.  We do this because the adaptivity level of a data analysis is a key measure to choose the right technique.  Based on this definition, we design a program analysis for soundly approximating this quantity.  The program analysis generates a representation of the data analysis as a weighted dependency graph, where the weight is an upper bound on the number of times each variable can be reached, and uses a path search strategy to guarantee an upper bound on the adaptivity.  We implement our program analysis and show that it can help to analyze the adaptivity of several concrete data analyses with different adaptivity structures.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "184",
        "numpages": "25",
        "keywords": "Adaptive data analysis, dependency graph, program analysis"
    },
    "Adaptfun: Program analysis for Adaptive analysis": {
        "type": "software",
        "key": "10.5281/zenodo.10806763",
        "author": "Liu, Jiawen and Qu, Weihao and Gaboardi, Marco and Garg, Deepak and Ullman, Jonathan",
        "title": "Adaptfun: Program analysis for Adaptive analysis",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10806763",
        "abstract": "    <p>The software of program anlaysis tool Adaptfun, which provides the estimated upper bound on the adaptivity of adaptive data analysis algorithms. The tool is implemened using OCaml and Python. The reuslts are also evaluated in Python.</p>",
        "keywords": "Adaptive data analysis, dependency graph, program analysis"
    },
    "Superfusion: Eliminating Intermediate Data Structures via Inductive Synthesis": {
        "type": "article",
        "key": "10.1145/3656415",
        "author": "Ji, Ruyi and Zhao, Yuwei and Polikarpova, Nadia and Xiong, Yingfei and Hu, Zhenjiang",
        "title": "Superfusion: Eliminating Intermediate Data Structures via Inductive Synthesis",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656415",
        "doi": "10.1145/3656415",
        "abstract": "Intermediate data structures are a common cause of inefficiency in functional programming. Fusion attempts to eliminate intermediate data structures by combining adjacent data traversals into one; existing fusion techniques, however, are based on predefined rewrite rules and hence are limited in expressiveness. In this work we explore a different approach to eliminating intermediate data structures, based on inductive program synthesis. We dub this approach superfusion (by analogy with superoptimization, which uses inductive synthesis for program optimization). Starting from a reference program annotated with data structures to be eliminated, superfusion first generates a sketch where program fragments operating on those data structures are replaced with holes; it then fills the holes with constant-time expressions such that the resulting program is equivalent to the reference. The main technical challenge here is scalability because optimized programs are often complex, making the search space intractably large for naive enumeration. To address this challenge, our key insight is to first synthesize a ghost function that describes the relationship between the original intermediate data structure and its compressed version; this function, although not used in the final program, serves to decompose the joint sketch filling problem into independent simpler problems for each hole. We implement superfusion in a tool called SuFu and evaluate it on a dataset of 290 tasks collected from prior work on deductive fusion and program restructuring. The results show that SuFu solves 264 out of 290 tasks, exceeding the capabilities of rewriting-based fusion systems and achieving comparable performance with specialized approaches to program restructuring on their respective domains.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "185",
        "numpages": "26",
        "keywords": "Fusion, Inductive Program Synthesis, Program Optimization"
    },
    "Artifact for PLDI'24: Superfusion: Eliminating Intermediate Data Structures via Inductive Synthesis": {
        "type": "software",
        "key": "10.5281/zenodo.10951760",
        "author": "Ji, Ruyi and Zhao, Yuwei and Polikarpova, Nadia and Xiong, Yingfei and Hu, Zhenjiang",
        "title": "Artifact for PLDI'24: Superfusion: Eliminating Intermediate Data Structures via Inductive Synthesis",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10951760",
        "abstract": "    <p>This project will be maintained at https://github.com/jiry17/SuFu.</p>",
        "keywords": "Fusion, Inductive Program Synthesis, Program Optimization"
    },
    "Consolidating Smart Contracts with Behavioral Contracts": {
        "type": "article",
        "key": "10.1145/3656416",
        "author": "Wei, Guannan and Xie, Danning and Zhang, Wuqi and Yuan, Yongwei and Zhang, Zhuo",
        "title": "Consolidating Smart Contracts with Behavioral Contracts",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656416",
        "doi": "10.1145/3656416",
        "abstract": "Ensuring the reliability of smart contracts is of vital importance due to the wide adoption of smart contract  programs in decentralized financial applications. However, statically checking many rich properties of smart  contract programs can be challenging. On the other hand, dynamic validation approaches have shown promise  for widespread adoption in practice. Nevertheless, as part of the programming environment for smart contracts",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "186",
        "numpages": "25",
        "keywords": "behavioral contracts, runtime verification, smart contracts, specification"
    },
    "Scaling Type-Based Points-to Analysis with Saturation": {
        "type": "article",
        "key": "10.1145/3656417",
        "author": "Wimmer, Christian and Stancu, Codrut and Kozak, David and W\\\"{u}rthinger, Thomas",
        "title": "Scaling Type-Based Points-to Analysis with Saturation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656417",
        "doi": "10.1145/3656417",
        "abstract": "Designing a whole-program static analysis requires trade-offs between precision and scalability. While a context-insensitive points-to analysis is often considered a good compromise, it still has non-linear complexity that leads to scalability problems when analyzing large applications. On the other hand, rapid type analysis scales well but lacks precision. We use saturation in a context-insensitive type-based points-to analysis to make it as scalable as a rapid type analysis, while preserving most of the precision of the points-to analysis. With saturation, the points-to analysis only propagates small points-to sets for variables. If a variable can have more values than a certain threshold, the variable and all its usages are considered saturated and no longer analyzed.   Our implementation in the points-to analysis of GraalVM Native Image, a closed-world approach to build standalone binaries for Java applications, shows that saturation allows GraalVM Native Image to analyze large Java applications with hundreds of thousands of methods in less than two\u2004\u200dminutes.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "187",
        "numpages": "24",
        "keywords": "GraalVM, Java, pointer analysis, points-to analysis, static analysis"
    },
    "Scaling Points-to Analysis using Saturation - Artifact": {
        "type": "software",
        "key": "10.5281/zenodo.10961908",
        "author": "Wimmer, Christian and Stancu, Codrut and Kozak, David and W\\\"{u}rthinger, Thomas",
        "title": "Scaling Points-to Analysis using Saturation - Artifact",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10961908",
        "abstract": "    <p>The artifact presents our work on scaling points-to analysis using saturation. The content is a docker image containing GraalVM release and our benchmarking infrastructure. There is no need for special hardware, everything should work out of the box. The evaluation runs GraalVM Native Image in various configurations on our benchmarks and output the results, so that they can be compared with the values presented in the paper. However, due to resource constraints, we chose only a small subset that finishes fast. We provide a full configuration as well, but please note the full setup would take weeks to finish is executed on a single machine.</p>",
        "keywords": "GraalVM, Java, pointer analysis, points-to analysis, static analysis"
    },
    "From Batch to Stream: Automatic Generation of Online Algorithms": {
        "type": "article",
        "key": "10.1145/3656418",
        "author": "Wang, Ziteng and Pailoor, Shankara and Prakash, Aaryan and Wang, Yuepeng and Dillig, I\\c{s}\\i{}l",
        "title": "From Batch to Stream: Automatic Generation of Online Algorithms",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656418",
        "doi": "10.1145/3656418",
        "abstract": "Online streaming algorithms, tailored for continuous data processing, offer substantial benefits but are often more intricate to design than their offline counterparts. This paper introduces a novel approach for automatically synthesizing online streaming algorithms from their offline versions. In particular, we propose a novel methodology, based on the notion of relational function signature (RFS), for deriving an online algorithm given its offline version. Then, we propose a concrete synthesis algorithm that is an instantiation of the proposed methodology. Our algorithm uses the RFS to decompose the synthesis problem into a set of independent subtasks and uses a combination of symbolic reasoning and search to solve each subproblem. We implement the proposed technique in a new tool called Opera and evaluate it on over 50 tasks spanning two domains: statistical computations and online auctions. Our results show that Opera can automatically derive the online version of the original algorithm for 98\\% of the tasks. Our experiments also demonstrate that Opera significantly outperforms alternative approaches, including adaptations of SyGuS solvers to this problem as well as two of Opera's own ablations.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "188",
        "numpages": "26",
        "keywords": "Incremental Computation, Online Algorithms, Program Synthesis, Stream Processing"
    },
    "Software Artifact for `From Batch to Stream: Automatic Generation of Online Algorithms'": {
        "type": "software",
        "key": "10.5281/zenodo.10901598",
        "author": "Wang, Ziteng and Pailoor, Shankara and Prakash, Aaryan and Wang, Yuepeng and Dillig, I\\c{s}\\i{}l",
        "title": "Software Artifact for `From Batch to Stream: Automatic Generation of Online Algorithms'",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10901598",
        "abstract": "    <p>Opera is written in Python using both Poetry and Nix for managing dependencies. A recent installation of Nix (version 2.18.1 or higher) is the only prerequisite to get started. Additionally, we offer a Docker-based solution for running Nix.</p>",
        "keywords": "Incremental Computation, Online Algorithms, Program Synthesis, Stream Processing"
    },
    "Symbolic Execution for Quantum Error Correction Programs": {
        "type": "article",
        "key": "10.1145/3656419",
        "author": "Fang, Wang and Ying, Mingsheng",
        "title": "Symbolic Execution for Quantum Error Correction Programs",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656419",
        "doi": "10.1145/3656419",
        "abstract": "We define QSE, a symbolic execution framework for quantum programs by integrating symbolic variables into quantum states and the outcomes of quantum measurements.  The soundness of QSE is established through a theorem that ensures the correctness of symbolic execution within operational semantics.  We further introduce symbolic stabilizer states, which symbolize the phases of stabilizer generators, for the efficient analysis of quantum error correction (QEC) programs.  Within the QSE framework, we can use symbolic expressions to characterize the possible discrete Pauli errors in QEC, providing a significant improvement over existing methods that rely on sampling with simulators.  We implement QSE with the support of symbolic stabilizer states in a prototype tool named QuantumSE.jl.  Our experiments on representative QEC codes, including quantum repetition codes, Kitaev's toric codes, and quantum Tanner codes, demonstrate the efficiency of QuantumSE.jl for debugging QEC programs with over 1000 qubits.  In addition, by substituting concrete values in symbolic expressions of measurement results, QuantumSE.jl is also equipped with a sampling feature for stabilizer circuits.  Despite a longer initialization time than the state-of-the-art stabilizer simulator, Google's Stim, QuantumSE.jl offers a quicker sampling rate in the experiments.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "189",
        "numpages": "26",
        "keywords": "stabilizer formalism, symbolic execution"
    },
    "Artifact: Symbolic Execution for Quantum Error Correction Programs": {
        "type": "software",
        "key": "10.5281/zenodo.10781381",
        "author": "Fang, Wang and Ying, Mingsheng",
        "title": "Artifact: Symbolic Execution for Quantum Error Correction Programs",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10781381",
        "abstract": "    <p>We define QSE, a symbolic execution framework for quantum programs by integrating symbolic variables into quantum states and the outcomes of quantum measurements.&nbsp;The soundness of QSE is established through a theorem that ensures the correctness of symbolic execution within operational semantics.&nbsp;We further introduce symbolic stabilizer states, which symbolize the phases of stabilizer generators, for the efficient analysis of quantum error correction (QEC) programs.&nbsp;Within the QSE framework, we can use symbolic expressions to characterize the possible discrete Pauli errors in QEC, providing a significant improvement over existing methods that rely on sampling with simulators.&nbsp;We implement QSE &nbsp;with the support of symbolic stabilizer states in a prototype tool named QuantumSE.jl. Our experiments on representative QEC codes, including quantum repetition codes, Kitaev\u2019s toric codes, and quantum Tanner codes, demonstrate the efficiency of QuantumSE.jl for debugging QEC programs with over 1000 qubits.&nbsp;In addition, by substituting concrete values in symbolic expressions of measurement results, QuantumSE.jl is also equipped with a sampling feature for stabilizer circuits.&nbsp;Despite a longer initialization time than the state-of-the-art stabilizer simulator, Google\u2019s Stim, QuantumSE.jl offers a quicker sampling rate in the experiments.</p>",
        "keywords": "quantum error correction, quantum programs, stabilizer formalism, symbolic execution"
    },
    "Wavefront Threading Enables Effective High-Level Synthesis": {
        "type": "article",
        "key": "10.1145/3656420",
        "author": "Pelton, Blake and Sapek, Adam and Eguro, Ken and Lo, Daniel and Forin, Alessandro and Humphrey, Matt and Xi, Jinwen and Cox, David and Karandikar, Rajas and de Fine Licht, Johannes and Babin, Evgeny and Caulfield, Adrian and Burger, Doug",
        "title": "Wavefront Threading Enables Effective High-Level Synthesis",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656420",
        "doi": "10.1145/3656420",
        "abstract": "Digital systems are growing in importance and computing hardware is growing more heterogeneous. Hardware design, however, remains laborious and expensive, in part due to the limitations of conventional hardware description languages (HDLs) like VHDL and Verilog. A longstanding research goal has been programming hardware like software, with high-level languages that can generate efficient hardware designs. This paper describes Kanagawa, a language that takes a new approach to combine the programmer productivity benefits of traditional High-Level Synthesis (HLS) approaches with the expressibility and hardware efficiency of Register-Transfer Level (RTL) design. The language's concise syntax, matched with a hardware design-friendly execution model, permits a relatively simple toolchain to map high-level code into efficient hardware implementations.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "190",
        "numpages": "25",
        "keywords": "Hardware Threading, Wavefront Consistency, Wavefront Threading"
    },
    "Decidable Subtyping of Existential Types for Julia": {
        "type": "article",
        "key": "10.1145/3656421",
        "author": "Belyakova, Julia and Chung, Benjamin and Tate, Ross and Vitek, Jan",
        "title": "Decidable Subtyping of Existential Types for Julia",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656421",
        "doi": "10.1145/3656421",
        "abstract": "Julia is a modern scientific-computing language that relies on  multiple dispatch to implement generic libraries. While the language  does not have a static type system, method declarations are  decorated with expressive type annotations to determine when they  are applicable. To find applicable methods, the implementation uses  subtyping at run-time. We show that Julia's subtyping is undecidable,  and we propose a restriction on types to recover decidability by  stratifying types into method signatures over value types---where the  former can freely use bounded existential types but the latter are  restricted to use-site variance. A corpus analysis suggests that  nearly all Julia programs written in practice already conform to  this restriction.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "191",
        "numpages": "24",
        "keywords": "Decidability, Julia, Subtyping"
    },
    "RefinedRust: A Type System for High-Assurance Verification of Rust Programs": {
        "type": "article",
        "key": "10.1145/3656422",
        "author": "G\\\"{a}her, Lennard and Sammler, Michael and Jung, Ralf and Krebbers, Robbert and Dreyer, Derek",
        "title": "RefinedRust: A Type System for High-Assurance Verification of Rust Programs",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656422",
        "doi": "10.1145/3656422",
        "abstract": "Rust is a modern systems programming language whose ownership-based type system statically guarantees memory safety, making it particularly well-suited to the domain of safety-critical systems. In recent years, a wellspring of automated deductive verification tools have emerged for establishing functional correctness of Rust code. However, none of the previous tools produce foundational proofs (machine-checkable in a general-purpose proof assistant), and all of them are restricted to the safe fragment of Rust. This is a problem because the vast majority of Rust programs make use of unsafe code at critical points, such as in the implementation of widely-used APIs. We propose RefinedRust, a refinement type system\u2014proven sound in the Coq proof assistant\u2014with the goal of establishing foundational semi-automated functional correctness verification of both safe and unsafe Rust code. We have developed a prototype verification tool implementing RefinedRust. Our tool translates Rust code (with user annotations) into a model of Rust embedded in Coq, and then checks its adherence to the RefinedRust type system using separation logic automation in Coq. All proofs generated by RefinedRust are checked by the Coq proof assistant, so the automation and type system do not have to be trusted. We evaluate the effectiveness of RefinedRust by verifying a variant of Rust\u2019s Vec implementation that involves intricate reasoning about unsafe pointer-manipulating code.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "192",
        "numpages": "25",
        "keywords": "Iris, Rust, program verification, separation logic"
    },
    "Artifact for \"RefinedRust: A Type System for High-Assurance Verification of Rust Programs": {
        "type": "software",
        "key": "10.5281/zenodo.10912439",
        "author": "G\\\"{a}her, Lennard and Sammler, Michael and Jung, Ralf and Krebbers, Robbert and Dreyer, Derek",
        "title": "Artifact for \"RefinedRust: A Type System for High-Assurance Verification of Rust Programs",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10912439",
        "abstract": "    <p>This is the artifact for the PLDI\u201924 paper \u201cRefinedRust: A Type System for High-Assurance Verification of Rust Programs\u201d. It contains the implementation of RefinedRust and Coq development formalizing the results of the paper.</p>",
        "keywords": "Iris, program verification, Rust, separation logic"
    },
    "LiDO: Linearizable Byzantine Distributed Objects with Refinement-Based Liveness Proofs": {
        "type": "article",
        "key": "10.1145/3656423",
        "author": "Qiu, Longfei and Kim, Yoonseung and Shin, Ji-Yong and Kim, Jieung and Honor\\'{e}, Wolf and Shao, Zhong",
        "title": "LiDO: Linearizable Byzantine Distributed Objects with Refinement-Based Liveness Proofs",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656423",
        "doi": "10.1145/3656423",
        "abstract": "Byzantine fault-tolerant state machine replication (SMR) protocols, such as PBFT, HotStuff, and Jolteon, are essential for modern blockchain technologies. However, they are challenging to implement correctly because they have to deal with any unexpected message from byzantine peers and ensure safety and liveness at all times. Many formal frameworks have been developed to verify the safety of SMR implementations, but there is still a gap in the verification of their liveness. Existing liveness proofs are either limited to the network level or do not cover popular partially synchronous protocols.  We introduce LiDO, a consensus model that enables the verification of both safety and liveness of implementations through refinement. We observe that current consensus models cannot handle liveness because they do not include a pacemaker state. We show that by adding a pacemaker state to the LiDO model, we can express the liveness properties of SMR protocols as a few safety properties that can be easily verified by refinement proofs. Based on our LiDO model, we provide mechanized safety and liveness proofs for both unpipelined and pipelined Jolteon in Coq. This is the first mechanized liveness proof for a byzantine consensus protocol with non-trivial optimizations such as pipelining.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "193",
        "numpages": "25",
        "keywords": "byzantine fault-tolerance, consensus protocols, distributed systems, formal verification, liveness, proof assistants, refinement, safety"
    },
    "Artifact for PLDI 2024 paper #290: LiDO: Linearizable Byzantine Distributed Objects with Refinement-Based Liveness Proofs.": {
        "type": "software",
        "key": "10.5281/zenodo.10909272",
        "author": "Qiu, Longfei and Kim, Yoonseung and Shin, Ji-Yong and Kim, Jieung and Honor\\'{e}, Wolf and Shao, Zhong",
        "title": "Artifact for PLDI 2024 paper #290: LiDO: Linearizable Byzantine Distributed Objects with Refinement-Based Liveness Proofs.",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10909272",
        "abstract": "    <p>This is artifact for PLDI 2024 paper #290: LiDO: Linearizable Byzantine Distributed Objects with Refinement-Based Liveness Proofs.</p><p>Included files are the LiDO model formalized in Coq, together with three implementations of LiDO (unpipelined Jolteon, unpipelined Jolteon with improved pacemaker, and pipelined Jolteon), each having safety and liveness proofs.</p><p>See README.md inside artifact package for more details.</p>",
        "keywords": "byzantine fault-tolerance, consensus protocols, distributed systems, formal verification, liveness, proof assistants, refinement, safety"
    },
    "Reducing Static Analysis Unsoundness with Approximate Interpretation": {
        "type": "article",
        "key": "10.1145/3656424",
        "author": "Laursen, Mathias Rud and Xu, Wenyuan and M\\o{}ller, Anders",
        "title": "Reducing Static Analysis Unsoundness with Approximate Interpretation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656424",
        "doi": "10.1145/3656424",
        "abstract": "Static program analysis for JavaScript is more difficult than for many other programming languages. One of the main reasons is the presence of dynamic property accesses that read and write object properties via dynamically computed property names. To ensure scalability and precision, existing state-of-the-art analyses for JavaScript mostly ignore these operations although it results in missed call edges and aliasing relations.  We present a novel dynamic analysis technique named approximate interpretation that is designed to efficiently and fully automatically infer likely determinate facts about dynamic property accesses, in particular those that occur in complex library API initialization code, and how to use the produced information in static analysis to recover much of the abstract information that is otherwise missed.  Our implementation of the technique and experiments on 141 real-world Node.js-based JavaScript applications and libraries show that the approach leads to significant improvements in call graph construction. On average the use of approximate interpretation leads to 55.1\\% more call edges, 21.8\\% more reachable functions, 17.7\\% more resolved call sites, and only 1.5\\% fewer monomorphic call sites. For 36 JavaScript projects where dynamic call graphs are available, average analysis recall is improved from 75.9\\% to 88.1\\% with a negligible reduction in precision.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "194",
        "numpages": "24",
        "keywords": "JavaScript, call graphs, points-to analysis, program analysis"
    },
    "Artifact for \"Reducing Static Analysis Unsoundness with Approximate Interpretation\", PLDI 2024": {
        "type": "software",
        "key": "10.5281/zenodo.10930752",
        "author": "Laursen, Mathias Rud and Xu, Wenyuan and M\\o{}ller, Anders",
        "title": "Artifact for \"Reducing Static Analysis Unsoundness with Approximate Interpretation\", PLDI 2024",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10930752",
        "abstract": "    <p>This artifact consists of a VirtualBox image that contains program code and experimental data for the paper Reducing Static Analysis Unsoundness with Approximate Interpretation by Mathias Rud Laursen, Wenyuan Xu and Anders M\\o{}ller, PLDI 2024.</p>",
        "keywords": "call graphs, JavaScript, points-to analysis, program analysis"
    },
    "Verification under Intel-x86 with Persistency": {
        "type": "article",
        "key": "10.1145/3656425",
        "author": "Abdulla, Parosh and Atig, Mohamed Faouzi and Bouajjani, Ahmed and Kumar, K. Narayan and Saivasan, Prakash",
        "title": "Verification under Intel-x86 with Persistency",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656425",
        "doi": "10.1145/3656425",
        "abstract": "The full semantics of the Intel-x86 architecture has been defined by Raad et al in POPL 2022, extending the earlier formalization based on the TSO memory model incorporating persistency. This new semantics involves an intricate combination of the SC, TSO, and PSO models to account for the diverse features of the enlarged instruction set. In this paper we investigate the reachability problem under this semantics, including both its consistency and persistency aspects each of which requires reasoning about unbounded operation reorderings. Our first contribution is to show that reachability under this model can be reduced to reachability under a model without the persistency component. This is achieved by showing that the persistency semantics can be simulated by a finite-state protocol running in parallel with the program. Our second contribution is to prove that reachability under the consistency model of Intel-x86 (even without crashes and persistency) is undecidable. Undecidability is obtained as soon as one thread in the program is allowed to use both TSO variables and two PSO variables. The third contribution is showing that for any fixed bound on the alternation between TSO writes (write-backs), and PSO writes (non-temporal writes), the reachability problem is decidable. This defines a complete parametrized schema for under-approximate analysis that can be used for bug finding.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "195",
        "numpages": "24",
        "keywords": "TSO memory model, model checking, persistent memories, program verification"
    },
    "Compilation of Modular and General Sparse Workspaces": {
        "type": "article",
        "key": "10.1145/3656426",
        "author": "Zhang, Genghan and Hsu, Olivia and Kjolstad, Fredrik",
        "title": "Compilation of Modular and General Sparse Workspaces",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656426",
        "doi": "10.1145/3656426",
        "abstract": "Recent years have seen considerable work on compiling sparse tensor algebra expressions. This paper addresses a shortcoming in that work, namely how to generate efficient code (in time and space) that scatters values into a sparse result tensor. We address this shortcoming through a compiler design that generates code that uses sparse intermediate tensors (sparse workspaces) as efficient adapters between compute code that scatters and result tensors that do not support random insertion. Our compiler automatically detects sparse scattering behavior in tensor expressions and inserts necessary intermediate workspace tensors. We present an algorithm template for workspace insertion that is the backbone of our code generation algorithm. Our algorithm template is modular by design, supporting sparse workspaces that span multiple user-defined implementations. Our evaluation shows that sparse workspaces can be up to 27.12\\texttimes{} faster than the dense workspaces of prior work. On the other hand, dense workspaces can be up to 7.58\\texttimes{} faster than the sparse workspaces generated by our compiler in other situations, which motivates our compiler design that supports both. Our compiler produces sequential code that is competitive with hand-optimized linear and tensor algebra libraries on the expressions they support, but that generalizes to any other expression. Sparse workspaces are also more memory efficient than dense workspaces as they compress away zeros. This compression can asymptotically decrease memory usage, enabling tensor computations on data that would otherwise run out of memory.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "196",
        "numpages": "26",
        "keywords": "code composition, compilation, sparse tensor algebra, sparse workspaces"
    },
    "Maximum Consensus Floating Point Solutions for Infeasible Low-Dimensional Linear Programs with Convex Hull as the Intermediate Representation": {
        "type": "article",
        "key": "10.1145/3656427",
        "author": "Aanjaneya, Mridul and Nagarakatte, Santosh",
        "title": "Maximum Consensus Floating Point Solutions for Infeasible Low-Dimensional Linear Programs with Convex Hull as the Intermediate Representation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656427",
        "doi": "10.1145/3656427",
        "abstract": "This paper proposes a novel method to efficiently solve infeasible low-dimensional linear programs (LDLPs) with billions of constraints and a small number of unknown variables, where all the constraints cannot be satisfied simultaneously. We focus on infeasible linear programs generated in the RLibm project for creating correctly rounded math libraries. Specifically, we are interested in generating a floating point solution that satisfies the maximum number of constraints. None of the existing methods can solve such large linear programs while producing floating point solutions. We observe that the convex hull can serve as an intermediate representation (IR) for solving infeasible LDLPs using the geometric duality between linear programs and convex hulls. Specifically, some of the constraints that correspond to points on the convex hull are precisely those constraints that make the linear program infeasible. Our key idea is to split the entire set of constraints into two subsets using the convex hull IR: (a) a set X of feasible constraints and (b) a superset V of infeasible constraints. Using the special structure of the RLibm constraints and the presence of a method to check whether a system is feasible or not, we identify a superset of infeasible constraints by computing the convex hull in 2-dimensions. Subsequently, we identify the key constraints (i.e., basis constraints) in the set of feasible constraints X and use them to create a new linear program whose solution identifies the maximum set of constraints satisfiable in V while satisfying all the constraints in X. This new solver enabled us to improve the performance of the resulting RLibm polynomials while solving the corresponding linear programs significantly faster.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "197",
        "numpages": "25",
        "keywords": "RLIBM, convex hull, infeasible linear programs, math libraries"
    },
    "Qubit Recycling Revisited": {
        "type": "article",
        "key": "10.1145/3656428",
        "author": "Jiang, Hanru",
        "title": "Qubit Recycling Revisited",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656428",
        "doi": "10.1145/3656428",
        "abstract": "Reducing the width of quantum circuits is crucial due to limited number of qubits in quantum devices. This paper revisit an optimization strategy known as qubit recycling (alternatively wire-recycling or measurement-and-reset), which leverages gate commutativity to reuse discarded qubits, thereby reducing circuit width. We introduce qubit dependency graphs (QDGs) as a key abstraction for this optimization. With QDG, we isolate the computationally demanding components, and observe that qubit recycling is essentially a matrix triangularization problem. Based on QDG and this observation, we study qubit recycling with a focus on complexity, algorithmic, and verification aspects. Firstly, we establish qubit recycling\u2019s NP-hardness through reduction from Wilf\u2019s question, another matrix triangularization problem. Secondly, we propose a QDG-guided solver featuring multiple heuristic options for effective qubit recycling. Benchmark tests conducted on RevLib illustrate our solver\u2019s superior or comparable performance to existing alternatives. Notably, it achieves optimal solutions for the majority of circuits. Finally, we develop a certified qubit recycler that integrates verification and validation techniques, with its correctness proof mechanized in Coq.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "198",
        "numpages": "24",
        "keywords": "Certified Compilation, Complexity, Quantum Circuit Optimization"
    },
    "Artifact for PLDI' 24 submission #350 \"Qubit Recycling Revisited": {
        "type": "software",
        "key": "10.5281/zenodo.10910395",
        "author": "Jiang, Hanru",
        "title": "Artifact for PLDI' 24 submission #350 \"Qubit Recycling Revisited",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10910395",
        "abstract": "    <p>This is the artifact of PLDI\u201924 submission #350 \u201cQubit Recycling Revisited\u201d, containing a certified prototype qubit recycler featuring various heuristics reported in the paper, and a subset of RevLib circuits for evaluation purpose. It is provided to reproduce the results of Sec. 7, and to check the mechanized proof of Theorem6.8. It also comes with a Docker image with the experimental environment setup, to make this artifact cross-platform.</p>",
        "keywords": "Certified Compilation, Quantum Circuit Optimization"
    },
    "A Lightweight Polyglot Code Transformation Language": {
        "type": "article",
        "key": "10.1145/3656429",
        "author": "Ketkar, Ameya and Ramos, Daniel and Clapp, Lazaro and Barik, Raj and Ramanathan, Murali Krishna",
        "title": "A Lightweight Polyglot Code Transformation Language",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656429",
        "doi": "10.1145/3656429",
        "abstract": "In today's software industry, large-scale, multi-language codebases are the norm. This brings substantial challenges in developing automated tools for code maintenance tasks such as API migration or dead code cleanup. Tool builders often find themselves caught between two less-than-ideal tooling options: (1) language-specific code rewriting tools or (2) generic, lightweight match-replace transformation tools with limited expressiveness. The former leads to tool fragmentation and a steep learning curve for each language, while the latter forces developers to create ad-hoc, throwaway scripts to handle realistic tasks.    To fill this gap, we introduce a new declarative domain-specific language (DSL) for expressing interdependent multi-language code transformations. Our key insight is that we can increase the expressiveness and applicability of lightweight match-replace tools by extending them to support for composition, ordering, and flow. We implemented an open-source tool for our language, called PolyglotPiranha, and deployed it in an industrial setting. We demonstrate its effectiveness through three case studies, where it deleted 210K lines of dead code and migrated 20K lines, across 1611 pull requests. We compare our DSL against state-of-the-art alternatives, and show that the tools we developed are faster, more concise, and easier to maintain.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "199",
        "numpages": "25",
        "keywords": "Automated refactoring, Code cleanup, Source-code rewriting"
    },
    "Replication of A Lightweight Polyglot Code Transformation Language": {
        "type": "software",
        "key": "10.5281/zenodo.10948026",
        "author": "Ketkar, Ameya and Ramos, Daniel and Clapp, Lazaro and Barik, Raj and Ramanathan, Murali Krishna",
        "title": "Replication of A Lightweight Polyglot Code Transformation Language",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10948026",
        "abstract": "    <p>The artifact contains the necessary instructions to replicate part of the experiments in the paper. It also provides with users with an environment to run our tool.</p><p>Detailed instructions can be found within the zip file.</p>",
        "keywords": "code cleanup, code transformation, refactoring"
    },
    "An Algebraic Language for Specifying Quantum Networks": {
        "type": "article",
        "key": "10.1145/3656430",
        "author": "Buckley, Anita and Chuprikov, Pavel and Otoni, Rodrigo and Soul\\'{e}, Robert and Rand, Robert and Eugster, Patrick",
        "title": "An Algebraic Language for Specifying Quantum Networks",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656430",
        "doi": "10.1145/3656430",
        "abstract": "Quantum networks connect quantum capable nodes in order to achieve capabilities that are impossible only using classical information. Their fundamental unit of communication is the Bell pair, which consists of two entangled quantum bits. Unfortunately, Bell pairs are fragile and difficult to transmit directly, necessitating a network of repeaters, along with software and hardware that can ensure the desired results. Challenging intrinsic features of quantum networks, such as dealing with resource competition, motivate formal reasoning about quantum network protocols. To this end, we developed BellKAT, a novel specification language for quantum networks based upon Kleene algebra.  To cater to the specific needs of quantum networks, we designed an algebraic structure, called BellSKA, which we use as the basis of BellKAT's denotational semantics. BellKAT's constructs describe entanglement distribution rules that allow for modular specification. We give BellKAT a sound and complete equational theory, allowing us to verify network protocols. We provide a prototype tool to showcase the expressiveness of BellKAT and how to optimize and verify networks in practice.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "200",
        "numpages": "23",
        "keywords": "Kleene algebra, entanglement, quantum networks"
    },
    "Artifact for the article An Algebraic Language for Specifying Quantum Networks": {
        "type": "software",
        "key": "10.5281/zenodo.10909730",
        "author": "Buckley, Anita and Chuprikov, Pavel and Otoni, Rodrigo and Soul\\'{e}, Robert and Rand, Robert and Eugster, Patrick",
        "title": "Artifact for the article An Algebraic Language for Specifying Quantum Networks",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10909730",
        "abstract": "    <p>The artifact is a Haskell library bellkat plus several examples provided as executables within the same Haskell package.</p>",
        "keywords": "entanglement, Kleene algebra, quantum networks"
    },
    "Linear Matching of JavaScript Regular Expressions": {
        "type": "article",
        "key": "10.1145/3656431",
        "author": "Barri\\`{e}re, Aur\\`{e}le and Pit-Claudel, Cl\\'{e}ment",
        "title": "Linear Matching of JavaScript Regular Expressions",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656431",
        "doi": "10.1145/3656431",
        "abstract": "Modern regex languages have strayed far from well-understood traditional regular expressions: they include features that fundamentally transform the matching problem. In exchange for these features, modern regex engines at times suffer from exponential complexity blowups, a frequent source of denial-of-service vulnerabilities in JavaScript applications. Worse, regex semantics differ across languages, and the impact of these divergences on algorithmic design and worst-case matching complexity has seldom been investigated. This paper provides a novel perspective on JavaScript's regex semantics by identifying a larger-than-previously-understood subset of the language that can be matched with linear time guarantees. In the process, we discover several cases where state-of-the-art algorithms were either wrong (semantically incorrect), inefficient (suffering from superlinear complexity) or excessively restrictive (assuming certain features could not be matched linearly). We introduce novel algorithms to restore correctness and linear complexity. We further advance the state-of-the-art in linear regex matching by presenting the first nonbacktracking algorithms for matching lookarounds in linear time: one supporting captureless lookbehinds in any regex language, and another leveraging a JavaScript property to support unrestricted lookaheads and lookbehinds. Finally, we describe new time and space complexity tradeoffs for regex engines. All of our algorithms are practical: we validated them in a prototype implementation, and some have also been merged in the V8 JavaScript implementation used in Chrome and Node.js.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "201",
        "numpages": "25",
        "keywords": "Automata, JavaScript, Regex"
    },
    "Artifact for \"Linear Matching of JavaScript Regular Expressions\" at PLDI 2024": {
        "type": "software",
        "key": "10.5281/zenodo.10806044",
        "author": "Barri\\`{e}re, Aur\\`{e}le and Pit-Claudel, Cl\\'{e}ment",
        "title": "Artifact for \"Linear Matching of JavaScript Regular Expressions\" at PLDI 2024",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10806044",
        "abstract": "    <p>Description of artifact</p><p>The artifact consists of the source code and build scripts for our OCaml matchers; the patches that we wrote for V8; scripts to compute statistics on regex corpora; and the scripts to run our performance experiments and plot their results.</p><p>We have documented the directory structure of the OCaml matcher and the correspondence between the paper\u2019s definitions and the source code in the OCaml matcher\u2019s README in ocaml/allf/README.md. We recommend using this README as a guide to the OCaml code while reading the paper. Required hardware</p><p>We recommend running on an Ubuntu 22.04 LTS machine with at least 16GB of RAM. The VM is configured to use:</p><pre><code>12GB of RAM (to run experiments)A CPU supporting the RDTSC instruction (for benchmarking)40GB of free space on your hard drive (each V8 build takes ~12GB)</code></pre>",
        "keywords": "Automata, JavaScript, Regex"
    },
    "Static Posterior Inference of Bayesian Probabilistic Programming via Polynomial Solving": {
        "type": "article",
        "key": "10.1145/3656432",
        "author": "Wang, Peixin and Yang, Tengshun and Fu, Hongfei and Li, Guanyan and Ong, C.-H. Luke",
        "title": "Static Posterior Inference of Bayesian Probabilistic Programming via Polynomial Solving",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656432",
        "doi": "10.1145/3656432",
        "abstract": "In Bayesian probabilistic programming, a central problem is to estimate the normalised posterior distribution (NPD) of a probabilistic program with conditioning via score (a.k.a. observe) statements. Most previous approaches address this problem by Markov Chain Monte Carlo and variational inference, and therefore could not generate guaranteed outcomes within a finite time limit. Moreover, existing methods for exact inference either impose syntactic restrictions or cannot guarantee successful inference in general. In this work, we propose a novel automated approach to derive guaranteed bounds for NPD via polynomial solving. We first establish a fixed-point theorem for the wide class of score-at-end Bayesian probabilistic programs that terminate almost-surely and have a single bounded score statement at program termination. Then, we propose a multiplicative variant of Optional Stopping Theorem (OST) to address score-recursive Bayesian programs where score statements with weights greater than one could appear inside a loop. Bayesian nonparametric models, enjoying a renaissance in statistics and machine learning, can be represented by score-recursive Bayesian programs and are difficult to handle due to an integrability issue. Finally, we use polynomial solving to implement our fixed-point theorem and OST variant. To improve the accuracy of the polynomial solving, we further propose a truncation operation and the synthesis of multiple bounds over various program inputs. Our approach can handle Bayesian probabilistic programs with unbounded while loops and continuous distributions with infinite supports. Experiments over a wide range of benchmarks show that compared with the most relevant approach (Beutner et al., PLDI 2022) for guaranteed NPD analysis via recursion unrolling, our approach is more time efficient and derives comparable or even tighter NPD bounds. Furthermore, our approach can handle score-recursive programs which previous approaches could not.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "202",
        "numpages": "26",
        "keywords": "Bayesian inference, fixed-point theory, martingales, posterior distributions, probabilistic programming, static analysis"
    },
    "Updated Artifact for \"Static Posterior Inference of Bayesian Probabilistic Programming via Polynomial Solving": {
        "type": "software",
        "key": "10.5281/zenodo.10897200",
        "author": "Wang, Peixin and Yang, Tengshun and Fu, Hongfei and Li, Guanyan and Ong, C.-H. Luke",
        "title": "Updated Artifact for \"Static Posterior Inference of Bayesian Probabilistic Programming via Polynomial Solving",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10897200",
        "abstract": "    <p>This is the artifact for the paper \u201cStatic Posterior Inference of Bayesian Probabilistic Programming via Polynomial Solving\u201d, which aims to derive guaranteed bounds for the normalised posterior distribution (NPD) over probabilistic programs.</p>",
        "keywords": "F#, Matlab, Mosek"
    },
    "A HAT Trick: Automatically Verifying Representation Invariants using Symbolic Finite Automata": {
        "type": "article",
        "key": "10.1145/3656433",
        "author": "Zhou, Zhe and Ye, Qianchuan and Delaware, Benjamin and Jagannathan, Suresh",
        "title": "A HAT Trick: Automatically Verifying Representation Invariants using Symbolic Finite Automata",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656433",
        "doi": "10.1145/3656433",
        "abstract": "Functional programs typically interact with stateful libraries that  hide state behind typed abstractions. One particularly important  class of applications are data structure implementations that rely  on such libraries to provide a level of efficiency and scalability  that may be otherwise difficult to achieve. However, because the  specifications of the methods provided by these libraries are  necessarily general and rarely specialized to the needs of any  specific client, any required application-level invariants must  often be expressed in terms of additional constraints on the (often)  opaque state maintained by the library.  In this paper, we consider the specification and verification of  such representation invariants using symbolic finite   automata (SFA). We show that SFAs can be used to succinctly and  precisely capture fine-grained temporal and data-dependent histories  of interactions between functional clients and stateful libraries.  To facilitate modular and compositional reasoning, we integrate SFAs  into a refinement type system to qualify stateful computations  resulting from such interactions. The particular instantiation we  consider, Hoare Automata Types (HATs), allows us to both  specify and automatically type-check the representation invariants  of a datatype, even when its implementation depends on stateful  library methods that operate over hidden state.  We also develop a new bidirectional type checking algorithm that  implements an efficient subtyping inclusion check over HATs",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "203",
        "numpages": "25",
        "keywords": "refinement types, representation invariants, symbolic finite automata"
    },
    "PLDI2024 Artifact: A HAT Trick: Automatically Verifying Representation Invariants Using Symbolic Finite Automata": {
        "type": "software",
        "key": "10.5281/zenodo.10806686",
        "author": "Zhou, Zhe and Ye, Qianchuan and Delaware, Benjamin and Jagannathan, Suresh",
        "title": "PLDI2024 Artifact: A HAT Trick: Automatically Verifying Representation Invariants Using Symbolic Finite Automata",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10806686",
        "abstract": "    <p>This artifact contains:</p><ol type=\"1\"><li>README.md\u202f: the artifact guide.</li><li>marple-original-submission.pdf: the original submitted paper.</li><li>marple:pldi-2024.tar.gz: the docker image (optional, we recommend to pull from the docker hub, see README.md).</li><li>Dockerfile: the docker file that can reproduce the docker image (optional, we recommend to pull from the docker hub, see README.md).</li></ol>",
        "keywords": "refinement types, representation invariants, symbolic finite automata"
    },
    "Stream Types": {
        "type": "article",
        "key": "10.1145/3656434",
        "author": "Cutler, Joseph W. and Watson, Christopher and Nkurumeh, Emeka and Hilliard, Phillip and Goldstein, Harrison and Stanford, Caleb and Pierce, Benjamin C.",
        "title": "Stream Types",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656434",
        "doi": "10.1145/3656434",
        "abstract": "We propose a rich foundational theory of typed data streams and stream transformers, motivated by two high-level goals. First, the type of a stream should be able to express complex sequential patterns of events over time. And second, it should describe the internal parallel structure of the stream, to support deterministic stream processing on parallel and distributed systems. To these ends, we introduce stream types, with operators capturing sequential composition, parallel composition, and iteration, plus a core calculus \u03bbST of transformers over typed streams that naturally supports a number of common streaming idioms, including punctuation, windowing, and parallel partitioning, as first-class constructions. \u03bbST exploits a Curry-Howard-like correspondence with an ordered variant of the Logic of Bunched Implication to program with streams compositionally and uses Brzozowski-style derivatives to enable an incremental, prefix-based operational semantics. To illustrate the programming style supported by the rich types of \u03bbST, we present a number of examples written in Delta, a prototype high-level language design based on \u03bbST.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "204",
        "numpages": "25",
        "keywords": "Bunched Implication, Ordered Logic, Stream Processing, Type Systems"
    },
    "SuperStack: Superoptimization of Stack-Bytecode via Greedy, Constraint-Based, and SAT Techniques": {
        "type": "article",
        "key": "10.1145/3656435",
        "author": "Albert, Elvira and Garcia de la Banda, Maria and Hern\\'{a}ndez-Cerezo, Alejandro and Ignatiev, Alexey and Rubio, Albert and Stuckey, Peter J.",
        "title": "SuperStack: Superoptimization of Stack-Bytecode via Greedy, Constraint-Based, and SAT Techniques",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656435",
        "doi": "10.1145/3656435",
        "abstract": "Given a loop-free sequence of instructions, superoptimization techniques use a constraint solver to search for an equivalent sequence that is optimal for a desired objective. The complexity of the search grows exponentially with the length of the solution being constructed, and the problem becomes intractable for large sequences of instructions. This paper presents a new approach to superoptimizing stack-bytecode via three novel components: (1) a greedy algorithm to refine the bound on the length of the optimal solution; (2) a new representation of the optimization problem as a set of weighted soft clauses in MaxSAT; (3) a series of domain-specific dominance and redundant constraints to reduce the search space for optimal solutions. We have developed a tool, named SuperStack, which can be used to find optimal code translations of modern stack-based bytecode, namely WebAssembly or Ethereum bytecode. Experimental evaluation on more than 500,000 sequences shows the proposed greedy, constraint-based and SAT combination is able to greatly increase optimization gains achieved by existing superoptimizers and reduce to at least a fourth the optimization time.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "205",
        "numpages": "26",
        "keywords": "EVM, Program Synthesis, SAT, Superoptimization, WebAssembly"
    },
    "Artifact for \"SuperStack: Superoptimization of Stack-Bytecode via Greedy, Constraint-based, and SAT Techniques": {
        "type": "software",
        "key": "10.5281/zenodo.10801691",
        "author": "Albert, Elvira and Garcia de la Banda, Maria and Hern\\'{a}ndez-Cerezo, Alejandro and Ignatiev, Alexey and Rubio, Albert and Stuckey, Peter J.",
        "title": "Artifact for \"SuperStack: Superoptimization of Stack-Bytecode via Greedy, Constraint-based, and SAT Techniques",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10801691",
        "abstract": "    <p>This artifact includes the necessary data to reproduce the experiments in the paper \u201cSuperStack: Superoptimization of Stack-Bytecode via Greedy, Constraint-based, and SAT Techniques,\u201d accepted in PLDI\u201924.</p>",
        "keywords": "EVM, Program Synthesis, SAT, Superoptimization, WebAssembly"
    },
    "Compiling Conditional Quantum Gates without Using Helper Qubits": {
        "type": "article",
        "key": "10.1145/3656436",
        "author": "Huang, Keli and Palsberg, Jens",
        "title": "Compiling Conditional Quantum Gates without Using Helper Qubits",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656436",
        "doi": "10.1145/3656436",
        "abstract": "We present a compilation scheme for conditional quantum gates. Our scheme compiles a multi-qubit conditional to a linear number of two-qubit conditionals. This can be done straightforwardly with helper qubits, but we show how to do it without using helper qubits and with much fewer gates than in previous work. Specifically, our scheme requires 1/3 as many gates as the previous best scheme without using helper qubits, which is essential for practical use. Our experiments show that several quantum-circuit optimizers have little impact on the compiled code from the previous best scheme, confirming the need for our new scheme. Our experiments with Grover's algorithm and quantum walk also show that our scheme has a major impact on the reliability of the compiled code.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "206",
        "numpages": "22",
        "keywords": "code size, compilers, quantum computing, reliability"
    },
    "Hyper Hoare Logic: (Dis-)Proving Program Hyperproperties": {
        "type": "article",
        "key": "10.1145/3656437",
        "author": "Dardinier, Thibault and M\\\"{u}ller, Peter",
        "title": "Hyper Hoare Logic: (Dis-)Proving Program Hyperproperties",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656437",
        "doi": "10.1145/3656437",
        "abstract": "Hoare logics are proof systems that allow one to formally establish properties of computer programs. Traditional Hoare logics prove properties of individual program executions (such as functional correctness). Hoare logic has been generalized to prove also properties of multiple executions of a program (so-called hyperproperties, such as determinism or non-interference). These program logics prove the absence of (bad combinations of) executions. On the other hand, program logics similar to Hoare logic have been proposed to disprove program properties (e.g., Incorrectness Logic), by proving the existence of (bad combinations of) executions. All of these logics have in common that they specify program properties using assertions over a fixed number of states, for instance, a single pre- and post-state for functional properties or pairs of pre- and post-states for non-interference.  In this paper, we present Hyper Hoare Logic, a generalization of Hoare logic that lifts assertions to properties of arbitrary sets of states. The resulting logic is simple yet expressive: its judgments can express arbitrary program hyperproperties, a particular class of hyperproperties over the set of terminating executions of a program (including properties of individual program executions). By allowing assertions to reason about sets of states, Hyper Hoare Logic can reason about both the absence and the existence of (combinations of) executions, and, thereby, supports both proving and disproving program (hyper-)properties within the same logic, including (hyper-)properties that no existing Hoare logic can express. We prove that Hyper Hoare Logic is sound and complete, and demonstrate that it captures important proof principles naturally. All our technical results have been proved in Isabelle/HOL.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "207",
        "numpages": "25",
        "keywords": "Hyperproperties, Incorrectness Logic, Non-Interference, Program Logic"
    },
    "Hyper Hoare Logic: (Dis-)Proving Program Hyperproperties (artifact)": {
        "type": "software",
        "key": "10.5281/zenodo.10808236",
        "author": "Dardinier, Thibault and M\\\"{u}ller, Peter",
        "title": "Hyper Hoare Logic: (Dis-)Proving Program Hyperproperties (artifact)",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10808236",
        "abstract": "    <p>This artifact supports the PLDI 2024 paper \u201cHyper Hoare Logic: (Dis-)Proving Program Hyperproperties\u201d. It consists of an Isabelle/HOL mechanization that fully supports the formal claims made in the paper and a VirtualBox VM image with Ubuntu 22.04 that contains Isabelle 2023 and our mechanization.</p>",
        "keywords": "Compositionality, Hoare Logic, Hyper Hoare Logic, Hyperproperties, Incorrectness Logic, Isabelle, Program Logic"
    },
    "Towards Trustworthy Automated Program Verifiers: Formally Validating Translations into an Intermediate Verification Language": {
        "type": "article",
        "key": "10.1145/3656438",
        "author": "Parthasarathy, Gaurav and Dardinier, Thibault and Bonneau, Benjamin and M\\\"{u}ller, Peter and Summers, Alexander J.",
        "title": "Towards Trustworthy Automated Program Verifiers: Formally Validating Translations into an Intermediate Verification Language",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656438",
        "doi": "10.1145/3656438",
        "abstract": "Automated program verifiers are typically implemented using an intermediate verification language (IVL), such as Boogie or Why3. A verifier front-end translates the input program and specification into an IVL program, while the back-end generates proof obligations for the IVL program and employs an SMT solver to discharge them. Soundness of such verifiers therefore requires that the front-end translation faithfully captures the semantics of the input program and specification in the IVL program, and that the back-end reports success only if the IVL program is actually correct. For a verification tool to be trustworthy, these soundness conditions must be satisfied by its actual implementation, not just the program logic it uses.   In this paper, we present a novel validation methodology that, given a formal semantics for the input language and IVL, provides formal soundness guarantees for front-end implementations. For each run of the verifier, we automatically generate a proof in Isabelle showing that the correctness of the produced IVL program implies the correctness of the input program. This proof can be checked independently from the verifier, in Isabelle, and can be combined with existing work on validating back-ends to obtain an end-to-end soundness result. Our methodology based on forward simulation employs several modularisation strategies to handle the large semantic gap between the input language and the IVL, as well as the intricacies of practical, optimised translations. We present our methodology for the widely-used Viper and Boogie languages. Our evaluation shows that it is effective in validating the translations performed by the existing Viper implementation.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "208",
        "numpages": "25",
        "keywords": "Formal Semantics, Intermediate Verification Languages, Proof Certification, Software Verification"
    },
    "Towards Trustworthy Automated Program Verifiers: Formally Validating Translations into an Intermediate Verification Language -- Artifact": {
        "type": "software",
        "key": "10.5281/zenodo.10802176",
        "author": "Parthasarathy, Gaurav and Dardinier, Thibault and Bonneau, Benjamin and M\\\"{u}ller, Peter and Summers, Alexander J.",
        "title": "Towards Trustworthy Automated Program Verifiers: Formally Validating Translations into an Intermediate Verification Language -- Artifact",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10802176",
        "abstract": "    <p>This artifact includes (1) a formalisation in the Isabelle theorem prover formalising the rules and definitions in the paper, (2) our proof-producing fork of the existing Viper-to-Boogie translation (implemented in Scala), which generates proofs in Isabelle on every run, and (3) the verifier test suites on which we evaluated our tool on as well as some of the corresponding verifiers (Gobra and VerCors) to generate the corresponding Viper files. The artifact describes the Isabelle formalisation and the proof-producing fork, and shows how to do the evaluation described in the paper. The entire artifact is packaged as a virtual machine using VirtualBox.</p>",
        "keywords": "Boogie, Intermediate Verification Languages, Proof Certification, Viper"
    },
    "Live Verification in an Interactive Proof Assistant": {
        "type": "article",
        "key": "10.1145/3656439",
        "author": "Gruetter, Samuel and Fukala, Viktor and Chlipala, Adam",
        "title": "Live Verification in an Interactive Proof Assistant",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656439",
        "doi": "10.1145/3656439",
        "abstract": "We present a prototype for a tool that enables programmers to verify their code as they write it in real-time.        After each line of code that the programmer writes, the tool tells the programmer whether it was able to prove absence of undefined behavior so far, and it displays a concise representation of the symbolic state of the program right after the added line.        The user can then either write the next line of code, or if needed or desired, write a specially marked comment that provides hints on how to solve side conditions or on how to represent the symbolic state more nicely.        Once the programmer has finished writing the program, it is already verified with a mathematical correctness proof.        Other tools providing real-time feedback already exist, but ours is the first one that only relies on a small trusted proof checker and that provides a concise summary of the symbolic state at the point in the program currently being edited, as opposed to only indicating whether user-stated assertions or postconditions hold.                Program verification requires loop invariants, which are hard to find and tedious to spell out.        We explore a middle ground in the design space between the two extremes of requiring users to spell out loop invariants manually and attempting to infer loop invariants automatically:        Since a loop invariant often looks quite similar to the symbolic state right before the loop, our tool asks the user to express the desired loop invariant as a diff from the symbolic state before the loop, which has the potential to lead to shorter, more maintainable proofs.                We prototyped our technique in the interactive proof assistant Coq, so our framework creates machine-checked proofs that the developed functions satisfy their specifications when executed according to the formal semantics of the source language.        Using a verified compiler proven against the same source-language semantics, we can ensure that the behavior of the compiled program matches the program's behavior as represented by the framework during the proof.        Additionally, since our polyglot source files can be viewed as Coq or C files at the same time, users willing to accept a larger trusted code base can compile them with GCC.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "209",
        "numpages": "24",
        "keywords": "interactive proof assistants, software verification, symbolic execution"
    },
    "Code Artifact for Live Verification in an Interactive Proof Assistant": {
        "type": "software",
        "key": "10.5281/zenodo.10806323",
        "author": "Gruetter, Samuel and Fukala, Viktor and Chlipala, Adam",
        "title": "Code Artifact for Live Verification in an Interactive Proof Assistant",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10806323",
        "abstract": "    <p>Code artifact submitted to Artifact Evaluation</p>",
        "keywords": "interactive proof assistants, software verification, symbolic execution"
    },
    "Bringing the WebAssembly Standard up to Speed with SpecTec": {
        "type": "article",
        "key": "10.1145/3656440",
        "author": "Youn, Dongjun and Shin, Wonho and Lee, Jaehyun and Ryu, Sukyoung and Breitner, Joachim and Gardner, Philippa and Lindley, Sam and Pretnar, Matija and Rao, Xiaojia and Watt, Conrad and Rossberg, Andreas",
        "title": "Bringing the WebAssembly Standard up to Speed with SpecTec",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656440",
        "doi": "10.1145/3656440",
        "abstract": "WebAssembly (Wasm) is a portable low-level bytecode language and virtual machine that has seen increasing use in a variety of ecosystems.  Its specification is unusually rigorous \u2013 including a full formal semantics for the language \u2013 and every new feature must be specified in this formal semantics, in prose, and in the official reference interpreter before it can be standardized.  With the growing size of the language, this manual process with its redundancies has become laborious and error-prone, and in this work, we offer a solution.   We present SpecTec, a domain-specific language (DSL) and toolchain that facilitates both the Wasm specification and the generation of artifacts necessary to standardize new features.  SpecTec serves as a single source of truth \u2014 from a SpecTec definition of the Wasm semantics, we can generate a typeset specification, including formal definitions and prose pseudocode descriptions, and a meta-level interpreter.  Further backends for test generation and interactive theorem proving are planned.  We evaluate SpecTec\u2019s ability to represent the latest Wasm 2.0  and show that the generated meta-level interpreter passes 100\\% of the applicable official test suite.  We show that SpecTec is highly effective at discovering and preventing errors  by detecting historical errors in the specification that have been corrected  and ten errors in five proposals ready for inclusion in the next version of Wasm.  Our ultimate aim is that SpecTec should be adopted by the Wasm standards community and used to specify future versions of the standard.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "210",
        "numpages": "26",
        "keywords": "DSL, WebAssembly, executable prose, language specification"
    },
    "Artifact for \"Bringing the WebAssembly Standard up to Speed with SpecTec": {
        "type": "software",
        "key": "10.5281/zenodo.10807169",
        "author": "Youn, Dongjun and Shin, Wonho and Lee, Jaehyun and Ryu, Sukyoung and Breitner, Joachim and Gardner, Philippa and Lindley, Sam and Pretnar, Matija and Rao, Xiaojia and Watt, Conrad and Rossberg, Andreas",
        "title": "Artifact for \"Bringing the WebAssembly Standard up to Speed with SpecTec",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10807169",
        "abstract": "    <p>Artifact for \u201cBringing the WebAssembly Standard up to Speed with SpecTec\u201d, containing the source code and evaluation results.</p>",
        "keywords": "DSL, executable prose, language specification, WebAssembly"
    },
    "Space-Efficient Polymorphic Gradual Typing, Mostly Parametric": {
        "type": "article",
        "key": "10.1145/3656441",
        "author": "Igarashi, Atsushi and Ozaki, Shota and Sekiyama, Taro and Tanabe, Yudai",
        "title": "Space-Efficient Polymorphic Gradual Typing, Mostly Parametric",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656441",
        "doi": "10.1145/3656441",
        "abstract": "Since the arrival of gradual typing, which allows partially typed code in a single program, efficient implementations of gradual typing have been an active research topic. In this paper, we study the space-efficiency problem of gradual typing in the presence of parametric polymorphism. Based on the existing work that showed the impossibility of a space-efficient implementation that supports fully parametric polymorphism, this paper will show that a space-efficient implementation is, in principle, possible by slightly relaxing parametricity. We first develop \u03bbCmp, which is a coercion calculus with mostly parametric polymorphism, and show its relaxed parametricity. Then, we present \u03bbSmp, a space-efficient version of \u03bbCmp, and prove that \u03bbSmp programs can be executed in a space-efficient manner and that translation from \u03bbCmp to \u03bbSmp is type- and semantics-preserving.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "211",
        "numpages": "24",
        "keywords": "Gradual typing, Parametricity, Space efficiency"
    },
    "Quest Complete: The Holy Grail of Gradual Security": {
        "type": "article",
        "key": "10.1145/3656442",
        "author": "Chen, Tianyu and Siek, Jeremy G.",
        "title": "Quest Complete: The Holy Grail of Gradual Security",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656442",
        "doi": "10.1145/3656442",
        "abstract": "Languages with gradual information-flow control combine static and dynamic techniques to prevent security leaks. Gradual languages should satisfy the gradual guarantee: programs that only differ in the precision of their type annotations should behave the same modulo cast errors. Unfortunately, Toro et al. [2018] identify a tension between the gradual guarantee and information security; they were unable to satisfy both properties in the language GSLRef and had to settle for only satisfying information-flow security. Azevedo de Amorim et al. [2020] show that by sacrificing type-guided classification, one obtains a language that satisfies both noninterference and the gradual guarantee. Bichhawat et al. [2021] show that both properties can be satisfied by sacrificing the no-sensitive-upgrade mechanism, replacing it with a static analysis.  In this paper we present a language design, \ud835\udf06\u2605IFC, that satisfies both noninterference and the gradual guarantee without making any sacrifices. We keep the type-guided classification of GSLRef and use the standard no-sensitive-upgrade mechanism to prevent implicit flows through mutable references. The key to the design of \ud835\udf06\u2605IFC is to walk back the decision in GSLRef to include the unknown label \u2605 among the runtime security labels. We give a formal definition of \ud835\udf06\u2605IFC, prove the gradual guarantee, and prove noninterference. Of technical note, the semantics of \ud835\udf06\u2605IFC is the first gradual information-flow control language to be specified using coercion calculi (a la Henglein), thereby expanding the coercion-based theory of gradual typing.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "212",
        "numpages": "24",
        "keywords": "Agda, gradual typing, information flow security, machine-checked proofs"
    },
    "Agda code for 'Quest Complete: The Holy Grail of Gradual Security'": {
        "type": "software",
        "key": "10.5281/zenodo.10933110",
        "author": "Chen, Tianyu and Siek, Jeremy G.",
        "title": "Agda code for 'Quest Complete: The Holy Grail of Gradual Security'",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10933110",
        "abstract": "    <p>The artifact contains Agda code of the definitions and proofs in the paper \u2018Quest Complete: The Holy Grail of Gradual Security\u2019.</p>",
        "keywords": "Agda, gradual typing, information flow security, machine-checked proofs"
    },
    "Compatible Branch Coverage Driven Symbolic Execution for Efficient Bug Finding": {
        "type": "article",
        "key": "10.1145/3656443",
        "author": "Yi, Qiuping and Yu, Yifan and Yang, Guowei",
        "title": "Compatible Branch Coverage Driven Symbolic Execution for Efficient Bug Finding",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656443",
        "doi": "10.1145/3656443",
        "abstract": "Symbolic execution is a powerful technique for bug finding by generating test inputs to systematically explore all feasible paths within a given threshold. However, its practical usage is often limited by the path explosion problem. In this paper, we propose compatible branch coverage driven symbolic execution for efficient bug finding. Our new technique owns a novel path-pruning strategy obtained from program dependency analysis to effectively avoid unnecessary explorations. Specifically, based on a Compatible Branch Set, our technique directs symbolic execution to explore feasible branches while soundly pruning redundant paths that have no new contributions to branch coverage. We have implemented our approach atop KLEE and conducted experiments on a set of programs from Siemens Suite, GNU Coreutils, and other real-world programs. Experimental results show that, compared with the state-of-the-art symbolic execution techniques, our approach always uses significantly less time to reproduce bugs while achieving the same or better branch coverage. On average, our approach got over 45\\% path reduction and 3x speedup on the GNU Coreutils programs.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "213",
        "numpages": "23",
        "keywords": "program analysis, software testing, symbolic execution"
    },
    "Reproduction Package For Article `Compatible Branch Coverage Driven Symbolic Execution for Efficient Bug Finding`": {
        "type": "software",
        "key": "10.5281/zenodo.10960926",
        "author": "Yi, Qiuping and Yu, Yifan and Yang, Guowei",
        "title": "Reproduction Package For Article `Compatible Branch Coverage Driven Symbolic Execution for Efficient Bug Finding`",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10960926",
        "abstract": "    <p>The current artifact comprises all the tool source code related to the paper, along with the scripts and data needed to reproduce the experiments. We provide both source code and Docker build options.</p>",
        "keywords": "program analysis, software testing, symbolic execution"
    },
    "RichWasm: Bringing Safe, Fine-Grained, Shared-Memory Interoperability Down to WebAssembly": {
        "type": "article",
        "key": "10.1145/3656444",
        "author": "Fitzgibbons, Michael and Paraskevopoulou, Zoe and Mushtak, Noble and Thalakottur, Michelle and Sulaiman Manzur, Jose and Ahmed, Amal",
        "title": "RichWasm: Bringing Safe, Fine-Grained, Shared-Memory Interoperability Down to WebAssembly",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656444",
        "doi": "10.1145/3656444",
        "abstract": "Safe, shared-memory interoperability between languages with different type systems and memory-safety guarantees is an intricate problem as crossing language boundaries may result in memory-safety violations. In this paper, we present RichWasm, a novel richly typed intermediate language designed to serve as a compilation target for typed high-level languages with different memory-safety guarantees. RichWasm is based on WebAssembly and enables safe shared-memory interoperability by incorporating a variety of type features that support fine-grained memory ownership and sharing. RichWasm is rich enough to serve as a typed compilation target for both typed garbage-collected languages and languages with an ownership-based type system and manually managed memory. We demonstrate this by providing compilers from core ML and L3, a type-safe language with strong updates, to RichWasm. RichWasm is compiled to regular Wasm, allowing for use in existing environments. We formalize RichWasm in Coq and prove type safety.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "214",
        "numpages": "24",
        "keywords": "WebAssembly, capability types, memory ownership, memory sharing"
    },
    "RichWasm Artifact": {
        "type": "software",
        "key": "10.5281/zenodo.10906088",
        "author": "Fitzgibbons, Michael and Paraskevopoulou, Zoe and Mushtak, Noble and Thalakottur, Michelle and Sulaiman Manzur, Jose and Ahmed, Amal",
        "title": "RichWasm Artifact",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10906088",
        "abstract": "    <p>This artifact is a self-contained environment to reproduce the claims in the PLDI\u201924 paper \u201cRichWasm: Bringing Safe, Fine-Grained, Shared-Memory Interoperability Down to WebAssembly\u201d. This artifact contains, a mechanized proof of RichWasm\u2019s type safety, compilers from ML and L3 to RIchWasm, an annotator and type checker for RichWasm code and a compiler from RichWasm to WebAssembly. This artifact can be used to compile the proofs, use the various compilers and run and inspect their tests.</p>",
        "keywords": "RichWasm, Type-Preserving Compilation, WebAssembly"
    },
    "SpEQ: Translation of Sparse Codes using Equivalences": {
        "type": "software",
        "key": "10.5281/zenodo.10906216",
        "author": "Laird, Avery and Liu, Bangtian and Bj\\o{}rner, Nikolaj and Dehnavi, Maryam Mehri",
        "title": "SpEQ: Translation of Sparse Codes using Equivalences",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10906216",
        "abstract": "    <p>Software to replicate the results of \u201cSpEQ: Translation of Sparse Codes using Equivalences.\u201d</p>",
        "keywords": "Equality Saturation, Equivalence Checking, Program Analysis, Verification"
    },
    "Foundational Integration Verification of a Cryptographic Server": {
        "type": "article",
        "key": "10.1145/3656446",
        "author": "Erbsen, Andres and Philipoom, Jade and Jamner, Dustin and Lin, Ashley and Gruetter, Samuel and Pit-Claudel, Cl\\'{e}ment and Chlipala, Adam",
        "title": "Foundational Integration Verification of a Cryptographic Server",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656446",
        "doi": "10.1145/3656446",
        "abstract": "We present verification of a bare-metal server built using diverse implementation techniques and languages against a whole-system input-output specification in terms of machine code, network packets, and mathematical specifications of elliptic-curve cryptography. We used very different formal-reasoning techniques throughout the stack, ranging from computer algebra, symbolic execution, and verification-condition generation to interactive verification of functional programs including compilers for C-like and functional languages. All these component specifications and domain-specific reasoning techniques are defined and justified against common foundations in the Coq proof assistant. Connecting these components is a minimalistic specification style based on functional programs and assertions over simple objects, omnisemantics for program execution, and basic separation logic for memory layout. This design enables us to bring the components together in a top-level correctness theorem that can be audited without understanding or trusting the internal interfaces and tools. Our case study is a simple cryptographic server for flipping of a bit of state through public-key authenticated network messages, and its proof shows total functional correctness including static bounds on memory usage. This paper also describes our experiences with the specific verification tools we build upon, along with detailed analysis of reasons behind the widely varying levels of productivity we experienced between combinations of tools and tasks.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "216",
        "numpages": "26",
        "keywords": "bare-metal programming, elliptic-curve cryptography, proof assistants"
    },
    "Proof Artifact for `Foundational Integration Verification of a Cryptographic Server'": {
        "type": "software",
        "key": "10.5281/zenodo.10807084",
        "author": "Erbsen, Andres and Philipoom, Jade and Jamner, Dustin and Lin, Ashley and Gruetter, Samuel and Pit-Claudel, Cl\\'{e}ment and Chlipala, Adam",
        "title": "Proof Artifact for `Foundational Integration Verification of a Cryptographic Server'",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10807084",
        "abstract": "    <p>This is the computer-checked-proof artifact for `Foundational Integration Verification of a Cryptographic Server\u2019. It contains the component proofs, integration proofs, and software for checking them, supporting all verification claims in the paper. Static quantiative-evaluation claims about memory usage are also supported by these proofs.</p>",
        "keywords": "bare-metal programming, elliptic-curve cryptography, proof assistants"
    },
    "Reward-Guided Synthesis of Intelligent Agents with Control Structures": {
        "type": "article",
        "key": "10.1145/3656447",
        "author": "Cui, Guofeng and Wang, Yuning and Qiu, Wenjie and Zhu, He",
        "title": "Reward-Guided Synthesis of Intelligent Agents with Control Structures",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656447",
        "doi": "10.1145/3656447",
        "abstract": "Deep reinforcement learning (RL) has led to encouraging successes in numerous challenging robotics applications. However, the lack of inductive biases to support logic deduction and generalization in the representation of a deep RL model causes it less effective in exploring complex long-horizon robot-control tasks with sparse reward signals. Existing program synthesis algorithms for RL problems inherit the same limitation, as they either adapt conventional RL algorithms to guide program search or synthesize robot-control programs to imitate an RL model. We propose ReGuS, a reward-guided synthesis paradigm, to unlock the potential of program synthesis to overcome the exploration challenges. We develop a novel hierarchical synthesis algorithm with decomposed search space for loops, on-demand synthesis of conditional statements, and curriculum synthesis for procedure calls, to effectively compress the exploration space for long-horizon, multi-stage, and procedural robot-control tasks that are difficult to address by conventional RL techniques. Experiment results demonstrate that ReGuS significantly outperforms state-of-the-art RL algorithms and standard program synthesis baselines on challenging robot tasks including autonomous driving, locomotion control, and object manipulation.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "217",
        "numpages": "25",
        "keywords": "Program Synthesis, Sequential Decision Making"
    },
    "ReGuS - Reproduction Package for Article `Reward-Guided Synthesis of Intelligent Agents with Control Structures`": {
        "type": "software",
        "key": "10.5281/zenodo.10976438",
        "author": "Cui, Guofeng and Wang, Yuning and Qiu, Wenjie and Zhu, He",
        "title": "ReGuS - Reproduction Package for Article `Reward-Guided Synthesis of Intelligent Agents with Control Structures`",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10976438",
        "abstract": "    <p>This artifact is provided as a Docker image. Before proceeding, ensure you have Docker installed. The artifact was tested with Docker version 20.10.23. We recommend that your machine has at least 16GB of memory and 16GB of available disk space for building and running Docker images. All benchmarks were tested on a Mac Mini 2023 with an Apple M2 Pro CPU and 16GB of RAM. Please refer to the README file for instructions on reproducing the experiments.</p>",
        "keywords": "Program Synthesis, Reinforcement Learning, Sequential Decision Making"
    },
    "Compiling Probabilistic Programs for Variable Elimination with Information Flow": {
        "type": "article",
        "key": "10.1145/3656448",
        "author": "Li, Jianlin and Wang, Eric and Zhang, Yizhou",
        "title": "Compiling Probabilistic Programs for Variable Elimination with Information Flow",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656448",
        "doi": "10.1145/3656448",
        "abstract": "A key promise of probabilistic programming is the ability to specify rich models using an expressive program- ming language. However, the expressive power that makes probabilistic programming languages enticing also poses challenges to inference, so much so that specialized approaches to inference ban language features such as recursion. We present an approach to variable elimination and marginal inference for probabilistic programs featuring bounded recursion, discrete distributions, and sometimes continuous distributions. A compiler eliminates probabilistic side effects, using a novel information-flow type system to factorize probabilistic computations and hoist independent subcomputations out of sums or integrals. For a broad class of recursive programs with dynamically recurring substructure, the compiler effectively decomposes a global marginal-inference problem, which may otherwise be intractable, into tractable subproblems. We prove the compilation correct by showing that it preserves denotational semantics. Experiments show that the compiled programs subsume widely used PTIME algorithms for recursive models and that the compilation time scales with the size of the inference problems. As a separate contribution, we develop a denotational, logical-relations model of information-flow types in the novel measure-theoretic setting of probabilistic programming; we use it to prove noninterference and consequently the correctness of variable elimination.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "218",
        "numpages": "26",
        "keywords": "Probabilistic programming, information flow"
    },
    "Artifact for Paper 'Variable Elimination for an Expressive Probabilistic Programming Language'": {
        "type": "software",
        "key": "10.5281/zenodo.10951893",
        "author": "Li, Jianlin and Wang, Eric and Zhang, Yizhou",
        "title": "Artifact for Paper 'Variable Elimination for an Expressive Probabilistic Programming Language'",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10951893",
        "abstract": "    <p>This repository contains the tool source code, benchmarks and instructions to reproduce the results in paper \u2018Variable Elimination for an Expressive Probabilistic Programming Language\u2019.</p>",
        "keywords": "compiler, continuation-passing style, CPS, information flow type system, probabilistic programming, type checker, type system, variable elimination"
    },
    "SPORE: Combining Symmetry and Partial Order Reduction": {
        "type": "article",
        "key": "10.1145/3656449",
        "author": "Kokologiannakis, Michalis and Marmanis, Iason and Vafeiadis, Viktor",
        "title": "SPORE: Combining Symmetry and Partial Order Reduction",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656449",
        "doi": "10.1145/3656449",
        "abstract": "Symmetry reduction (SR) and partial order reduction (POR) aim to scale   up model checking by exploiting the underlying program structure: SR   avoids exploring executions equivalent up to some permutation of   symmetric threads, while POR avoids exploring executions equivalent up   to reordering of independent instructions. While both SR and POR have   been well studied individually, their combination in the context of   stateless model checking has remained an open problem.     In this paper, we present SPORE, the first stateless model checker   that combines SR and POR in a sound, complete and optimal   manner. SPORE can leverage both symmetries in the client program   itself, but also internal symmetries in the underlying   implementation (i.e., idempotent operations), a novel   symmetry notion we introduce in this paper. Our experiments confirm   that SPORE explores drastically fewer executions than tools that   solely employ SR/POR, thereby greatly advancing the state-of-the-art.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "219",
        "numpages": "23",
        "keywords": "Dynamic Partial Order Reduction, Model Checking, Symmetry Reduction"
    },
    "Replication Package for \"SPORE: Combining Symmetry and Partial Order Reduction": {
        "type": "software",
        "key": "10.5281/zenodo.10971411",
        "author": "Kokologiannakis, Michalis and Marmanis, Iason and Vafeiadis, Viktor",
        "title": "Replication Package for \"SPORE: Combining Symmetry and Partial Order Reduction",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10971411",
        "abstract": "    <p>The artifact contains the tools GenMC (which implements theTruStalgorithm) and SPORE, as well as the tests used in the evaluation section of the paper.</p><p>SPORE is publicly available as part of GenMC: https://github.com/MPI-SWS/genmc.</p>",
        "keywords": "concurrency, model checking, partial order reduction, symmetry reduction, weak memory models"
    },
    "Predictable Verification using Intrinsic Definitions": {
        "type": "article",
        "key": "10.1145/3656450",
        "author": "Murali, Adithya and Rivera, Cody and Madhusudan, P.",
        "title": "Predictable Verification using Intrinsic Definitions",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656450",
        "doi": "10.1145/3656450",
        "abstract": "We propose a novel mechanism of defining data structures using intrinsic definitions that avoids recursion and instead utilizes monadic maps satisfying local conditions. We show that intrinsic definitions are a powerful mechanism that can capture a variety of data structures naturally. We show that they also enable a predictable verification methodology that allows engineers to write ghost code to update monadic maps and perform verification using reduction to decidable logics. We evaluate our methodology using Boogie and prove a suite of data structure manipulating programs correct.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "220",
        "numpages": "26",
        "keywords": "Decidability, Ghost-Code Annotations, Intrinsic Definitions, Predictable Verification, Verification of Linked Data Structures"
    },
    "Artifact for ``Predictable Verification using Intrinsic Definitions''": {
        "type": "software",
        "key": "10.5281/zenodo.10963124",
        "author": "Murali, Adithya and Rivera, Cody and Madhusudan, P.",
        "title": "Artifact for ``Predictable Verification using Intrinsic Definitions''",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10963124",
        "abstract": "    <p>This is the artifact for our paper \u201cPredictable Verification using Intrinsic Definitions\u201d.</p><p>ids-artifact.zip contains our benchmarks, while ids-docker.zip contains a Docker image. Please see README.md for instructions on how to use the artifact.</p>",
        "keywords": "Boogie, Dafny, Decidability, Ghost-Code Annotations, Intrinsic Definitions, Predictable Verification, Verification of Linked Data Structures"
    },
    "Context-Free Language Reachability via Skewed Tabulation": {
        "type": "article",
        "key": "10.1145/3656451",
        "author": "Lei, Yuxiang and Bossut, Camille and Sui, Yulei and Zhang, Qirun",
        "title": "Context-Free Language Reachability via Skewed Tabulation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656451",
        "doi": "10.1145/3656451",
        "abstract": "Context-free language reachability (CFL-reachability) is a prominent model for formulating program analysis problems. Almost all CFL-reachability algorithms are based on the Reps-Horwitz-Sagiv (RHS) tabulation. In essence, the RHS tabulation, based on normalized context-free grammars, is similar to the CYK algorithm for CFL-parsing. Consider a normalized rule S\u202f::= A B and a CFL-reachability problem instance of computing S-edges in the input graph. The RHS tabulation obtains all summary edges (i.e., S-, A-, and B-edges) based on the grammar rules. However, many A- and B-edges are wasted because only a subset of those edges eventually contributes to generating S-edges in the input graph.    This paper proposes a new tabulation strategy for speeding up CFL-reachability by eliminating wasted and unnecessary summary edges. We particularly focus on recursive nonterminals. Our key technical insight is that the wasted edge generations and insertions caused by recursive nonterminals can be avoided by modifying the parse trees either statically (by transforming the grammar) or dynamically (using a specialized online CFL-reachability solver). For example, if a recursive nonterminal B, generated by a rule B\u202f::= B X, appears on the right-hand side of a rule S\u202f::= A B, we can make S recursive (by introducing a new rule S\u202f::= S X) and eliminate the original recursive rule (B\u202f::= B X). Due to the rule S\u202f::= S X, the shapes of the parse trees associated with the left-hand-side nonterminal S become more \"skewed\". Thus, we name our approach skewed tabulation for CFL-reachability.    Skewed tabulation can significantly improve the scalability of CFL-reachability by reducing wasted and unnecessary summary edges. We have implemented skewed tabulation and applied the corresponding CFL-reachability algorithm to an alias analysis, a value-flow analysis, and a taint analysis. Our extensive evaluation based on SPEC 2017 benchmarks yields promising results. For the three client analyses, CFL-reachability based on skewed tabulation can achieve 3.34\\texttimes{}, 1.13\\texttimes{} and 2.05\\texttimes{} speedup over the state-of-the-art RHS-tabulation-based CFL-reachability solver and consume 60.05\\%, 20.38\\% and 63.06\\% less memory, respectively. Furthermore, the cost of grammar transformation for skewed tabulation is negligible, typically taking less than one second.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "221",
        "numpages": "24",
        "keywords": "CFL-reachability, performance, tabulation schemes"
    },
    "Artifact of \"Context-Free Language Reachability via Skewed Tabulation": {
        "type": "software",
        "key": "10.5281/zenodo.10892936",
        "author": "Lei, Yuxiang and Bossut, Camille and Sui, Yulei and Zhang, Qirun",
        "title": "Artifact of \"Context-Free Language Reachability via Skewed Tabulation",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10892936",
        "abstract": "    <p>This is the artifact of the paper \u201cContext-Free Language Reachability via Skewed Tabulation\u201d accepted to PLDI 2024. The artifact is packaged as a Docker image \u201ccflskewed.tar.gz\u201d, which is to reproduce the experiment results of the paper.</p>",
        "keywords": "CFL-reachability, performance, tabulation schemes"
    },
    "Falcon: A Scalable Analytical Cache Model": {
        "type": "article",
        "key": "10.1145/3656452",
        "author": "Pitchanathan, Arjun and Grover, Kunwar and Grosser, Tobias",
        "title": "Falcon: A Scalable Analytical Cache Model",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656452",
        "doi": "10.1145/3656452",
        "abstract": "Compilers often use performance models to decide how to optimize code. This is often preferred over using hardware performance measurements, since hardware measurements can be expensive, limited by hardware availability, and makes the output of compilation non-deterministic. Analytical models, on the other hand, serve as efficient and noise-free performance indicators. Since many optimizations focus on improving memory performance, memory cache miss rate estimations can serve as an effective and noise-free performance indicator for superoptimizers, worst-case execution time analyses, manual program optimization, and many other performance-focused use cases. Existing methods to model the cache behavior of affine programs work on small programs such as those in the Polybench benchmark but do not scale to the larger programs we would like to optimize in production, which can be orders of magnitude bigger by lines of code. These analytical approaches hand of the whole program to a Presburger solver and perform expensive mathematical operations on the huge resulting formulas. We develop a scalable cache model for affine programs that splits the computation into smaller pieces that do not trigger the worst-case asymptotic behavior of these solvers. We evaluate our approach on 46 TorchVision neural networks, finding that our model has a geomean runtime of 44.9 seconds compared to over 32 minutes for the state-of-the-art prior cache model, and the latter is actually smaller than the true value because the prior model reached our four hour time limit on 54\\% of the networks, and this limit was never reached by our tool. Our model exploits parallelism effectively: running it on sixteen cores is 8.2x faster than running it single-threaded. While the state-of-the-art model takes over four hours to analyze a majority of the benchmark programs, Falcon produces results in at most 3 minutes and 3 seconds; moreover, after a local modification to the program being analyzed, our model efficiently updates the predictions in 513 ms on average (geomean). Thus, we provide the first scalable analytical cache model.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "222",
        "numpages": "25",
        "keywords": "cache modeling, performance analysis, static analysis"
    },
    "Artifact for \"Falcon: A Scalable Analytical Cache Model": {
        "type": "software",
        "key": "10.5281/zenodo.10972076",
        "author": "Pitchanathan, Arjun and Grover, Kunwar and Grosser, Tobias",
        "title": "Artifact for \"Falcon: A Scalable Analytical Cache Model",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10972076",
        "abstract": "    <h2 id=\"artifact-for-falcon-a-scalable-analytical-cache-model\">Artifact for Falcon: A Scalable Analytical Cache Model</h2><p>This is the supporting artifact for the Falcon paper. It can be used to replicate all results in the submitted version of the paper (submission_paper.pdf), given enough time and appropriate hardware. To be precise, it takes around two weeks to reproduce all the results.</p><h3 id=\"quick-version-of-the-artifact\">Quick Version of the Artifact</h3><p>To facilitate evaluation, we also provide a \u201cquick\u201d version of the artifact that reproduces the main results. It reproduces all the evaluation figures in the paper, with the following differences:</p><ul><li><p>Figure 1: we run the models at all x values of 20 up to 100 instead of multiples of 5. This is sufficient to establish the same trend.</p></li><li><p>Figure 6 \\&amp; 7: we evaluate on the last four benchmarks files in the ordering of Figure 6, showing that even on the \u201cworst\u201d inputs in the benchmark, Falcon takes minutes. (base models time out after four hours)</p></li><li><p>Figure 8: we evaluate on thread counts [1, 2, 4, 8, 12, 16] instead of all counts up to 16. This is sufficient to establish the same trend.</p></li></ul><p>All of the above choices can be easily customized by the user before running the artifact; see \u201cRunning the Artifact\u201d below.</p><h3 id=\"hardware-requirements\">Hardware Requirements</h3><h4 id=\"requirements-for-the-quick-version\">Requirements for the quick version</h4><p>For hardware measurement, our method requires a machine with an AMD Zen3/Zen4 CPU and access to the <code>perf_event_open</code> syscall. Note that many cloud machines disallow this syscall. If such a machine is not available, you can still use the hardware measurement data from our machine and run the rest of the artifact. The only difference will be that the accuracy figures will be plotted against our measurement data on our machine instead of yours.</p><p>For the parallelism experiment, a machine with 16 cores is required. If there are fewer cores, then running on 16 threads will not improve performance as much so the speedup would be less than that reported in the paper. Other than that, the artifact will still work fine on a machine with fewer cores.</p><h4 id=\"requirements-for-the-full-version\">Requirements for the full version</h4><p>For the complete version, a machine with 192 GiB RAM is required. This is because the baseline model Haystack that we compare against can sometimes take a large amount of RAM.</p><p>When the full artifact is run on a machine with insufficient RAM, if Haystack runs out of memory when running some file, that file will be gracefully dropped from Figure 6. Otherwise, the rest of the artifact will continue to function normally. In such a scenario it may help system stability to run <code>./earlyoom.sh</code> before running the models, though when we tested on a low RAM machine, we did not find this to be necessary.</p><p>It may be difficult to obtain a single machine satisfying the high RAM requirement as well as the requirement to have access to the <code>perf_event_open</code> syscall, as the latter is often not available on cloud machines. Therefore, we provide the option to run each part on a different machine, as long as one machine is available with high RAM and another with the requirements specific for hardware measurements.</p><h3 id=\"software-requirements\">Software Requirements</h3><p>The artifact requires <a href=\"https://docs.docker.com/get-docker/\">Docker</a>. We tested on version <code>24.0.6</code> on a Linux machine.</p><h3 id=\"getting-started\">Getting Started</h3><p>The artifact comes with pre-built binaries. To rebuild from scratch, see that section below. To setup the artifact and docker image:</p><ol type=\"1\"><li>Extract the provided archive and <code>cd</code> into the extracted directory.</li><li>Load the provided docker image with <code>docker load -i docker/docker_image.tar</code>.</li><li>Run the image with <code>docker run -v $(pwd):/app -it --security-opt seccomp=docker/seccomp.json falcon-artifact</code></li></ol><p>This mounts the project root directory (which should be the current directory) to the VM. Changes made in the VM will be persisted here.</p><p>The argument <code>--security-opt seccomp=docker/seccomp.json</code> loads a custom security configuration. The only difference between the custom one and the default is that the <code>perf_event_open</code> syscall is permitted, which is required for hardware measurement. The argument can be omitted if hardware measurement is not needed. If the measurement test assert-fails as described in the next section even though you expect it to work on your system, you can try adding the flag <code>--privileged</code>, though we did not need it during testing.</p><p>Note that during development, our tool was called <code>lazystack</code>, so it is referred to as such in scripts and source code.</p><h4 id=\"test-running-hardware-measurement\">Test-running hardware measurement</h4><p>To test the hardware measurement, run <code>examples/measurement-example</code>. If it succeeds, the output will contain four numbers. On our system, we got:</p><pre><code>0.1813413591521698168</code></pre><p>The first output number is runtime and the next three are cache accesses and misses; none of these numbers are expected to be zero. If any of the last three lines are zero then your CPU is probably unsupported. In this case, you can use the measurement data from our system (see below for more details).</p><p>On the other hand, if the <code>perf_event_open</code> syscall is not supported, an error like the following will be reported:</p><pre><code>measurement-example: ../src/c-gen-perf-main.cpp:66: read_format&lt;nr&gt; disable_and_get_count_group(int) [nr = 3U]: Assertion `data.nr == nr' failed.Aborted (core dumped)</code></pre><h4 id=\"test-running-the-cache-models\">Test-running the cache models</h4><p><code>cd</code> into the <code>experiments</code> directory and run <code>python perf-all.py -b polybench-S -c 512,512 --output-suffix test --filter gemm</code>. This should produce output like the following.</p><pre><code>root@53846d9af34b:/app/experiments# python perf-all.py -b polybench-S -c 512,512 --output-suffix test --filter gemmWill output to data/perf-polybench-S-512-512-test.jsonRunnning haystack on gemmRunning warping on gemmRunning lazystack on gemm... 78ms{  \"gemm\": {    \"haystack\": {      \"L1\": 0",
        "\"time\": 293.0   } }}Saving to data/perf-polybench-S-512-512-test.json</code></pre><p>The main thing to check is that all three models haystack, lazystack, and warping ran successfully, producing a section in the output for each.</p><h3 id": "running-the-experiments\">Running the experiments</h3><p>To perform experiments, <code>cd</code> into the <code>experiments</code> directory.</p><p>To perform hardware measurement, run <code>./run_measurement.py</code>. This may take around 4 hours and should be done on a machine with an AMD Zen3/Zen4 CPU and access to the <code>perf_event_open</code> syscall. You should run the docker image with the provided seccomp for this part. Now run <code>./get_system_cache_conf.sh</code> on the same machine and note down the two numbers; these are the number of cache lines in the cache and the size of each cache line in bytes.</p><p>To run the full version of the model evaluation, run <code>./run_prediction.sh &lt;cache line count&gt; &lt;line size&gt;</code>. This may take around 2 weeks and requires a machine with 192 GiB of RAM.</p><p>To run the quick version of the model evaluation, run <code>./run_prediction_fast.sh &lt;cache line count&gt; &lt;line size&gt;</code>. This may take around 2-3 days. If you want to customize any of the parameters of the quick run described in the introduction of this document, it is easy to do so by modifying the environment variables exported in that script; they are well documented.</p><p>For more info on manually customizing the running of the benchmarks beyond the provided shell scripts see <code>MoreInfo.md</code>.</p><h4 id=\"using-our-hardware-measurement-data-in-case-of-unsupported-cpu\">Using our hardware measurement data in case of unsupported CPU</h4><p>If you do not have a supported AMD CPU for the hardware measurement, you can use our hardware measurement data. To do this, simply copy the <code>perf-perf-1.json</code> file from <code>experiments</code> into the <code>experiments/data</code> directory and run the prediction scripts as usual. Our machine has 512 cache lines and the line size is 64 (bytes); use those settings when running the prediction. You can then proceed to plotting (see below).</p><h3 id=\"plotting-and-comparing-results\">Plotting and comparing results</h3><p>To plot the data collected, run <code>./plot.sh</code> in the <code>experiments</code> directory. This will generate a report in <code>experiments/report/report.pdf</code>. This document lists all the generated figures along with their figure number in the submitted paper.</p><p>You can now compare these figures with those in the submitted paper <code>submitted_paper.pdf</code> and confirm that the interpretations in the captions of the figures in the submission continue to hold in the figures in the report generated from the artifact.</p><p>If you ran Polybench, you can print the speedups on it with <code>python print_polybench_speedup.py</code>. It also mentions the speedup reported in the paper with line number for reference.</p><p>Finally, in the paper we checked our tool\u2019s correctness by comparing the outputs against Haystack. Run <code>python check.py</code> to replicate this; it will compare against all available Haystack outputs from the artifact.</p><h3 id=\"building-from-scratch\">Building from Scratch</h3><p>The artifact comes with pre-built binaries for convenience, but can support a fresh build too. To do so, first run <code>./clean_all.sh</code> to delete pre-built data.</p><p>To build the binaries used for hardware measurement, run <code>./setup_measurement.sh</code></p><p>To build the cache models, run <code>./setup_prediction.sh</code>. The binary of our tool will be produced in <code>cmake-build-release/bin/lazystack</code>, which can be used for manually running it on a given MLIR file.</p><h3 id=\"source-code-organization\">Source Code Organization</h3><p>The source code of our tool is in the directories <code>include</code>, <code>src</code>, and <code>lib</code>. Some modifications have also been made to the external libraries in <code>polybase/barvinok</code> and <code>polybase/isl</code>. The entrypoint into the actual cache model is in the <code>CacheModel::compute</code> function in <code>lib/Analysis/CacheModel.cpp</code>.</p><p>The key function is the <code>CacheModel::computeSink</code> function. This computes the cache misses at each level, for the given sink in the program. This corresponds to the body of the loop in Algorithm 1 in the paper.</p><p>The dependences are computed by the call to <code>Lazy::compute</code>, implemented in <code>lib/Analysis/Lazy.cpp</code>. Threshold couting is performed in the call to <code>ThresholdCounting::compute</code>, implemented in <code>lib/Analysis/ThresholdCounting.cpp</code>.</p>",
        "keywords": "cache modeling, performance analysis, static analysis"
    },
    "Equivalence by Canonicalization for Synthesis-Backed Refactoring": {
        "type": "article",
        "key": "10.1145/3656453",
        "author": "Lubin, Justin and Ferguson, Jeremy and Ye, Kevin and Yim, Jacob and Chasins, Sarah E.",
        "title": "Equivalence by Canonicalization for Synthesis-Backed Refactoring",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656453",
        "doi": "10.1145/3656453",
        "abstract": "We present an enumerative program synthesis framework called component-based refactoring that can refactor \"direct\" style code that does not use library components into equivalent \"combinator\" style code that does use library components. This framework introduces a sound but incomplete technique to check the equivalence of direct code and combinator code called equivalence by canonicalization that does not rely on input-output examples or logical specifications. Moreover, our approach can repurpose existing compiler optimizations, leveraging decades of research from the programming languages community. We instantiated our new synthesis framework in two contexts: (i) higher-order functional combinators such as map and filter in the statically-typed functional programming language Elm and (ii) high-performance numerical computing combinators provided by the NumPy library for Python. We implemented both instantiations in a tool called Cobbler and evaluated it on thousands of real programs to test the performance of the component-based refactoring framework in terms of execution time and output quality. Our work offers evidence that synthesis-backed refactoring can apply across a range of domains without specification beyond the input program.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "223",
        "numpages": "26",
        "keywords": "Program Equivalence Checking, Program Synthesis, Refactoring"
    },
    "Reproduction Package for \"Equivalence by Canonicalization for Synthesis-Backed Refactoring": {
        "type": "software",
        "key": "10.5281/zenodo.10802503",
        "author": "Lubin, Justin and Ferguson, Jeremy and Ye, Kevin and Yim, Jacob and Chasins, Sarah E.",
        "title": "Reproduction Package for \"Equivalence by Canonicalization for Synthesis-Backed Refactoring",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10802503",
        "abstract": "    <p>This artifact is a self-contained virtual machine for the artifact evaluation of the paper \u201cEquivalence by Canonicalization for Synthesis-Backed Refactoring.\u201d It includes our program synthesizer (Cobbler) as well as the data necessary for our empirical evaluation.</p>",
        "keywords": "Program Equivalence Checking, Program Synthesis, Refactoring"
    },
    "KATch: A Fast Symbolic Verifier for NetKAT": {
        "type": "software",
        "key": "10.5281/zenodo.10961123",
        "author": "Moeller, Mark and Jacobs, Jules and Belanger, Olivier Savary and Darais, David and Schlesinger, Cole and Smolka, Steffen and Foster, Nate and Silva, Alexandra",
        "title": "KATch: A Fast Symbolic Verifier for NetKAT",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10961123",
        "abstract": "    <p>The artifact is the Scala implementation of the symbolic NetKAT verifier described in the paper, along with the NetKAT input files corresponding to the benchmark sets.</p>",
        "keywords": "Automata equivalence, Kleene Algebra with Tests, NetKAT Verifier, Network Verification"
    },
    "Hyperblock Scheduling for Verified High-Level Synthesis": {
        "type": "article",
        "key": "10.1145/3656455",
        "author": "Herklotz, Yann and Wickerson, John",
        "title": "Hyperblock Scheduling for Verified High-Level Synthesis",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656455",
        "doi": "10.1145/3656455",
        "abstract": "High-level synthesis (HLS) is the automatic compilation of software programs into custom hardware designs. With programmable hardware devices (such as FPGAs) now widespread, HLS is increasingly relied upon, but existing HLS tools are too unreliable for safety- and security-critical applications. Herklotz et al. partially addressed this concern by building Vericert, a prototype HLS tool that is proven correct in Coq (\\`{a} la CompCert), but it cannot compete performance-wise with unverified tools. This paper reports on our efforts to close this performance gap, thus obtaining the first practical verified HLS tool. We achieve this by implementing a flexible operation scheduler based on hyperblocks (basic blocks of predicated instructions) that supports operation chaining (packing dependent operations into a single clock cycle). Correctness is proven via translation validation: each schedule is checked using a Coq-verified validator that uses a SAT solver to reason about predicates. Avoiding exponential blow-up in this validation process is a key challenge, which we address by using final-state predicates and value summaries. Experiments on the PolyBench/C suite indicate that scheduling makes Vericert-generated hardware 2.1\\texttimes{} faster, thus bringing Vericert into competition with a state-of-the-art open-source HLS tool when a similar set of optimisations is enabled.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "225",
        "numpages": "25",
        "keywords": "CompCert, Coq, operation chaining, symbolic evaluation, translation validation"
    },
    "Artefact: Hyperblock Scheduling for Verified High-Level Synthesis": {
        "type": "software",
        "key": "10.5281/zenodo.10808233",
        "author": "Herklotz, Yann and Wickerson, John",
        "title": "Artefact: Hyperblock Scheduling for Verified High-Level Synthesis",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10808233",
        "abstract": "    <p>Artefact of the implementation and verification of hyperblock scheduling on top of an existing verified high-level synthesis tool called Vericert. The artefact includes a VM with all software pre-installed to reproduce the results of the paper. The instructions can be found in README.pdf and README.md.</p>",
        "keywords": "CompCert, Coq, operation chaining, symbolic evaluation, translation validation"
    },
    "Numerical Fuzz: A Type System for Rounding Error Analysis": {
        "type": "article",
        "key": "10.1145/3656456",
        "author": "Kellison, Ariel E. and Hsu, Justin",
        "title": "Numerical Fuzz: A Type System for Rounding Error Analysis",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656456",
        "doi": "10.1145/3656456",
        "abstract": "Algorithms operating on real numbers are implemented as floating-point computations in practice, but floating-point operations introduce roundoff errors that can degrade the accuracy of the result. We propose \u039bnum, a functional programming language with a type system that can express quantitative bounds on roundoff error. Our type system combines a sensitivity analysis, enforced through a linear typing discipline, with a novel graded monad to track the accumulation of roundoff errors. We prove that our type system is sound by relating the denotational semantics of our language to the exact and floating-point operational semantics. To demonstrate our system, we instantiate \u039bnum with error metrics proposed in the numerical analysis literature and we show how to incorporate rounding operations that faithfully model aspects of the IEEE 754 floating-point standard. To show that \u039bnum can be a useful tool for automated error analysis, we develop a prototype implementation for \u039bnum that infers error bounds that are competitive with existing tools, while often running significantly faster. Finally, we consider semantic extensions of our graded monad to bound error under more complex rounding behaviors, such as non-deterministic and randomized rounding.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "226",
        "numpages": "25",
        "keywords": "Floating point, Linear type systems, Roundoff error"
    },
    "Artifact for Numerical Fuzz: A Type System for Rounding Error Analysis": {
        "type": "software",
        "key": "10.5281/zenodo.10802849",
        "author": "Kellison, Ariel E. and Hsu, Justin",
        "title": "Artifact for Numerical Fuzz: A Type System for Rounding Error Analysis",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10802849",
        "abstract": "    <p>This is the artifact for NumFuzz (\u201cNumerical Fuzz\u201d), a prototype implementation of the type system and floating-point error analysis tool described in the paper \u201cNumerical Fuzz: A Type System for Rounding Error Analysis\u201d.</p>",
        "keywords": "Floating point, Linear type systems, Roundoff error"
    },
    "Inductive Approach to Spacer": {
        "type": "article",
        "key": "10.1145/3656457",
        "author": "Tsukada, Takeshi and Unno, Hiroshi",
        "title": "Inductive Approach to Spacer",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656457",
        "doi": "10.1145/3656457",
        "abstract": "The constrained Horn clause satisfiability problem is at the core of many automated verification methods, and Spacer is one of the most efficient solvers of this problem. The standard description of Spacer is based on an abstract transition system, dividing the whole procedure into small rules. This division makes individual rules easier to understand but, conversely, makes it difficult to discuss the procedure as a whole. As evidence of the difficulty in understanding the whole procedure, we point out that the claimed refutational completeness actually fails for several reasons, some of which were not present in the original version and subsequently added. It is also difficult to grasp the differences between Spacer and another procedure, such as GPDR.  This paper aims to provide a better understanding of Spacer by developing a Spacer-like procedure defined by structural induction. We first formulate the problem to be solved inductively, then give its na\\\"{\\i}ve solver and transform it to obtain a Spacer-like procedure. Interestingly, our inductive approach almost unifies Spacer and GPDR, which differ in only one respect in our understanding. To demonstrate the usefulness of our inductive approach in understanding Spacer, we examine Spacer variants in the literature in terms of inductive procedures and discuss why they are not refutationally complete and how to fix them. We also implemented the proposed procedure and evaluated it experimentally.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "227",
        "numpages": "24",
        "keywords": "constrained Horn clause, model-based projection, refutational completeness, tree interpolation"
    },
    "V-Star: Learning Visibly Pushdown Grammars from Program Inputs": {
        "type": "article",
        "key": "10.1145/3656458",
        "author": "Jia, Xiaodong and Tan, Gang",
        "title": "V-Star: Learning Visibly Pushdown Grammars from Program Inputs",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656458",
        "doi": "10.1145/3656458",
        "abstract": "Accurate description of program inputs remains a critical challenge in the field of programming languages. Active learning, as a well-established field, achieves exact learning for regular languages. We offer an innovative grammar inference tool, V-Star, based on the active learning of visibly pushdown automata. V-Star deduces nesting structures of program input languages from sample inputs, employing a novel inference mechanism based on nested patterns. This mechanism identifies token boundaries and converts languages such as XML documents into VPLs. We then adapted Angluin's L-Star, an exact learning algorithm, for VPA learning, which improves the precision of our tool. Our evaluation demonstrates that V-Star effectively and efficiently learns a variety of practical grammars, including S-Expressions, JSON, and XML, and outperforms other state-of-the-art tools.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "228",
        "numpages": "24",
        "keywords": "grammar inference, visibly pushdown grammars"
    },
    "V-Star Artifact": {
        "type": "software",
        "key": "10.5281/zenodo.10918754",
        "author": "Jia, Xiaodong and Tan, Gang",
        "title": "V-Star Artifact",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10918754",
        "abstract": "    <p>This artifact includes the V-Star library, detailed instructions, and a Docker image file necessary to reproduce the results presented in Table 1 of the paper V-Star: Learning Visibly Pushdown Grammars from Program Inputs.</p>",
        "keywords": "artifact, grammar inference, v-star, visibly pushdown grammars"
    },
    "Hashing Modulo Context-Sensitive \ud835\udefc-Equivalence": {
        "type": "article",
        "key": "10.1145/3656459",
        "author": "Blaauwbroek, Lasse and Ol\\v{s}\\'{a}k, Miroslav and Geuvers, Herman",
        "title": "Hashing Modulo Context-Sensitive \ud835\udefc-Equivalence",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656459",
        "doi": "10.1145/3656459",
        "abstract": "The notion of \u03b1-equivalence between \u03bb-terms is commonly used to   identify terms that are considered equal. However, due to the primitive   treatment of free variables, this notion falls short when comparing subterms   occurring within a larger context. Depending on the usage of the Barendregt   convention (choosing different variable names for all involved binders), it   will equate either too few or too many subterms. We introduce a formal notion   of context-sensitive \u03b1-equivalence, where two open terms can be   compared within a context that resolves their free variables. We show that   this equivalence coincides exactly with the notion of bisimulation   equivalence. Furthermore, we present an efficient O(nlogn) runtime   hashing scheme that identifies \u03bb-terms modulo context-sensitive   \u03b1-equivalence, generalizing over traditional bisimulation partitioning   algorithms and improving upon a previously established O(nlog2 n)   bound for a hashing modulo ordinary \u03b1-equivalence by Maziarz et   al. Hashing \u03bb-terms is useful in   many applications that require common subterm elimination and structure   sharing. We hav employed the algorithm to obtain a large-scale, densely packed,   interconnected graph of mathematical knowledge from the Coq proof assistant   for machine learning purposes.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "229",
        "numpages": "24",
        "keywords": "Alpha Equivalence, Bisimilarity, Hashing, Lambda Calculus, Syntax Tree"
    },
    "Artifact for: Hashing Modulo Context-Sensitive Alpha-Equivalence": {
        "type": "software",
        "key": "10.5281/zenodo.11097757",
        "author": "Blaauwbroek, Lasse and Ol\\v{s}\\'{a}k, Miroslav and Geuvers, Herman",
        "title": "Artifact for: Hashing Modulo Context-Sensitive Alpha-Equivalence",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.11097757",
        "abstract": "    <p>Reference implementation for hashing modulo context-sensitive alpha-equivalence</p>",
        "keywords": "Alpha Equivalence, Bisimilarity, Hashing, Lambda Calculus, Syntax Tree"
    },
    "Syntactic Code Search with Sequence-to-Tree Matching: Supporting Syntactic Search with Incomplete Code Fragments": {
        "type": "article",
        "key": "10.1145/3656460",
        "author": "Matute, Gabriel and Ni, Wode and Barik, Titus and Cheung, Alvin and Chasins, Sarah E.",
        "title": "Syntactic Code Search with Sequence-to-Tree Matching: Supporting Syntactic Search with Incomplete Code Fragments",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656460",
        "doi": "10.1145/3656460",
        "abstract": "Lightweight syntactic analysis tools like Semgrep and Comby leverage the tree structure of code, making them more expressive than string and regex search. Unlike traditional language frameworks (e.g., ESLint) that analyze codebases via explicit syntax tree manipulations, these tools use query languages that closely resemble the source language. However, state-of-the-art matching techniques for these tools require queries to be complete and parsable snippets, which makes in-progress query specifications useless.       We propose a new search architecture that relies only on tokenizing (not parsing) a query. We introduce a novel language and matching algorithm to support tree-aware wildcards on this architecture by building on tree automata. We also present stsearch, a syntactic search tool leveraging our approach.       In contrast to past work, our approach supports syntactic search even for previously unparsable queries. We show empirically that stsearch can support all tokenizable queries, while still providing results comparable to Semgrep for existing queries. Our work offers evidence that lightweight syntactic code search can accept in-progress specifications, potentially improving support for interactive settings.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "230",
        "numpages": "22",
        "keywords": "Code Search, Syntactic Analysis, Tree Wildcards"
    },
    "Syntactic Code Search with Sequence-to-Tree Matching": {
        "type": "software",
        "key": "10.5281/zenodo.10937816",
        "author": "Matute, Gabriel and Ni, Wode and Barik, Titus and Cheung, Alvin and Chasins, Sarah E.",
        "title": "Syntactic Code Search with Sequence-to-Tree Matching",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10937816",
        "abstract": "    <p>A docker image <code>stbench</code> to evaluate our tool <code>stsearch</code>, under <code>/artifact</code> we include: * <code>stsearch</code>, the Rust source code (together with a release build) of our tool; * <code>queries</code>, the Semgrep rules used to curate our real-world query benchmark; * <code>corpus</code>, the Javascript corpus (scrapped from <code>npm</code>) to search over; and * <code>stbench</code>, a Python package to collect and analyze matches.</p>",
        "keywords": "Code Search, Syntactic Analysis, Tree Wildcards"
    },
    "Static Analysis for Checking the Disambiguation Robustness of Regular Expressions": {
        "type": "article",
        "key": "10.1145/3656461",
        "author": "Mamouras, Konstantinos and Le Glaunec, Alexis and Li, Wu Angela and Chattopadhyay, Agnishom",
        "title": "Static Analysis for Checking the Disambiguation Robustness of Regular Expressions",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656461",
        "doi": "10.1145/3656461",
        "abstract": "Regular expressions are commonly used for finding and extracting matches from sequence data. Due to the inherent ambiguity of regular expressions, a disambiguation policy must be considered for the match extraction problem, in order to uniquely determine the desired match out of the possibly many matches. The most common disambiguation policies are the POSIX policy and the greedy (PCRE) policy. The POSIX policy chooses the longest match out of the leftmost ones. The greedy policy chooses a leftmost match and further disambiguates using a greedy interpretation of Kleene iteration to match as many times as possible. The choice of disambiguation policy can affect the output of match extraction, which can be an issue for reusing regular expressions across regex engines. In this paper, we introduce and study the notion of disambiguation robustness for regular expressions. A regular expression is robust if its extraction semantics is indifferent to whether the POSIX or greedy disambiguation policy is chosen. This gives rise to a decision problem for regular expressions, which we prove to be PSPACE-complete. We propose a static analysis algorithm for checking the (non-)robustness of regular expressions and two performance optimizations. We have implemented the proposed algorithms and we have shown experimentally that they are practical for analyzing large datasets of regular expressions derived from various application domains.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "231",
        "numpages": "25",
        "keywords": "automata, disambiguation strategy, parsing, regex, static analysis"
    },
    "Equivalence and Similarity Refutation for Probabilistic Programs": {
        "type": "software",
        "key": "10.5281/zenodo.10723168",
        "author": "Chatterjee, Krishnendu and Goharshady, Ehsan Kafshdar and Novotn\\'{y}, Petr and \\v{Z}ikeli\\'{c}, undefinedor\\dj{}e",
        "title": "Equivalence and Similarity Refutation for Probabilistic Programs",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10723168",
        "abstract": "    <p>This repository contains the artifact of the paper titled \u201cEquivalence and Similarity Refutation for Probabilistic Programs\u201d accepted at PLDI 2024.</p><p>The tool takes two probabilistic transition systems with specified initial configurations as input and based on user preferences either (i) tries to prove whether the two programs generate equivalent output distributions, or (ii) tries to find a lowerbound on Kantorovich distance between the output distributions of the input programs.</p>",
        "keywords": "Kantorovich distance, Martingales, Probabilistic programming, Probability distribution equivalence, Static program analysis"
    },
    "Probabilistic Programming with Programmable Variational Inference": {
        "type": "article",
        "key": "10.1145/3656463",
        "author": "Becker, McCoy R. and Lew, Alexander K. and Wang, Xiaoyan and Ghavami, Matin and Huot, Mathieu and Rinard, Martin C. and Mansinghka, Vikash K.",
        "title": "Probabilistic Programming with Programmable Variational Inference",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3656463",
        "doi": "10.1145/3656463",
        "abstract": "Compared to the wide array of advanced Monte Carlo methods supported by modern probabilistic programming languages (PPLs), PPL support for variational inference (VI) is less developed: users are typically limited to a predefined selection of variational objectives and gradient estimators, which are implemented monolithically (and without formal correctness arguments) in PPL backends. In this paper, we propose a more modular approach to supporting variational inference in PPLs, based on compositional program transformation. In our approach, variational objectives are expressed as programs, that may employ first-class constructs for computing densities of and expected values under user-defined models and variational families. We then transform these programs systematically into unbiased gradient estimators for optimizing the objectives they define. Our design makes it possible to prove unbiasedness by reasoning modularly about many interacting concerns in PPL implementations of variational inference, including automatic differentiation, density accumulation, tracing, and the application of unbiased gradient estimation strategies. Additionally, relative to existing support for VI in PPLs, our design increases expressiveness along three axes: (1) it supports an open-ended set of user-defined variational objectives, rather than a fixed menu of options; (2) it supports a combinatorial space of gradient estimation strategies, many not automated by today\u2019s PPLs; and (3) it supports a broader class of models and variational families, because it supports constructs for approximate marginalization and normalization (previously introduced for Monte Carlo inference). We implement our approach in an extension to the Gen probabilistic programming system (genjax.vi, implemented in JAX), and evaluate our automation on several deep generative modeling tasks, showing minimal performance overhead vs. hand-coded implementations and performance competitive with well-established open-source PPLs.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "233",
        "numpages": "25",
        "keywords": "automatic differentiation, correctness, probabilistic programming, semantics, variational inference"
    },
    "Reproduction Packager for Article \"Probabilistic Programming with Programmable Variational Inference": {
        "type": "software",
        "key": "10.5281/zenodo.10935596",
        "author": "Becker, McCoy R. and Lew, Alexander K. and Wang, Xiaoyan and Ghavami, Matin and Huot, Mathieu and Rinard, Martin C. and Mansinghka, Vikash K.",
        "title": "Reproduction Packager for Article \"Probabilistic Programming with Programmable Variational Inference",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10935596",
        "abstract": "    <p>A package which contains the JAX implementation that accompanies the paper \u201cProbabilistic Programming with Programmable Variational Inference\u201d, as well as the experiments used to generate figures and numbers in the empirical evaluation section.</p>",
        "keywords": "automatic differentiation, probabilistic programming, variational inference"
    },
    "PL4XGL: A Programming Language Approach to Explainable Graph Learning": {
        "type": "software",
        "key": "10.5281/zenodo.10783891",
        "author": "Jeon, Minseok and Park, Jihyeok and Oh, Hakjoo",
        "title": "PL4XGL: A Programming Language Approach to Explainable Graph Learning",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10783891",
        "abstract": "    <p>This artifact aims to reproduce the main results of the technique PL4XGL on eleven node and graph classification datasets in our paper \u201cPL4XGL: A Programming Language Approach to Explainable Graph Learning\u201d submitted to PLDI 2024. Specifically, the artifact will reproduce the results of PL4XGL in Figure 11 (Sparsity and Fidelity), Table 5 (accuracy) in our paper, and Figure 2 (Precision and Generality) of our supplementary material. This artifact requires only Python 3.x (we used Python 3.8.8).</p>",
        "keywords": "Domain-Specific Language, Graph Learning, Program Synthesis"
    },
    "A Family of Fast and Memory Efficient Lock- and Wait-Free Reclamation": {
        "type": "article",
        "key": "10.1145/3658851",
        "author": "Nikolaev, Ruslan and Ravindran, Binoy",
        "title": "A Family of Fast and Memory Efficient Lock- and Wait-Free Reclamation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3658851",
        "doi": "10.1145/3658851",
        "abstract": "Historically, memory management based on lock-free reference counting was very inefficient, especially for read-dominated workloads. Thus, approaches such as epoch-based reclamation (EBR), hazard pointers (HP), or a combination thereof have received significant attention. EBR exhibits excellent performance but is blocking due to potentially unbounded memory usage. In contrast, HP are non-blocking and achieve good memory efficiency but are much slower. Moreover, HP are only lock-free in the general case. Recently, several new memory reclamation approaches such as WFE and Hyaline have been proposed. WFE achieves wait-freedom, but is less memory efficient and performs suboptimally in oversubscribed scenarios; Hyaline achieves higher performance and memory efficiency, but lacks wait-freedom.  We present a family of non-blocking memory reclamation schemes, called Crystalline, that simultaneously addresses the challenges of high performance, high memory efficiency, and wait-freedom. Crystalline can guarantee complete wait-freedom even when threads are dynamically recycled, asynchronously reclaims memory in the sense that any thread can reclaim memory retired by any other thread, and ensures (an almost) balanced reclamation workload across all threads. The latter two properties result in Crystalline's high performance and memory efficiency. Simultaneously ensuring all three properties requires overcoming unique challenges. Crystalline supports ubiquitous x86-64 and ARM64 architectures, while achieving superior throughput than prior fast schemes such as EBR as the number of threads grows.  We also accentuate that many recent approaches, unlike HP, lack strict non-blocking guarantees when used with multiple data structures. By providing full wait-freedom, Crystalline addresses this problem as well.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "235",
        "numpages": "25",
        "keywords": "hazard pointers, memory reclamation, wait-free"
    },
    "A Family of Fast and Memory Efficient Lock- and Wait-Free Reclamation - Artifact for PLDI'24": {
        "type": "software",
        "key": "10.5281/zenodo.10775789",
        "author": "Nikolaev, Ruslan and Ravindran, Binoy",
        "title": "A Family of Fast and Memory Efficient Lock- and Wait-Free Reclamation - Artifact for PLDI'24",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10775789",
        "abstract": "    <p>The artifact contains a VM image (VirtualBox) with preinstalled Ubuntu 18.04 and the (precompiled) benchmark. The artifact also contains source code and instructions for manual (bare-metal) installations. The artifact also includes our data measurements and scripts for generating plots. Please see README.txt for more details.</p>",
        "keywords": "hazard pointers, memory reclamation, wait-free"
    }
}