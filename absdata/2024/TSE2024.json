{
    "Towards Efficient Fine-Tuning of Language Models With Organizational Data for Automated Software Review": {
        "type": "article",
        "key": "10.1109/TSE.2024.3428324",
        "author": "Nashaat, Mona and Miller, James",
        "title": "Towards Efficient Fine-Tuning of Language Models With Organizational Data for Automated Software Review",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3428324",
        "doi": "10.1109/TSE.2024.3428324",
        "abstract": "Large language models like BERT and GPT possess significant capabilities and potential impacts across various applications. Software engineers often use these models for code-related tasks, including generating, debugging, and summarizing code. Nevertheless, large language models still have several flaws, including model hallucination. (e.g., generating erroneous code and producing outdated and inaccurate programs) and the substantial computational resources and energy required for training and fine-tuning. To tackle these challenges, we propose CodeMentor, a framework for few-shot learning to train large language models with the data available within the organization. We employ the framework to train a language model for code review activities, such as code refinement and review generation. The framework utilizes heuristic rules and weak supervision techniques to leverage available data, such as previous review comments, issue reports, and related code updates. Then, the framework employs the constructed dataset to fine-tune LLMs for code review tasks. Additionally, the framework integrates domain expertise by employing reinforcement learning with human feedback. This allows domain experts to assess the generated code and enhance the model performance. Also, to assess the performance of the proposed model, we evaluate it with four state-of-the-art techniques in various code review tasks. The experimental results attest that CodeMentor enhances the performance in all tasks compared to the state-of-the-art approaches, with an improvement of up to 22.3%, 43.4%, and 24.3% in code quality estimation, review generation, and bug report summarization tasks, respectively.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2240\u20132253",
        "numpages": "14"
    },
    "LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3428972",
        "author": "Fakhoury, Sarah and Naik, Aaditya and Sakkas, Georgios and Chakraborty, Saikat and Lahiri, Shuvendu K.",
        "title": "LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3428972",
        "doi": "10.1109/TSE.2024.3428972",
        "abstract": "Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow &lt;sc&gt;TiCoder&lt;/sc&gt; for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97\\% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2254\u20132268",
        "numpages": "15"
    },
    "A Systematic Literature Review of Model-Driven Engineering Using Machine Learning": {
        "type": "article",
        "key": "10.1109/TSE.2024.3430514",
        "author": "Marc\\'{e}n, Ana C. and Iglesias, Antonio and Lape\\~{n}a, Ra\\'{u}l and P\\'{e}rez, Francisca and Cetina, Carlos",
        "title": "A Systematic Literature Review of Model-Driven Engineering Using Machine Learning",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3430514",
        "doi": "10.1109/TSE.2024.3430514",
        "abstract": "Model-driven engineering (MDE) is a software engineering paradigm based on the systematic use of models. Over the past few years, engineers have significantly increased the use of MDE, which has been reported as a successful paradigm for developing industrial software. Recently, there have also been remarkable advancements in the Artificial Intelligence (AI) domain, with a significant increase in advanced Machine Learning (ML) techniques. The advances in both fields have led to a surge in works that dwell within the intersection of ML and MDE. This work places the focus on systematically reviewing works that leverage ML to solve MDE problems. We have reviewed a total of 9,194 papers, selecting 98 studies for further analysis. The results of our Systematic Literature Review (SLR) bring light to the current state of the art and trends in the field, discussing the drift in the usage of the different available ML techniques along with the remaining research gaps and open challenges. Our SLR has the potential to produce a positive impact in the research community by steering it towards ML techniques that have been successfully applied to solve MDE challenges.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2269\u20132293",
        "numpages": "25"
    },
    "FairBalance: How to Achieve Equalized Odds With Data Pre-Processing": {
        "type": "article",
        "key": "10.1109/TSE.2024.3431445",
        "author": "Yu, Zhe and Chakraborty, Joymallya and Menzies, Tim",
        "title": "FairBalance: How to Achieve Equalized Odds With Data Pre-Processing",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3431445",
        "doi": "10.1109/TSE.2024.3431445",
        "abstract": "This research seeks to benefit the software engineering society by providing a simple yet effective pre-processing approach to achieve equalized odds fairness in machine learning software. Fairness issues have attracted increasing attention since machine learning software is increasingly used for high-stakes and high-risk decisions. It is the responsibility of all software developers to make their software accountable by ensuring that the machine learning software do not perform differently on different sensitive demographic groups\u2014satisfying equalized odds. Different from prior works which either optimize for an equalized odds related metric during the learning process like a black-box, or manipulate the training data following some intuition; this work studies the root cause of the violation of equalized odds and how to tackle it. We found that equalizing the class distribution in each demographic group with sample weights is a necessary condition for achieving equalized odds without modifying the normal training process. In addition, an important partial condition for equalized odds (zero average odds difference) can be guaranteed when the class distributions are weighted to be not only equal but also balanced (1:1). Based on these analyses, we proposed FairBalance, a pre-processing algorithm which balances the class distribution in each demographic group by assigning calculated weights to the training data. On eight real-world datasets, our empirical results show that, at low computational overhead, the proposed pre-processing algorithm FairBalance can significantly improve equalized odds without much, if any damage to the utility. FairBalance also outperforms existing state-of-the-art approaches in terms of equalized odds. To facilitate reuse, reproduction, and validation, we made our scripts available at &lt;uri&gt;https://github.com/hil-se/FairBalance&lt;/uri&gt;.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2294\u20132312",
        "numpages": "19"
    },
    "Enforcing Correctness of Collaborative Business Processes Using Plans": {
        "type": "article",
        "key": "10.1109/TSE.2024.3431585",
        "author": "Mo, Qi and Wang, Jianeng and Xie, Zhongwen and Liu, Cong and Dai, Fei",
        "title": "Enforcing Correctness of Collaborative Business Processes Using Plans",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3431585",
        "doi": "10.1109/TSE.2024.3431585",
        "abstract": "Generally, a collaborative business process is a distributed process, in which a set of parallel business processes are involved. These business processes have complementary competencies and knowledge, and cooperate with each other to achieve their common business goals. To ensure the correctness of collaborative business processes, we propose a novel plan-based correctness enforcement approach in this article, which is privacy-preserving, available and efficient. This approach first requires participating organizations to define their business processes. Then, each participating organization employs a set of reduction rules to build the public process of its business process, in which all internal private activities and the flows formed by them are removed. Next, a set of correct plans is generated from these public processes. A plan is essentially a process fragment without alternative routings. From the external perspective (i.e., ignoring all internal private activities and the flows formed by them), a parallel execution of the business processes corresponding to these public processes follows only one such plan. Lastly, each participating organization independently refactors its business process using these resulting correct plans. Using the message places (corresponding to the actual communication interfaces), these refactored processes are composed in parallel. Thus, a correct and loosely coupled enforced process is constructed. This approach is evaluated on actual collaborative business processes, and the experimental results show that compared with state-of-the-art enforcement proposals, it can achieve correctness enforcement while protecting the business privacy of organizations and is available. Meanwhile, it is also more efficient and scalable, even a collaborative business process with tens of millions of states can be enforced within a few seconds.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2313\u20132336",
        "numpages": "24"
    },
    "Assessing Evaluation Metrics for Neural Test Oracle Generation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3433463",
        "author": "Shin, Jiho and Hemmati, Hadi and Wei, Moshi and Wang, Song",
        "title": "Assessing Evaluation Metrics for Neural Test Oracle Generation",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3433463",
        "doi": "10.1109/TSE.2024.3433463",
        "abstract": "Recently, deep learning models have shown promising results in test oracle generation. Neural Oracle Generation (NOG) models are commonly evaluated using static (automatic) metrics which are mainly based on textual similarity of the output, e.g. BLEU, ROUGE-L, METEOR, and Accuracy. However, these textual similarity metrics may not reflect the testing effectiveness of the generated oracle within a test suite, which is often measured by dynamic (execution-based) test adequacy metrics such as code coverage and mutation score. In this work, we revisit existing oracle generation studies plus &lt;italic&gt;gpt-3.5&lt;/italic&gt; to empirically investigate the current standing of their performance in textual similarity and test adequacy metrics. Specifically, we train and run four state-of-the-art test oracle generation models on seven textual similarity and two test adequacy metrics for our analysis. We apply two different correlation analyses between these two different sets of metrics. Surprisingly, we found no significant correlation between the textual similarity metrics and test adequacy metrics. For instance, &lt;italic&gt;gpt-3.5&lt;/italic&gt; on the &lt;italic&gt;jackrabbit-oak&lt;/italic&gt; project had the highest performance on all seven textual similarity metrics among the studied NOGs. However, it had the lowest test adequacy metrics compared to all the studied NOGs. We further conducted a qualitative analysis to explore the reasons behind our observations. We found that oracles with high textual similarity metrics but low test adequacy metrics tend to have complex or multiple chained method invocations within the oracle's parameters, making them hard for the model to generate completely, affecting the test adequacy metrics. On the other hand, oracles with low textual similarity metrics but high test adequacy metrics tend to have to call different assertion types or a different method that functions similarly to the ones in the ground truth. Overall, this work complements prior studies on test oracle generation with an extensive performance evaluation on textual similarity and test adequacy metrics and provides guidelines for better assessment of deep learning applications in software test generation in the future.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2337\u20132349",
        "numpages": "13"
    },
    "Mitigating the Uncertainty and Imprecision of Log-Based Code Coverage Without Requiring Additional Logging Statements": {
        "type": "article",
        "key": "10.1109/TSE.2024.3435067",
        "author": "Xu, Xiaoyan and Cogo, Filipe R. and McIntosh, Shane",
        "title": "Mitigating the Uncertainty and Imprecision of Log-Based Code Coverage Without Requiring Additional Logging Statements",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3435067",
        "doi": "10.1109/TSE.2024.3435067",
        "abstract": "Understanding code coverage is an important precursor to software maintenance activities (e.g., better testing). Although modern code coverage tools provide key insights, they typically rely on code instrumentation, resulting in significant performance overhead. An alternative approach to code instrumentation is to process an application's source code and the associated log traces in tandem. This so-called \u201clog-based code coverage\u201d approach does not impose the same performance overhead as code instrumentation. Chen et al. proposed &lt;sc&gt;LogCoCo&lt;/sc&gt; \u2014 a tool that implements log-based code coverage for &lt;sc&gt;Java&lt;/sc&gt;. While &lt;sc&gt;LogCoCo&lt;/sc&gt; breaks important new ground, it has fundamental limitations, namely: uncertainty due to the lack of logging statements in conditional branches, and imprecision caused by dependency injection. In this study, we propose &lt;sc&gt;Log2Cov&lt;/sc&gt;, a tool that generates log-based code coverage for programs written in &lt;sc&gt;Python&lt;/sc&gt; and addresses uncertainty and imprecision issues. We evaluate &lt;sc&gt;Log2Cov&lt;/sc&gt; on three large and active open-source systems. More specifically, we compare the performance of &lt;sc&gt;Log2Cov&lt;/sc&gt; to that of &lt;sc&gt;Coverage.py&lt;/sc&gt;, an instrumentation-based coverage tool for &lt;sc&gt;Python&lt;/sc&gt;. Our results indicate that 1) &lt;sc&gt;Log2Cov&lt;/sc&gt; achieves high precision without introducing runtime overhead; and 2) uncertainty and imprecision can be reduced by up to 11\\% by statically analyzing the program's source code and execution logs, without requiring additional logging instrumentation from developers. While our enhancements make substantial improvements, we find that future work is needed to handle conditional statements and exception handling blocks to achieve parity with instrumentation-based approaches. We conclude the paper by drawing attention to these promising directions for future work.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2350\u20132362",
        "numpages": "13"
    },
    "Long Live the Image: On Enabling Resilient Production Database Containers for Microservice Applications": {
        "type": "article",
        "key": "10.1109/TSE.2024.3436623",
        "author": "Li, Zheng and Sald\\'{\\i}as-Vallejos, Nicol\\'{a}s and Seco, Diego and Rodr\\'{\\i}guez, Mar\\'{\\i}a Andrea and Ranjan, Rajiv",
        "title": "Long Live the Image: On Enabling Resilient Production Database Containers for Microservice Applications",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3436623",
        "doi": "10.1109/TSE.2024.3436623",
        "abstract": "Microservices architecture advocates decentralized data ownership for building software systems. Particularly, in the Database per Service pattern, each microservice is supposed to maintain its own database and to handle the data related to its functionality. When implementing microservices in practice, however, there seems to be a paradox: The de facto technology (i.e., containerization) for microservice implementation is claimed to be unsuitable for the microservice component (i.e., database) in production environments, mainly due to the data persistence issues (e.g., dangling volumes) and security concerns. As a result, the existing discussions generally suggest replacing database containers with cloud database services, while leaving the on-premises microservice implementation out of consideration. After identifying three statelessness-dominant application scenarios, we proposed container-native data persistence as a conditional solution to enable resilient database containers in production. In essence, this data persistence solution distinguishes stateless data access (i.e., reading) from stateful data processing (i.e., creating, updating, and deleting), and thus it aims at the development of stateless microservices for suitable applications. In addition to developing our proposal, this research is particularly focused on its validation, via prototyping the solution and evaluating its performance, and via applying this solution to two real-world microservice applications. From the industrial perspective, the validation results have proved the feasibility, usability, and efficiency of fully containerized microservices for production in applicable situations. From the academic perspective, this research has shed light on the operation-side micro-optimization of individual microservices, which fundamentally expands the scope of \u201csoftware micro-optimization\u201d and reveals new research opportunities.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2363\u20132378",
        "numpages": "16"
    },
    "Pearl: A Multi-Derivation Approach to Efficient CFL-Reachability Solving": {
        "type": "article",
        "key": "10.1109/TSE.2024.3437684",
        "author": "Shi, Chenghang and Li, Haofeng and Sui, Yulei and Lu, Jie and Li, Lian and Xue, Jingling",
        "title": "Pearl: A Multi-Derivation Approach to Efficient CFL-Reachability Solving",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3437684",
        "doi": "10.1109/TSE.2024.3437684",
        "abstract": "Context-free language (CFL) reachability is a fundamental framework for formulating program analyses. CFL-reachability analysis works on top of an edge-labeled graph by deriving reachability relations and adding them as labeled edges to the graph. Existing CFL-reachability algorithms typically adopt a single-reachability relation derivation (SRD) strategy, i.e., one reachability relation is derived at a time. Unfortunately, this strategy can lead to redundancy, hindering the efficiency of the analysis. To address this problem, this paper proposes &lt;sc&gt;Pearl&lt;/sc&gt;, a &lt;italic&gt;multi-derivation&lt;/italic&gt; approach that reduces derivation redundancy for CFL-reachability solving, which significantly improves the efficiency of CFL-reachability analysis. Our key insight is that multiple edges can be simultaneously derived via batch propagation of reachability relations. We also tailor our multi-derivation approach to tackle transitive relations that frequently arise when solving CFL-reachability. Specifically, we present a highly efficient transitive-aware variant, &lt;sc&gt;Pearl&lt;sup&gt;PG&lt;/sup&gt;&lt;/sc&gt;, which enhances &lt;sc&gt;Pearl&lt;/sc&gt; with &lt;italic&gt;propagation graphs&lt;/italic&gt;, a lightweight but effective graph representation, to further diminish redundant derivations. We evaluate the performance of our approach on two clients, i.e., context-sensitive value-flow analysis and field-sensitive alias analysis for C/C++. By eliminating a large amount of redundancy, our approach outperforms two baselines including the standard CFL-reachability algorithm and a state-of-the-art solver &lt;sc&gt;Pocr&lt;/sc&gt; specialized for fast transitivity solving. In particular, the empirical results demonstrate that, for value-flow analysis and alias analysis respectively, &lt;sc&gt;Pearl&lt;sup&gt;PG&lt;/sup&gt;&lt;/sc&gt; runs 3.09&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$times$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u00d7&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"li-ieq1-3437684.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; faster on average (up to 4.44&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$times$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u00d7&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"li-ieq2-3437684.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;) and 2.25&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$times$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u00d7&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"li-ieq3-3437684.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; faster on average (up to 3.31&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$times$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u00d7&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"li-ieq4-3437684.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;) than &lt;sc&gt;Pocr&lt;/sc&gt;, while also consuming less memory.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2379\u20132397",
        "numpages": "19"
    },
    "AddressWatcher: Sanitizer-Based Localization of Memory Leak Fixes": {
        "type": "article",
        "key": "10.1109/TSE.2024.3438119",
        "author": "Murali, Aniruddhan and Alfadel, Mahmoud and Nagappan, Meiyappan and Xu, Meng and Sun, Chengnian",
        "title": "AddressWatcher: Sanitizer-Based Localization of Memory Leak Fixes",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3438119",
        "doi": "10.1109/TSE.2024.3438119",
        "abstract": "Memory leak bugs are a major problem in C/C++ programs. They occur when memory objects are not deallocated. Developers need to manually deallocate these objects to prevent memory leaks. As such, several techniques have been proposed to automatically fix memory leaks. Although proposed approaches have merit in automatically fixing memory leaks, they present limitations. Static-based approaches attempt to trace the complete semantics of memory object across all paths. However, they have scalability-related challenges when the target program has a large number of paths (path explosion). On the other hand, dynamic approaches can spell out precise semantics of memory object only on a single execution path (it does not consider multiple execution paths). In this paper, we complement prior approaches by designing and implementing a novel framework named &lt;italic&gt;AddressWatcher&lt;/italic&gt;. AddressWatcher allows the semantics of a memory object to be tracked on multiple execution paths. Addresswatcher accomplishes this by using a leak database that allows one to store and compare different execution paths of a leak over several test cases. Also, AddressWatcher performs lightweight instrumentation during compile time that is utilized during the program execution to watch and track memory leak read/writes. We conduct an evaluation of AddressWatcher over five popular packages, namely binutils, openssh, tmux, openssl and git. In 23 out of 50 real-world memory leak bugs, AddressWatcher correctly points to a free location to fix memory leaks. Finally, we submit 25 Pull Requests across 12 popular OSS repositories using AddressWatcher suggestions. Among these, 21 were merged leading to 5 open issues being addressed. In fact, our critical fix prompted a new version release for the calc repository, a program used to find large primes. Furthermore, our contributions through these PRs sparked intense discussions and appreciation in various repositories such as coturn, h2o, and radare2.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2398\u20132411",
        "numpages": "14"
    },
    "DBInputs: Exploiting Persistent Data to Improve Automated GUI Testing": {
        "type": "article",
        "key": "10.1109/TSE.2024.3439002",
        "author": "Clerissi, Diego and Denaro, Giovanni and Mobilio, Marco and Mariani, Leonardo",
        "title": "DBInputs: Exploiting Persistent Data to Improve Automated GUI Testing",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3439002",
        "doi": "10.1109/TSE.2024.3439002",
        "abstract": "The generation of syntactically and semantically valid input data, able to exercise functionalities imposing constraints on the validity of the inputs, is a key challenge in automatic GUI (Graphical User Interface) testing. Existing test case generation techniques often rely on manually curated catalogs of values, although they might require significant effort to be created and maintained, and could hardly scale to applications with several input forms. Alternatively, it is possible to extract values from external data sources, such as the Web or publicly available knowledge bases. However, external sources are unlikely to provide the domain-specific and application-specific data that are often required to thoroughly exercise applications. This paper proposes &lt;sc&gt;DBInputs&lt;/sc&gt;, a novel approach that automatically identifies domain-specific and application-specific inputs to effectively fulfill the validity constraints present in the tested GUI screens. The approach exploits syntactic and semantic similarities between the identifiers of the input fields shown on GUI screens and those of the tables of the target GUI application database, and extracts valid inputs from such database, automatically resolving the mismatch between the user interface and the database schema. &lt;sc&gt;DBInputs&lt;/sc&gt; can properly cope with system testing and maintenance testing efforts, since databases are naturally and inexpensively available in those phases. Our experiments with 4 Web applications and 11 Mobile apps provide evidence that &lt;sc&gt;DBInputs&lt;/sc&gt; can outperform techniques like random input selection and &lt;sc&gt;Link&lt;/sc&gt;, a competing approach for searching inputs from knowledge bases, in both Web and Mobile domains.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2412\u20132436",
        "numpages": "25"
    },
    "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3440503",
        "author": "Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Zhuo, Terry Yue and Chen, Taolue",
        "title": "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3440503",
        "doi": "10.1109/TSE.2024.3440503",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models (&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq1-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq2-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach &lt;monospace&gt;COTTON&lt;/monospace&gt; which can leverage &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq3-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by &lt;monospace&gt;COTTON&lt;/monospace&gt; outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by &lt;monospace&gt;COTTON&lt;/monospace&gt; boost various &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq4-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that &lt;monospace&gt;COTTON&lt;/monospace&gt; not only improves the performance of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq5-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs, but also enhances the performance of LLMs. Our study showcases the potential of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq6-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs in software engineering applications.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2437\u20132457",
        "numpages": "21"
    },
    "Parameterized Verification of Leader/Follower Systems via Arithmetic Constraints": {
        "type": "article",
        "key": "10.1109/TSE.2024.3440587",
        "author": "Kourtis, Georgios and Dixon, Clare and Fisher, Michael",
        "title": "Parameterized Verification of Leader/Follower Systems via Arithmetic Constraints",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3440587",
        "doi": "10.1109/TSE.2024.3440587",
        "abstract": "We introduce a variant of a formalism appearing in recent work geared towards modelling systems in which a distinguished entity (leader) orchestrates the operation of an arbitrary number of identical entities (followers). Our variant is better suited for the verification of system properties involving complex arithmetic conditions. Whereas the original formalism is translated into a tractable fragment of first-order temporal logic, aiming to utilize automated (first-order temporal logic) theorem provers for verification, our variant is translated into linear integer arithmetic, aiming to utilize satisfiability modulo theories (SMT) solvers for verification. In particular, for any given system specified in our formalism, we prove, for any natural number &lt;italic&gt;n&lt;/italic&gt;, the existence of a linear integer arithmetic formula whose models are in one-to-one correspondence with certain counting abstractions (profiles) of executions of the system for &lt;italic&gt;n&lt;/italic&gt; time steps. Thus, one is able to verify, for any natural number &lt;italic&gt;n&lt;/italic&gt;, that all executions for &lt;italic&gt;n&lt;/italic&gt; time steps of any such system have a given property by establishing that said formula logically entails the property. To highlight the practical utility of our approach, we specify and verify three consensus protocols, actively used in distributed database systems and low-power wireless networks.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2458\u20132471",
        "numpages": "14"
    },
    "&lt;sc&gt;rCanary&lt;/sc&gt;: Detecting Memory Leaks Across Semi-Automated Memory Management Boundary in Rust": {
        "type": "article",
        "key": "10.1109/TSE.2024.3443624",
        "author": "Cui, Mohan and Xu, Hui and Tian, Hongliang and Zhou, Yangfan",
        "title": "&lt;sc&gt;rCanary&lt;/sc&gt;: Detecting Memory Leaks Across Semi-Automated Memory Management Boundary in Rust",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3443624",
        "doi": "10.1109/TSE.2024.3443624",
        "abstract": "Rust is an effective system programming language that guarantees memory safety via compile-time verifications. It employs a novel ownership-based resource management model to facilitate automated deallocation. This model is anticipated to eliminate memory leaks. However, we observed that user intervention drives it into semi-automated memory management and makes it error-prone to cause leaks. In contrast to violating memory-safety guarantees restricted by the &lt;italic&gt;unsafe&lt;/italic&gt; keyword, the boundary of leaking memory is implicit, and the compiler would not emit any warnings for developers. In this paper, we present &lt;sc&gt;rCanary&lt;/sc&gt;, a static, non-intrusive, and fully automated model checker to detect leaks across the semi-automated boundary. We design an encoder to abstract data with heap allocation and formalize a refined leak-free memory model based on boolean satisfiability. It can generate SMT-Lib2 format constraints for Rust MIR and is implemented as a Cargo component. We evaluate &lt;sc&gt;rCanary&lt;/sc&gt; by using flawed package benchmarks collected from the pull requests of open-source Rust projects. The results indicate that it is possible to recall all these defects with acceptable false positives. We further apply our tool to more than 1,200 real-world crates from crates.io and GitHub, identifying 19 crates having memory leaks. Our analyzer is also efficient, that costs 8.4 seconds per package.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2472\u20132484",
        "numpages": "13"
    },
    "Reducing the Length of Field-Replay Based Load Testing": {
        "type": "article",
        "key": "10.1109/TSE.2024.3408079",
        "author": "Xia, Yuanjie and Liao, Lizhi and Chen, Jinfu and Li, Heng and Shang, Weiyi",
        "title": "Reducing the Length of Field-Replay Based Load Testing",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3408079",
        "doi": "10.1109/TSE.2024.3408079",
        "abstract": "As software systems continuously grow in size and complexity, performance and load related issues have become more common than functional issues. Load testing is usually performed before software releases to ensure that the software system can still provide quality service under a certain load. Therefore, one of the common challenges of load testing is to design realistic workloads that can represent the actual workload in the field. In particular, one of the most widely adopted and intuitive approaches is to directly replay the field workloads in the load testing environment. However, replaying a lengthy, e.g., 48 hours, field workloads is rather resource- and time-consuming, and sometimes even infeasible for large-scale software systems that adopt a rapid release cycle. On the other hand, replaying a short duration of the field workloads may still result in unrealistic load testing. In this work, we propose an automated approach to reduce the length of load testing that is driven by replaying the field workloads. The intuition of our approach is: if the measured performance associated with a particular system behaviour is already stable, we can skip subsequent testing of this system behaviour to reduce the length of the field workloads. In particular, our approach first clusters execution logs that are generated during the system runtime to identify similar system behaviours during the field workloads. Then, we use statistical methods to determine whether the measured performance associated with a system behaviour has been stable. We evaluate our approach on three open-source projects (i.e., &lt;italic&gt;OpenMRS&lt;/italic&gt;, &lt;italic&gt;TeaStore&lt;/italic&gt;, and &lt;italic&gt;Apache James&lt;/italic&gt;). The results show that our approach can significantly reduce the length of field workloads while the workloads-after-reduction produced by our approach are representative of the original set of workloads. More importantly, the load testing results obtained by replaying the workloads after the reduction have high correlation and similar trend with the original set of workloads. Practitioners can leverage our approach to perform realistic field-replay based load testing while saving the needed resources and time. Our approach sheds light on future research that aims to reduce the cost of load testing for large-scale software systems.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1967\u20131983",
        "numpages": "17"
    },
    "ExplanaSC: A Framework for Determining Information Requirements for Explainable Blockchain Smart Contracts": {
        "type": "article",
        "key": "10.1109/TSE.2024.3408632",
        "author": "Al Ghanmi, Hanouf and Bahsoon, Rami",
        "title": "ExplanaSC: A Framework for Determining Information Requirements for Explainable Blockchain Smart Contracts",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3408632",
        "doi": "10.1109/TSE.2024.3408632",
        "abstract": "Blockchain smart contracts (SCs) have emerged as a transformative technology, enabling the automation and execution of contractual agreements without the need for intermediaries. However, as SCs evolve to become more complex in their decentralised decision-making abilities, there are notable difficulties in comprehending the underlying reasoning process and ensuring users\u2019 understanding. The existing literature primarily focuses on the technical aspects of SC, overlooking the exploration of the decision-making process within these systems and the involvement of humans. In this paper, we propose a framework that integrates human-centered design principles by applying Situation Awareness (SA) and goal directed task analysis (GDTA) concepts to determine information requirements necessary to design eXplainable smart contracts (XSC). The framework provides a structured approach for requirements engineers to identify information that can keep users well-informed throughout the decision-making process. The framework considers factors such as the business logic model, data model, and roles and responsibilities model to define specific information requirements that shape SC behaviour and necessitate explanations. To guide the determination of information requirements, the framework categorises SC decision mechanisms into autonomy, governance, processing, and behaviour. The ExplanaSC framework promotes the generation of XSC explanations through three levels aligned with SA: XSC explanation for perception, XSC explanation for comprehension, and XSC explanation for projection. Overall, this framework contributes to the development of XSC systems and lays the foundation for more transparent, and trustworthy decentralised applications. The XSC explanations aims to facilitate user awareness of complex decision-making processes. The evaluation of the framework uses a case to exemplify the working of our framework, its added value and limitations, and consults experts in the field for feedback and refinements.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "1984\u20132004",
        "numpages": "21"
    },
    "Optimization of Automated and Manual Software Tests in Industrial Practice: A Survey and Historical Analysis": {
        "type": "article",
        "key": "10.1109/TSE.2024.3418191",
        "author": "Haas, Roman and N\\\"{o}mmer, Raphael and Juergens, Elmar and Apel, Sven",
        "title": "Optimization of Automated and Manual Software Tests in Industrial Practice: A Survey and Historical Analysis",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3418191",
        "doi": "10.1109/TSE.2024.3418191",
        "abstract": "&lt;bold&gt;&lt;italic&gt;Context&lt;/italic&gt;&lt;/bold&gt;: Both automated and manual software testing are widely applied in practice. While being essential for project success and software quality, they are very resource-intensive, thus motivating the pursuit for optimization. &lt;bold&gt;&lt;italic&gt;Goal&lt;/italic&gt;&lt;/bold&gt;: We aim at understanding to what extent test optimization techniques for &lt;italic&gt;automated&lt;/italic&gt; testing from the field of test case selection, prioritization, and test suite minimization can be applied to &lt;italic&gt;manual&lt;/italic&gt; testing processes in practice. &lt;bold&gt;&lt;italic&gt;Method&lt;/italic&gt;&lt;/bold&gt;: We have studied the automated and manual testing process of five industrial study subjects from five different domains with different technological backgrounds and assessed the costs and benefits of test optimization techniques in industrial practice. In particular, we have carried out a cost\u2013benefit analysis of two language-agnostic optimization techniques (test impact analysis and Pareto testing a technique we introduce in this paper) on 2,622 real-world failures from our subject's histories. &lt;bold&gt;&lt;italic&gt;Results&lt;/italic&gt;&lt;/bold&gt;: Both techniques maintain most of the fault detection capability while significantly reducing the test runtime. For automated testing, optimized test suites detect, on average, 80\\% of failures, while saving 66\\% of execution time, as compared to 81\\% failure detection rate for manual test suites and an average time saving of 43\\%. We observe an average speedup of the time to first failure of around 49 compared to a random test ordering. &lt;bold&gt;&lt;italic&gt;Conclusion&lt;/italic&gt;&lt;/bold&gt;: Our results suggest that optimization techniques from automated testing can be transferred to manual testing in industrial practice, resulting in lower test execution time and much lower time-to-feedback, but coming with process-related limitations and requirements for a successful implementation. All study subjects implemented one of our test optimization techniques in their processes, which demonstrates the practical impact of our findings.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "2005\u20132020",
        "numpages": "16"
    },
    "A Scalable &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$t$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq1-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-Wise Coverage Estimator: Algorithms and Applications": {
        "type": "article",
        "key": "10.1109/TSE.2024.3419919",
        "author": "Baranov, Eduard and Chakraborty, Sourav and Legay, Axel and Meel, Kuldeep S. and Vinodchandran, N. Variyam",
        "title": "A Scalable &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$t$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq1-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-Wise Coverage Estimator: Algorithms and Applications",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3419919",
        "doi": "10.1109/TSE.2024.3419919",
        "abstract": "Owing to the pervasiveness of software in our modern lives, software systems have evolved to be highly configurable. Combinatorial testing has emerged as a dominant paradigm for testing highly configurable systems. Often constraints are employed to define the environments where a given system is expected to work. Therefore, there has been a sustained interest in designing constraint-based test suite generation techniques. A significant goal of test suite generation techniques is to achieve &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$t$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq2-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-wise coverage for higher values of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$t$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq3-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;. Therefore, designing scalable techniques that can estimate &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$t$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq4-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-wise coverage for a given set of tests and/or the estimation of maximum achievable &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$t$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq5-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-wise coverage under a given set of constraints is of crucial importance. The existing estimation techniques face significant scalability hurdles. We designed scalable algorithms with mathematical guarantees to estimate (i) &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$t$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq6-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-wise coverage for a given set of tests, and (ii) maximum &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$t$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq7-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-wise coverage for a given set of constraints. In particular, &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathsf{ApproxCov}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"sans-serif\"&gt;ApproxCov&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq8-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; takes in a test set &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathcal{U}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"script\"&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq9-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; and returns an estimate of the &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$t$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq10-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-wise coverage of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathcal{U}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"script\"&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq11-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; that is guaranteed to be within &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$(1pmvarepsilon)$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo stretchy=\"false\"&gt;(&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;\u00b1&lt;/mml:mo&gt;&lt;mml:mi&gt;\u03b5&lt;/mml:mi&gt;&lt;mml:mo stretchy=\"false\"&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq12-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-factor of the ground truth with probability at least &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$1-delta$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;\u2212&lt;/mml:mo&gt;&lt;mml:mi&gt;\u03b4&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq13-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; for a given tolerance parameter &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$varepsilon$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;\u03b5&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq14-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; and a confidence parameter &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$delta$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;\u03b4&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq15-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;. A scalable framework &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${mathsf{ApproxMaxCov}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"sans-serif\"&gt;ApproxMaxCov&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq16-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; for a given formula &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${mathsf{F}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"sans-serif\"&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq17-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; outputs an approximation which is guaranteed to be within &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$(1pmvarepsilon)$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo stretchy=\"false\"&gt;(&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;\u00b1&lt;/mml:mo&gt;&lt;mml:mi&gt;\u03b5&lt;/mml:mi&gt;&lt;mml:mo stretchy=\"false\"&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq18-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; factor of the maximum achievable &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$t$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq19-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-wise coverage under &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${mathsf{F}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"sans-serif\"&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq20-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, with probability &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$geq 1-delta$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u2265&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;\u2212&lt;/mml:mo&gt;&lt;mml:mi&gt;\u03b4&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq21-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; for a given tolerance parameter &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$varepsilon$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;\u03b5&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq22-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; and a confidence parameter &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$delta$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;\u03b4&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq23-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;. Our comprehensive evaluation demonstrates that &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathsf{ApproxCov}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"sans-serif\"&gt;ApproxCov&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq24-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; and &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${mathsf{ApproxMaxCov}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"sans-serif\"&gt;ApproxMaxCov&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq25-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; can handle benchmarks that are beyond the reach of current state-of-the-art approaches. In this paper we present proofs of correctness of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathsf{ApproxCov}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"sans-serif\"&gt;ApproxCov&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq26-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${mathsf{ApproxMaxCov}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"sans-serif\"&gt;ApproxMaxCov&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq27-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, and of their generalizations. We show how the algorithms can improve the scalability of a test suite generator while maintaining its effectiveness. In addition, we compare several test suite generators on different feature combination sizes &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$t$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"baranov-ieq28-3419919.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "2021\u20132039",
        "numpages": "19"
    },
    "Boundary State Generation for Testing and Improvement of Autonomous Driving Systems": {
        "type": "article",
        "key": "10.1109/TSE.2024.3420816",
        "author": "Biagiola, Matteo and Tonella, Paolo",
        "title": "Boundary State Generation for Testing and Improvement of Autonomous Driving Systems",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3420816",
        "doi": "10.1109/TSE.2024.3420816",
        "abstract": "Recent advances in Deep Neural Networks (DNNs) and sensor technologies are enabling autonomous driving systems (ADSs) with an ever-increasing level of autonomy. However, assessing their dependability remains a critical concern. State-of-the-art ADS testing approaches modify the controllable attributes of a simulated driving environment until the ADS misbehaves. In such approaches, environment instances in which the ADS is successful are discarded, despite the possibility that they could contain hidden driving conditions in which the ADS may misbehave. In this paper, we present &lt;sc&gt;GenBo&lt;/sc&gt; (GENerator of BOundary state pairs), a novel test generator for ADS testing. &lt;sc&gt;GenBo&lt;/sc&gt; mutates the driving conditions of the ego vehicle (position, velocity and orientation), collected in a failure-free environment instance, and efficiently generates challenging driving conditions at the behavior boundary (i.e., where the model starts to misbehave) in the same environment instance. We use such boundary conditions to augment the initial training dataset and retrain the DNN model under test. Our evaluation results show that the retrained model has, on average, up to 3&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$times$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u00d7&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"biagiola-ieq1-3420816.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; higher success rate on a separate set of evaluation tracks with respect to the original DNN model.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2040\u20132053",
        "numpages": "14"
    },
    "Multi-Objective Software Defect Prediction via Multi-Source Uncertain Information Fusion and Multi-Task Multi-View Learning": {
        "type": "article",
        "key": "10.1109/TSE.2024.3421591",
        "author": "Yang, Minghao and Yang, Shunkun and Wong, W. Eric",
        "title": "Multi-Objective Software Defect Prediction via Multi-Source Uncertain Information Fusion and Multi-Task Multi-View Learning",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3421591",
        "doi": "10.1109/TSE.2024.3421591",
        "abstract": "Effective software defect prediction (SDP) is important for software quality assurance. Numerous advanced SDP methods have been proposed recently. However, how to consider the task correlations and achieve multi-objective SDP accurately and efficiently still remains to be further explored. In this paper, we propose a novel multi-objective SDP method via multi-source uncertain information fusion and multi-task multi-view learning (MTMV) to accurately and efficiently predict the proneness, location, and type of defects. Firstly, multi-view features are extracted from multi-source static analysis results, reflecting uncertain defect location distribution and semantic information. Then, a novel MTMV model is proposed to fully fuse the uncertain defect information in multi-view features and realize effective multi-objective SDP. Specifically, the convolutional GRU encoders capture the consistency and complementarity of multi-source defect information to automatically filter the noise of false and missed alarms, and reduce location and type uncertainty of static analysis results. A global attention mechanism combined with the hard parameter sharing in MTMV fuse features according to their global importance of all tasks for balanced learning. Then, considering the latent task and feature correlations, multiple task-specific decoders jointly optimize all SDP tasks by sharing the learning experience. Through the extensive experiments on 14 datasets, the proposed method significantly improves the prediction performance over 12 baseline methods for all SDP objectives. The average improvements are 30.7%, 31.2%, and 32.4% for defect proneness, location, and type prediction, respectively. Therefore, the proposed multi-objective SDP method can provide more sufficient and precise insights for developers to significantly improve the efficiency of software analysis and testing.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2054\u20132076",
        "numpages": "23"
    },
    "Esale: &lt;underline&gt;E&lt;/underline&gt;nhancing Code-&lt;underline&gt;S&lt;/underline&gt;ummary &lt;underline&gt;A&lt;/underline&gt;lignment &lt;underline&gt;Le&lt;/underline&gt;arning for Source Code Summarization": {
        "type": "article",
        "key": "10.1109/TSE.2024.3422274",
        "author": "Fang, Chunrong and Sun, Weisong and Chen, Yuchen and Chen, Xiao and Wei, Zhao and Zhang, Quanjun and You, Yudu and Luo, Bin and Liu, Yang and Chen, Zhenyu",
        "title": "Esale: &lt;underline&gt;E&lt;/underline&gt;nhancing Code-&lt;underline&gt;S&lt;/underline&gt;ummary &lt;underline&gt;A&lt;/underline&gt;lignment &lt;underline&gt;Le&lt;/underline&gt;arning for Source Code Summarization",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3422274",
        "doi": "10.1109/TSE.2024.3422274",
        "abstract": "(Source) code summarization aims to automatically generate succinct natural language summaries for given code snippets. Such summaries play a significant role in promoting developers to understand and maintain code. Inspired by neural machine translation, deep learning-based code summarization techniques widely adopt an encoder-decoder framework, where the encoder transforms given code snippets into context vectors, and the decoder decodes context vectors into summaries. Recently, large-scale pre-trained models for source code (e.g., CodeBERT and UniXcoder) are equipped with encoders capable of producing general context vectors and have achieved substantial improvements on the code summarization task. However, although they are usually trained mainly on code-focused tasks and can capture general code features, they still fall short in capturing specific features that need to be summarized. In a nutshell, they fail to learn the alignment between code snippets and summaries (code-summary alignment for short). In this paper, we propose a novel approach to improve code summarization based on summary-focused tasks. Specifically, we exploit a multi-task learning paradigm to train the encoder on three summary-focused tasks to enhance its ability to learn code-summary alignment, including unidirectional language modeling (ULM), masked language modeling (MLM), and action word prediction (AWP). Unlike pre-trained models that mainly predict masked tokens in code snippets, we design ULM and MLM to predict masked words in summaries. Intuitively, predicting words based on given code snippets would help learn the code-summary alignment. In addition, existing work shows that AWP affects the prediction of the entire summary. Therefore, we further introduce the domain-specific task AWP to enhance the ability of the encoder to learn the alignment between action words and code snippets. We evaluate the effectiveness of our approach, called &lt;sc&gt;Esale&lt;/sc&gt;, by conducting extensive experiments on four datasets, including two widely used datasets JCSD and PCSD, a cross-project Java dataset CPJD, and a multilingual language dataset CodeSearchNet. Experimental results show that &lt;sc&gt;Esale&lt;/sc&gt; significantly outperforms state-of-the-art baselines in all three widely used metrics, including BLEU, METEOR, and ROUGE-L. Moreover, the human evaluation proves that the summaries generated by &lt;sc&gt;Esale&lt;/sc&gt; are more informative and closer to the ground-truth summaries.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2077\u20132095",
        "numpages": "19"
    },
    "Characterizing the Prevalence, Distribution, and Duration of Stale Reviewer Recommendations": {
        "type": "article",
        "key": "10.1109/TSE.2024.3422369",
        "author": "Kazemi, Farshad and Lamothe, Maxime and McIntosh, Shane",
        "title": "Characterizing the Prevalence, Distribution, and Duration of Stale Reviewer Recommendations",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3422369",
        "doi": "10.1109/TSE.2024.3422369",
        "abstract": "The appropriate assignment of reviewers is a key factor in determining the value that organizations can derive from code review. While inappropriate reviewer recommendations can hinder the benefits of the code review process, identifying these assignments is challenging. Stale reviewers, i.e., those who no longer contribute to the project, are one type of reviewer recommendation that is certainly inappropriate. Understanding and minimizing this type of recommendation can thus enhance the benefits of the code review process. While recent work demonstrates the existence of stale reviewers, to the best of our knowledge, attempts have yet to be made to characterize and mitigate them. In this paper, we study the prevalence and potential effects. We then propose and assess a strategy to mitigate stale recommendations in existing code reviewer recommendation tools. By applying five code reviewer recommendation approaches (LearnRec, RetentionRec, cHRev, Sofia, and WLRRec) to three thriving open-source systems with 5,806 contributors, we observe that, on average, 12.59\\% of incorrect recommendations are stale due to developer turnover; however, fewer stale recommendations are made when the recency of contributions is considered by the recommendation objective function. We also investigate which reviewers appear in stale recommendations and observe that the top reviewers account for a considerable proportion of stale recommendations. For instance, in 15.31\\% of cases, the top-3 reviewers account for at least half of the stale recommendations. Finally, we study how long stale reviewers linger after the candidate leaves the project, observing that contributors who left the project 7.7 years ago are still suggested to review change sets. Based on our findings, we propose separating the reviewer contribution recency from the other factors that are used by the CRR objective function to filter out developers who have not contributed during a specified duration. By evaluating this strategy with different intervals, we assess the potential impact of this choice on the recommended reviewers. The proposed filter reduces the staleness of recommendations, i.e., the Staleness Reduction Ratio (SRR) improves between 21.44\\%\u201392.39\\%. Yet since the strategy may increase active reviewer workload, careful project-specific exploration of the impact of the cut-off setting is crucial.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2096\u20132109",
        "numpages": "14"
    },
    "Local and Global Explainability for Technical Debt Identification": {
        "type": "article",
        "key": "10.1109/TSE.2024.3422427",
        "author": "Tsoukalas, Dimitrios and Mittas, Nikolaos and Arvanitou, Elvira-Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Kehagias, Dionysios",
        "title": "Local and Global Explainability for Technical Debt Identification",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3422427",
        "doi": "10.1109/TSE.2024.3422427",
        "abstract": "In recent years, we have witnessed an important increase in research focusing on how machine learning (ML) techniques can be used for software quality assessment and improvement. However, the derived methodologies and tools lack transparency, due to the black-box nature of the employed machine learning models, leading to decreased trust in their results. To address this shortcoming, in this paper we extend the state-of-the-art and -practice by building explainable AI models on top of machine learning ones, to interpret the factors (i.e. software metrics) that constitute a module as in risk of having high technical debt (HIGH TD), to obtain thresholds for metric scores that are alerting for poor maintainability, and finally, we dig further to achieve local interpretation that explains the specific problems of each module, pinpointing to specific opportunities for improvement during TD management. To achieve this goal, we have developed project-specific classifiers (characterizing modules as HIGH and NOT-HIGH TD) for 21 open-source projects, and we explain their rationale using the SHapley Additive exPlanation (SHAP) analysis. Based on our analysis, complexity, comments ratio, cohesion, nesting of control flow statements, coupling, refactoring activity, and code churn are the most important reasons for characterizing classes as in HIGH TD risk. The analysis is complemented with global and local means of interpretation, such as metric thresholds and case-by-case reasoning for characterizing a class as in-risk of having HIGH TD. The results of the study are compared against the state-of-the-art and are interpreted from the point of view of both researchers and practitioners.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2110\u20132123",
        "numpages": "14"
    },
    "To Do or Not to Do: Semantics and Patterns for Do Activities in UML PSSM State Machines": {
        "type": "article",
        "key": "10.1109/TSE.2024.3422845",
        "author": "Elekes, M\\'{a}rton and Moln\\'{a}r, Vince and Micskei, Zolt\\'{a}n",
        "title": "To Do or Not to Do: Semantics and Patterns for Do Activities in UML PSSM State Machines",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3422845",
        "doi": "10.1109/TSE.2024.3422845",
        "abstract": "State machines are used in engineering many types of software-intensive systems. UML State Machines extend simple finite state machines with powerful constructs. Among the many extensions, there is one seemingly simple and innocent language construct that fundamentally changes state machines\u2019 reactive model of computation: doActivity behaviors. DoActivity behaviors describe behavior that is executed independently from the state machine once entered in a given state, typically modeling complex computation or communication as background tasks. However, the UML specification or textbooks are vague about how the doActivity behavior construct should be appropriately used. This lack of guidance is a severe issue as, when improperly used, doActivities can cause concurrent, non-deterministic bugs that are especially challenging to find and could ruin a seemingly correct software design. The Precise Semantics of UML State Machines (PSSM) specification introduced detailed operational semantics for state machines. To the best of our knowledge, there is no rigorous review yet of doActivity's semantics as specified in PSSM. We analyzed the semantics by collecting evidence from cross-checking the text of the specification, its semantic model and executable test cases, and the simulators supporting PSSM. We synthesized insights about subtle details and emergent behaviors relevant to tool developers and advanced modelers. We reported inconsistencies and missing clarifications in more than 20 issues to the standardization committee. Based on these insights, we studied 11 patterns for doActivities detailing the consequences of using a doActivity in a given situation and discussing countermeasures or alternative design choices. We hope that our analysis of the semantics and the patterns help vendors develop conformant simulators or verification tools and engineers design better state machine models.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2124\u20132141",
        "numpages": "18"
    },
    "API2Vec++: Boosting API Sequence Representation for Malware Detection and Classification": {
        "type": "article",
        "key": "10.1109/TSE.2024.3422990",
        "author": "Cui, Lei and Yin, Junnan and Cui, Jiancong and Ji, Yuede and Liu, Peng and Hao, Zhiyu and Yun, Xiaochun",
        "title": "API2Vec++: Boosting API Sequence Representation for Malware Detection and Classification",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3422990",
        "doi": "10.1109/TSE.2024.3422990",
        "abstract": "Analyzing malware based on API call sequences is an effective approach, as these sequences reflect the dynamic execution behavior of malware. Recent advancements in deep learning have facilitated the application of these techniques to mine valuable information from API call sequences. However, these methods typically operate on raw sequences and may not effectively capture crucial information, especially in the case of multi-process malware, due to the &lt;italic&gt;API call interleaving problem&lt;/italic&gt;. Furthermore, they often fail to capture contextual behaviors within or across processes, which is particularly important for identifying and classifying malicious activities. Motivated by this, we present API2Vec++, a graph-based API embedding method for malware detection and classification. First, we construct a graph model to represent the raw sequence. Specifically, we design the Temporal Process Graph (TPG) to model inter-process behaviors and the Temporal API Property Graph (TAPG) to model intra-process behaviors. Compared to our previous graph model, the TAPG model exposes operations with associated behaviors within the process through node properties and thus enhances detection and classification abilities. Using these graphs, we develop a heuristic random walk algorithm to generate numerous paths that can capture fine-grained malicious familial behavior. By pre-training these paths using the BERT model, we generate embeddings of paths and APIs, which can then be used for malware detection and classification. Experiments on a real-world malware dataset demonstrate that API2Vec++ outperforms state-of-the-art embedding methods and detection/classification methods in both accuracy and robustness, particularly for multi-process malware.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2142\u20132162",
        "numpages": "21"
    },
    "Revisiting the Performance of Deep Learning-Based Vulnerability Detection on Realistic Datasets": {
        "type": "article",
        "key": "10.1109/TSE.2024.3423712",
        "author": "Chakraborty, Partha and Arumugam, Krishna Kanth and Alfadel, Mahmoud and Nagappan, Meiyappan and McIntosh, Shane",
        "title": "Revisiting the Performance of Deep Learning-Based Vulnerability Detection on Realistic Datasets",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3423712",
        "doi": "10.1109/TSE.2024.3423712",
        "abstract": "The impact of software vulnerabilities on everyday software systems is concerning. Although deep learning-based models have been proposed for vulnerability detection, their reliability remains a significant concern. While prior evaluation of such models reports impressive recall/F1 scores of up to 99\\%, we find that these models underperform in practical scenarios, particularly when evaluated on the entire codebases rather than only the fixing commit. In this paper, we introduce a comprehensive dataset (&lt;italic&gt;Real-Vul&lt;/italic&gt;) designed to accurately represent real-world scenarios for evaluating vulnerability detection models. We evaluate DeepWukong, LineVul, ReVeal, and IVDetect vulnerability detection approaches and observe a surprisingly significant drop in performance, with precision declining by up to 95 percentage points and F1 scores dropping by up to 91 percentage points. A closer inspection reveals a substantial overlap in the embeddings generated by the models for vulnerable and uncertain samples (non-vulnerable or vulnerability not reported yet), which likely explains why we observe such a large increase in the quantity and rate of false positives. Additionally, we observe fluctuations in model performance based on vulnerability characteristics (e.g., vulnerability types and severity). For example, the studied models achieve 26 percentage points better F1 scores when vulnerabilities are related to information leaks or code injection rather than when vulnerabilities are related to path resolution or predictable return values. Our results highlight the substantial performance gap that still needs to be bridged before deep learning-based vulnerability detection is ready for deployment in practical settings. We dive deeper into why models underperform in realistic settings and our investigation revealed overfitting as a key issue. We address this by introducing an augmentation technique, potentially improving performance by up to 30\\%. We contribute (a) an approach to creating a dataset that future research can use to improve the practicality of model evaluation; (b) &lt;italic&gt;Real-Vul&lt;/italic&gt;\u2013 a comprehensive dataset that adheres to this approach; and (c) empirical evidence that the deep learning-based models struggle to perform in a real-world setting.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2163\u20132177",
        "numpages": "15"
    },
    "Vulnerability Detection via Multiple-Graph-Based Code Representation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3427815",
        "author": "Qiu, Fangcheng and Liu, Zhongxin and Hu, Xing and Xia, Xin and Chen, Gang and Wang, Xinyu",
        "title": "Vulnerability Detection via Multiple-Graph-Based Code Representation",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3427815",
        "doi": "10.1109/TSE.2024.3427815",
        "abstract": "During software development and maintenance, vulnerability detection is an essential part of software quality assurance. Even though many program-analysis-based and machine-learning-based approaches have been proposed to automatically detect vulnerabilities, they rely on explicit rules or patterns defined by security experts and suffer from either high false positives or high false negatives. Recently, an increasing number of studies leverage deep learning techniques, especially Graph Neural Network (GNN), to detect vulnerabilities. These approaches leverage program analysis to represent the program semantics as graphs and perform graph analysis to detect vulnerabilities. However, they suffer from two main problems: (i) Existing GNN-based techniques do not effectively learn the structural and semantic features from source code for vulnerability detection. (ii) These approaches tend to ignore fine-grained information in source code. To tackle these problems, in this paper, we propose a novel vulnerability detection approach, named &lt;sc&gt;MGVD&lt;/sc&gt; (&lt;bold&gt;M&lt;/bold&gt; &lt;sc&gt;ultiple&lt;/sc&gt;-&lt;bold&gt;G&lt;/bold&gt; &lt;sc&gt;raph-Based&lt;/sc&gt; &lt;bold&gt;V&lt;/bold&gt; &lt;sc&gt;ulnerability&lt;/sc&gt; &lt;bold&gt;D&lt;/bold&gt; &lt;sc&gt;etection)&lt;/sc&gt;, to detect vulnerable functions. To effectively learn the structural and semantic features from source code, &lt;sc&gt;MGVD&lt;/sc&gt; uses three different ways to represent each function into multiple forms, i.e., two statement graphs and a sequence of tokens. Then we encode such representations to a three-channel feature matrix. The feature matrix contains the structural feature and the semantic feature of the function. And we add a weight allocation layer to distribute the weights between structural and semantic features. To overcome the second problem, &lt;sc&gt;MGVD&lt;/sc&gt; constructs each graph representation of the input function using multiple different graphs instead of a single graph. Each graph focuses on one statement in the function and its nodes denote the related statements and their fine-grained code elements. Finally, &lt;sc&gt;MGVD&lt;/sc&gt; leverages CNN to identify whether this function is vulnerable based on such feature matrix. We conduct experiments on 3 vulnerability datasets with a total of 30,341 vulnerable functions and 127,931 non-vulnerable functions. The experimental results show that our method outperforms the state-of-the-art by 9.68\\% \u2013 10.28\\% in terms of F1-score.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2178\u20132199",
        "numpages": "22"
    },
    "Mole: Efficient Crash Reproduction in Android Applications With Enforcing Necessary UI Events": {
        "type": "article",
        "key": "10.1109/TSE.2024.3428543",
        "author": "Masoudian, Maryam and Huang, Heqing and Amini, Morteza and Zhang, Charles",
        "title": "Mole: Efficient Crash Reproduction in Android Applications With Enforcing Necessary UI Events",
        "year": "2024",
        "issue_date": "Aug. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3428543",
        "doi": "10.1109/TSE.2024.3428543",
        "abstract": "To improve the quality of Android apps, developers use automated debugging and testing solutions to determine whether the previously found crashes are reproducible. However, existing GUI fuzzing solutions for Android apps struggle to reproduce crashes efficiently based solely on a crash stack trace. This trace provides the location in the app where the crash occurs. GUI fuzzing solutions currently in use rely on heuristics to generate UI events. Unfortunately, these events often do not align with the investigation of an app's UI event space to reach a specific location of code. Hence, they generate numerous events unrelated to the crash, leading to an event explosion. To address this issue, a precise static UI model of widgets and screens can greatly enhance the efficiency of a fuzzing tool in its search. Building such a model requires considering all possible combinations of event sequences on widgets since the execution order of events is not statically determined. However, this approach presents scalability challenges in complex apps with several widgets. In this paper, we propose a directed-based fuzzing solution to reduce an app's event domain to the necessary ones to trigger a crash. Our insight is that the dependencies between widgets in their visual presentation and attribute states provide valuable information in precisely identifying events that trigger a crash. We propose an attribute-sensitive reachability analysis (ASRA) to track dependent widgets in reachable paths to the crash point and distinguish between events in terms of their relevancy to be generated in the crash reproduction process. With instrumentation, we inject code to prune irrelevant events, reducing the event domain to search at run time. We used four famous fuzzing tools, Monkey, Ape, Stoat, and FastBot2, to assess the impact of our solution in decreasing the crash reproduction time and increasing the possibility of reproducing a crash. Our results show that the success ratio of reproducing a crash has increased for &lt;italic&gt;one-fourth&lt;/italic&gt; of crashes. In addition, the average reproduction time of a crash becomes at least 2x faster. Wilcoxon Mann-Whitney test shows this enhancement is significant when our tool is used compared to baseline and insensitive reachability analysis.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2200\u20132218",
        "numpages": "19"
    },
    "Distilling Quality Enhancing Comments From Code Reviews to Underpin Reviewer Recommendation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3356819",
        "author": "Rong, Guoping and Yu, Yongda and Zhang, Yifan and Zhang, He and Shen, Haifeng and Shao, Dong and Kuang, Hongyu and Wang, Min and Wei, Zhao and Xu, Yong and Wang, Juhong",
        "title": "Distilling Quality Enhancing Comments From Code Reviews to Underpin Reviewer Recommendation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3356819",
        "doi": "10.1109/TSE.2024.3356819",
        "abstract": "Code review is an important practice in software development. One of its main objectives is for the assurance of code quality. For this purpose, the efficacy of code review is subject to the credibility of reviewers, i.e., reviewers who have demonstrated strong evidence of previously making quality-enhancing comments are more credible than those who have not. Code reviewer recommendation (CRR) is designed to assist in recommending suitable reviewers for a specific objective and, in this context, assurance of code quality. Its performance is susceptible to the relevance of its training dataset to this objective, composed of all reviewers\u2019 historical review comments, which, however, often contains a plethora of comments that are irrelevant to the enhancement of code quality. Furthermore, recommendation accuracy has been adopted as the sole metric to evaluate a recommender's performance, which is inadequate as it does not take reviewers\u2019 relevant credibility into consideration. These two issues form the ground truth problem in CRR as they both originate from the relevance of dataset used to train and evaluate CRR algorithms. To tackle this problem, we first propose the concept of Quality-Enhancing Review Comments (&lt;italic&gt;QERC&lt;/italic&gt;), which includes three types of comments - change-triggering inline comments, informative general comments, and approve-to-merge comments. We then devise a set of algorithms and procedures to obtain a distilled dataset by applying &lt;italic&gt;QERC&lt;/italic&gt; to the original dataset. We finally introduce a new metric \u2013 reviewer's credibility for quality enhancement (RCQE) \u2013 as a complementary metric to recommendation accuracy for evaluating the performance of recommenders. To validate the proposed QERC-based approach to CRR, we conduct empirical studies using real data from seven projects containing over 82K pull requests and 346K review comments. Results show that: (a) &lt;italic&gt;QERC&lt;/italic&gt; can effectively address the ground truth problem by distilling quality-enhancing comments from the dataset containing original code reviews, (b) &lt;italic&gt;QERC&lt;/italic&gt; can assist recommenders in finding highly credible reviewers at a slight cost of recommendation accuracy, and (c) even \u201cwrong\u201d recommendations using the distilled dataset are likely to be more credible than those using the original dataset.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "1658\u20131674",
        "numpages": "17"
    },
    "Automated Code Editing With Search-Generate-Modify": {
        "type": "article",
        "key": "10.1109/TSE.2024.3376387",
        "author": "Liu, Changshu and Cetin, Pelin and Patodia, Yogesh and Ray, Baishakhi and Chakraborty, Saikat and Ding, Yangruibo",
        "title": "Automated Code Editing With Search-Generate-Modify",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3376387",
        "doi": "10.1109/TSE.2024.3376387",
        "abstract": "Code editing is essential in evolving software development. In literature, several automated code editing tools are proposed, which leverage Information Retrieval-based techniques and Machine Learning-based code generation and code editing models. Each technique comes with its own promises and perils, and for this reason, they are often used together to complement their strengths and compensate for their weaknesses. This paper proposes a hybrid approach to better synthesize code edits by leveraging the power of code search, generation, and modification. Our key observation is that a patch that is obtained by search &amp; retrieval, even if incorrect, can provide helpful guidance to a code generation model. However, a retrieval-guided patch produced by a code generation model can still be a few tokens off from the intended patch. Such generated patches can be slightly modified to create the intended patches. We developed a novel tool to solve this challenge: &lt;sc&gt;SarGaM&lt;/sc&gt;, which is designed to follow a real developer's code editing behavior. Given an original code version, the developer may &lt;italic&gt;search&lt;/italic&gt; for the related patches, &lt;italic&gt;generate&lt;/italic&gt; or write the code, and then &lt;italic&gt;modify&lt;/italic&gt; the generated code to adapt it to the right context. Our evaluation of &lt;sc&gt;SarGaM&lt;/sc&gt; on edit generation shows superior performance w.r.t. the current state-of-the-art techniques. &lt;sc&gt;SarGaM&lt;/sc&gt; also shows its effectiveness on automated program repair tasks.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1675\u20131686",
        "numpages": "12"
    },
    "Evaluating Search-Based Software Microbenchmark Prioritization": {
        "type": "article",
        "key": "10.1109/TSE.2024.3380836",
        "author": "Laaber, Christoph and Yue, Tao and Ali, Shaukat",
        "title": "Evaluating Search-Based Software Microbenchmark Prioritization",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3380836",
        "doi": "10.1109/TSE.2024.3380836",
        "abstract": "Ensuring that software performance does not degrade after a code change is paramount. A solution is to regularly execute software microbenchmarks, a performance testing technique similar to (functional) unit tests, which, however, often becomes infeasible due to extensive runtimes. To address that challenge, research has investigated regression testing techniques, such as test case prioritization (TCP), which reorder the execution within a microbenchmark suite to detect larger performance changes sooner. Such techniques are either designed for unit tests and perform sub-par on microbenchmarks or require complex performance models, drastically reducing their potential application. In this paper, we empirically evaluate single- and multi-objective search-based microbenchmark prioritization techniques to understand whether they are more effective and efficient than greedy, coverage-based techniques. For this, we devise three search objectives, i.e., coverage to maximize, coverage overlap to minimize, and historical performance change detection to maximize. We find that search algorithms (SAs) are only competitive with but do not outperform the best greedy, coverage-based baselines. However, a simple greedy technique utilizing solely the performance change history (without coverage information) is equally or more effective than the best coverage-based techniques while being considerably more efficient, with a runtime overhead of less than &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$1$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"laaber-ieq1-3380836.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;\\%. These results show that simple, non-coverage-based techniques are a better fit for microbenchmarks than complex coverage-based techniques.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1687\u20131703",
        "numpages": "17"
    },
    "A Platform-Agnostic Framework for Automatically Identifying Performance Issue Reports With Heuristic Linguistic Patterns": {
        "type": "article",
        "key": "10.1109/TSE.2024.3390623",
        "author": "Zhao, Yutong and Xiao, Lu and Wong, Sunny",
        "title": "A Platform-Agnostic Framework for Automatically Identifying Performance Issue Reports With Heuristic Linguistic Patterns",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3390623",
        "doi": "10.1109/TSE.2024.3390623",
        "abstract": "Software performance is critical for system efficiency, with performance issues potentially resulting in budget overruns, project delays, and market losses. Such problems are reported to developers through issue tracking systems, which are often under-tagged, as the manual tagging process is voluntary and time-consuming. Existing automated performance issue tagging techniques, such as keyword matching and machine/deep learning models, struggle due to imbalanced datasets and a high degree of variance. This paper presents a novel hybrid classification approach, combining Heuristic Linguistic Patterns (&lt;italic&gt;HLP&lt;/italic&gt;s) with machine/deep learning models to enable practitioners to automatically identify performance-related issues. The proposed approach works across three progressive levels: &lt;italic&gt;HLP&lt;/italic&gt; tagging, sentence tagging, and issue tagging, with a focus on linguistic analysis of issue descriptions. The authors evaluate the approach on three different datasets collected from different projects and issue-tracking platforms to prove that the proposed framework is accurate, project- and platform-agnostic, and robust to imbalanced datasets. Furthermore, this study also examined how the two unique techniques of the framework, including the fuzzy &lt;italic&gt;HLP&lt;/italic&gt; matching and the &lt;italic&gt;Issue HLP Matrix&lt;/italic&gt;, contribute to the accuracy. Finally, the study explored the effectiveness and impact of two off-the-shelf feature selection techniques, &lt;italic&gt;Boruta&lt;/italic&gt; and &lt;italic&gt;RFE&lt;/italic&gt;, with the proposed framework. The results showed that the proposed framework has great potential for practitioners to accurately (with up to 100\\% precision, 66\\% recall, and 79\\% &lt;italic&gt;F1&lt;/italic&gt;-score) identify performance issues, with robustness to imbalanced data and good transferability to new projects and issue tracking platforms.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1704\u20131725",
        "numpages": "22"
    },
    "Clopper-Pearson Algorithms for Efficient Statistical Model Checking Estimation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3392720",
        "author": "Bu, Hao and Sun, Meng",
        "title": "Clopper-Pearson Algorithms for Efficient Statistical Model Checking Estimation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3392720",
        "doi": "10.1109/TSE.2024.3392720",
        "abstract": "Statistical model checking (SMC) is a simulation-based formal verification technique to deal with the scalability problem faced by traditional model checking. The main workflow of SMC is to perform iterative simulations. The number of simulations depends on users\u2019 requirement for the verification results, which can be very large if users require a high level of confidence and precision. Therefore, how to perform as fewer simulations as possible while achieving the same level of confidence and precision is one of the core problems of SMC. In this paper, we consider the estimation problem of SMC. Most existing statistical model checkers use the Okamoto bound to decide the simulation number. Although the Okamoto bound is sound, it is well known to be overly conservative. The simulation number decided by the Okamoto bound is usually much higher than it actually needs, which leads to a waste of time and computation resources. To tackle this problem, we propose an efficient, sound and lightweight estimation algorithm using the Clopper-Pearson confidence interval. We perform comprehensive numerical experiments and case studies to evaluate the performance of our algorithm, and the results show that our algorithm uses 40\\%-60\\% fewer simulations than the Okamoto bound. Our algorithm can be directly integrated into existing model checkers to reduce the verification time of SMC estimation problems.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1726\u20131746",
        "numpages": "21"
    },
    "Concretely Mapped Symbolic Memory Locations for Memory Error Detection": {
        "type": "article",
        "key": "10.1109/TSE.2024.3395412",
        "author": "Tu, Haoxin and Jiang, Lingxiao and Hong, Jiaqi and Ding, Xuhua and Jiang, He",
        "title": "Concretely Mapped Symbolic Memory Locations for Memory Error Detection",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3395412",
        "doi": "10.1109/TSE.2024.3395412",
        "abstract": "Memory allocation is a fundamental operation for managing memory objects in many programming languages. Misusing allocated memory objects (e.g., &lt;italic&gt;buffer overflow&lt;/italic&gt; and &lt;italic&gt;use-after-free&lt;/italic&gt;) can have catastrophic consequences. Symbolic execution-based approaches have been used to detect such memory errors, benefiting from their capabilities in automatic path exploration and test case generation. However, existing symbolic execution engines still suffer from fundamental limitations in modeling dynamic memory layouts; they either represent the locations of memory objects as concrete addresses and thus limit their analyses only to specific address layouts and miss errors that may only occur when the objects are located at special addresses, or represent the locations as simple symbolic variables without sufficient constraints and thus suffer from memory state explosion when they execute read/write operations involving symbolic addresses. Such limitations hinder the existing symbolic execution engines from effectively detecting certain memory errors. In this study, we propose &lt;sc&gt;SymLoc&lt;/sc&gt;, a symbolic execution-based approach that uses concretely mapped symbolic memory locations to alleviate the limitations mentioned above. Specifically, a new integration of three techniques is designed in &lt;sc&gt;SymLoc&lt;/sc&gt;: (1) the symbolization of addresses and encoding of symbolic addresses into path constraints, (2) the symbolic memory read/write operations using a symbolic-concrete memory map, and (3) the automatic tracking of the uses of symbolic memory locations. We build &lt;sc&gt;SymLoc&lt;/sc&gt; on top of the well-known symbolic execution engine KLEE and demonstrate its benefits in terms of memory error detection and code coverage capabilities. Our evaluation results show that: for address-specific spatial memory errors, &lt;sc&gt;SymLoc&lt;/sc&gt; can detect 23 more errors in &lt;monospace&gt;GNU Coreutils&lt;/monospace&gt;, &lt;monospace&gt;Make&lt;/monospace&gt;, and &lt;monospace&gt;m4&lt;/monospace&gt; programs that are difficult for other approaches to detect, and cover 15\\% and 48\\% more unique lines of code in the programs than two baseline approaches; for temporal memory errors, &lt;sc&gt;SymLoc&lt;/sc&gt; can detect 8\\%-64\\% more errors in the Juliet Test Suite than various existing state-of-the-art memory error detectors. We also present two case studies to show sample memory errors detected by &lt;sc&gt;SymLoc&lt;/sc&gt; along with their root causes and implications.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1747\u20131767",
        "numpages": "21"
    },
    "Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3397822",
        "author": "Tu, Haoxin and Zhou, Zhide and Jiang, He and Yusuf, Imam Nur Bani and Li, Yuxian and Jiang, Lingxiao",
        "title": "Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3397822",
        "doi": "10.1109/TSE.2024.3397822",
        "abstract": "Compiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task. Existing compiler bug isolation approaches convert the problem into a test program mutation problem, but they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named &lt;sc&gt;LLM4CBI&lt;/sc&gt; to utilize LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation may not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts. To overcome the challenges, three new components are designed in &lt;sc&gt;LLM4CBI&lt;/sc&gt;. First, &lt;sc&gt;LLM4CBI&lt;/sc&gt; utilizes a program complexity-guided prompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in programs for mutation. Second, &lt;sc&gt;LLM4CBI&lt;/sc&gt; employs a memorized prompt selection component, which adopts reinforcement learning to select specialized prompts for mutating test programs continuously. Third, a test program validation component is proposed to select specialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with the state-of-the-art approaches (DiWi and RecBi) over 120 real bugs from the two most popular compilers, namely GCC and LLVM, our evaluation demonstrates the advantages of &lt;sc&gt;LLM4CBI&lt;/sc&gt;: It can isolate 69.70\\%/21.74\\% and 24.44\\%/8.92\\% more bugs than DiWi and RecBi within Top-1/Top-5 ranked results. Additionally, we demonstrate that the LLMs component (i.e., GPT-3.5) used in &lt;sc&gt;LLM4CBI&lt;/sc&gt; can be easily replaced by other LLMs while still achieving reasonable results in comparison to related studies.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1768\u20131788",
        "numpages": "21"
    },
    "ContractCheck: Checking Ethereum Smart Contracts in Fine-Grained Level": {
        "type": "article",
        "key": "10.1109/TSE.2024.3400294",
        "author": "Wang, Xite and Tian, Senping and Cui, Wei",
        "title": "ContractCheck: Checking Ethereum Smart Contracts in Fine-Grained Level",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3400294",
        "doi": "10.1109/TSE.2024.3400294",
        "abstract": "The blockchain has been the main computing scenario for smart contracts, and the decentralized infrastructure of the blockchain is effectively implemented in a de-trusted and executable environment. However, vulnerabilities in smart contracts are particularly vulnerable to exploitation by malicious attackers and have always been a key issue in blockchain security. Existing traditional tools are inefficient in detecting vulnerabilities and have a high rate of false positives when detecting contracts. Some neural network methods have improved the detection efficiency, but they are not competent for fine-grained (code line level) vulnerability detection. We propose the ContractCheck model for detecting contract vulnerabilities based on neural network methods. ContractCheck extracts fine-grained segments from the abstract syntax tree (AST) and function call graph of smart contract source code. Furthermore, the segments are parsed into token flow retaining semantic information as uint, which are used to generate numerical vector sequences that can be trained using neural network methods. We conduct multiple rounds of experiments using a dataset constructed from 36,885 smart contracts and identified the optimal ContractCheck model structure by employing the Fasttext embedding vector algorithm and constructing a composite model using CNN and BiGRU for training the network. Evaluation on other datasets demonstrates that ContractCheck exhibits significant improvement in contract-level detection performance compared to other methods, with an increase of 23.60\\% in F1 score over the best existing method. Particularly, it achieves fine-grained detection based on neural network methods. The cases provide indicate that ContractCheck can effectively assist developers in accurately locating the presence of vulnerabilities, thereby enhancing the security of Ethereum smart contracts.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1789\u20131806",
        "numpages": "18"
    },
    "SQLPsdem: A Proxy-Based Mechanism Towards Detecting, Locating and Preventing Second-Order SQL Injections": {
        "type": "article",
        "key": "10.1109/TSE.2024.3400404",
        "author": "Zhang, Bing and Ren, Rong and Liu, Jia and Jiang, Mingcai and Ren, Jiadong and Li, Jingyue",
        "title": "SQLPsdem: A Proxy-Based Mechanism Towards Detecting, Locating and Preventing Second-Order SQL Injections",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3400404",
        "doi": "10.1109/TSE.2024.3400404",
        "abstract": "Due to well-hidden and stage-triggered properties of second-order SQL injections in web applications, current approaches are ineffective in addressing them and still report high false negatives and false positives. To reduce false results, we propose a &lt;underline&gt;P&lt;/underline&gt;roxy-based &lt;underline&gt;s&lt;/underline&gt;tatic analysis and &lt;underline&gt;dy&lt;/underline&gt;namic &lt;underline&gt;ex&lt;/underline&gt;ecution &lt;underline&gt;m&lt;/underline&gt;echanism towards detecting, locating and preventing second-order &lt;underline&gt;SQL&lt;/underline&gt; injections (SQLPsdem). The static analysis first locates SQL statements in web applications and identifies all data sources and injection points (e.g., Post, Sessions, Database, File names) that injection attacks can exploit. After that, we reconstruct the SQL statements and use attack engines to jointly generate attacks to cover all the state-of-the-art attack patterns so as to exploit these applications. We then use proxy-based dynamic execution to capture the data transmitted between web applications and their databases. The data are the reconstructed SQL statements with variable values from the attack payloads. If a web application is vulnerable, the data will contain malicious attacks on the database. We match the data with rules formulated by attack patterns to detect first and second-order SQL injection vulnerabilities in web applications, particularly the second-order ones. We use a representative and complete coverage of attack patterns and precise matching rules to reduce false results. By escaping and truncating malicious payloads in the data transmitted from the web application to the database, we can eliminate the possible negative impact of the data on the database. In the evaluation, by generating 52,771 SQL injection attacks using four attack generators, SQLPsdem successfully detects 26 second-order (including 13 newly discovered ones) and 375 first-order SQL injection vulnerabilities in 12 open-source web applications. SQLPsdem can also 100\\% eliminate the malicious impact of the data with negligible overhead.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1807\u20131826",
        "numpages": "20"
    },
    "A Lean Simulation Framework for Stress Testing IoT Cloud Systems": {
        "type": "article",
        "key": "10.1109/TSE.2024.3402157",
        "author": "Li, Jia and Moeini, Behrad and Nejati, Shiva and Sabetzadeh, Mehrdad and McCallen, Michael",
        "title": "A Lean Simulation Framework for Stress Testing IoT Cloud Systems",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3402157",
        "doi": "10.1109/TSE.2024.3402157",
        "abstract": "The Internet of Things (IoT) connects a plethora of smart devices globally across various applications like smart cities, autonomous vehicles, and health monitoring. Simulation plays a key role in the testing of IoT systems, noting that field testing of a complete IoT product may be infeasible or prohibitively expensive. This paper addresses a specific yet important need in simulation-based testing for IoT: Stress testing of cloud systems that are increasingly employed in IoT applications. Existing stress testing solutions for IoT demand significant computational resources, making them ill-suited and costly. We propose a lean simulation framework designed for IoT cloud stress testing. The framework enables efficient simulation of a large array of IoT and edge devices that communicate with the cloud. To facilitate simulation construction for practitioners, we develop a &lt;italic&gt;domain-specific language (DSL)&lt;/italic&gt;, named &lt;monospace&gt;IoTECS&lt;/monospace&gt;, for generating simulators from model-based specifications. We provide the syntax and semantics of &lt;monospace&gt;IoTECS&lt;/monospace&gt; and implement &lt;monospace&gt;IoTECS&lt;/monospace&gt; using Xtext and Xtend. We assess simulators generated from &lt;monospace&gt;IoTECS&lt;/monospace&gt; specifications for stress testing two real-world systems: a cloud-based IoT monitoring system developed by our industry partner and an IoT-connected vehicle system. Our empirical results indicate that simulators created using &lt;monospace&gt;IoTECS&lt;/monospace&gt;: (1) achieve best performance when configured with Docker containerization; (2) effectively assess the service capacity of our case-study systems, and (3) outperform industrial stress-testing baseline tools, JMeter and Locust, by a factor of 3.5 in terms of the number of IoT and edge devices they can simulate using identical hardware resources. To gain initial insights about the usefulness of &lt;monospace&gt;IoTECS&lt;/monospace&gt; in practice, we interviewed two engineers from our industry partner who have firsthand experience with &lt;monospace&gt;IoTECS&lt;/monospace&gt;. Feedback from these interviews suggests that &lt;monospace&gt;IoTECS&lt;/monospace&gt; is effective in stress testing IoT cloud systems, saving significant time and effort.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1827\u20131851",
        "numpages": "25"
    },
    "Fusing Code Searchers": {
        "type": "article",
        "key": "10.1109/TSE.2024.3403042",
        "author": "Wang, Shangwen and Geng, Mingyang and Lin, Bo and Sun, Zhensu and Wen, Ming and Liu, Yepang and Li, Li and Bissyand\\'{e}, Tegawend\\'{e} F. and Mao, Xiaoguang",
        "title": "Fusing Code Searchers",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3403042",
        "doi": "10.1109/TSE.2024.3403042",
        "abstract": "Code search, which consists in retrieving relevant code snippets from a codebase based on a given query, provides developers with useful references during software development. Over the years, techniques alternatively adopting different mechanisms to compute the relevance score between a query and a code snippet have been proposed to advance the state of the art in this domain, including those relying on information retrieval, supervised learning, and pre-training. Despite that, the usefulness of existing techniques is still compromised since they cannot effectively handle all the diversified queries and code in practice. To tackle this challenge, we present &lt;sc&gt;Dancer&lt;/sc&gt;, a data fusion based code searcher. Our intuition (also the basic hypothesis of this study) is that existing techniques may complement each other because of the intrinsic differences in their working mechanisms. We have validated this hypothesis via an exploratory study. Based on that, we propose to fuse the results generated by different code search techniques so that the advantage of each standalone technique can be fully leveraged. Specifically, we treat each technique as a retrieval system and leverage well-known data fusion approaches to aggregate the results from different systems. We evaluate six existing code search techniques on two large-scale datasets, and exploit eight classic data fusion approaches to incorporate their results. Our experiments show that the best fusion approach is able to outperform the standalone techniques by 35\\% - 550\\% and 65\\% - 825\\% in terms of MRR (mean reciprocal rank) on the two datasets, respectively.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1852\u20131866",
        "numpages": "15"
    },
    "MR&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}^{2}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msup&gt;&lt;mml:mi&gt;&nbsp;&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"gong-ieq1-3403108.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-KG: A Multi-Relation Multi-Rationale Knowledge Graph for Modeling Software Engineering Knowledge on Stack Overflow": {
        "type": "article",
        "key": "10.1109/TSE.2024.3403108",
        "author": "Gong, Lina and Zhang, Haoxiang",
        "title": "MR&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}^{2}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msup&gt;&lt;mml:mi&gt;&nbsp;&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"gong-ieq1-3403108.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-KG: A Multi-Relation Multi-Rationale Knowledge Graph for Modeling Software Engineering Knowledge on Stack Overflow",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3403108",
        "doi": "10.1109/TSE.2024.3403108",
        "abstract": "Stack Overflow is a knowledge sharing platform where its users create and share informative content from both inside and outside the site. Prior studies have leveraged the relation across Stack Overflow posts through internal links to build services and applications to enhance the accessibility of knowledge. However, they focused on studying a knowledge unit that consists of a question post and all the associated answer posts to represent the relation. It is unknown whether such representation of knowledge on Stack Overflow could comprehensively model various complex relations among webpages, such as questions, answers, internal and external links. In addition, the rationales behind sharing knowledge on Stack Overflow have yet to be explored among distinct user groups, such as askers, answerers, readers who wish to learn. Thus, in this study, we first investigate the real-world characteristics of Stack Overflow knowledge by abstracting the complex knowledge representation into relations among its building blocks. We observe that a question thread includes three basic knowledge relations to reassemble into complex knowledge, that is, the hierarchy relation within the associated answers in a question, the coupling relation between knowledge artifacts (i.e., question or answer posts) through internal links, and the complimentary relation between Stack Overflow posts and external websites. All these three basic knowledge relations are informative and could be caused by different rationales when the crowdsourced knowledge is shared on Stack Overflow. Our findings highlight that it is necessary to propose a comprehensive knowledge graph to represent the real-world knowledge on Stack Overflow. Therefore, we further propose a Multi-Relation Multi-Rationale Knowledge Graph (MR&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}^{2}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msup&gt;&lt;mml:mi&gt;&nbsp;&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"gong-ieq2-3403108.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-KG), whose nodes represent questions, answers, and external webpages. Edges in the MR&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}^{2}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msup&gt;&lt;mml:mi&gt;&nbsp;&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"gong-ieq3-3403108.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-KG represent the rationales included in the three structures (i.e., question answering, duplicate, priori, posterior, parallelism, containment, and working examples knowledge). In addition, we develop an automated approach to model the nodes and edges to represent Stack Overflow knowledge associated with a question thread. Our case study shows that the automated knowledge representation generation can achieve an ROC AUC of 96\\% and MCC of 89\\% to identify edges in the MR&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}^{2}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msup&gt;&lt;mml:mi&gt;&nbsp;&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"gong-ieq4-3403108.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-KG. To further evaluate the applicability of MR&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}^{2}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msup&gt;&lt;mml:mi&gt;&nbsp;&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"gong-ieq5-3403108.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-KG, we develop an answer generator to help developers efficiently identify the answers that meet their intent. Our user study of 100 real-world Java questions indicates the usefulness of MR&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}^{2}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msup&gt;&lt;mml:mi&gt;&nbsp;&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"gong-ieq6-3403108.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-KG. Finally, we discuss the implications of our findings for developers, researchers, and Stack Overflow moderators.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1867\u20131887",
        "numpages": "21"
    },
    "&lt;sc&gt;GenMorph&lt;/sc&gt;: Automatically Generating Metamorphic Relations via Genetic Programming": {
        "type": "article",
        "key": "10.1109/TSE.2024.3407840",
        "author": "Ayerdi, Jon and Terragni, Valerio and Jahangirova, Gunel and Arrieta, Aitor and Tonella, Paolo",
        "title": "&lt;sc&gt;GenMorph&lt;/sc&gt;: Automatically Generating Metamorphic Relations via Genetic Programming",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3407840",
        "doi": "10.1109/TSE.2024.3407840",
        "abstract": "Metamorphic testing is a popular approach that aims to alleviate the oracle problem in software testing. At the core of this approach are Metamorphic Relations (MRs), specifying properties that hold among multiple test inputs and corresponding outputs. Deriving MRs is mostly a manual activity, since their automated generation is a challenging and largely unexplored problem. This paper presents &lt;sc&gt;GenMorph&lt;/sc&gt;, a technique to automatically generate MRs for Java methods that involve inputs and outputs that are boolean, numerical, or ordered sequences. &lt;sc&gt;GenMorph&lt;/sc&gt; uses an evolutionary algorithm to search for &lt;italic&gt;effective&lt;/italic&gt; test oracles, i.e., oracles that trigger no false alarms and expose software faults in the method under test. The proposed search algorithm is guided by two fitness functions that measure the number of false alarms and the number of missed faults for the generated MRs. Our results show that &lt;sc&gt;GenMorph&lt;/sc&gt; generates effective MRs for 18 out of 23 methods (mutation score &gt; 20\\%). Furthermore, it can increase &lt;sc&gt;Randoop&lt;/sc&gt;'s fault detection capability in 7 out of 23 methods, and &lt;sc&gt;Evosuite&lt;/sc&gt;'s in 14 out of 23 methods. When compared with &lt;sc&gt;AutoMR&lt;/sc&gt;, a state-of-the-art MR generator, &lt;sc&gt;GenMorph&lt;/sc&gt; also outperformed its fault detection capability in 9 out of 10 methods.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1888\u20131900",
        "numpages": "13"
    },
    "Improving Issue-PR Link Prediction via Knowledge-Aware Heterogeneous Graph Learning": {
        "type": "article",
        "key": "10.1109/TSE.2024.3408448",
        "author": "Bai, Shuotong and Liu, Huaxiao and Dai, Enyan and Liu, Lei",
        "title": "Improving Issue-PR Link Prediction via Knowledge-Aware Heterogeneous Graph Learning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3408448",
        "doi": "10.1109/TSE.2024.3408448",
        "abstract": "Links between issues and pull requests (PRs) assist GitHub developers in tackling technical challenges, gaining development inspiration, and improving repository maintenance. In realistic repositories, these links are still insufficiently established. Aiming at this situation, existing works focus on issues and PRs themselves and employ text similarity with additional information like issue size to predict issue-PR links, yet their effectiveness is unsatisfactory. The limitation is that issues and PRs are not isolated on GitHub. Rather, they are related to multiple GitHub sources, including repositories and submitters, which, through their diverse relationships, can supply potential and crucial knowledge about technical domains, developmental insights, and cross-repository technical details. To this end, we propose &lt;underline&gt;A&lt;/underline&gt;uto &lt;bold&gt;IP&lt;/bold&gt; &lt;underline&gt;L&lt;/underline&gt;inker (AIPL), which introduces the heterogeneous graph to model multiple GitHub sources with their relationships. Further, it leverages the metapath-based technique to reveal and incorporate the potential information for a more comprehensive understanding of issues and PRs. Firstly, we identify 4 types of GitHub sources related to issues and PRs (repositories, users, issues, PRs) as well as their relationships, and model them into task-specific heterogeneous graphs. Next, we analyze information transmitted among issues or PRs to reveal which knowledge is crucial for them. Based on our analysis, we formulate a series of metapaths and employ the metapath-based technique to incorporate various information for learning the knowledge-aware embedding of issues and PRs. Finally, we can infer whether an issue and a PR can be linked based on their embedding. We evaluate the performance of AIPL on real-world data sets collected from GitHub. The results show that, compared to the baselines, AIPL can achieve average improvements of 15.94\\%, 15.19\\%, 20.52\\%, and 18.50\\% in terms of Accuracy, Precision, Recall, and F1-score.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "1901\u20131920",
        "numpages": "20"
    },
    "LUNA: A Model-Based Universal Analysis Framework for Large Language Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3411928",
        "author": "Song, Da and Xie, Xuan and Song, Jiayang and Zhu, Derui and Huang, Yuheng and Juefei-Xu, Felix and Ma, Lei",
        "title": "LUNA: A Model-Based Universal Analysis Framework for Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3411928",
        "doi": "10.1109/TSE.2024.3411928",
        "abstract": "Over the past decade, Artificial Intelligence (AI) has had great success recently and is being used in a wide range of academic and industrial fields. More recently, Large Language Models (LLMs) have made rapid advancements that have propelled AI to a new level, enabling and empowering even more diverse applications and industrial domains with intelligence, particularly in areas like software engineering and natural language processing. Nevertheless, a number of emerging trustworthiness concerns and issues exhibited in LLMs, e.g., robustness and hallucination, have already recently received much attention, without properly solving which the widespread adoption of LLMs could be greatly hindered in practice. The distinctive characteristics of LLMs, such as the self-attention mechanism, extremely large neural network scale, and autoregressive generation usage contexts, differ from classic AI software based on Convolutional Neural Networks and Recurrent Neural Networks and present new challenges for quality analysis. Up to the present, it still lacks universal and systematic analysis techniques for LLMs despite the urgent industrial demand across diverse domains. Towards bridging such a gap, in this paper, we initiate an early exploratory study and propose a universal analysis framework for LLMs, named &lt;italic&gt;LUNA&lt;/italic&gt;, which is designed to be general and extensible and enables versatile analysis of LLMs from multiple quality perspectives in a human-interpretable manner. In particular, we first leverage the data from desired trustworthiness perspectives to construct an abstract model as an auxiliary analysis asset and proxy, which is empowered by various abstract model construction methods built-in &lt;italic&gt;LUNA&lt;/italic&gt;. To assess the quality of the abstract model, we collect and define a number of evaluation metrics, aiming at both the abstract model level and the semantics level. Then, the semantics, which is the degree of satisfaction of the LLM w.r.t. the trustworthiness perspective, is bound to and enriches the abstract model with semantics, which enables more detailed analysis applications for diverse purposes, e.g., abnormal behavior detection. To better understand the potential usefulness of our analysis framework &lt;italic&gt;LUNA&lt;/italic&gt;, we conduct a large-scale evaluation, the results of which demonstrate that 1) the abstract model has the potential to distinguish normal and abnormal behavior in LLM, 2) &lt;italic&gt;LUNA&lt;/italic&gt; is effective for the real-world analysis of LLMs in practice, and the hyperparameter settings influence the performance, 3) different evaluation metrics are in different correlations with the analysis performance. In order to encourage further studies in the quality assurance of LLMs, we made all of the code and more detailed experimental results data available on the supplementary website of this paper &lt;uri&gt;https://sites.google.com/view/llm-luna&lt;/uri&gt;.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "1921\u20131948",
        "numpages": "28"
    },
    "Practical, Automated Scenario-Based Mobile App Testing": {
        "type": "article",
        "key": "10.1109/TSE.2024.3414672",
        "author": "Yu, Shengcheng and Fang, Chunrong and Du, Mingzhe and Ding, Zimin and Chen, Zhenyu and Su, Zhendong",
        "title": "Practical, Automated Scenario-Based Mobile App Testing",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3414672",
        "doi": "10.1109/TSE.2024.3414672",
        "abstract": "The importance of mobile application (app) quality assurance is increasing with the rapid development of the mobile Internet. Automated test generation approaches, as a dominant direction of app quality assurance, follow specific models or strategies, targeting at optimizing the code coverage. Such approaches lead to a huge gap between testing execution and app business logic. Test scripts developed by human testers consider business logic by focusing on testing scenarios. Due to the GUI-intensive feature of mobile apps, human testers always understand app GUI to organize test scripts for scenarios. This inspires us to utilize domain knowledge from app GUI understanding for scenario-based test generation. In this paper, we propose a novel approach, &lt;sc&gt;ScenTest&lt;/sc&gt;, for scenario-based mobile app testing with event knowledge graph (EKG) via GUI image understanding. &lt;sc&gt;ScenTest&lt;/sc&gt; tries to start automated testing by imitating human practices and integrating domain knowledge into scenario-based mobile app testing, realizing fully automated testing on target testing scenarios for the first time. &lt;sc&gt;ScenTest&lt;/sc&gt; extracts four kinds of entities and five kinds of corresponding relationships from crowdsourced test reports, where the test events and app GUI information are presented, and constructs the EKGs for specific scenarios. Then, &lt;sc&gt;ScenTest&lt;/sc&gt; conducts test generation for specific scenarios on different apps with the guidance of EKG with the combination consideration of app current state and testing context. We conduct an evaluation on &lt;sc&gt;ScenTest&lt;/sc&gt; on different aspects. The results show that the test generation of &lt;sc&gt;ScenTest&lt;/sc&gt; on the basis of EKG is effective, and &lt;sc&gt;ScenTest&lt;/sc&gt; reveals 150+ distinct real-world bugs in specific scenarios compared with representative baselines.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "1949\u20131966",
        "numpages": "18"
    },
    "Understanding and Detecting Real-World Safety Issues in Rust": {
        "type": "article",
        "key": "10.1109/TSE.2024.3380393",
        "author": "Qin, Boqin and Chen, Yilun and Liu, Haopeng and Zhang, Hua and Wen, Qiaoyan and Song, Linhai and Zhang, Yiying",
        "title": "Understanding and Detecting Real-World Safety Issues in Rust",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3380393",
        "doi": "10.1109/TSE.2024.3380393",
        "abstract": "Rust is a relatively new programming language designed for systems software development. Its objective is to combine the safety guarantees typically associated with high-level languages with the performance efficiency often found in executable programs implemented in low-level languages. The core design of Rust is a set of strict safety rules enforced through compile-time checks. However, to support more low-level controls, Rust also allows programmers to bypass its compiler checks by writing &lt;italic&gt;unsafe&lt;/italic&gt; code. As the adoption of Rust grows in the development of safety-critical software, it becomes increasingly important to understand what safety issues may elude Rust's compiler checks and manifest in real Rust programs. In this paper, we conduct a comprehensive, empirical study of Rust safety issues by close, manual inspection of 70 memory bugs, 100 concurrency bugs, and 110 programming errors leading to unexpected execution panics from five open-source Rust projects, five widely-used Rust libraries, and two online security databases. Our study answers three important questions: what memory-safety issues real Rust programs have, what concurrency bugs Rust programmers make, and how unexpected panics in Rust programs are caused. Our study reveals interesting real-world Rust program behaviors and highlights new issues made by Rust programmers. Building upon the findings of our study, we design and implement five static detectors. After being applied to the studied Rust programs and another 12 selected Rust projects, our checkers pinpoint 96 previously unknown bugs and report a negligible number of false positives, confirming their effectiveness and the value of our empirical study.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1306\u20131324",
        "numpages": "19"
    },
    "LIVABLE: Exploring Long-Tailed Classification of Software Vulnerability Types": {
        "type": "article",
        "key": "10.1109/TSE.2024.3382361",
        "author": "Wen, Xin-Cheng and Gao, Cuiyun and Luo, Feng and Wang, Haoyu and Li, Ge and Liao, Qing",
        "title": "LIVABLE: Exploring Long-Tailed Classification of Software Vulnerability Types",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3382361",
        "doi": "10.1109/TSE.2024.3382361",
        "abstract": "Prior studies generally focus on software vulnerability detection and have demonstrated the effectiveness of Graph Neural Network (GNN)-based approaches for the task. Considering the various types of software vulnerabilities and the associated different degrees of severity, it is also beneficial to determine the type of each vulnerable code for developers. In this paper, we observe that the distribution of vulnerability type is long-tailed in practice, where a small portion of classes have massive samples (i.e., head classes) but the others contain only a few samples (i.e., tail classes). Directly adopting previous vulnerability detection approaches tends to result in poor detection performance, mainly due to two reasons. First, it is difficult to effectively learn the vulnerability representation due to the over-smoothing issue of GNNs. Second, vulnerability types in tails are hard to be predicted due to the extremely few associated samples. To alleviate these issues, we propose a &lt;bold&gt;L&lt;/bold&gt;ong-ta&lt;bold&gt;I&lt;/bold&gt;led software &lt;bold&gt;V&lt;/bold&gt;ulner&lt;bold&gt;AB&lt;/bold&gt;i&lt;bold&gt;L&lt;/bold&gt;ity typ&lt;bold&gt;E&lt;/bold&gt; classification approach, called &lt;bold&gt;LIVABLE&lt;/bold&gt;. LIVABLE mainly consists of two modules, including (1) vulnerability representation learning module, which improves the propagation steps in GNN to distinguish node representations by a differentiated propagation method. A sequence-to-sequence model is also involved to enhance the vulnerability representations. (2) adaptive re-weighting module, which adjusts the learning weights for different types according to the training epochs and numbers of associated samples by a novel training loss. We verify the effectiveness of LIVABLE in both type classification and vulnerability detection tasks. For vulnerability type classification, the experiments on the Fan et al. dataset show that LIVABLE outperforms the state-of-the-art methods by 24.18\\% in terms of the accuracy metric, and also improves the performance in predicting tail classes by 7.7\\%. To evaluate the efficacy of the vulnerability representation learning module in LIVABLE, we further compare it with the recent vulnerability detection approaches on three benchmark datasets, which shows that the proposed representation learning module improves the best baselines by 4.03\\% on average in terms of accuracy.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1325\u20131339",
        "numpages": "15"
    },
    "ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3382365",
        "author": "Tang, Yutian and Liu, Zhijie and Zhou, Zhichao and Luo, Xiapu",
        "title": "ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3382365",
        "doi": "10.1109/TSE.2024.3382365",
        "abstract": "Recent advancements in large language models (LLMs) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, LLMs have shown potential in various software engineering applications. In this study, we present a systematic comparison of test suites generated by the ChatGPT LLM and the state-of-the-art SBST tool EvoSuite. Our comparison is based on several critical factors, including correctness, readability, code coverage, and bug detection capability. By highlighting the strengths and weaknesses of LLMs (specifically ChatGPT) in generating unit test cases compared to EvoSuite, this work provides valuable insights into the performance of LLMs in solving software engineering problems. Overall, our findings underscore the potential of LLMs in software engineering and pave the way for further research in this area.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1340\u20131359",
        "numpages": "20"
    },
    "DAppSCAN: Building Large-Scale Datasets for Smart Contract Weaknesses in DApp Projects": {
        "type": "article",
        "key": "10.1109/TSE.2024.3383422",
        "author": "Zheng, Zibin and Su, Jianzhong and Chen, Jiachi and Lo, David and Zhong, Zhijie and Ye, Mingxi",
        "title": "DAppSCAN: Building Large-Scale Datasets for Smart Contract Weaknesses in DApp Projects",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3383422",
        "doi": "10.1109/TSE.2024.3383422",
        "abstract": "The Smart Contract Weakness Classification Registry (SWC Registry) is a widely recognized list of smart contract weaknesses specific to the Ethereum platform. Despite the SWC Registry not being updated with new entries since 2020, the sustained development of smart contract analysis tools for detecting SWC-listed weaknesses highlights their ongoing significance in the field. However, evaluating these tools has proven challenging due to the absence of a large, unbiased, real-world dataset. To address this problem, we aim to build a large-scale SWC weakness dataset from real-world DApp projects. We recruited 22 participants and spent 44 person-months analyzing 1,199 open-source audit reports from 29 security teams. In total, we identified 9,154 weaknesses and developed two distinct datasets, i.e., &lt;sc&gt;DAppSCAN-Source&lt;/sc&gt; and &lt;sc&gt;DAppSCAN-Bytecode&lt;/sc&gt;. The &lt;sc&gt;DAppSCAN-Source&lt;/sc&gt; dataset comprises 39,904 Solidity files, featuring 1,618 SWC weaknesses sourced from 682 real-world DApp projects. However, the Solidity files in this dataset may not be directly compilable for further analysis. To facilitate automated analysis, we developed a tool capable of automatically identifying dependency relationships within DApp projects and completing missing public libraries. Using this tool, we created &lt;sc&gt;DAppSCAN-Bytecode&lt;/sc&gt; dataset, which consists of 6,665 compiled smart contract with 888 SWC weaknesses. Based on &lt;sc&gt;DAppSCAN-Bytecode&lt;/sc&gt;, we conducted an empirical study to evaluate the performance of state-of-the-art smart contract weakness detection tools. The evaluation results revealed sub-par performance for these tools in terms of both effectiveness and success detection rate, indicating that future development should prioritize real-world datasets over simplistic toy contracts.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1360\u20131373",
        "numpages": "14"
    },
    "Controller Synthesis for Autonomous Systems With Deep-Learning Perception Components": {
        "type": "article",
        "key": "10.1109/TSE.2024.3385378",
        "author": "Calinescu, Radu and Imrie, Calum and Mangal, Ravi and Rodrigues, Gena\\'{\\i}na Nunes and P\\u{a}s\\u{a}reanu, Corina and Santana, Misael Alpizar and V\\'{a}zquez, Gricel",
        "title": "Controller Synthesis for Autonomous Systems With Deep-Learning Perception Components",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3385378",
        "doi": "10.1109/TSE.2024.3385378",
        "abstract": "We present DeepDECS, a new method for the synthesis of correct-by-construction software controllers for autonomous systems that use deep neural network (DNN) classifiers for the perception step of their decision-making processes. Despite major advances in deep learning in recent years, providing safety guarantees for these systems remains very challenging. Our controller synthesis method addresses this challenge by integrating DNN verification with the synthesis of verified Markov models. The synthesised models correspond to discrete-event software controllers guaranteed to satisfy the safety, dependability and performance requirements of the autonomous system, and to be Pareto optimal with respect to a set of optimisation objectives. We evaluate the method in simulation by using it to synthesise controllers for mobile-robot collision limitation, and for maintaining driver attentiveness in shared-control autonomous driving.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1374\u20131395",
        "numpages": "22"
    },
    "Test Input Prioritization for Graph Neural Networks": {
        "type": "article",
        "key": "10.1109/TSE.2024.3385538",
        "author": "Li, Yinghua and Dang, Xueqi and Pian, Weiguo and Habib, Andrew and Klein, Jacques and Bissyand\\'{e}, Tegawend\\'{e} F.",
        "title": "Test Input Prioritization for Graph Neural Networks",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3385538",
        "doi": "10.1109/TSE.2024.3385538",
        "abstract": "GNNs have shown remarkable performance in a variety of classification tasks. The reliability of GNN models needs to be thoroughly validated before their deployment to ensure their accurate functioning. Therefore, effective testing is essential for identifying vulnerabilities in GNN models. However, given the complexity and size of graph-structured data, the cost of manual labelling of GNN test inputs can be prohibitively high for real-world use cases. Although several approaches have been proposed in the general domain of Deep Neural Network (DNN) testing to alleviate this labelling cost issue, these approaches are not suitable for GNNs because they do not account for the interdependence between GNN test inputs, which is crucial for GNN inference. In this paper, we propose NodeRank, a novel test prioritization approach specifically for GNNs, guided by ensemble learning-based mutation analysis. Inspired by traditional mutation testing, where specific operators are applied to mutate code statements to identify whether provided test cases reveal faults, NodeRank operates on a crucial premise: If a test input (node) can kill many mutated models and produce different prediction results with many mutated inputs, this input is considered more likely to be misclassified by the GNN model and should be prioritized higher. Through prioritization, these potentially misclassified inputs can be identified earlier with limited manual labeling cost. NodeRank introduces mutation operators suitable for GNNs, focusing on three key aspects: the graph structure, the features of the graph nodes, and the GNN model itself. NodeRank generates mutants and compares their predictions against that of the initial test inputs. Based on the comparison results, a mutation feature vector is generated for each test input and used as the input to ranking models for test prioritization. Leveraging ensemble learning techniques, NodeRank combines the prediction results of the base ranking models and produces a misclassification score for each test input, which can indicate the likelihood of this input being misclassified. NodeRank sorts all the test inputs based on their scores in descending order. To evaluate NodeRank, we build 124 GNN subjects (i.e., a pair of dataset and GNN model), incorporating both natural and adversarial contexts. Our results demonstrate that NodeRank outperforms all the compared test prioritization approaches in terms of both APFD and PFD, which are widely-adopted metrics in this field. Specifically, NodeRank achieves an average improvement of between 4.41\\% and 58.11\\% on original datasets and between 4.96\\% and 62.15\\% on adversarial datasets.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1396\u20131424",
        "numpages": "29"
    },
    "Domain-Driven Design for Microservices: An Evidence-Based Investigation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3385835",
        "author": "Zhong, Chenxing and Li, Shanshan and Huang, Huang and Liu, Xiaodong and Chen, Zhikun and Zhang, Yi and Zhang, He",
        "title": "Domain-Driven Design for Microservices: An Evidence-Based Investigation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3385835",
        "doi": "10.1109/TSE.2024.3385835",
        "abstract": "MicroService Architecture (MSA), a predominant architectural style in recent years, still faces the arduous task of identifying the boundaries of microservices. Domain-Driven Design (DDD) is regarded as one of the major design methods for addressing this task in practice, which aims to iteratively build domain models using a series of patterns, principles, and practices. The adoption of DDD for MSA (&lt;italic&gt;DDD4M&lt;/italic&gt; in short) can, however, present considerable challenges in terms of a sufficient understanding of the methodological requirements and the application domains. It is imperative to establish a systematic understanding about the various aspects of employing DDD4M and provide effective guidance. This study reports an empirical inquiry that integrates a systematic literature review and a confirmatory survey. By reviewing 34 scientific studies and consulting 63 practitioners, this study reveals several distinctive findings with regard to the state and challenges of as well as the possible solutions for DDD4M applications, from the &lt;italic&gt;5W1H&lt;/italic&gt; perspectives: &lt;italic&gt;When&lt;/italic&gt;, &lt;italic&gt;Where&lt;/italic&gt;, &lt;italic&gt;Why&lt;/italic&gt;, &lt;italic&gt;Who&lt;/italic&gt;, &lt;italic&gt;What&lt;/italic&gt;, and &lt;italic&gt;How&lt;/italic&gt;. The analysis and synthesis of evidence show a wide variation in understanding of domain modeling artifacts. The status quo indicates the need for further methodological support in terms of application process, domain model design and implementation, and domain knowledge acquisition and management. To advance the state-of-the-practice, our findings were organized into a preliminary checklist that intends to assist practitioners by illuminating a DDD4M application process and the specific key considerations along the way.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1425\u20131449",
        "numpages": "25"
    },
    "Characterizing Timeout Builds in Continuous Integration": {
        "type": "article",
        "key": "10.1109/TSE.2024.3387840",
        "author": "Weeraddana, Nimmi and Alfadel, Mahmoud and McIntosh, Shane",
        "title": "Characterizing Timeout Builds in Continuous Integration",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3387840",
        "doi": "10.1109/TSE.2024.3387840",
        "abstract": "Compute resources that enable Continuous Integration (CI, i.e., the automatic build and test cycle applied to the change sets that development teams produce) are a shared commodity that organizations need to manage. To prevent (erroneous) builds from consuming a large amount of resources, CI service providers often impose a time limit. CI builds that exceed the time limit are automatically terminated. While imposing a time limit helps to prevent abuse of the service, builds that timeout (a) consume the maximum amount of resources that a CI service is willing to provide and (b) leave CI users without an indication of whether the change set will pass or fail the CI process. Therefore, understanding timeout builds and the factors that contribute to them is important for improving the stability and quality of a CI service. In this paper, we investigate the prevalence of timeout builds and the characteristics associated with them. By analyzing a curated dataset of 936 projects that adopt the CircleCI service and report at least one timeout build, we find that the median duration of a timeout build (19.7 minutes) is more than five times that of a build that produces a pass or fail result (3.4 minutes). To better understand the factors contributing to timeout builds, we model timeout builds using characteristics of project build history, build queued time, timeout tendency, size, and author experience based on data collected from 105,663 CI builds. Our model demonstrates a discriminatory power that vastly surpasses that of a random predictor (Area Under the Receiver Operating characteristic Curve, i.e., &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$AUROC$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;U&lt;/mml:mi&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"weeraddana-ieq1-3387840.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; = 0.939) and is highly stable in its performance (&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$AUROC$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;U&lt;/mml:mi&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"weeraddana-ieq2-3387840.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; optimism = 0.0001). Moreover, our model reveals that the build history and timeout tendency features are strong indicators of timeout builds, with the timeout status of the most recent build accounting for the largest proportion of the explanatory power. A longitudinal analysis of the incidences of timeout builds (i.e., a study conducted over a period of time) indicates that 64.03\\% of timeout builds occur consecutively. In such cases, it takes a median of 24 hours before a build that passes or fails occurs. Our results imply that CI providers should exploit build history to anticipate timeout builds.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1450\u20131463",
        "numpages": "14"
    },
    "Pretrain, Prompt, and Transfer: Evolving Digital Twins for Time-to-Event Analysis in Cyber-Physical Systems": {
        "type": "article",
        "key": "10.1109/TSE.2024.3388572",
        "author": "Xu, Qinghua and Yue, Tao and Ali, Shaukat and Arratibel, Maite",
        "title": "Pretrain, Prompt, and Transfer: Evolving Digital Twins for Time-to-Event Analysis in Cyber-Physical Systems",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3388572",
        "doi": "10.1109/TSE.2024.3388572",
        "abstract": "Cyber-physicalnd systems (CPSs), e.g., elevators and autonomous driving systems, are progressively permeating our everyday lives. To ensure their safety, various analyses need to be conducted, such as anomaly detection and time-to-event analysis (the focus of this paper). Recently, it has been widely accepted that digital Twins (DTs) can be an efficient method to aid in developing, maintaining, and safe and secure operation of CPSs. However, CPSs frequently evolve, e.g., with new or updated functionalities, which demand their corresponding DTs be co-evolved, i.e., in synchronization with the CPSs. To that end, we propose a novel method, named &lt;monospace&gt;PPT&lt;/monospace&gt;, utilizing an uncertainty-aware transfer learning for DT evolution. Specifically, we first pretrain &lt;monospace&gt;PPT&lt;/monospace&gt; with a pretraining dataset to acquire generic knowledge about the CPSs, followed by adapting it to a specific CPS with the help of prompt tuning. Results highlight that &lt;monospace&gt;PPT&lt;/monospace&gt; is effective in time-to-event analysis in both elevator and autonomous driving case studies, on average, outperforming a baseline method by 7.31 and 12.58 in terms of Huber loss, respectively. The experiment results also affirm the effectiveness of transfer learning, prompt tuning, and uncertainty quantification in terms of reducing Huber loss by at least 21.32, 3.14, and 4.08, respectively, in both case studies.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1464\u20131477",
        "numpages": "14"
    },
    "MMO: Meta Multi-Objectivization for Software Configuration Tuning": {
        "type": "article",
        "key": "10.1109/TSE.2024.3388910",
        "author": "Chen, Pengzhou and Chen, Tao and Li, Miqing",
        "title": "MMO: Meta Multi-Objectivization for Software Configuration Tuning",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3388910",
        "doi": "10.1109/TSE.2024.3388910",
        "abstract": "Software configuration tuning is essential for optimizing a given performance objective (e.g., minimizing latency). Yet, due to the software's intrinsically complex configuration landscape and expensive measurement, there has been a rather mild success, particularly in preventing the search from being trapped in local optima. To address this issue, in this paper we take a different perspective. Instead of focusing on improving the optimizer, we work on the level of optimization model and propose a meta multi-objectivization (MMO) model that considers an auxiliary performance objective (e.g., throughput in addition to latency). What makes this model distinct is that we do not optimize the auxiliary performance objective, but rather use it to make similarly-performing while different configurations less comparable (i.e. Pareto nondominated to each other), thus preventing the search from being trapped in local optima. Importantly, by designing a new normalization method, we show how to effectively use the MMO model without worrying about its weight\u2014the only yet highly sensitive parameter that can affect its effectiveness. Experiments on 22 cases from 11 real-world software systems/environments confirm that our MMO model with the new normalization performs better than its state-of-the-art single-objective counterparts on 82\\% cases while achieving up to &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$2.09times$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mn&gt;2.09&lt;/mml:mn&gt;&lt;mml:mo&gt;\u00d7&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"chen-ieq1-3388910.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; speedup. For 68\\% of the cases, the new normalization also enables the MMO model to outperform the instance when using it with the normalization from our prior FSE work under pre-tuned best weights, saving a great amount of resources which would be otherwise necessary to find a good weight. We also demonstrate that the MMO model with the new normalization can consolidate recent model-based tuning tools on 68\\% of the cases with up to &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$1.22times$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mn&gt;1.22&lt;/mml:mn&gt;&lt;mml:mo&gt;\u00d7&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"chen-ieq2-3388910.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; speedup in general.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1478\u20131504",
        "numpages": "27"
    },
    "VarGAN: Adversarial Learning of Variable Semantic Representations": {
        "type": "article",
        "key": "10.1109/TSE.2024.3391730",
        "author": "Lin, Yalan and Wan, Chengcheng and Bai, Shuwen and Gu, Xiaodong",
        "title": "VarGAN: Adversarial Learning of Variable Semantic Representations",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3391730",
        "doi": "10.1109/TSE.2024.3391730",
        "abstract": "Variable names are of critical importance in code representation learning. However, due to diverse naming conventions, variables often receive arbitrary names, leading to long-tail, out-of-vocabulary (OOV), and other well-known problems. While the Byte-Pair Encoding (BPE) tokenizer has addressed the surface-level recognition of low-frequency tokens, it has not noticed the inadequate training of low-frequency identifiers by code representation models, resulting in an imbalanced distribution of rare and common identifiers. Consequently, code representation models struggle to effectively capture the semantics of low-frequency variable names. In this paper, we propose VarGAN, a novel method for variable name representations. VarGAN strengthens the training of low-frequency variables through adversarial training. Specifically, we regard the code representation model as a generator responsible for producing vectors from source code. Additionally, we employ a discriminator that detects whether the code input to the generator contains low-frequency variables. This adversarial setup regularizes the distribution of rare variables, making them overlap with their corresponding high-frequency counterparts in the vector space. Experimental results demonstrate that VarGAN empowers CodeBERT to generate code vectors that exhibit more uniform distribution for both low- and high-frequency identifiers. There is an improvement of 8\\% in similarity and relatedness scores compared to VarCLR in the IdBench benchmark. VarGAN is also validated in downstream tasks, where it exhibits enhanced capabilities in capturing token- and code-level semantics.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1505\u20131517",
        "numpages": "13"
    },
    "Cross-Language Taint Analysis: Generating Caller-Sensitive Native Code Specification for Java": {
        "type": "article",
        "key": "10.1109/TSE.2024.3392254",
        "author": "Kan, Shuangxiang and Gao, Yuhao and Zhong, Zexin and Sui, Yulei",
        "title": "Cross-Language Taint Analysis: Generating Caller-Sensitive Native Code Specification for Java",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3392254",
        "doi": "10.1109/TSE.2024.3392254",
        "abstract": "Cross-language programming is a common practice within the software development industry, offering developers a multitude of advantages such as expressiveness, interoperability, and cross-platform compatibility, for developing large-scale applications. As an important example, JNI (Java Native Interface) programming is widely used in diverse scenarios where Java interacts with code written in other programming languages, such as C or C++. Conventional static analysis based on a single programming language faces challenges when it comes to tracing the flow of values across multiple modules that are coded in different programming languages. In this paper, we introduce CSS, a new &lt;italic&gt;Caller-Sensitive Specification&lt;/italic&gt; approach designed to enhance the static taint analysis of Java programs employing JNI to interface with C/C++ code. In contrast to conservative specifications, this approach takes into consideration the calling context of the invoked C/C++ functions (or cross-language context), resulting in more precise and concise specifications for the side effects of native code. Furthermore, CSS specifically enhances the capabilities of Java analyzers, enabling them to perform precise static taint analysis across language boundaries into native code. The experimental results show that CSS can accurately summarize value-flow information and enhance the ability of Java monolingual static analyzers for cross-language taint flow tracking.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1518\u20131533",
        "numpages": "16"
    },
    "CRPWarner: Warning the Risk of Contract-Related Rug Pull in DeFi Smart Contracts": {
        "type": "article",
        "key": "10.1109/TSE.2024.3392451",
        "author": "Lin, Zewei and Chen, Jiachi and Wu, Jiajing and Zhang, Weizhe and Wang, Yongjuan and Zheng, Zibin",
        "title": "CRPWarner: Warning the Risk of Contract-Related Rug Pull in DeFi Smart Contracts",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3392451",
        "doi": "10.1109/TSE.2024.3392451",
        "abstract": "In recent years, Decentralized Finance (DeFi) has grown rapidly due to the development of blockchain technology and smart contracts. As of March 2023, the estimated global cryptocurrency market cap has reached approximately $949 billion. However, security incidents continue to plague the DeFi ecosystem, and one of the most notorious examples is the \u201cRug Pull\u201d scam. This type of cryptocurrency scam occurs when the developer of a particular token project intentionally abandons the project and disappears with investors\u2019 funds. Despite only emerging in recent years, Rug Pull events have already caused significant financial losses. In this work, we manually collected and analyzed 103 real-world rug pull events, categorizing them based on their scam methods. Two primary categories were identified: &lt;italic&gt;Contract-related&lt;/italic&gt; Rug Pull (through malicious functions in smart contracts) and &lt;italic&gt;Transaction-related&lt;/italic&gt; Rug Pull (through cryptocurrency trading without utilizing malicious functions). Based on the analysis of rug pull events, we propose CRPWarner (short for &lt;bold&gt;C&lt;/bold&gt;ontract-related &lt;bold&gt;R&lt;/bold&gt;ug &lt;bold&gt;P&lt;/bold&gt;ull Risk &lt;bold&gt;Warner&lt;/bold&gt;) to identify malicious functions in smart contracts and issue warnings regarding potential rug pulls. We evaluated CRPWarner on 69 open-source smart contracts related to rug pull events and achieved a 91.8\\% precision, 85.9\\% recall, and 88.7\\% F1-score. Additionally, when evaluating CRPWarner on 13,484 real-world token contracts on Ethereum, it successfully detected 4168 smart contracts with malicious functions, including zero-day examples. The precision of large-scale experiments reaches 84.9\\%.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1534\u20131547",
        "numpages": "14"
    },
    "No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT": {
        "type": "article",
        "key": "10.1109/TSE.2024.3392499",
        "author": "Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng",
        "title": "No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3392499",
        "doi": "10.1109/TSE.2024.3392499",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks, such as machine translation, question answering, summarization, and so on. Additionally, LLMs are also highly valuable in supporting software engineering tasks, particularly in the field of code generation. Automatic code generation is a process of automatically generating source code or executable code based on given specifications or requirements, improving developer productivity. In this study, we perform a systematic empirical assessment to the quality of code generation using &lt;italic&gt;ChatGPT&lt;/italic&gt;, a recent state-of-the-art product LLM. We leverage 728 algorithm problems in five languages (i.e., C, C++, Java, Python, and JavaScript) and 18 CWEs with 54 code scenarios for the code generation task. Our evaluation encompasses a comprehensive analysis of code snippets generated by &lt;italic&gt;ChatGPT&lt;/italic&gt;, focusing on three critical aspects: correctness, complexity, and security. We also specifically investigate &lt;italic&gt;ChatGPT&lt;/italic&gt;'s ability to engage in multi-round fixing process (i.e., &lt;italic&gt;ChatGPT&lt;/italic&gt;'s dialog ability, chatting between users and &lt;italic&gt;ChatGPT&lt;/italic&gt; for fixing generated buggy code) of facilitating code generation. By delving into the generated code and examining the experimental results, this work provides valuable insights into the performance of &lt;italic&gt;ChatGPT&lt;/italic&gt; in tackling code generation tasks over the three critical aspects. The experimental results demonstrate that (1) &lt;italic&gt;ChatGPT&lt;/italic&gt; is better at generating functionally correct code for problems before 2021 in different languages than problems after 2021 with &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$48.14\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;48.14&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"tang-ieq1-3392499.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; advantage in &lt;italic&gt;Accepted&lt;/italic&gt; rate on judgment platform, but &lt;italic&gt;ChatGPT&lt;/italic&gt;'s ability to directly fix erroneous code with multi-round fixing process to achieve correct functionality is relatively weak; (2) the distribution of cyclomatic and cognitive complexity levels for code snippets in different languages varies. Furthermore, the multi-round fixing process with &lt;italic&gt;ChatGPT &lt;/italic&gt; generally preserves or increases the complexity levels of code snippets; (3) in algorithm scenarios with languages of C, C++, and Java, and CWE scenarios with languages of C and Python3, the code generated by &lt;italic&gt;ChatGPT &lt;/italic&gt; has relevant vulnerabilities. However, the multi-round fixing process for vulnerable code snippets demonstrates promising results, with more than &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$89\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;89&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"tang-ieq2-3392499.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; of vulnerabilities successfully addressed; and (4) code generation may be affected by &lt;italic&gt;ChatGPT&lt;/italic&gt;'s non-determinism factor, resulting in variations of code snippets in functional correctness, complexity, and security. Overall, our findings uncover potential issues and limitations that arise in the &lt;italic&gt;ChatGPT&lt;/italic&gt;-based code generation and lay the groundwork for improving AI and LLM-based code generation techniques.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1548\u20131584",
        "numpages": "37"
    },
    "Automated Infrastructure as Code Program Testing": {
        "type": "article",
        "key": "10.1109/TSE.2024.3393070",
        "author": "Sokolowski, Daniel and Spielmann, David and Salvaneschi, Guido",
        "title": "Automated Infrastructure as Code Program Testing",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3393070",
        "doi": "10.1109/TSE.2024.3393070",
        "abstract": "Infrastructure as Code (IaC) enables efficient deployment and operation, which are crucial to releasing software quickly. As setups can be complex, developers implement IaC programs in general-purpose programming languages like TypeScript and Python, using PL-IaC solutions like Pulumi and AWS CDK. The reliability of such IaC programs is even more relevant than in traditional software because a bug in IaC impacts the whole system. Yet, even though testing is a standard development practice, it is rarely used for IaC programs. For instance, in August 2022, less than 1 \\% of the public Pulumi IaC programs on GitHub implemented tests. Available IaC program testing techniques severely limit the development velocity or require much development effort. To solve these issues, we propose Automated Configuration Testing (ACT), a methodology to test IaC programs in many configurations quickly and with low effort. ACT automatically mocks all resource definitions in the IaC program and uses generator and oracle plugins for test generation and validation. We implement ACT in &lt;monospace&gt;ProTI&lt;/monospace&gt;, a testing tool for Pulumi TypeScript with a type-based generator and oracle, and support for application specifications. Our evaluation with 6 081 programs from GitHub and artificial benchmarks shows that &lt;monospace&gt;ProTI&lt;/monospace&gt; can directly be applied to existing IaC programs, quickly finds bugs where current techniques are infeasible, and enables reusing existing generators and oracles thanks to its pluggable architecture.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1585\u20131599",
        "numpages": "15"
    },
    "TransformCode: A Contrastive Learning Framework for Code Embedding via Subtree Transformation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3393419",
        "author": "Xian, Zixiang and Huang, Rubing and Towey, Dave and Fang, Chunrong and Chen, Zhenyu",
        "title": "TransformCode: A Contrastive Learning Framework for Code Embedding via Subtree Transformation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3393419",
        "doi": "10.1109/TSE.2024.3393419",
        "abstract": "Artificial intelligence (AI) has revolutionized software engineering (SE) by enhancing software development efficiency. The advent of pre-trained models (PTMs) leveraging transfer learning has significantly advanced AI for SE. However, existing PTMs that operate on individual code tokens suffer from several limitations: They are costly to train and fine-tune; and they rely heavily on labeled data for fine-tuning on task-specific datasets. In this paper, we present &lt;bold&gt;TransformCode&lt;/bold&gt;, a novel framework that learns code embeddings in a contrastive learning manner. Our framework is encoder-agnostic and language-agnostic, which means that it can leverage any encoder model and handle any programming language. We also propose a novel data-augmentation technique called &lt;bold&gt;abstract syntax tree (AST) transformation&lt;/bold&gt;, which applies syntactic and semantic transformations to the original code snippets, to generate more diverse and robust samples for contrastive learning. Our framework has several advantages over existing methods: (1) It is flexible and adaptable, because it can easily be extended to other downstream tasks that require code representation (such as code-clone detection and classification); (2) it is efficient and scalable, because it does not require a large model or a large amount of training data, and it can support any programming language; (3) it is not limited to unsupervised learning, but can also be applied to some supervised learning tasks by incorporating task-specific labels or objectives; and (4) it can also adjust the number of encoder parameters based on computing resources. We evaluate our framework on several code-related tasks, and demonstrate its effectiveness and superiority over the state-of-the-art methods such as SourcererCC, Code2vec, and InferCode.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1600\u20131619",
        "numpages": "20"
    },
    "Neural Library Recommendation by Embedding Project-Library Knowledge Graph": {
        "type": "article",
        "key": "10.1109/TSE.2024.3393504",
        "author": "Li, Bo and Quan, Haowei and Wang, Jiawei and Liu, Pei and Cai, Haipeng and Miao, Yuan and Yang, Yun and Li, Li",
        "title": "Neural Library Recommendation by Embedding Project-Library Knowledge Graph",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3393504",
        "doi": "10.1109/TSE.2024.3393504",
        "abstract": "The prosperity of software applications brings fierce market competition to developers. Employing third-party libraries (TPLs) to add new features to projects under development and to reduce the time to market has become a popular way in the community. However, given the tremendous TPLs ready for use, it is challenging for developers to effectively and efficiently identify the most suitable TPLs. To tackle this obstacle, we propose an innovative approach named PyRec to recommend potentially useful TPLs to developers for their projects. Taking Python project development as a use case, PyRec embeds Python projects, TPLs, contextual information, and relations between those entities into a knowledge graph. Then, it employs a graph neural network to capture useful information from the graph to make TPL recommendations. Different from existing approaches, PyRec can make full use of not only project-library interaction information but also contextual information to make more accurate TPL recommendations. Comprehensive evaluations are conducted based on 12,421 Python projects involving 963 TPLs, 9,675 extra entities, 121,474 library usage records, and 73,277 contextual records. Compared with five representative approaches, PyRec improves the recommendation performance significantly in all cases.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1620\u20131638",
        "numpages": "19"
    },
    "Darcy: Automatic Architectural Inconsistency Resolution in Java": {
        "type": "article",
        "key": "10.1109/TSE.2024.3396433",
        "author": "Ghorbani, Negar and Singh, Tarandeep and Garcia, Joshua and Malek, Sam",
        "title": "Darcy: Automatic Architectural Inconsistency Resolution in Java",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3396433",
        "doi": "10.1109/TSE.2024.3396433",
        "abstract": "Many mainstream programming languages lack extensive support for architectural constructs, such as software components, which limits software developers in employing many benefits of architecture-based development. To address this issue, Java, one of the most popular and widely-used programming languages, has introduced the Java Platform Module System (JPMS) in its 9th and subsequent versions. JPMS provides the notion of architectural constructs, i.e., software components, as an encapsulation of modules that helps developers construct and maintain large applications efficiently\u2014as well as improving the encapsulation, security, and maintainability of Java applications in general and the JDK itself. However, ensuring that module declarations reflect the actual usage of modules in an application remains a challenge that results in developers mistakenly introducing inconsistent module dependencies at both compile- and run-time. In this paper, we studied JPMS properties and architectural notions in-depth and defined a defect model consisting of eight inconsistent modular dependencies that may arise in Java applications. Based on this defect model, we also present &lt;sc&gt;Darcy&lt;/sc&gt;, a framework that leverages the defect model and static analysis techniques to automatically detect and repair the specified inconsistent dependencies within Java applications at both compile- and run-time. The results of our experiments, conducted over 52 open-source Java 9+ applications, indicate that architectural inconsistencies are widespread and demonstrate &lt;sc&gt;Darcy&lt;/sc&gt;'s effectiveness for automated resolution of these inconsistencies.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1639\u20131657",
        "numpages": "19"
    },
    "On the Understandability of MLOps System Architectures": {
        "type": "article",
        "key": "10.1109/TSE.2024.3367488",
        "author": "Warnett, Stephen John and Zdun, Uwe",
        "title": "On the Understandability of MLOps System Architectures",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3367488",
        "doi": "10.1109/TSE.2024.3367488",
        "abstract": "Machine Learning Operations (MLOps) is the practice of streamlining and optimising the machine learning (ML) workflow, from development to deployment, using DevOps (software development and IT operations) principles and ML-specific activities. Architectural descriptions of MLOps systems often consist of informal textual descriptions and informal graphical system diagrams that vary considerably in consistency, quality, detail, and content. Such descriptions only sometimes follow standards or schemata and may be hard to understand. We aimed to investigate informal textual descriptions and informal graphical MLOps system architecture representations and compare them with semi-formal MLOps system diagrams for those systems. We report on a controlled experiment with sixty-three participants investigating the understandability of MLOps system architecture descriptions based on informal and semi-formal representations. The results indicate that the understandability (quantified by task correctness) of MLOps system descriptions is significantly greater using supplementary semi-formal MLOps system diagrams, that using semi-formal MLOps system diagrams does not significantly increase task duration (and thus hinder understanding), and that task correctness is only significantly correlated with task duration when semi-formal MLOps system diagrams are provided.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "1015\u20131039",
        "numpages": "25"
    },
    "Provably Valid and Diverse Mutations of Real-World Media Data for DNN Testing": {
        "type": "article",
        "key": "10.1109/TSE.2024.3370807",
        "author": "Yuan, Yuanyuan and Pang, Qi and Wang, Shuai",
        "title": "Provably Valid and Diverse Mutations of Real-World Media Data for DNN Testing",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3370807",
        "doi": "10.1109/TSE.2024.3370807",
        "abstract": "Deep neural networks (DNNs) often accept high-dimensional media data (e.g., photos, text, and audio) and understand their perceptual content (e.g., a cat). To test DNNs, &lt;italic&gt;diverse&lt;/italic&gt; inputs are needed to trigger mis-predictions. Some preliminary works use byte-level mutations or domain-specific filters (e.g., foggy), whose enabled mutations may be limited and likely error-prone. State-of-the-art (SOTA) works employ deep generative models to generate (infinite) inputs. Also, to keep the mutated inputs perceptually &lt;italic&gt;valid&lt;/italic&gt; (e.g., a cat remains a \u201ccat\u201d after mutation), existing efforts rely on imprecise and less generalizable heuristics. This study revisits two key objectives in media input mutation \u2014 perception diversity (&lt;sc&gt;Div&lt;/sc&gt;) and validity (&lt;sc&gt;Val&lt;/sc&gt;) \u2014 in a rigorous manner based on manifold, a well-developed theory capturing perceptions of high-dimensional media data in a low-dimensional space. We show important results that &lt;sc&gt;Div&lt;/sc&gt; and &lt;sc&gt;Val&lt;/sc&gt; inextricably bound each other, and prove that SOTA generative model-based methods fundamentally fail to mutate &lt;italic&gt;real-world media data&lt;/italic&gt; (either sacrificing &lt;sc&gt;Div&lt;/sc&gt; or &lt;sc&gt;Val&lt;/sc&gt;). In contrast, we discuss the feasibility of mutating real-world media data with provably high &lt;sc&gt;Div&lt;/sc&gt; and &lt;sc&gt;Val&lt;/sc&gt; based on manifold. Following, we concretize the technical solution of mutating media data of various formats (images, audios, text) via a &lt;italic&gt;unified&lt;/italic&gt; manner based on manifold. Specifically, when media data are projected into a low-dimensional manifold, the data can be mutated by walking on the manifold with certain directions and step sizes. When contrasted with the input data, the mutated data exhibit encouraging &lt;sc&gt;Div&lt;/sc&gt; in the perceptual traits (e.g., lying vs. standing dog) while retaining reasonably high &lt;sc&gt;Val&lt;/sc&gt; (i.e., a dog remains a dog). We implement our techniques in &lt;sc&gt;DeepWalk&lt;/sc&gt; for testing DNNs. &lt;sc&gt;DeepWalk&lt;/sc&gt; constructs manifolds for media data offline. In online testing, &lt;sc&gt;DeepWalk&lt;/sc&gt; walks on manifolds to generate mutated media data with provably high &lt;sc&gt;Div&lt;/sc&gt; and &lt;sc&gt;Val&lt;/sc&gt;. Our evaluation tests DNNs executing various tasks (e.g., classification, self-driving, machine translation) and media data of different types (image, audio, text). &lt;sc&gt;DeepWalk&lt;/sc&gt; outperforms prior methods in terms of the testing comprehensiveness and can find more error-triggering inputs with higher quality. The tested DNNs, after repaired using &lt;sc&gt;DeepWalk&lt;/sc&gt;'s findings, exhibit better accuracy.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1040\u20131064",
        "numpages": "25"
    },
    "Exploring the Role of Team Security Climate in the Implementation of Security by Design: A Case Study in the Defense Sector": {
        "type": "article",
        "key": "10.1109/TSE.2024.3374114",
        "author": "Prudjinski, Micha and Hadar, Irit and Luria, Gil",
        "title": "Exploring the Role of Team Security Climate in the Implementation of Security by Design: A Case Study in the Defense Sector",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3374114",
        "doi": "10.1109/TSE.2024.3374114",
        "abstract": "The rapid diffusion of software systems into all aspects of human life has exacerbated security threats and thus amplified the requirement for proactive approaches for designing security as a default. Following evidence from previous studies, indicating organizational climate as a key influencer on developers\u2019 security mindsets and behaviors, this study was focused on examining the relationship between team security climate level and developers\u2019 actual practices when addressing security threats during software development. The empirical study was conducted in a defense software development organization and included a survey questionnaire completed by 212 developers from 50 software teams. The results were compared to managers\u2019 evaluations regarding the implementation level of security mechanisms in the teams\u2019 development. The findings indicate a positive relationship between team security climate level and the implementation level of security mechanisms in the teams' software development and that team productivity climate moderates this relationship. The results also reveal that team security climate mediates the association between manager\u2013developer relationships and the implementation level of security mechanisms in software development. The study provides support to organizational climate theory and to the specific scale of organizational security climate, demonstrating the predictive validity of this scale, and sheds light on the influence of leadership and competitive facets on security engineering.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1065\u20131079",
        "numpages": "15"
    },
    "Active Code Learning: Benchmarking Sample-Efficient Training of Code Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3376964",
        "author": "Hu, Qiang and Guo, Yuejun and Xie, Xiaofei and Cordy, Maxime and Ma, Lei and Papadakis, Mike and Traon, Yves Le",
        "title": "Active Code Learning: Benchmarking Sample-Efficient Training of Code Models",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3376964",
        "doi": "10.1109/TSE.2024.3376964",
        "abstract": "The costly human effort required to prepare the training data of machine learning (ML) models hinders their practical development and usage in software engineering (ML4Code), especially for those with limited budgets. Therefore, efficiently training models of code with less human effort has become an emergent problem. Active learning is such a technique to address this issue that allows developers to train a model with reduced data while producing models with desired performance, which has been well studied in computer vision and natural language processing domains. Unfortunately, there is no such work that explores the effectiveness of active learning for code models. In this paper, we bridge this gap by building the first benchmark to study this critical problem - active code learning. Specifically, we collect 11 acquisition functions (which are used for data selection in active learning) from existing works and adapt them for code-related tasks. Then, we conduct an empirical study to check whether these acquisition functions maintain performance for code data. The results demonstrate that feature selection highly affects active learning and using output vectors to select data is the best choice. For the code summarization task, active code learning is ineffective which produces models with over a 29.64\\% gap compared to the expected performance. Furthermore, we explore future directions of active code learning with an exploratory study. We propose to replace distance calculation methods with evaluation metrics and find a correlation between these evaluation-based distance methods and the performance of code models.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1080\u20131095",
        "numpages": "16"
    },
    "Asking and Answering Questions During Memory Profiling": {
        "type": "article",
        "key": "10.1109/TSE.2024.3377127",
        "author": "Blanco, Alison Fernandez and C\\'{o}rdova, Araceli Queirolo and Bergel, Alexandre and Alcocer, Juan Pablo Sandoval",
        "title": "Asking and Answering Questions During Memory Profiling",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3377127",
        "doi": "10.1109/TSE.2024.3377127",
        "abstract": "The software engineering community has produced numerous tools, techniques, and methodologies for practitioners to analyze and optimize memory usage during software execution. However, little is known about the actual needs of programmers when analyzing memory behavior and how they use tools to address those needs. We conducted an exploratory study (i) to understand what a programmer needs to know when analyzing memory behavior and (ii) how a programmer finds that information with current tools. From our observations, we provide a catalog of 34 questions programmers ask themselves when analyzing memory behavior. We also report a detailed analysis of how some tools are used to answer these questions and the difficulties participants face during the process. Finally, we present four recommendations to guide researchers and developers in designing, evaluating, and improving memory behavior analysis tools.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1096\u20131117",
        "numpages": "22"
    },
    "Methods and Benchmark for Detecting Cryptographic API Misuses in Python": {
        "type": "article",
        "key": "10.1109/TSE.2024.3377182",
        "author": "Frantz, Miles and Xiao, Ya and Pias, Tanmoy Sarkar and Meng, Na and Yao, Danfeng",
        "title": "Methods and Benchmark for Detecting Cryptographic API Misuses in Python",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3377182",
        "doi": "10.1109/TSE.2024.3377182",
        "abstract": "Extensive research has been conducted to explore cryptographic API misuse in Java. However, despite the tremendous popularity of the Python language, uncovering similar issues has not been fully explored. The current static code analysis tools for Python are unable to scan the increasing complexity of the source code. This limitation decreases the analysis depth, resulting in more undetected cryptographic misuses. In this research, we propose Cryptolation, a Static Code Analysis (SCA) tool that provides security guarantees for complex Python cryptographic code. Most existing analysis tools for Python solely focus on specific Frameworks such as Django or Flask. However, using a SCA approach, Cryptolation focuses on the language and not any framework. Cryptolation performs an inter-procedural data-flow analysis to handle many Python language features through variable inference (statically predicting what the variable value is) and SCA. Cryptolation covers 59 Python cryptographic modules and can identify 18 potential cryptographic misuses that involve complex language features. In this paper, we also provide a comprehensive analysis and a state-of-the-art benchmark for understanding the Python cryptographic Application Program Interface (API) misuses and their detection. Our state-of-the-art benchmark PyCryptoBench includes 1,836 Python cryptographic test cases that covers both 18 cryptographic rules and five language features. PyCryptoBench also provides a framework for evaluating and comparing different cryptographic scanners for Python. To evaluate the performance of our proposed cryptographic Python scanner, we evaluated Cryptolation against three other state-of-the-art tools: Bandit, Semgrep, and Dlint. We evaluated these four tools using our benchmark PyCryptoBench and manual evaluation of (four Top-Ranked and 939 Un-Ranked) real-world projects. Our results reveal that, overall, Cryptolation achieved the highest precision throughout our testing; and the highest accuracy on our benchmark. Cryptolation had 100\\% precision on PyCryptoBench, and the highest precision on the real-world projects.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1118\u20131129",
        "numpages": "12"
    },
    "Mutation Testing in Practice: Insights From Open-Source Software Developers": {
        "type": "article",
        "key": "10.1109/TSE.2024.3377378",
        "author": "S\\'{a}nchez, Ana B. and Parejo, Jos\\'{e} A. and Segura, Sergio and Dur\\'{a}n, Amador and Papadakis, Mike",
        "title": "Mutation Testing in Practice: Insights From Open-Source Software Developers",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3377378",
        "doi": "10.1109/TSE.2024.3377378",
        "abstract": "Mutation testing drives the creation and improvement of test cases by evaluating their ability to identify synthetic faults. Over the past decades, the technique has gained popularity in academic circles. In practice, however, little is known about its adoption and use. While there are some pilot studies applying mutation testing in industry, the overall usage of mutation testing among developers remains largely unexplored. To fill this gap, this paper presents the results of a qualitative study among open-source developers on the use of mutation testing. Specifically, we report the results of a survey of 104 contributors to open-source projects using a variety of mutation testing tools. The findings of our study provide helpful insights into the use of mutation testing in practice, including its main benefits and limitations. Overall, we observe a high degree of satisfaction with mutation testing across different programming languages and mutation testing tools. Developers find the technique helpful for improving the quality of test suites, detecting bugs, and improving code maintainability. Popularity, usability, and configurability emerge as key factors for the adoption of mutation tools, whereas performance stands overwhelmingly as their main limitation. These results lay the groundwork for new research contributions and tools that meet the needs of developers and boost the widespread adoption of mutation testing.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1130\u20131143",
        "numpages": "14"
    },
    "Generic Sensitivity: Generics-Guided Context Sensitivity for Pointer Analysis": {
        "type": "article",
        "key": "10.1109/TSE.2024.3377645",
        "author": "Li, Haofeng and Tan, Tian and Li, Yue and Lu, Jie and Meng, Haining and Cao, Liqing and Huang, Yongheng and Li, Lian and Gao, Lin and Di, Peng and Lin, Liang and Cui, ChenXi",
        "title": "Generic Sensitivity: Generics-Guided Context Sensitivity for Pointer Analysis",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3377645",
        "doi": "10.1109/TSE.2024.3377645",
        "abstract": "Generic programming has found widespread application in object-oriented languages like Java. However, existing context-sensitive pointer analyses fail to leverage the benefits of generic programming. This paper introduces &lt;italic&gt;generic sensitivity&lt;/italic&gt;, a new context customization scheme targeting generics. We design our context customization scheme in such a way that generic instantiation sites, i.e., locations instantiating generic classes/methods with concrete types, are always preserved as key context elements. This is realized by augmenting contexts with a type variable lookup map, which is efficiently generated in a context-sensitive manner throughout the analysis process. We have implemented various variants of generic-sensitive analysis in &lt;sc&gt;WALA&lt;/sc&gt; and conducted extensive experiments to compare it with state-of-the-art approaches, including both traditional and selective context-sensitivity methods. The evaluation results demonstrate that generic sensitivity effectively enhances existing context-sensitivity approaches, striking a new balance between efficiency and precision. For instance, it enables a 1-object-sensitive analysis to achieve overall better precision compared to a 2-object-sensitive analysis, with an average speedup of 12.6 times (up to 62 times).",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1144\u20131162",
        "numpages": "19"
    },
    "hmCodeTrans: Human\u2013Machine Interactive Code Translation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3379583",
        "author": "Liu, Jiaqi and Zhang, Fengming and Zhang, Xin and Yu, Zhiwen and Wang, Liang and Zhang, Yao and Guo, Bin",
        "title": "hmCodeTrans: Human\u2013Machine Interactive Code Translation",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3379583",
        "doi": "10.1109/TSE.2024.3379583",
        "abstract": "Code translation, i.e., translating one kind of code language to another, plays an important role in scenarios such as application modernization and multi-language versions of applications on different platforms. Even the most advanced machine-based code translation methods can not guarantee an error-free result. Therefore, the participance of software engineer is necessary. Considering both accuracy and efficiency, it is suggested to work in a human-machine collaborative way. However, in many realistic scenarios, human and machine collaborate ineffectively - model translates first and then human makes further editing, without any interaction. To solve this problem, we propose hmCodeTrans, a novel method that achieves code translation in an &lt;italic&gt;interactive human-machine collaborative way&lt;/italic&gt;. It can (1) save the human effort by introducing two novel human-machine collaboration patterns: prefix-based and segment-based ones, which feed the software engineer's sequential or scattered editing back to model and thus enabling the model to make a better retranslation; (2) reduce the response time based on two proposed modules: attention cache module that avoids duplicate prefix inference with cached attention information, and suffix splicing module that reduces invalid suffix inference by splicing a predefined suffix. The experiments are conducted on two real datasets. Results show that compared with the baselines, our approach can effectively save the human effort and reduce the response time. Last but not least, a user study involving five real software engineers is given, which validates that the proposed approach owns the lowest human effort and shows the users\u2019 satisfaction towards the approach.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1163\u20131181",
        "numpages": "19"
    },
    "Toward Cost-Effective Adaptive Random Testing: An Approximate Nearest Neighbor Approach": {
        "type": "article",
        "key": "10.1109/TSE.2024.3379592",
        "author": "Huang, Rubing and Cui, Chenhui and Lian, Junlong and Towey, Dave and Sun, Weifeng and Chen, Haibo",
        "title": "Toward Cost-Effective Adaptive Random Testing: An Approximate Nearest Neighbor Approach",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3379592",
        "doi": "10.1109/TSE.2024.3379592",
        "abstract": "&lt;italic&gt;Adaptive Random Testing&lt;/italic&gt; (ART) enhances the testing effectiveness (including fault-detection capability) of &lt;italic&gt;Random Testing&lt;/italic&gt; (RT) by increasing the diversity of the random test cases throughout the input domain. Many ART algorithms have been investigated such as &lt;italic&gt;Fixed-Size-Candidate-Set ART&lt;/italic&gt; (FSCS) and &lt;italic&gt;Restricted Random Testing&lt;/italic&gt; (RRT), and have been widely used in many practical applications. Despite its popularity, ART suffers from the problem of high computational costs during test-case generation, especially as the number of test cases increases. Although several strategies have been proposed to enhance the ART testing efficiency, such as the &lt;italic&gt;forgetting strategy&lt;/italic&gt; and the &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$k$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"huang-ieq1-3379592.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; &lt;italic&gt;-dimensional tree strategy&lt;/italic&gt;, these algorithms still face some challenges, including: (1) Although these algorithms can reduce the computation time, their execution costs are still very high, especially when the number of test cases is large; and (2) To achieve low computational costs, they may sacrifice some fault-detection capability. In this paper, we propose an approach based on &lt;italic&gt;Approximate Nearest Neighbors&lt;/italic&gt; (ANNs), called &lt;italic&gt;Locality-Sensitive Hashing ART&lt;/italic&gt; (LSH-ART). When calculating distances among different test inputs, LSH-ART identifies the approximate (not necessarily exact) nearest neighbors for candidates in an efficient way. LSH-ART attempts to balance ART testing effectiveness and efficiency.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1182\u20131214",
        "numpages": "33"
    },
    "Toward a Theory of Causation for Interpreting Neural Code Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3379943",
        "author": "Nader Palacio, David and Velasco, Alejandro and Cooper, Nathan and Rodriguez, Alvaro and Moran, Kevin and Poshyvanyk, Denys",
        "title": "Toward a Theory of Causation for Interpreting Neural Code Models",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3379943",
        "doi": "10.1109/TSE.2024.3379943",
        "abstract": "Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces &lt;italic&gt;do&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}_{textbf{code}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msub&gt;&lt;mml:mrow/&gt;&lt;mml:mrow&gt;&lt;mml:mtext mathvariant=\"bold\"&gt;code&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"naderpalacio-ieq1-3379943.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; &lt;/italic&gt;, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. &lt;italic&gt;do&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}_{textbf{code}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msub&gt;&lt;mml:mrow/&gt;&lt;mml:mrow&gt;&lt;mml:mtext mathvariant=\"bold\"&gt;code&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"naderpalacio-ieq2-3379943.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; &lt;/italic&gt; is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of &lt;italic&gt;do&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}_{textbf{code}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msub&gt;&lt;mml:mrow/&gt;&lt;mml:mrow&gt;&lt;mml:mtext mathvariant=\"bold\"&gt;code&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"naderpalacio-ieq3-3379943.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; &lt;/italic&gt; are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of &lt;italic&gt;spurious correlations&lt;/italic&gt; by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical benefit of &lt;italic&gt;do&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}_{textbf{code}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msub&gt;&lt;mml:mrow/&gt;&lt;mml:mrow&gt;&lt;mml:mtext mathvariant=\"bold\"&gt;code&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"naderpalacio-ieq4-3379943.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; &lt;/italic&gt;, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and ten NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax. All our NCMs, except for the BERT-like model, statistically learn to predict tokens related to blocks of code (&lt;italic&gt;e.g.,&lt;/italic&gt; brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of &lt;italic&gt;do&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}_{textbf{code}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:msub&gt;&lt;mml:mrow/&gt;&lt;mml:mrow&gt;&lt;mml:mtext mathvariant=\"bold\"&gt;code&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"naderpalacio-ieq5-3379943.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; &lt;/italic&gt; as a useful method to detect and facilitate the elimination of confounding bias in NCMs.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1215\u20131243",
        "numpages": "29"
    },
    "Microservice Extraction Based on a Comprehensive Evaluation of Logical Independence and Performance": {
        "type": "article",
        "key": "10.1109/TSE.2024.3380194",
        "author": "Ding, Zhijun and Xu, Yuehao and Feng, Binbin and Jiang, Changjun",
        "title": "Microservice Extraction Based on a Comprehensive Evaluation of Logical Independence and Performance",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3380194",
        "doi": "10.1109/TSE.2024.3380194",
        "abstract": "Monolithic architectures are becoming increasingly difficult to cope with complex applications, and microservice architectures, which offer flexibility and logical independence in development and maintenance, are the new choice for companies and developers. Migrating a legacy monolithic architecture application to a microservice architecture rather than building it from scratch is considered an easy way to use it. To ensure that the migrated microservice applications can take advantage of their benefits, we need to propose a reasonable and effective microservice extraction method. Considering the single responsibility principle in the microservice design principle, most existing microservice extraction methods only pursue the high logical independence of the extraction results and pay little attention to whether the extraction results have good performance. Applications need to perform well, and studies have shown that poor microservice extraction schemes can negatively impact the performance of the migrated application. As a result, when extracting, we should also consider the performance of the results. A few studies consider the performance of extraction results, but only in terms of a few factors affecting performance, such as network overhead, rather than considering all factors affecting performance comprehensively, which leads to an inaccurate evaluation of performance. Therefore, oriented toward the most widely used managed languages today, we propose an effective Microservice Extraction method based on a Comprehensive Evaluation of logical independence and performance (MECE). Firstly, we propose a workflow-based approach to evaluate the performance of microservice extraction results by considering multiple influencing factors, focusing on the management cost ignored in existing studies, and designing an effective management cost evaluation model. After that, we propose a meta-heuristic search-based algorithm to obtain feasible microservice extraction results. In experiments based on actual deployments, the extraction results of the MECE method obtained a performance improvement of up to 46.15\\% without significant loss of logical independence compared to existing methods, which verifies the effectiveness of the method.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1244\u20131263",
        "numpages": "20"
    },
    "Shaken, Not Stirred: How Developers Like Their Amplified Tests": {
        "type": "article",
        "key": "10.1109/TSE.2024.3381015",
        "author": "Brandt, Carolin and Khatami, Ali and Wessel, Mairieli and Zaidman, Andy",
        "title": "Shaken, Not Stirred: How Developers Like Their Amplified Tests",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3381015",
        "doi": "10.1109/TSE.2024.3381015",
        "abstract": "Test amplification makes systematic changes to existing, manually written tests to provide tests complementary to an automated test suite. We consider developer-centric test amplification, where the developer explores, judges and edits the amplified tests before adding them to their maintained test suite. However, it is as yet unclear which kind of selection and editing steps developers take before including an amplified test into the test suite. In this paper we conduct an open source contribution study, amplifying tests of open source Java projects from GitHub. We report which deficiencies we observe in the amplified tests while manually filtering and editing them to open 39 pull requests with amplified tests. We present a detailed analysis of the maintainer's feedback regarding proposed changes, requested information, and expressed judgment. Our observations provide a basis for practitioners to take an informed decision on whether to adopt developer-centric test amplification. As several of the edits we observe are based on the developer's understanding of the amplified test, we conjecture that developer-centric test amplification should invest in supporting the developer to understand the amplified tests.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1264\u20131280",
        "numpages": "17"
    },
    "MASTER: Multi-Source Transfer Weighted Ensemble Learning for Multiple Sources Cross-Project Defect Prediction": {
        "type": "article",
        "key": "10.1109/TSE.2024.3381235",
        "author": "Tong, Haonan and Zhang, Dalin and Liu, Jiqiang and Xing, Weiwei and Lu, Lingyun and Lu, Wei and Wu, Yumei",
        "title": "MASTER: Multi-Source Transfer Weighted Ensemble Learning for Multiple Sources Cross-Project Defect Prediction",
        "year": "2024",
        "issue_date": "May 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3381235",
        "doi": "10.1109/TSE.2024.3381235",
        "abstract": "Multi-source cross-project defect prediction (MSCPDP) attempts to transfer defect knowledge learned from multiple source projects to the target project. MSCPDP has drawn increasing attention from academic and industry communities owing to its advantages compared with single-source cross-project defect prediction (SSCPDP). However, two main problems, which are how to effectively extract the transferable knowledge from each source dataset and how to measure the amount of knowledge transferred from each source dataset to the target dataset, seriously restrict the performance of existing MSCPDP models. In this paper, we propose a novel &lt;bold&gt;m&lt;/bold&gt;ulti-source tr&lt;bold&gt;a&lt;/bold&gt;n&lt;bold&gt;s&lt;/bold&gt;fer weigh&lt;bold&gt;t&lt;/bold&gt;ed &lt;bold&gt;e&lt;/bold&gt;nsemble lea&lt;bold&gt;r&lt;/bold&gt;ning (MASTER) method for MSCPDP. MASTER measures the weight of each source dataset based on feature importance and distribution difference and then extracts the transferable knowledge based on the proposed feature-weighted transfer learning algorithm. Experiments are performed on 30 software projects. We compare MASTER with the latest state-of-the-art MSCPDP methods with statistical test in terms of famous effort-unaware measures (i.e., PD, PF, AUC, and MCC) and two widely used effort-aware measures (&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$P_{opt}20\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mn&gt;20&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"tong-ieq1-3381235.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; and IFA). The experiment results show that: 1) MASTER can substantially improve the prediction performance compared with the baselines, e.g., an improvement of at least 49.1\\% in MCC, 48.1\\% in IFA; 2) MASTER significantly outperforms each baseline on most datasets in terms of AUC, MCC, &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$P_{opt}20\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:msub&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mn&gt;20&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"tong-ieq2-3381235.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; and IFA; 3) MSCPDP model significantly performs better than the mean case of SSCPDP model on most datasets and even outperforms the best case of SSCPDP on some datasets. It can be concluded that 1) it is very necessary to conduct MSCPDP, and 2) the proposed MASTER is a more promising alternative for MSCPDP.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1281\u20131305",
        "numpages": "25"
    },
    "Behind the Intent of Extract Method Refactoring: A Systematic Literature Review": {
        "type": "article",
        "key": "10.1109/TSE.2023.3345800",
        "author": "AlOmar, Eman Abdullah and Mkaouer, Mohamed Wiem and Ouni, Ali",
        "title": "Behind the Intent of Extract Method Refactoring: A Systematic Literature Review",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3345800",
        "doi": "10.1109/TSE.2023.3345800",
        "abstract": "&lt;bold&gt;Background:&lt;/bold&gt; Code refactoring is widely recognized as an essential software engineering practice to improve the understandability and maintainability of the source code. The &lt;italic&gt;Extract Method&lt;/italic&gt; refactoring is considered as \u201cSwiss army knife\u201d of refactorings, as developers often apply it to improve their code quality, &lt;italic&gt;e.g.,&lt;/italic&gt; decompose long code fragments, reduce code complexity, eliminate duplicated code, etc. In recent years, several studies attempted to recommend &lt;italic&gt;Extract Method&lt;/italic&gt; refactorings allowing the collection, analysis, and revelation of actionable data-driven insights about refactoring practices within software projects. &lt;bold&gt;Aim:&lt;/bold&gt; In this paper, we aim at reviewing the current body of knowledge on existing Extract Method refactoring research and explore their limitations and potential improvement opportunities for future research efforts. That is, &lt;italic&gt;Extract Method&lt;/italic&gt; is considered one of the most widely-used refactorings, but difficult to apply in practice as it involves low-level code changes such as statements, variables, parameters, return types, etc. Hence, researchers and practitioners begin to be aware of the state-of-the-art and identify new research opportunities in this context. &lt;bold&gt;Method:&lt;/bold&gt; We review the body of knowledge related to &lt;italic&gt;Extract Method&lt;/italic&gt; refactoring in the form of a systematic literature review (SLR). After compiling an initial pool of 1,367 papers, we conducted a systematic selection and our final pool included 83 primary studies. We define three sets of research questions and systematically develop and refine a classification schema based on several criteria including their methodology, applicability, and degree of automation. &lt;bold&gt;Results:&lt;/bold&gt; The results construct a catalog of 83 &lt;italic&gt;Extract Method&lt;/italic&gt; approaches indicating that several techniques have been proposed in the literature. Our results show that: (i) 38.6\\% of &lt;italic&gt;Extract Method&lt;/italic&gt; refactoring studies primarily focus on addressing code clones; (ii) Several of the &lt;italic&gt;Extract Method&lt;/italic&gt; tools incorporate the developer's involvement in the decision-making process when applying the method extraction, and (iii) the existing benchmarks are heterogeneous and do not contain the same type of information, making standardizing them for the purpose of benchmarking difficult. &lt;bold&gt;Conclusions:&lt;/bold&gt; Our study serves as an \u201cindex\u201d to the body of knowledge in this area for researchers and practitioners in determining the &lt;italic&gt;Extract Method&lt;/italic&gt; refactoring approach that is most appropriate for their needs. Our findings also empower the community with information to guide the future development of refactoring tools.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "668\u2013694",
        "numpages": "27"
    },
    "Automated Smell Detection and Recommendation in Natural Language Requirements": {
        "type": "article",
        "key": "10.1109/TSE.2024.3361033",
        "author": "Veizaga, Alvaro and Shin, Seung Yeob and Briand, Lionel C.",
        "title": "Automated Smell Detection and Recommendation in Natural Language Requirements",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3361033",
        "doi": "10.1109/TSE.2024.3361033",
        "abstract": "Requirement specifications are typically written in natural language (NL) due to its usability across multiple domains and understandability by all stakeholders. However, unstructured NL is prone to quality problems (e.g., ambiguity) when writing requirements, which can result in project failures. To address this issue, we present a tool, named Paska, that takes as input any NL requirements, automatically detects quality problems as smells in the requirements, and offers recommendations to improve their quality. Our approach relies on natural language processing (NLP) techniques and a state-of-the-art controlled natural language (CNL) for requirements (Rimay), to detect smells and suggest recommendations using patterns defined in Rimay to improve requirement quality. We evaluated Paska through an industrial case study in the financial domain involving 13 systems and 2725 annotated requirements. The results show that our tool is accurate in detecting smells (89\\% precision and recall) and suggesting appropriate Rimay pattern recommendations (96\\% precision and 94\\% recall).",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "695\u2013720",
        "numpages": "26"
    },
    "Stealthy Backdoor Attack for Code Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3361661",
        "author": "Yang, Zhou and Xu, Bowen and Zhang, Jie M. and Kang, Hong Jin and Shi, Jieke and He, Junda and Lo, David",
        "title": "Stealthy Backdoor Attack for Code Models",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3361661",
        "doi": "10.1109/TSE.2024.3361661",
        "abstract": "Code models, such as CodeBERT and CodeT5, offer general-purpose representations of code and play a vital role in supporting downstream automated software engineering tasks. Most recently, code models were revealed to be vulnerable to backdoor attacks. A code model that is backdoor-attacked can behave normally on clean examples but will produce pre-defined malicious outputs on examples injected with &lt;italic&gt;triggers&lt;/italic&gt; that activate the backdoors. Existing backdoor attacks on code models use unstealthy and easy-to-detect triggers. This paper aims to investigate the vulnerability of code models with &lt;italic&gt;stealthy&lt;/italic&gt; backdoor attacks. To this end, we propose &lt;sc&gt;Afraidoor&lt;/sc&gt; (&lt;italic&gt;A&lt;/italic&gt;dversarial &lt;italic&gt;F&lt;/italic&gt;eatu&lt;italic&gt;r&lt;/italic&gt;e as &lt;italic&gt;A&lt;/italic&gt;dapt&lt;italic&gt;i&lt;/italic&gt;ve Back&lt;italic&gt;door&lt;/italic&gt;). &lt;sc&gt;Afraidoor&lt;/sc&gt; achieves stealthiness by leveraging adversarial perturbations to inject adaptive triggers into different inputs. We apply &lt;sc&gt;Afraidoor&lt;/sc&gt; to three widely adopted code models (CodeBERT, PLBART, and CodeT5) and two downstream tasks (code summarization and method name prediction). We evaluate three widely used defense methods and find that &lt;sc&gt;Afraidoor&lt;/sc&gt; is more unlikely to be detected by the defense methods than by baseline methods. More specifically, when using spectral signature as defense, around 85\\% of adaptive triggers in &lt;sc&gt;Afraidoor&lt;/sc&gt; bypass the detection in the defense process. By contrast, only less than 12\\% of the triggers from previous work bypass the defense. When the defense method is not applied, both &lt;sc&gt;Afraidoor&lt;/sc&gt; and baselines have almost perfect attack success rates. However, once a defense is applied, the attack success rates of baselines decrease dramatically, while the success rate of &lt;sc&gt;Afraidoor&lt;/sc&gt; remains high. Our finding exposes security weaknesses in code models under stealthy backdoor attacks and shows that state-of-the-art defense methods cannot provide sufficient protection. We call for more research efforts in understanding security threats to code models and developing more effective countermeasures.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "721\u2013741",
        "numpages": "21"
    },
    "Measuring and Characterizing (Mis)compliance of the Android Permission System": {
        "type": "article",
        "key": "10.1109/TSE.2024.3362921",
        "author": "Barzolevskaia, Anna and Branca, Enrico and Stakhanova, Natalia",
        "title": "Measuring and Characterizing (Mis)compliance of the Android Permission System",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3362921",
        "doi": "10.1109/TSE.2024.3362921",
        "abstract": "Within the Android mobile operating system, Android permissions act as a system of safeguards designed to restrict access to potentially sensitive data and privileged components. Multiple research studies indicate flaws and limitations of the Android permission system, prompting Google to implement a more regulated and fine-grained permission model. This newly-introduced complexity creates confusion for developers leading to incorrect permissions and a significant risk to users security and privacy. We present a systematic study of theoretical and practical misuse of permissions. For this analysis we derive the unified permissions and call mappings that represent theoretical requirements of permissions and calls. We develop PChecker, an approach that identifies the discrepancies between the official Android permissions documentation and permission implementation in the Android platform source code based on these mappings. We evaluate four versions of the Android Open Source Project code (major versions 10\u201313) and shed light on the prevalence of discrepancies between the official Android guidelines for permissions and their implementation in the Android platform source code. We further show that these discrepancies result in miscompliance in third-party Android apps.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "742\u2013764",
        "numpages": "23"
    },
    "DynAMICS: A Tool-Based Method for the Specification and Dynamic Detection of Android Behavioral Code Smells": {
        "type": "article",
        "key": "10.1109/TSE.2024.3363223",
        "author": "Prestat, Dimitri and Moha, Naouel and Villemaire, Roger and Avellaneda, Florent",
        "title": "DynAMICS: A Tool-Based Method for the Specification and Dynamic Detection of Android Behavioral Code Smells",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3363223",
        "doi": "10.1109/TSE.2024.3363223",
        "abstract": "Code smells are the result of poor design choices within software systems that complexify source code and impede evolution and performance. Therefore, detecting code smells within software systems is an important priority to decrease technical debt. Furthermore, the emergence of mobile applications (apps) has brought new types of Android-specific code smells, which relate to limitations and constraints on resources like memory, performance and energy consumption. Among these Android-specific smells are those that describe inappropriate behaviour during the execution that may negatively impact software quality. Static analysis tools, however, show limitations for detecting these behavioural code smells and properly detecting behavioural code smells requires considering the dynamic behaviour of the apps. To dynamically detect behavioural code smells, we hence propose three contributions: (1) A method, the &lt;sc&gt;Dynamics&lt;/sc&gt; method, a step-by-step method for the specification and dynamic detection of Android behavioural code smells; (2) A tool, the &lt;sc&gt;Dynamics&lt;/sc&gt; tool, implementing this method on seven code smells; and (3) A validation of our approach on 538 apps from &lt;sc&gt;F-Droid&lt;/sc&gt; with a comparison with the static analysis detection tools, &lt;sc&gt;aDoctor&lt;/sc&gt; and &lt;sc&gt;Paprika&lt;/sc&gt;, from the literature. Our method consists of four steps: (1) the specification of the code smells, (2) the instrumentation of the app, (3) the execution of the apps, and (4) the detection of the behavioural code smells. Our results show that many instances of code smells that cannot be detected with static detection tools are indeed detected with our dynamic approach with an average precision of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$92.8\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;92.8&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"prestat-ieq1-3363223.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; and an average recall of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$53.4\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;53.4&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"prestat-ieq2-3363223.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "765\u2013784",
        "numpages": "20"
    },
    "A Systematic Review of IoT Systems Testing: Objectives, Approaches, Tools, and Challenges": {
        "type": "article",
        "key": "10.1109/TSE.2024.3363611",
        "author": "Minani, Jean Baptiste and Sabir, Fatima and Moha, Naouel and Gu\\'{e}h\\'{e}neuc, Yann-Ga\\\"{e}l",
        "title": "A Systematic Review of IoT Systems Testing: Objectives, Approaches, Tools, and Challenges",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3363611",
        "doi": "10.1109/TSE.2024.3363611",
        "abstract": "Internet of Things (IoT) systems are becoming prevalent in various domains, from healthcare to smart homes. Testing IoT systems is critical in ensuring their reliability. Previous papers studied separately the objectives, approaches, tools, and challenges of IoT systems testing. However, despite the rapid evolution of the IoT domain, no review has been undertaken to investigate all four aspects collectively. This paper presents a systematic literature review that aggregates, synthesizes, and discusses the results of 83 primary studies (PSs) concerning IoT testing objectives, approaches, tools, and challenges. We followed the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) protocol to report our findings and answer research questions (RQs). To select PSs, we applied inclusion and exclusion criteria to relevant studies published between 2012 and 2022. We extracted and analyzed the data from PSs to understand IoT systems testing. The results reveal that IoT systems testing embraces traditional software quality attributes but also introduces new ones like connectivity, energy efficiency, device lifespan, distributivity, and dynamicity. They also show that existing IoT systems testing approaches are limited to specific aspects and should be expanded for more comprehensive testing. They also show 19 testing tools and 15 testbeds for testing IoT systems with their limitations, necessitating the development or enhancement for wider coverage. The large number of heterogeneous devices generating data in different formats, along with the need for testing in real-world scenarios, poses a challenge. Thus, our study offers insights into the testing objectives, approaches, tools, and challenges associated with IoT systems. Based on the results, we also provide practical guidance for IoT practitioners by cataloging existing tools and approaches, while also identifying new research opportunities for interested researchers.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "785\u2013815",
        "numpages": "31"
    },
    "Automatic Commit Message Generation: A Critical Review and Directions for Future Work": {
        "type": "article",
        "key": "10.1109/TSE.2024.3364675",
        "author": "Zhang, Yuxia and Qiu, Zhiqing and Stol, Klaas-Jan and Zhu, Wenhui and Zhu, Jiaxin and Tian, Yingchen and Liu, Hui",
        "title": "Automatic Commit Message Generation: A Critical Review and Directions for Future Work",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3364675",
        "doi": "10.1109/TSE.2024.3364675",
        "abstract": "Commit messages are critical for code comprehension and software maintenance. Writing a high-quality message requires skill and effort. To support developers and reduce their effort on this task, several approaches have been proposed to automatically generate commit messages. Despite the promising performance reported, we have identified three significant and prevalent threats in these automated approaches: 1) the datasets used to train and evaluate these approaches contain a considerable amount of \u2018noise\u2019; 2) current approaches only consider commits of a limited diff size; and 3) current approaches can only generate the subject of a commit message, not the message body. The first limitation may let the models \u2018learn\u2019 inappropriate messages in the training stage, and also lead to inflated performance results in their evaluation. The other two threats can considerably weaken the practical usability of these approaches. Further, with the rapid emergence of large language models (LLMs) that show superior performance in many software engineering tasks, it is worth asking: can LLMs address the challenge of long diffs and whole message generation? This article first reports the results of an empirical study to assess the impact of these three threats on the performance of the state-of-the-art auto generators of commit messages. We collected commit data of the Top 1,000 most-starred Java projects in GitHub and systematically removed noisy commits with bot-submitted and meaningless messages. We then compared the performance of four approaches representative of the state-of-the-art before and after the removal of noisy messages, or with different lengths of commit diffs. We also conducted a qualitative survey with developers to investigate their perspectives on simply generating message subjects. Finally, we evaluate the performance of two representative LLMs, namely UniXcoder and ChatGPT, in generating more practical commit messages. The results demonstrate that generating commit messages is of great practical value, considerable work is needed to mature the current state-of-the-art, and LLMs can be an avenue worth trying to address the current limitations. Our analyses provide insights for future work to achieve better performance in practice.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "816\u2013835",
        "numpages": "20"
    },
    "Guess the State: Exploiting Determinism to Improve GUI Exploration Efficiency": {
        "type": "article",
        "key": "10.1109/TSE.2024.3366586",
        "author": "Clerissi, Diego and Denaro, Giovanni and Mobilio, Marco and Mariani, Leonardo",
        "title": "Guess the State: Exploiting Determinism to Improve GUI Exploration Efficiency",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3366586",
        "doi": "10.1109/TSE.2024.3366586",
        "abstract": "Many automatic Web testing techniques generate test cases by analyzing the GUI of the Web applications under test, aiming to exercise sequences of actions that are similar to the ones that testers could manually execute. However, the efficiency of the test generation process is severely limited by the cost of analyzing the content of the GUI screens after executing each action. In this paper, we introduce an inference component, &lt;sc&gt;Sibilla&lt;/sc&gt;, which accumulates knowledge about the behavior of the GUI after each action. &lt;sc&gt;Sibilla&lt;/sc&gt; enables the test generators to &lt;italic&gt;reuse&lt;/italic&gt; the results computed for GUI screens that recur multiple times during the test generation process, thus improving the efficiency of Web testing techniques. We experimented &lt;sc&gt;Sibilla&lt;/sc&gt; with Web testing techniques based on three different GUI exploration strategies (Random, Depth-first, and Q-learning) and nine target systems, observing reductions from 22\\% to 96\\% of the test generation time.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "836\u2013853",
        "numpages": "18"
    },
    "Coverage Goal Selector for Combining Multiple Criteria in Search-Based Unit Test Generation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3366613",
        "author": "Zhou, Zhichao and Zhou, Yuming and Fang, Chunrong and Chen, Zhenyu and Luo, Xiapu and He, Jingzhu and Tang, Yutian",
        "title": "Coverage Goal Selector for Combining Multiple Criteria in Search-Based Unit Test Generation",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3366613",
        "doi": "10.1109/TSE.2024.3366613",
        "abstract": "Unit testing is critical to the software development process, ensuring the correctness of basic programming units in a program (e.g., a method). Search-based software testing (SBST) is an automated approach to generating test cases. SBST generates test cases with genetic algorithms by specifying the coverage criterion (e.g., branch coverage). However, a good test suite must have different properties, which cannot be captured using an individual coverage criterion. Therefore, the state-of-the-art approach combines multiple criteria to generate test cases. Since combining multiple coverage criteria brings multiple objectives for optimization, it hurts the test suites\u2019 coverage for certain criteria compared with using the single criterion. To cope with this problem, we propose a novel approach named &lt;bold&gt;smart selection&lt;/bold&gt;. Based on the coverage correlations among criteria and the subsumption relationships among coverage goals, smart selection selects a subset of coverage goals to reduce the number of optimization objectives and avoid missing any properties of all criteria. We conduct experiments to evaluate smart selection on &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$400$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;400&lt;/mml:mn&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq1-3366613.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; Java classes with three state-of-the-art genetic algorithms under the &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$2$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq2-3366613.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-minute budget. On average, smart selection outperforms combining all goals on &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$65.1\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;65.1&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq3-3366613.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; of the classes having significant differences between the two approaches. Secondly, we conduct experiments to verify our assumptions about coverage criteria relationships. Furthermore, we assess the coverage performance of smart selection under varying budgets of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$5$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq4-3366613.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$8$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;8&lt;/mml:mn&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq5-3366613.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, and &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$10$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq6-3366613.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; minutes and explore its effect on bug detection, confirming the advantage of smart selection over combining all goals.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "854\u2013883",
        "numpages": "30"
    },
    "Factoring Expertise, Workload, and Turnover Into Code Review Recommendation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3366753",
        "author": "Hajari, Fahimeh and Malmir, Samaneh and Mirsaeedi, Ehsan and Rigby, Peter C.",
        "title": "Factoring Expertise, Workload, and Turnover Into Code Review Recommendation",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3366753",
        "doi": "10.1109/TSE.2024.3366753",
        "abstract": "Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects. Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers. In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover while more evenly distributing review workload. We conduct historical analyses to understand the natural concentration of review workload and the degree of knowledge spreading that is inherent in code review. Even though review workload is highly concentrated, we show that code review natural spreads knowledge thereby reducing the files at risk to turnover. Using simulation, we evaluate existing code review recommenders and develop novel recommenders to understand their impact on the level of expertise during review, the workload of reviewers, and the files at risk to turnover. Our simulations use seeded random replacement of reviewers to allow us to compare the reviewer recommenders without the confounding variation of different reviewers being replaced for each recommender. We find that prior work that assigns reviewers based on file ownership concentrates knowledge on a small group of core developers increasing the risk of knowledge loss from turnover. Recent work, WhoDo, that considers developer workload, assigns developers that are not sufficiently committed to the project and we see an increase in files at risk to turnover. We propose learning and retention aware review recommenders that when combined are effective at reducing the risk of turnover, but they unacceptably reduce the overall expertise during reviews. Combining recommenders, we develop the &lt;italic&gt;SofiaWL&lt;/italic&gt; recommender that suggests experts with low active review workload when none of the files under review are known by only one developer. In contrast, when knowledge is concentrated on one developer, it sends the review to other reviewers to spread knowledge. For the projects we study, we are able to globally increase expertise during reviews, &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$+3$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"rigby-ieq1-3366753.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;\\%, reduce workload concentration, &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$-12$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u2212&lt;/mml:mo&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"rigby-ieq2-3366753.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;\\%, and reduce the files at risk, &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$-28$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u2212&lt;/mml:mo&gt;&lt;mml:mn&gt;28&lt;/mml:mn&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"rigby-ieq3-3366753.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;\\%. We make our scripts and data available in our replication package &lt;xref ref-type=\"bibr\" rid=\"ref1\"&gt;[1]&lt;/xref&gt;. Developers can optimize for a particular outcome measure based on the needs of their project, or use our GitHub bot to automatically balance the outcomes &lt;xref ref-type=\"bibr\" rid=\"ref2\"&gt;[2]&lt;/xref&gt;.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "884\u2013899",
        "numpages": "16"
    },
    "Mask\u2013Mediator\u2013Wrapper Architecture as a Data Mesh Driver": {
        "type": "article",
        "key": "10.1109/TSE.2024.3367126",
        "author": "Don\\v{c}evi\\'{c}, Juraj and Fertalj, Kre\\v{s}imir and Brcic, Mario and Kova\\v{c}, Mihael",
        "title": "Mask\u2013Mediator\u2013Wrapper Architecture as a Data Mesh Driver",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3367126",
        "doi": "10.1109/TSE.2024.3367126",
        "abstract": "The data mesh is a novel data management concept that emphasizes the importance of a domain before technology. The concept is still in the early stages of development and many efforts to implement and use it are expected to have negative consequences for organizations due to a lack of technological guidelines and best practices. To mitigate the risk of negative outcomes this paper proposes the use of the mask\u2013mediator\u2013wrapper architecture as a driver for a data mesh implementation. The mask\u2013mediator\u2013wrapper architecture provides a set of prefabricated configurable components that provide basic functionalities that a data mesh requires. This paper shows how the two concepts are compatible in terms of functionality, data modeling, evolvability, and aligned capabilities. A mask\u2013mediator\u2013wrapper-driven data mesh facilitates low-risk adoption trials, rapid prototyping, standardization, and a guarantee of evolvability. We demonstrate a mask\u2013mediator\u2013wrapper-driven data mesh by using our open-source Janus system to experimentally drive an exemplified data mesh.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "900\u2013910",
        "numpages": "11"
    },
    "Software Testing With Large Language Models: Survey, Landscape, and Vision": {
        "type": "article",
        "key": "10.1109/TSE.2024.3368208",
        "author": "Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing",
        "title": "Software Testing With Large Language Models: Survey, Landscape, and Vision",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3368208",
        "doi": "10.1109/TSE.2024.3368208",
        "abstract": "Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "911\u2013936",
        "numpages": "26"
    },
    "A Testing Program and Pragma Combination Selection Based Framework for High-Level Synthesis Tool Pragma-Related Bug Detection": {
        "type": "article",
        "key": "10.1109/TSE.2024.3368553",
        "author": "Jiang, He and Wang, Zun and Zhou, Zhide and Li, Xiaochen and Guo, Shikai and Sun, Weifeng and Zhang, Tao",
        "title": "A Testing Program and Pragma Combination Selection Based Framework for High-Level Synthesis Tool Pragma-Related Bug Detection",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3368553",
        "doi": "10.1109/TSE.2024.3368553",
        "abstract": "High-Level Synthesis (HLS) tools convert C/C++ design code into Hardware Description Language (HDL) code automatically, which are often used for Field Programmable Gate Array (FPGA) design. HLS tools provide many pragmas, which are a kind of directive to be inserted into C/C++ code, for designers to efficiently control the synthesis of code components (e.g., arrays and loops) to generate FPGA implementations with varying performances and costs. However, the use of some pragmas may trigger HLS tool bugs (e.g., tool crashes). Although many formal methods have been proposed to verify the correctness of various HLS phases, no relevant work addresses the problem on detecting HLS tool pragma-related bugs. To resolve this problem, two challenges need to be addressed, namely the selection of testing programs and the acquisition of pragma combinations, due to the enormous number of testing programs and pragma combinations. In this paper, we propose TEPACS, a TEsting Program and prAgma Combination Selection-based framework, to construct diverse testing programs with pragmas for effectively detecting HLS tool pragma-related bugs. TEPACS follows the idea of fuzzing, which is a widely used technique in software testing. First, TEPACS selects the representative testing program according to the cosine distance between the code component vectors of testing programs. Then, for a selected program, TEPACS generates its golden output and uses the pragma combination selection method based on combinatorial testing to generate a set of programs with different pragmas. TEPACS uses the HLS tool under test to convert these testing programs into HDL codes and obtains the simulation results of the HDL code. Finally, based on differential testing, TEPACS identifies HLS tool bugs triggered if the simulation result and golden output are inconsistent. We evaluate TEPACS and its five variants on Vitis HLS, a widely used FPGA HLS tool. Experimental results show that TEPACS outperforms the baselines by at least 11.17\\% in terms of the bug-finding capability. In one month, TEPACS detected 34 bugs on the latest version of Vitis HLS, of which 9 bugs have been confirmed.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "937\u2013955",
        "numpages": "19"
    },
    "Automatic Debugging of Design Faults in MapReduce Applications": {
        "type": "article",
        "key": "10.1109/TSE.2024.3369766",
        "author": "Mor\\'{a}n, Jes\\'{u}s and Bertolino, Antonia and de la Riva, Claudio and Tuya, Javier",
        "title": "Automatic Debugging of Design Faults in MapReduce Applications",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3369766",
        "doi": "10.1109/TSE.2024.3369766",
        "abstract": "Among the current technologies to analyse large data, the MapReduce processing model stands out in Big Data. MapReduce is implemented in frameworks such as Hadoop, Spark or Flink that are able to manage the program executions according to the resources available at runtime. The developer should design the program in order to support all possible non-deterministic executions. However, the program may fail due to a design fault. Debugging these kinds of faults is difficult because the data are executed non-deterministically in parallel and the fault is not caused directly by the code, but by its design. This paper presents a framework called MRDebug which includes two debugging techniques focused on the MapReduce design faults. A spectrum-based fault localization technique locates the root cause of these faults analysing several executions of the test case, and a Delta Debugging technique isolates the data relevant to trigger the failure. An empirical evaluation with 13 programs shows that MRDebug is effective in debugging the faults, especially when the localization is done with the reduced data. In summary, MRDebug automatically provides valuable information to understand &lt;italic&gt;MapReduce&lt;/italic&gt; design faults as it helps locate their root cause and obtains a minimal data that triggers the failure.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "956\u2013978",
        "numpages": "23"
    },
    "An Empirical Study of JVMs\u2019 Behaviors on Erroneous JNI Interoperations": {
        "type": "article",
        "key": "10.1109/TSE.2024.3373239",
        "author": "Hwang, Sungjae and Lee, Sungho and Ryu, Sukyoung",
        "title": "An Empirical Study of JVMs\u2019 Behaviors on Erroneous JNI Interoperations",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3373239",
        "doi": "10.1109/TSE.2024.3373239",
        "abstract": "Java Native Interface (JNI) allows Java applications to access native libraries, but it is challenging to develop correct JNI programs. By leveraging native code, the JNI enables Java developers to implement efficient applications and reuse code written in other programming languages such as C and C++. The core Java libraries use the JNI to provide system features like graphical user interfaces, and mainstream Java Virtual Machines (JVMs) support the JNI. However, implementing correct JNI programs is not trivial due to the complex interoperation semantics between different programming languages. While JVMs do not validate JNI interoperations by default because of the performance overhead, they provide two methods. First, JVMs report the interoperation failures defined in the JNI specification at runtime. Second, they support a debug option, which validates JNI interoperations, degrading the runtime performance. To the best of our knowledge, literature has not thoroughly studied the quality of JVMs\u2019 methods, even though erroneous JNI interoperations may result in incorrect behaviors. In this paper, we empirically study the behaviors of JVMs on erroneous JNI interoperations. For a systematic study, we propose &lt;sc&gt;JUSTGen&lt;/sc&gt;, a semi-automatic tool that generates JNI test programs incurring erroneous interoperations from the JNI specification. &lt;sc&gt;JUSTGen&lt;/sc&gt; receives the JNI specification written in our domain-specific language (DSL) and automatically discovers cases that may lead to runtime errors on interoperations using an SMT solver. It then generates test programs that trigger the behaviors on the erroneous cases. Using the generated tests, we empirically evaluate JVM's failure handling mechanisms and the debug option capabilities on erroneous JNI interoperations. Our experiment results show that there exist erroneous cases in which JVMs do not handle failures or handle them differently from the specification. We also found that the JNI debug option does not validate thousands of erroneous cases, which can cause critical runtime errors such as memory corruption and violation of the Java type system. We reported 18 erroneous cases of which JVMs do not handle failures correctly to their respective vendors. Among them, 16 cases have been resolved.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "979\u2013994",
        "numpages": "16"
    },
    "Evaluation Framework for Autonomous Systems: The Case of Programmable Electronic Medical Systems": {
        "type": "article",
        "key": "10.1109/TSE.2024.3374382",
        "author": "Bombarda, Andrea and Bonfanti, Silvia and De Sanctis, Martina and Gargantini, Angelo and Pelliccione, Patrizio and Riccobene, Elvinia and Scandurra, Patrizia",
        "title": "Evaluation Framework for Autonomous Systems: The Case of Programmable Electronic Medical Systems",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3374382",
        "doi": "10.1109/TSE.2024.3374382",
        "abstract": "This paper proposes an evaluation framework for autonomous systems, called LENS. It is an instrument to make an assessment of a system through the lens of abilities related to adaptation and smartness. The assessment can then help engineers understand in which direction it is worth investing to make their system smarter. It also helps to identify possible improvement directions and to plan for concrete activities. Finally, it helps to make a re-assessment when the improvement has been performed in order to check whether the activity plan has been accomplished. Given the high variability in the various domains in which autonomous systems are and can be used, LENS is defined in abstract terms and instantiated to a specific and important class of medical devices, i.e., Programmable Electronic Medical Systems (PEMS). The instantiation, called LENS&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}_{textit{PEMS}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:msub&gt;&lt;mml:mrow/&gt;&lt;mml:mrow&gt;&lt;mml:mtext mathvariant=\"italic\"&gt;PEMS&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"desanctis-ieq1-3374382.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, is validated in terms of &lt;italic&gt;applicability&lt;/italic&gt;, i.e., how it is applicable to real PEMS, &lt;italic&gt;generalizability&lt;/italic&gt;, i.e., to what extent LENS&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${}_{textit{PEMS}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:msub&gt;&lt;mml:mrow/&gt;&lt;mml:mrow&gt;&lt;mml:mtext mathvariant=\"italic\"&gt;PEMS&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"desanctis-ieq2-3374382.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; is generalizable to the PEMS class of systems, and &lt;italic&gt;usefulness&lt;/italic&gt;, i.e., how it is useful in making an assessment and identifying possible directions of improvement towards smartness.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "995\u20131014",
        "numpages": "20"
    },
    "Distinguished Reviewers 2023": {
        "type": "article",
        "key": "10.1109/TSE.2024.3373234",
        "author": "Uchitel, Sebastian",
        "title": "Distinguished Reviewers 2023",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3373234",
        "doi": "10.1109/TSE.2024.3373234",
        "abstract": "Lists the reviewers who contributed to this publication in 2023.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "359",
        "numpages": "1"
    },
    "Meta-Path Based Attentional Graph Learning Model for Vulnerability Detection": {
        "type": "article",
        "key": "10.1109/TSE.2023.3340267",
        "author": "Wen, Xin-Cheng and Gao, Cuiyun and Ye, Jiaxin and Li, Yichen and Tian, Zhihong and Jia, Yan and Wang, Xuan",
        "title": "Meta-Path Based Attentional Graph Learning Model for Vulnerability Detection",
        "year": "2023",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3340267",
        "doi": "10.1109/TSE.2023.3340267",
        "abstract": "In recent years, deep learning (DL)-based methods have been widely used in code vulnerability detection. The DL-based methods typically extract structural information from source code, e.g., code structure graph, and adopt neural networks such as Graph Neural Networks (GNNs) to learn the graph representations. However, these methods fail to consider the heterogeneous relations in the code structure graph, i.e., the heterogeneous relations mean that the different types of edges connect different types of nodes in the graph, which may obstruct the graph representation learning. Besides, these methods are limited in capturing long-range dependencies due to the deep levels in the code structure graph. In this paper, we propose a &lt;bold&gt;M&lt;/bold&gt;eta-path based &lt;bold&gt;A&lt;/bold&gt;ttentional &lt;bold&gt;G&lt;/bold&gt;raph learning model for code vul&lt;bold&gt;NE&lt;/bold&gt;rability de&lt;bold&gt;T&lt;/bold&gt;ection, called &lt;bold&gt;MAGNET&lt;/bold&gt;. MAGNET constructs a multi-granularity meta-path graph for each code snippet, in which the heterogeneous relations are denoted as meta-paths to represent the structural information. A meta-path based hierarchical attentional graph neural network is also proposed to capture the relations between distant nodes in the graph. We evaluate MAGNET on three public datasets and the results show that MAGNET outperforms the best baseline method in terms of F1 score by 6.32\\%, 21.50\\%, and 25.40\\%, respectively. MAGNET also achieves the best performance among all the baseline methods in detecting Top-25 most dangerous Common Weakness Enumerations (CWEs), further demonstrating its effectiveness in vulnerability detection.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "360\u2013375",
        "numpages": "16"
    },
    "Safety and Performance, Why Not Both? Bi-Objective Optimized Model Compression Against Heterogeneous Attacks Toward AI Software Deployment": {
        "type": "article",
        "key": "10.1109/TSE.2023.3348515",
        "author": "Zhu, Jie and Wang, Leye and Han, Xiao and Liu, Anmin and Xie, Tao",
        "title": "Safety and Performance, Why Not Both? Bi-Objective Optimized Model Compression Against Heterogeneous Attacks Toward AI Software Deployment",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3348515",
        "doi": "10.1109/TSE.2023.3348515",
        "abstract": "The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, hindering the large-scale deployment on resource-restricted devices (&lt;italic&gt;e.g&lt;/italic&gt;., smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in a big model may be inherited by the compressed one. Such defects may be easily leveraged by adversaries, since a compressed model is usually deployed in a large number of devices without adequate protection. In this article, we aim to address the safe model compression problem from the perspective of safety-performance co-optimization. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called &lt;italic&gt;SafeCompress&lt;/italic&gt;. By simulating the attack mechanism as safety testing, SafeCompress can automatically compress a big model to a small one following the dynamic sparse training paradigm. Then, considering two kinds of representative and heterogeneous attack mechanisms, &lt;italic&gt;i.e&lt;/italic&gt;., black-box membership inference attack and white-box membership inference attack, we develop two concrete instances called BMIA-SafeCompress and WMIA-SafeCompress. Further, we implement another instance called MMIA-SafeCompress by extending SafeCompress to defend against the occasion when adversaries conduct black-box and white-box membership inference attacks simultaneously. We conduct extensive experiments on five datasets for both computer vision and natural language processing tasks. The results show the effectiveness and generalizability of our framework. We also discuss how to adapt SafeCompress to other attacks besides membership inference attack, demonstrating the flexibility of SafeCompress.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "376\u2013390",
        "numpages": "15"
    },
    "An Empirical Study on Correlations Between Deep Neural Network Fairness and Neuron Coverage Criteria": {
        "type": "article",
        "key": "10.1109/TSE.2023.3349001",
        "author": "Zheng, Wei and Lin, Lidan and Wu, Xiaoxue and Chen, Xiang",
        "title": "An Empirical Study on Correlations Between Deep Neural Network Fairness and Neuron Coverage Criteria",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3349001",
        "doi": "10.1109/TSE.2023.3349001",
        "abstract": "Recently, with the widespread use of deep neural networks (DNNs) in high-stakes decision-making systems (such as fraud detection and prison sentencing), concerns have arisen about the fairness of DNNs in terms of the potential negative impact they may have on individuals and society. Therefore, fairness testing has become an important research topic in DNN testing. At the same time, the neural network coverage criteria (such as criteria based on neuronal activation) is considered as an adequacy test for DNN white-box testing. It is implicitly assumed that improving the coverage can enhance the quality of test suites. Nevertheless, the correlation between DNN fairness (a test property) and coverage criteria (a test method) has not been adequately explored. To address this issue, we conducted a systematic empirical study on seven coverage criteria, six fairness metrics, three fairness testing techniques, and five bias mitigation methods on five DNN models and nine fairness datasets to assess the correlation between coverage criteria and DNN fairness. Our study achieved the following findings: 1) with the increase in the size of the test suite, some of the coverage and fairness metrics changed significantly, as the size of the test suite increased; 2) the statistical correlation between coverage criteria and DNN fairness is limited; and 3) after bias mitigation for improving the fairness of DNN, the change pattern in coverage criteria is different; 4) Models debiased by different bias mitigation methods have a lower correlation between coverage and fairness compared to the original models. Our findings cast doubt on the validity of coverage criteria concerning DNN fairness (i.e., increasing the coverage may even have a negative impact on the fairness of DNNs). Therefore, we warn DNN testers against blindly pursuing higher coverage of coverage criteria at the cost of test properties of DNNs (such as fairness).",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "391\u2013412",
        "numpages": "22"
    },
    "Test Input Prioritization for Machine Learning Classifiers": {
        "type": "article",
        "key": "10.1109/TSE.2024.3350019",
        "author": "Dang, Xueqi and Li, Yinghua and Papadakis, Mike and Klein, Jacques and Bissyand\\'{e}, Tegawend\\'{e} F. and Traon, Yves Le",
        "title": "Test Input Prioritization for Machine Learning Classifiers",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3350019",
        "doi": "10.1109/TSE.2024.3350019",
        "abstract": "Machine learning has achieved remarkable success across diverse domains. Nevertheless, concerns about interpretability in black-box models, especially within Deep Neural Networks (DNNs), have become pronounced in safety-critical fields like healthcare and finance. Classical machine learning (ML) classifiers, known for their higher interpretability, are preferred in these domains. Similar to DNNs, classical ML classifiers can exhibit bugs that could lead to severe consequences in practice. Test input prioritization has emerged as a promising approach to ensure the quality of an ML system, which prioritizes potentially misclassified tests so that such tests can be identified earlier with limited manual labeling costs. However, when applying to classical ML classifiers, existing DNN test prioritization methods are constrained from three perspectives: 1) Coverage-based methods are inefficient and time-consuming; 2) Mutation-based methods cannot be adapted to classical ML models due to mismatched model mutation rules; 3) Confidence-based methods are restricted to a single dimension when applying to binary ML classifiers, solely depending on the model's prediction probability for one class. To overcome the challenges, we propose MLPrior, a test prioritization approach specifically tailored for classical ML models. MLPrior leverages the characteristics of classical ML classifiers (i.e., interpretable models and carefully engineered attribute features) to prioritize test inputs. The foundational principles are: 1) tests more sensitive to mutations are more likely to be misclassified, and 2) tests closer to the model's decision boundary are more likely to be misclassified. Building on the first concept, we design mutation rules to generate two types of mutation features (i.e., &lt;bold&gt;model mutation features&lt;/bold&gt; and &lt;bold&gt;input mutation features&lt;/bold&gt;) for each test. Drawing from the second notion, MLPrior generates &lt;bold&gt;attribute features&lt;/bold&gt; of each test based on its attribute values, which can indirectly reveal the proximity between the test and the decision boundary. For each test, MLPrior combines all three types of features of it into a final vector. Subsequently, MLPrior employs a pre-trained ranking model to predict the misclassification probability of each test based on its final vector and ranks tests accordingly. We conducted an extensive study to evaluate MLPrior based on 185 subjects, encompassing natural datasets, mixed noisy datasets, and fairness datasets. The results demonstrate that MLPrior outperforms all the compared test prioritization approaches, with an average improvement of 14.74\\%&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$sim$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u223c&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"li-ieq1-3350019.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;66.93\\% on natural datasets, 18.55\\%&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$sim$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u223c&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"li-ieq2-3350019.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;67.73\\% on mixed noisy datasets, and 15.34\\%&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$sim$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u223c&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"li-ieq3-3350019.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;62.72\\% on fairness datasets.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "413\u2013442",
        "numpages": "30"
    },
    "Understanding Newcomers\u2019 Onboarding Process in Deep Learning Projects": {
        "type": "article",
        "key": "10.1109/TSE.2024.3353297",
        "author": "Han, Junxiao and Zhang, Jiahao and Lo, David and Xia, Xin and Deng, Shuiguang and Wu, Minghui",
        "title": "Understanding Newcomers\u2019 Onboarding Process in Deep Learning Projects",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3353297",
        "doi": "10.1109/TSE.2024.3353297",
        "abstract": "Attracting and retaining newcomers are critical for the sustainable development of Open Source Software (OSS) projects. Considerable efforts have been made to help newcomers identify and overcome barriers in the onboarding process. However, fewer studies focus on newcomers\u2019 activities before their successful onboarding. Given the rising popularity of deep learning (DL) techniques, we wonder what the onboarding process of DL newcomers is, and if there exist commonalities or differences in the onboarding process for DL and non-DL newcomers. Therefore, we reported a study to understand the growth trends of DL and non-DL newcomers, mine DL and non-DL newcomers\u2019 activities before their successful onboarding (i.e., past activities), and explore the relationships between newcomers\u2019 past activities and their first commit patterns and retention rates. By analyzing 20 DL projects with 9,191 contributors and 20 non-DL projects with 9,839 contributors, and conducting email surveys with contributors, we derived the following findings: 1) DL projects have attracted and retained more newcomers than non-DL projects. 2) Compared to non-DL newcomers, DL newcomers encounter more deployment, documentation, and version issues before their successful onboarding. 3) DL newcomers statistically require more time to successfully onboard compared to non-DL newcomers, and DL newcomers with more past activities (e.g., issues, issue comments, and watch) are prone to submit an intensive first commit (i.e., a commit with many source code and documentation files being modified). Based on the findings, we shed light on the onboarding process for DL and non-DL newcomers, highlight future research directions, and provide practical suggestions to newcomers, researchers, and projects.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "443\u2013460",
        "numpages": "18"
    },
    "Range Specification Bug Detection in Flight Control System Through Fuzzing": {
        "type": "article",
        "key": "10.1109/TSE.2024.3354739",
        "author": "Han, Ruidong and Ma, Siqi and Li, Juanru and Nepal, Surya and Lo, David and Ma, Zhuo and Ma, JianFeng",
        "title": "Range Specification Bug Detection in Flight Control System Through Fuzzing",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3354739",
        "doi": "10.1109/TSE.2024.3354739",
        "abstract": "Developers and manufacturers provide configurable control parameters for flight control programs to support various environments and missions, along with suggested ranges for these parameters to ensure flight safety. However, this flexible mechanism can also introduce a vulnerability known as range specification bugs. The vulnerability originates from the evidence that certain combinations of parameter values may affect the drone's physical stability even though its parameters are within the suggested range. The paper introduces a novel system called &lt;sc&gt;icsearcher&lt;/sc&gt;, designed to identify incorrect configurations or unreasonable combinations of parameters and suggest more reasonable ranges for these parameters. &lt;sc&gt;icsearcher&lt;/sc&gt; applies a metaheuristic search algorithm to find configurations with a high probability of driving the drone into unstable states. In particular, &lt;sc&gt;icsearcher&lt;/sc&gt; adopts a machine learning-based predictor to assist the searcher in evaluating the fitness of configuration. Finally, leveraging searched incorrect configurations, &lt;sc&gt;icsearcher&lt;/sc&gt; can summarize the feasible ranges through multi-objective optimization. &lt;sc&gt;icsearcher&lt;/sc&gt; applies a predictor to guide the search, which eliminates the need for realistic/simulation executions when evaluating configurations and further promotes search efficiency. We have carried out experimental evaluations of &lt;sc&gt;icsearcher&lt;/sc&gt; in different control programs. The evaluation results show that the system successfully reports potentially incorrect configurations, of which over &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$94\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;94&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"han-ieq1-3354739.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; leads to unstable states.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "461\u2013473",
        "numpages": "13"
    },
    "APPT: Boosting Automated Patch Correctness Prediction via Fine-Tuning Pre-Trained Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3354969",
        "author": "Zhang, Quanjun and Fang, Chunrong and Sun, Weisong and Liu, Yan and He, Tieke and Hao, Xiaodong and Chen, Zhenyu",
        "title": "APPT: Boosting Automated Patch Correctness Prediction via Fine-Tuning Pre-Trained Models",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3354969",
        "doi": "10.1109/TSE.2024.3354969",
        "abstract": "Automated program repair (APR) aims to fix software bugs automatically without human debugging efforts and plays a crucial role in software development and maintenance. Despite the recent significant progress in the number of fixed bugs, APR is still challenged by a long-standing overfitting problem (i.e., the generated patch is plausible but overfitting). Various techniques have thus been proposed to address the overfitting problem. Recently, researchers have employed BERT to extract code features, which are then used to train a classifier for patch correctness prediction, indicating the potential of such pre-trained models in reasoning about patch correctness. However, BERT is restricted to feature extraction for classifier training without benefiting from the training process, potentially generating sub-optimal vector representations for patched code snippets. In this paper, we propose APPT, a pre-trained model-based automated patch correctness assessment technique by both pre-training and fine-tuning. APPT adopts a pre-trained model as the encoder stack, followed by an LSTM stack and a deep learning classifier. More importantly, the pre-trained model is fine-tuned in conjunction with other components as a whole pipeline to fully adapt it specifically for reasoning about patch correctness. Although our idea is general and can be built on various existing pre-trained models, we have implemented APPT based on the BERT model. We conduct an extensive experiment on 1,183 Defects4J patches and the experimental results show that APPT achieves prediction accuracy of 79.7\\% and recall of 83.2\\%, outperforming the state-of-the-art technique CACHE by 4.3\\% and 6.7\\%. Our additional investigation on 49,694 real-world patches shows that APPT achieves the optimum performance (exceeding 99\\% in five common metrics for assessing patch classification techniques) compared with existing representation learning techniques. We further investigate the impact of each component and find that they all positively contribute to APPT, e.g., the fine-tuning process and the LSTM stack increase F1-score by 10.22\\% and 4.11\\%, respectively. We also prove that adopting advanced pre-trained models can further provide substantial advancement (e.g., GraphCodeBERT-based APPT improves BERT-based APPT by 2.8\\% and 3.3\\% in precision and AUC, respectively), highlighting the generalizability of APPT. Overall, our study highlights the promising future of fine-tuning pre-trained models to assess patch correctness and reduce the manual inspection effort of debugging experts when deploying APR tools in practice.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "474\u2013494",
        "numpages": "21"
    },
    "Improving Test Data Generation for MPI Program Path Coverage With FERPSO-IMPR and Surrogate-Assisted Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3354971",
        "author": "Wang, Yong and Cui, Wenzhong and Wang, Gai-Ge and Wang, Jian and Gong, Dunwei",
        "title": "Improving Test Data Generation for MPI Program Path Coverage With FERPSO-IMPR and Surrogate-Assisted Models",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3354971",
        "doi": "10.1109/TSE.2024.3354971",
        "abstract": "Message passing interface (MPI) is a powerful tool for parallel computing, originally designed for high-performance computing on massively parallel computers. In this paper, we combine FERPSO-IMPR (fitness Euclidean distance ratio particle swarm optimizer with information migration-based penalty and population reshaping) and surrogate-assisted models to generate test cases for MPI program path coverage testing. In our proposed method, FERPSO-IMPR employs a dual population strategy to initialize data and calculate fitness. Then, we create a sample set based on the initial data and its fitness. Subsequently, we train the master-slave surrogate models to predict individual fitness. Finally, a small number of elite individuals are selected to execute the program to decide whether to generate the required test data and guide the subsequent evolution process. We apply the proposed method to seven MPI programs and perform experimental comparisons from five directions. Experimental results show that compared with the comparative method, the time consumption of the proposed method is reduced by 33.2%, the number of evaluations is reduced by 38.8%, and the success rate is increased by 7.6%. These results prove that our method can effectively reduce the test data generation cost of MPI programs.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "495\u2013511",
        "numpages": "17"
    },
    "Multi-Language Software Development: Issues, Challenges, and Solutions": {
        "type": "article",
        "key": "10.1109/TSE.2024.3358258",
        "author": "Yang, Haoran and Nong, Yu and Wang, Shaowei and Cai, Haipeng",
        "title": "Multi-Language Software Development: Issues, Challenges, and Solutions",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3358258",
        "doi": "10.1109/TSE.2024.3358258",
        "abstract": "Developing software projects that incorporate multiple languages has been a prevalent practice for many years. However, the &lt;italic&gt;issues&lt;/italic&gt; encountered by developers during the development process, the underlying &lt;italic&gt;challenges&lt;/italic&gt; causing these issues, and the &lt;italic&gt;solutions&lt;/italic&gt; provided to developers remain unknown. In this paper, our objective is to provide answers to these questions by conducting a study on developer discussions on Stack Overflow (SO). Through a manual analysis of 586 highly relevant posts spanning 14 years, we revealed that multilingual development is a highly and sustainably active topic on SO, with older questions becoming inactive and newer ones getting first asked (and then mostly remaining active for more than one year). From these posts, we observed a diverse array of issues (11 categories), primarily centered around interfacing and data handling across different languages. Our analysis suggests that error/exception handling issues were the most difficult to resolve among those issue categories, while security related issues were most likely to receive an accepted answer. The primary challenge faced by developers was the complexity and diversity inherent in building multilingual code and ensuring interoperability. Additionally, developers often struggled due to a lack of technical expertise on the varied features of different programming languages (e.g., threading and memory management mechanisms). In addition, properly handling message passing across languages constituted a key challenge with using implicit language interfacing. Notably, Stack Overflow emerged as a crucial source of solutions to these challenges, with the majority (73\\%) of the posts receiving accepted answers, most within a week (36.5\\% within 24 hours and 25\\% in the following six days). Based on our analysis results, we have formulated actionable insights and recommendations that can be utilized by researchers and developers in this field.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "512\u2013533",
        "numpages": "22"
    },
    "Tracking the Evolution of Static Code Warnings: The State-of-the-Art and a Better Approach": {
        "type": "article",
        "key": "10.1109/TSE.2024.3358283",
        "author": "Li, Junjie and Yang, Jinqiu",
        "title": "Tracking the Evolution of Static Code Warnings: The State-of-the-Art and a Better Approach",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3358283",
        "doi": "10.1109/TSE.2024.3358283",
        "abstract": "Static bug detection tools help developers detect problems in the code, including bad programming practices and potential defects. Recent efforts to integrate static bug detectors in modern software development workflows, such as in code review and continuous integration, are shown to better motivate developers to fix the reported warnings on the fly. A proper mechanism to track the evolution of the reported warnings can better support such integration. Moreover, tracking the static code warnings will benefit many downstream software engineering tasks, such as learning the fix patterns for automated program repair, and learning which warnings are of more interest, so they can be prioritized automatically. In addition, the utilization of tracking tools enables developers to concentrate on the most recent and actionable static warnings rather than being overwhelmed by the thousands of warnings from the entire project. This, in turn, enhances the utilization of static analysis tools. Hence, precisely tracking the warnings by static bug detectors is critical to improving the utilization of static bug detectors further. In this paper, we study the effectiveness of the state-of-the-art (SOTA) solution in tracking static code warnings and propose a better solution based on our analysis of the insufficiency of the SOTA solution. In particular, we examined over 2,000 commits in four large-scale open-source systems (i.e., JClouds, Kafka, Spring-boot, and Guava) and crafted a dataset of 3,451 static code warnings by two static bug detectors (i.e., Spotbugs and PMD). We manually uncovered the ground-truth evolution status of the static warnings: persistent, removed&lt;sub&gt;fix&lt;/sub&gt;, removed&lt;sub&gt;non-fix&lt;/sub&gt; and newly-introduced. Upon manual analysis, we identified the main reasons behind the insufficiency of the SOTA solution. Furthermore, we propose StaticTracker to track static warnings over software development history. Our evaluation shows that StaticTracker significantly improves the tracking precision, i.e., from 64.4\\% to 90.3\\% for the evolution statuses combined (removed&lt;sub&gt;fix&lt;/sub&gt;, removed&lt;sub&gt;non-fix&lt;/sub&gt; and newly-introduced).",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "534\u2013550",
        "numpages": "17"
    },
    "Test Data Generation for Mutation Testing Based on Markov Chain Usage Model and Estimation of Distribution Algorithm": {
        "type": "article",
        "key": "10.1109/TSE.2024.3358297",
        "author": "Wei, Changqing and Yao, Xiangjuan and Gong, Dunwei and Liu, Huai",
        "title": "Test Data Generation for Mutation Testing Based on Markov Chain Usage Model and Estimation of Distribution Algorithm",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3358297",
        "doi": "10.1109/TSE.2024.3358297",
        "abstract": "Mutation testing, a mainstream fault-based software testing technique, can mimic a wide variety of software faults by seeding them into the target program and resulting in the so-called mutants. Test data generated in mutation testing should be able to kill as many mutants as possible, hence guaranteeing a high fault-detection effectiveness of testing. Nevertheless, the test data generation can be very expensive, because mutation testing normally involves an extremely large number of mutants and some mutants are hard to kill. It is thus a critical yet challenging job to find an efficient way to generate a small set of test data that are able to kill multiple mutants at the same time as well as reveal those hard-to-detect faults. In this paper, we propose a new approach for test data generation in mutation testing, through the novel applications of the Markov chain usage model and the estimation of distribution algorithm. We first utilize the Markov chain usage model to reduce the so-called mutant branches in weak mutation testing and generate a minimal set of extended paths. Then, we regard the problem of generating test data as the problem of covering extended paths and use an estimation of distribution algorithm based on probability model to solve the problem. Finally, we develop a framework, TAMMEA, to implement the new approach of generating test data for mutation testing. The empirical studies based on fifteen object programs show that TAMMEA can kill more mutants using fewer test data compared with baseline techniques. In addition, the computation overhead of TAMMEA is lower than that of the baseline technique based on the traditional genetic algorithm, and comparable to that of the random method. It is clear that the new approach improves both the effectiveness and efficiency of mutation testing, thus promoting its practicability.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "551\u2013573",
        "numpages": "23"
    },
    "Accelerating Finite State Machine-Based Testing Using Reinforcement Learning": {
        "type": "article",
        "key": "10.1109/TSE.2024.3358416",
        "author": "T\\\"{u}rker, Uraz Cengiz and Hierons, Robert M. and El-Fakih, Khaled and Mousavi, Mohammad Reza and Tyukin, Ivan Y.",
        "title": "Accelerating Finite State Machine-Based Testing Using Reinforcement Learning",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3358416",
        "doi": "10.1109/TSE.2024.3358416",
        "abstract": "Testing is a crucial phase in the development of complex systems, and this has led to interest in automated test generation techniques based on state-based models. Many approaches use models that are types of finite state machine (FSM). Corresponding test generation algorithms typically require that certain test components, such as reset sequences (RSs) and preset distinguishing sequences (PDSs), have been produced for the FSM specification. Unfortunately, the generation of RSs and PDSs is computationally expensive, and this affects the scalability of such FSM-based test generation algorithms. This paper addresses this scalability problem by introducing a reinforcement learning framework: the &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathcal{Q}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"script\"&gt;Q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"turker-ieq1-3358416.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-Graph framework for MBT. We show how this framework can be used in the generation of RSs and PDSs and consider both (potentially partial) timed and untimed models. The proposed approach was evaluated using three types of FSMs: randomly generated FSMs, FSMs from a benchmark, and an FSM of an Engine Status Manager for a printer. In experiments, the proposed approach was much faster and used much less memory than the state-of-the-art methods in computing PDSs and RSs.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "574\u2013597",
        "numpages": "24"
    },
    "Code Comment Inconsistency Detection Based on Confidence Learning": {
        "type": "article",
        "key": "10.1109/TSE.2024.3358489",
        "author": "Xu, Zhengkang and Guo, Shikai and Wang, Yumiao and Chen, Rong and Li, Hui and Li, Xiaochen and Jiang, He",
        "title": "Code Comment Inconsistency Detection Based on Confidence Learning",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3358489",
        "doi": "10.1109/TSE.2024.3358489",
        "abstract": "Code comments are a crucial source of software documentation that captures various aspects of the code. Such comments play a vital role in understanding the source code and facilitating communication between developers. However, with the iterative release of software, software projects become larger and more complex, leading to a corresponding increase in issues such as mismatched, incomplete, or outdated code comments. These inconsistencies in code comments can misguide developers and result in potential bugs, and there has been a steady rise in reports of such inconsistencies over time. Despite numerous methods being proposed for detecting code comment inconsistencies, their learning effect remains limited due to a lack of consideration for issues such as characterization noise and labeling errors in datasets. To overcome these limitations, we propose a novel approach called MCCL that first removes noise from the dataset and then detects inconsistent code comments in a timely manner, thereby enhancing the model's learning ability. Our proposed model facilitates better matching between code and comments, leading to improved development of software engineering projects. MCCL comprises two components, namely method comment detection and confidence learning denoising. The method comment detection component captures the intricate relationships between code and comments by learning their syntactic and semantic structures. It correlates the code and comments through an attention mechanism to identify how changes in the code affect the comments. Furthermore, confidence learning denoising component of MCCL identifies and removes characterization noises and labeling errors to enhance the quality of the datasets. This is achieved by implementing principles such as pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. By effectively eliminating noise from the dataset, our model is able to more accurately learn inconsistencies between comments and source code. Our experiments on 1,518 open-source projects demonstrate that MCCL can accurately detect inconsistencies, achieving an average &lt;italic&gt;F1-score&lt;/italic&gt; of 82.6\\%. This result outperforms state-of-the-art methods by 2.4\\% to 28.0\\%. Therefore, MCCL is more effective in identifying inconsistent comments based on code changes compared to existing approaches.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "598\u2013617",
        "numpages": "20"
    },
    "Accelerating Patch Validation for Program Repair With Interception-Based Execution Scheduling": {
        "type": "article",
        "key": "10.1109/TSE.2024.3359969",
        "author": "Xiao, Yuan-An and Yang, Chenyang and Wang, Bo and Xiong, Yingfei",
        "title": "Accelerating Patch Validation for Program Repair With Interception-Based Execution Scheduling",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3359969",
        "doi": "10.1109/TSE.2024.3359969",
        "abstract": "Long patch validation time is a limiting factor for automated program repair (APR). Though the duality between patch validation and mutation testing is recognized, so far there exists no study of systematically adapting mutation testing techniques to general-purpose patch validation. To address this gap, we investigate existing mutation testing techniques and identify five classes of acceleration techniques that are suitable for general-purpose patch validation. Among them, mutant schemata and mutant deduplication have not been adapted to general-purpose patch validation due to the arbitrary changes that third-party APR approaches may introduce. This presents two problems for adaption: 1) the difficulty of implementing the static equivalence analysis required by the state-of-the-art mutant deduplication approach; 2) the difficulty of capturing the changes of patches to the system state at runtime. To overcome these problems, we propose two novel approaches: 1) execution scheduling, which detects the equivalence between patches online, avoiding the static equivalence analysis and its imprecision; 2) interception-based instrumentation, which intercepts the changes of patches to the system state, avoiding a full interpreter and its overhead. Based on the contributions above, we implement ExpressAPR, a general-purpose patch validator for Java that integrates all recognized classes of techniques suitable for patch validation. Our large-scale evaluation with four APR approaches shows that ExpressAPR accelerates patch validation by 137.1x over plain validation or 8.8x over the state-of-the-art approach, making patch validation no longer the time bottleneck of APR. Patch validation time for a single bug can be reduced to within a few minutes on mainstream CPUs.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "618\u2013635",
        "numpages": "18"
    },
    "Neural Density Estimation of Response Times in Layered Software Systems": {
        "type": "article",
        "key": "10.1109/TSE.2024.3360093",
        "author": "Niu, Zifeng and Casale, Giuliano",
        "title": "Neural Density Estimation of Response Times in Layered Software Systems",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3360093",
        "doi": "10.1109/TSE.2024.3360093",
        "abstract": "Layered queueing networks (LQNs) are a class of performance models for software systems in which multiple distributed resources may be possessed simultaneously by a job. Estimating response times in a layered system is an essential but challenging analysis dimension in Quality of Service (QoS) assessment. Current analytic methods are capable of providing accurate estimates of mean response times. However, accurately approximating response time distributions used in service-level objective analysis is a demanding task. This paper proposes a novel hybrid framework that leverages phase-type (PH) distributions and neural networks to provide accurate density estimates of response times in layered queueing networks. The core step of this framework is to recursively obtain response time distributions in the submodels that are used to analyze the network by means of decomposition. We describe these response time distributions as a mixture of density functions for which we learn the parameters through a Mixture Density Network (MDN). The approach recursively propagates MDN predictions across software layers using PH distributions and performs repeated moment-matching based refitting to efficiently estimate end-to-end response time densities. Extensive numerical experiment results show that our scheme significantly improves density estimations compared to the state-of-the-art.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "636\u2013650",
        "numpages": "15"
    },
    "On the Usefulness of Automatically Generated Microservice Architectures": {
        "type": "article",
        "key": "10.1109/TSE.2024.3361209",
        "author": "Carvalho, Luiz and Colanzi, Thelma Elita and Assun\\c{c}\\~{a}o, Wesley K. G. and Garcia, Alessandro and Pereira, Juliana Alves and Kalinowski, Marcos and de Mello, Rafael Maiani and de Lima, Maria Julia and Lucena, Carlos",
        "title": "On the Usefulness of Automatically Generated Microservice Architectures",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3361209",
        "doi": "10.1109/TSE.2024.3361209",
        "abstract": "The modernization of monolithic legacy systems with microservices has been a trend in recent years. As part of this modernization, identifying microservice candidates starting from legacy code is challenging, as maintainers may consider many criteria simultaneously. Multi-objective search-based approaches represent a promising state-of-the-art solution to support this decision-making process. However, the rationale to adopt each microservice candidate automatically identified by these approaches is poorly investigated in industrial cases. Furthermore, studies with these approaches have not carefully investigated how maintainers reason and make decisions when designing microservice architectures from legacy systems. To address this gap, we conducted an on-site case study with maintainers of an industrial legacy system to investigate the usefulness of automatically generated microservice architectures. We analyze design decisions pointed out by the maintainers when reasoning about microservice candidates using several criteria at the same time. Our study is the first to assess a search-based approach involving actual maintainers conceiving microservice architectures in an industrial setting. Therefore, firstly, we considered individual evaluation of microservice candidates to understand the rationale for identifying a service. Secondly, we conducted a focus group study with maintainers with the goal of investigating design decisions at an architectural level. The results show that: &lt;italic&gt;(i)&lt;/italic&gt; the automated approach is able to identify useful microservices; &lt;italic&gt;(ii)&lt;/italic&gt; the criteria observed by previous studies are, in fact, considered by maintainers; and &lt;italic&gt;(iii)&lt;/italic&gt; the maintainer profiles, i.e., the preferred granularity for microservice, highly affect design decisions. Finally, we observed the maintainers needed little effort in adjusting the automatically identified microservices to make them adoptable. In addition to indicating a promising potential of search-based approaches to generate microservice architectures, our findings highlight the need for: &lt;italic&gt;(i)&lt;/italic&gt; interactive and/or customizable approaches that enable maintainers to include their preferences during the search process, and &lt;italic&gt;(ii)&lt;/italic&gt; flexible or automated selection of criteria that fits the scenario in which the modernization is taking place.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "651\u2013667",
        "numpages": "17"
    },
    "Properties and Styles of Software Technology Tutorials": {
        "type": "article",
        "key": "10.1109/TSE.2023.3332568",
        "author": "Arya, Deeksha M. and Guo, Jin L. C. and Robillard, Martin P.",
        "title": "Properties and Styles of Software Technology Tutorials",
        "year": "2023",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3332568",
        "doi": "10.1109/TSE.2023.3332568",
        "abstract": "A large number of tutorials for popular software development technologies are available online, and those about the same technology vary widely in their presentation. We studied the design of tutorials in the software documentation landscape for five popular programming languages: Java, C#, Python, Javascript, and Typescript. We investigated the extent to which tutorial pages, i.e. &lt;italic&gt;resources&lt;/italic&gt;, differ and report statistics of variations in resource properties. We developed a framework for characterizing resources based on their &lt;italic&gt;distinguishing attributes&lt;/italic&gt;, i.e. properties that vary widely for the resource, relative to other resources. Additionally, we propose that a resource can be represented by its &lt;italic&gt;resource style&lt;/italic&gt;, i.e. the combination of its distinguishing attributes. We discuss three techniques for characterizing resources based on our framework, to capture notable and relevant content and presentation properties of tutorial pages. We apply these techniques on a data set of 2551 resources to validate that our framework identifies valid and interpretable styles. We contribute this framework for reasoning about the design of resources in the online software documentation landscape.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "159\u2013172",
        "numpages": "14"
    },
    "AccessFixer: Enhancing GUI Accessibility for Low Vision Users With R-GCN Model": {
        "type": "article",
        "key": "10.1109/TSE.2023.3337421",
        "author": "Zhang, Mengxi and Liu, Huaxiao and Chen, Chunyang and Gao, Guangyong and Li, Han and Zhao, Jian",
        "title": "AccessFixer: Enhancing GUI Accessibility for Low Vision Users With R-GCN Model",
        "year": "2023",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3337421",
        "doi": "10.1109/TSE.2023.3337421",
        "abstract": "The Graphical User Interface (GUI) plays a critical role in the interaction between users and mobile applications (apps), aiming at facilitating the operation process. However, due to the variety of functions and non-standardized design, GUIs might have many accessibility issues, like the size of components being too small or their intervals being narrow. These issues would hinder the operation of low vision users, preventing them from obtaining information accurately and conveniently. Although several technologies and methods have been proposed to address these issues, they are typically confined to issue identification, leaving the resolution in the hands of developers. Moreover, it can be challenging to ensure that the color, size, and interval of the fixed GUIs are appropriately compared to the original ones. In this work, we propose a novel approach named AccessFixer (&lt;bold&gt;Access&lt;/bold&gt;ibility Issues &lt;bold&gt;Fix&lt;/bold&gt;ing Method), which utilizes the Relational-Graph Convolutional Neural Network (R-GCN) to simultaneously fix three kinds of accessibility issues, including small sizes, narrow intervals, and low color contrast in GUIs. With AccessFixer, the fixed GUIs would have a consistent color palette, uniform intervals, and adequate size changes achieved through coordinated adjustments to the attributes of related components. Our experiments demonstrate the effectiveness and usefulness of AccessFixer in fixing GUI accessibility issues. After fixing 30 real-world apps, our approach solves an average of 81.2\\% of their accessibility issues. Compared with the baseline tool that can only fix size-related issues, AccessFixer not only fixes both the interval and color contrast of components, but also ensures that no new issues arise in the fixed results. Also, we apply AccessFixer to 10 open-source apps by submitting the fixed results with pull requests (PRs) on GitHub. The results demonstrate that developers approve of our submitted fixed GUIs, with 8 PRs being merged or under fixing. A user study examines that low vision users host a positive attitude toward the GUIs fixed by our method.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "173\u2013189",
        "numpages": "17"
    },
    "Better Pay Attention Whilst Fuzzing": {
        "type": "article",
        "key": "10.1109/TSE.2023.3338129",
        "author": "Zhu, Shunkai and Wang, Jingyi and Sun, Jun and Yang, Jie and Lin, Xingwei and Wang, Tianyi and Zhang, Liyi and Cheng, Peng",
        "title": "Better Pay Attention Whilst Fuzzing",
        "year": "2023",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3338129",
        "doi": "10.1109/TSE.2023.3338129",
        "abstract": "Fuzzing is one of the prevailing methods for vulnerability detection. However, even state-of-the-art fuzzing methods become ineffective after some period of time, i.e., the coverage hardly improves as existing methods are ineffective to focus the attention of fuzzing on covering the hard-to-trigger program paths. In other words, they cannot generate inputs that can break the bottleneck due to the fundamental difficulty in capturing the complex relations between the test inputs and program coverage. In particular, existing fuzzers suffer from the following main limitations: 1) lacking an overall analysis of the program to identify the most \u201crewarding\u201d seeds, and 2) lacking an effective mutation strategy which could continuously select and mutates the more relevant \u201cbytes\u201d of the seeds. In this work, we propose an approach called &lt;sc&gt;ATTuzz&lt;/sc&gt; to address these two issues systematically. First, we propose a lightweight dynamic analysis technique that estimates the \u201creward\u201d of covering each basic block and selects the most rewarding seeds accordingly. Second, we mutate the selected seeds according to a neural network model which predicts whether a certain \u201crewarding\u201d block will be covered given certain mutations on certain bytes of a seed. The model is a deep learning model equipped with an attention mechanism which is learned and updated periodically whilst fuzzing. Our evaluation shows that &lt;sc&gt;ATTuzz&lt;/sc&gt; significantly outperforms 5 state-of-the-art grey-box fuzzers on 6 popular real-world programs and MAGMA data sets at achieving higher edge coverage and finding new bugs. In particular, &lt;sc&gt;ATTuzz&lt;/sc&gt; achieved 1.2X edge coverage and 1.8X bugs detected than AFL++ over 24-hour runs. In addition, &lt;sc&gt;ATTuzz&lt;/sc&gt; also finds 4 new bugs in the latest version of some popular software including p7zip and openUSD.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "190\u2013208",
        "numpages": "19"
    },
    "An Assessment of Rules of Thumb for Software Phase Management, and the Relationship Between Phase Effort and Schedule Success": {
        "type": "article",
        "key": "10.1109/TSE.2023.3339383",
        "author": "Long, Daniel and Drylie, Scott and Ritschel, Jonathan D. and Koschnick, Clay",
        "title": "An Assessment of Rules of Thumb for Software Phase Management, and the Relationship Between Phase Effort and Schedule Success",
        "year": "2023",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3339383",
        "doi": "10.1109/TSE.2023.3339383",
        "abstract": "In the planning of a software development project, managers must estimate the amount of effort needed for distinct phases of activity. A number of rules of thumb exist in the literature to help the program manager in this task. However, very little work has been done to validate these rules of thumb. Applying least square models and Hotelling's &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$T^{2}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:msup&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"drylie-ieq1-3339383.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; test, we evaluate these rules of thumb against a large database of Department of Defense projects. We determine that variability limits the simple application of any such rule. However, there are some worthy of closer attention, and we recommend adjustments for improved application. We also determine that projects which give extra attention to early phases experience less schedule growth. These findings were robust across developmental process type, military service, and project size.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "209\u2013219",
        "numpages": "11"
    },
    "INSPECT: Intrinsic and Systematic Probing Evaluation for Code Transformers": {
        "type": "article",
        "key": "10.1109/TSE.2023.3341624",
        "author": "Karmakar, Anjan and Robbes, Romain",
        "title": "INSPECT: Intrinsic and Systematic Probing Evaluation for Code Transformers",
        "year": "2023",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3341624",
        "doi": "10.1109/TSE.2023.3341624",
        "abstract": "Pre-trained models of source code have recently been successfully applied to a wide variety of Software Engineering tasks; they have also seen some practical adoption in practice, e.g. for code completion. Yet, we still know very little about &lt;italic&gt;what&lt;/italic&gt; these pre-trained models learn about source code. In this article, we use &lt;italic&gt;probing&lt;/italic&gt;\u2014simple diagnostic tasks that do not further train the models\u2014to discover to what extent pre-trained models learn about specific aspects of source code. We use an extensible framework to define 15 probing tasks that exercise surface, syntactic, structural and semantic characteristics of source code. We probe 8 pre-trained source code models, as well as a natural language model (&lt;monospace&gt;BERT&lt;/monospace&gt;) as our baseline. We find that models that incorporate some structural information (such as &lt;monospace&gt;GraphCodeBERT&lt;/monospace&gt;) have a better representation of source code characteristics. Surprisingly, we find that for some probing tasks, &lt;monospace&gt;BERT&lt;/monospace&gt; is competitive with the source code models, indicating that there are ample opportunities to improve source-code specific pre-training on the respective code characteristics. We encourage other researchers to evaluate their models with our probing task suite, so that they may peer into the hidden layers of the models and identify what intrinsic code characteristics are encoded.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "220\u2013238",
        "numpages": "19"
    },
    "Simulation-Based Testing of Simulink Models With Test Sequence and Test Assessment Blocks": {
        "type": "article",
        "key": "10.1109/TSE.2023.3343753",
        "author": "Formica, Federico and Fan, Tony and Rajhans, Akshay and Pantelic, Vera and Lawford, Mark and Menghi, Claudio",
        "title": "Simulation-Based Testing of Simulink Models With Test Sequence and Test Assessment Blocks",
        "year": "2023",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3343753",
        "doi": "10.1109/TSE.2023.3343753",
        "abstract": "Simulation-based software testing supports engineers in finding faults in Simulink&lt;sup&gt;\u00ae&lt;/sup&gt; models. It typically relies on search algorithms that iteratively generate test inputs used to exercise models in simulation to detect design errors. While simulation-based software testing techniques are effective in many practical scenarios, they are typically not fully integrated within the Simulink environment and require additional manual effort. Many techniques require engineers to specify requirements using logical languages that are neither intuitive nor fully supported by Simulink, thereby limiting their adoption in industry. This work presents &lt;sc&gt;HECATE&lt;/sc&gt;, a testing approach for Simulink models using Test Sequence and Test Assessment blocks from Simulink&lt;sup&gt;\u00ae&lt;/sup&gt; Test&lt;sup&gt;\u2122&lt;/sup&gt;. Unlike existing testing techniques, &lt;sc&gt;HECATE&lt;/sc&gt; uses information from Simulink models to guide the search-based exploration. Specifically, &lt;sc&gt;HECATE&lt;/sc&gt; relies on information provided by the Test Sequence and Test Assessment blocks to guide the search procedure. Across a benchmark of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$18$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mn&gt;18&lt;/mml:mn&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"menghi-ieq1-3343753.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; Simulink models from different domains and industries, our comparison of &lt;sc&gt;HECATE&lt;/sc&gt; with the state-of-the-art testing tool &lt;sc&gt;S-Taliro&lt;/sc&gt; indicates that &lt;sc&gt;HECATE&lt;/sc&gt; is both more effective (more failure-revealing test cases) and efficient (less iterations and computational time) than &lt;sc&gt;S-Taliro&lt;/sc&gt; for &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$approx$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u2248&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"menghi-ieq2-3343753.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;94\\% and &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$approx$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u2248&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"menghi-ieq3-3343753.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;83\\% of benchmark models respectively. Furthermore, &lt;sc&gt;HECATE&lt;/sc&gt; successfully generated a failure-revealing test case for a representative case study from the automotive domain demonstrating its practical usefulness.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "239\u2013257",
        "numpages": "19"
    },
    "Revisiting Knowledge-Based Inference of Python Runtime Environments: A Realistic and Adaptive Approach": {
        "type": "article",
        "key": "10.1109/TSE.2023.3346474",
        "author": "Cheng, Wei and Hu, Wei and Ma, Xiaoxing",
        "title": "Revisiting Knowledge-Based Inference of Python Runtime Environments: A Realistic and Adaptive Approach",
        "year": "2023",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3346474",
        "doi": "10.1109/TSE.2023.3346474",
        "abstract": "The reuse and integration of existing code is a common practice for efficient software development. Constantly updated Python interpreters and third-party packages introduce many challenges to Python runtime environment inference. Existing knowledge-based approaches have achieved good performance but still suffer from several limitations in the real world, especially from incomplete domain knowledge. In this paper, we propose ReadPyE, a realistic and adaptive approach to Python runtime environment inference. To leverage the rich code information, we present an automated approach to the construction and maintenance of our designed Python ecosystem knowledge graph (KG). Moreover, we are the first to handle real-world challenges such as complex dependency specifications and incomplete domain knowledge. Specifically, we define a naming similarity measure to match candidate packages for unknown modules and set priorities for multiple candidate packages. ReadPyE solves the optimization problems of candidate package selection and generates compatible runtime environments step by step based on the current Python environment. The inferred environments are iteratively validated and adjusted by matched exception templates in the validation logs. The evaluation results on three real-world datasets show the superior effectiveness and good efficiency of our ReadPyE compared to the existing knowledge-based approaches. ReadPyE solves the environment-related exceptions for 79.75\\% single-file code snippets, 93\\% Python projects, and 63.34\\% program pairs for code integration. We believe ReadPyE can help programmers reduce the time spent on inferring Python runtime environments and facilitate automated software configuration management.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "258\u2013279",
        "numpages": "22"
    },
    "Answering Uncertain, Under-Specified API Queries Assisted by Knowledge-Aware Human-AI Dialogue": {
        "type": "article",
        "key": "10.1109/TSE.2023.3346954",
        "author": "Huang, Qing and Li, Zishuai and Xing, Zhenchang and Zuo, Zhengkang and Peng, Xin and Xu, Xiwei and Lu, Qinghua",
        "title": "Answering Uncertain, Under-Specified API Queries Assisted by Knowledge-Aware Human-AI Dialogue",
        "year": "2023",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3346954",
        "doi": "10.1109/TSE.2023.3346954",
        "abstract": "Developers\u2019 API needs should be more pragmatic, such as seeking suggestive, explainable, and extensible APIs rather than the so-called best result. Existing API search research cannot meet these pragmatic needs because they are solely concerned with query-API relevance. This necessitates a focus on enhancing the entire query process, from query definition to query refinement through intent clarification to query results promoting divergent thinking about results. This paper designs a novel Knowledge-Aware Human-AI Dialog agent (KAHAID) which guides the developer to clarify the uncertain, under-specified query through multi-round question answering and recommends APIs for the clarified query with relevance explanation and extended suggestions (e.g., alternative, collaborating or opposite-function APIs). We systematically evaluate KAHAID. In terms of human-AI dialogue process, it achieves a high diversity of question options (the average diversity between any two options is 74.9\\%) and the ability to guide developers to find APIs using fewer dialogue rounds (no more than 3 rounds on average). For API recommendation, KAHAID achieves an MRR and MAP of 0.769 and 0.794, outperforming state-of-the-art API search approaches BIKER and CLEAR by at least 47\\% in MRR and 226.7\\% in MAP. For knowledge extension, KAHAID obtains an MRR and MAP of 0.815 and 0.864, surpassing state-of-the-art query clarification approaches by at least 42\\% in MRR and 45.2\\% in MAP. As the first of its kind, KAHAID opens the door to integrating the immediate response capability of API research and the interaction, clarification, explanation, and extensibility capability of social-technical information seeking.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "280\u2013295",
        "numpages": "16"
    },
    "Federated Learning for Software Engineering: A Case Study of Code Clone Detection and Defect Prediction": {
        "type": "article",
        "key": "10.1109/TSE.2023.3347898",
        "author": "Yang, Yanming and Hu, Xing and Gao, Zhipeng and Chen, Jinfu and Ni, Chao and Xia, Xin and Lo, David",
        "title": "Federated Learning for Software Engineering: A Case Study of Code Clone Detection and Defect Prediction",
        "year": "2024",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3347898",
        "doi": "10.1109/TSE.2023.3347898",
        "abstract": "In various research domains, artificial intelligence (AI) has gained significant prominence, leading to the development of numerous learning-based models in research laboratories, which are evaluated using benchmark datasets. While the models proposed in previous studies may demonstrate satisfactory performance on benchmark datasets, translating academic findings into practical applications for industry practitioners presents challenges. This can entail either the direct adoption of trained academic models into industrial applications, leading to a performance decrease, or retraining models with industrial data, a task often hindered by insufficient data instances or skewed data distributions. Real-world industrial data is typically significantly more intricate than benchmark datasets, frequently exhibiting data-skewing issues, such as label distribution skews and quantity skews. Furthermore, accessing industrial data, particularly source code, can prove challenging for Software Engineering (SE) researchers due to privacy policies. This limitation hinders SE researchers\u2019 ability to gain insights into industry developers\u2019 concerns and subsequently enhance their proposed models. To bridge the divide between academic models and industrial applications, we introduce a federated learning (FL)-based framework called &lt;sc&gt;Almity&lt;/sc&gt;. Our aim is to simplify the process of implementing research findings into practical use for both SE researchers and industry developers. &lt;sc&gt;Almity&lt;/sc&gt; enhances model performance on sensitive skewed data distributions while ensuring data privacy and security. It introduces an innovative aggregation strategy that takes into account three key attributes: data scale, data balance, and minority class learnability. This strategy is employed to refine model parameters, thereby enhancing model performance on sensitive skewed datasets. In our evaluation, we employ two well-established SE tasks, i.e., code clone detection and defect prediction, as evaluation tasks. We compare the performance of &lt;sc&gt;Almity&lt;/sc&gt; on both machine learning (ML) and deep learning (DL) models against two mainstream training methods, specifically the Centralized Training Method (CTM) and Vanilla Federated Learning (VFL), to validate the effectiveness and generalizability of &lt;sc&gt;Almity&lt;/sc&gt;. Our experimental results demonstrate that our framework is not only feasible but also practical in real-world scenarios. &lt;sc&gt;Almity&lt;/sc&gt; consistently enhances the performance of learning-based models, outperforming baseline training methods across all types of data distributions.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "296\u2013321",
        "numpages": "26"
    },
    "On Effectiveness and Efficiency of Gamified Exploratory GUI Testing": {
        "type": "article",
        "key": "10.1109/TSE.2023.3348036",
        "author": "Coppola, Riccardo and Fulcini, Tommaso and Ardito, Luca and Torchiano, Marco and Al\\`{e}groth, Emil",
        "title": "On Effectiveness and Efficiency of Gamified Exploratory GUI Testing",
        "year": "2023",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3348036",
        "doi": "10.1109/TSE.2023.3348036",
        "abstract": "&lt;italic&gt;Context&lt;/italic&gt;: Gamification appears to improve enjoyment and quality of execution of software engineering activities, including software testing. Though commonly employed in industry, manual exploratory testing of web application GUIs was proven to be mundane and expensive. Gamification applied to that kind of testing activity has the potential to overcome its limitations, though no empirical research has explored this area yet. &lt;italic&gt;Goal&lt;/italic&gt;: Collect preliminary insights on how gamification, when performed by novice testers, affects the effectiveness, efficiency, test case realism, and user experience in exploratory testing of web applications. &lt;italic&gt;Method&lt;/italic&gt;: Common gamification features augment an existing exploratory testing tool: Final Score with Leaderboard, Injected Bugs, Progress Bar, and Exploration Highlights. The original tool and the gamified version are then compared in an experiment involving 144 participants. User experience is elicited using the Technology Acceptance Model (TAM) questionnaire instrument. &lt;italic&gt;Results&lt;/italic&gt;: Statistical analysis identified several significant differences for metrics that represent the effectiveness and efficiency of tests showing an improvement in coverage when they were developed with gamification. Additionally, user experience is improved with gamification. &lt;italic&gt;Conclusions&lt;/italic&gt;: Gamification of exploratory testing has a tangible effect on how testers create test cases for web applications. While the results are mixed, the effects are most beneficial and interesting and warrant more research in the future. Further research shall be aimed at confirming the presented results in the context of state-of-the-art testing tools and real-world development environments.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "322\u2013337",
        "numpages": "16"
    },
    "Code Review Automation: Strengths and Weaknesses of the State of the Art": {
        "type": "article",
        "key": "10.1109/TSE.2023.3348172",
        "author": "Tufano, Rosalia and Dabi\\'{c}, Ozren and Mastropaolo, Antonio and Ciniselli, Matteo and Bavota, Gabriele",
        "title": "Code Review Automation: Strengths and Weaknesses of the State of the Art",
        "year": "2024",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3348172",
        "doi": "10.1109/TSE.2023.3348172",
        "abstract": "The automation of code review has been tackled by several researchers with the goal of reducing its cost. The adoption of deep learning in software engineering pushed the automation to new boundaries, with techniques &lt;italic&gt;imitating&lt;/italic&gt; developers in generative tasks, such as commenting on a code change as a reviewer would do or addressing a reviewer's comment by modifying code. The performance of these techniques is usually assessed through quantitative metrics, &lt;italic&gt;e.g.,&lt;/italic&gt; the percentage of instances in the test set for which correct predictions are generated, leaving many open questions on the techniques\u2019 capabilities. For example, knowing that an approach is able to correctly address a reviewer's comment in 10\\% of cases is of little value without knowing what was asked by the reviewer: What if in all successful cases the code change required to address the comment was just the removal of an empty line? In this paper we aim at characterizing the cases in which three code review automation techniques tend to succeed or fail in the two above-described tasks. The study has a strong qualitative focus, with &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$sim$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u223c&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"bavota-ieq1-3348172.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;105 man-hours of manual inspection invested in manually analyzing correct and wrong predictions generated by the three techniques, for a total of 2,291 inspected predictions. The output of this analysis are two taxonomies reporting, for each of the two tasks, the types of code changes on which the experimented techniques tend to succeed or to fail, pointing to areas for future work. A result of our manual analysis was also the identification of several issues in the datasets used to train and test the experimented techniques. Finally, we assess the importance of researching in techniques specialized for code review automation by comparing their performance with ChatGPT, a general purpose large language model, finding that ChatGPT struggles in commenting code as a human reviewer would do.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "338\u2013353",
        "numpages": "16"
    },
    "FA-Fuzz: A Novel Scheduling Scheme Using Firefly Algorithm for Mutation-Based Fuzzing": {
        "type": "article",
        "key": "10.1109/TSE.2023.3326144",
        "author": "Gao, Zicong and Xiong, Hao and Dong, Weiyu and Chang, Rui and Yang, Rui and Zhou, Yajin and Jiang, Liehui",
        "title": "FA-Fuzz: A Novel Scheduling Scheme Using Firefly Algorithm for Mutation-Based Fuzzing",
        "year": "2023",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3326144",
        "doi": "10.1109/TSE.2023.3326144",
        "abstract": "Mutation-based fuzzing has been widely used in both academia and industry. Recently, researchers observe that the mutation scheduling scheme affects the efficiency of fuzzing. Accordingly, they propose PSO algorithm or machine learning-based technique to optimize the scheduling process. However, these methods fail to consider the fact that the optimal operator distribution of different seeds is different, even for the same program. In this paper, we propose a novel general scheduling scheme, named FA-fuzz, to find the optimal selecting probability distribution of mutation operators, which is based on the observations that the effective mutation operators are different for different seeds. Specifically, our method is based on the firefly algorithm. The positions of fireflies are mapped to the selection probability distribution of different mutation operators. The brightness of fireflies is expressed as the efficiency of discovering unique testcases. We implement prototype systems on multiple state-of-art fuzzers, and perform evaluations on two datasets. Our proposed method improves both the number of unique paths and unique bugs on real-world datasets. In addition, we discover 30 zero-day vulnerabilities in eight real-world programs, which demonstrate the effectiveness of FA-fuzz.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "1\u201315",
        "numpages": "15"
    },
    "UMLsecRT: Reactive Security Monitoring of Java Applications With Round-Trip Engineering": {
        "type": "article",
        "key": "10.1109/TSE.2023.3326366",
        "author": "Peldszus, Sven and B\\\"{u}rger, Jens and J\\\"{u}rjens, Jan",
        "title": "UMLsecRT: Reactive Security Monitoring of Java Applications With Round-Trip Engineering",
        "year": "2023",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3326366",
        "doi": "10.1109/TSE.2023.3326366",
        "abstract": "Today's software systems tend to be long-living and often process security-critical data, so keeping up with ever-changing security measures, attacks, and mitigations is critical to maintaining their security. While it has become common practice to consider security aspects during the design of a system, OWASP still identifies insecure design as one of the top 10 threats to security. Furthermore, even if the planned design is secure, verifying that the planned security assumptions hold at run-time and investigating any violations that may have occurred is cumbersome. In particular, the configuration of run-time monitors such as the Java Security Manager, which could enforce design-time security assumptions, is non-trivial and therefore used in practice rarely. To address these challenges, we present UMLsecRT for automatically supporting model-based security engineering with run-time monitoring of design-time security specifications and round-trip engineering for propagating run-time observations to the design level. Following the established security-by-design approach UMLsec, security experts annotate system models with security properties that UMLsecRT automatically synchronizes with corresponding source code annotations for the automatic configuration of UMLsecRT's run-time monitor. To this end, UMLecRT monitors these security properties at run-time without additional effort to specify monitoring policies. Developers can define mitigations for attacks detected at run-time in advance by adjusting the automatically synchronized annotations at implementation time. Triggered by a security violation, UMLsecRT can adapt the design-time models based on run-time findings to facilitate the investigation of security violations. We evaluated UMLsecRT concerning its effectiveness and applicability to security violations extracted from real-world attacks and the DaCapo benchmark, conducted user studies on the usability of the adapted models and the feasibility of UMLsecRT in practice, especially concerning countermeasures, and investigated the scalability of UMLsecRT. To study the applicability of the whole development process, we applied UMLsecRT in two case studies to the Eclipse Secure Storage and the electronic health record system iTrust.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "oct",
        "pages": "16\u201347",
        "numpages": "32"
    },
    "Concretization of Abstract Traffic Scene Specifications Using Metaheuristic Search": {
        "type": "article",
        "key": "10.1109/TSE.2023.3331254",
        "author": "Babikian, Aren A. and Semer\\'{a}th, Oszk\\'{a}r and Varr\\'{o}, D\\'{a}niel",
        "title": "Concretization of Abstract Traffic Scene Specifications Using Metaheuristic Search",
        "year": "2023",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3331254",
        "doi": "10.1109/TSE.2023.3331254",
        "abstract": "Existing safety assurance approaches for autonomous vehicles (AVs) perform system-level safety evaluation by placing the AV-under-test in challenging traffic scenarios captured by abstract scenario specifications and investigated in realistic traffic simulators. As a first step towards scenario-based testing of AVs, the initial scene of a traffic scenario must be concretized. In this context, the scene concretization challenge takes as input a high-level specification of abstract traffic scenes and aims to map them to concrete scenes where exact numeric initial values are defined for each attribute of a vehicle (e.g. position or velocity). In this paper, we propose a traffic scene concretization approach that places vehicles on realistic road maps such that they satisfy an extensible set of abstract constraints defined by an expressive scene specification language which also supports static detection of inconsistencies. Then, abstract constraints are mapped to corresponding numeric constraints, which are solved by metaheuristic search with customizable objective functions and constraint aggregation strategies. We conduct a series of experiments over three realistic road maps to compare eight configurations of our approach with three variations of the state-of-the-art &lt;sc&gt;Scenic&lt;/sc&gt; tool, and to evaluate its scalability.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "48\u201368",
        "numpages": "21"
    },
    "Stakeholder Preference Extraction From Scenarios": {
        "type": "article",
        "key": "10.1109/TSE.2023.3333265",
        "author": "Shen, Yuchen and Breaux, Travis",
        "title": "Stakeholder Preference Extraction From Scenarios",
        "year": "2023",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3333265",
        "doi": "10.1109/TSE.2023.3333265",
        "abstract": "Companies use personalization to tailor user experiences. Personalization appears in search engines and online stores, which include salutations and statistically learned correlations over search-, browsing- and purchase-histories. However, users have a wider variety of substantive, domain-specific preferences that affect their choices when they use directory services, and these have largely been overlooked or ignored. The contributions of this paper include: (1) a grounded theory describing how stakeholder preferences are expressed in text scenarios; (2) an app feature survey to assess whether elicited preferences represent missing requirements in existing systems; (3) an evaluation of three classifiers to label preference words in scenarios; and (4) a linker to build preference phrases by linking labeled preference words to each other based on word position. In this study, the authors analyzed 217 elicited directory service scenarios across 12 domain categories to yield a total of 7,661 stakeholder preferences labels. The app survey yielded 43 stakeholder preferences that were missed on average 49.7\\% by 15 directory service websites studied. The BERT-based transformer showed the best average overall 81.1\\% precision, 84.4\\% recall and 82.6\\% F1-score when tested on unseen domains. Finally, the preference linker correctly links preference phrases with 90.1\\% accuracy. Given these results, we believe directory service developers can use this approach to automatically identify user preferences to improve service designs.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "69\u201384",
        "numpages": "16"
    },
    "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3334955",
        "author": "Sch\\\"{a}fer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank",
        "title": "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation",
        "year": "2023",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3334955",
        "doi": "10.1109/TSE.2023.3334955",
        "abstract": "Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in &lt;sc&gt;TestPilot&lt;/sc&gt;, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate &lt;sc&gt;TestPilot&lt;/sc&gt; using OpenAI's &lt;italic&gt;gpt3.5-turbo&lt;/italic&gt; LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2\\% and branch coverage of 52.8\\%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3\\% statement coverage and 25.6\\% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8\\% of &lt;sc&gt;TestPilot&lt;/sc&gt;'s generated tests have &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$leq$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u2264&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"schaefer-ieq1-3334955.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; 50\\% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run &lt;sc&gt;TestPilot&lt;/sc&gt; with two additional LLMs, OpenAI's older &lt;italic&gt;code-cushman-002&lt;/italic&gt; LLM and &lt;italic&gt;StarCoder&lt;/italic&gt;, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2\\% median statement coverage), and somewhat worse results with the latter (54.0\\% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "85\u2013105",
        "numpages": "21"
    },
    "Which Animation API Should I Use Next? A Multimodal Real-Time Animation API Recommendation Model for Android Apps": {
        "type": "article",
        "key": "10.1109/TSE.2023.3338728",
        "author": "Gao, Shanquan and Zhang, Liyuan and Liu, Huaxiao and Wang, Yihui",
        "title": "Which Animation API Should I Use Next? A Multimodal Real-Time Animation API Recommendation Model for Android Apps",
        "year": "2023",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3338728",
        "doi": "10.1109/TSE.2023.3338728",
        "abstract": "UI animation is a widely adopted design element in the UI of Android apps. There are many animation APIs available for a variety of purposes, and developers can utilize them to realize the UI animations to avoid reinventing the wheel and thus improve the development efficiency. However, the number of animation APIs is as high as thousands and it is non-trivial for developers to systematically master their use. Facing such a problem, we construct a multi-modal real-time animation API recommendation model called U-A2A in this paper, which can provide the available animation API for developers of Android apps in real-time throughout the animation realization according to the multi-modal information, that is, the information of UI animation task and the animation API context of current program (i.e., the animation API sequence that has been used). The reason for considering the animation API context is that realizing a UI animation requires the use of multiple animation APIs and relevant animation APIs roughly follow a sequence. U-A2A consists of two important parts: feature extractor and predictor. The feature extractor, which is constructed based on 3D CNN and GRU, can gain the combined feature of UI animation task as well as animation API context. The predictor consists of a fully connected layer as well as a softmax layer, and it can predict and recommend the next available animation API according to the result from feature extractor. Furthermore, we use the development experience about animation APIs of existing app products as the basis to adjust the parameters of U-A2A, thereby completing the training work of recommendation model. The experimental result shows that when 1, 3, 5, and 10 animation APIs are considered, U-A2A can achieve 45.13\\%, 65.72\\%, 72.97\\% and 81.85\\% accuracy respectively, which is much higher than the baseline LUPE.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "106\u2013122",
        "numpages": "17"
    },
    "Making Sense of AI Systems Development": {
        "type": "article",
        "key": "10.1109/TSE.2023.3338857",
        "author": "Dolata, Mateusz and Crowston, Kevin",
        "title": "Making Sense of AI Systems Development",
        "year": "2023",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3338857",
        "doi": "10.1109/TSE.2023.3338857",
        "abstract": "We identify and describe episodes of sensemaking around challenges in modern Artificial-Intelligence (AI)-based systems development that emerged in projects carried out by IBM and client companies. All projects used IBM Watson as the development platform for building tailored AI-based solutions to support workers or customers of the client companies. Yet, many of the projects turned out to be significantly more challenging than IBM and its clients had expected. The analysis reveals that project members struggled to establish reliable meanings about the technology, the project, context, and data to act upon. The project members report multiple aspects of the projects that they were not expecting to need to make sense of yet were problematic. Many issues bear upon the current-generation AI\u2019s inherent characteristics, such as dependency on large data sets and continuous improvement as more data becomes available. Those characteristics increase the complexity of the projects and call for balanced mindfulness to avoid unexpected problems.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "123\u2013140",
        "numpages": "18"
    },
    "The Double-Edged Sword of Diversity: How Diversity, Conflict, and Psychological Safety Impact Software Teams": {
        "type": "article",
        "key": "10.1109/TSE.2023.3339881",
        "author": "Verwijs, Christiaan and Russo, Daniel",
        "title": "The Double-Edged Sword of Diversity: How Diversity, Conflict, and Psychological Safety Impact Software Teams",
        "year": "2023",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3339881",
        "doi": "10.1109/TSE.2023.3339881",
        "abstract": "Team diversity can be seen as a double-edged sword. It brings additional cognitive resources to teams at the risk of increased conflict. Few studies have investigated how different types of diversity impact software teams. This study views diversity through the lens of the &lt;italic&gt;categorization-elaboration model (CEM)&lt;/italic&gt;. We investigated how diversity in gender, age, role, and cultural background impacts team effectiveness and conflict, and how these associations are moderated by psychological safety. Our sample consisted of 1,118 participants from 161 teams and was analyzed with Covariance-Based Structural Equation Modeling (CB-SEM). We found a positive effect of age diversity on team effectiveness and gender diversity on relational conflict. Psychological safety contributed directly to effective teamwork and less conflict but did not moderate the diversity-effectiveness link. While our results are consistent with the CEM theory for age and gender diversity, other types of diversity did not yield similar results. We discuss several reasons for this, including curvilinear effects, moderators such as task interdependence, or the presence of a diversity mindset. With this paper, we argue that a dichotomous nature of diversity is oversimplified. Indeed, it is a complex relationship where context plays a pivotal role. A more nuanced understanding of diversity through the lens of theories, such as the CEM, may lead to more effective teamwork.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "dec",
        "pages": "141\u2013157",
        "numpages": "17"
    },
    "Corrections to \u201cUncovering Bugs in Code Coverage Profilers via Control Flow Constraint Solving\u201d": {
        "type": "article",
        "key": "10.1109/TSE.2023.3339345",
        "author": "Wang, Yang and Zhang, Peng and Sun, Maolin and Lu, Zeyu and Yang, Yibiao and Tang, Yutian and Qian, Junyan and Li, Zhi and Zhou, Yuming",
        "title": "Corrections to \u201cUncovering Bugs in Code Coverage Profilers via Control Flow Constraint Solving\u201d",
        "year": "2024",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3339345",
        "doi": "10.1109/TSE.2023.3339345",
        "abstract": "In [1, p. 4967], a figure citation is incorrect and \u201cFig. 3(c)\u201d should be \u201cFig. 1(c)\u201d in the left column, the fourth line from the bottom. It is corrected below.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "158",
        "numpages": "1"
    },
    "BinCola: Diversity-Sensitive Contrastive Learning for Binary Code Similarity Detection": {
        "type": "article",
        "key": "10.1109/TSE.2024.3411072",
        "author": "Jiang, Shuai and Fu, Cai and He, Shuai and Lv, Jianqiang and Han, Lansheng and Hu, Hong",
        "title": "BinCola: Diversity-Sensitive Contrastive Learning for Binary Code Similarity Detection",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3411072",
        "doi": "10.1109/TSE.2024.3411072",
        "abstract": "Binary Code Similarity Detection (BCSD) is a fundamental binary analysis technique in the area of software security. Recently, advanced deep learning algorithms are integrated into BCSD platforms to achieve superior performance on well-known benchmarks. However, real-world large programs embed more complex diversities due to different compilers, various optimization levels, multiple architectures and even obfuscations. Existing BCSD solutions suffer from low accuracy issues in such complicated real-world application scenarios. In this paper, we propose BinCola, a novel Transformer-based dual diversity-sensitive contrastive learning framework that comprehensively considers the diversity of compiler options and candidate functions in the real-world application scenarios and employs the attention mechanism to fuse multi-granularity function features for enhancing generality and scalability. BinCola simultaneously compares multiple candidate functions across various compilation option scenarios to learn the differences caused by distinct compiler options and different candidate functions. We evaluate BinCola's performance in a variety of ways, including binary similarity detection and real-world vulnerability search in multiple application scenarios. The results demonstrate that BinCola achieves superior performance compared to state-of-the-art (SOTA) methods, with improvements of 2.80\\%, 33.62\\%, 22.41\\%, and 34.25\\% in cross-architecture, cross-optimization level, cross-compiler, and cross-obfuscation scenarios, respectively.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2485\u20132497",
        "numpages": "13"
    },
    "A Controlled Experiment in Age and Gender Bias When Reading Technical Articles in Software Engineering": {
        "type": "article",
        "key": "10.1109/TSE.2024.3437355",
        "author": "Liang, Anda and Murphy-Hill, Emerson and Weimer, Westley and Huang, Yu",
        "title": "A Controlled Experiment in Age and Gender Bias When Reading Technical Articles in Software Engineering",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3437355",
        "doi": "10.1109/TSE.2024.3437355",
        "abstract": "Online platforms and communities are a critical part of modern software engineering, yet are often affected by human biases. While previous studies investigated human biases and their potential harms against the efficiency and fairness of online communities, they have mainly focused on the open source and &lt;italic&gt;Q &amp; A&lt;/italic&gt; platforms, such as &lt;italic&gt;GitHub&lt;/italic&gt; and &lt;italic&gt;Stack Overflow&lt;/italic&gt;, but overlooked the audience-focused online platforms for delivering programming and SE-related technical articles, where millions of software engineering practitioners share, seek for, and learn from high-quality software engineering articles (i.e., &lt;italic&gt;technical articles&lt;/italic&gt; for SE). Furthermore, most of the previous work has revealed gender and race bias, but we have little knowledge about the effect of age on software engineering practice. In this paper, we propose to investigate the effect of authors\u2019 demographic information (gender and age) on the evaluation of technical articles on software engineering and potential behavioral differences among participants. We conducted a survey-based and controlled human study and collected responses from 540 participants to investigate developers\u2019 evaluation of technical articles for software engineering. By controlling the gender and age of the author profiles of technical articles for SE, we found that raters tend to have more positive content depth evaluations for younger male authors when compared to older male authors and that male participants conduct technical article evaluations faster than female participants, consistent with prior study findings. Surprisingly, different from other software engineering evaluation activities (e.g., code review, pull request, etc.), we did not find a significant difference in the genders of authors on the evaluation outcome of technical articles in SE.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2498\u20132511",
        "numpages": "14"
    },
    "Learning to Generate Structured Code Summaries From Hybrid Code Context": {
        "type": "article",
        "key": "10.1109/TSE.2024.3439562",
        "author": "Zhou, Ziyi and Li, Mingchen and Yu, Huiqun and Fan, Guisheng and Yang, Penghui and Huang, Zijie",
        "title": "Learning to Generate Structured Code Summaries From Hybrid Code Context",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3439562",
        "doi": "10.1109/TSE.2024.3439562",
        "abstract": "Code summarization aims to automatically generate natural language descriptions for code, and has become a rapidly expanding research area in the past decades. Unfortunately, existing approaches mainly focus on the \u201cone-to-one\u201d mapping from methods to short descriptions, which hinders them from becoming practical tools: 1) The program context is ignored, so they have difficulty in predicting keywords outside the target method; 2) They are typically trained to generate brief function descriptions with only one sentence in length, and therefore have difficulty in providing specific information. These drawbacks are partially due to the limitations of public code summarization datasets. In this paper, we first build a large code summarization dataset including different code contexts and summary content annotations, and then propose a deep learning framework that learns to generate structured code summaries from hybrid program context, named StructCodeSum. It provides both an LLM-based approach and a lightweight approach which are suitable for different scenarios. Given a target method, StructCodeSum predicts its function description, return description, parameter description, and usage description through hybrid code context, and ultimately builds a Javadoc-style code summary. The hybrid code context consists of path context, class context, documentation context and call context of the target method. Extensive experimental results demonstrate: 1) The hybrid context covers more than 70\\% of the summary tokens in average and significantly boosts the model performance; 2) When generating function descriptions, StructCodeSum outperforms the state-of-the-art approaches by a large margin; 3) According to human evaluation, the quality of the structured summaries generated by our approach is better than the documentation generated by Code Llama.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2512\u20132528",
        "numpages": "17"
    },
    "Predicting the First Response Latency of Maintainers and Contributors in Pull Requests": {
        "type": "article",
        "key": "10.1109/TSE.2024.3443741",
        "author": "Khatoonabadi, SayedHassan and Abdellatif, Ahmad and Costa, Diego Elias and Shihab, Emad",
        "title": "Predicting the First Response Latency of Maintainers and Contributors in Pull Requests",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3443741",
        "doi": "10.1109/TSE.2024.3443741",
        "abstract": "The success of a Pull Request (PR) depends on the responsiveness of the maintainers and the contributor during the review process. Being aware of the expected waiting times can lead to better interactions and managed expectations for both the maintainers and the contributor. In this paper, we propose a machine-learning approach to predict the first response latency of the maintainers following the submission of a PR, and the first response latency of the contributor after receiving the first response from the maintainers. We curate a dataset of 20 large and popular open-source projects on GitHub and extract 21 features to characterize projects, contributors, PRs, and review processes. Using these features, we then evaluate seven types of classifiers to identify the best-performing models. We also conduct permutation feature importance and SHAP analyses to understand the importance and the impact of different features on the predicted response latencies. We find that our CatBoost models are the most effective for predicting the first response latencies of both maintainers and contributors. Compared to a dummy classifier that always returns the majority class, these models achieved an average improvement of 29\\% in AUC-ROC and 51\\% in AUC-PR for maintainers, as well as 39\\% in AUC-ROC and 89\\% in AUC-PR for contributors across the studied projects. The results indicate that our models can aptly predict the first response latencies using the selected features. We also observe that PRs submitted earlier in the week, containing an average number of commits, and with concise descriptions are more likely to receive faster first responses from the maintainers. Similarly, PRs with a lower first response latency from maintainers, that received the first response of maintainers earlier in the week, and containing an average number of commits tend to receive faster first responses from the contributors. Additionally, contributors with a higher acceptance rate and a history of timely responses in the project are likely to both obtain and provide faster first responses. Moreover, we show the effectiveness of our approach in a cross-project setting. Finally, we discuss key guidelines for maintainers, contributors, and researchers to help facilitate the PR review process.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2529\u20132543",
        "numpages": "15"
    },
    "Runtime Verification and Field-Based Testing for ROS-Based Robotic Systems": {
        "type": "article",
        "key": "10.1109/TSE.2024.3444697",
        "author": "Caldas, Ricardo and Garc\\'{\\i}a, Juan Antonio Pi\\~{n}era and Schiopu, Matei and Pelliccione, Patrizio and Rodrigues, Gena\\'{\\i}na and Berger, Thorsten",
        "title": "Runtime Verification and Field-Based Testing for ROS-Based Robotic Systems",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3444697",
        "doi": "10.1109/TSE.2024.3444697",
        "abstract": "Robotic systems are becoming pervasive and adopted in increasingly many domains, such as manufacturing, healthcare, and space exploration. To this end, engineering software has emerged as a crucial discipline for building maintainable and reusable robotic systems. The field of robotics software engineering research has received increasing attention, fostering autonomy as a fundamental goal. However, robotics developers are still challenged trying to achieve this goal given that simulation is not able to deliver solutions to realistically emulate real-world phenomena. Robots also need to operate in unpredictable and uncontrollable environments, which require safe and trustworthy self-adaptation capabilities implemented in software. Typical techniques to address the challenges are runtime verification, field-based testing, and mitigation techniques that enable fail-safe solutions. However, there is no clear guidance to architect ROS-based systems to enable and facilitate runtime verification and field-based testing. This paper aims to fill in this gap by providing guidelines that can help developers and quality assurance (QA) teams when developing, verifying or testing their robots in the field. These guidelines are carefully tailored to address the challenges and requirements of testing robotics systems in real-world scenarios. We conducted (i) a literature review on studies addressing runtime verification and field-based testing for robotic systems, (ii) mined ROS-based applications repositories, and (iii) validated the applicability, clarity, and usefulness via two questionnaires with 55 answers overall. We contribute 20 guidelines: 8 for developers and 12 for QA teams formulated for researchers and practitioners in robotic software engineering. Finally, we map our guidelines to open challenges thus far in runtime verification and field-based testing for ROS-based systems and, we outline promising research directions in the field. &lt;bold&gt;Guidelines website and replication package:&lt;/bold&gt; &lt;uri&gt;https://ros-rvft.github.io&lt;/uri&gt;.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2544\u20132567",
        "numpages": "24"
    },
    "Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration": {
        "type": "article",
        "key": "10.1109/TSE.2024.3445338",
        "author": "Paltenghi, Matteo and Pandita, Rahul and Henley, Austin Z. and Ziegler, Albert",
        "title": "Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3445338",
        "doi": "10.1109/TSE.2024.3445338",
        "abstract": "Recent neural models of code, such as OpenAI Codex and AlphaCode, have demonstrated remarkable proficiency at code generation due to the underlying attention mechanism. However, it often remains unclear how the models actually process code, and to what extent their reasoning and the way their attention mechanism scans the code matches the patterns of developers. A poor understanding of the model reasoning process limits the way in which current neural models are leveraged today, so far mostly for their raw prediction. To fill this gap, this work studies how the processed attention signal of three open large language models - CodeGen, InCoder and GPT-J - agrees with how developers look at and explore code when each answers the same sensemaking questions about code. Furthermore, we contribute an open-source eye-tracking dataset comprising 92 manually-labeled sessions from 25 developers engaged in sensemaking tasks. We empirically evaluate five heuristics that do not use the attention and ten attention-based post-processing approaches of the attention signal of CodeGen against our ground truth of developers exploring code, including the novel concept of &lt;italic&gt;follow-up attention&lt;/italic&gt; which exhibits the highest agreement between model and human attention. Our follow-up attention method can predict the next line a developer will look at with 47\\% accuracy. This outperforms the baseline prediction accuracy of 42.3\\%, which uses the session history of other developers to recommend the next line. These results demonstrate the potential of leveraging the attention signal of pre-trained models for effective code exploration.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2568\u20132582",
        "numpages": "15"
    },
    "iTCRL: Causal-Intervention-Based Trace Contrastive Representation Learning for Microservice Systems": {
        "type": "article",
        "key": "10.1109/TSE.2024.3446532",
        "author": "Tian, Xiangbo and Ying, Shi and Li, Tiangang and Yuan, Mengting and Wang, Ruijin and Zhao, Yishi and Shang, Jianga",
        "title": "iTCRL: Causal-Intervention-Based Trace Contrastive Representation Learning for Microservice Systems",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3446532",
        "doi": "10.1109/TSE.2024.3446532",
        "abstract": "Nowadays, microservice architecture has become mainstream way of cloud applications delivery. Distributed tracing is crucial to preserve the observability of microservice systems. However, existing trace representation approaches only concentrate on operations, relationships and metrics related to service invocations. They ignore service events that denotes meaningful, singular point in time during the service's duration. In this paper, we propose iTCRL, a novel trace contrastive representation learning approach based on causal intervention. This approach first constructs a unified graph representation for each trace to describe the runtime status of service events in traces and the complex relationships between them. Then, Causal-intervention-based Trace Contrastive Learning is proposed, which learns trace representations from causal perspective based on the unified graph representations of traces. It uses causal intervention to generate contrastive views, heterogeneous graph neural network-based trace encoder to learn trace representations, and direct causal effect to guide the training of trace encoder. Experimental results on three datasets show that iTCRL outperforms all baselines in terms of trace classification, trace anomaly detection, trace sampling and noise robustness, and also validate the contribution of Causal-intervention-based Trace Contrastive Learning.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2583\u20132601",
        "numpages": "19"
    },
    "Yuga: Automatically Detecting Lifetime Annotation Bugs in the Rust Language": {
        "type": "article",
        "key": "10.1109/TSE.2024.3447671",
        "author": "Nitin, Vikram and Mulhern, Anne and Arora, Sanjay and Ray, Baishakhi",
        "title": "Yuga: Automatically Detecting Lifetime Annotation Bugs in the Rust Language",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3447671",
        "doi": "10.1109/TSE.2024.3447671",
        "abstract": "The Rust programming language is becoming increasingly popular among systems programmers due to its efficient performance and robust memory safety guarantees. Rust employs an ownership model to ensure these guarantees by allowing each value to be owned by only one identifier at a time. It uses the concept of borrowing and lifetimes to enable other variables to temporarily borrow values. Despite its benefits, security vulnerabilities have been reported in Rust projects, often attributed to the use of \u201cunsafe\u201d Rust code. These vulnerabilities, in part, arise from incorrect lifetime annotations on function signatures. However, existing tools fail to detect these bugs, primarily because such bugs are rare, challenging to detect through dynamic analysis, and require explicit memory models. To overcome these limitations, we characterize incorrect lifetime annotations as a source of memory safety bugs and leverage this understanding to devise a novel static analysis tool, &lt;sc&gt;Yuga&lt;/sc&gt;, to detect potential lifetime annotation bugs. &lt;sc&gt;Yuga&lt;/sc&gt; uses a multi-phase analysis approach, starting with a quick pattern-matching algorithm to identify potential buggy components and then conducting a flow and field-sensitive alias analysis to confirm the bugs. We also curate new datasets of lifetime annotation bugs. &lt;sc&gt;Yuga&lt;/sc&gt; successfully detects bugs with good precision on these datasets, and we make the code and datasets publicly available.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2602\u20132613",
        "numpages": "12"
    },
    "EpiTESTER: Testing Autonomous Vehicles With Epigenetic Algorithm and Attention Mechanism": {
        "type": "article",
        "key": "10.1109/TSE.2024.3449429",
        "author": "Lu, Chengjie and Ali, Shaukat and Yue, Tao",
        "title": "EpiTESTER: Testing Autonomous Vehicles With Epigenetic Algorithm and Attention Mechanism",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3449429",
        "doi": "10.1109/TSE.2024.3449429",
        "abstract": "Testing autonomous vehicles (AVs) under various environmental scenarios that lead the vehicles to unsafe situations is challenging. Given the infinite possible environmental scenarios, it is essential to find critical scenarios efficiently. To this end, we propose a novel testing method, named &lt;italic&gt;EpiTESTER&lt;/italic&gt;, by taking inspiration from epigenetics, which enables species to adapt to sudden environmental changes. In particular, &lt;italic&gt;EpiTESTER&lt;/italic&gt; adopts gene silencing as its epigenetic mechanism, which regulates gene expression to prevent the expression of a certain gene, and the probability of gene expression is dynamically computed as the environment changes. Given different data modalities (e.g., images, lidar point clouds) in the context of AV, &lt;italic&gt;EpiTESTER&lt;/italic&gt; benefits from a multi-model fusion transformer to extract high-level feature representations from environmental factors. Next, it calculates probabilities based on these features with the attention mechanism. To assess the cost-effectiveness of &lt;italic&gt;EpiTESTER&lt;/italic&gt;, we compare it with a probabilistic search algorithm (Simulated Annealing, SA), a classical genetic algorithm (GA) (i.e., without any epigenetic mechanism implemented), and &lt;italic&gt;EpiTESTER&lt;/italic&gt; with equal probability for each gene. We evaluate &lt;italic&gt;EpiTESTER&lt;/italic&gt; with six initial environments from CARLA, an open-source simulator for autonomous driving research, and two end-to-end AV controllers, Interfuser and TCP. Our results show that &lt;italic&gt;EpiTESTER&lt;/italic&gt; achieved a promising performance in identifying critical scenarios compared to the baselines, showing that applying epigenetic mechanisms is a good option for solving practical problems.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2614\u20132632",
        "numpages": "19"
    },
    "3Erefactor: Effective, Efficient and Executable Refactoring Recommendation for Software Architectural Consistency": {
        "type": "article",
        "key": "10.1109/TSE.2024.3449564",
        "author": "Liu, Jingwen and Jin, Wuxia and Zhou, Junhui and Feng, Qiong and Fan, Ming and Wang, Haijun and Liu, Ting",
        "title": "3Erefactor: Effective, Efficient and Executable Refactoring Recommendation for Software Architectural Consistency",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3449564",
        "doi": "10.1109/TSE.2024.3449564",
        "abstract": "As software continues to evolve and business functions become increasingly complex, architectural inconsistency arises when the implementation architecture deviates from the expected architecture design. This architectural problem makes maintenance difficult and requires significant effort to refactor. To assist labor-intensive refactoring, automated refactoring has received much attention such as searching for optimal refactoring solutions. However, there are still three limitations: The recommended refactorings are insufficiently effective in addressing architectural consistency; the search process for refactoring solution is inefficient; and there is a lack of executable refactoring solutions. To address these limitations, we propose an effective, efficient, and executable refactoring recommendation approach namely the 3Erefactor for software architectural consistency. To achieve effective refactoring, 3Erefactor uses NSGA-II to generate refactoring solutions that minimize architectural inconsistencies at module level and entity level. To achieve efficient refactoring, 3Erefactor leverages architecture recovery technique to locate files requiring refactoring, helping accelerate the convergence of refactoring algorithm. To achieve executable refactoring, 3Erefactor designs a set of refactoring executability constraint strategies during the refactoring solution search and generation, including improving refactoring pre-conditions and removing invalid operations in refactoring solutions. We evaluated our approach on six open source systems. Statistical analysis of our experiments shows that, the refactoring solution generated by 3Erefactor performed significantly better than 3 state-of-the-art approaches in terms of reducing the number of architectural inconsistencies, improving the efficiency of the refactoring algorithm and improving the executability of refactorings.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2633\u20132655",
        "numpages": "23"
    },
    "Method-Level Test-to-Code Traceability Link Construction by Semantic Correlation Learning": {
        "type": "article",
        "key": "10.1109/TSE.2024.3449917",
        "author": "Sun, Weifeng and Guo, Zhenting and Yan, Meng and Liu, Zhongxin and Lei, Yan and Zhang, Hongyu",
        "title": "Method-Level Test-to-Code Traceability Link Construction by Semantic Correlation Learning",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3449917",
        "doi": "10.1109/TSE.2024.3449917",
        "abstract": "Test-to-code traceability links (TCTLs) establish links between test artifacts and code artifacts. These links enable developers and testers to quickly identify the specific pieces of code tested by particular test cases, thus facilitating more efficient debugging, regression testing, and maintenance activities. Various approaches, based on distinct concepts, have been proposed to establish method-level TCTLs, specifically linking unit tests to corresponding focal methods. Static methods, such as naming-convention-based methods, use heuristic- and similarity-based strategies. However, such methods face the following challenges: \u2460 Developers, driven by specific scenarios and development requirements, may deviate from naming conventions, leading to TCTL identification failures. \u2461 Static methods often overlook the rich semantics embedded within tests, leading to erroneous associations between tests and semantically unrelated code fragments. Although dynamic methods achieve promising results, they require the project to be compilable and the tests to be executable, limiting their usability. This limitation is significant for downstream tasks requiring massive test-code pairs, as not all projects can meet these requirements. To tackle the abovementioned limitations, we propose a novel static method-level TCTL approach, named &lt;sc&gt;TestLinker&lt;/sc&gt;. For the first challenge of existing static approaches, &lt;sc&gt;TestLinker&lt;/sc&gt; introduces a two-phase TCTL framework to accommodate different project types in a triage manner. As for the second challenge, we employ the &lt;italic&gt;semantic correlation learning&lt;/italic&gt;, which learns and establishes the semantic correlations between tests and focal methods based on Pre-trained Code Models (PCMs). &lt;sc&gt;TestLinker&lt;/sc&gt; further establishes mapping rules to accurately link the recommended function name to the concrete production function declaration. Empirical evaluation on a meticulously labeled dataset reveals that &lt;sc&gt;TestLinker&lt;/sc&gt; significantly outperforms traditional static techniques, showing average F1-score improvements ranging from 73.48\\% to 202.00\\%. Moreover, compared to state-of-the-art dynamic methods, &lt;sc&gt;TestLinker&lt;/sc&gt;, which only leverages static information, demonstrates comparable or even better performance, with an average F1-score increase of 37.40\\%.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2656\u20132676",
        "numpages": "21"
    },
    "Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction": {
        "type": "article",
        "key": "10.1109/TSE.2024.3450837",
        "author": "Kang, Sungmin and Yoon, Juyeon and Askarbekkyzy, Nargiz and Yoo, Shin",
        "title": "Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3450837",
        "doi": "10.1109/TSE.2024.3450837",
        "abstract": "Bug reproduction is a critical developer activity that is also challenging to automate, as bug reports are often in natural language and thus can be difficult to transform to test cases consistently. As a result, existing techniques mostly focused on crash bugs, which are easier to automatically detect and verify. In this work, we overcome this limitation by using large language models (LLMs), which have been demonstrated to be adept at natural language processing and code generation. By prompting LLMs to generate bug-reproducing tests, and via a post-processing pipeline to automatically identify promising generated tests, our proposed technique &lt;sc&gt;Libro&lt;/sc&gt; could successfully reproduce about one-third of all bugs in the widely used Defects4J benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11 open-source LLMs, suggests that open-source LLMs also demonstrate substantial potential, with the StarCoder LLM achieving 70\\% of the reproduction performance of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J benchmark, and 90\\% of performance on a held-out bug dataset likely not part of any LLM's training data. In addition, our experiments on LLMs of different sizes show that bug reproduction using &lt;sc&gt;Libro&lt;/sc&gt; improves as LLM size increases, providing information as to which LLMs can be used with the &lt;sc&gt;Libro&lt;/sc&gt; pipeline.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "2677\u20132694",
        "numpages": "18"
    },
    "RLocator: Reinforcement Learning for Bug Localization": {
        "type": "article",
        "key": "10.1109/TSE.2024.3452595",
        "author": "Chakraborty, Partha and Alfadel, Mahmoud and Nagappan, Meiyappan",
        "title": "RLocator: Reinforcement Learning for Bug Localization",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3452595",
        "doi": "10.1109/TSE.2024.3452595",
        "abstract": "Software developers spend a significant portion of time fixing bugs in their projects. To streamline this process, bug localization approaches have been proposed to identify the source code files that are likely responsible for a particular bug. Prior work proposed several similarity-based machine-learning techniques for bug localization. Despite significant advances in these techniques, they do not directly optimize the evaluation measures. We argue that directly optimizing evaluation measures can positively contribute to the performance of bug localization approaches. Therefore, in this paper, we utilize Reinforcement Learning (RL) techniques to directly optimize the ranking metrics. We propose &lt;sc&gt;RLocator&lt;/sc&gt;, a Reinforcement Learning-based bug localization approach. We formulate RLocator using a Markov Decision Process (MDP) to optimize the evaluation measures directly. We present the technique and experimentally evaluate it based on a benchmark dataset of 8,316 bug reports from six highly popular Apache projects. The results of our evaluation reveal that RLocator achieves a Mean Reciprocal Rank (MRR) of 0.62, a Mean Average Precision (MAP) of 0.59, and a Top 1 score of 0.46. We compare RLocator with three state-of-the-art bug localization tools, FLIM, BugLocator, and BL-GAN. Our evaluation reveals that RLocator outperforms both approaches by a substantial margin, with improvements of 38.3\\% in MAP, 36.73\\% in MRR, and 23.68\\% in the Top K metric. These findings highlight that directly optimizing evaluation measures considerably contributes to performance improvement of the bug localization problem.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2695\u20132708",
        "numpages": "14"
    }
}