{
    "Improving Cross-Language Code Clone Detection via Code Representation Learning and Graph Neural Networks": {
        "type": "article",
        "key": "10.1109/TSE.2023.3311796",
        "author": "Mehrotra, Nikita and Sharma, Akash and Jindal, Anmol and Purandare, Rahul",
        "title": "Improving Cross-Language Code Clone Detection via Code Representation Learning and Graph Neural Networks",
        "year": "2023",
        "issue_date": "Nov. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "11",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3311796",
        "doi": "10.1109/TSE.2023.3311796",
        "abstract": "Code clone detection is an important aspect of software development and maintenance. The extensive research in this domain has helped reduce the complexity and increase the robustness of source code, thereby assisting bug detection tools. However, the majority of the clone detection literature is confined to a single language. With the increasing prevalence of cross-platform applications, functionality replication across multiple languages is common, resulting in code fragments having similar functionality but belonging to different languages. Since such clones are syntactically unrelated, single language clone detection tools are not applicable in their case. In this article, we propose a semi-supervised deep learning-based tool &lt;sc&gt;Rubhus&lt;/sc&gt;, capable of detecting clones across different programming languages. &lt;sc&gt;Rubhus&lt;/sc&gt; uses the control and data flow enriched abstract syntax trees (ASTs) of code fragments to leverage their syntactic and structural information and then applies graph neural networks (GNNs) to extract this information for the task of clone detection. We demonstrate the effectiveness of our proposed system through experiments conducted over datasets consisting of Java, C, and Python programs and evaluate its performance in terms of precision, recall, and F1 score. Our results indicate that &lt;sc&gt;Rubhus&lt;/sc&gt; outperforms the state-of-the-art cross-language clone detection tools.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4846\u20134868",
        "numpages": "23"
    },
    "Prompt Tuning in Code Intelligence: An Experimental Evaluation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3313881",
        "author": "Wang, Chaozheng and Yang, Yuanhang and Gao, Cuiyun and Peng, Yun and Zhang, Hongyu and Lyu, Michael R.",
        "title": "Prompt Tuning in Code Intelligence: An Experimental Evaluation",
        "year": "2023",
        "issue_date": "Nov. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "11",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3313881",
        "doi": "10.1109/TSE.2023.3313881",
        "abstract": "Pre-trained models have been shown effective in many code intelligence tasks, such as automatic code summarization and defect prediction. These models are pre-trained on large-scale unlabeled corpus and then fine-tuned in downstream tasks. However, as the inputs to pre-training and downstream tasks are in different forms, it is hard to fully explore the knowledge of pre-trained models. Besides, the performance of fine-tuning strongly relies on the amount of downstream task data, while in practice, the data scarcity scenarios are common. Recent studies in the natural language processing (NLP) field show that prompt tuning, a new paradigm for tuning, alleviates the above issues and achieves promising results in various NLP tasks. In prompt tuning, the prompts inserted during tuning provide task-specific knowledge, which is especially beneficial for tasks with relatively scarce data. In this article, we empirically evaluate the usage and effect of prompt tuning in code intelligence tasks. We conduct prompt tuning on popular pre-trained models CodeBERT and CodeT5 and experiment with four code intelligence tasks including defect prediction, code search, code summarization, and code translation. Our experimental results show that prompt tuning consistently outperforms fine-tuning in all four tasks. In addition, prompt tuning shows great potential in low-resource scenarios, e.g., improving the BLEU scores of fine-tuning by more than 26\\% on average for code summarization. Our results suggest that instead of fine-tuning, we could adapt prompt tuning for code intelligence tasks to achieve better performance, especially when lacking task-specific data. We also discuss the implications for adapting prompt tuning in code intelligence tasks.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "oct",
        "pages": "4869\u20134885",
        "numpages": "17"
    },
    "VulHunter: Hunting Vulnerable Smart Contracts at EVM Bytecode-Level via Multiple Instance Learning": {
        "type": "article",
        "key": "10.1109/TSE.2023.3317209",
        "author": "Li, Zhaoxuan and Lu, Siqi and Zhang, Rui and Zhao, Ziming and Liang, Rujin and Xue, Rui and Li, Wenhao and Zhang, Fan and Gao, Sheng",
        "title": "VulHunter: Hunting Vulnerable Smart Contracts at EVM Bytecode-Level via Multiple Instance Learning",
        "year": "2023",
        "issue_date": "Nov. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "11",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3317209",
        "doi": "10.1109/TSE.2023.3317209",
        "abstract": "With the economic development of Ethereum, the frequent security incidents involving smart contracts running on this platform have caused billions of dollars in losses. Consequently, there is a pressing need to identify the vulnerabilities in contracts, while the state-of-the-art (SOTA) detection methods have been limited in this regard as they cannot overcome three challenges at the same time. &lt;monospace&gt;(i)&lt;/monospace&gt; Meet the requirements of detecting the source code, bytecode, and opcode of contracts simultaneously; &lt;monospace&gt;(ii)&lt;/monospace&gt; reduce the reliance on manual pre-defined rules/patterns and expert involvement; &lt;monospace&gt;(iii)&lt;/monospace&gt; assist contract developers in completing the contract lifecycle more safely, &lt;italic&gt;e.g.,&lt;/italic&gt; vulnerability repair and abnormal monitoring. With the development of machine learning (ML), using it to detect the contract runtime execution sequences (called instances) has made it possible to address these challenges. However, the lack of datasets with fine-grained sequence labels poses a significant obstacle, given the unreadability of bytecode/opcode. To this end, we propose a method named VulHunter that extracts the instances by traversing the Control Flow Graph built from contract opcodes. Based on the hybrid attention and multi-instance learning mechanisms, VulHunter reasons the instance labels and designs an optional classifier to automatically capture the subtle features of both normal and defective contracts, thereby identifying the vulnerable instances. Then, it combines the symbolic execution to construct and solve symbolic constraints to validate their feasibility. Finally, we implement a prototype of VulHunter with 15K lines of code and compare it with 9 SOTA methods on five open source datasets including 52,042 source codes and 184,289 bytecodes. The results indicate that VulHunter can detect contract vulnerabilities more accurately (90.04\\% accuracy and 85.60\\% F1 score), efficiently (only 4.4 seconds per contract), and robustly (0\\% analysis failure rate) than SOTA methods. Also, it can focus on specific metrics such as precision and recall by employing different baseline models and hyperparameters to meet the various user requirements, &lt;italic&gt;e.g.,&lt;/italic&gt; vulnerability discovery and misreport mitigation. More importantly, compared with the previous ML-based arts, it can not only provide classification results, defective contract source code statements, key opcode fragments, and vulnerable execution paths, but also eliminate misreports and facilitate more operations such as vulnerability repair and attack simulation during the contract lifecycle.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4886\u20134916",
        "numpages": "31"
    },
    "Cross-Instance Regulatory Compliance Checking of Business Process Event Logs": {
        "type": "article",
        "key": "10.1109/TSE.2023.3319086",
        "author": "van Beest, Nick and Groefsema, Heerko and Cryer, Adrian and Governatori, Guido and Tosatto, Silvano Colombo and Burke, Hannah",
        "title": "Cross-Instance Regulatory Compliance Checking of Business Process Event Logs",
        "year": "2023",
        "issue_date": "Nov. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "11",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3319086",
        "doi": "10.1109/TSE.2023.3319086",
        "abstract": "Event logs capture the execution of business processes, such that each task is represented by an event and each individual execution is a chronological sequence of events, called an event trace. Event logs allow after-the-act and runtime analysis of deployed business processes to verify whether their execution complies with regulations and business requirements. Checking the compliance of a single sequence of events in a trace is straightforward and a number of approaches have been proposed to address this. However, some regulations or business rules span multiple process instances, requiring a cross-instance analysis. In order to check whether such requirements are maintained at all times, multiple traces need to be analysed together, which can result in a combinatorial computational complexity. In this article, we present a novel approach that efficiently checks runtime regulatory compliance based on event logs, while supporting cross-instance rule evaluation and extensible function evaluation over sequences of attribute data values. The efficiency and applicability of the proposed method is tested in a two-pronged evaluation, showing a significant improvement over existing techniques with respect to capabilities as well as computational complexity. The approach presented in this paper is subject to a patent application, with patent number WO2021/248201.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4917\u20134931",
        "numpages": "15"
    },
    "Are Your Dependencies Code Reviewed?: Measuring Code Review Coverage in Dependency Updates": {
        "type": "article",
        "key": "10.1109/TSE.2023.3319509",
        "author": "Imtiaz, Nasif and Williams, Laurie",
        "title": "Are Your Dependencies Code Reviewed?: Measuring Code Review Coverage in Dependency Updates",
        "year": "2023",
        "issue_date": "Nov. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "11",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3319509",
        "doi": "10.1109/TSE.2023.3319509",
        "abstract": "As modern software extensively uses free open source packages as dependencies, developers have to regularly pull in new third-party code through frequent updates. However, without a proper review of every incoming change, vulnerable and malicious code can sneak into the codebase through these dependencies. The goal of this study is to aid developers in securely accepting dependency updates by measuring if the code changes in an update have passed through a code review process. We implement Depdive, an update audit tool for packages in Crates.io, npm, PyPI, and RubyGems registry. Depdive first (i) identifies the files and the code changes in an update that cannot be traced back to the package's source repository, i.e., &lt;italic&gt;phantom artifacts&lt;/italic&gt;; and then (ii) measures what portion of changes in the update, excluding the phantom artifacts, has passed through a code review process, i.e., &lt;italic&gt;code review coverage&lt;/italic&gt;. Using Depdive, we present an empirical study across the latest ten updates of the most downloaded 1000 packages in each of the four registries. We further evaluated our results through a maintainer agreement survey. We find that phantom artifacts are not uncommon in the updates (20.1\\% of the analyzed updates had at least one phantom file). The phantoms can appear either due to legitimate reasons, such as in the case of programmatically generated files, or from accidental inclusion, such as in the case of files that are ignored in the repository. Regarding code review coverage (&lt;italic&gt;CRC&lt;/italic&gt;), we find the updates are typically only partially code-reviewed (52.5\\% of the time). Further, only 9.0\\% of the packages had all their updates in our data set fully code-reviewed, indicating that even the most used packages can introduce non-reviewed code in the software supply chain. We also observe that updates either tend to have high &lt;italic&gt;CRC&lt;/italic&gt; or low &lt;italic&gt;CRC&lt;/italic&gt;, suggesting that packages at the opposite end of the spectrum may require a separate set of treatments.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4932\u20134945",
        "numpages": "14"
    },
    "A Study of the Electrum and DynAlloy Dynamic Behavior Notations": {
        "type": "article",
        "key": "10.1109/TSE.2023.3320625",
        "author": "Cornejo, C\\'{e}sar and Regis, Germ\\'{a}n E. and Aguirre, Nazareno and Frias, Marcelo F.",
        "title": "A Study of the Electrum and DynAlloy Dynamic Behavior Notations",
        "year": "2023",
        "issue_date": "Nov. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "11",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3320625",
        "doi": "10.1109/TSE.2023.3320625",
        "abstract": "&lt;monospace&gt;Alloy&lt;/monospace&gt; is a formal specification language, which despite featuring a simple syntax and relational semantics, is very expressive and supports efficient automated specification analysis, based on SAT solving. While the language is sufficiently expressive to accommodate both &lt;italic&gt;static&lt;/italic&gt; and &lt;italic&gt;dynamic&lt;/italic&gt; properties of systems within specifications, the latter kind of properties require intricate, ad-hoc, constructions to encode system executions. Thus, extensions to the language have been proposed, that internalize these encodings and provide analysis techniques, specifically tailored to properties of executions. In this paper we study two particular extensions to &lt;monospace&gt;Alloy&lt;/monospace&gt; that incorporate elements for the specification of properties of executions. These are &lt;monospace&gt;DynAlloy&lt;/monospace&gt;, whose syntax and semantics are inspired by dynamic logic, and &lt;monospace&gt;Electrum&lt;/monospace&gt;, based on linear-time temporal logic and inspired by languages such as &lt;monospace&gt;TLA+&lt;/monospace&gt;. We analyze and compare the syntactic characteristics of the languages, their corresponding expressiveness, and the effectiveness and efficiency of their associated analysis tools. The comparison is based on a set of &lt;monospace&gt;Alloy&lt;/monospace&gt; specifications that are taken from the literature and demand dynamic behavior analysis, including an &lt;monospace&gt;Alloy&lt;/monospace&gt; model of the Chord ring-maintenance protocol, that drives our qualitative comparison of the notations.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4946\u20134963",
        "numpages": "18"
    },
    "Uncovering Bugs in Code Coverage Profilers via Control Flow Constraint Solving": {
        "type": "article",
        "key": "10.1109/TSE.2023.3321381",
        "author": "Wang, Yang and Zhang, Peng and Sun, Maolin and Lu, Zeyu and Yang, Yibiao and Tang, Yutian and Qian, Junyan and Li, Zhi and Zhou, Yuming",
        "title": "Uncovering Bugs in Code Coverage Profilers via Control Flow Constraint Solving",
        "year": "2023",
        "issue_date": "Nov. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "11",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3321381",
        "doi": "10.1109/TSE.2023.3321381",
        "abstract": "Code coverage has been widely used as the basis for various software quality assurance techniques. Therefore, it is of great importance to ensure that coverage profilers provide reliable code coverage. However, it is challenging to validate the correctness of the code coverage generated due to the lack of an effective oracle. In this paper, we propose an effective approach based on control flow constraint solving to test coverage profilers and have implemented a coverage bug hunting tool, DOG (finD cOverage buGs). Our core idea is to leverage inherent control flow features to generate control flow constraints that the resulting coverage statistics should respect. If DOG identifies any unsatisfiable constraints, it signifies the presence of incorrect coverage statistics. In such cases, DOG provides detailed diagnostic information about the suspicious coverage statistics for manual inspection. Compared with the state-of-the-art works, DOG has the following prominent advantages: (1) wide applicability: DOG eliminates the need for multiple coverage profilers (as required by differential testing) and program variants (as needed in metamorphic testing), making it highly versatile; (2) unique testing capability: DOG effectively analyzes and utilizes relationships among available coverage statistics, boosting its testing capabilities; and (3) enhanced interpretability: DOG provides clear control flow explanations for incorrect code coverage, enabling the localization of suspicious coverage areas. During our testing period with DOG, we successfully identified and reported 27 bugs in Gcov and llvm-cov, both widely-used coverage profilers. Of these, 17 bugs have been confirmed (11 have been fixed), 3 were deemed expected behaviors by developers, and 7 remain unresolved. Remarkably, 21 out of 24 unexpected bugs had been latent for over two and a half years, and nearly half of the coverage bugs (10 out of 24) were undetectable by state-of-the-art coverage profiler validators. These results demonstrate the effectiveness and importance of using DOG to improve the reliability of code coverage profilers.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "oct",
        "pages": "4964\u20134987",
        "numpages": "24"
    },
    "Augmenting Diffs With Runtime Information": {
        "type": "article",
        "key": "10.1109/TSE.2023.3324258",
        "author": "Etemadi, Khashayar and Sharma, Aman and Madeiral, Fernanda and Monperrus, Martin",
        "title": "Augmenting Diffs With Runtime Information",
        "year": "2023",
        "issue_date": "Nov. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "11",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3324258",
        "doi": "10.1109/TSE.2023.3324258",
        "abstract": "Source code diffs are used on a daily basis as part of code review, inspection, and auditing. To facilitate understanding, they are typically accompanied by explanations that describe the essence of what is changed in the program. As manually crafting high-quality explanations is a cumbersome task, researchers have proposed automatic techniques to generate code diff explanations. Existing explanation generation methods solely focus on static analysis, i.e., they do not take advantage of runtime information to explain code changes. In this article, we propose &lt;sc&gt;Collector-Sahab&lt;/sc&gt;, a novel tool that augments code diffs with runtime difference information. &lt;sc&gt;Collector-Sahab&lt;/sc&gt; compares the program states of the original (old) and patched (new) versions of a program to find unique variable values. Then, &lt;sc&gt;Collector-Sahab&lt;/sc&gt; adds this novel runtime information to the source code diff as shown, for instance, in code reviewing systems. As an evaluation, we run &lt;sc&gt;Collector-Sahab&lt;/sc&gt; on &lt;sc&gt;584&lt;/sc&gt; code diffs for Defects4J bugs and find it successfully augments the code diff for 95\\% (555/&lt;sc&gt;584&lt;/sc&gt;) of them. We also perform a user study and ask eight participants to score the augmented code diffs generated by &lt;sc&gt;Collector-Sahab&lt;/sc&gt;. Per this user study, we conclude that developers find the idea of adding runtime data to code diffs promising and useful. Overall, our experiments show the effectiveness and usefulness of &lt;sc&gt;Collector-Sahab&lt;/sc&gt; in augmenting code diffs with runtime difference information. &lt;bold&gt;Publicly-available repository:&lt;/bold&gt; &lt;uri&gt;https://github.com/ASSERT-KTH/collector-sahab&lt;/uri&gt;.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "oct",
        "pages": "4988\u20135007",
        "numpages": "20"
    },
    "RefactorScore: Evaluating Refactor Prone Code": {
        "type": "article",
        "key": "10.1109/TSE.2023.3324613",
        "author": "Jesse, Kevin and Kuhmuench, Christoph and Sawant, Anand",
        "title": "RefactorScore: Evaluating Refactor Prone Code",
        "year": "2023",
        "issue_date": "Nov. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "11",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3324613",
        "doi": "10.1109/TSE.2023.3324613",
        "abstract": "We propose &lt;sc&gt;RefactorScore&lt;/sc&gt;, an automatic evaluation metric for code. &lt;sc&gt;RefactorScore&lt;/sc&gt; computes the number of refactor prone locations on each token in a candidate file and maps the occurrences into a quantile to produce a score. &lt;sc&gt;RefactorScore&lt;/sc&gt; is evaluated across 61,735 commits and uses a model called &lt;sc&gt;RefactorBERT&lt;/sc&gt; trained to predict refactors on 1,111,246 commits. Finally, we validate &lt;sc&gt;RefactorScore&lt;/sc&gt; on a set of industry leading projects providing each with a &lt;sc&gt;RefactorScore&lt;/sc&gt;. We calibrate &lt;sc&gt;RefactorScore&lt;/sc&gt;'s detection of low quality code with human developers through a human subject study. &lt;sc&gt;RefactorBERT&lt;/sc&gt;, the model driving the scoring mechanism, is capable of predicting defects and refactors predicted by &lt;sc&gt;RefDiff 2.0&lt;/sc&gt;. To our knowledge, our approach, coupled with the use of large scale data for training and validated with human developers, is the first code quality scoring metric of its kind.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "oct",
        "pages": "5008\u20135026",
        "numpages": "19"
    },
    "Automatic Specialization of Third-Party Java Dependencies": {
        "type": "article",
        "key": "10.1109/TSE.2023.3324950",
        "author": "Soto-Valero, C\\'{e}sar and Tiwari, Deepika and Toady, Tim and Baudry, Benoit",
        "title": "Automatic Specialization of Third-Party Java Dependencies",
        "year": "2023",
        "issue_date": "Nov. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "11",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3324950",
        "doi": "10.1109/TSE.2023.3324950",
        "abstract": "Large-scale code reuse significantly reduces both development costs and time. However, the massive share of third-party code in software projects poses new challenges, especially in terms of maintenance and security. In this paper, we propose a novel technique to specialize dependencies of Java projects, based on their actual usage. Given a project and its dependencies, we systematically identify the subset of each dependency that is necessary to build the project, and we remove the rest. As a result of this process, we package each specialized dependency in a &lt;monospace&gt;JAR&lt;/monospace&gt; file. Then, we generate specialized dependency trees where the original dependencies are replaced by the specialized versions. This allows building the project with significantly less third-party code than the original. As a result, the specialized dependencies become a first-class concept in the software supply chain, rather than a transient artifact in an optimizing compiler toolchain. We implement our technique in a tool called &lt;sc&gt;DepTrim&lt;/sc&gt;, which we evaluate with 30 notable open-source Java projects. &lt;sc&gt;DepTrim&lt;/sc&gt; specializes a total of 343 (86.6%) dependencies across these projects, and successfully rebuilds each project with a specialized dependency tree. Moreover, through this specialization, &lt;sc&gt;DepTrim&lt;/sc&gt; removes a total of 57,444 (42.2%) classes from the dependencies, reducing the ratio of dependency classes to project classes from 8.7&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$boldsymbol{times}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo mathvariant=\"bold\"&gt;\u00d7&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"sotovalero-ieq1-3324950.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; in the original projects to 5.0&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$boldsymbol{times}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo mathvariant=\"bold\"&gt;\u00d7&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"sotovalero-ieq2-3324950.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; after specialization. These novel results indicate that dependency specialization significantly reduces the share of third-party code in Java projects.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "oct",
        "pages": "5027\u20135045",
        "numpages": "19"
    },
    "DeepManeuver: Adversarial Test Generation for Trajectory Manipulation of Autonomous Vehicles": {
        "type": "article",
        "key": "10.1109/TSE.2023.3301443",
        "author": "von Stein, Meriel and Shriver, David and Elbaum, Sebastian",
        "title": "DeepManeuver: Adversarial Test Generation for Trajectory Manipulation of Autonomous Vehicles",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3301443",
        "doi": "10.1109/TSE.2023.3301443",
        "abstract": "Adversarial test generation techniques aim to produce input perturbations that cause a DNN to compute incorrect outputs. For autonomous vehicles driven by a DNN, however, the effect of such perturbations are attenuated by other parts of the system and are less effective as vehicle state evolves. In this work we argue that for adversarial testing perturbations to be effective on autonomous vehicles, they must account for the subtle interplay between the DNN and vehicle states. Building on that insight, we develop DeepManeuver, an automated framework that interleaves adversarial test generation with vehicle trajectory physics simulation. Thus, as the vehicle moves along a trajectory, DeepManeuver enables the refinement of candidate perturbations to: (1) account for changes in the state of the vehicle that may affect how the perturbation is perceived by the system; (2) retain the effect of the perturbation on previous states so that the current state is still reachable and past trajectory is preserved; and (3) result in multi-target maneuvers that require fulfillment of vehicle state sequences (e.g. reaching locations in a road to navigate a tight turn). Our assessment reveals that DeepManeuver can generate perturbations to force maneuvers more effectively and consistently than state-of-the-art techniques by 20.7 percentage points on average. We also show DeepManeuver's effectiveness at disrupting vehicle behavior to achieve multi-target maneuvers with a minimum 52\\% rate of success.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4496\u20134509",
        "numpages": "14"
    },
    "Runtime Verification of Crypto APIs: An Empirical Study": {
        "type": "article",
        "key": "10.1109/TSE.2023.3301660",
        "author": "Torres, Adriano and Costa, Pedro and Amaral, Luis and Pastro, Jonata and Bonif\\'{a}cio, Rodrigo and d'Amorim, Marcelo and Legunsen, Owolabi and Bodden, Eric and Dias Canedo, Edna",
        "title": "Runtime Verification of Crypto APIs: An Empirical Study",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3301660",
        "doi": "10.1109/TSE.2023.3301660",
        "abstract": "Misuse of cryptographic (crypto) APIs is a noteworthy cause of security vulnerabilities. For this reason, static analyzers were recently proposed for detecting crypto API misuses. They differ in strengths and weaknesses, and they might miss bugs. Motivated by the inherent limitations of static analyzers, this article reports on a study of runtime verification (RV) as a dynamic-analysis-based alternative for crypto API misuse detection. RV monitors program runs against formal specifications; it was shown to be effective and efficient for amplifying the bug-finding ability of software tests. We focus on the popular JCA crypto API and write 22 RV specifications based on expert-validated rules in a static analyzer. We monitor these specifications while running tests in five benchmarks. Lastly, we compare the accuracy of our RV-based approach, RVSec, with those of three state-of-the-art crypto API misuses detectors: CogniCrypt, CryptoGuard, and CryLogger. Results show that RVSec has higher accuracy in four benchmarks and is on par with CryptoGuard in the fifth. Overall, RVSec achieves an average &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${boldsymbol{F}_{1}$&lt;/tex-math&gt;&lt;alternatives&gt; &lt;mml:math&gt; &lt;mml:msub&gt; &lt;mml:mrow&gt; &lt;mml:mi mathvariant=\"bold-italic\"&gt;F&lt;/mml:mi&gt; &lt;/mml:mrow&gt; &lt;mml:mrow&gt; &lt;mml:mn&gt;1&lt;/mml:mn&gt; &lt;/mml:mrow&gt; &lt;/mml:msub&gt; &lt;/mml:math&gt; &lt;inline-graphic xlink:href=\"bonifacio-ieq1-3301660.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; measure of 95\\%, compared with 83\\%, 78\\%, and 86\\% for CogniCrypt, CryptoGuard, and CryLogger, respectively. We highlight the strengths and limitations of these tools and show that RV is effective for detecting crypto API misuses. We also discuss how static and dynamic analysis can complement each other for detecting crypto API misuses.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4510\u20134525",
        "numpages": "16"
    },
    "Human-in-the-Loop Automatic Program Repair": {
        "type": "article",
        "key": "10.1109/TSE.2023.3305052",
        "author": "Geethal, Charaka and B\\\"{o}hme, Marcel and Pham, Van-Thuan",
        "title": "Human-in-the-Loop Automatic Program Repair",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3305052",
        "doi": "10.1109/TSE.2023.3305052",
        "abstract": "&lt;sc&gt;learn2fix&lt;/sc&gt; is a &lt;italic&gt;human-in-the-loop interactive program repair&lt;/italic&gt; technique, which can be applied when no bug oracle\u2014except the user who is reporting the bug\u2014is available. This approach incrementally learns the condition under which the bug is observed by systematic negotiation with the user. In this process, &lt;sc&gt;learn2fix&lt;/sc&gt; generates alternative test inputs and sends some of those to the user for obtaining their labels. A limited query budget is assigned to the user for this task. A &lt;italic&gt;query&lt;/italic&gt; is a &lt;italic&gt;Yes/No&lt;/italic&gt; question: \u201cWhen executing this alternative test input, the program under test produces the following output; is the bug observed?\u201d. Using the labelled test inputs, &lt;sc&gt;learn2fix&lt;/sc&gt; incrementally learns an &lt;italic&gt;automatic bug oracle&lt;/italic&gt; to predict the user's response. A classification algorithm in machine learning is used for this task. Our key challenge is to maximise the oracle's accuracy in predicting the tests that expose the bug given a practical, small budget of queries. After learning the automatic oracle, an existing program repair tool attempts to repair the bug using the alternative tests that the user has labelled. Our experiments demonstrate that &lt;sc&gt;learn2fix&lt;/sc&gt; trains a sufficiently accurate automatic oracle with a reasonably low labelling effort (lt. 20 queries), and the oracles represented by &lt;italic&gt;interpolation-based&lt;/italic&gt; classifiers produce more accurate predictions than those represented by &lt;italic&gt;approximation-based&lt;/italic&gt; classifiers. Given the user-labelled test inputs, generated using the interpolation-based approach, the &lt;italic&gt;GenProg&lt;/italic&gt; and &lt;italic&gt;Angelix&lt;/italic&gt; automatic program repair tools produce patches that pass a much larger proportion of validation tests than the manually constructed test suites provided by the repair benchmark.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4526\u20134549",
        "numpages": "24"
    },
    "VulExplainer: A Transformer-Based Hierarchical Distillation for Explaining Vulnerability Types": {
        "type": "article",
        "key": "10.1109/TSE.2023.3305244",
        "author": "Fu, Michael and Nguyen, Van and Tantithamthavorn, Chakkrit Kla and Le, Trung and Phung, Dinh",
        "title": "VulExplainer: A Transformer-Based Hierarchical Distillation for Explaining Vulnerability Types",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3305244",
        "doi": "10.1109/TSE.2023.3305244",
        "abstract": "Deep learning-based vulnerability prediction approaches are proposed to help under-resourced security practitioners to detect vulnerable functions. However, security practitioners still do not know what type of vulnerabilities correspond to a given prediction (aka CWE-ID). Thus, a novel approach to explain the type of vulnerabilities for a given prediction is imperative. In this paper, we propose &lt;italic&gt;VulExplainer&lt;/italic&gt;, an approach to explain the type of vulnerabilities. We represent &lt;italic&gt;VulExplainer&lt;/italic&gt; as a vulnerability classification task. However, vulnerabilities have diverse characteristics (i.e., CWE-IDs) and the number of labeled samples in each CWE-ID is highly imbalanced (known as a highly imbalanced multi-class classification problem), which often lead to inaccurate predictions. Thus, we introduce a Transformer-based hierarchical distillation for software vulnerability classification in order to address the highly imbalanced types of software vulnerabilities. Specifically, we split a complex label distribution into sub-distributions based on CWE abstract types (i.e., categorizations that group similar CWE-IDs). Thus, similar CWE-IDs can be grouped and each group will have a more balanced label distribution. We learn TextCNN teachers on each of the simplified distributions respectively, however, they only perform well in their group. Thus, we build a transformer student model to generalize the performance of TextCNN teachers through our hierarchical knowledge distillation framework. Through an extensive evaluation using the real-world 8,636 vulnerabilities, our approach outperforms all of the baselines by 5\\%\u201329\\%. The results also demonstrate that our approach can be applied to Transformer-based architectures such as CodeBERT, GraphCodeBERT, and CodeGPT. Moreover, our method maintains compatibility with any Transformer-based model without requiring any architectural modifications but only adds a special distillation token to the input. These results highlight our significant contributions towards the fundamental and practical problem of explaining software vulnerability.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4550\u20134565",
        "numpages": "16"
    },
    "An Effective Approach to High Strength Covering Array Generation in Combinatorial Testing": {
        "type": "article",
        "key": "10.1109/TSE.2023.3306461",
        "author": "Guo, Xu and Song, Xiaoyu and Zhou, Jian-tao and Wang, Feiyu and Tang, Kecheng",
        "title": "An Effective Approach to High Strength Covering Array Generation in Combinatorial Testing",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3306461",
        "doi": "10.1109/TSE.2023.3306461",
        "abstract": "Combinatorial testing (CT) is an effective testing method that can detect failures caused by the interaction of parameters of the software under test (SUT). With the increasing complexity of SUT and the parameters involved, the variable strength test suite supporting high strength interaction is challenging in a practical testing scenario. This paper presents a multi-learning-based quantum particle swarm optimization (IQIPSO) for high and variable strength covering array generation (VSCAG). Specifically, a specially designed data structure and several combination location methods are proposed to support and speed up the high-strength VSCAG. Besides, multi-learning strategies, including Lamarckian and Baldwinian learning, are applied to IQIPSO to address the premature convergence leading to a large test suite size. Studies for parameter settings of IQIPSO are presented systematically. The IQIPSO method successfully builds test suites where strength is up to 15 and totally reports 13 new best test suite size records. Extensive experiments demonstrate that IQIPSO tends to outperform most other existing methods.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4566\u20134593",
        "numpages": "28"
    },
    "ADPTriage: Approximate Dynamic Programming for Bug Triage": {
        "type": "article",
        "key": "10.1109/TSE.2023.3307243",
        "author": "Jahanshahi, Hadi and Cevik, Mucahit and Mousavi, Kianoush and Ba\\c{s}ar, Ay\\c{s}e",
        "title": "ADPTriage: Approximate Dynamic Programming for Bug Triage",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3307243",
        "doi": "10.1109/TSE.2023.3307243",
        "abstract": "Bug triaging is a critical task in any software development project. It entails triagers going over a list of open bugs, deciding whether each is required to be addressed, and, if so, which developer should fix it. However, the manual bug assignment in Issue Tracking Systems (ITS) offers only a limited solution and might easily fail when triagers are required to handle a large number of bug reports. During the automated assignment, there are multiple sources of uncertainties in the ITS, which should be addressed meticulously. In this study, we develop a Markov decision process (MDP) model for an online bug triage problem. In addition to an optimization-based myopic technique, we provide an ADP-based bug triage solution, called ADPTriage, which has the ability to reflect the downstream uncertainty in the bug arrivals and developers\u2019 timetables. Specifically, without placing any limits on the underlying stochastic process, this technique enables real-time decision-making on bug assignments while taking into consideration developers\u2019 expertise, bug type, and bug fixing time. Our result shows a significant improvement over the myopic approach in terms of assignment accuracy and fixing time. We also demonstrate the empirical convergence of the model and conduct sensitivity analysis with various model parameters. Accordingly, this work constitutes a significant step forward in addressing the uncertainty in bug triage.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4594\u20134609",
        "numpages": "16"
    },
    "An Empirical Study on the Effectiveness of Privacy Indicators": {
        "type": "article",
        "key": "10.1109/TSE.2023.3308392",
        "author": "Guerra, Michele and Scalabrino, Simone and Fasano, Fausto and Oliveto, Rocco",
        "title": "An Empirical Study on the Effectiveness of Privacy Indicators",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3308392",
        "doi": "10.1109/TSE.2023.3308392",
        "abstract": "The increasing diffusion of mobile devices and their integration with sophisticated hardware and software components has promoted the development of numerous applications in which developers find new ingenious ways to exploit the possibilities offered by the access to resources such as cameras, biometric sensors, and GPS receivers. As a result, we are increasingly used to seeing applications that make extensive use of sensitive resources, potentially dangerous for our privacy. To address this problem, the latest approach to support user awareness in terms of privacy is represented by the Privacy Indicators (PI), a software solution implemented by the operating system to provide a visual stimulus to inform users whenever a dangerous resource is exploited by the app. However, the effectiveness of this approach has not been assessed yet. In this article, we present the result of a study on the effectiveness of using the PI to inform the user every time an app accesses the mobile device camera or microphone. We have chosen these two resources as the PI are currently implemented only for a very limited number of permissions. The controlled experiment involved 122 Android users who were asked to complete a series of tasks on their smartphone through prototypes using the involved resources in an explicit and latent way. Although the PI mechanism is very similar between Android and iOS, we have decided to focus on the former due to its greater diffusion. The results show no significant correlation between the use of PI and the detection of the resource being used by the app, suggesting that the effectiveness of PI in improving sensitive-related resources usage awareness, as currently implemented, is still unsatisfactory. In order to understand if the problem was due to the specific implementation of the PI, we implemented an enhanced version and compared it with the standard one. The results confirmed that an implementation that makes the indicators more visible and that is clearer in highlighting the fact that the app is accessing a resource improves resources usage awareness.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4610\u20134623",
        "numpages": "14"
    },
    "Multi-Misconfiguration Diagnosis via Identifying Correlated Configuration Parameters": {
        "type": "article",
        "key": "10.1109/TSE.2023.3308755",
        "author": "Zhou, Yingnan and Hu, Xue and Xu, Sihan and Jia, Yan and Liu, Yuhao and Wang, Junyong and Xu, Guangquan and Wang, Wei and Liu, Shaoying and Baker, Thar",
        "title": "Multi-Misconfiguration Diagnosis via Identifying Correlated Configuration Parameters",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3308755",
        "doi": "10.1109/TSE.2023.3308755",
        "abstract": "Software configuration requires that the user sets appropriate values to specified variables, known as configuration parameters, which potentially affect the behaviors of software system. It is an essential means for software reliability, but how to ensure correct configurations remains a great challenge, especially when a large number of parameter settings are involved. Existing studies on misconfiguration diagnosis treat all configurations independently, ignoring the constraints and correlations among different configurations. In this article, we reveal the phenomenon of multi-misconfigurations and present a tool, MMD, for multi-misconfigurations diagnosis. Specifically, MMD consists of two modules: Correlated Configurations Analysis and Primary Misconfigurations Diagnosis. The former determines the correlation among each pair of configurations by analyzing the control and data flows related to each configuration. The latter is responsible for collecting a list of configurations ranked according to their suspiciousness. Combining the outputs of two modules, MMD is able to assist the user in multi-misconfigurations diagnosis. We evaluate MMD on seven popular Java projects: Randoop, Soot, Synoptic, Hdfs, Hbase, Yarn, and Zookeeper. MMD identifies 510 configuration correlations with a 4.9\\% false positive rate. Furthermore, it effectively diagnoses 22 multi-misconfigurations collected from StackOverflow, outperforming two state-of-the-art baselines.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4624\u20134638",
        "numpages": "15"
    },
    "Do Pretrained Language Models Indeed Understand Software Engineering Tasks?": {
        "type": "article",
        "key": "10.1109/TSE.2023.3308952",
        "author": "Li, Yao and Zhang, Tao and Luo, Xiapu and Cai, Haipeng and Fang, Sen and Yuan, Dawei",
        "title": "Do Pretrained Language Models Indeed Understand Software Engineering Tasks?",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3308952",
        "doi": "10.1109/TSE.2023.3308952",
        "abstract": "Artificial intelligence (AI) for software engineering (SE) tasks has recently achieved promising performance. In this article, we investigate to what extent the pre-trained language model truly understands those SE tasks such as code search, code summarization, etc. We conduct a comprehensive empirical study on a board set of AI for SE (AI4SE) tasks by feeding them with variant inputs: 1) with various masking rates and 2) with sufficient input subset method. Then, the trained models are evaluated on different SE tasks, including code search, code summarization, and duplicate bug report detection. Our experimental results show that pre-trained language models are insensitive to the given input, thus they achieve similar performance in these three SE tasks. We refer to this phenomenon as &lt;italic&gt;overinterpretation&lt;/italic&gt;, where a model confidently makes a decision without salient features, or where a model finds some irrelevant relationships between the final decision and the dataset. Our study investigates two approaches to mitigate the overinterpretation phenomenon: whole word mask strategy and ensembling. To the best of our knowledge, we are the &lt;italic&gt;first&lt;/italic&gt; to reveal this overinterpretation phenomenon to the AI4SE community, which is an important reminder for researchers to design the input for the models and calls for necessary future work in understanding and implementing AI4SE tasks.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4639\u20134655",
        "numpages": "17"
    },
    "&lt;sc&gt;scenoRITA&lt;/sc&gt;: Generating Diverse, Fully Mutable, Test Scenarios for Autonomous Vehicle Planning": {
        "type": "article",
        "key": "10.1109/TSE.2023.3309610",
        "author": "Huai, Yuqi and Almanee, Sumaya and Chen, Yuntianyi and Wu, Xiafa and Chen, Qi Alfred and Garcia, Joshua",
        "title": "&lt;sc&gt;scenoRITA&lt;/sc&gt;: Generating Diverse, Fully Mutable, Test Scenarios for Autonomous Vehicle Planning",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3309610",
        "doi": "10.1109/TSE.2023.3309610",
        "abstract": "Autonomous Vehicles (AVs) leverage advanced sensing and networking technologies (e.g., camera, LiDAR, RADAR, GPS, DSRC, 5G, etc.) to enable safe and efficient driving without human drivers. Although still in its infancy, AV technology is becoming increasingly common and could radically transform our transportation system and by extension, our economy and society. As a result, there is tremendous global enthusiasm for research, development, and deployment of AVs, e.g., self-driving taxis and trucks from Waymo and Baidu. The current practice for testing AVs uses virtual tests\u2014where AVs are tested in software simulations\u2014since they offer a more efficient and safer alternative compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, systematically creating &lt;italic&gt;valid&lt;/italic&gt; and &lt;italic&gt;effective&lt;/italic&gt; tests for AV software remains a major challenge. To address this challenge, we introduce &lt;sc&gt;scenoRITA&lt;/sc&gt;, a test generation approach for AVs that uses an evolutionary algorithm with (1) a novel gene representation that allows obstacles to be &lt;italic&gt;fully mutable&lt;/italic&gt;, hence, resulting in more reported violations and more diverse scenarios, (2) 5 test oracles to determine both safety and motion sickness-inducing violations and (3) a novel technique to identify and eliminate duplicate tests. Our extensive evaluation shows that &lt;sc&gt;scenoRITA&lt;/sc&gt; can produce test scenarios that are more effective in revealing ADS bugs and more diverse in covering different parts of the map compared to other state-of-the-art test generation approaches.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4656\u20134676",
        "numpages": "21"
    },
    "CombTransformers: Statement-Wise Transformers for Statement-Wise Representations": {
        "type": "article",
        "key": "10.1109/TSE.2023.3310793",
        "author": "Bertolotti, Francesco and Cazzola, Walter",
        "title": "CombTransformers: Statement-Wise Transformers for Statement-Wise Representations",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3310793",
        "doi": "10.1109/TSE.2023.3310793",
        "abstract": "This study presents a novel category of Transformer architectures known as comb transformers, which effectively reduce the space complexity of the self-attention layer from a quadratic to a subquadratic level. This is achieved by processing sequence segments independently and incorporating &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathcal{X}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"script\"&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"cazzola-ieq1-3310793.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-word embeddings to merge cross-segment information. The reduction in attention memory requirements enables the deployment of deeper architectures, potentially leading to more competitive outcomes. Furthermore, we design an abstract syntax tree (AST)-based code representation to effectively exploit comb transformer properties. To explore the potential of our approach, we develop nine specific instances based on three popular architectural concepts: funnel, hourglass, and encoder-decoder. These architectures are subsequently trained on three code-related tasks: method name generation, code search, and code summarization. These tasks encompass a range of capabilities: short/long sequence generation and classification. In addition to the proposed comb transformers, we also evaluate several baseline architectures for comparative analysis. Our findings demonstrate that the comb transformers match the performance of the baselines and frequently perform better.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4677\u20134690",
        "numpages": "14"
    },
    "DexBERT: Effective, Task-Agnostic and Fine-Grained Representation Learning of Android Bytecode": {
        "type": "article",
        "key": "10.1109/TSE.2023.3310874",
        "author": "Sun, Tiezhu and Allix, Kevin and Kim, Kisub and Zhou, Xin and Kim, Dongsun and Lo, David and Bissyand\\'{e}, Tegawend\\'{e} F. and Klein, Jacques",
        "title": "DexBERT: Effective, Task-Agnostic and Fine-Grained Representation Learning of Android Bytecode",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3310874",
        "doi": "10.1109/TSE.2023.3310874",
        "abstract": "The automation of an increasingly large number of software engineering tasks is becoming possible thanks to Machine Learning (ML). One foundational building block in the application of ML to software artifacts is the &lt;italic&gt;representation&lt;/italic&gt; of these artifacts (&lt;italic&gt;e.g.&lt;/italic&gt;, source code or executable code) into a form that is suitable for learning. Traditionally, researchers and practitioners have relied on manually selected features, based on expert knowledge, for the task at hand. Such knowledge is sometimes imprecise and generally incomplete. To overcome this limitation, many studies have leveraged representation learning, delegating to ML itself the job of automatically devising suitable representations and selections of the most relevant features. Yet, in the context of Android problems, existing models are either limited to coarse-grained whole-app level (&lt;italic&gt;e.g.&lt;/italic&gt;, &lt;monospace&gt;apk2vec&lt;/monospace&gt;) or conducted for one specific downstream task (&lt;italic&gt;e.g.&lt;/italic&gt;, &lt;monospace&gt;smali2vec&lt;/monospace&gt;). Thus, the produced representation may turn out to be unsuitable for fine-grained tasks or cannot generalize beyond the task that they have been trained on. Our work is part of a new line of research that investigates effective, task-agnostic, and fine-grained universal representations of bytecode to mitigate both of these two limitations. Such representations aim to capture information relevant to various low-level downstream tasks (&lt;italic&gt;e.g.&lt;/italic&gt;, at the class-level). We are inspired by the field of Natural Language Processing, where the problem of universal representation was addressed by building Universal Language Models, such as BERT, whose goal is to capture abstract semantic information about sentences, in a way that is reusable for a variety of tasks. We propose DexBERT, a BERT-like Language Model dedicated to representing chunks of DEX bytecode, the main binary format used in Android applications. We empirically assess whether DexBERT is able to model the DEX &lt;italic&gt;language&lt;/italic&gt; and evaluate the suitability of our model in three distinct class-level software engineering tasks: Malicious Code Localization, Defect Prediction, and Component Type Classification. We also experiment with strategies to deal with the problem of catering to apps having vastly different sizes, and we demonstrate one example of using our technique to investigate what information is relevant to a given task.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4691\u20134706",
        "numpages": "16"
    },
    "Fast Parametric Model Checking With Applications to Software Performability Analysis": {
        "type": "article",
        "key": "10.1109/TSE.2023.3313645",
        "author": "Fang, Xinwei and Calinescu, Radu and Gerasimou, Simos and Alhwikem, Faisal",
        "title": "Fast Parametric Model Checking With Applications to Software Performability Analysis",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3313645",
        "doi": "10.1109/TSE.2023.3313645",
        "abstract": "We present an efficient parametric model checking technique for the analysis of software &lt;italic&gt;performability&lt;/italic&gt;, i.e., of the performance and dependability properties of software systems. The new parametric model checking (pMC) technique works by using a heuristic to automatically decompose a parametric discrete-time Markov chain (pDTMC) model of the software system under verification into fragments that can be analysed independently, yielding results that are then combined to establish the required software performability properties. Our fast parametric model checking (fPMC) technique enables the formal analysis of software systems modelled by pDTMCs that are too complex to be handled by existing pMC methods. Furthermore, for many pDTMCs that state-of-the-art parametric model checkers can analyse, fPMC produces solutions (i.e., algebraic formulae) that are simpler and much faster to evaluate. We show experimentally that adding fPMC to the existing repertoire of pMC methods improves the efficiency of parametric model checking significantly, and extends its applicability to software systems with more complex behaviour than currently possible.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4707\u20134730",
        "numpages": "24"
    },
    "A Grounded Theory of Cross-Community SECOs: Feedback Diversity Versus Synchronization": {
        "type": "article",
        "key": "10.1109/TSE.2023.3313875",
        "author": "Foundjem, Armstrong and Eghan, Ellis E. and Adams, Bram",
        "title": "A Grounded Theory of Cross-Community SECOs: Feedback Diversity Versus Synchronization",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3313875",
        "doi": "10.1109/TSE.2023.3313875",
        "abstract": "Despite their proliferation, growing sustainable software ecosystems (SECOs) remains a substantial challenge. One approach to mitigate this challenge is by collecting and integrating feedback from distributors (distros) and end-users of the SECO releases into future SECO releases, tools, or policies. This paper performs a socio-technical analysis of cross-community collaboration in the OpenStack SECO, which consists of the upstream OpenStack project and 21 distribution (distro) communities. First, we followed Masood et al.'s adaptation of Strauss-Corbinian grounded theory methodology for socio-technical contexts on data from an open-ended unstructured interview, a survey, focus groups, and 384 mailing list threads to investigate how SECOs manage to sustain cross-community collaboration. Our theory has 15 constructs divided into four categories: diverse feedback types and mechanisms (2), characteristics of feedback (2), challenges (7), and the benefits (4) of cross-community collaboration. We then empirically study the salient aspects of the theory, i.e., diversity and synchronization, among 21 OpenStack distros. We empirically mined feedback that distros contribute to upstream, i.e., 140,261 mailing list threads, 142,914 bugs reported, 65,179 bugs resolved, and 4,349 new features. Then, we use influence maximization social network analysis to model the synchronization of feedback in the OpenStack SECO. Our results suggest that distros contribute substantially towards the sustainability of the SECO in the form of 25.6\\% of new features, 30.7\\% of emails, 44.3\\% of bug reports, and 30.7\\% of bug fixes. Finally, we found evidence of distros playing different roles in a SECO, with nine distros contributing all four types of feedback in equal proportions, while 12 distros specialize in one type of feedback. Distros that are influential in propagating a given type of feedback to the SECO community are not necessarily specialized in that feedback type.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4731\u20134750",
        "numpages": "20"
    },
    "An Easy Data Augmentation Approach for Application Reviews Event Inference": {
        "type": "article",
        "key": "10.1109/TSE.2023.3313989",
        "author": "Guo, Shikai and Lin, Haorui and Zhao, Jiaoru and Li, Hui and Chen, Rong and Li, Xiaochen and Jiang, He",
        "title": "An Easy Data Augmentation Approach for Application Reviews Event Inference",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3313989",
        "doi": "10.1109/TSE.2023.3313989",
        "abstract": "Application review event inference aims to assess the effectiveness of application problems in response to user actions, which enables application developers to promptly discover and address potential issues in various applications, thereby improving their development and maintenance efficiency. Despite the development of event inference models for app reviews, which extract them as user action and app problem events and establish a relationship model between events and inference labels, the accuracy of these models is constrained due to limitations in labeling and characterizing noise and the lack of robustness and generalization. To address this challenge, we propose a model called Easy Data Augmentation for Application Reviews Event Inference (short for EDA-AREI), which comprises a denoising component, data augmentation component, and event inference prediction component. Specifically, the denoising component identifies labels and characterizes noisy data to enhance dataset quality, the data augmentation component replaces non-stop words with synonyms to increase textual diversity, and the event inference and prediction component reconstructs the classifier using denoised and augmented data. Experimental results on six datasets of one-star app reviews in the Apple App Store demonstrate that the EDA-AREI method achieves an &lt;italic&gt;Accuracy&lt;/italic&gt; of 71.19\\%, 79.14\\%, 69.05\\%, 69.02\\%, 68.24\\% and 68.48\\%, respectively, representing an improvement of 0.83\\%\u20132.09\\% compared to state-of-the-art models. Regarding the &lt;italic&gt;F1-score&lt;/italic&gt;, EDA-AREI achieves values of 71.30\\%, 69.93\\%, and 68.76\\% on the threshold_0.5, k-means_2, and random datasets, respectively, outperforming state-of-the-art models by 1.89\\%\u20134.02\\%. Furthermore, EDA-AREI achieves &lt;italic&gt;AUC&lt;/italic&gt; values of 75.66\\% and 73.37\\% on the threshold_0.5 and k-means_2 datasets, respectively. As a result, EDA-AREI demonstrates substantial improvements in &lt;italic&gt;Accuracy&lt;/italic&gt;, as well as enhanced &lt;italic&gt;F1-score&lt;/italic&gt; and &lt;italic&gt;AUC&lt;/italic&gt; across most datasets, thereby enhancing the model's accuracy and robustness in identifying related action-problem pairs.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4751\u20134772",
        "numpages": "22"
    },
    "Let's Go to the Whiteboard (Again): Perceptions From Software Architects on Whiteboard Architecture Meetings": {
        "type": "article",
        "key": "10.1109/TSE.2023.3314410",
        "author": "Santana de Almeida, Eduardo and Ahmed, Iftekhar and van der Hoek, Andr\\'{e",
        "title": "Let's Go to the Whiteboard (Again): Perceptions From Software Architects on Whiteboard Architecture Meetings",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3314410",
        "doi": "10.1109/TSE.2023.3314410",
        "abstract": "The whiteboard plays a crucial role in the day-to-day lives of software architects, as they frequently will organize meetings at the whiteboard to discuss a new architecture, some proposed changes to an existing architecture, a mismatch between a prescribed architecture and its code, and more. While much has been studied about software architects, the architectures they produce, and how they produce them, a detailed understanding of these whiteboards meetings is still lacking. In this paper, we contribute a mixed-methods study involving semi-structured interviews and a subsequent survey to understand the perceptions of software architects on whiteboard architecture meetings. We focus on four aspects: (1) why do they hold these meetings, (2) what is the impact of the experience levels of the participants in these meetings, (3) how do the architects document the meetings, and (4) what kinds of changes are made in downstream activities to the work produced after the meetings have concluded? In studying these aspects, we identify eleven observations related to both technical aspects and social aspects of the meetings. These insights have implications for further research, offer concrete advice to practitioners, and suggest ways of educating future software architects.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "oct",
        "pages": "4773\u20134795",
        "numpages": "23"
    },
    "K-ST: A Formal Executable Semantics of the Structured Text Language for PLCs": {
        "type": "article",
        "key": "10.1109/TSE.2023.3315292",
        "author": "Wang, Kun and Wang, Jingyi and Poskitt, Christopher M. and Chen, Xiangxiang and Sun, Jun and Cheng, Peng",
        "title": "K-ST: A Formal Executable Semantics of the Structured Text Language for PLCs",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3315292",
        "doi": "10.1109/TSE.2023.3315292",
        "abstract": "Programmable Logic Controllers (PLCs) are responsible for automating process control in many industrial systems (e.g. in manufacturing and public infrastructure), and thus it is critical to ensure that they operate correctly and safely. The majority of PLCs are programmed in languages such as Structured Text (ST). However, a lack of formal semantics makes it difficult to ascertain the correctness of their translators and compilers, which vary from vendor-to-vendor. In this work, we develop K-ST, a formal executable semantics for ST in the &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$boldsymbol{mathbb{K}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"double-struck\"&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"wang-ieq1-3315292.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; framework. Defined with respect to the IEC 61131-3 standard and PLC vendor manuals, K-ST is a high-level reference semantics that can be used to evaluate the correctness and consistency of different ST implementations. We validate K-ST by executing 567 ST programs extracted from GitHub and comparing the results against existing commercial compilers (i.e., CODESYS, CX-Programmer, and GX Works2). We then apply K-ST to validate the implementation of the open source OpenPLC platform, comparing the executions of several test programs to uncover five bugs and nine functional defects in the compiler.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4796\u20134813",
        "numpages": "18"
    },
    "Hyperparameter Optimization for AST Differencing": {
        "type": "article",
        "key": "10.1109/TSE.2023.3315935",
        "author": "Martinez, Matias and Falleri, Jean-R\\'{e}my and Monperrus, Martin",
        "title": "Hyperparameter Optimization for AST Differencing",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3315935",
        "doi": "10.1109/TSE.2023.3315935",
        "abstract": "Computing the differences between two versions of the same program is an essential task for software development and software evolution research. AST differencing is the most advanced way of doing so, and an active research area. Yet, AST differencing algorithms rely on configuration parameters that may have a strong impact on their effectiveness. In this paper, we present a novel approach named &lt;monospace&gt;DAT&lt;/monospace&gt; (D &lt;italic&gt;iff &lt;underline&gt;A&lt;/underline&gt;uto &lt;underline&gt;T&lt;/underline&gt;uning&lt;/italic&gt;) for hyperparameter optimization of AST differencing. We thoroughly state the problem of hyper-configuration for AST differencing. We evaluate our data-driven approach &lt;monospace&gt;DAT&lt;/monospace&gt; to optimize the edit-scripts generated by the state-of-the-art AST differencing algorithm named GumTree in different scenarios. &lt;monospace&gt;DAT&lt;/monospace&gt; is able to find a new configuration for GumTree that improves the edit-scripts in 21.8\\% of the evaluated cases.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "oct",
        "pages": "4814\u20134828",
        "numpages": "15"
    },
    "Behavior Trees and State Machines in Robotics Applications": {
        "type": "article",
        "key": "10.1109/TSE.2023.3269081",
        "author": "Ghzouli, Razan and Berger, Thorsten and Johnsen, Einar Broch and Wasowski, Andrzej and Dragule, Swaib",
        "title": "Behavior Trees and State Machines in Robotics Applications",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3269081",
        "doi": "10.1109/TSE.2023.3269081",
        "abstract": "Autonomous robots combine skills to form increasingly complex behaviors, called missions. While skills are often programmed at a relatively low abstraction level, their coordination is architecturally separated and often expressed in higher-level languages or frameworks. State machines have been the go-to language to model behavior for decades, but recently, behavior trees have gained attention among roboticists. Originally designed to model autonomous actors in computer games, behavior trees offer an extensible tree-based representation of missions and are claimed to support modular design and code reuse. Although several implementations of behavior trees are in use, little is known about their usage and scope in the real world. How do concepts offered by behavior trees relate to traditional languages, such as state machines? How are concepts in behavior trees and state machines used in actual applications? This paper is a study of the key language concepts in behavior trees as realized in domain-specific languages (DSLs), internal and external DSLs offered as libraries, and their use in open-source robotic applications supported by the Robot Operating System (ROS). We analyze behavior-tree DSLs and compare them to the standard language for behavior models in robotics: state machines. We identify DSLs for both behavior-modeling languages, and we analyze five in-depth. We mine open-source repositories for robotic applications that use the analyzed DSLs and analyze their usage. We identify similarities between behavior trees and state machines in terms of language design and the concepts offered to accommodate the needs of the robotics domain. We observed that the usage of behavior-tree DSLs in open-source projects is increasing rapidly. We observed similar usage patterns at model structure and at code reuse in the behavior-tree and state-machine models within the mined open-source projects. We contribute all extracted models as a dataset, hoping to inspire the community to use and further develop behavior trees, associated tools, and analysis techniques.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4243\u20134267",
        "numpages": "25"
    },
    "Function Call Graph Context Encoding for Neural Source Code Summarization": {
        "type": "article",
        "key": "10.1109/TSE.2023.3279774",
        "author": "Bansal, Aakash and Eberhart, Zachary and Karas, Zachary and Huang, Yu and McMillan, Collin",
        "title": "Function Call Graph Context Encoding for Neural Source Code Summarization",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3279774",
        "doi": "10.1109/TSE.2023.3279774",
        "abstract": "Source code summarization is the task of writing natural language descriptions of source code. The primary use of these descriptions is in documentation for programmers. Automatic generation of these descriptions is a high value research target due to the time cost to programmers of writing these descriptions themselves. In recent years, a confluence of software engineering and artificial intelligence research has made inroads into automatic source code summarization through applications of neural models of that source code. However, an Achilles\u2019 heel to a vast majority of approaches is that they tend to rely solely on the context provided by the source code being summarized. But empirical studies in program comprehension are quite clear that the information needed to describe code much more often resides in the context in the form of Function Call Graph surrounding that code. In this paper, we present a technique for encoding this call graph context for neural models of code summarization. We implement our approach as a supplement to existing approaches, and show statistically significant improvement over existing approaches. In a human study with 20 programmers, we show that programmers perceive generated summaries to generally be as accurate, readable, and concise as human-written summaries.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4268\u20134281",
        "numpages": "14"
    },
    "NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR": {
        "type": "article",
        "key": "10.1109/TSE.2023.3288901",
        "author": "Cejas, Orlando Amaral and Azeem, Muhammad Ilyas and Abualhaija, Sallam and Briand, Lionel C.",
        "title": "NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3288901",
        "doi": "10.1109/TSE.2023.3288901",
        "abstract": "When the entity processing personal data (the processor) differs from the one collecting personal data (the controller), processing personal data is regulated in Europe by the General Data Protection Regulation (GDPR) through &lt;italic&gt;data processing agreements (DPAs)&lt;/italic&gt;. Checking the compliance of DPAs contributes to the compliance verification of software systems as DPAs are an important source of requirements for software development involving the processing of personal data. However, manually checking whether a given DPA complies with GDPR is challenging as it requires significant time and effort for understanding and identifying DPA-relevant compliance requirements in GDPR and then verifying these requirements in the DPA. Legal texts introduce additional complexity due to convoluted language and inherent ambiguity leading to potential misunderstandings. In this paper, we propose an automated solution to check the compliance of a given DPA against GDPR. In close interaction with legal experts, we first built two artifacts: (i) the \u201cshall\u201d requirements extracted from the GDPR provisions relevant to DPA compliance and (ii) a glossary table defining the legal concepts in the requirements. Then, we developed an automated solution that leverages natural language processing (NLP) technologies to check the compliance of a given DPA against these \u201cshall\u201d requirements. Specifically, our approach automatically generates phrasal-level representations for the textual content of the DPA and compares them against predefined representations of the \u201cshall\u201d requirements. By comparing these two representations, the approach not only assesses whether the DPA is GDPR compliant but it further provides recommendations about missing information in the DPA. Over a dataset of 30 actual DPAs, the approach correctly finds 618 out of 750 genuine violations while raising 76 false violations, and further correctly identifies 524 satisfied requirements. The approach has thus an average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%. Compared to a baseline that relies on off-the-shelf NLP tools, our approach provides an average accuracy gain of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$approx$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u2248&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"amaralcejas-ieq1-3288901.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;20 percentage points. The accuracy of our approach can be improved to &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$approx$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u2248&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"amaralcejas-ieq2-3288901.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;94% with limited manual verification effort.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4282\u20134303",
        "numpages": "22"
    },
    "Self-Admitted Technical Debt in Ethereum Smart Contracts: A Large-Scale Exploratory Study": {
        "type": "article",
        "key": "10.1109/TSE.2023.3289808",
        "author": "Ebrahimi, Amir Mohammad and Oliva, Gustavo A. and Hassan, Ahmed E.",
        "title": "Self-Admitted Technical Debt in Ethereum Smart Contracts: A Large-Scale Exploratory Study",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3289808",
        "doi": "10.1109/TSE.2023.3289808",
        "abstract": "Programmable blockchain platforms such as Ethereum offer unique benefits to application development, including a decentralized infrastructure, tamper-proof transactions, and auditability. These benefits enable new types of applications that can bring competitive advantage to several business segments. Nonetheless, the pressure of time-to-market combined with relatively immature development technologies (e.g., the Solidity programming language), lack of high-quality training resources, and an unclear roadmap for Ethereum creates a context that favors the introduction of technical debt (e.g., code hacks, workarounds, and suboptimal implementations) into application code. In this paper, we study self-admitted technical debt (SATD) in smart contracts. SATD refers to technical debt that is explicitly acknowledged in the source code by developers via code comments. We extract 726 k real-world contracts from Ethereum and apply both quantitative and qualitative methods in order to (i) determine SATD prevalence, (ii) understand the relationship between code cloning and SATD prevalence, and (iii) uncover the different categories of SATD. Our findings reveal that, while SATD is not a widespread phenomenon (1.5% of real-world contracts contain SATD), SATD does occur in extremely relevant contracts (e.g., multi-million contracts). We also observed a strong connection between SATD prevalence and code cloning activities, leading us to conclude that the former cannot be reliably studied without taking the latter into consideration. Finally, we produced a taxonomy for SATD that consists of 6 major and 26 minor categories. We note that several minor categories are bound to the domain of blockchain and smart contracts, including gas-inefficient implementations and Solidity-induced workarounds. Based on our results, we derive a set of practical recommendations for contract developers and introduce open research questions to guide future research on the topic.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4304\u20134323",
        "numpages": "20"
    },
    "Privacy Engineering in the Wild: Understanding the Practitioners\u2019 Mindset, Organizational Aspects, and Current Practices": {
        "type": "article",
        "key": "10.1109/TSE.2023.3290237",
        "author": "Iwaya, Leonardo Horn and Babar, Muhammad Ali and Rashid, Awais",
        "title": "Privacy Engineering in the Wild: Understanding the Practitioners\u2019 Mindset, Organizational Aspects, and Current Practices",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3290237",
        "doi": "10.1109/TSE.2023.3290237",
        "abstract": "Privacy engineering, as an emerging field of research and practice, comprises the technical capabilities and management processes needed to implement, deploy, and operate privacy features and controls in working systems. For that, software practitioners and other stakeholders in software companies need to work cooperatively toward building privacy-preserving businesses and engineering solutions. Significant research has been done to understand the software practitioners\u2019 perceptions of information privacy, but more emphasis should be given to the uptake of concrete privacy engineering components. This research delves into the software practitioners\u2019 perspectives and mindset, organizational aspects, and current practices on privacy and its engineering processes. A total of 30 practitioners from nine countries and backgrounds were interviewed, sharing their experiences and voicing their opinions on a broad range of privacy topics. The thematic analysis methodology was adopted to code the interview data qualitatively and construct a rich and nuanced thematic framework. As a result, we identified three critical interconnected themes that compose our thematic framework for privacy engineering \u201cin the wild\u201d: (1) personal privacy mindset and stance, categorised into practitioners\u2019 privacy knowledge, attitudes and behaviours; (2) organizational privacy aspects, such as decision-power and positive and negative examples of privacy climate; and, (3) privacy engineering practices, such as procedures and controls concretely used in the industry. Among the main findings, this study provides many insights about the state-of-the-practice of privacy engineering, pointing to a positive influence of privacy laws (e.g., EU General Data Protection Regulation) on practitioners\u2019 behaviours and organizations\u2019 cultures. Aspects such as organizational privacy culture and climate were also confirmed to have a powerful influence on the practitioners\u2019 privacy behaviours. A conducive environment for privacy engineering needs to be created, aligning the privacy values of practitioners and their organizations, with particular attention to the leaders and top management's commitment to privacy. Organizations can also facilitate education and awareness training for software practitioners on existing privacy engineering theories, methods and tools that have already been proven effective.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4324\u20134348",
        "numpages": "25"
    },
    "Dealing With Data Challenges When Delivering Data-Intensive Software Solutions": {
        "type": "article",
        "key": "10.1109/TSE.2023.3291003",
        "author": "Graetsch, Ulrike M. and Khalajzadeh, Hourieh and Shahin, Mojtaba and Hoda, Rashina and Grundy, John",
        "title": "Dealing With Data Challenges When Delivering Data-Intensive Software Solutions",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3291003",
        "doi": "10.1109/TSE.2023.3291003",
        "abstract": "The predicted increase in demand for data-intensive solution development is driving the need for software, data, and domain experts to effectively collaborate in multi-disciplinary data-intensive software teams (MDSTs). We conducted a socio-technical grounded theory study through interviews with 24 practitioners in MDSTs to better understand the challenges these teams face when delivering data-intensive software solutions. The interviews provided perspectives across different types of roles including domain, data and software experts, and covered different organisational levels from team members, team managers to executive leaders. We found that the key concern for these teams is dealing with data-related challenges. In this article, we present a theory of dealing with data challenges that explains the &lt;italic&gt;challenges&lt;/italic&gt; faced by MDSTs including gaining access to data, aligning data, understanding data, and resolving data quality issues; the &lt;italic&gt;context&lt;/italic&gt; in and &lt;italic&gt;condition&lt;/italic&gt; under which these challenges occur, the &lt;italic&gt;causes&lt;/italic&gt; that lead to the challenges, and the related &lt;italic&gt;consequences&lt;/italic&gt; such as having to conduct remediation activities, inability to achieve expected outcomes and lack of trust in the delivered solutions. We also identified &lt;italic&gt;contingencies&lt;/italic&gt; or strategies applied to address the challenges including high-level strategic approaches such as implementing data governance, implementing new tools and techniques such as data quality visualisation and monitoring tools, as well as building stronger teams by focusing on people dynamics, communication skill development and cross-skilling. Our findings have direct implications for practitioners and researchers to better understand the landscape of data challenges and how to deal with them.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4349\u20134370",
        "numpages": "22"
    },
    "Incomplete Adaptive Distinguishing Sequences for Non-Deterministic FSMs": {
        "type": "article",
        "key": "10.1109/TSE.2023.3291137",
        "author": "T\\\"{u}rker, Uraz Cengiz and Hierons, Robert M. and Barlas, Gerassimos and El-Fakih, Khaled",
        "title": "Incomplete Adaptive Distinguishing Sequences for Non-Deterministic FSMs",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3291137",
        "doi": "10.1109/TSE.2023.3291137",
        "abstract": "The increasing complexity and criticality of software systems have led to growing interest in automated test generation. One of the most promising approaches is to use model-based testing (MBT), in which test automation is based on a model of the &lt;italic&gt;implementation under test (IUT)&lt;/italic&gt;, with much of the work concerning finite state machine (FSM) models. Many FSM-based test generation techniques use, possibly adaptive, sequences to check the state of the IUT. Of particular interest are adaptive distinguishing sequences (ADSs) because their use can lead to relatively small tests. However, not all systems possess an ADS. In this work, we generalise the notion of incomplete ADSs to non-deterministic partial and observable FSMs. We show that the problem of checking the existence of a set of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$k$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"turker-ieq1-3291137.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; incomplete ADSs that separates every pair of states is PSPACE-hard. Further, we generalise the notion of invertible sequences to non-deterministic partial and observable FSMs and show how invertible sequences can be used to derive additional incomplete ADSs. We propose a novel algorithm to generate incomplete ADSs and describe the results of experiments that evaluated its performance. The results indicate that the proposed method can generate sequences to identify states of the IUT and is faster and can process larger FSMs than other existing methods.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4371\u20134389",
        "numpages": "19"
    },
    "Automated Question Title Reformulation by Mining Modification Logs From Stack Overflow": {
        "type": "article",
        "key": "10.1109/TSE.2023.3292399",
        "author": "Liu, Ke and Chen, Xiang and Chen, Chunyang and Xie, Xiaofei and Cui, Zhanqi",
        "title": "Automated Question Title Reformulation by Mining Modification Logs From Stack Overflow",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3292399",
        "doi": "10.1109/TSE.2023.3292399",
        "abstract": "In Stack Overflow, developers may not clarify and summarize the critical problems in the question titles due to a lack of domain knowledge or poor writing skills. Previous studies mainly focused on automatically generating the question titles by analyzing the posts\u2019 problem descriptions and code snippets. In this study, we aim to improve title quality from the perspective of question title reformulation and propose a novel approach &lt;monospace&gt;QETRA&lt;/monospace&gt; motivated by the findings of our formative study. Specifically, by mining modification logs from Stack Overflow, we first extract title reformulation pairs containing the original title and the reformulated title. Then we resort to multi-task learning by formalizing title reformulation for each programming language as separate but related tasks. Later we adopt a pre-trained model T5 to automatically learn the title reformulation patterns. Automated evaluation and human study both show the competitiveness of &lt;monospace&gt;QETRA&lt;/monospace&gt; after compared with six state-of-the-art baselines. Moreover, our ablation study results also confirm that our studied question title reformulation task is more practical than the direct question title generation task for generating high-quality titles. Finally, we develop a browser plugin based on &lt;monospace&gt;QETRA&lt;/monospace&gt; to facilitate the developers to perform title reformulation. Our study provides a new perspective for studying the quality of post titles and can further generate high-quality titles.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4390\u20134410",
        "numpages": "21"
    },
    "Animation2API: API Recommendation for the Implementation of Android UI Animations": {
        "type": "article",
        "key": "10.1109/TSE.2023.3294971",
        "author": "Wang, Yihui and Liu, Huaxiao and Gao, Shanquan and Tang, Xiao",
        "title": "Animation2API: API Recommendation for the Implementation of Android UI Animations",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3294971",
        "doi": "10.1109/TSE.2023.3294971",
        "abstract": "UI animations, such as card movement and menu slide in/out, provide appealing user experience and enhance the usability of mobile applications. In the process of UI animation implementation, it is difficult for developers to identify suitable APIs for the animation to be implemented from a large number of APIs. Fortunately, the huge app market contains millions of apps, and they can provide valuable data resources for solving this problem. By summarizing the API usage for the same or similar animations in apps, reusable knowledge can be mined for the API recommendation. In this paper, we propose a novel method Animation2API, which mines the knowledge about APIs from existing apps and recommends APIs for UI animations. Different from existing text-based API recommendation approaches, Animation2API takes the UI animation in GIF/video format as query input. Firstly, we construct a database containing mappings between UI animations and APIs by analyzing a broad set of apps. Then, we build a UI animation feature extractor, which can be used to gain temporal-spatial feature vectors of UI animations. By comparing the temporal-spatial feature vectors between UI animations, we identify animations that are similar to the query animation from the database. Finally, we summarize the APIs used for implementing these animations and recommend a list of APIs for developers. The empirical evaluation results show that our method can achieve 82.66\\% &lt;bold&gt;&lt;italic&gt;Success&nbsp;rate&lt;/italic&gt;&lt;/bold&gt; and outperform the baseline Guru by 230.77\\% and 184.95\\% in terms of &lt;bold&gt;&lt;italic&gt;Precision&lt;/italic&gt;&lt;/bold&gt; and &lt;bold&gt;&lt;italic&gt;Recall&lt;/italic&gt;&lt;/bold&gt; when considering twenty APIs. In the user study, we take the scenarios of using web search and ChatGPT to implement animations as baselines, and the results show that participants can complete animations faster (14.54\\%) after using Animation2API. Furthermore, participants\u2019 positive feedbacks on the questionnaire indicate the usefulness of Animation2API.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4411\u20134428",
        "numpages": "18"
    },
    "Code2Img: Tree-Based Image Transformation for Scalable Code Clone Detection": {
        "type": "article",
        "key": "10.1109/TSE.2023.3295801",
        "author": "Hu, Yutao and Fang, Yilin and Sun, Yifan and Jia, Yaru and Wu, Yueming and Zou, Deqing and Jin, Hai",
        "title": "Code2Img: Tree-Based Image Transformation for Scalable Code Clone Detection",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3295801",
        "doi": "10.1109/TSE.2023.3295801",
        "abstract": "Code clone detection is an active research domain of software engineering. There are two core demands for clone detection: scalable detection and complicated clone detection. For scalable detection, existing approaches treat the source code as a text or token sequence and then calculate their similarity. However, the text-based and token-based approaches are difficult to detect complicated clone types due to the lack of consideration of code structure. The methods based on intermediate representations of code can effectively achieve complex clone types detection but are limited by the complexity of representations to be scalable. In this paper, we propose &lt;italic&gt;Code2Img&lt;/italic&gt;, a tree-based code clone detector, which satisfies scalability while detecting complicated clones effectively. Given the source code, we first perform clone filtering by the inverted index to locate the suspected clones. For each suspected clone, we create the adjacency image based on the adjacency matrix of the normalized abstract syntax tree (AST). Then we design an image encoder to highlight the structural details further and refine pixels of the image. Specifically, we employ the Markov model to encode the adjacency image into a state probability image and remove its useless pixels. By this, the original complex tree can be transformed into a one-dimensional vector while preserving the structural feature of the AST. Finally, we detect clones by calculating the Jaccard Similarity of these vectors. We conduct comparative evaluations on effectiveness and scalability with eight other state-of-the-art clone detectors (&lt;italic&gt;SourcererCC&lt;/italic&gt;, &lt;italic&gt;NIL&lt;/italic&gt;, &lt;italic&gt;LVMapper&lt;/italic&gt;, &lt;italic&gt;Nicad&lt;/italic&gt;, &lt;italic&gt;Siamese&lt;/italic&gt;, &lt;italic&gt;CCAligner&lt;/italic&gt;, &lt;italic&gt;Deckard&lt;/italic&gt;, and &lt;italic&gt;Yang2018&lt;/italic&gt;). The experimental results show that &lt;italic&gt;Code2Img&lt;/italic&gt; achieves the best performance among all the comparative tools in terms of both detection effectiveness and scalability. It indicates that &lt;italic&gt;Code2Img&lt;/italic&gt; can be applicable to scalable complicated clone detection.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4429\u20134442",
        "numpages": "14"
    },
    "Optimizing Highly-Parallel Simulation-Based Verification of Cyber-Physical Systems": {
        "type": "article",
        "key": "10.1109/TSE.2023.3298432",
        "author": "Mancini, Toni and Melatti, Igor and Tronci, Enrico",
        "title": "Optimizing Highly-Parallel Simulation-Based Verification of Cyber-Physical Systems",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3298432",
        "doi": "10.1109/TSE.2023.3298432",
        "abstract": "Cyber-Physical Systems (CPSs), comprising both software and physical components, arise in many industry-relevant domains and are often mission- or safety-critical. System-Level Verification (SLV) of CPSs aims at certifying that given (&lt;italic&gt;e.g.&lt;/italic&gt;, safety or liveness) specifications are met, or at estimating the value of some Key Performance Indicators, when the system runs in its operational environment, that is in presence of inputs and/or of additional, uncontrolled disturbances. To enable SLV of complex systems from the early design phases, the currently most adopted approach envisions the &lt;italic&gt;simulation&lt;/italic&gt; of a &lt;italic&gt;system model&lt;/italic&gt; under the (time bounded) &lt;italic&gt;operational scenarios&lt;/italic&gt; deemed of interest. Unfortunately, simulation-based SLV can be computationally prohibitive (years of sequential simulation), since system model simulation is computationally intensive and the set of scenarios of interest can be extremely large. In this article, we present a technique that, given a collection of scenarios of interest (extracted from databases or from symbolic structures), computes &lt;italic&gt;parallel shortest simulation campaigns&lt;/italic&gt;, which drive a possibly large number of system model simulators running in parallel in a HPC infrastructure through all (and only) those scenarios in the user-defined (possibly random) order, by wisely avoiding multiple simulations of repeated trajectories, thus minimising completion time. Our experiments on SLV of Modelica/FMU and Simulink models with up to almost &lt;italic&gt;200 million scenarios&lt;/italic&gt; show that our optimisation yields &lt;italic&gt;speedups as high as&lt;/italic&gt; 8&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$boldsymbol{times}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo mathvariant=\"bold\"&gt;\u00d7&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"mancini-ieq1-3298432.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;. This, together with the enabled &lt;italic&gt;massive parallelisation&lt;/italic&gt;, makes practically viable (a few weeks in a HPC infrastructure) verification tasks (both statistical and exhaustive) which would otherwise take &lt;italic&gt;inconceivably&lt;/italic&gt; long time.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4443\u20134455",
        "numpages": "13"
    },
    "&lt;italic&gt;BiAn:&lt;/italic&gt; Smart Contract Source Code Obfuscation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3298609",
        "author": "Zhang, Pengcheng and Yu, Qifan and Xiao, Yan and Dong, Hai and Luo, Xiapu and Wang, Xiao and Zhang, Meng",
        "title": "&lt;italic&gt;BiAn:&lt;/italic&gt; Smart Contract Source Code Obfuscation",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3298609",
        "doi": "10.1109/TSE.2023.3298609",
        "abstract": "With the rising prominence of smart contracts, security attacks targeting them have increased, posing severe threats to their security and intellectual property rights. Existing simplistic datasets hinder effective vulnerability detection, raising security concerns. To address these challenges, we propose &lt;italic&gt;BiAn&lt;/italic&gt;, a source code level smart contract obfuscation method that generates complex vulnerability test datasets. &lt;italic&gt;BiAn&lt;/italic&gt; protects contracts by obfuscating data flows, control flows, and code layouts, increasing complexity and making it harder for attackers to discover vulnerabilities. Our experiments with buggy contracts showed an average complexity enhancement of approximately 174\\% after obfuscation. Decompilers Vandal and Gigahorse had total failure rate increments of 38.8\\% and 40.5\\% respectively. Obfuscated contracts also decreased vulnerability detection rates in more than 50\\% of cases for ten widely-used static analysis detection tools.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4456\u20134476",
        "numpages": "21"
    },
    "Runtime Evolution of Bitcoin's Consensus Rules": {
        "type": "article",
        "key": "10.1109/TSE.2023.3304851",
        "author": "Notland, Jakob Svennevik and Nowostawski, Mariusz and Li, Jingyue",
        "title": "Runtime Evolution of Bitcoin's Consensus Rules",
        "year": "2023",
        "issue_date": "Sept. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3304851",
        "doi": "10.1109/TSE.2023.3304851",
        "abstract": "The runtime evolution of a system concerns the ability to make changes during runtime without disrupting the service. Blockchain systems need to provide continuous service and integrity. Similar challenges have been observed in centrally controlled distributed systems or mobile applications that handle runtime evolution, mainly by supporting compatible changes or running different versions concurrently. However, these solutions are not applicable in the case of blockchains, and thus, new solutions are required. This study investigates Bitcoin consensus evolution by analysing over a decade of data from Bitcoin's development channels using Strauss\u2019 grounded theory approach and root cause analysis. The results show nine deployment features which form nine deployment techniques and ten lessons learned. Our results illustrate how different deployment techniques fit different contexts and pose different levels of consensus failure risks. Furthermore, we provide guidelines for risk minimisation during consensus rule deployment for blockchain in general and Bitcoin in particular.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4477\u20134495",
        "numpages": "19"
    },
    "&lt;sc&gt;CfgNet&lt;/sc&gt;: A Framework for Tracking Equality-Based Configuration Dependencies Across a Software Project": {
        "type": "article",
        "key": "10.1109/TSE.2023.3274349",
        "author": "Simon, Sebastian and Ruckel, Nicolai and Siegmund, Norbert",
        "title": "&lt;sc&gt;CfgNet&lt;/sc&gt;: A Framework for Tracking Equality-Based Configuration Dependencies Across a Software Project",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3274349",
        "doi": "10.1109/TSE.2023.3274349",
        "abstract": "Modern software development incorporates various technologies, such as containerization, CI/CD pipelines, and build tools, which have to be jointly configured to enable building, testing, deployment, and execution of software systems. The vast configuration space spans several different configuration artifacts with their own syntax and semantics, encoding hundreds of configuration options and their values. The interplay of these technologies requires some level of coordination, which is realized by matching configurations. That is, configuration options and their according values may depend on other options and values from entirely different technologies and artifacts. This creates non-obvious configuration dependencies that are hard to track. The missing awareness and overview of such configuration dependencies across diverse configuration artifacts, tools, and frameworks can lead to dependency conflicts and severe configuration errors. We propose &lt;sc&gt;CfgNet&lt;/sc&gt;, a framework that models the configuration landscape of a software project as a configuration network in an extensible and artifact-independent way. This way, we enable the early detection of possible dependency violations and proactively prevent misconfigurations during software development and maintenance. In a literature study, we found that the most common form of dependencies is the equality of values of different options. Based on this result, we developed an equality-based linker to determine dependent options across different artifacts. To demonstrate the extensibility of our framework, we also implemented nine plugins for popular technologies, such as Maven and Docker. To evaluate our approach, we injected and violated five real-world configuration dependencies extracted from Stack Overflow, which we support with our technology plugins, in five subject systems. &lt;sc&gt;CfgNet&lt;/sc&gt; found all injected dependency violations and four additional ones already present in these systems. Moreover, we applied &lt;sc&gt;CfgNet&lt;/sc&gt; to the commit history of 50 repositories selected from GitHub and found dependency conflicts in about two thirds of these repositories. We manually inspected 883 conflicts, with about 89\u2009% true positives, demonstrating the need to reliably track cross-technology configuration dependencies and prevent their misconfiguration.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "3955\u20133971",
        "numpages": "17"
    },
    "Graph-of-Code: Semantic Clone Detection Using Graph Fingerprints": {
        "type": "article",
        "key": "10.1109/TSE.2023.3276780",
        "author": "Alhazami, Essa A. and Sheneamer, Abdullah M.",
        "title": "Graph-of-Code: Semantic Clone Detection Using Graph Fingerprints",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3276780",
        "doi": "10.1109/TSE.2023.3276780",
        "abstract": "The code clone detection issue has been researched using a number of explicit factors based on the tokens and contents and found effective results. However, exposing code contents may be an impractical option because of privacy and security factors. Moreover, the lack of scalability of past methods is an important challenge. The code flow states can be inferred by code structure and implicitly represented using empirical graphs. The assumption is that modelling of the code clone detection problem can be achieved without the content of the codes being revealed. Here, a Graph-of-Code concept for the code clone detection problem is introduced, which represents codes into graphs. While Graph-of-Code provides structural properties and quantification of its characteristics, it can exclude code contents or tokens to identify the clone type. The aim is to evaluate the impact of graph-of-code structural properties on the performance of code clone detection. This work employs a feature extraction-based approach for unlabelled graphs. The approach generates a \u201cGraph Fingerprint\u201d which represents different topological feature levels. The results of code clone detection indicate that code structure has a significant role in detecting clone types. We found different GoC-models outperform others. The models achieve between 96% to 99% in detecting code clones based on recall, precision, and F1-Score. The GoC approach is capable in detecting code clones with scalable dataset and with preserving codes privacy.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "3972\u20133988",
        "numpages": "17"
    },
    "Designing, Modeling and Analysis of GALS Software Systems": {
        "type": "article",
        "key": "10.1109/TSE.2023.3278055",
        "author": "Zhang, Weiyi and Salcic, Zoran and Malik, Avinash",
        "title": "Designing, Modeling and Analysis of GALS Software Systems",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3278055",
        "doi": "10.1109/TSE.2023.3278055",
        "abstract": "Designing software systems underpinned by a formal model of computation (MoC) is crucial for safety-critical, real-time and all industrial applications as it allows formal analysis of those designs and support for correct by design systems. In this paper, we focus on Globally Asynchronous Locally Synchronous (GALS) software systems and Coloured Petri Nets (CPNs) based approach to formally model and analyse GALS software systems specified in SystemJ GALS programming language. The approach translates SystemJ constructs into CPN modules and composes them into CPN GALS model based on control flow and concurrency specified in the SystemJ program. It preserves GALS MoC by automatically integrating synchronizer modules, asynchronous channel interface modules, and scheduling modules to result in the execution model of SystemJ program equivalent CPN. The created CPN GALS model allows system developers to verify the properties of the design formally with the use of Computation Tree Logic (CTL). An industrial automation example is provided as a use case.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "3989\u20134003",
        "numpages": "15"
    },
    "Automating Dependency Updates in Practice: An Exploratory Study on GitHub Dependabot": {
        "type": "article",
        "key": "10.1109/TSE.2023.3278129",
        "author": "He, Runzhi and He, Hao and Zhang, Yuxia and Zhou, Minghui",
        "title": "Automating Dependency Updates in Practice: An Exploratory Study on GitHub Dependabot",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3278129",
        "doi": "10.1109/TSE.2023.3278129",
        "abstract": "Dependency management bots automatically open pull requests to update software dependencies on behalf of developers. Early research shows that developers are suspicious of updates performed by dependency management bots and feel tired of overwhelming notifications from these bots. Despite this, dependency management bots are becoming increasingly popular. Such contrast motivates us to investigate Dependabot, currently the most visible bot on GitHub, to reveal the effectiveness and limitations of state-of-art dependency management bots. We use exploratory data analysis and a developer survey to evaluate the effectiveness of Dependabot in keeping dependencies up-to-date, interacting with developers, reducing update suspicion, and reducing notification fatigue. We obtain mixed findings. On the positive side, projects do reduce technical lag after Dependabot adoption and developers are highly receptive to its pull requests. On the negative side, its compatibility scores are too scarce to be effective in reducing update suspicion; developers tend to configure Dependabot toward reducing the number of notifications; and 11.3% of projects have deprecated Dependabot in favor of other alternatives. The survey confirms our findings and provides insights into the key missing features of Dependabot. Based on our findings, we derive and summarize the key characteristics of an ideal dependency management bot which can be grouped into four dimensions: configurability, autonomy, transparency, and self-adaptability.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4004\u20134022",
        "numpages": "19"
    },
    "Generalized Coverage Criteria for Combinatorial Sequence Testing": {
        "type": "article",
        "key": "10.1109/TSE.2023.3279570",
        "author": "Elyasaf, Achiya and Farchi, Eitan and Margalit, Oded and Weiss, Gera and Weiss, Yeshayahu",
        "title": "Generalized Coverage Criteria for Combinatorial Sequence Testing",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3279570",
        "doi": "10.1109/TSE.2023.3279570",
        "abstract": "We present a new model-based approach for testing systems that use sequences of actions and assertions as test vectors. Our solution includes a method for quantifying testing quality, a tool for generating high-quality test suites based on the coverage criteria we propose, and a framework for assessing risks. For testing quality, we propose a method that specifies generalized coverage criteria over sequences of actions, which extends previous approaches. Our publicly available tool demonstrates how to extract effective test suites from test plans based on these criteria. We also present a Bayesian approach for measuring the probabilities of bugs or risks, and show how this quantification can help achieve an informed balance between exploitation and exploration in testing. Finally, we provide an empirical evaluation demonstrating the effectiveness of our tool in finding bugs, assessing risks, and achieving coverage.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4023\u20134034",
        "numpages": "12"
    },
    "Multi-Granularity Detector for Vulnerability Fixes": {
        "type": "article",
        "key": "10.1109/TSE.2023.3281275",
        "author": "Nguyen, Truong Giang and Le-Cong, Thanh and Kang, Hong Jin and Widyasari, Ratnadira and Yang, Chengran and Zhao, Zhipeng and Xu, Bowen and Zhou, Jiayuan and Xia, Xin and Hassan, Ahmed E. and Le, Xuan-Bach D. and Lo, David",
        "title": "Multi-Granularity Detector for Vulnerability Fixes",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3281275",
        "doi": "10.1109/TSE.2023.3281275",
        "abstract": "With the increasing reliance on Open Source Software, users are exposed to third-party library vulnerabilities. Software Composition Analysis (SCA) tools have been created to alert users of such vulnerabilities. SCA requires the identification of vulnerability-fixing commits. Prior works have proposed methods that can automatically identify such vulnerability-fixing commits. However, identifying such commits is highly challenging, as only a very small minority of commits are vulnerability fixing. Moreover, code changes can be noisy and difficult to analyze. We observe that noise can occur at different levels of detail, making it challenging to detect vulnerability fixes accurately. To address these challenges and boost the effectiveness of prior works, we propose MiDas (Multi-Granularity Detector for Vulnerability Fixes). Unique from prior works, MiDas constructs different neural networks for each level of code change granularity, corresponding to commit-level, file-level, hunk-level, and line-level, following their natural organization and then use an ensemble model combining all base models to output the final prediction. This design allows MiDas to better cope with the noisy and highly-imbalanced nature of vulnerability-fixing commit data. In addition, to reduce the human effort required to inspect code changes, we have designed an effort-aware adjustment for MiDas's outputs based on commit length. The evaluation result demonstrates that MiDas outperforms the current state-of-the-art baseline on both Java and Python-based datasets in terms of AUC by 4.9% and 13.7%, respectively. Furthermore, in terms of two effort-aware metrics, i.e., EffortCost@L and Popt@L, MiDas also performs better than the state-of-the-art baseline up to 28.2% and 15.9% on Java, 60% and 51.4% on Python, respectively.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4035\u20134057",
        "numpages": "23"
    },
    "&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$\\mathtt {SIEGE}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant=\"monospace\"&gt;SIEGE&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"ma-ieq1-3282981.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;: A Semantics-Guided Safety Enhancement Framework for AI-Enabled Cyber-Physical Systems": {
        "type": "article",
        "key": "10.1109/TSE.2023.3282981",
        "author": "Song, Jiayang and Xie, Xuan and Ma, Lei",
        "title": "&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$\\mathtt {SIEGE}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant=\"monospace\"&gt;SIEGE&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"ma-ieq1-3282981.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;: A Semantics-Guided Safety Enhancement Framework for AI-Enabled Cyber-Physical Systems",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3282981",
        "doi": "10.1109/TSE.2023.3282981",
        "abstract": "Cyber-Physical Systems (CPSs) have been widely adopted in various industry domains to support many important tasks that impact our daily lives, such as automotive vehicles, robotics manufacturing, and energy systems. As Artificial Intelligence (AI) has demonstrated its promising abilities in diverse tasks like decision-making, prediction, and optimization, a growing number of CPSs adopt AI components in the loop to further extend their efficiency and performance. However, these modern AI-enabled CPSs have to tackle pivotal problems that the AI-enabled control systems might need to compensate the balance across &lt;italic&gt;multiple operation requirements&lt;/italic&gt; and avoid possible defections in advance to safeguard human lives and properties. Modular redundancy and ensemble method are two widely adopted solutions in the traditional CPSs and AI communities to enhance the functionality and flexibility of a system. Nevertheless, there is a lack of deep understanding of the effectiveness of such ensemble design on AI-CPSs across diverse industrial applications. Considering the complexity of AI-CPSs, existing ensemble methods fall short of handling such huge state space and sophisticated system dynamics. Furthermore, an ideal control solution should consider the multiple system specifications in real-time and avoid erroneous behaviors beforehand. Such that, a new specification-oriented ensemble control system is of urgent need for AI-CPSs. In this paper, we propose &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathtt {SIEGE}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant=\"monospace\"&gt;SIEGE&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"ma-ieq3-3282981.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, a semantics-guided ensemble control framework to initiate an early exploratory study of ensemble methods on AI-CPSs and aim to construct an efficient, robust, and reliable control solution for multi-tasks AI-CPSs. We first utilize a semantic-based abstraction to decompose the large state space, capture the ongoing system status and predict future conditions in terms of the satisfaction of specifications. We propose a series of new semantics-aware ensemble strategies and an end-to-end Deep Reinforcement Learning (DRL) hierarchical ensemble method to improve the flexibility and reliability of the control systems. Our large-scale, comprehensive evaluations over five subject CPSs show that 1) the semantics abstraction can efficiently narrow the large state space and predict the semantics of incoming states, 2) our semantics-guided methods outperform state-of-the-art individual controllers and traditional ensemble methods, and 3) the DRL hierarchical ensemble approach shows promising capabilities to deliver a more robust, efficient, and safety-assured control system. To enable further research along this direction to build better AI-enabled CPS, we made all of the code and experimental results data publicly. (&lt;uri&gt;https://sites.google.com/view/ai-cps-siege/home&lt;/uri&gt;).",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4058\u20134080",
        "numpages": "23"
    },
    "Task-Oriented ML/DL Library Recommendation Based on a Knowledge Graph": {
        "type": "article",
        "key": "10.1109/TSE.2023.3285280",
        "author": "Liu, Mingwei and Zhao, Chengyuan and Peng, Xin and Yu, Simin and Wang, Haofen and Sha, Chaofeng",
        "title": "Task-Oriented ML/DL Library Recommendation Based on a Knowledge Graph",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3285280",
        "doi": "10.1109/TSE.2023.3285280",
        "abstract": "AI applications often use ML/DL (Machine Learning/Deep Learning) models to implement specific AI tasks. As application developers usually are not AI experts, they often choose to integrate existing implementations of ML/DL models as libraries for their AI tasks. As an active research area, AI attracts many researchers and produces a lot of papers every year. Many of the papers propose ML/DL models for specific tasks and provide their implementations. However, it is not easy for developers to find ML/DL libraries that are suitable for their tasks. The challenges lie in not only the fast development of AI application domains and techniques, but also the lack of detailed information of the libraries such as environmental dependencies and supporting resources. In this paper, we conduct an empirical study on ML/DL library seeking questions on Stack Overflow to understand the developers\u2019 requirements for ML/DL libraries. Based on the findings of the study, we propose a task-oriented ML/DL library recommendation approach, called MLTaskKG. It constructs a knowledge graph that captures AI tasks, ML/DL models, model implementations, repositories, and their relationships by extracting knowledge from different sources such as ML/DL resource websites, papers, ML/DL frameworks, and repositories. Based on the knowledge graph, MLTaskKG recommends ML/DL libraries for developers by matching their requirements on tasks, model characteristics, and implementation information. Our evaluation shows that 92.8% of the tuples sampled from the resulting knowledge graph are correct, demonstrating the high quality of the knowledge graph. A further experiment shows that MLTaskKG can help developers find suitable ML/DL libraries using 47.6% shorter time and with 68.4% higher satisfaction.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4081\u20134096",
        "numpages": "16"
    },
    "Enhancing Fault Injection Testing of Service Systems via Fault-Tolerance Bottleneck": {
        "type": "article",
        "key": "10.1109/TSE.2023.3285357",
        "author": "Wu, Huayao and Yu, Senyao and Niu, Xintao and Nie, Changhai and Pei, Yu and He, Qiang and Yang, Yun",
        "title": "Enhancing Fault Injection Testing of Service Systems via Fault-Tolerance Bottleneck",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3285357",
        "doi": "10.1109/TSE.2023.3285357",
        "abstract": "Modern large-scale service systems are usually deployed with redundant components to ensure high dependability in distributed and volatile environments. Fault Injection Testing (FIT) is a popular technique for testing such systems, while the application of FIT to validating the correctness of redundant components remains a challenging task, especially when the system's structural information is unavailable when testing starts. In this study, we refer to a minimum set of faults that, when injected, will cut off all execution paths in a service system as a &lt;italic&gt;fault-tolerance bottleneck&lt;/italic&gt;, and we propose a novel Fault-tolerance Bottleneck driven Fault Injection (FBFI) approach to the exploration and validation of redundant components without prior knowledge of the system's business structure. The core idea of FBFI is to iteratively infer and inject bottlenecks of the business structure constructed so far. In this way, FBFI is able to discover and test redundant components by repeatedly triggering new system behaviors. The effectiveness and efficiency of FBFI is evaluated using two microservice benchmark systems with different deployment scales. The results reveal that FBFI is more practical and cost-effective than random and lineage-driven FIT approaches in testing service systems of high redundancy levels.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4097\u20134114",
        "numpages": "18"
    },
    "Mobile App Crowdsourced Test Report Consistency Detection via Deep Image-and-Text Fusion Understanding": {
        "type": "article",
        "key": "10.1109/TSE.2023.3285787",
        "author": "Yu, Shengcheng and Fang, Chunrong and Zhang, Quanjun and Cao, Zhihao and Yun, Yexiao and Cao, Zhenfei and Mei, Kai and Chen, Zhenyu",
        "title": "Mobile App Crowdsourced Test Report Consistency Detection via Deep Image-and-Text Fusion Understanding",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3285787",
        "doi": "10.1109/TSE.2023.3285787",
        "abstract": "Crowdsourced testing, as a distinct testing paradigm, has attracted much attention in software testing, especially in mobile application (app) testing field. Compared with in-house testing, crowdsourced testing shows superiority with the diverse testing environments when faced with the mobile testing fragmentation problem. However, crowdsourced testing also encounters the low-quality test report problem caused by unprofessional crowdworkers involved with different expertise. In order to handle the submitted reports of uneven quality, app developers have to distinguish high-quality reports from low-quality ones to help the bug inspection. One kind of typical low-quality test report is inconsistent test reports, which means the textual descriptions are not focusing on the attached bug-occurring screenshots. According to our empirical survey, only 18.07% crowdsourced test reports are consistent. Inconsistent reports cause waste on mobile app testing. To solve the inconsistency problem, we propose &lt;bold&gt;&lt;sc&gt;ReCoDe&lt;/sc&gt;&lt;/bold&gt; to detect the consistency of crowdsourced test reports via deep image-and-text fusion understanding. &lt;bold&gt;&lt;sc&gt;ReCoDe&lt;/sc&gt;&lt;/bold&gt; is a two-stage approach that first classifies the reports based on textual descriptions into different categories according to the bug feature. In the second stage, &lt;bold&gt;&lt;sc&gt;ReCoDe&lt;/sc&gt;&lt;/bold&gt; has a deep understanding of the GUI image features of the app screenshots and then applies different strategies to handle different types of bugs to detect the consistency of the crowdsourced test reports. We conduct an experiment on a dataset with over 22 k test reports to evaluate &lt;bold&gt;&lt;sc&gt;ReCoDe&lt;/sc&gt;&lt;/bold&gt;, and the results show the effectiveness of &lt;bold&gt;&lt;sc&gt;ReCoDe&lt;/sc&gt;&lt;/bold&gt; in detecting the consistency of crowdsourced test reports. Besides, a user study is conducted to prove the practical value of &lt;bold&gt;&lt;sc&gt;ReCoDe&lt;/sc&gt;&lt;/bold&gt; in effectively helping app developers improve the efficiency of reviewing the crowdsourced test reports.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4115\u20134134",
        "numpages": "20"
    },
    "STRE: An Automated Approach to Suggesting App Developers When to Stop Reading Reviews": {
        "type": "article",
        "key": "10.1109/TSE.2023.3285743",
        "author": "Tan, Youshuai and Chen, Jinfu and Shang, Weiyi and Zhang, Tao and Fang, Sen and Luo, Xiapu and Chen, Zijie and Qi, Shuhao",
        "title": "STRE: An Automated Approach to Suggesting App Developers When to Stop Reading Reviews",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3285743",
        "doi": "10.1109/TSE.2023.3285743",
        "abstract": "It is well known that user feedback (i.e., reviews) plays an essential role in mobile app maintenance. Users upload their troubles, app issues, or praises, to help developers refine their apps. However, reading tremendous amounts of reviews to retrieve useful information is a challenging job. According to our manual studies, reviews are full of repetitive opinions, thus developers could stop reading reviews when no more new helpful information appears. Developers can extract useful information from partial reviews to ameliorate their app and then develop a new version. However, it is tough to have a good trade-off between getting enough useful feedback and saving more time. In this paper, we propose a novel approach, named STRE, which utilizes historical reviews to suggest the time when most of the useful information appears in reviews of a certain version. We evaluate STRE on 62 recent versions of five apps from Apple's App Store. Study results demonstrate that our approach can help developers save their time by up to 98.33% and reserve enough useful reviews before stopping to read reviews such that developers do not spend additional time in reading redundant reviews over the suggested stopping time. At the same time, STRE can complement existing review categorization approaches that categorize reviews to further assist developers. In addition, we find that the missed top-word-related reviews appearing after the suggested stopping time contain limited useful information for developers. Finally, we find that 12 out of 13 of the emerging bugs from the studied versions appear before the suggested stopping time. Our approach demonstrates the value of automatically refining information from reviews.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4135\u20134151",
        "numpages": "17"
    },
    "CPVD: Cross Project Vulnerability Detection Based on Graph Attention Network and Domain Adaptation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3285910",
        "author": "Zhang, Chunyong and Liu, Bin and Xin, Yang and Yao, Liangwei",
        "title": "CPVD: Cross Project Vulnerability Detection Based on Graph Attention Network and Domain Adaptation",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3285910",
        "doi": "10.1109/TSE.2023.3285910",
        "abstract": "Code vulnerability detection is critical for software security prevention. Vulnerability annotation in large-scale software code is quite tedious and challenging, which requires domain experts to spend a lot of time annotating. This work offers CPVD, a cross-domain vulnerability detection approach based on the challenge of \u201dlearning to predict the vulnerability labels of another item quickly using one item with rich vulnerability labels.\u201d CPVD uses the code property graph to represent the code and uses the Graph Attention Network and Convolution Pooling Network to extract the graph feature vector. It reduces the distribution between the source domain and target domain data in the Domain Adaptation Representation Learning stage for cross-domain vulnerability detection. In this paper, we test each other on different real-world project codes. Compared with methods without domain adaptation and domain adaptation methods based on natural language processing, CPVD is more general and performs better in cross-domain vulnerability detection tasks. Specifically, for the four datasets of chr_deb, qemu, libav, and sard, they achieved the best results of 70.2%, 81.1%, 59.7%, and 78.1% respectively on the F1-Score, and 88.4%,86.3%, 85.2%, and 88.6% on the AUC.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4152\u20134168",
        "numpages": "17"
    },
    "An Architectural Technical Debt Index Based on Machine Learning and Architectural Smells": {
        "type": "article",
        "key": "10.1109/TSE.2023.3286179",
        "author": "Sas, Darius and Avgeriou, Paris",
        "title": "An Architectural Technical Debt Index Based on Machine Learning and Architectural Smells",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3286179",
        "doi": "10.1109/TSE.2023.3286179",
        "abstract": "A key aspect of technical debt (TD) management is the ability to measure the amount of principal accumulated in a system. The current literature contains an array of approaches to estimate TD principal, however, only a few of them focus specifically on architectural TD, but none of them satisfies all three of the following criteria: being fully automated, freely available, and thoroughly validated. Moreover, a recent study has shown that many of the current approaches suffer from certain shortcomings, such as relying on hand-picked thresholds. In this article, we propose a novel approach to estimate architectural technical debt principal based on machine learning and architectural smells to address such shortcomings. Our approach can estimate the amount of technical debt principal generated by a single architectural smell instance. To do so, we adopt novel techniques from Information Retrieval to train a learning-to-rank machine learning model (more specifically, a gradient boosting machine) that estimates the severity of an architectural smell and ensure the transparency of the predictions. Then, for each instance, we statically analyse the source code to calculate the exact number of lines of code creating the smell. Finally, we combine these two values to calculate the technical debt principal. To validate the approach, we conducted a case study and interviewed 16 practitioners, from both open source and industry, and asked them about their opinions on the TD principal estimations for several smells detected in their projects. The results show that for 71% of instances, practitioners agreed that the estimations provided were &lt;italic&gt;representative&lt;/italic&gt; of the effort necessary to refactor the smell.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4169\u20134195",
        "numpages": "27"
    },
    "Vulnerability Detection by Learning From Syntax-Based Execution Paths of Code": {
        "type": "article",
        "key": "10.1109/TSE.2023.3286586",
        "author": "Zhang, Junwei and Liu, Zhongxin and Hu, Xing and Xia, Xin and Li, Shanping",
        "title": "Vulnerability Detection by Learning From Syntax-Based Execution Paths of Code",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3286586",
        "doi": "10.1109/TSE.2023.3286586",
        "abstract": "Vulnerability detection is essential to protect software systems. Various approaches based on deep learning have been proposed to learn the pattern of vulnerabilities and identify them. Although these approaches have shown vast potential in this task, they still suffer from the following issues: (1) It is difficult for them to distinguish vulnerability-related information from a large amount of irrelevant information, which hinders their effectiveness in capturing vulnerability features. (2) They are less effective in handling long code because many neural models would limit the input length, which hinders their ability to represent the long vulnerable code snippets. To mitigate these two issues, in this work, we proposed to decompose the syntax-based Control Flow Graph (CFG) of the code snippet into multiple execution paths to detect the vulnerability. Specifically, given a code snippet, we first build its CFG based on its Abstract Syntax Tree (AST), refer to such CFG as syntax-based CFG, and decompose the CFG into multiple paths from an entry node to its exit node. Next, we adopt a pre-trained code model and a convolutional neural network to learn the path representations with intra- and inter-path attention. The feature vectors of the paths are combined as the representation of the code snippet and fed into the classifier to detect the vulnerability. Decomposing the code snippet into multiple paths can filter out some redundant information unrelated to the vulnerability and help the model focus on the vulnerability features. Besides, since the decomposed paths are usually shorter than the code snippet, the information located in the tail of the long code is more likely to be processed and learned. To evaluate the effectiveness of our model, we build a dataset with over 231 k code snippets, in which there are 24 k vulnerabilities. Experimental results demonstrate that the proposed approach outperforms state-of-the-art baselines by at least 22.30%, 42.92%, and 32.58% in terms of Precision, Recall, and F1-Score, respectively. Our further analysis investigates the reason for the proposed approach's superiority.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4196\u20134212",
        "numpages": "17"
    },
    "Decomposition of Monolith Applications Into Microservices Architectures: A Systematic Review": {
        "type": "article",
        "key": "10.1109/TSE.2023.3287297",
        "author": "Abgaz, Yalemisew and McCarren, Andrew and Elger, Peter and Solan, David and Lapuz, Neil and Bivol, Marin and Jackson, Glenn and Yilmaz, Murat and Buckley, Jim and Clarke, Paul",
        "title": "Decomposition of Monolith Applications Into Microservices Architectures: A Systematic Review",
        "year": "2023",
        "issue_date": "Aug. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "8",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3287297",
        "doi": "10.1109/TSE.2023.3287297",
        "abstract": "Microservices architecture has gained significant traction, in part owing to its potential to deliver scalable, robust, agile, and failure-resilient software products. Consequently, many companies that use large and complex software systems are actively looking for automated solutions to decompose their monolith applications into microservices. This paper rigorously examines 35 research papers selected from well-known databases using a Systematic Literature Review (SLR) protocol and snowballing method, extracting data to answer the research questions, and presents the following four contributions. First, the Monolith to Microservices Decomposition Framework (M2MDF) which identifies the major phases and key elements of decomposition. Second, a detailed analysis of existing decomposition approaches, tools and methods. Third, we identify the metrics and datasets used to evaluate and validate monolith to microservice decomposition processes. Fourth, we propose areas for future research. Overall, the findings suggest that monolith decomposition into microservices remains at an early stage and there is an absence of methods for combining static, dynamic, and evolutionary data. Insufficient tool support is also in evidence. Furthermore, standardised metrics, datasets, and baselines have yet to be established. These findings can assist practitioners seeking to understand the various dimensions of monolith decomposition and the community's current capabilities in that endeavour. The findings are also of value to researchers looking to identify areas to further extend research in the monolith decomposition space.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4213\u20134242",
        "numpages": "30"
    },
    "Identifying Concepts in Software Projects": {
        "type": "article",
        "key": "10.1109/TSE.2023.3265855",
        "author": "Nassif, Mathieu and Robillard, Martin P.",
        "title": "Identifying Concepts in Software Projects",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3265855",
        "doi": "10.1109/TSE.2023.3265855",
        "abstract": "When working on a project, software developers must be familiar with computing concepts, standards, and technologies related to the project. We present a novel approach, called Scode, to automatically identify those concepts using the project's documentation. Scode combines entity linking and network analysis techniques specialized for the software development domain. In addition to concepts explicitly mentioned in the documentation, Scode can retrieve implicit concepts related to the project's domain. Concepts identified by Scode have a recognized meaning that is consistent across projects. We compared Scode to different baselines and found that it is more effective at mapping projects to a consistent concept space.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3660\u20133674",
        "numpages": "15"
    },
    "MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3267446",
        "author": "Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav",
        "title": "MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3267446",
        "doi": "10.1109/TSE.2023.3267446",
        "abstract": "Large language models have demonstrated the ability to generate both natural language and programming language text. Although contemporary code generation models are trained on corpora with several programming languages, they are tested using benchmarks that are typically monolingual. The most widely used code generation benchmarks only target Python, so there is little quantitative evidence of how code generation models perform on other programming languages. We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages. We use MultiPL-E to extend the HumanEval benchmark (Chen et al., 2021) and MBPP benchmark (Austin et al., 2021) to 18 languages that encompass a range of programming paradigms and popularity. Using these new parallel benchmarks, we evaluate the multi-language performance of three state-of-the-art code generation models: Codex (Chen et al., 2021), CodeGen (Nijkamp et al., 2022) and InCoder (Fried et al., 2022). We find that Codex matches or even exceeds its performance on Python for several other languages. The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance. Finally, the MultiPL-E approach of compiling code generation benchmarks to new programming languages is both scalable and extensible, making it straightforward to evaluate new models, benchmarks, and languages.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3675\u20133691",
        "numpages": "17"
    },
    "JavaScript Dead Code Identification, Elimination, and Empirical Assessment": {
        "type": "article",
        "key": "10.1109/TSE.2023.3267848",
        "author": "Malavolta, Ivano and Nirghin, Kishan and Scoccia, Gian Luca and Romano, Simone and Lombardi, Salvatore and Scanniello, Giuseppe and Lago, Patricia",
        "title": "JavaScript Dead Code Identification, Elimination, and Empirical Assessment",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3267848",
        "doi": "10.1109/TSE.2023.3267848",
        "abstract": "Web apps are built by using a combination of HTML, CSS, and JavaScript. While building modern web apps, it is common practice to make use of third-party libraries and frameworks, as to improve developers\u2019 productivity and code quality. Alongside these benefits, the adoption of such libraries results in the introduction of &lt;italic&gt;JavaScript dead code&lt;/italic&gt;, i.e., code implementing unused functionalities. The costs for downloading and parsing dead code can negatively contribute to the loading time and resource usage of web apps. The goal of our study is two-fold. First, we present &lt;italic&gt;Lacuna&lt;/italic&gt;, an approach for automatically detecting and eliminating JavaScript dead code from web apps. The proposed approach supports both static and dynamic analyses, it is extensible and can be applied to any JavaScript code base, without imposing constraints on the coding style or on the use of specific JavaScript constructs. Second, by leveraging Lacuna we conduct an experiment to empirically evaluate the run-time overhead of JavaScript dead code in terms of energy consumption, performance, network usage, and resource usage in the context of mobile web apps. We applied Lacuna four times on 30 mobile web apps independently developed by third-party developers, each time eliminating dead code according to a different optimization level provided by Lacuna. Afterward, each different version of the web app is executed on an Android device, while collecting measures to assess the potential run-time overhead caused by dead code. Experimental results, among others, highlight that the removal of JavaScript dead code has a positive impact on the loading time of mobile web apps, while significantly reducing the number of bytes transferred over the network.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3692\u20133714",
        "numpages": "23"
    },
    "A Search-Based Testing Approach for Deep Reinforcement Learning Agents": {
        "type": "article",
        "key": "10.1109/TSE.2023.3269804",
        "author": "Zolfagharian, Amirhossein and Abdellatif, Manel and Briand, Lionel C. and Bagherzadeh, Mojtaba and S, Ramesh",
        "title": "A Search-Based Testing Approach for Deep Reinforcement Learning Agents",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3269804",
        "doi": "10.1109/TSE.2023.3269804",
        "abstract": "Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving, trading decisions, and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One of the ways to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Furthermore, their main goal is to test the robustness of DRL agents rather than testing the compliance of the agents\u2019 policies with respect to requirements. Due to the huge state space of DRL environments, the high cost of test execution, and the black-box nature of DRL algorithms, exhaustive testing of DRL agents is impossible. In this paper, we propose a Search-based Testing Approach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL agent by effectively searching for failing executions of the agent within a limited testing budget. We rely on machine learning models and a dedicated genetic algorithm to narrow the search toward faulty episodes (i.e., sequences of states and actions produced by the DRL agent). We apply STARLA on Deep-Q-Learning agents trained on two different RL problems widely used as benchmarks and show that STARLA significantly outperforms Random Testing by detecting more faults related to the agent's policy. We also investigate how to extract rules that characterize faulty episodes of the DRL agent using our search results. Such rules can be used to understand the conditions under which the agent fails and thus assess the risks of deploying it.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3715\u20133735",
        "numpages": "21"
    },
    "CirFix: Automated Hardware Repair and its Real-World Applications": {
        "type": "article",
        "key": "10.1109/TSE.2023.3269899",
        "author": "Santiesteban, Priscila and Huang, Yu and Weimer, Westley and Ahmad, Hammad",
        "title": "CirFix: Automated Hardware Repair and its Real-World Applications",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3269899",
        "doi": "10.1109/TSE.2023.3269899",
        "abstract": "This article presents CirFix, a framework for automatically repairing defects in hardware designs implemented in languages like Verilog. We propose a novel fault localization approach based on assignments to wires and registers, and a fitness function tailored to the hardware domain to bridge the gap between software-level automated program repair and hardware descriptions. We also present a benchmark suite of 32 defect scenarios corresponding to a variety of hardware projects. Overall, CirFix produces plausible repairs for 21/32 and correct repairs for 16/32 of the defect scenarios. Additionally, we evaluate CirFix's fault localization independently through a human study (n = 41), and find that the approach may be a beneficial debugging aid for complex multi-line hardware defects.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3736\u20133752",
        "numpages": "17"
    },
    "Driving the Technology Value Stream by Analyzing App Reviews": {
        "type": "article",
        "key": "10.1109/TSE.2023.3270708",
        "author": "Das, Souvick and Deb, Novarun and Chaki, Nabendu and Cortesi, Agostino",
        "title": "Driving the Technology Value Stream by Analyzing App Reviews",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3270708",
        "doi": "10.1109/TSE.2023.3270708",
        "abstract": "An emerging feature of mobile application software is the need to quickly produce new versions to solve problems that emerged in previous versions. This helps adapt to changing user needs and preferences. In a continuous software development process, the user reviews collected by the apps themselves can play a crucial role to detect which components need to be reworked. This paper proposes a novel framework that enables software companies to drive their technology value stream based on the feedback (or reviews) provided by the end-users of an application. The proposed end-to-end framework exploits different Natural Language Processing (NLP) tasks to best understand the needs and goals of the end users. We also provide a thorough and in-depth analysis of the framework, the performance of each of the modules, and the overall contribution in driving the technology value stream. An analysis of reviews with sixteen popular Android Play Store applications from various genres over a long period of time provides encouraging evidence of the effectiveness of the proposed approach.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3753\u20133770",
        "numpages": "18"
    },
    "Self-Supervised Learning to Prove Equivalence Between Straight-Line Programs via Rewrite Rules": {
        "type": "article",
        "key": "10.1109/TSE.2023.3271065",
        "author": "Kommrusch, Steve and Monperrus, Martin and Pouchet, Louis-No\\\"{e}l",
        "title": "Self-Supervised Learning to Prove Equivalence Between Straight-Line Programs via Rewrite Rules",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3271065",
        "doi": "10.1109/TSE.2023.3271065",
        "abstract": "We target the problem of automatically synthesizing proofs of semantic equivalence between two programs made of sequences of statements. We represent programs using abstract syntax trees (AST), where a given set of semantics-preserving rewrite rules can be applied on a specific AST pattern to generate a transformed and semantically equivalent program. In our system, two programs are equivalent if there exists a sequence of application of these rewrite rules that leads to rewriting one program into the other. We propose a neural network architecture based on a transformer model to generate proofs of equivalence between program pairs. The system outputs a sequence of rewrites, and the validity of the sequence is simply checked by verifying it can be applied. If no valid sequence is produced by the neural network, the system reports the programs as non-equivalent, ensuring by design no programs may be incorrectly reported as equivalent. Our system is fully implemented for one single grammar which can represent straight-line programs with function calls and multiple types. To efficiently train the system to generate such sequences, we develop an original incremental training technique, named self-supervised sample selection. We extensively study the effectiveness of this novel training approach on proofs of increasing complexity and length. Our system, &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathsf {S4Eq}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"sans-serif\"&gt;S&lt;/mml:mi&gt;&lt;mml:mn mathvariant=\"sans-serif\"&gt;4&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"sans-serif\"&gt;Eq&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"kommrusch-ieq1-3271065.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, achieves 97% proof success on a curated dataset of 10,000 pairs of equivalent programs.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3771\u20133792",
        "numpages": "22"
    },
    "Demystifying Random Number in Ethereum Smart Contract: Taxonomy, Vulnerability Identification, and Attack Detection": {
        "type": "article",
        "key": "10.1109/TSE.2023.3271417",
        "author": "Qian, Peng and He, Jianting and Lu, Lingling and Wu, Siwei and Lu, Zhipeng and Wu, Lei and Zhou, Yajin and He, Qinming",
        "title": "Demystifying Random Number in Ethereum Smart Contract: Taxonomy, Vulnerability Identification, and Attack Detection",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3271417",
        "doi": "10.1109/TSE.2023.3271417",
        "abstract": "Recent years have witnessed explosive growth in blockchain smart contract applications. As smart contracts become increasingly popular and carry trillion dollars worth of digital assets, they become more of an appealing target for attackers, who have exploited vulnerabilities in smart contracts to cause catastrophic economic losses. Notwithstanding a proliferation of work that has been developed to detect an impressive list of vulnerabilities, the bad randomness vulnerability is overlooked by many existing tools. In this article, we make the first attempt to provide a systematic analysis of random numbers in Ethereum smart contracts, by investigating the principles behind pseudo-random number generation and organizing them into a taxonomy. We also lucubrate various attacks against bad random numbers and group them into four categories. Furthermore, we present &lt;italic&gt;RNVulDet&lt;/italic&gt; \u2013 a tool that incorporates taint analysis techniques to automatically identify bad randomness vulnerabilities and detect corresponding attack transactions. To extensively verify the effectiveness of &lt;italic&gt;RNVulDet&lt;/italic&gt;, we construct three new datasets: i) 34 well-known contracts that are reported to possess bad randomness vulnerabilities, ii) 214 popular contracts that have been rigorously audited before launch and are regarded as free of bad randomness vulnerabilities, and iii) a dataset consisting of 47,668 smart contracts and 49,951 suspicious transactions. We compare &lt;italic&gt;RNVulDet&lt;/italic&gt; with three state-of-the-art smart contract vulnerability detectors, and our tool significantly outperforms them. Meanwhile, &lt;italic&gt;RNVulDet&lt;/italic&gt; spends 2.98 s per contract on average, in most cases orders-of-magnitude faster than other tools. &lt;italic&gt;RNVulDet&lt;/italic&gt; successfully reveals 44,264 attack transactions. Our implementation and datasets are released, hoping to inspire others.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3793\u20133810",
        "numpages": "18"
    },
    "What Not to Test (For Cyber-Physical Systems)": {
        "type": "article",
        "key": "10.1109/TSE.2023.3272309",
        "author": "Ling, Xiao and Menzies, Tim",
        "title": "What Not to Test (For Cyber-Physical Systems)",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3272309",
        "doi": "10.1109/TSE.2023.3272309",
        "abstract": "For simulation-based systems, finding a set of test cases with the least cost by exploring multiple goals is a complex task. Domain-specific optimization goals (e.g., maximize output variance) are useful for guiding the rapid selection of test cases via mutation. But evaluating the selected test cases via mutation (that can distinguish the current program from) is a different goal to domain-specific optimizations. While the optimization goals can be used to guide the mutation analysis, that guidance should be viewed as a weak indicator since it can hurt the mutation effectiveness goals by focusing too much on the optimization goals. Based on the above, this paper proposes &lt;monospace&gt;DoLesS&lt;/monospace&gt; (&lt;bold&gt;Do&lt;/bold&gt;mination with &lt;bold&gt;Le&lt;/bold&gt;a&lt;bold&gt;s&lt;/bold&gt;t &lt;bold&gt;S&lt;/bold&gt;quares Approximation) that selects the minimal and effective test cases by averaging over a coarse-grained grid of the information gained from multiple optimizations goals. &lt;monospace&gt;DoLesS&lt;/monospace&gt; applies an inverted least squares approximation approach to find a minimal set of tests that can distinguish better from worse parts of the optimization goals. When tested on multiple simulation-based systems, &lt;monospace&gt;DoLesS&lt;/monospace&gt; performs as well or even better as the prior state-of-the-art, while running 80-360 times faster on average (seconds instead of hours).",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3811\u20133826",
        "numpages": "16"
    },
    "Discovering Reusable Functional Features in Legacy Object-Oriented Systems": {
        "type": "article",
        "key": "10.1109/TSE.2023.3272631",
        "author": "Mili, Hafedh and Benzarti, Imen and Elkharraz, Amel and Elboussaidi, Ghizlane and Gu\\'{e}h\\'{e}neuc, Yann-Ga\\\"{e}l and Valtchev, Petko",
        "title": "Discovering Reusable Functional Features in Legacy Object-Oriented Systems",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3272631",
        "doi": "10.1109/TSE.2023.3272631",
        "abstract": "Typical object-oriented (OO) systems implement several functional features that are interwoven into class hierarchies. In the absence of aspect-oriented techniques to develop and compose these features, developers resort to object-oriented design and programming idioms to separate features as well as possible. Given a legacy OO system, discovering existing functional features helps understand the design of the system and extract these features to ease their maintenance and reuse. We want to discover candidate functional features in OO systems. We first define functional features and then discuss the footprints that such features are likely to leave in an OO system. We identify three such footprints: (1) multiple inheritance, (2) delegation, and (3) ad-hoc. We develop a set of algorithms for identifying such footprints in OO code and implemented them for the Java language using Eclipse JDT. In this article, we present the algorithms, and the results of applying the corresponding tools on five open-source systems: FreeMind, JavaWebMail, JHotDraw, JReversePro, and Lucene. Our experimental results show that: (1) the different algorithms can identify interesting and useful candidate functional features in OO systems, (2) they can identify opportunities for refactoring, and (3) they are complementary and could help developers.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3827\u20133856",
        "numpages": "30"
    },
    "Detecting Android API Compatibility Issues With API Differences": {
        "type": "article",
        "key": "10.1109/TSE.2023.3274153",
        "author": "Mahmud, Tarek and Che, Meiru and Yang, Guowei",
        "title": "Detecting Android API Compatibility Issues With API Differences",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3274153",
        "doi": "10.1109/TSE.2023.3274153",
        "abstract": "Android application programming interface (API) enables app developers to harness the functionalities of Android devices by interfacing with services and hardware using a Software Development Kit (SDK). However, API frequently evolves together with its associated SDK, and compatibility issues may arise when the API level supported by the underlying device differs from the API level targeted by app developers. These issues can lead to unexpected behaviors, resulting in a bad user experience. This article presents ACID, a novel approach to detecting Android API compatibility issues induced by API evolution. It detects both API invocation compatibility issues and API callback compatibility issues using API differences and static analysis of the app code. Experiments with 20 benchmark apps show that ACID is more accurate and faster than the state-of-the-art techniques in detecting API compatibility issues. The application of ACID on 2965 real-world apps further demonstrates its practical applicability. To eliminate the false positives reported by ACID, this article also presents a simple yet effective method to quickly verify the compatibility issues by selecting and executing the relevant tests from app's test suite, and experimental results demonstrate the verification method can eliminate most false positives when app's test suite has good coverage of the API usages.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3857\u20133871",
        "numpages": "15"
    },
    "Learning the Relation Between Code Features and Code Transforms With Structured Prediction": {
        "type": "article",
        "key": "10.1109/TSE.2023.3275380",
        "author": "Yu, Zhongxing and Martinez, Matias and Chen, Zimin and Bissyand\\'{e}, Tegawend\\'{e} F. and Monperrus, Martin",
        "title": "Learning the Relation Between Code Features and Code Transforms With Structured Prediction",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3275380",
        "doi": "10.1109/TSE.2023.3275380",
        "abstract": "To effectively guide the exploration of the code transform space for automated code evolution techniques, we present in this article the first approach for structurally predicting code transforms at the level of AST nodes using conditional random fields (CRFs). Our approach first learns offline a probabilistic model that captures how certain code transforms are applied to certain AST nodes, and then uses the learned model to predict transforms for arbitrary new, unseen code snippets. Our approach involves a novel representation of both programs and code transforms. Specifically, we introduce the formal framework for defining the so-called AST-level code transforms and we demonstrate how the CRF model can be accordingly designed, learned, and used for prediction. We instantiate our approach in the context of repair transform prediction for Java programs. Our instantiation contains a set of carefully designed code features, deals with the training data imbalance issue, and comprises transform constraints that are specific to code. We conduct a large-scale experimental evaluation based on a dataset of bug fixing commits from real-world Java projects. The results show that when the popular evaluation metric &lt;italic&gt;top-3&lt;/italic&gt; is used, our approach predicts the code transforms with an accuracy varying from 41% to 53% depending on the transforms. Our model outperforms two baselines based on history probability and neural machine translation (NMT), suggesting the importance of considering code structure in achieving good prediction accuracy. In addition, a proof-of-concept synthesizer is implemented to concretize some repair transforms to get the final patches. The evaluation of the synthesizer on the Defects4j benchmark confirms the usefulness of the predicted AST-level repair transforms in producing high-quality patches.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3872\u20133900",
        "numpages": "29"
    },
    "Can We Trust the Phone Vendors? Comprehensive Security Measurements on the Android Firmware Ecosystem": {
        "type": "article",
        "key": "10.1109/TSE.2023.3275655",
        "author": "Hou, Qinsheng and Diao, Wenrui and Wang, Yanhao and Mao, Chenglin and Ying, Lingyun and Liu, Song and Liu, Xiaofeng and Li, Yuanzhi and Guo, Shanqing and Nie, Meining and Duan, Haixin",
        "title": "Can We Trust the Phone Vendors? Comprehensive Security Measurements on the Android Firmware Ecosystem",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3275655",
        "doi": "10.1109/TSE.2023.3275655",
        "abstract": "Android is the most popular smartphone platform with over 85% market share. Its success is built on openness, and phone vendors can utilize the Android source code to make customized products with unique software/hardware features. On the other hand, the fragmentation and customization of Android also bring many security risks that have attracted the attention of researchers. Many efforts were put in to investigate the security of customized Android firmware. However, most of the previous works focus on designing efficient analysis tools or analyzing particular aspects of the firmware. There still lacks a panoramic view of Android firmware ecosystem security and the corresponding understandings based on large-scale firmware datasets. In this work, we made a large-scale comprehensive measurement of the Android firmware ecosystem security. Our study is based on 8,325 firmware images from 153 vendors and 813 Android-related CVEs, which is the largest Android firmware dataset ever used for security measurements. In particular, our study followed a series of research questions, covering vulnerabilities, patches, security updates, and pre-installed apps. To automate the analysis process, we designed a framework, &lt;sc&gt;AndScanner+&lt;/sc&gt;, to complete firmware crawling, firmware parsing, patch analysis, and app analysis. Through massive data analysis and case explorations, several interesting findings are obtained. For example, the patch delay and missing issues are widespread in Android firmware images, say 31.4% and 5.6% of all images, respectively. The latest images of several phones still contain vulnerable pre-installed apps, and even the corresponding vulnerabilities have been publicly disclosed. In addition to data measurements, we also explore the causes behind these security threats through case studies and demonstrate that the discovered security threats can be converted into exploitable vulnerabilities. There are 46 new vulnerabilities found by &lt;sc&gt;AndScanner+&lt;/sc&gt;, 36 of which have been assigned CVE/CNVD IDs. This study provides much new knowledge of the Android firmware ecosystem with a deep understanding of software engineering security practices.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3901\u20133921",
        "numpages": "21"
    },
    "Syntactic Versus Semantic Similarity of Artificial and Real Faults in Mutation Testing Studies": {
        "type": "article",
        "key": "10.1109/TSE.2023.3277564",
        "author": "Ojdanic, Milos and Garg, Aayush and Khanfir, Ahmed and Degiovanni, Renzo and Papadakis, Mike and Le Traon, Yves",
        "title": "Syntactic Versus Semantic Similarity of Artificial and Real Faults in Mutation Testing Studies",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3277564",
        "doi": "10.1109/TSE.2023.3277564",
        "abstract": "Fault seeding is typically used in empirical studies to evaluate and compare test techniques. Central to these techniques lies the hypothesis that artificially seeded faults involve some form of realistic properties and thus provide realistic experimental results. In an attempt to strengthen realism, a recent line of research uses machine learning techniques, such as deep learning and Natural Language Processing, to seed faults that look like (syntactically) real ones, implying that fault realism is related to syntactic similarity. This raises the question of whether seeding syntactically similar faults indeed results in semantically similar faults and, more generally whether syntactically dissimilar faults are far away (semantically) from the real ones. We answer this question by employing 4 state-of-the-art fault-seeding techniques (PiTest - a popular mutation testing tool, IBIR - a tool with manually crafted fault patterns, DeepMutation - a learning-based fault seeded framework and &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mu$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u03bc&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"ojdanic-ieq1-3277564.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;BERT - a mutation testing tool based on the pre-trained language model CodeBERT) that operate in a fundamentally different way, and demonstrate that syntactic similarity does not reflect semantic similarity. We also show that 65.11%, 76.44%, 61.39% and 9.76% of the real faults of Defects4J V2 are semantically resembled by PiTest, IBIR, &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mu$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u03bc&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"ojdanic-ieq2-3277564.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;BERT and DeepMutation faults, respectively.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3922\u20133938",
        "numpages": "17"
    },
    "Context-Aware Neural Fault Localization": {
        "type": "article",
        "key": "10.1109/TSE.2023.3279125",
        "author": "Zhang, Zhuo and Lei, Yan and Mao, Xiaoguang and Yan, Meng and Xia, Xin and Lo, David",
        "title": "Context-Aware Neural Fault Localization",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3279125",
        "doi": "10.1109/TSE.2023.3279125",
        "abstract": "Numerous fault localization techniques identify suspicious statements potentially responsible for program failures by discovering the statistical correlation between test results (i.e., &lt;italic&gt;failing&lt;/italic&gt; or &lt;italic&gt;passing&lt;/italic&gt;) and the executions of the different statements of a program (i.e., &lt;italic&gt;covered&lt;/italic&gt; or &lt;italic&gt;not covered&lt;/italic&gt;). They rarely incorporate a failure context into their suspiciousness evaluation despite the fact that a failure context showing how a failure is produced is useful for analyzing and locating faults. Since a failure context usually contains the transitive relationships among the statements of causing a failure, its relationship complexity becomes one major obstacle for the context incorporation in suspiciousness evaluation of fault localization. To overcome the obstacle, our insight is that leveraging the promising learning ability may be a candidate solution to learn a feasible model for incorporating a failure context into fault localization. Thus, we propose a context-aware neural fault localization approach (CAN). Specifically, CAN represents the failure context by constructing a program dependency graph, which shows how a set of statements interact with each other (i.e., data and control dependencies) to cause a failure. Then, CAN utilizes graph neural networks to analyze and incorporate the context (e.g., the dependencies among the statements) into suspiciousness evaluation. Our empirical results on the 12 large-sized programs show that CAN achieves promising results (e.g., 29.23% faults are ranked within top 5), and it significantly improves the state-of-the-art baselines with a substantial margin.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3939\u20133954",
        "numpages": "16"
    },
    "Dependent or Not: Detecting and Understanding Collections of Refactorings": {
        "type": "article",
        "key": "10.1109/TSE.2023.3244123",
        "author": "Ferreira, Thiago and Ivers, James and Yackley, Jeffrey J. and Kessentini, Marouane and Ozkaya, Ipek and Gaaloul, Khouloud",
        "title": "Dependent or Not: Detecting and Understanding Collections of Refactorings",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3244123",
        "doi": "10.1109/TSE.2023.3244123",
        "abstract": "Refactoring is a program transformation to improve the internal structure of a program while preserving its external behavior. Developers frequently apply multiple refactorings that depend on each other to achieve goals such as improving code reusability. Although manually applying a sequence of dependent refactorings is a common practice, existing refactoring recommendation tools treat refactorings in isolation without revealing the dependencies among them to developers. One reason is that these relationships among refactorings are poorly understood. Current approaches treat refactoring recommendations as a strictly ordered sequence limiting developers\u2019 ability to understand, validate, and apply recommended refactorings. To address this gap, this paper describes a theory for reasoning about collections of refactorings through defining an ordering dependency relation among refactorings and organizing collection of refactorings as a set of refactoring graphs. We propose an algorithm for identifying refactoring dependencies and illustrate these concepts with a tool for visualizing such refactoring dependencies and refactoring graphs. Our validation results demonstrate that 43% of the 1,457,873 recommended refactorings from 9,595 projects that we studied are part of dependent refactoring graphs. Furthermore, refactorings are not only commonly involved in dependent relations, but also when applied, dependent refactoring graphs improve all of the quality attribute metrics in our experiments more than individual refactorings.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3344\u20133358",
        "numpages": "15"
    },
    "LeakageVerif: Efficient and Scalable Formal Verification of Leakage in Symbolic Expressions": {
        "type": "article",
        "key": "10.1109/TSE.2023.3252671",
        "author": "Meunier, Quentin L. and Pons, Etienne and Heydemann, Karine",
        "title": "LeakageVerif: Efficient and Scalable Formal Verification of Leakage in Symbolic Expressions",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3252671",
        "doi": "10.1109/TSE.2023.3252671",
        "abstract": "Side-channel attacks are a powerful class of attacks targeting cryptographic devices. Masking is a popular protection technique to thwart such attacks as it can be theoretically proven secure. However, correctly implementing masking schemes is a non-trivial task and error-prone. If several techniques have been proposed to formally verify masked implementations, they all come with limitations regarding expressiveness, scalability or accuracy. In this work, we propose a symbolic approach, based on a variant of the classical substitution method, for formally verifying arithmetic and boolean masked programs. This approach is more accurate and scalable than existing approaches thanks to a careful design and implementation of key heuristics, algorithms and data structures involved in the verification process. We present all the details of this approach and the open-source tool called &lt;monospace&gt;LeakageVerif&lt;/monospace&gt; which implements it as a python library, and which offers constructions for symbolic expressions and functions for their verification. We compare &lt;monospace&gt;LeakageVerif&lt;/monospace&gt; to three existing state-of-the-art tools on a set of 46 masked programs, and we show that it has very good scalability and accuracy results while providing all the necessary constructs for describing algorithmic to assembly masking schemes. Finally, we also provide the set of 46 benchmarks, named &lt;monospace&gt;MaskedVerifBenchs&lt;/monospace&gt; and written for comparing the different verification tools, in the hope that they will be useful to the community for future comparisons.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3359\u20133375",
        "numpages": "17"
    },
    "SLocator: Localizing the Origin of SQL Queries in Database-Backed Web Applications": {
        "type": "article",
        "key": "10.1109/TSE.2023.3253700",
        "author": "Liu, Wei and Chen, Tse-Hsun",
        "title": "SLocator: Localizing the Origin of SQL Queries in Database-Backed Web Applications",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3253700",
        "doi": "10.1109/TSE.2023.3253700",
        "abstract": "In database-backed web applications, developers often leverage Object-Relational Mapping (ORM) frameworks for database accesses. ORM frameworks provide an abstraction of the underlying database access details so that developers can focus on implementing the business logic of the application. However, due to the abstraction, developers may not know where and how a problematic SQL query is generated in the application code, causing challenges in debugging database access problems. In this paper, we propose an approach, called SLocator, which locates where a SQL query is generated in the application code. SLocator is a hybrid approach that leverages both static analysis and information retrieval (IR) techniques. SLocator uses static analysis to infer the database access for every possible path in the control flow graph. Then, given a SQL query, SLocator applies IR techniques to find the control flow path (i.e., a sequence of methods called in an interprocedural control flow graph) whose inferred database access has the highest similarity ranking. We implement SLocator for Java\u2019s official ORM API specification (JPA) and evaluate SLocator on seven open source Java applications. We find that SLocator is able to locate the control flow path that generates a SQL query with a Top@1 accuracy ranging from 37.4% to 70% for SQL queries in sessions, and 30.7% to 69.2% for individual SQL queries; and Top@5 ranging from 78.3% to 95.5% for SQL queries in sessions, and 59.1% to 100% for individual SQL queries. We also conduct a study to illustrate how SLocator may be used for locating issues in the database access code.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3376\u20133390",
        "numpages": "15"
    },
    "Specification-Based Autonomous Driving System Testing": {
        "type": "article",
        "key": "10.1109/TSE.2023.3254142",
        "author": "Zhou, Yuan and Sun, Yang and Tang, Yun and Chen, Yuqi and Sun, Jun and Poskitt, Christopher M. and Liu, Yang and Yang, Zijiang",
        "title": "Specification-Based Autonomous Driving System Testing",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3254142",
        "doi": "10.1109/TSE.2023.3254142",
        "abstract": "Autonomous vehicle (AV) systems must be comprehensively tested and evaluated before they can be deployed. High-fidelity simulators such as CARLA or LGSVL allow this to be done safely in very realistic and highly customizable environments. Existing testing approaches, however, fail to test simulated AVs systematically, as they focus on specific scenarios and oracles (e.g., lane following scenario with the \u201cno collision\u201d requirement) and lack any coverage criteria measures. In this paper, we propose &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathtt {AVUnit}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant=\"monospace\"&gt;AVUnit&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq1-3254142.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, a framework for systematically testing AV systems against customizable correctness specifications. Designed modularly to support different simulators, &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathtt {AVUnit}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant=\"monospace\"&gt;AVUnit&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq2-3254142.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; consists of two new languages for specifying dynamic properties of scenes (e.g., changing pedestrian behaviour after waypoints) and fine-grained assertions about the AV's journey. &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathtt {AVUnit}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant=\"monospace\"&gt;AVUnit&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq3-3254142.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; further supports multiple fuzzing algorithms that automatically search for test cases that violate these assertions, using robustness and coverage measures as fitness metrics. We evaluated the implementation of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathtt {AVUnit}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant=\"monospace\"&gt;AVUnit&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq4-3254142.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; for the LGSVL+Apollo simulation environment, finding 19 kinds of issues in Apollo, which indicate that the open-source Apollo does not perform well in complex intersections and lane-changing related scenarios.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3391\u20133410",
        "numpages": "20"
    },
    "Invalidator: Automated Patch Correctness Assessment Via Semantic and Syntactic Reasoning": {
        "type": "article",
        "key": "10.1109/TSE.2023.3255177",
        "author": "Le-Cong, Thanh and Luong, Duc-Minh and Le, Xuan Bach D. and Lo, David and Tran, Nhat-Hoa and Quang-Huy, Bui and Huynh, Quyet-Thang",
        "title": "Invalidator: Automated Patch Correctness Assessment Via Semantic and Syntactic Reasoning",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3255177",
        "doi": "10.1109/TSE.2023.3255177",
        "abstract": "Automated program repair (APR) faces the challenge of test overfitting, where generated patches pass validation tests but fail to generalize. Existing methods for patch assessment involve generating new tests or manual inspection, which can be time-consuming or biased. In this paper, we propose a novel technique, &lt;sc&gt;Invalidator&lt;/sc&gt;, to automatically assess the correctness of APR-generated patches via semantic and syntactic reasoning. &lt;sc&gt;Invalidator&lt;/sc&gt; leverages program invariants to reason about program semantics while also capturing program syntax through language semantics learned from a large code corpus using a pre-trained language model. Given a buggy program and the developer-patched program, &lt;sc&gt;Invalidator&lt;/sc&gt; infers likely invariants on both programs. Then, &lt;sc&gt;Invalidator&lt;/sc&gt; determines that an APR-generated patch overfits if: (1) it violates correct specifications or (2) maintains erroneous behaviors from the original buggy program. In case our approach fails to determine an overfitting patch based on invariants, &lt;sc&gt;Invalidator&lt;/sc&gt; utilizes a trained model from labeled patches to assess patch correctness based on program syntax. The benefit of &lt;sc&gt;Invalidator&lt;/sc&gt; is threefold. First, &lt;sc&gt;Invalidator&lt;/sc&gt; leverages both semantic and syntactic reasoning to enhance its discriminative capability. Second, &lt;sc&gt;Invalidator&lt;/sc&gt; does not require new test cases to be generated, but instead only relies on the current test suite and uses invariant inference to generalize program behaviors. Third, &lt;sc&gt;Invalidator&lt;/sc&gt; is fully automated. Experimental results demonstrate that &lt;sc&gt;Invalidator&lt;/sc&gt; outperforms existing methods in terms of Accuracy and F-measure, correctly identifying 79% of overfitting patches and detecting 23% more overfitting patches than the best baseline.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3411\u20133429",
        "numpages": "19"
    },
    "Metamorphic Testing for Web System Security": {
        "type": "article",
        "key": "10.1109/TSE.2023.3256322",
        "author": "Chaleshtari, Nazanin Bayati and Pastore, Fabrizio and Goknil, Arda and Briand, Lionel C.",
        "title": "Metamorphic Testing for Web System Security",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3256322",
        "doi": "10.1109/TSE.2023.3256322",
        "abstract": "Security testing aims at verifying that the software meets its security properties. In modern Web systems, however, this often entails the verification of the outputs generated when exercising the system with a very large set of inputs. Full automation is thus required to lower costs and increase the effectiveness of security testing. Unfortunately, to achieve such automation, in addition to strategies for automatically deriving test inputs, we need to address the oracle problem, which refers to the challenge, given an input for a system, of distinguishing correct from incorrect behavior (e.g., the response to be received after a specific HTTP GET request). In this paper, we propose Metamorphic Security Testing for Web-interactions (&lt;italic&gt;MST-wi&lt;/italic&gt;), a metamorphic testing approach that integrates test input generation strategies inspired by mutational fuzzing and alleviates the oracle problem in security testing. It enables engineers to specify metamorphic relations (MRs) that capture many security properties of Web systems. To facilitate the specification of such MRs, we provide a domain-specific language accompanied by an Eclipse editor. &lt;italic&gt;MST-wi&lt;/italic&gt; automatically collects the input data and transforms the MRs into executable Java code to automatically perform security testing. It automatically tests Web systems to detect vulnerabilities based on the relations and collected data. We provide a catalog of 76 system-agnostic MRs to automate security testing in Web systems. It covers 39% of the OWASP security testing activities not automated by state-of-the-art techniques; further, our MRs can automatically discover 102 different types of vulnerabilities, which correspond to 45% of the vulnerabilities due to violations of security design principles according to the MITRE CWE database. We also define guidelines that enable test engineers to improve the testability of the system under test with respect to our approach. We evaluated &lt;italic&gt;MST-wi&lt;/italic&gt; effectiveness and scalability with two well-known Web systems (i.e., Jenkins and Joomla). It automatically detected 85% of their vulnerabilities and showed a high specificity (99.81% of the generated inputs do not lead to a false positive); our findings include a new security vulnerability detected in Jenkins. Finally, our results demonstrate that the approach scale, thus enabling automated security testing overnight.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3430\u20133471",
        "numpages": "42"
    },
    "CoSS: Leveraging Statement Semantics for Code Summarization": {
        "type": "article",
        "key": "10.1109/TSE.2023.3256362",
        "author": "Shi, Chaochen and Cai, Borui and Zhao, Yao and Gao, Longxiang and Sood, Keshav and Xiang, Yong",
        "title": "CoSS: Leveraging Statement Semantics for Code Summarization",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3256362",
        "doi": "10.1109/TSE.2023.3256362",
        "abstract": "Automated code summarization tools allow generating descriptions for code snippets in natural language, which benefits software development and maintenance. Recent studies demonstrate that the quality of generated summaries can be improved by using additional code representations beyond token sequences. The majority of contemporary approaches mainly focus on extracting code syntactic and structural information from abstract syntax trees (ASTs). However, from the view of macro-structures, it is challenging to identify and capture semantically meaningful features due to fine-grained syntactic nodes involved in ASTs. To fill this gap, we investigate how to learn more code semantics and control flow features from the perspective of code statements. Accordingly, we propose a novel model entitled CoSS for code summarization. CoSS adopts a Transformer-based encoder and a graph attention network-based encoder to capture token-level and statement-level semantics from code token sequence and control flow graph, respectively. Then, after receiving two-level embeddings from encoders, a joint decoder with a multi-head attention mechanism predicts output sequences verbatim. Performance evaluations on Java, Python, and Solidity datasets validate that CoSS outperforms nine state-of-the-art (SOTA) neural code summarization models in effectiveness and is competitive in execution efficiency. Further, the ablation study reveals the contribution of each model component.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3472\u20133486",
        "numpages": "15"
    },
    "New Techniques for Static Symmetry Breaking in Many-Sorted Finite Model Finding": {
        "type": "article",
        "key": "10.1109/TSE.2023.3256939",
        "author": "Poremba, Joseph and Day, Nancy A. and Vakili, Amirhossein",
        "title": "New Techniques for Static Symmetry Breaking in Many-Sorted Finite Model Finding",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3256939",
        "doi": "10.1109/TSE.2023.3256939",
        "abstract": "Symmetry in finite model finding problems of many-sorted first-order logic (MSFOL) can be exploited to reduce the number of interpretations considered during search, thereby improving solver performance for tools such as the Alloy Analyzer. We present a framework to soundly compose static symmetry breaking schemes for many-sorted finite model finding. Then, we introduce and prove the correctness of three static symmetry breaking schemes for MSFOL: 1) one for functions with distinct sorts in the domain and range; 2) one for functions where the range sort appears in the domain; and 3) one for predicates. We provide a novel presentation of sort inference in the context of symmetry breaking that yields a new mathematical link between sorts and symmetries. We empirically investigate how our symmetry breaking approaches affect solving performance.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3487\u20133503",
        "numpages": "17"
    },
    "Evolutionary Generation of Test Suites for Multi-Path Coverage of MPI Programs With Non-Determinism": {
        "type": "article",
        "key": "10.1109/TSE.2023.3263509",
        "author": "Sun, Baicai and Gong, Dunwei and Pan, Feng and Yao, Xiangjuan and Tian, Tian",
        "title": "Evolutionary Generation of Test Suites for Multi-Path Coverage of MPI Programs With Non-Determinism",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3263509",
        "doi": "10.1109/TSE.2023.3263509",
        "abstract": "When a large number of target paths in a sequential program need to be covered, we can divide similar target paths into the same group, and generate a test suite covering the same group of target paths at the same time, so as to reduce the testing cost. However, different communication edges may be run under a same test input when executing a &lt;bold&gt;M&lt;/bold&gt;essage-&lt;bold&gt;P&lt;/bold&gt;assing &lt;bold&gt;I&lt;/bold&gt;nterface (MPI) program with non-determinism, which cause different code fragments may be traversed, indicating the difficulty of generating a test suite to cover each group of target paths. This paper proposes an approach to evolutionary generation of test suites for multi-path coverage of MPI programs with non-determinism, which can significantly reduce the testing cost and difficulty. We first design an indicator for evaluating each traversal set of communication edges, which is used to form a relation matrix between each target path and each traversal set of communication edges, so as to divide all the target paths into a certain amount of groups. Then, we construct an optimization model for test suite generation associated with each group. Finally, an evolutionary optimization algorithm is extended to solve each model, and used to generate a test suite covering each group of target paths. The proposed approach is utilized and compared with several state-of-the-art approaches to seven benchmark MPI programs, as well as the experimental results illustrate that the proposed approach can efficiently generate a test suite, thus supporting the superiority of the proposed approach.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3504\u20133523",
        "numpages": "20"
    },
    "Specializing Neural Networks for Cryptographic Code Completion Applications": {
        "type": "article",
        "key": "10.1109/TSE.2023.3265362",
        "author": "Xiao, Ya and Song, Wenjia and Qi, Jingyuan and Viswanath, Bimal and McDaniel, Patrick and Yao, Danfeng",
        "title": "Specializing Neural Networks for Cryptographic Code Completion Applications",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3265362",
        "doi": "10.1109/TSE.2023.3265362",
        "abstract": "Similarities between natural languages and programming languages have prompted researchers to apply neural network models to software problems, such as code generation and repair. However, program-specific characteristics pose unique prediction challenges that require the design of new and specialized neural network solutions. In this work, we identify new prediction challenges in application programming interface (API) completion tasks and find that existing solutions are unable to capture complex program dependencies in program semantics and structures. We design a new neural network model Multi-HyLSTM to overcome the newly identified challenges and comprehend complex dependencies between API calls. Our neural network is empowered with a specialized dataflow analysis to extract multiple global API dependence paths for neural network predictions. We evaluate Multi-HyLSTM on 64,478 Android Apps and predict 774,460 Java cryptographic API calls that are usually challenging for developers to use correctly. Our Multi-HyLSTM achieves an excellent top-1 API completion accuracy at 98.99%. Moreover, we show the effectiveness of our design choices through an ablation study and have released our dataset.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3524\u20133535",
        "numpages": "12"
    },
    "Detecting and Characterizing Propagation of Security Weaknesses in Puppet-Based Infrastructure Management": {
        "type": "article",
        "key": "10.1109/TSE.2023.3265962",
        "author": "Rahman, Akond and Parnin, Chris",
        "title": "Detecting and Characterizing Propagation of Security Weaknesses in Puppet-Based Infrastructure Management",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3265962",
        "doi": "10.1109/TSE.2023.3265962",
        "abstract": "Despite being beneficial for managing computing infrastructure automatically, Puppet manifests are susceptible to security weaknesses, e.g., hard-coded secrets and use of weak cryptography algorithms. Adequate mitigation of security weaknesses in Puppet manifests is thus necessary to secure computing infrastructure that are managed with Puppet manifests. A characterization of how security weaknesses propagate and affect Puppet-based infrastructure management, can inform practitioners on the relevance of the detected security weaknesses, as well as help them take necessary actions for mitigation. We conduct an empirical study with 17,629 Puppet manifests with &lt;bold&gt;Taint&lt;/bold&gt; Tracker for &lt;bold&gt;Pup&lt;/bold&gt;pet Manifests (&lt;bold&gt;TaintPup&lt;/bold&gt;). We observe 2.4 times more precision, and 1.8 times more F-measure for TaintPup, compared to that of a state-of-the-art security static analysis tool. From our empirical study, we observe security weaknesses to propagate into 4,457 resources, i.e, Puppet-specific code elements used to manage infrastructure. A single instance of a security weakness can propagate into as many as 35 distinct resources. We observe security weaknesses to propagate into 7 categories of resources, which include resources used to manage continuous integration servers and network controllers. According to our survey with 24 practitioners, propagation of security weaknesses into data storage-related resources is rated to have the most severe impact for Puppet-based infrastructure management.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3536\u20133553",
        "numpages": "18"
    },
    "DeLag: Using Multi-Objective Optimization to Enhance the Detection of Latency Degradation Patterns in Service-Based Systems": {
        "type": "article",
        "key": "10.1109/TSE.2023.3266041",
        "author": "Traini, Luca and Cortellessa, Vittorio",
        "title": "DeLag: Using Multi-Objective Optimization to Enhance the Detection of Latency Degradation Patterns in Service-Based Systems",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3266041",
        "doi": "10.1109/TSE.2023.3266041",
        "abstract": "Performance debugging in production is a fundamental activity in modern service-based systems. The diagnosis of performance issues is often time-consuming, since it requires thorough inspection of large volumes of traces and performance indices. In this paper we present DeLag, a novel automated search-based approach for diagnosing performance issues in service-based systems. DeLag identifies subsets of requests that show, in the combination of their Remote Procedure Call execution times, symptoms of potentially relevant performance issues. We call such symptoms &lt;italic&gt;Latency Degradation Patterns&lt;/italic&gt;. DeLag simultaneously searches for multiple &lt;italic&gt;latency degradation patterns&lt;/italic&gt; while optimizing precision, recall and latency dissimilarity. Experimentation on 700 datasets of requests generated from two microservice-based systems shows that our approach provides better and more stable effectiveness than three state-of-the-art approaches and general purpose machine learning clustering algorithms. DeLag is more effective than all baseline techniques in at least one case study (with &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$pleq 0.05$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;\u2264&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mn&gt;05&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"traini-ieq1-3266041.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; and non-negligible effect size). Moreover, DeLag outperforms in terms of efficiency the second and the third most effective baseline techniques on the largest datasets used in our evaluation (up to 22%).",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3554\u20133580",
        "numpages": "27"
    },
    "A Zone-Based Model for Analysis of Dependent Failures in Requirements Inspection": {
        "type": "article",
        "key": "10.1109/TSE.2023.3266157",
        "author": "Li, Boyuan and Smidts, Carol",
        "title": "A Zone-Based Model for Analysis of Dependent Failures in Requirements Inspection",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3266157",
        "doi": "10.1109/TSE.2023.3266157",
        "abstract": "In the software development life cycle, the quality of the requirements specification affects the overall quality of the subsequent phases and hence, the software product. The requirements specification is usually inspected by an inspection team to detect defects. To enhance the quality of the requirements specification, one conventional strategy usually used is adding redundancies to the inspection team. However, this strategy suffers from the problem of dependent failures of the redundant inspectors which was not studied systematically in previous research. To analyze the dependent failures and independent failures in an inspection team, this paper first defines the independent failures and dependent failures in an inspection team from the perspective of human errors. Then a quantification model, i.e., the Zone-based Model, is proposed to analyze the dependent and independent failures. The Zone-based Model considers the following situations: 1) the probability of failures of an inspector may be high; 2) the probability of failures of the inspectors may be different; 3) the failures in an inspection team can be a combination of dependent failures and independent failures. By considering all those situations, the Z model has a meaningful interpretation and a convincing assessment of the failures of an inspection team. To verify the effectiveness of the new model, the Zone-based model is compared to conventional models using simulation data. The results show that the Zone-based model is significantly better than the traditional models in analyzing the independent and dependent failures.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3581\u20133598",
        "numpages": "18"
    },
    "Taming Android Fragmentation Through Lightweight Crowdsourced Testing": {
        "type": "article",
        "key": "10.1109/TSE.2023.3266324",
        "author": "Sun, Xiaoyu and Chen, Xiao and Liu, Yonghui and Grundy, John and Li, Li",
        "title": "Taming Android Fragmentation Through Lightweight Crowdsourced Testing",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3266324",
        "doi": "10.1109/TSE.2023.3266324",
        "abstract": "Android fragmentation refers to the overwhelming diversity of Android devices and OS versions. These lead to the impossibility of testing an app on every supported device, leaving a number of compatibility bugs scattered in the community and thereby resulting in poor user experiences. To mitigate this, our fellow researchers have designed various works to automatically detect such compatibility issues. However, the current state-of-the-art tools can only be used to detect specific kinds of compatibility issues (i.e., compatibility issues caused by API signature evolution), i.e., many other essential types of compatibility issues are still unrevealed. For example, customized OS versions on real devices and semantic changes of OS could lead to serious compatibility issues, which are non-trivial to be detected statically. To this end, we propose a novel, lightweight, crowdsourced testing approach, &lt;sc&gt;LazyCow&lt;/sc&gt;, to fill this research gap and enable the possibility of taming Android fragmentation through crowdsourced efforts. Specifically, crowdsourced testing is an emerging alternative to conventional mobile testing mechanisms that allow developers to test their products on real devices to pinpoint platform-specific issues. Experimental results on thousands of test cases on real-world Android devices show that &lt;sc&gt;LazyCow&lt;/sc&gt; is effective in automatically identifying and verifying API-induced compatibility issues. Also, after investigating the user experience through qualitative metrics, users\u2019 satisfaction provides strong evidence that &lt;sc&gt;LazyCow&lt;/sc&gt; is useful and welcome in practice.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3599\u20133615",
        "numpages": "17"
    },
    "&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${\\text{FS}^{3}_{\\text{change}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mtext&gt;FS&lt;/mml:mtext&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;mml:mtext&gt;change&lt;/mml:mtext&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"janke-ieq1-3269500.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;: A Scalable Method for Change Pattern Mining": {
        "type": "article",
        "key": "10.1109/TSE.2023.3269500",
        "author": "Janke, Mario and M\\\"{a}der, Patrick",
        "title": "&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;${\\text{FS}^{3}_{\\text{change}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mtext&gt;FS&lt;/mml:mtext&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;mml:mtext&gt;change&lt;/mml:mtext&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"janke-ieq1-3269500.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;: A Scalable Method for Change Pattern Mining",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3269500",
        "doi": "10.1109/TSE.2023.3269500",
        "abstract": "Mining change patterns can give unique understanding on the evolution of dynamically changing systems like social relation graphs, weblinks, hardware descriptions and models. A more recent focus is source code change pattern mining that may qualitatively justify expected or uncover unexpected patterns. These patterns then offer a basis, e.g., for program language evolution or auto-completion support. We present a change pattern mining method that greatly expands the limits of input data and pattern complexity, over existing methods. We propose scalability solutions on conceptual and algorithmic level, thereby evolving the state-of-the-art sampling-based frequent subgraph mining method FS&lt;sup&gt;3&lt;/sup&gt;, resulting in 75% reduction in memory consumption and a speedup of 6500 for a large scale dataset. Patterns can have 100,000 s of occurrences for which manual review is impossible and may lead to misinterpretation. We propose the novel content track approach for interactively exploring pattern contents in context, based on marginal distributions. We evaluate our approach by mining 1,000 open source projects contributing a total of 558 million changes and 2 billion contextual connections among them, thereby, demonstrating its scalability. A manual interpretation of 19 patterns shows sensible mined patterns allowing to deduct implications for language design and demonstrating the soundness of the approach.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3616\u20133629",
        "numpages": "14"
    },
    "Combatting Front-Running in Smart Contracts: Attack Mining, Benchmark Construction and Vulnerability Detector Evaluation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3270117",
        "author": "Zhang, Wuqi and Wei, Lili and Cheung, Shing-Chi and Liu, Yepang and Li, Shuqing and Liu, Lu and Lyu, Michael R.",
        "title": "Combatting Front-Running in Smart Contracts: Attack Mining, Benchmark Construction and Vulnerability Detector Evaluation",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3270117",
        "doi": "10.1109/TSE.2023.3270117",
        "abstract": "Front-running attacks have been a major concern on the blockchain. Attackers launch front-running attacks by inserting additional transactions before upcoming victim transactions to manipulate victim transaction executions and make profits. Recent studies have shown that front-running attacks are prevalent on the Ethereum blockchain and have caused millions of US dollars loss. It is the vulnerabilities in smart contracts, which are blockchain programs invoked by transactions, that enable the front-running attack opportunities. Although techniques to detect front-running vulnerabilities have been proposed, their performance on real-world vulnerable contracts is unclear. There is no large-scale benchmark based on real attacks to evaluate their capabilities. We make four contributions in this paper. First, we design an effective algorithm to mine real-world attacks in the blockchain history. The evaluation shows that our mining algorithm is more effective and comprehensive, achieving higher recall in finding real attacks than the previous study. Second, we propose an automated and scalable vulnerability localization approach to localize code snippets in smart contracts that enable front-running attacks. The evaluation also shows that our localization approaches are effective in achieving higher precision in pinpointing vulnerabilities compared to the baseline technique. Third, we build a benchmark consisting of 513 real-world attacks with vulnerable code labeled in 235 distinct smart contracts, which is useful to help understand the nature of front-running attacks, vulnerabilities in smart contracts, and evaluate vulnerability detection techniques. Last but not least, we conduct an empirical evaluation of seven state-of-the-art vulnerability detection techniques on our benchmark. The evaluation experiment reveals the inadequacy of existing techniques in detecting front-running vulnerabilities, with a low recall of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$leq$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u2264&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"cheung-ieq1-3270117.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; 6.04%. Our further analysis identifies four common limitations in existing techniques: lack of support for inter-contract analysis, inefficient constraint solving for cryptographic operations, improper vulnerability patterns, and lack of token support.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3630\u20133646",
        "numpages": "17"
    },
    "A Taxonomy of Testable HTML5 Canvas Issues": {
        "type": "article",
        "key": "10.1109/TSE.2023.3270740",
        "author": "Macklon, Finlay and Viggiato, Markos and Romanova, Natalia and Buzon, Chris and Paas, Dale and Bezemer, Cor-Paul",
        "title": "A Taxonomy of Testable HTML5 Canvas Issues",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3270740",
        "doi": "10.1109/TSE.2023.3270740",
        "abstract": "The HTML5 &lt;monospace&gt;&lt;canvas&gt;&lt;/monospace&gt; is widely used to display high quality graphics in web applications. However, the combination of web, GUI, and visual techniques that are required to build &lt;monospace&gt;&lt;canvas&gt;&lt;/monospace&gt; applications, together with the lack of testing and debugging tools, makes developing such applications very challenging. To help direct future research on testing &lt;monospace&gt;&lt;canvas&gt;&lt;/monospace&gt; applications, in this paper we present a taxonomy of testable &lt;monospace&gt;&lt;canvas&gt;&lt;/monospace&gt; issues. First, we extracted 2,403 &lt;monospace&gt;&lt;canvas&gt;&lt;/monospace&gt;-related issue reports from 123 open source GitHub projects that use the HTML5 &lt;monospace&gt;&lt;canvas&gt;&lt;/monospace&gt;. Second, we constructed our taxonomy by manually classifying a random sample of 332 issue reports. Our manual classification identified five broad categories of testable &lt;monospace&gt;&lt;canvas&gt;&lt;/monospace&gt; issues, such as &lt;italic&gt;Visual&lt;/italic&gt; and &lt;italic&gt;Performance&lt;/italic&gt; issues. We found that &lt;italic&gt;Visual&lt;/italic&gt; issues are the most frequent (35%), while &lt;italic&gt;Performance&lt;/italic&gt; issues are relatively infrequent (5%). We also found that many testable &lt;monospace&gt;&lt;canvas&gt;&lt;/monospace&gt; issues that present themselves visually on the &lt;monospace&gt;&lt;canvas&gt;&lt;/monospace&gt; are actually caused by other components of the web application. Our taxonomy of testable &lt;monospace&gt;&lt;canvas&gt;&lt;/monospace&gt; issues can be used to steer future research into &lt;monospace&gt;&lt;canvas&gt;&lt;/monospace&gt; issues and testing.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3647\u20133659",
        "numpages": "13"
    },
    "Test Report Generation for Android App Testing Via Heterogeneous Data Analysis": {
        "type": "article",
        "key": "10.1109/TSE.2023.3237247",
        "author": "Fang, Chunrong and Yu, Shengcheng and Su, Ting and Zhang, Jing and Tian, Yuanhan and Liu, Yang",
        "title": "Test Report Generation for Android App Testing Via Heterogeneous Data Analysis",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3237247",
        "doi": "10.1109/TSE.2023.3237247",
        "abstract": "The rising of the Android market demands higher quality assurance of Android applications (apps) to sharpen the competitive edge, and techniques for traditional software have problems adapting for mobile apps. Android apps often require testing on a large-scale device cluster, which produces a large amount of test reports consisting of heterogeneous data, e.g., hardware information, GUI screenshots, runtime logs. Such data are hard to merge to be unified analyzed, while they serve as an essential basis for bug inspection and fixing. Existing test report generation or analysis techniques can only handle testing data from different devices separately. They simply list all the information to app developers and have no further processing to summarize test reports. Besides, they neglect the inner connection of the heterogeneous data. Such techniques cannot improve the report reviewing effectiveness and efficiency, and they can hardly find the inner links and rules of the bug occurrence on different devices. As a result, developers still need to devote many efforts to inspect and fix bugs. In this paper, a large amount of test reports are investigated by the authors, as to construct a structured bug model to analyze heterogeneous data of the testing results. According to the investigation, we also define the &lt;sc&gt;Bug Inconsistency&lt;/sc&gt; of testing results from multiple devices and build a novel bug taxonomy. In general, an automated approach is proposed to generate structured and comprehensible test reports from raw testing results from multiple devices. Based on the approach, a tool, namely &lt;sc&gt;BreGat&lt;/sc&gt;, is implemented to evaluate the classification and deduplication capability of our approach. The experimental results of 30 Android apps on 20 devices show that &lt;sc&gt;BreGat&lt;/sc&gt; can successfully cover 83% bug categories and exclude 76% duplicate bugs. Furthermore, a user study involving 16 developers shows that our test reports are more comprehensible and &lt;sc&gt;BreGat&lt;/sc&gt; greatly improves the bug inspection efficiency compared to the state-of-the-art tool.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3032\u20133051",
        "numpages": "20"
    },
    "Challenging Machine Learning-Based Clone Detectors via Semantic-Preserving Code Transformations": {
        "type": "article",
        "key": "10.1109/TSE.2023.3240118",
        "author": "Zhang, Weiwei and Guo, Shengjian and Zhang, Hongyu and Sui, Yulei and Xue, Yinxing and Xu, Yun",
        "title": "Challenging Machine Learning-Based Clone Detectors via Semantic-Preserving Code Transformations",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3240118",
        "doi": "10.1109/TSE.2023.3240118",
        "abstract": "Software clone detection identifies similar or identical code snippets. It has been an active research topic that attracts extensive attention over the last two decades. In recent years, machine learning (ML) based detectors, especially deep learning-based ones, have demonstrated impressive capability on clone detection. It seems that this longstanding problem has already been tamed owing to the advances in ML techniques. In this work, we would like to challenge the robustness of the recent ML-based clone detectors through code semantic-preserving transformations. We first utilize fifteen simple code transformation operators combined with commonly-used heuristics (i.e., Random Search, Genetic Algorithm, and Markov Chain Monte Carlo) to perform equivalent program transformation. Furthermore, we propose a deep reinforcement learning-based sequence generation (DRLSG) strategy to effectively guide the search process of generating clones that could escape from the detection. We then evaluate the ML-based detectors with the pairs of original and generated clones. We realize our method in a framework named CloneGen (stands for Clone Generator). CloneGen In evaluation, we challenge the three state-of-the-art ML-based detectors and four traditional detectors with the code clones after semantic-preserving transformations via the aid of CloneGen. Surprisingly, our experiments show that, despite the notable successes achieved by existing clone detectors, the ML models inside these detectors still cannot distinguish numerous clones produced by the code transformations in CloneGen. In addition, adversarial training of ML-based clone detectors using clones generated by CloneGen can improve their robustness and accuracy. Meanwhile, compared with the commonly-used heuristics, the DRLSG strategy has shown the best effectiveness in generating code clones to decrease the detection accuracy of the ML-based detectors. Our investigation reveals an explicable but always ignored robustness issue of the latest ML-based detectors. Therefore, we call for more attention to the robustness of these new ML-based detectors.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3052\u20133070",
        "numpages": "19"
    },
    "TrinityRCL: Multi-Granular and Code-Level Root Cause Localization Using Multiple Types of Telemetry Data in Microservice Systems": {
        "type": "article",
        "key": "10.1109/TSE.2023.3241299",
        "author": "Gu, Shenghui and Rong, Guoping and Ren, Tian and Zhang, He and Shen, Haifeng and Yu, Yongda and Li, Xian and Ouyang, Jian and Chen, Chunan",
        "title": "TrinityRCL: Multi-Granular and Code-Level Root Cause Localization Using Multiple Types of Telemetry Data in Microservice Systems",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3241299",
        "doi": "10.1109/TSE.2023.3241299",
        "abstract": "The microservice architecture has been commonly adopted by large scale software systems exemplified by a wide range of online services. Service monitoring through anomaly detection and root cause analysis (RCA) is crucial for these microservice systems to provide stable and continued services. However, compared with monolithic systems, software systems based on the layered microservice architecture are inherently complex and commonly involve entities at different levels of granularity. Therefore, for effective service monitoring, these systems have a special requirement of multi-granular RCA. Furthermore, as a large proportion of anomalies in microservice systems pertain to problematic code, to timely troubleshoot these anomalies, these systems have another special requirement of RCA at the finest code-level. Microservice systems rely on telemetry data to perform service monitoring and RCA of service anomalies. The majority of existing RCA approaches are only based on a single type of telemetry data and as a result can only support uni-granular RCA at either application-level or service-level. Although there are attempts to combine metric and tracing data in RCA, their objective is to improve RCA's efficiency or accuracy rather than to support multi-granular RCA. In this article, we propose a new RCA solution &lt;italic&gt;TrinityRCL&lt;/italic&gt; that is able to localize the root causes of anomalies at multiple levels of granularity including application-level, service-level, host-level, and metric-level, with the unique capability of code-level localization by harnessing all three types of telemetry data to construct a causal graph representing the intricate, dynamic, and nondeterministic relationships among the various entities related to the anomalies. By implementing and deploying &lt;italic&gt;TrinityRCL&lt;/italic&gt; in a real production environment, we evaluate &lt;italic&gt;TrinityRCL&lt;/italic&gt; against two baseline methods and the results show that &lt;italic&gt;TrinityRCL&lt;/italic&gt; has a significant performance advantage in terms of accuracy at the same level of granularity with comparable efficiency and is particularly effective to support large-scale systems with massive telemetry data.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3071\u20133088",
        "numpages": "18"
    },
    "Static Analysis of JNI Programs via Binary Decompilation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3241639",
        "author": "Park, Jihee and Lee, Sungho and Hong, Jaemin and Ryu, Sukyoung",
        "title": "Static Analysis of JNI Programs via Binary Decompilation",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3241639",
        "doi": "10.1109/TSE.2023.3241639",
        "abstract": "JNI programs are widely used thanks to the combined benefits of C and Java programs. However, because understanding the interaction behaviors between two different programming languages is challenging, JNI program development is difficult to get right and vulnerable to security attacks. Thus, researchers have proposed static analysis of JNI program source code to detect bugs and security vulnerabilities in JNI programs. Unfortunately, such source code analysis is not applicable to compiled JNI programs that are not open-sourced or open-source JNI programs containing third-party binary libraries. While JN-SAF, the state-of-the-art analyzer for compiled JNI programs, can analyze binary code, it has several limitations due to its symbolic execution and summary-based bottom-up analysis. In this paper, we propose a novel approach to statically analyze compiled JNI programs without their source code using binary decompilation. Unlike JN-SAF that analyzes binaries directly, our approach decompiles binaries and analyzes JNI programs with the decompiled binaries using an existing JNI program analyzer for source code. To decompile binaries to compilable C source code with precise JNI-interoperation-related types, we improve an existing decompilation tool by leveraging the characteristics of JNI programs. Our evaluation shows that the approach is precise as almost the same as the state-of-the-art JNI program analyzer for source code, and more precise than JN-SAF.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3089\u20133105",
        "numpages": "17"
    },
    "Understanding Mentors\u2019 Engagement in OSS Communities via Google Summer of Code": {
        "type": "article",
        "key": "10.1109/TSE.2023.3242415",
        "author": "Tan, Xin and Zhou, Minghui and Zhang, Li",
        "title": "Understanding Mentors\u2019 Engagement in OSS Communities via Google Summer of Code",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3242415",
        "doi": "10.1109/TSE.2023.3242415",
        "abstract": "A constant influx of newcomers is essential for the sustainability and success of open source software (OSS) projects. However, successful onboarding is always challenging because newcomers face various initial contributing barriers. To support newcomer onboarding, OSS communities widely adopt the mentoring approach. Despite its significance, previous mentoring studies tend to focus on the newcomer's perspective, leaving the mentor's perspective relatively under-studied. To better support mentoring, we study the popular Google Summer of Code (GSoC). It is a well-established global program that offers stipends and mentors to students aiming to bring more student developers into OSS development. We combine online data analysis, an email survey, and semi-structured interviews with the GSoC mentors to understand their motivations, challenges, strategies, and gains. We propose a taxonomy of GSoC mentors\u2019 engagement with four themes, ten categories, 34 sub-categories, and 118 codes, as well as the mentors\u2019 attitudes toward the codes. In particular, we find that mentors participating in GSoC are primarily intrinsically motivated, and some new motivators emerge adapting to the contemporary challenges, e.g., sustainability and advertisement of projects. Forty-one challenges and 52 strategies associated with the program timeline are identified, most of which are first time revealed. Although almost all the challenges are agreed upon by specific mentors, some mentors believe that several challenges are reasonable and even have a positive effect. For example, the cognitive differences between mentors and mentees can stimulate new perspectives. Most of the mentors agreed that they had adopted these strategies during the mentoring process, but a few strategies recommended by the GSoC administration were not agreed upon. Self-satisfaction, different skills, and peer recognition are the main gains of mentors to participate in GSoC. Eventually, we discuss practical implications for mentors, students, OSS communities, GSoC programs, and researchers.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3106\u20133130",
        "numpages": "25"
    },
    "Trace Diagnostics for Signal-Based Temporal Properties": {
        "type": "article",
        "key": "10.1109/TSE.2023.3242588",
        "author": "Boufaied, Chaima and Menghi, Claudio and Bianculli, Domenico and Briand, Lionel C.",
        "title": "Trace Diagnostics for Signal-Based Temporal Properties",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3242588",
        "doi": "10.1109/TSE.2023.3242588",
        "abstract": "Trace checking is a verification technique widely used in Cyber-physical system (CPS) development, to verify whether execution traces satisfy or violate properties expressing system requirements. Often these properties characterize complex signal behaviors and are defined using domain-specific languages, such as SB-TemPsy-DSL, a pattern-based specification language for signal-based temporal properties. Most of the trace-checking tools only yield a Boolean verdict. However, when a property is violated by a trace, engineers usually inspect the trace to understand the cause of the violation; such manual diagnostic is time-consuming and error-prone. Existing approaches that complement trace-checking tools with diagnostic capabilities either produce low-level explanations that are hardly comprehensible by engineers or do not support complex signal-based temporal properties. In this paper, we propose &lt;italic&gt;TD-SB-TemPsy&lt;/italic&gt;, a trace-diagnostic approach for properties expressed using SB-TemPsy-DSL. Given a property and a trace that violates the property, &lt;italic&gt;TD-SB-TemPsy&lt;/italic&gt; determines the root cause of the property violation. &lt;italic&gt;TD-SB-TemPsy&lt;/italic&gt; relies on the concepts of &lt;italic&gt;violation cause&lt;/italic&gt;, which characterizes one of the behaviors of the system that may lead to a property violation, and &lt;italic&gt;diagnoses&lt;/italic&gt;, which are associated with violation causes and provide additional information to help engineers understand the violation cause. As part of &lt;italic&gt;TD-SB-TemPsy&lt;/italic&gt;, we propose a language-agnostic methodology to define violation causes and diagnoses. In our context, its application resulted in a catalog of 34 violation causes, each associated with one diagnosis, tailored to properties expressed in SB-TemPsy-DSL. We assessed the applicability of &lt;italic&gt;TD-SB-TemPsy&lt;/italic&gt; on two datasets, including one based on a complex industrial case study. The results show that &lt;italic&gt;TD-SB-TemPsy&lt;/italic&gt; could finish within a timeout of 1 min for &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$approx 83.66\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;\u2248&lt;/mml:mo&gt;&lt;mml:mn&gt;83&lt;/mml:mn&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mn&gt;66&lt;/mml:mn&gt;&lt;mml:mo&gt;\\%&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"boufaied-ieq1-3242588.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; of the trace-property combinations in the industrial dataset, yielding a diagnosis in &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$approx 99.84\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;\u2248&lt;/mml:mo&gt;&lt;mml:mn&gt;99&lt;/mml:mn&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mn&gt;84&lt;/mml:mn&gt;&lt;mml:mo&gt;\\%&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"boufaied-ieq2-3242588.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; of these cases; moreover, it also yielded a diagnosis for all the trace-property combinations in the other dataset. These results suggest that our tool is applicable and efficient in most cases.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3131\u20133154",
        "numpages": "24"
    },
    "&lt;sc&gt;Plumber&lt;/sc&gt;: Boosting the Propagation of Vulnerability Fixes in the &lt;italic&gt;npm&lt;/italic&gt; Ecosystem": {
        "type": "article",
        "key": "10.1109/TSE.2023.3243262",
        "author": "Wang, Ying and Sun, Peng and Pei, Lin and Yu, Yue and Xu, Chang and Cheung, Shing-Chi and Yu, Hai and Zhu, Zhiliang",
        "title": "&lt;sc&gt;Plumber&lt;/sc&gt;: Boosting the Propagation of Vulnerability Fixes in the &lt;italic&gt;npm&lt;/italic&gt; Ecosystem",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3243262",
        "doi": "10.1109/TSE.2023.3243262",
        "abstract": "Vulnerabilities are known reported security threats that affect a large amount of packages in the &lt;italic&gt;npm&lt;/italic&gt; ecosystem. To mitigate these security threats, the open-source community strongly suggests vulnerable packages to timely publish vulnerability fixes and recommends affected packages to update their dependencies. However, there are still serious lags in the propagation of vulnerability fixes in the ecosystem. In our preliminary study on the latest versions of 356,283 active &lt;italic&gt;npm&lt;/italic&gt; packages, we found that 20.0% of them can still introduce vulnerabilities via direct or transitive dependencies although the involved vulnerable packages have already published fix versions for over a year. Prior study by (Chinthanet et al. 2021) lays the groundwork for research on how to mitigate propagation lags of vulnerability fixes in an ecosystem. They conducted an empirical investigation to identify lags that might occur between the vulnerable package release and its fixing release. They found that factors such as the branch upon which a fix landed and the severity of the vulnerability had a small effect on its propagation trajectory throughout the ecosystem. To ensure quick adoption and propagation of a release that contains the fix, they gave several actionable advice to developers and researchers. However, it is still an open question how to design an effective technique to accelerate the propagation of vulnerability fixes. Motivated by this problem, in this paper, we conducted an empirical study to learn the scale of packages that block the propagation of vulnerability fixes in the ecosystem and investigate their evolution characteristics. Furthermore, we distilled the remediation strategies that have better effects on mitigating the fix propagation lags. Leveraging our empirical findings, we propose an ecosystem-level technique, &lt;sc&gt;Plumber&lt;/sc&gt;, for deriving feasible remediation strategies to boost the propagation of vulnerability fixes. To precisely diagnose the causes of fix propagation blocking, &lt;sc&gt;Plumber&lt;/sc&gt; models the vulnerability metadata, and &lt;italic&gt;npm&lt;/italic&gt; dependency metadata and continuously monitors their evolution. By analyzing a full-picture of the ecosystem-level dependency graph and the corresponding fix propagation statuses, it derives remediation schemes for pivotal packages. In the schemes, &lt;sc&gt;Plumber&lt;/sc&gt; provides customized remediation suggestions with vulnerability impact analysis to arouse package developers\u2019 awareness. We applied &lt;sc&gt;Plumber&lt;/sc&gt; to generating 268 remediation reports for the identified pivotal packages, to evaluate its remediation effectiveness based on developers\u2019 feedback. Encouragingly, 47.4% our remediation reports received positive feedback from many well-known &lt;italic&gt;npm&lt;/italic&gt; projects, such as &lt;monospace&gt;Tensorflow/tfjs&lt;/monospace&gt;, &lt;monospace&gt;Ethers.js&lt;/monospace&gt;, and &lt;monospace&gt;GoogleChrome/workbox&lt;/monospace&gt;. Our reports have boosted the propagation of vulnerability fixes into 16,403 root packages through 92,469 dependency paths. On average, each remediated package version is receiving 72,678 downloads per week by the time of this work.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3155\u20133181",
        "numpages": "27"
    },
    "Black-Box Testing of Deep Neural Networks through Test Case Diversity": {
        "type": "article",
        "key": "10.1109/TSE.2023.3243522",
        "author": "Aghababaeyan, Zohreh and Abdellatif, Manel and Briand, Lionel and S, Ramesh and Bagherzadeh, Mojtaba",
        "title": "Black-Box Testing of Deep Neural Networks through Test Case Diversity",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3243522",
        "doi": "10.1109/TSE.2023.3243522",
        "abstract": "Deep Neural Networks (DNNs) have been extensively used in many areas including image processing, medical diagnostics and autonomous driving. However, DNNs can exhibit erroneous behaviours that may lead to critical errors, especially when used in safety-critical systems. Inspired by testing techniques for traditional software systems, researchers have proposed neuron coverage criteria, as an analogy to source code coverage, to guide the testing of DNNs. Despite very active research on DNN coverage, several recent studies have questioned the usefulness of such criteria in guiding DNN testing. Further, from a practical standpoint, these criteria are white-box as they require access to the internals or training data of DNNs, which is often not feasible or convenient. Measuring such coverage requires executing DNNs with candidate inputs to guide testing, which is not an option in many practical contexts. In this paper, we investigate diversity metrics as an alternative to white-box coverage criteria. For the previously mentioned reasons, we require such metrics to be black-box and not rely on the execution and outputs of DNNs under test. To this end, we first select and adapt three diversity metrics and study, in a controlled manner, their capacity to measure actual diversity in input sets. We then analyze their statistical association with fault detection using four datasets and five DNNs. We further compare diversity with state-of-the-art white-box coverage criteria. As a mechanism to enable such analysis, we also propose a novel way to estimate fault detection in DNNs. Our experiments show that relying on the diversity of image features embedded in test input sets is a more reliable indicator than coverage criteria to effectively guide DNN testing. Indeed, we found that one of our selected black-box diversity metrics far outperforms existing coverage criteria in terms of fault-revealing capability and computational time. Results also confirm the suspicions that state-of-the-art coverage criteria are not adequate to guide the construction of test input sets to detect as many faults as possible using natural inputs.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3182\u20133204",
        "numpages": "23"
    },
    "NCQ: Code Reuse Support for Node.js Developers": {
        "type": "article",
        "key": "10.1109/TSE.2023.3248113",
        "author": "Reid, Brittany and d'Amorim, Marcelo and Wagner, Markus and Treude, Christoph",
        "title": "NCQ: Code Reuse Support for Node.js Developers",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3248113",
        "doi": "10.1109/TSE.2023.3248113",
        "abstract": "Code reuse is an important part of software development. The adoption of code reuse practices is especially common among Node.js developers. The Node.js package manager, NPM, indexes over 1 Million packages and developers often seek out packages to solve programming tasks. Due to the vast number of packages, selecting the right package is difficult and time consuming. With the goal of improving productivity of developers that heavily reuse code through third-party packages, we present &lt;italic&gt;Node Code Query&lt;/italic&gt; (NCQ), a Read-Eval-Print-Loop environment that allows developers to 1) search for NPM packages using natural language queries, 2) search for code snippets related to those packages, 3) automatically correct errors in these code snippets, 4) quickly setup new environments for testing those snippets, and 5) transition between search and editing modes. In two user studies with a total of 20 participants, we find that participants begin programming faster and conclude tasks faster with NCQ than with baseline approaches, and that they like, among other features, the search for code snippets and packages. Our results suggest that NCQ makes Node.js developers more efficient in reusing code.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3205\u20133225",
        "numpages": "21"
    },
    "Beyond Literal Meaning: Uncover and Explain Implicit Knowledge in Code Through Wikipedia-Based Concept Linking": {
        "type": "article",
        "key": "10.1109/TSE.2023.3250029",
        "author": "Wang, Chong and Peng, Xin and Xing, Zhenchang and Meng, Xiujie",
        "title": "Beyond Literal Meaning: Uncover and Explain Implicit Knowledge in Code Through Wikipedia-Based Concept Linking",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3250029",
        "doi": "10.1109/TSE.2023.3250029",
        "abstract": "When reusing or modifying code, developers need to understand the implicit knowledge behind a piece of code in addition to the literal meaning of code. Such implicit knowledge involves related concepts and their explanations. Uncovering and understanding the implicit knowledge in code are challenging due to the extensive use of abbreviations, scattered expressions of concepts, and ambiguity of concept mentions. In this paper, we propose an automatic approach (called CoLiCo) that can uncover implicit concepts in code and link the uncovered concepts to Wikipedia. Based on a trained identifier embedding model, CoLiCo identifies Wikipedia concepts mentioned in a given code snippet and excerpts a paragraph-level explanation from Wikipedia for each concept. During the process, CoLiCo resolves identifier abbreviation (i.e., concepts mentioned in the form of abbreviations) and identifier aggregation (i.e., concepts mentioned by an aggregation of multiple identifiers) based on identifier embedding and mining of identifier abbreviation/aggregation relations. Experimental study shows that CoLiCo outperforms a general entity linking approach by 38.7% in the correctness of concept linking and identifies 96.7% more correct concept linkings on a dataset with 629 code snippets. The concept linking is significant for program understanding in 54% code snippets. Our user study shows that CoLiCo can significantly shorten the time and improve the correctness in code comprehension tasks that intensively involve implicit knowledge.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3226\u20133240",
        "numpages": "15"
    },
    "Empirical Validation of Automated Vulnerability Curation and Characterization": {
        "type": "article",
        "key": "10.1109/TSE.2023.3250479",
        "author": "Okutan, Ahmet and Mell, Peter and Mirakhorli, Mehdi and Khokhlov, Igor and Santos, Joanna C. S. and Gonzalez, Danielle and Simmons, Steven",
        "title": "Empirical Validation of Automated Vulnerability Curation and Characterization",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3250479",
        "doi": "10.1109/TSE.2023.3250479",
        "abstract": "Prior research has shown that public vulnerability systems such as US National Vulnerability Database (NVD) rely on a manual, time-consuming, and error-prone process which has led to inconsistencies and delays in releasing final vulnerability results. This work provides an approach to curate vulnerability reports in real-time and map textual vulnerability reports to machine readable structured vulnerability attribute data. Designed to support the time consuming human analysis done by vulnerability databases, the system leverages the Common Vulnerabilities and Exposures (CVE) list of vulnerabilities and the vulnerability attributes described by the National Institute of Standards and Technology (NIST) Vulnerability Description Ontology (VDO) framework. Our work uses Natural Language Processing (NLP), Machine Learning (ML) and novel Information Theoretical (IT) methods to provide automated techniques for near real-time publishing, and characterization of vulnerabilities using 28 attributes in 5 domains. Experiment results indicate that vulnerabilities can be evaluated up to 95 hours earlier than using manual methods, they can be characterized with F-Measure values over 0.9, and the proposed automated approach could save up to 47% of the time spent for CVE characterization.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3241\u20133260",
        "numpages": "20"
    },
    "Modelling Second-Order Uncertainty in State Machines": {
        "type": "article",
        "key": "10.1109/TSE.2023.3250835",
        "author": "Walkinshaw, Neil and Hierons, Robert M.",
        "title": "Modelling Second-Order Uncertainty in State Machines",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3250835",
        "doi": "10.1109/TSE.2023.3250835",
        "abstract": "Modelling the behaviour of state-based systems can be challenging, especially when the modeller is not entirely certain about its intended interactions with the user or the environment. Currently, it is possible to associate a stated level of uncertainty with a given event by attaching probabilities to transitions (producing \u2018Probabilistic State Machines\u2019). This captures the \u2018First-order uncertainty\u2019 - the (un-)certainty that a given event will occur. However, this does not permit the modeller to capture their own uncertainty (or lack thereof) about that stated probability - also known as \u2018Second-order uncertainty\u2019. In this article we introduce a generalisation of probabilistic finite state machines that makes it possible to incorporate this important additional dimension of uncertainty. For this we adopt a formalism for reasoning about uncertainty called Subjective Logic. We present an algorithm to create these enhanced state machines automatically from a conventional state machine and a set of observed sequences. We show how this approach can be used for reverse-engineering predictive state machines from traces.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3261\u20133276",
        "numpages": "16"
    },
    "Verification of Fuzzy Decision Trees": {
        "type": "article",
        "key": "10.1109/TSE.2023.3251858",
        "author": "Good, Jack H. and Gisolfi, Nicholas and Miller, Kyle and Dubrawski, Artur",
        "title": "Verification of Fuzzy Decision Trees",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3251858",
        "doi": "10.1109/TSE.2023.3251858",
        "abstract": "In recent years, there have been major strides in the safety verification of machine learning models such as neural networks and tree ensembles. However, fuzzy decision trees (FDT), also called soft or differentiable decision trees, are yet unstudied in the context of verification. They present unique verification challenges resulting from multiplications of input values; in the simplest case with a piecewise-linear splitting function, an FDT is piecewise-polynomial with degree up to the depth of the tree. We propose an abstraction-refinement algorithm for verification of properties of FDTs. We show that the problem is NP-Complete, like many other machine learning verification problems, and that our algorithm is complete in a finite precision setting. We benchmark on a selection of public data sets against an off-the-shelf SMT solver and a baseline variation of our algorithm that uses a refinement strategy from similar methods for neural network verification, finding the proposed method to be the fastest. Code for our algorithm along with our experiments and demos are available on GitHub at &lt;uri&gt;https://github.com/autonlab/fdt_verification&lt;/uri&gt;.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3277\u20133288",
        "numpages": "12"
    },
    "API Usage Recommendation Via Multi-View Heterogeneous Graph Representation Learning": {
        "type": "article",
        "key": "10.1109/TSE.2023.3252259",
        "author": "Chen, Yujia and Gao, Cuiyun and Ren, Xiaoxue and Peng, Yun and Xia, Xin and Lyu, Michael R.",
        "title": "API Usage Recommendation Via Multi-View Heterogeneous Graph Representation Learning",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3252259",
        "doi": "10.1109/TSE.2023.3252259",
        "abstract": "Developers often need to decide which APIs to use for the functions being implemented. With the ever-growing number of APIs and libraries, it becomes increasingly difficult for developers to find appropriate APIs, indicating the necessity of automatic API usage recommendation. Previous studies adopt statistical models or collaborative filtering methods to mine the implicit API usage patterns for recommendation. However, they rely on the occurrence frequencies of APIs for mining usage patterns, thus prone to fail for the low-frequency APIs. Besides, prior studies generally regard the API call interaction graph as homogeneous graph, ignoring the rich information (e.g., edge types) in the structure graph. In this work, we propose a novel method named &lt;italic&gt;MEGA&lt;/italic&gt; for improving the recommendation accuracy especially for the low-frequency APIs. Specifically, besides &lt;italic&gt;call interaction graph&lt;/italic&gt;, MEGA considers another two new heterogeneous graphs: &lt;italic&gt;global API co-occurrence graph&lt;/italic&gt; enriched with the API frequency information and &lt;italic&gt;hierarchical structure graph&lt;/italic&gt; enriched with the project component information. With the three multi-view heterogeneous graphs, MEGA can capture the API usage patterns more accurately. Experiments on three Java benchmark datasets demonstrate that MEGA significantly outperforms the baseline models by at least 19% with respect to the Success Rate@1 metric. Especially, for the low-frequency APIs, MEGA also increases the baselines by at least 55% regarding the Success Rate@1 score.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3289\u20133304",
        "numpages": "16"
    },
    "Automatically Tagging the \u201cAAA\u201d Pattern in Unit Test Cases Using Machine Learning Models": {
        "type": "article",
        "key": "10.1109/TSE.2023.3252442",
        "author": "Wei, Chenhao and Xiao, Lu and Yu, Tingting and Chen, Xinyu and Wang, Xiao and Wong, Sunny and Clune, Abigail",
        "title": "Automatically Tagging the \u201cAAA\u201d Pattern in Unit Test Cases Using Machine Learning Models",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3252442",
        "doi": "10.1109/TSE.2023.3252442",
        "abstract": "The &lt;italic&gt;AAA&lt;/italic&gt; pattern (i.e., &lt;italic&gt;Arrange-Act-Assert&lt;/italic&gt;) is a common and natural layout to create a test case. Following this pattern in test cases may benefit comprehension, debugging, and maintenance. The &lt;italic&gt;AAA&lt;/italic&gt; structure of real-life test cases, however, may not be clear due to their high complexity. Manually labeling &lt;italic&gt;AAA&lt;/italic&gt; statements in test cases is tedious. Thus, we envision that an automated approach for labeling &lt;italic&gt;AAA&lt;/italic&gt; statements in existing test cases could benefit new developers and projects that practice collective code ownership and test-driven development. This paper contributes an automatic approach based on machine learning models. The \u201csecret sauce\u201d of this approach is a set of three learning features that are based on the semantic, syntax, and context information in test cases, derived from the manual tagging process. Thus, our approach mimics how developers may manually tag the &lt;italic&gt;AAA&lt;/italic&gt; pattern of a test case. We assess the precision, recall, and F-1 score of our approach based on 449 test cases, containing about 16,612 statements, across 4 Apache open source projects. To achieve the best performance in our approach, we explore the usage of six machine learning models; the contribution of the SMOTE data balancing technique; the comparison of the three learning features; and the comparison of five different methods for calculating the semantic feature. The results show our approach is able to identify &lt;italic&gt;Arrangement&lt;/italic&gt;, &lt;italic&gt;Action&lt;/italic&gt;, and &lt;italic&gt;Assertion&lt;/italic&gt; statements with a precision upwards of 92%, and recall up to 74%. We also summarize some experience based on our experiments\u2014regarding the choice of machine learning models, data balancing algorithm, and feature engineering methods\u2014which could potentially provide some reference to related future research.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3305\u20133324",
        "numpages": "20"
    },
    "A Framework for Emotion-Oriented Requirements Change Handling in Agile Software Engineering": {
        "type": "article",
        "key": "10.1109/TSE.2023.3253145",
        "author": "Madampe, Kashumi and Hoda, Rashina and Grundy, John",
        "title": "A Framework for Emotion-Oriented Requirements Change Handling in Agile Software Engineering",
        "year": "2023",
        "issue_date": "May 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "5",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3253145",
        "doi": "10.1109/TSE.2023.3253145",
        "abstract": "&lt;bold&gt;Background:&lt;/bold&gt; Requirements Changes (RCs) \u2013 the additions/modifications/deletions of functional/non-functional requirements in software products \u2013 are challenging for software practitioners to handle. Handling some changes may significantly impact the emotions of the practitioners. &lt;bold&gt;Objective:&lt;/bold&gt; We wanted to know the key challenges that make RC handling difficult, how these impact the emotions of software practitioners, what influences their RC handling, and how RC handling can be made less emotionally challenging. &lt;bold&gt;Method:&lt;/bold&gt; We followed a mixed-methods approach. We conducted two survey studies, with 40 participants and 201 participants respectively. The presentation of key quantitative data was followed by descriptive statistical analysis, and the qualitative data was analysed using Strauss\u2013Corbinian Grounded Theory, and Socio\u2013Technical Grounded Theory analysis techniques. &lt;bold&gt;Findings:&lt;/bold&gt; We found (1) several key factors that make RC handling an emotional challenge, (2) varying emotions that practitioners feel when it is challenging to handle RCs, (3) how stakeholders, including practitioners themselves, peers, managers and customers, influence the RC handling and how practitioners feel due to the stakeholder influence, and (4) practices that can be used to better handle RCs. &lt;bold&gt;Conclusion:&lt;/bold&gt; Some challenges are technical and some are social which also belong to aspects of agile practice, emotional intelligence, and likely belong to cognitive intelligence. Therefore, to better handle RCs with positive emotions in socio\u2013technical environments, agility, emotional intelligence, and cognitive intelligence need to work in synergy with each other.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "3325\u20133343",
        "numpages": "19"
    },
    "Finding Trends in Software Research": {
        "type": "article",
        "key": "10.1109/TSE.2018.2870388",
        "author": "Mathew, George and Agrawal, Amritanshu and Menzies, Tim",
        "title": "Finding Trends in Software Research",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2018.2870388",
        "doi": "10.1109/TSE.2018.2870388",
        "abstract": "Text mining methods can find large scale trends within research communities. For example, using stable Latent Dirichlet Allocation (a topic modeling algorithm) this study found 10 major topics in 35,391 SE research papers from 34 leading SE venues over the last 25 years (divided, evenly, between conferences and journals). Out study also shows how those topics have changed over recent years. Also, we note that (in the historical record) mono-focusing on a single topic can lead to fewer citations than otherwise. Further, while we find no overall gender bias in SE authorship, we note that women are under-represented in the top-most cited papers in our field. Lastly, we show a previously unreported dichotomy between software conferences and journals (so research topics that succeed at conferences might not succeed at journals, and vice versa). An important aspect of this work is that it is automatic and quickly repeatable (unlike prior SE bibliometric studies that used tediously slow and labor intensive methods). Automation is important since, like any data mining study, its conclusions are skewed by the data used in the analysis. The automatic methods of this paper make it far easier for other researchers to re-apply the analysis to new data, or if they want to use different modeling assumptions.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1397\u20131410",
        "numpages": "14"
    },
    "An Actionable Framework for Understanding and Improving Developer Experience": {
        "type": "article",
        "key": "10.1109/TSE.2022.3175660",
        "author": "Greiler, Michaela and Storey, Margaret-Anne and Noda, Abi",
        "title": "An Actionable Framework for Understanding and Improving Developer Experience",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3175660",
        "doi": "10.1109/TSE.2022.3175660",
        "abstract": "Developer experience is an important concern for software organizations as enhancing developer experience improves productivity, satisfaction, engagement and retention. We set out to understand what affects developer experience through semi-structured interviews with 21 developers from industry, which we transcribed and iteratively coded. Our findings elucidate factors that affect developer experience and characteristics that influence their respective importance to individual developers. We also identify strategies employed by individuals and teams to improve developer experience and the barriers that stand in their way. Lastly, we describe the coping mechanisms of developers when developer experience cannot be sufficiently improved. Our findings result in the DX Framework, an actionable conceptual framework for understanding and improving developer experience. The DX Framework provides a go-to reference for organizations that want to enable more productive and effective work environments for their developers.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1411\u20131425",
        "numpages": "15"
    },
    "SCS-Gan: Learning Functionality-Agnostic Stylometric Representations for Source Code Authorship Verification": {
        "type": "article",
        "key": "10.1109/TSE.2022.3177228",
        "author": "Ou, Weihan and Ding, Steven H. H. and Tian, Yuan and Song, Leo",
        "title": "SCS-Gan: Learning Functionality-Agnostic Stylometric Representations for Source Code Authorship Verification",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3177228",
        "doi": "10.1109/TSE.2022.3177228",
        "abstract": "In recent years, the number of anonymous script-based fileless malware attacks and software copyright disputes has increased rapidly. In the literature, automated Code Authorship Analysis (CAA) techniques have been proposed to reduce the manual effort in identifying those attacks and issues. Most CAA techniques aim to solve the task of Authorship Attribution (AA), i.e., identifying the actual author of a source code fragment from a given set of candidate authors. However, in many real-world scenarios, investigators do not have a predefined set of authors containing the actual author at the time of investigation, i.e., contradicting AA's assumption. Additionally, existing AA techniques ignore the influence of code functionality when identifying the authorship, which leads to biased matching simply based on code functionality. Different from AA, the task of (extreme) Authorship Verification (AV) is to decide if two texts were written by the same person or not. AV techniques do not need a predefined author set and thus could be applied in more code authorship-related applications than AA. To our knowledge, there is no previous work attempting to solve the AV problem for the source code. To fill the gap, we propose a novel adversarial neural network, namely SCS-Gan, that can learn a stylometric representation of code for automated AV. With the multi-head attention mechanism, SCS-Gan focuses on the code parts that are most informative regarding personal styles and generates functionality-agnostic stylometric representations through adversarial training. We benchmark SCS-Gan and two state-of-the-art code representation models on four out-of-sample datasets collected from a real-world programming competition. Our experiment results show that SCS-Gan outperforms the baselines on all four out-of-sample datasets.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1426\u20131442",
        "numpages": "17"
    },
    "BugBuilder: An Automated Approach to Building Bug Repository": {
        "type": "article",
        "key": "10.1109/TSE.2022.3177713",
        "author": "Jiang, Yanjie and Liu, Hui and Luo, Xiaoqing and Zhu, Zhihao and Chi, Xiaye and Niu, Nan and Zhang, Yuxia and Hu, Yamin and Bian, Pan and Zhang, Lu",
        "title": "BugBuilder: An Automated Approach to Building Bug Repository",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3177713",
        "doi": "10.1109/TSE.2022.3177713",
        "abstract": "Bug-related research, e.g., fault localization, program repair, and software testing, relies heavily on high-quality and large-scale software bug repositories. The importance of such repositories is twofold. On one side, real-world bugs and their associated patches may inspire novel approaches for finding, locating, and repairing software bugs. On the other side, the real-world bugs and their patches are indispensable for rigorous and meaningful evaluation of approaches to software testing, fault localization, and program repair. To this end, a number of software bug repositories, e.g., iBUGS and Defects4J, have been constructed recently by mining version control systems and bug tracking systems. However, fully automated construction of bug repositories by simply taking bug-fixing commits from version control systems often results in inaccurate patches that contain many bug-irrelevant changes. Although we may request experts or developers to manually exclude the bug-irrelevant changes (as the authors of Defects4J did), such extensive human intervention makes it difficult to build large-scale bug repositories. To this end, in this paper, we propose an automatic approach, called &lt;italic&gt;BugBuilder&lt;/italic&gt;, to construct bug repositories from version control systems. Different from existing approaches, it automatically extracts complete and concise bug-fixing patches and excludes bug-irrelevant changes. It first detects and excludes software refactorings involved in bug-fixing commits. &lt;italic&gt;BugBuilder&lt;/italic&gt; then enumerates all subsets of the remaining part, and discards invalid subsets by compilation and software testing. If exactly a single subset survives the validation, this subset is taken as the complete and concise bug-fixing patch for the associated bug. In case multiple subsets survive, BugBuilder employs a sequence of heuristics to select the most likely one. Evaluation results on 809 real-world bug-fixing commits in Defects4J suggest that &lt;italic&gt;BugBuilder&lt;/italic&gt; successfully extracted complete and concise bug-fixing patches from forty-three percent of the bug-fixing commits, and its precision (99%) was even higher than human experts. We also built a bug repository, called GrowingBugs, with the proposed approach. The resulting repository serves as evidence of the usefulness of the proposed approach, as well as a publicly available benchmark for bug-related research.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1443\u20131463",
        "numpages": "21"
    },
    "Towards Automatically Localizing Function Errors in Mobile Apps With User Reviews": {
        "type": "article",
        "key": "10.1109/TSE.2022.3178096",
        "author": "Yu, Le and Wang, Haoyu and Luo, Xiapu and Zhang, Tao and Liu, Kang and Chen, Jiachi and Zhou, Hao and Tang, Yutian and Xiao, Xusheng",
        "title": "Towards Automatically Localizing Function Errors in Mobile Apps With User Reviews",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3178096",
        "doi": "10.1109/TSE.2022.3178096",
        "abstract": "Removing all function errors is critical for making successful mobile apps. Since app testing may miss some function errors given limited time and resource, the user reviews of mobile apps are very important to developers for learning the uncaught errors. Unfortunately, manually handling each review is time-consuming and even error-prone. Existing studies on mobile apps\u2019 reviews could not help developers effectively locate the problematic code according to the reviews, because the majority of such research focus on review classification, requirements engineering, sentiment analysis, and summarization [1]. They do not localize the function errors described in user reviews in apps\u2019 code. Moreover, recent studies on mapping reviews to problematic source files look for the matching between the words in reviews and that in source code, bug reports, commit messages, and stack traces, thus may result in false positives and false negatives since they do not consider the semantic meaning and part of speech tag of each word. In this paper, we propose a novel approach to localize function errors in mobile apps by exploiting the context information in user reviews and correlating the reviews and bytecode through their semantic meanings. We realize our new approach as a tool named &lt;monospace&gt;ReviewSolver&lt;/monospace&gt;, and carefully evaluate it with reviews of real apps. The experimental result shows that &lt;monospace&gt;ReviewSolver&lt;/monospace&gt; has much better performance than the state-of-the-art tools (i.e., &lt;monospace&gt;ChangeAdvisor&lt;/monospace&gt; and &lt;monospace&gt;Where2Change&lt;/monospace&gt;).",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1464\u20131486",
        "numpages": "23"
    },
    "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain": {
        "type": "article",
        "key": "10.1109/TSE.2022.3178469",
        "author": "von der Mosel, Julian and Trautsch, Alexander and Herbold, Steffen",
        "title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3178469",
        "doi": "10.1109/TSE.2022.3178469",
        "abstract": "Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare BERT transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1487\u20131507",
        "numpages": "21"
    },
    "Learning to Predict User-Defined Types": {
        "type": "article",
        "key": "10.1109/TSE.2022.3178945",
        "author": "Jesse, Kevin and Devanbu, Premkumar T. and Sawant, Anand",
        "title": "Learning to Predict User-Defined Types",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3178945",
        "doi": "10.1109/TSE.2022.3178945",
        "abstract": "TypeScript is a widely adopted gradual typed language where developers can optionally type variables, functions, parameters and more. Probabilistic type inference approaches with ML (machine learning) work well especially for commonly occurring types such as &lt;monospace&gt;boolean&lt;/monospace&gt;, &lt;monospace&gt;number&lt;/monospace&gt;, and &lt;monospace&gt;string&lt;/monospace&gt;. TypeScript permits a wide range of types including developer defined class names and type interfaces. These developer defined types, termed &lt;italic&gt;user-defined types&lt;/italic&gt;, can be written within the realm of language naming conventions. The set of user-defined types is boundless and existing bounded type guessing approaches are an imperfect solution. Existing works either under perform in user-defined types or ignore user-defined types altogether. This work leverages a BERT-style pre-trained model, with multi-task learning objectives, to learn how to type user-defined classes and interfaces. Thus we present &lt;sc&gt;DiverseTyper&lt;/sc&gt;, a solution that explores the diverse set of user-defined types by uniquely aligning classes and interfaces declarations to the places in which they are used. &lt;sc&gt;DiverseTyper&lt;/sc&gt; surpasses all existing works including those that model user-defined types.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1508\u20131522",
        "numpages": "15"
    },
    "&lt;sc&gt;IoTCom&lt;/sc&gt;: Dissecting Interaction Threats in IoT Systems": {
        "type": "article",
        "key": "10.1109/TSE.2022.3179294",
        "author": "Alhanahnah, Mohannad and Stevens, Clay and Chen, Bocheng and Yan, Qiben and Bagheri, Hamid",
        "title": "&lt;sc&gt;IoTCom&lt;/sc&gt;: Dissecting Interaction Threats in IoT Systems",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3179294",
        "doi": "10.1109/TSE.2022.3179294",
        "abstract": "Due to the growing presence of Internet of Things (IoT) apps and devices in smart homes and smart cities, there are more and more concerns about their security and privacy risks. IoT apps normally interact with each other and the physical world to offer utility to the users. In this paper, we investigate the safety and security risks brought by the interactive behaviors of IoT apps. Two major challenges ensue in identifying the interaction threats: i) how to discover the threats across both cyber and physical channels; and ii) how to ensure the scalability of the detection approach. To address these challenges, we first provide a taxonomy of interaction threats between IoT apps, which contains seven classes of coordination threats categorized based on their interaction behaviors. Then, we present &lt;sc&gt;IoTCom&lt;/sc&gt;, a compositional threat detection system capable of automatically detecting and verifying unsafe interactions between IoT apps and devices. &lt;sc&gt;IoTCom&lt;/sc&gt; applies static analysis to automatically infer relevant apps\u2019 behaviors, and uses a novel strategy to trim the extracted app's behaviors prior to translating them into analyzable formal specifications, mitigating the state explosion associated with formal analysis. Our experiments with numerous bundles of real-world IoT apps have corroborated &lt;sc&gt;IoTCom&lt;/sc&gt;'s ability to effectively identify a broad spectrum of interaction threats triggered through cyber and physical channels, many of which were previously unknown. Finally, &lt;sc&gt;IoTCom&lt;/sc&gt; uses an automatic verifier to validate the discovered threats. Our experimental results show that &lt;sc&gt;IoTCom&lt;/sc&gt; significantly outperforms the existing techniques in terms of the computational time, and maintains the capability to perform its analysis across different IoT platforms.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1523\u20131539",
        "numpages": "17"
    },
    "Open or Sneaky? Fast or Slow? Light or Heavy?: Investigating Security Releases of Open Source Packages": {
        "type": "article",
        "key": "10.1109/TSE.2022.3181010",
        "author": "Imtiaz, Nasif and Khanom, Aniqa and Williams, Laurie",
        "title": "Open or Sneaky? Fast or Slow? Light or Heavy?: Investigating Security Releases of Open Source Packages",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3181010",
        "doi": "10.1109/TSE.2022.3181010",
        "abstract": "Vulnerabilities in open source packages can be a security risk for the downstream client projects. When a new vulnerability is discovered, a package should quickly release a fix in a new version, referred to as a &lt;italic&gt;security release&lt;/italic&gt; in this study. The security release should be well-documented and require minimal migration effort to facilitate fast adoption by the clients. However, to what extent the open source packages follow these recommendations is not known. In this paper, we study (1) the time lag between fix and release; (2) how security fixes are documented in the release notes; (3) code change characteristics (size and semantic versioning) of the release; and (4) the time lag between the release and an advisory publication for security releases over a dataset of 4,377 security advisories across seven package ecosystems. We find that the median security release becomes available within 4 days of the corresponding fix and contains 131 lines of code (LOC) change. However, one-fourth of the releases in our data set still came at least 20 days after the fix was made.Further, we find that 61.5% of the security releases come with a release note that documents the corresponding security fix. Still, Snyk and NVD, two popular databases, take a median of 17 days (from the release) to publish a security advisory, possibly resulting in delayed notifications to the client projects. We also find that security releases may contain breaking change(s) as 13.2% indicated backward incompatibility through semantic versioning, while 6.4% mentioned breaking change(s) in the release notes. Based on our findings, we point out areas for future work, such as private fork for security fixes and standardized practice for announcing security releases.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1540\u20131560",
        "numpages": "21"
    },
    "Bootstrapping Automated Testing for RESTful Web Services": {
        "type": "article",
        "key": "10.1109/TSE.2022.3182663",
        "author": "Lei, Zhanyao and Chen, Yixiong and Yang, Yang and Xia, Mingyuan and Qi, Zhengwei",
        "title": "Bootstrapping Automated Testing for RESTful Web Services",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3182663",
        "doi": "10.1109/TSE.2022.3182663",
        "abstract": "Modern RESTful services expose RESTful APIs to integrate with diversified applications. Most RESTful API parameters are weakly typed, which greatly increases the possible input value space. Weakly-typed parameters pose difficulties for automated testing tools to generate effective test cases to reveal web service defects related to parameter validation. We call this phenomenon the type collapse problem. To remedy this problem, we introduce FET (Format-encoded Type) techniques, including the FET, the FET lattice, and the FET inference to model fine-grained information for API parameters. Inferred FET can enhance parameter validation, such as generating a parameter validator for a certain RESTful server. Enhanced by FET techniques, automated testing tools can generate targeted test cases. We demonstrate Leif, a trace-driven fuzzing tool, as a proof-of-concept implementation of FET techniques. Experiment results on 27 commercial services show that FET inference precisely captures documented parameter definitions, which helps Leif discover 11 new bugs and reduce &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$72\\% - 86\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;72&lt;/mml:mn&gt;&lt;mml:mo&gt;\\%&lt;/mml:mo&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;86&lt;/mml:mn&gt;&lt;mml:mo&gt;\\%&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"qi-ieq1-3182663.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; fuzzing time compared to state-of-the-art fuzzers. Leveraged by the inter-parameter dependency inference, Leif saves &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$15\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;15&lt;/mml:mn&gt;&lt;mml:mo&gt;\\%&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"qi-ieq2-3182663.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; fuzzing time.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1561\u20131579",
        "numpages": "19"
    },
    "Using Transfer Learning for Code-Related Tasks": {
        "type": "article",
        "key": "10.1109/TSE.2022.3183297",
        "author": "Mastropaolo, Antonio and Cooper, Nathan and Palacio, David Nader and Scalabrino, Simone and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele",
        "title": "Using Transfer Learning for Code-Related Tasks",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3183297",
        "doi": "10.1109/TSE.2022.3183297",
        "abstract": "Deep learning (DL) techniques have been used to support several code-related tasks such as code summarization and bug-fixing. In particular, pre-trained transformer models are on the rise, also thanks to the excellent results they achieved in Natural Language Processing (NLP) tasks. The basic idea behind these models is to first pre-train them on a generic dataset using a self-supervised task (e.g., filling masked words in sentences). Then, these models are fine-tuned to support specific tasks of interest (e.g., language translation). A single model can be fine-tuned to support multiple tasks, possibly exploiting the benefits of &lt;italic&gt;transfer learning&lt;/italic&gt;. This means that knowledge acquired to solve a specific task (e.g., language translation) can be useful to boost performance on another task (e.g., sentiment classification). While the benefits of transfer learning have been widely studied in NLP, limited empirical evidence is available when it comes to code-related tasks. In this paper, we assess the performance of the Text-To-Text Transfer Transformer (T5) model in supporting four different code-related tasks: (i) automatic bug-fixing, (ii) injection of code mutants, (iii) generation of assert statements, and (iv) code summarization. We pay particular attention in studying the role played by pre-training and multi-task fine-tuning on the model's performance. We show that (i) the T5 can achieve better performance as compared to state-of-the-art baselines; and (ii) while pre-training helps the model, not all tasks benefit from a multi-task fine-tuning.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1580\u20131598",
        "numpages": "19"
    },
    "DeepMerge: Learning to Merge Programs": {
        "type": "article",
        "key": "10.1109/TSE.2022.3183955",
        "author": "Dinella, Elizabeth and Mytkowicz, Todd and Svyatkovskiy, Alexey and Bird, Christian and Naik, Mayur and Lahiri, Shuvendu",
        "title": "DeepMerge: Learning to Merge Programs",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3183955",
        "doi": "10.1109/TSE.2022.3183955",
        "abstract": "In collaborative software development, program merging is &lt;italic&gt;the&lt;/italic&gt; mechanism to integrate changes from multiple programmers. Merge algorithms in modern version control systems report a conflict when changes interfere textually. Merge conflicts require manual intervention and frequently stall modern continuous integration pipelines. Prior work found that, although costly, a large majority of resolutions involve re-arranging text without writing any new code. Inspired by this observation we propose the &lt;italic&gt;first data-driven approach&lt;/italic&gt; to resolve merge conflicts with a machine learning model. We realize our approach in a tool &lt;sc&gt;DeepMerge&lt;/sc&gt; that uses a novel combination of (i) an edit-aware embedding of merge inputs and (ii) a variation of pointer networks, to construct resolutions from input segments. We also propose an algorithm to localize manual resolutions in a resolved file and employ it to curate a ground-truth dataset comprising 8,719 non-trivial resolutions in JavaScript programs. Our evaluation shows that, on a held out test set, &lt;sc&gt;DeepMerge&lt;/sc&gt; can predict correct resolutions for 37% of non-trivial merges, compared to only 4% by a state-of-the-art semistructured merge technique. Furthermore, on the subset of merges with upto 3 lines (comprising 24% of the total dataset), &lt;sc&gt;DeepMerge&lt;/sc&gt; can predict correct resolutions with 78% accuracy.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1599\u20131614",
        "numpages": "16"
    },
    "Scalable and Accurate Test Case Prioritization in Continuous Integration Contexts": {
        "type": "article",
        "key": "10.1109/TSE.2022.3184842",
        "author": "Yaraghi, Ahmadreza Saboor and Bagherzadeh, Mojtaba and Kahani, Nafiseh and Briand, Lionel C.",
        "title": "Scalable and Accurate Test Case Prioritization in Continuous Integration Contexts",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3184842",
        "doi": "10.1109/TSE.2022.3184842",
        "abstract": "Continuous Integration (CI) requires efficient regression testing to ensure software quality without significantly delaying its CI builds. This warrants the need for techniques to reduce regression testing time, such as Test Case Prioritization (TCP) techniques that prioritize the execution of test cases to detect faults as early as possible. Many recent TCP studies employ various Machine Learning (ML) techniques to deal with the dynamic and complex nature of CI. However, most of them use a limited number of features for training ML models and evaluate the models on subjects for which the application of TCP makes little practical sense, due to their small regression testing time and low number of failed builds. In this work, we first define, at a conceptual level, a data model that captures data sources and their relations in a typical CI environment. Second, based on this data model, we define a comprehensive set of features that covers all features previously used by related studies. Third, we develop methods and tools to collect the defined features for 25 open-source software systems with enough failed builds and whose regression testing takes at least five minutes. Fourth, relying on the collected dataset containing a comprehensive feature set, we answer four research questions concerning data collection time, the effectiveness of ML-based TCP, the impact of the features on effectiveness, the decay of ML-based TCP models over time, and the trade-off between data collection time and the effectiveness of ML-based TCP techniques.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1615\u20131639",
        "numpages": "25"
    },
    "Predictive Comment Updating With Heuristics and AST-Path-Based Neural Learning: A Two-Phase Approach": {
        "type": "article",
        "key": "10.1109/TSE.2022.3185458",
        "author": "Lin, Bo and Wang, Shangwen and Liu, Zhongxin and Xia, Xin and Mao, Xiaoguang",
        "title": "Predictive Comment Updating With Heuristics and AST-Path-Based Neural Learning: A Two-Phase Approach",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3185458",
        "doi": "10.1109/TSE.2022.3185458",
        "abstract": "Just-in-time comment update is a promising way to reduce the burden of developers during software maintenance and evolution. Existing approaches can be divided into two categories: the heuristic-based approach and the deep-learning-based approach. The heuristic-based approach is restricted to a specific type of comment updates (i.e., code-indicative updates), but performs well on such type. The effectiveness of deep-learning-based approach is limited but it can handle diverse comment updates. Considering the complementary advantages of existing approaches, an intuitive idea is to combine them for better performance. To investigate this idea, we first conduct a pre-study experiment which shows that to construct an effective comment updater by combining heuristic-based and deep-learning-based approaches, we need to tackle two main challenges: 1) the heuristic-based approach may bring side effects to cases which cannot be updated by it; and 2) the current deep-learning-based approach is with limited effectiveness. Then, we propose a novel two-phase approach named S_Coachto cope with these two challenges and effectively perform comment updates. In the first phase, S_Coachintegrates nine distinctive features identified through our large-scale empirical analysis into a predictive model, which can predict whether the contents of the comment updates can be found in the corresponding code changes, namely, the comment updates are code-indicative updates. If so, the updates are then generated by an off-the-shelf heuristic-based approach; otherwise, S_Coachleverages a deep learning model, which we specially designed for non-code-indicative updates, to infer the new comment based on the old comment and code change. Motivated by our manual observation on the limitation of existing approaches on non-code-indicative updates, our deep learning model adopts the Abstract Syntax Tree path technique, which can capture the program structure information for effectively embedding code changes. Our evaluation shows that our approach outperforms the state-of-the-art by around 20% with respect to the number of correct comments it generates. Via in-depth analysis, we illustrate the rationale of each design decision as well as point out potential directions.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1640\u20131660",
        "numpages": "21"
    },
    "Revisiting Binary Code Similarity Analysis Using Interpretable Feature Engineering and Lessons Learned": {
        "type": "article",
        "key": "10.1109/TSE.2022.3187689",
        "author": "Kim, Dongkwan and Kim, Eunsoo and Cha, Sang Kil and Son, Sooel and Kim, Yongdae",
        "title": "Revisiting Binary Code Similarity Analysis Using Interpretable Feature Engineering and Lessons Learned",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3187689",
        "doi": "10.1109/TSE.2022.3187689",
        "abstract": "Binary code similarity analysis (BCSA) is widely used for diverse security applications, including plagiarism detection, software license violation detection, and vulnerability discovery. Despite the surging research interest in BCSA, it is significantly challenging to perform new research in this field for several reasons. First, most existing approaches focus only on the end results, namely, increasing the success rate of BCSA, by adopting uninterpretable machine learning. Moreover, they utilize their own benchmark, sharing neither the source code nor the entire dataset. Finally, researchers often use different terminologies or even use the same technique without citing the previous literature properly, which makes it difficult to reproduce or extend previous work. To address these problems, we take a step back from the mainstream and contemplate fundamental research questions for BCSA. Why does a certain technique or a certain feature show better results than the others? Specifically, we conduct the first systematic study on the basic features used in BCSA by leveraging interpretable feature engineering on a large-scale benchmark. Our study reveals various useful insights on BCSA. For example, we show that a simple interpretable model with a few basic features can achieve a comparable result to that of recent deep learning-based approaches. Furthermore, we show that the way we compile binaries or the correctness of underlying binary analysis tools can significantly affect the performance of BCSA. Lastly, we make all our source code and benchmark public and suggest future directions in this field to help further research.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1661\u20131682",
        "numpages": "22"
    },
    "A Comprehensive Study on ARM Disassembly Tools": {
        "type": "article",
        "key": "10.1109/TSE.2022.3187811",
        "author": "Jiang, Muhui and Dai, Qinming and Zhang, Wenlong and Chang, Rui and Zhou, Yajin and Luo, Xiapu and Wang, Ruoyu and Liu, Yang and Ren, Kui",
        "title": "A Comprehensive Study on ARM Disassembly Tools",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3187811",
        "doi": "10.1109/TSE.2022.3187811",
        "abstract": "Embedded devices are becoming ubiquitous, and ARM is becoming the dominant architecture for them. Meanwhile, there is a pressing need to perform security assessments for these devices. Due to different types of peripherals, emulating the software, i.e., firmware, of these devices in scale is challenging. Therefore, static analysis is still widely used. Existing works usually leverage off-the-shelf tools to disassemble stripped ARM binaries and (implicitly) assume that reliably disassembling binaries is a solved problem. However, whether this assumption really holds is unknown. In this paper, we conduct the first comprehensive study on ARM disassembly tools. Specifically, we build 1,896 ARM binaries (including 248 obfuscated ones) with different compilers, compiling options, and obfuscation methods. We then evaluate them using eight state-of-the-art ARM disassembly tools (including both commercial and noncommercial ones) in three different versions on their capabilities to locate instruction boundary, function boundary, and function signature. Instruction and function boundary are two fundamental primitives that the other primitives are built upon while function signature is significant for control flow integrity (CFI) techniques. Our work reveals some observations that have not been systematically summarized and/or confirmed. For instance, we find that the existence of both ARM and Thumb instruction sets, and the reuse of the &lt;italic&gt;BL&lt;/italic&gt; instruction for both function calls and branches bring serious challenges to disassembly tools. Our evaluation sheds light on the limitations of state-of-the-art disassembly tools and points out potential directions for improvement.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1683\u20131703",
        "numpages": "21"
    },
    "Automated Generation and Evaluation of JMH Microbenchmark Suites From Unit Tests": {
        "type": "article",
        "key": "10.1109/TSE.2022.3188005",
        "author": "Jangali, Mostafa and Tang, Yiming and Alexandersson, Niclas and Leitner, Philipp and Yang, Jinqiu and Shang, Weiyi",
        "title": "Automated Generation and Evaluation of JMH Microbenchmark Suites From Unit Tests",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3188005",
        "doi": "10.1109/TSE.2022.3188005",
        "abstract": "Performance is a crucial non-functional requirement of many software systems. Despite the widespread use of performance testing, developers still struggle to construct and evaluate the quality of performance tests. To address these two major challenges, we implement a framework, dubbed &lt;italic&gt;ju2jmh&lt;/italic&gt;, to automatically generate performance microbenchmarks from JUnit tests and use mutation testing to study the quality of generated microbenchmarks. Specifically, we compare our &lt;italic&gt;ju2jmh&lt;/italic&gt; generated benchmarks to manually written JMH benchmarks and to automatically generated JMH benchmarks using the AutoJMH framework, as well as directly measuring system performance with JUnit tests. For this purpose, we have conducted a study on three subjects (&lt;monospace&gt;Rxjava&lt;/monospace&gt;, &lt;monospace&gt;Eclipse-collections&lt;/monospace&gt;, and &lt;monospace&gt;Zipkin&lt;/monospace&gt;) with &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$sim$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u223c&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"tang-ieq1-3188005.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;454K &lt;italic&gt;source lines of code&lt;/italic&gt; (SLOC), 2,417 JMH benchmarks (including manually written and generated AutoJMH benchmarks) and 35,084 JUnit tests. Our results show that the &lt;italic&gt;ju2jmh&lt;/italic&gt; generated JMH benchmarks consistently outperform using the execution time and throughput of JUnit tests as a proxy of performance and JMH benchmarks automatically generated using the AutoJMH framework while being comparable to JMH benchmarks manually written by developers in terms of tests\u2019 stability and ability to detect performance bugs. Nevertheless, &lt;italic&gt;ju2jmh&lt;/italic&gt; benchmarks are able to cover more of the software applications than manually written JMH benchmarks during the microbenchmark execution. Furthermore, &lt;italic&gt;ju2jmh&lt;/italic&gt; benchmarks are generated automatically, while manually written JMH benchmarks require many hours of hard work and attention; therefore our study can reduce developers\u2019 effort to construct microbenchmarks. In addition, we identify three factors (too low test workload, unstable tests and limited mutant coverage) that affect a benchmark's ability to detect performance bugs. To the best of our knowledge, this is the first study aimed at assisting developers in fully automated microbenchmark creation and assessing microbenchmark quality for performance testing.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1704\u20131725",
        "numpages": "22"
    },
    "VID2XML: Automatic Extraction of a Complete XML Data From Mobile Programming Screencasts": {
        "type": "article",
        "key": "10.1109/TSE.2022.3188898",
        "author": "Alahmadi, Mohammad D.",
        "title": "VID2XML: Automatic Extraction of a Complete XML Data From Mobile Programming Screencasts",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3188898",
        "doi": "10.1109/TSE.2022.3188898",
        "abstract": "Developers often refer to video-hosting online platforms to find screencasts that provide a step-by-step guide to help them solve a programming task at hand or learn a new concept. More specifically, developers search for resources that help them design and implement effective mobile graphical user interfaces (GUI) using &lt;monospace&gt;XML&lt;/monospace&gt;. Although mobile programming screencasts contain a vast amount of &lt;monospace&gt;XML&lt;/monospace&gt; data at developers\u2019 disposal, they cannot be easily found and copied-pasted due to the image nature of videos. Given that the most common task developers perform online is copy-pasting, mobile programming screencasts must support that and be complemented with &lt;monospace&gt;XML&lt;/monospace&gt; data in a textual format. To overcome this challenge and aid developers, this paper presents &lt;monospace&gt;vid2XML&lt;/monospace&gt;, which is a three-phase approach that leverages both visual and textual information of video frames to locate &lt;monospace&gt;XML&lt;/monospace&gt; region in video frames, locate the currently opened file, and extract &lt;monospace&gt;XML&lt;/monospace&gt; data for each file presented in video frames. We evaluated each phase of &lt;monospace&gt;vid2XML&lt;/monospace&gt; in a comprehensive empirical evaluation on videos collected from YouTube. The results reveal that &lt;monospace&gt;vid2XML&lt;/monospace&gt; is able to accurately (i) locate &lt;monospace&gt;XML&lt;/monospace&gt; regions, outperforming four previous work, (ii) locate the bounding box of the selected file, and (iii) extract, fix, and merge &lt;monospace&gt;XML&lt;/monospace&gt; data for each file opened/created in a video.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1726\u20131740",
        "numpages": "15"
    },
    "Towards Better Dependency Management: A First Look at Dependency Smells in Python Projects": {
        "type": "article",
        "key": "10.1109/TSE.2022.3191353",
        "author": "Cao, Yulu and Chen, Lin and Ma, Wanwangying and Li, Yanhui and Zhou, Yuming and Wang, Linzhang",
        "title": "Towards Better Dependency Management: A First Look at Dependency Smells in Python Projects",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3191353",
        "doi": "10.1109/TSE.2022.3191353",
        "abstract": "Managing cross-project dependencies is tricky in modern software development. A primary way to manage dependencies is using dependency configuration files, which brings convenience to the entire software ecosystem, including developers, maintainers, and users. However, developers may introduce dependency smells if dependency configuration files are not well written and maintained. Dependency smells are recurring violations of dependency management in dependency configuration files and can potentially lead to severe consequences. This paper provides an in-depth look at three dependency smells, namely, &lt;italic&gt;Missing Dependency&lt;/italic&gt;, &lt;italic&gt;Bloated Dependency&lt;/italic&gt;, and &lt;italic&gt;Version Constraint Inconsistency&lt;/italic&gt; in Python projects. First, we implement a tool called &lt;underline&gt;Py&lt;/underline&gt;thon &lt;underline&gt;C&lt;/underline&gt;ross-project &lt;underline&gt;D&lt;/underline&gt;ependency- PyCD to accurately extract dependency information from configuration files. The evaluation result on 212 Python projects shows that PyCD outperforms state-of-the-art tools. Then, we make an empirical study for three dependency smells in 132 Python projects to investigate the pervasiveness, causes, and evolution. The results show that: 1) dependency smells are prevalent in Python projects and exist inconsistently in different projects; 2) dependency smells are introduced into Python projects for different reasons, mainly due to the problems of synchronous update and collaborative development; and 3) dependency smells can be removed with different patterns according to different dependency smells. Furthermore, we report and get responses for 40 harmful dependency smell instances, 34 of which have been responded that these dependency smells do exist in the projects, and 10 instances are fixed or under process. The feedback from developers indicates that dependency smells can have a negative impact on project maintenance. Our study highlights that these dependency smells deserve the attention of developers.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1741\u20131765",
        "numpages": "25"
    },
    "The Duality in Computing SSA Programs and Control Dependency": {
        "type": "article",
        "key": "10.1109/TSE.2022.3192249",
        "author": "Masud, Abu Naser",
        "title": "The Duality in Computing SSA Programs and Control Dependency",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3192249",
        "doi": "10.1109/TSE.2022.3192249",
        "abstract": "Control dependency (CD) and Static Single Assignment (SSA) form are the basis of many program analyses, transformation, and optimization techniques, and these are implemented and used by modern compilers such as GCC and LLVM. Most state-of-the-art algorithms approximate these computations by using postdominator relations and dominance frontiers (DF) respectively for efficiency reasons which have been used for over three decades. Dominator-based SSA transformation and control dependencies exhibit a non-dual relationship. Recently, it has been shown that DF-based SSA computation is grossly imprecise, and Weak and Strong Control Closure (WCC and SCC) have wider applicability in capturing control dependencies than postdominator-based CD computation. Our main contribution in this article is the proof of duality between the generation of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$phi$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u03d5&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"masud-ieq1-3192249.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; functions and the computation of weakly deciding (WD) vertices which are the most computationally expensive part of SSA program construction and WCC/SCC computation respectively. We have provided a duality theorem and its constructive proof by means of an algorithm that can compute both the &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$phi$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u03d5&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"masud-ieq2-3192249.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; functions and the WD vertices seamlessly. We have used this algorithm to compute SSA programs and WCC, and performed experiments on real-world industrial benchmarks. The practical efficiency of our algorithm is (i) almost equal to the best state-of-the-art algorithm in computing WCC, and (ii) closer to (but not as efficient as) the DF-based algorithms in computing SSA programs. Moreover, our algorithm achieves the ultimate precision in computing WCC and SSA programs with respect to the inputs of these algorithms and obtains wider applicability in the WCC computation (handling nonterminating programs).",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1766\u20131781",
        "numpages": "16"
    },
    "Translating Video Recordings of Complex Mobile App UI Gestures into Replayable Scenarios": {
        "type": "article",
        "key": "10.1109/TSE.2022.3192279",
        "author": "Bernal-C\\'{a}rdenas, Carlos and Cooper, Nathan and Havranek, Madeleine and Moran, Kevin and Chaparro, Oscar and Poshyvanyk, Denys and Marcus, Andrian",
        "title": "Translating Video Recordings of Complex Mobile App UI Gestures into Replayable Scenarios",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3192279",
        "doi": "10.1109/TSE.2022.3192279",
        "abstract": "Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (e.g., bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces &lt;sc&gt;V2S+&lt;/sc&gt;, an automated approach for translating video recordings of Android app usages into replayable scenarios. &lt;sc&gt;V2S+&lt;/sc&gt; is based primarily on computer vision techniques and adapts recent solutions for object detection and image classification to detect and classify user &lt;italic&gt;gestures&lt;/italic&gt; captured in a video, and convert these into a replayable test scenario. Given that &lt;sc&gt;V2S+&lt;/sc&gt; takes a computer vision-based approach, it is applicable to both hybrid and native Android applications. We performed an extensive evaluation of &lt;sc&gt;V2S+&lt;/sc&gt; involving 243 videos depicting 4,028 GUI-based actions collected from users exercising features and reproducing bugs from a collection of over 90 popular native and hybrid Android apps. Our results illustrate that &lt;sc&gt;V2S+&lt;/sc&gt; can accurately replay scenarios from screen recordings, and is capable of reproducing &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$approx$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u2248&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"cooper-ieq1-3192279.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; 90.2% of sequential actions recorded in native application scenarios on physical devices, and &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$approx$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u2248&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"cooper-ieq2-3192279.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; 83% of sequential actions recorded in hybrid application scenarios on emulators, both with low overhead. A case study with three industrial partners illustrates the potential usefulness of &lt;sc&gt;V2S+&lt;/sc&gt; from the viewpoint of developers.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1782\u20131803",
        "numpages": "22"
    },
    "On the Effectiveness of Transfer Learning for Code Search": {
        "type": "article",
        "key": "10.1109/TSE.2022.3192755",
        "author": "Salza, Pasquale and Schwizer, Christoph and Gu, Jian and Gall, Harald C.",
        "title": "On the Effectiveness of Transfer Learning for Code Search",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3192755",
        "doi": "10.1109/TSE.2022.3192755",
        "abstract": "The Transformer architecture and transfer learning have marked a quantum leap in natural language processing, improving the state of the art across a range of text-based tasks. This paper examines how these advancements can be applied to and improve code search. To this end, we pre-train a BERT-based model on combinations of natural language and source code data and fine-tune it on pairs of StackOverflow question titles and code answers. Our results show that the pre-trained models consistently outperform the models that were not pre-trained. In cases where the model was pre-trained on natural language \u201cand\u201d source code data, it also outperforms an information retrieval baseline based on Lucene. Also, we demonstrated that the combined use of an information retrieval-based approach followed by a Transformer leads to the best results overall, especially when searching into a large search pool. Transfer learning is particularly effective when much pre-training data is available and fine-tuning data is limited. We demonstrate that natural language processing models based on the Transformer architecture can be directly applied to source code analysis tasks, such as code search. With the development of Transformer models designed more specifically for dealing with source code data, we believe the results of source code analysis tasks can be further improved.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1804\u20131822",
        "numpages": "19"
    },
    "How do Developers &lt;italic&gt;Really&lt;/italic&gt; Feel About Bug Fixing? Directions for Automatic Program Repair": {
        "type": "article",
        "key": "10.1109/TSE.2022.3194188",
        "author": "Winter, Emily and Bowes, David and Counsell, Steve and Hall, Tracy and Haraldsson, S\\ae{}mundur and Nowack, Vesna and Woodward, John",
        "title": "How do Developers &lt;italic&gt;Really&lt;/italic&gt; Feel About Bug Fixing? Directions for Automatic Program Repair",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3194188",
        "doi": "10.1109/TSE.2022.3194188",
        "abstract": "Automatic program repair (APR) is a rapidly advancing field of software engineering that aims to supplement or replace manual bug fixing with an automated tool. For APR to be successfully adopted in industry, it is vital that APR tools respond to developer needs and preferences. However, very little research has considered developers\u2019 general attitudes to APR or developers\u2019 current bug fixing practices (the activity APR aims to replace). This article responds to this gap by reporting on a survey of 386 software developers about their bug finding and fixing practices and experiences, and their instinctive attitudes towards APR. We find that bug finding and fixing is not necessarily as onerous for developers as has often been suggested, being rated as more satisfying than developers\u2019 general work. The fact that developers derive satisfaction and benefit from bug fixing indicates that APR adoption is not as simple as APR replacing an unwanted activity. When it comes to potential APR approaches, we find a strong preference for developers being kept in the loop (for example, choosing between different fixes or validating fixes) as opposed to a fully automated process. This suggests that advances in APR should be careful to consider the agency of the developer, as well as what information is presented to developers alongside fixes. It also indicates that there are key barriers related to trust that would need to be overcome for full scale APR adoption, supported by the fact that even those developers who stated that they were positive about APR listed several caveats and concerns. We find very few statistically significant relationships between particular demographic variables (for example, developer experience, age, education) and key attitudinal variables, suggesting that developers\u2019 instinctive attitudes towards APR are little influenced by experience level but are held widely across the developer community.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1823\u20131841",
        "numpages": "19"
    },
    "FalsifAI: Falsification of AI-Enabled Hybrid Control Systems Guided by Time-Aware Coverage Criteria": {
        "type": "article",
        "key": "10.1109/TSE.2022.3194640",
        "author": "Zhang, Zhenya and Lyu, Deyun and Arcaini, Paolo and Ma, Lei and Hasuo, Ichiro and Zhao, Jianjun",
        "title": "FalsifAI: Falsification of AI-Enabled Hybrid Control Systems Guided by Time-Aware Coverage Criteria",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3194640",
        "doi": "10.1109/TSE.2022.3194640",
        "abstract": "Modern Cyber-Physical Systems (CPSs) that need to perform complex control tasks (e.g., autonomous driving) are increasingly using AI-enabled controllers, mainly based on deep neural networks (DNNs). The quality assurance of such types of systems is of vital importance. However, their verification can be extremely challenging, due to their complexity and uninterpretable decision logic. Falsification is an established approach for CPS quality assurance, which, instead of attempting to prove the system correctness, aims at finding a time-variant input signal violating a formal specification describing the desired behavior; it often employs a search-based testing approach that tries to minimize the &lt;italic&gt;robustness&lt;/italic&gt; of the specification, given by its quantitative semantics. However, guidance provided by robustness is mostly black-box and only related to the system output, but does not allow to understand whether the temporal internal behavior determined by multiple consecutive executions of the neural network controller has been explored sufficiently. To bridge this gap, in this paper, we make an early attempt at exploring the temporal behavior determined by the repeated executions of the neural network controllers in hybrid control systems and first propose eight time-aware coverage criteria specifically designed for neural network controllers in the context of CPS, which consider different features by design: the simple temporal activation of a neuron, the continuous activation of a neuron for a given duration, and the differential neuron activation behavior over time. Second, we introduce a falsification framework, named &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathtt {FalsifAI}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi mathvariant=\"monospace\"&gt;FalsifAI&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhang-ieq1-3194640.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;, that exploits the coverage information for better falsification guidance. Namely, inputs of the controller that increase the coverage (so improving the &lt;italic&gt;exploration&lt;/italic&gt; of the DNN behaviors), are prioritized in the &lt;italic&gt;exploitation&lt;/italic&gt; phase of robustness minimization. Our large-scale evaluation over a total of 3 typical CPS tasks, 6 system specifications, 18 DNN models and more than 12,000 experiment runs, demonstrates 1) the advantage of our proposed technique in outperforming two state-of-the-art falsification approaches, and 2) the usefulness of our proposed time-aware coverage criteria for effective falsification guidance.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1842\u20131859",
        "numpages": "18"
    },
    "Neural Network Guided Evolutionary Fuzzing for Finding Traffic Violations of Autonomous Vehicles": {
        "type": "article",
        "key": "10.1109/TSE.2022.3195640",
        "author": "Zhong, Ziyuan and Kaiser, Gail and Ray, Baishakhi",
        "title": "Neural Network Guided Evolutionary Fuzzing for Finding Traffic Violations of Autonomous Vehicles",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3195640",
        "doi": "10.1109/TSE.2022.3195640",
        "abstract": "Self-driving cars and trucks, autonomous vehicles (&lt;sc&gt;av&lt;/sc&gt;s), should not be accepted by regulatory bodies and the public until they have much higher confidence in their safety and reliability \u2014 which can most practically and convincingly be achieved by testing. But existing testing methods are inadequate for checking the end-to-end behaviors of &lt;sc&gt;av&lt;/sc&gt; controllers against complex, real-world corner cases involving interactions with multiple independent agents such as pedestrians and human-driven vehicles. While test-driving &lt;sc&gt;av&lt;/sc&gt;s on streets and highways fails to capture many rare events, existing simulation-based testing methods mainly focus on simple scenarios and do not scale well for complex driving situations that require sophisticated awareness of the surroundings. To address these limitations, we propose a new fuzz testing technique, called &lt;italic&gt;AutoFuzz&lt;/italic&gt;, which can leverage widely-used &lt;sc&gt;av&lt;/sc&gt; simulators\u2019 API grammars to generate semantically and temporally valid complex driving scenarios (sequences of scenes). To efficiently search for traffic violations-inducing scenarios in a large search space, we propose a constrained neural network (NN) evolutionary search method to optimize &lt;italic&gt;AutoFuzz&lt;/italic&gt;. Evaluation of our prototype on one state-of-the-art learning-based controller, two rule-based controllers, and one industrial-grade controller in five scenarios shows that &lt;italic&gt;AutoFuzz&lt;/italic&gt; efficiently finds hundreds of traffic violationsin high-fidelity simulation environments. For each scenario, &lt;italic&gt;AutoFuzz&lt;/italic&gt; can find on average 10-39% more unique traffic violationsthan the best-performing baseline method. Further, fine-tuning the learning-based controller with the traffic violationsfound by &lt;italic&gt;AutoFuzz&lt;/italic&gt; successfully reduced the traffic violationsfound in the new version of the &lt;sc&gt;av&lt;/sc&gt; controller software.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1860\u20131875",
        "numpages": "16"
    },
    "Revisiting, Benchmarking and Exploring API Recommendation: How Far Are We?": {
        "type": "article",
        "key": "10.1109/TSE.2022.3197063",
        "author": "Peng, Yun and Li, Shuqing and Gu, Wenwei and Li, Yichen and Wang, Wenxuan and Gao, Cuiyun and Lyu, Michael R.",
        "title": "Revisiting, Benchmarking and Exploring API Recommendation: How Far Are We?",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3197063",
        "doi": "10.1109/TSE.2022.3197063",
        "abstract": "Application Programming Interfaces (APIs), which encapsulate the implementation of specific functions as interfaces, greatly improve the efficiency of modern software development. As the number of APIs grows up fast nowadays, developers can hardly be familiar with all the APIs and usually need to search for appropriate APIs for usage. So lots of efforts have been devoted to improving the API recommendation task. However, it has been increasingly difficult to gauge the performance of new models due to the lack of a uniform definition of the task and a standardized benchmark. For example, some studies regard the task as a code completion problem, while others recommend relative APIs given natural language queries. To reduce the challenges and better facilitate future research, in this paper, we revisit the API recommendation task and aim at benchmarking the approaches. Specifically, the paper groups the approaches into two categories according to the task definition, i.e., query-based API recommendation and code-based API recommendation. We study 11 recently-proposed approaches along with 4 widely-used IDEs. One benchmark named APIBench is then built for the two respective categories of approaches. Based on APIBench, we distill some actionable insights and challenges for API recommendation. We also achieve some implications and directions for improving the performance of recommending APIs, including appropriate query reformulation, data source selection, low resource setting, user-defined APIs, and query-based API recommendation with usage patterns.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1876\u20131897",
        "numpages": "22"
    },
    "A Theory of Organizational Structures for Development and Infrastructure Professionals": {
        "type": "article",
        "key": "10.1109/TSE.2022.3199169",
        "author": "Leite, Leonardo and Lago, Nelson and Melo, Claudia and Kon, Fabio and Meirelles, Paulo",
        "title": "A Theory of Organizational Structures for Development and Infrastructure Professionals",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3199169",
        "doi": "10.1109/TSE.2022.3199169",
        "abstract": "DevOps and continuous delivery have impacted the organizational structures of development and infrastructure groups in software-producing organizations. Our research aims at revealing the different options adopted by the software industry to organize such groups, understanding why different organizations adopt distinct structures, and discovering how organizations handle the drawbacks of each structure. We interviewed 68 carefully-selected IT professionals, 45 working in Brazil, 10 in the USA, 8 in Europe, 1 in Canada, and 4 in globally distributed teams. By analyzing these conversations through a Grounded Theory process, we identified conditions, causes, reasons to avoid, consequences, and contingencies related to each discovered structure (segregated departments, collaborative departments, API-mediated departments, and single department). In this way, we offer a theory to explain organizational structures for development and infrastructure professionals. This theory can support practitioners and researchers in comprehending and discussing the DevOps phenomenon and its related issues, and also provides valuable input to practitioners\u2019 decision-making.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1898\u20131911",
        "numpages": "14"
    },
    "Flakify: A Black-Box, Language Model-Based Predictor for Flaky Tests": {
        "type": "article",
        "key": "10.1109/TSE.2022.3201209",
        "author": "Fatima, Sakina and Ghaleb, Taher A. and Briand, Lionel",
        "title": "Flakify: A Black-Box, Language Model-Based Predictor for Flaky Tests",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3201209",
        "doi": "10.1109/TSE.2022.3201209",
        "abstract": "Software testing assures that code changes do not adversely affect existing functionality. However, a test case can be flaky, i.e., passing and failing across executions, even for the same version of the source code. Flaky test cases introduce overhead to software development as they can lead to unnecessary attempts to debug production or testing code. Besides rerunning test cases multiple times, which is time-consuming and computationally expensive, flaky test cases can be predicted using machine learning (ML) models, thus reducing the wasted cost of re-running and debugging these test cases. However, the state-of-the-art ML-based flaky test case predictors rely on pre-defined sets of features that are either project-specific, i.e., inapplicable to other projects, or require access to production code, which is not always available to software test engineers. Moreover, given the non-deterministic behavior of flaky test cases, it can be challenging to determine a complete set of features that could potentially be associated with test flakiness. Therefore, in this article, we propose Flakify, a black-box, language model-based predictor for flaky test cases. Flakify relies exclusively on the source code of test cases, thus not requiring to (a) access to production code (black-box), (b) rerun test cases, (c) pre-define features. To this end, we employed CodeBERT, a pre-trained language model, and fine-tuned it to predict flaky test cases using the source code of test cases. We evaluated Flakify on two publicly available datasets (FlakeFlagger and IDoFT) for flaky test cases and compared our technique with the FlakeFlagger approach, the best state-of-the-art ML-based, white-box predictor for flaky test cases, using two different evaluation procedures: (1) cross-validation and (2) per-project validation, i.e., prediction on new projects. Flakify achieved F1-scores of 79% and 73% on the FlakeFlagger dataset using cross-validation and per-project validation, respectively. Similarly, Flakify achieved F1-scores of 98% and 89% on the IDoFT dataset using the two validation procedures, respectively. Further, Flakify surpassed FlakeFlagger by 10 and 18 percentage points (pp) in terms of precision and recall, respectively, when evaluated on the FlakeFlagger dataset, thus reducing the cost bound to be wasted on unnecessarily debugging test cases and production code by the same percentages (corresponding to reduction rates of 25% and 64%). Flakify also achieved significantly higher prediction results when used to predict test cases on new projects, suggesting better generalizability over FlakeFlagger. Our results further show that a black-box version of FlakeFlagger is not a viable option for predicting flaky test cases.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1912\u20131927",
        "numpages": "16"
    },
    "Mind the Gap! A Study on the Transferability of Virtual Versus Physical-World Testing of Autonomous Driving Systems": {
        "type": "article",
        "key": "10.1109/TSE.2022.3202311",
        "author": "Stocco, Andrea and Pulfer, Brian and Tonella, Paolo",
        "title": "Mind the Gap! A Study on the Transferability of Virtual Versus Physical-World Testing of Autonomous Driving Systems",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3202311",
        "doi": "10.1109/TSE.2022.3202311",
        "abstract": "Safe deployment of self-driving cars (SDC) necessitates thorough simulated and in-field testing. Most testing techniques consider virtualized SDCs within a simulation environment, whereas less effort has been directed towards assessing whether such techniques transfer to and are effective with a physical real-world vehicle. In this paper, we shed light on the problem of generalizing testing results obtained in a driving simulator to a physical platform and provide a characterization and quantification of the sim2real gap affecting SDC testing. In our empirical study, we compare SDC testing when deployed on a physical small-scale vehicle versus its digital twin. Due to the unavailability of driving quality indicators from the physical platform, we use neural rendering to estimate them through visual odometry, hence allowing full comparability with the digital twin. Then, we investigate the transferability of behavior and failure exposure between virtual and real-world environments, targeting both unintended abnormal test data and intended adversarial examples. Our study shows that, despite the usage of a faithful digital twin, there are still critical shortcomings that contribute to the reality gap between the virtual and physical world, threatening existing testing solutions that only consider virtual SDCs. On the positive side, our results present the test configurations for which physical testing can be avoided, either because their outcome does transfer between virtual and physical environments, or because the uncertainty profiles in the simulator can help predict their outcome in the real world.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1928\u20131940",
        "numpages": "13"
    },
    "DSSDPP: Data Selection and Sampling Based Domain Programming Predictor for Cross-Project Defect Prediction": {
        "type": "article",
        "key": "10.1109/TSE.2022.3204589",
        "author": "Li, Zhiqiang and Zhang, Hongyu and Jing, Xiao-Yuan and Xie, Juanying and Guo, Min and Ren, Jie",
        "title": "DSSDPP: Data Selection and Sampling Based Domain Programming Predictor for Cross-Project Defect Prediction",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3204589",
        "doi": "10.1109/TSE.2022.3204589",
        "abstract": "Cross-project defect prediction (CPDP) refers to recognizing defective software modules in one project (i.e., target) using historical data collected from other projects (i.e., source), which can help developers find defects and prioritize their testing efforts. Unfortunately, there often exists large distribution difference between the source and target data. Most CPDP methods neglect to select the appropriate source data for a given target at the project level. More importantly, existing CPDP models are parametric methods, which usually require intensive parameter selection and tuning to achieve better prediction performance. This would hinder wide applicability of CPDP in practice. Moreover, most CPDP methods do not address the cross-project class imbalance problem. These limitations lead to suboptimal CPDP results. In this paper, we propose a novel data selection and sampling based domain programming predictor (DSSDPP) for CPDP, which addresses the above limitations. DSSDPP is a non-parametric CPDP method, which can perform knowledge transfer across projects without the need for parameter selection and tuning. By exploiting the structures of source and target data, DSSDPP can learn a discriminative transfer classifier for identifying defects of the target project. Extensive experiments on 22 projects from four datasets indicate that DSSDPP achieves better &lt;italic&gt;MCC&lt;/italic&gt; and &lt;italic&gt;AUC&lt;/italic&gt; results against a range of competing methods both in the single-source and multi-source scenarios. Since DSSDPP is easy, effective, extensible, and efficient, we suggest that future work can use it with the well-chosen source data to conduct CPDP especially for the projects with limited computational budget.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1941\u20131963",
        "numpages": "23"
    },
    "Demystifying Performance Regressions in String Solvers": {
        "type": "article",
        "key": "10.1109/TSE.2022.3168373",
        "author": "Zhang, Yao and Xie, Xiaofei and Li, Yi and Lin, Yun and Chen, Sen and Liu, Yang and Li, Xiaohong",
        "title": "Demystifying Performance Regressions in String Solvers",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3168373",
        "doi": "10.1109/TSE.2022.3168373",
        "abstract": "Over the past few years, SMT string solvers have found their applications in an increasing number of domains, such as program analyses in mobile and Web applications, which require the ability to reason about string values. A series of research has been carried out to find quality issues of string solvers in terms of its correctness and performance. Yet, none of them has considered the performance regressions happening across multiple versions of a string solver. To fill this gap, in this paper, we focus on solver performance regressions (SPRs), i.e., unintended slowdowns introduced during the evolution of string solvers. To this end, we develop &lt;italic&gt;SPRFinder&lt;/italic&gt;to not only generate test cases demonstrating SPRs, but also localize the probable causes of them, in terms of commits. We evaluated the effectiveness of &lt;italic&gt;SPRFinder&lt;/italic&gt;on three state-of-the-art string solvers, i.e., Z3Seq, Z3Str3, and CVC4. The results demonstrate that &lt;italic&gt;SPRFinder&lt;/italic&gt;is effective in generating SPR-inducing test cases and also able to accurately locate the responsible commits. Specifically, the average running time on the target versions is 13.2\u00d7 slower than that of the reference versions. Besides, we also conducted the first empirical study to peek into the characteristics of SPRs, including the impact of random seed configuration for SPR detection, understanding the root causes of SPRs, and characterizing the regression test cases through case studies. Finally, we highlight that 149 unique SPR-inducing commits were discovered in total by &lt;italic&gt;SPRFinder&lt;/italic&gt;, and 27of them have been confirmed by the corresponding developers.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "947\u2013961",
        "numpages": "15"
    },
    "Program Synthesis for Cyber-Resilience": {
        "type": "article",
        "key": "10.1109/TSE.2022.3168672",
        "author": "Catano, Nestor",
        "title": "Program Synthesis for Cyber-Resilience",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3168672",
        "doi": "10.1109/TSE.2022.3168672",
        "abstract": "Architectural tactics enable stakeholders to achieve cyber-resilience requirements. They permit systems to react, resist, detect, and recover from cyber incidents. This paper presents an approach to generate source code for architectural tactics typically used in safety and mission-critical systems. Our approach extensively relies on the use of the &lt;sc&gt;Event-B&lt;/sc&gt; formal method and the &lt;sc&gt;EventB2Java&lt;/sc&gt; code generation plugin of the Rodin platform. It leverages the modeling of architectural tactics in the &lt;sc&gt;Event-B&lt;/sc&gt; formal language and uses a set of &lt;sc&gt;EventB2Java&lt;/sc&gt; transformation rules to generate certified code implementations for the said tactics. Since resilience requirements are statements about a system over time, and because of the fact that the &lt;sc&gt;Event-B&lt;/sc&gt; language does not provide (native) support for the writing of temporal specifications, we have implemented a novel Linear Temporal Logic (LTL) extension for &lt;sc&gt;Event-B&lt;/sc&gt;. We support several architectural tactics for availability, performance, and security. The generated code is certified in the following sense: discharging proof obligations in Rodin - the platform we use for writing the &lt;sc&gt;Event-B&lt;/sc&gt; models - attests to the soundness of the architectural tactics modelled in &lt;sc&gt;Event-B&lt;/sc&gt;, and the soundness of the translation encoded by the &lt;sc&gt;EventB2Java&lt;/sc&gt; tool attests to the code correctness. Finally, we demonstrate the usability of our resilience validation approach with the aid of an Autonomous Vehicle System. It further helped us increase our confidence in the soundness of our Event-B LTL extension.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "962\u2013972",
        "numpages": "11"
    },
    "Applying Human Values Theory to Software Engineering Practice: Lessons and Implications": {
        "type": "article",
        "key": "10.1109/TSE.2022.3170087",
        "author": "Ferrario, Maria Angela and Winter, Emily",
        "title": "Applying Human Values Theory to Software Engineering Practice: Lessons and Implications",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3170087",
        "doi": "10.1109/TSE.2022.3170087",
        "abstract": "The study of human values in software engineering (SE) is increasingly recognised as a fundamental human-centric issue of SE decision making. However, values studies in SE still face a number of issues, including the difficulty of eliciting values in a systematic and structured way, the challenges of measuring and tracking values over time, and the lack of practice-based understanding of values among software practitioners. This paper aims to help address these issues by: 1) outlining a research framework that supports a systematic approach to values elicitation, analysis, and understanding; 2) introducing tools and techniques that help elicit and measure values during SE decision making processes in a systematic way; and 3) applying such tools to a month-long research sprint co-designed with an industry partner and conducted with 27 software practitioners. The case study builds on lessons from an earlier pilot (12 participants) and combines in-situ observations with the use of two values-informed tools: the Values Q-Sort (V-QS), and the Values-Retro. The V-QS adapts instruments from values research to the SE context, the Values-Retro adapts existing SE techniques to values theory. We distil implications for research and practice in ten lessons learned.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "973\u2013990",
        "numpages": "18"
    },
    "Finding Critical Scenarios for Automated Driving Systems: A Systematic Mapping Study": {
        "type": "article",
        "key": "10.1109/TSE.2022.3170122",
        "author": "Zhang, Xinhai and Tao, Jianbo and Tan, Kaige and T\\\"{o}rngren, Martin and S\\'{a}nchez, Jos\\'{e} Manuel Gaspar and Ramli, Muhammad Rusyadi and Tao, Xin and Gyllenhammar, Magnus and Wotawa, Franz and Mohan, Naveen and Nica, Mihai and Felbinger, Hermann",
        "title": "Finding Critical Scenarios for Automated Driving Systems: A Systematic Mapping Study",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3170122",
        "doi": "10.1109/TSE.2022.3170122",
        "abstract": "Scenario-based approaches have been receiving a huge amount of attention in research and engineering of automated driving systems. Due to the complexity and uncertainty of the driving environment, and the complexity of the driving task itself, the number of possible driving scenarios that an Automated Driving System or Advanced Driving-Assistance System may encounter is virtually infinite. Therefore it is essential to be able to reason about the identification of scenarios and in particular critical ones that may impose unacceptable risk if not considered. Critical scenarios are particularly important to support design, verification and validation efforts, and as a basis for a safety case. In this paper, we present the results of a systematic mapping study in the context of autonomous driving. The main contributions are: (i) introducing a comprehensive taxonomy for critical scenario identification methods; (ii) giving an overview of the state-of-the-art research based on the taxonomy encompassing 86 papers between 2017 and 2020; and (iii) identifying open issues and directions for further research. The provided taxonomy comprises three main perspectives encompassing the problem definition (the why), the solution (the methods to derive scenarios), and the assessment of the established scenarios. In addition, we discuss open research issues considering the perspectives of coverage, practicability, and scenario space explosion.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "991\u20131026",
        "numpages": "36"
    },
    "Identifying Similar Test Cases That Are Specified in Natural Language": {
        "type": "article",
        "key": "10.1109/TSE.2022.3170272",
        "author": "Viggiato, Markos and Paas, Dale and Buzon, Chris and Bezemer, Cor-Paul",
        "title": "Identifying Similar Test Cases That Are Specified in Natural Language",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3170272",
        "doi": "10.1109/TSE.2022.3170272",
        "abstract": "Software testing is still a manual process in many industries, despite the recent improvements in automated testing techniques. As a result, test cases (which consist of one or more test steps that need to be executed manually by the tester) are often specified in natural language by different employees and many redundant test cases might exist in the test suite. This increases the (already high) cost of test execution. Manually identifying similar test cases is a time-consuming and error-prone task. Therefore, in this paper, we propose an unsupervised approach to identify similar test cases. Our approach uses a combination of text embedding, text similarity and clustering techniques to identify similar test cases. We evaluate five different text embedding techniques, two text similarity metrics, and two clustering techniques to cluster similar test steps and three techniques to identify similar test cases from the test step clusters. Through an evaluation in an industrial setting, we showed that our approach achieves a high performance to cluster test steps (an F-score of 87.39%) and identify similar test cases (an F-score of 86.13%). Furthermore, a validation with developers indicates several different practical usages of our approach (such as identifying redundant test cases), which help to reduce the testing manual effort and time.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1027\u20131043",
        "numpages": "17"
    },
    "Data Preparation for Software Vulnerability Prediction: A Systematic Literature Review": {
        "type": "article",
        "key": "10.1109/TSE.2022.3171202",
        "author": "Croft, Roland and Xie, Yongzheng and Babar, Muhammad Ali",
        "title": "Data Preparation for Software Vulnerability Prediction: A Systematic Literature Review",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3171202",
        "doi": "10.1109/TSE.2022.3171202",
        "abstract": "Software Vulnerability Prediction (SVP) is a data-driven technique for software quality assurance that has recently gained considerable attention in the Software Engineering research community. However, the difficulties of preparing Software Vulnerability (SV) related data is considered as the main barrier to industrial adoption of SVP approaches. Given the increasing, but dispersed, literature on this topic, it is needed and timely to systematically select, review, and synthesize the relevant peer-reviewed papers reporting the existing SV data preparation techniques and challenges. We have carried out a Systematic Literature Review (SLR) of SVP research in order to develop a systematized body of knowledge of the data preparation challenges, solutions, and the needed research. Our review of the 61 relevant papers has enabled us to develop a taxonomy of data preparation for SVP related challenges. We have analyzed the identified challenges and available solutions using the proposed taxonomy. Our analysis of the state of the art has enabled us identify the opportunities for future research. This review also provides a set of recommendations for researchers and practitioners of SVP approaches.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1044\u20131063",
        "numpages": "20"
    },
    "Evaluating the Impact of Possible Dependencies on Architecture-Level Maintainability": {
        "type": "article",
        "key": "10.1109/TSE.2022.3171288",
        "author": "Jin, Wuxia and Zhong, Dinghong and Cai, Yuanfang and Kazman, Rick and Liu, Ting",
        "title": "Evaluating the Impact of Possible Dependencies on Architecture-Level Maintainability",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3171288",
        "doi": "10.1109/TSE.2022.3171288",
        "abstract": "Dependencies among software entities are the foundation for much of the research on software architecture analysis and architecture analysis tools. Dynamically typed languages, such as Python, JavaScript and Ruby, tolerate the lack of explicit type references, making certain dependencies indiscernible by a purely syntactic analysis of source code. We call these &lt;italic&gt;possible dependencies&lt;/italic&gt;, in contrast with the &lt;italic&gt;explicit dependencies&lt;/italic&gt; that are directly manifested in source code. We find that existing architecture analysis tools have not taken possible dependencies into consideration. An important question therefore is: &lt;italic&gt;to what extent will these missing possible dependencies impact architecture analysis?&lt;/italic&gt;To answer this question, we conducted a study of 499 open-source Python projects, employing type inference techniques and type hint practices to discern possible dependencies. We investigated the consequences of possible dependencies in three software maintenance contexts, including capturing co-change relations recorded in revision history, measuring architectural maintainability, and detecting architecture anti-patterns that violate design principles and impact maintainability. Our study revealed that the impact of possible dependencies on architecture-level maintainability is substantial\u2014higher than that of explicit dependencies. Our findings suggest that architecture analysis and tools should take into account, assess, and highlight the impacts of possible dependencies caused by dynamic typing.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1064\u20131085",
        "numpages": "22"
    },
    "Fragment-Based Test Generation for Web Apps": {
        "type": "article",
        "key": "10.1109/TSE.2022.3171295",
        "author": "Yandrapally, Rahul Krishna and Mesbah, Ali",
        "title": "Fragment-Based Test Generation for Web Apps",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3171295",
        "doi": "10.1109/TSE.2022.3171295",
        "abstract": "Automated model-based test generation presents a viable alternative to the costly manual test creation currently employed for regression testing of web apps. However, existing model inference techniques rely on threshold-based whole-page comparison to establish state equivalence, which cannot reliably identify near-duplicate web pages in modern web apps. Consequently, existing techniques produce inadequate models for dynamic web apps, and fragile test oracles, rendering the generated regression test suites ineffective. We propose a model-based test generation technique, &lt;sc&gt;FragGen&lt;/sc&gt;, that eliminates the need for thresholds, by employing a novel state abstraction based on page fragmentation to establish state equivalence. &lt;sc&gt;FragGen&lt;/sc&gt; also uses fine-grained page fragment analysis to diversify state exploration and generate reliable test oracles. Our evaluation shows that &lt;sc&gt;FragGen&lt;/sc&gt; outperforms existing whole-page techniques by detecting more near-duplicates, inferring better web app models and generating test suites that are better suited for regression testing. On a dataset of 86,165 state-pairs, &lt;sc&gt;FragGen&lt;/sc&gt; detected 123% more near-duplicates on average compared to whole-page techniques. The crawl models inferred by &lt;sc&gt;FragGen&lt;/sc&gt; have 62% more precision and 70% more recall on average. &lt;sc&gt;FragGen&lt;/sc&gt; also generates reliable regression test suites with test actions that have nearly 100% success rate on the same version of the web app even if the execution environment is varied. The test oracles generated by &lt;sc&gt;FragGen&lt;/sc&gt; can detect 98.7% of the visible changes in web pages while being highly robust, making them suitable for regression testing.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1086\u20131101",
        "numpages": "16"
    },
    "&lt;italic&gt;We\u2019re Not Gonna Break It!&lt;/italic&gt; Consistency-Preserving Operators for Efficient Product Line Configuration": {
        "type": "article",
        "key": "10.1109/TSE.2022.3171404",
        "author": "Horcas, Jose-Miguel and Str\\\"{u}ber, Daniel and Burdusel, Alexandru and Martinez, Jabier and Zschaler, Steffen",
        "title": "&lt;italic&gt;We\u2019re Not Gonna Break It!&lt;/italic&gt; Consistency-Preserving Operators for Efficient Product Line Configuration",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3171404",
        "doi": "10.1109/TSE.2022.3171404",
        "abstract": "When configuring a software product line, finding a good trade-off between multiple orthogonal quality concerns is a challenging multi-objective optimisation problem. State-of-the-art solutions based on search-based techniques create invalid configurations in intermediate steps, requiring additional repair actions that reduce the efficiency of the search. In this work, we introduce &lt;italic&gt;consistency-preserving configuration operators&lt;/italic&gt; (CPCOs)\u2014genetic operators that maintain valid configurations throughout the entire search. CPCOs bundle coherent sets of changes: the activation or deactivation of a particular feature together with other (de)activations that are needed to preserve validity. In our evaluation, our instantiation of the IBEA algorithm with CPCOs outperforms two state-of-the-art tools for optimal product line configuration in terms of both speed and solution quality. The improvements are especially pronounced in large product lines with thousands of features.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1102\u20131117",
        "numpages": "16"
    },
    "Pride: Prioritizing Documentation Effort Based on a PageRank-Like Algorithm and Simple Filtering Rules": {
        "type": "article",
        "key": "10.1109/TSE.2022.3171469",
        "author": "Pan, Weifeng and Ming, Hua and Kim, Dae-Kyoo and Yang, Zijiang",
        "title": "Pride: Prioritizing Documentation Effort Based on a PageRank-Like Algorithm and Simple Filtering Rules",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3171469",
        "doi": "10.1109/TSE.2022.3171469",
        "abstract": "Code documentation can be helpful in many software quality assurance tasks. However, due to resource constraints (e.g., time, human resources, and budget), programmers often cannot document their work completely and timely. In the literature, two approaches (one is supervised and the other is unsupervised) have been proposed to prioritize documentation effort to ensure the most important classes to be documented first. However, both of them contain several limitations. The supervised approach overly relies on a difficult-to-obtain labeled data set and has high computation cost. The unsupervised one depends on a graph representation of the software structure, which is inaccurate since it neglects many important couplings between classes. In this paper, we propose an improved approach, named Pride, to prioritize documentation effort. First, Pride uses a weighted directed class coupling network to precisely describe classes and their couplings. Second, we propose a PageRank-like algorithm to quantify the importance of classes in the whole class coupling network. Third, we use a set of software metrics to quantify source code complexity and further propose a simple but easy-to-operate filtering rule. Fourth, we sort all the classes according to their importance in descending order and use the filtering rule to filter out unimportant classes. Finally, a threshold &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$k$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"ming-ieq1-3171469.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; is utilized, and the top-&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$k$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"ming-ieq2-3171469.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;% ranked classes are the identified important classes to be documented first. Empirical results on a set of nine software systems show that, according to the average ranking of the Friedman test, Pride is superior to the existing approaches in the whole data set.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1118\u20131151",
        "numpages": "34"
    },
    "Refactoring Test Smells With JUnit 5: Why Should Developers Keep Up-to-Date?": {
        "type": "article",
        "key": "10.1109/TSE.2022.3172654",
        "author": "Soares, Elvys and Ribeiro, M\\'{a}rcio and Gheyi, Rohit and Amaral, Guilherme and Santos, Andr\\'{e",
        "title": "Refactoring Test Smells With JUnit 5: Why Should Developers Keep Up-to-Date?",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3172654",
        "doi": "10.1109/TSE.2022.3172654",
        "abstract": "Test smells are symptoms in the test code that indicate possible design or implementation problems. Previous research demonstrated their harmfulness and the developers\u2019 acknowledgment of test smells\u2019 effects, prevention, and refactoring strategies. Test automation frameworks are constantly evolving, and the JUnit, one of the most used ones for Java projects, has its version 5 available since late 2017. However, we do not know the extent to which developers use the newly introduced features and whether such features indeed help refactor existing test code to remove test smells. This article conducts a mixed-method study investigation to minimize these knowledge gaps. Our study consists of three parts. First, we evaluate the usage of this framework and its features by analyzing the source code of 485 popular Java open-source projects on GitHub that use JUnit. We found that 15.9% of these projects use the JUnit 5 library. We also found that, from 17 new features detected in use, only 3 (i.e., 17.6%) are responsible for more than 70% of usages, limiting optimized propositions to test code creation and maintenance. Second, after identifying features in the JUnit 5 framework that could be considered to test smells removal and prevention, we use these features to propose novel refactorings. In particular, we present refactorings based on 7 introduced JUnit 5 features that help to remove 13 test smells, such as Assertion Roulette, Test Code Duplication, and Conditional Test Logic. Third, to evaluate our refactorings with the opinions of experienced developers, we (i) survey 212 developers for their preferences and comments about our refactorings, corroborating the benefits of our proposals and raising community feedback on JUnit 5 features, and (ii) we refactor actual test code from popular GitHub Java projects and submit 38 Pull Requests, reaching a 94% acceptance rate among respondents. As implications of our study, we alert the software testing community (i.e., practitioners and researchers) to the need to study the JUnit 5 features to effectively remove and prevent test smells. To better assist this process, we give directions on how test smells can be refactored using such features.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1152\u20131170",
        "numpages": "19"
    },
    "The Emotional Roller Coaster of Responding to Requirements Changes in Software Engineering": {
        "type": "article",
        "key": "10.1109/TSE.2022.3172925",
        "author": "Madampe, Kashumi and Hoda, Rashina and Grundy, John",
        "title": "The Emotional Roller Coaster of Responding to Requirements Changes in Software Engineering",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3172925",
        "doi": "10.1109/TSE.2022.3172925",
        "abstract": "&lt;italic&gt;Background:&lt;/italic&gt; A preliminary study we conducted showed that software practitioners respond to requirements changes (RCs) with different emotions, and that their emotions vary at stages of the RC handling life cycle, such as &lt;italic&gt;receiving, developing,&lt;/italic&gt; and &lt;italic&gt;delivering&lt;/italic&gt; RCs. Furthermore, such developer emotions have direct linkages to cognition, productivity, and decision making. Therefore, it is important to gain a comprehensive understanding the role of emotions in a critical scenarios like handling RCs. &lt;italic&gt;Objective:&lt;/italic&gt; We wanted to study how practitioners &lt;italic&gt;emotionally&lt;/italic&gt; respond to RCs. &lt;italic&gt;Method:&lt;/italic&gt; We conducted a world-wide survey with the participation of 201 software practitioners. In our survey, we used the Job-related Affective Well-being Scale (JAWS) and open-ended questions to capture participants\u2019 emotions when handling RCs in their work and query about the different circumstances when they feel these emotions. We used a combined approach of statistical analysis, JAWS, and Socio-Technical Grounded Theory (STGT) &lt;italic&gt;for Data Analysis&lt;/italic&gt; to analyse our survey data. &lt;italic&gt;Findings:&lt;/italic&gt; We identified (1) emotional responses to RCs, i.e., the most common &lt;italic&gt;emotions&lt;/italic&gt; felt by practitioners when handling RCs; (2) different &lt;italic&gt;stimuli&lt;/italic&gt; \u2013 such as the RC, the practitioner, team, manager, customer \u2013 that trigger these emotions through their own different characteristics; (3) &lt;italic&gt;emotion dynamics&lt;/italic&gt;, i.e., the changes in emotions during the RC handling life cycle; (4) &lt;italic&gt;RC stages&lt;/italic&gt; where particular emotions are triggered; and (5) &lt;italic&gt;time related aspects&lt;/italic&gt; that regulate the emotion dynamics. &lt;italic&gt;Conclusion:&lt;/italic&gt; Practitioners are not pleased with receiving RCs all the time. Last minute RCs introduced closer to a deadline especially violate emotional well-being of practitioners. We present some practical recommendations for practitioners to follow, including a dual-purpose emotion-centric decision guide to help decide when to introduce or accept an RC, and some future key research directions.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1171\u20131187",
        "numpages": "17"
    },
    "Machine/Deep Learning for Software Engineering: A Systematic Literature Review": {
        "type": "article",
        "key": "10.1109/TSE.2022.3173346",
        "author": "Wang, Simin and Huang, Liguo and Gao, Amiao and Ge, Jidong and Zhang, Tengfei and Feng, Haitao and Satyarth, Ishna and Li, Ming and Zhang, He and Ng, Vincent",
        "title": "Machine/Deep Learning for Software Engineering: A Systematic Literature Review",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3173346",
        "doi": "10.1109/TSE.2022.3173346",
        "abstract": "Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Software Engineering (SE) and Machine Learning (ML)/Deep Learning (DL). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the applicability and generalizability of ML/DL-related SE studies, we conducted a 12-year Systematic Literature Review (SLR) on 1,428 ML/DL-related SE papers published between 2009 and 2020. Our trend analysis demonstrated the impacts that ML/DL brought to SE. We examined the complexity of applying ML/DL solutions to SE problems and how such complexity led to issues concerning the reproducibility and replicability of ML/DL studies in SE. Specifically, we investigated how ML and DL differ in data preprocessing, model training, and evaluation when applied to SE tasks, and what details need to be provided to ensure that a study can be reproduced or replicated. By categorizing the rationales behind the selection of ML/DL techniques into five themes, we analyzed how model performance, robustness, interpretability, complexity, and data simplicity affected the choices of ML/DL models.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1188\u20131231",
        "numpages": "44"
    },
    "A Data Transfer and Relevant Metrics Matching Based Approach for Heterogeneous Defect Prediction": {
        "type": "article",
        "key": "10.1109/TSE.2022.3173678",
        "author": "Bal, Pravas Ranjan and Kumar, Sandeep",
        "title": "A Data Transfer and Relevant Metrics Matching Based Approach for Heterogeneous Defect Prediction",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3173678",
        "doi": "10.1109/TSE.2022.3173678",
        "abstract": "Heterogeneous defect prediction (HDP) is a promising research area in the software defect prediction domain to handle the unavailability of the past homogeneous data. In HDP, the prediction is performed using source dataset in which the independent features (metrics) are entirely different than the independent features of target dataset. One important assumption in machine learning is that independent features of the source and target datasets should be relevant to each other for better prediction accuracy. However, these assumptions do not generally hold in HDP. Further in HDP, the selected source dataset for a given target dataset may be of small size causing insufficient training. To resolve these issues, we have proposed a novel heterogeneous data preprocessing method, namely, Transfer of Data from Target dataset to Source dataset selected using Relevance score (TDTSR), for heterogeneous defect prediction. In the proposed approach, we have used chi-square test to select the relevant metrics between source and target datasets and have performed experiments using proposed approach with various machine learning algorithms. Our proposed method shows an improvement of at least 14% in terms of AUC score in the HDP scenario compared to the existing state of the art models.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1232\u20131245",
        "numpages": "14"
    },
    "Enhancing Mobile App Bug Reporting via Real-Time Understanding of Reproduction Steps": {
        "type": "article",
        "key": "10.1109/TSE.2022.3174028",
        "author": "Fazzini, Mattia and Moran, Kevin and Bernal-C\\'{a}rdenas, Carlos and Wendland, Tyler and Orso, Alessandro and Poshyvanyk, Denys",
        "title": "Enhancing Mobile App Bug Reporting via Real-Time Understanding of Reproduction Steps",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3174028",
        "doi": "10.1109/TSE.2022.3174028",
        "abstract": "One of the primary mechanisms by which developers receive feedback about in-field failures of software from users is through bug reports. Unfortunately, the quality of manually written bug reports can vary widely due to the effort required to include essential pieces of information, such as detailed reproduction steps (S2Rs). Despite the difficulty faced by reporters, few existing bug reporting systems attempt to offer automated assistance to users in crafting easily readable, and conveniently reproducible bug reports. To address the need for proactive bug reporting systems that actively aid the user in capturing crucial information, we introduce a novel bug reporting approach called &lt;sc&gt;EBug&lt;/sc&gt;. &lt;sc&gt;EBug&lt;/sc&gt; assists reporters in writing S2Rs for mobile applications by analyzing natural language information entered by reporters in real-time, and linking this data to information extracted via a combination of static and dynamic program analyses. As reporters write S2Rs, &lt;sc&gt;EBug&lt;/sc&gt; is capable of automatically suggesting potential future steps using predictive models trained on realistic app usages. To evaluate &lt;sc&gt;EBug&lt;/sc&gt;, we performed two user studies based on 20 failures from 11 real-world apps. The empirical studies involved ten participants that submitted ten bug reports each and ten developers that reproduced the submitted bug reports. In the studies, we found that reporters were able to construct bug reports 31% &lt;italic&gt;faster&lt;/italic&gt; with &lt;sc&gt;EBug&lt;/sc&gt; as compared to the state-of-the-art bug reporting system used as a baseline. &lt;sc&gt;EBug&lt;/sc&gt;'s reports were also &lt;italic&gt;more reproducible&lt;/italic&gt; with respect to the ones generated with the baseline. Furthermore, we compared &lt;sc&gt;EBug&lt;/sc&gt;'s prediction models to other predictive modeling approaches and found that, overall, the predictive models of our approach outperformed the baseline approaches. Our results are promising and demonstrate the feasibility and potential benefits provided by proactively assistive bug reporting systems.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1246\u20131272",
        "numpages": "27"
    },
    "SEGRESS: Software Engineering Guidelines for REporting Secondary Studies": {
        "type": "article",
        "key": "10.1109/TSE.2022.3174092",
        "author": "Kitchenham, Barbara and Madeyski, Lech and Budgen, David",
        "title": "SEGRESS: Software Engineering Guidelines for REporting Secondary Studies",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3174092",
        "doi": "10.1109/TSE.2022.3174092",
        "abstract": "&lt;italic&gt;Context&lt;/italic&gt;: Several tertiary studies have criticized the reporting of software engineering secondary studies. &lt;italic&gt;Objective&lt;/italic&gt;: Our objective is to identify guidelines for reporting software engineering (SE) secondary studies which would address problems observed in the reporting of software engineering systematic reviews (SRs). &lt;italic&gt;Method&lt;/italic&gt;: We review the criticisms of SE secondary studies and identify the major areas of concern. We assess the PRISMA 2020 (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement as a possible solution to the need for SR reporting guidelines, based on its status as the reporting guideline recommended by the Cochrane Collaboration whose SR guidelines were a major input to the guidelines developed for SE. We report its advantages and limitations in the context of SE secondary studies. We also assess reporting guidelines for mapping studies and qualitative reviews, and compare their structure and content with that of PRISMA 2020. &lt;italic&gt;Results&lt;/italic&gt;: Previous tertiary studies confirm that reports of secondary studies are of variable quality. However, &lt;italic&gt;ad hoc&lt;/italic&gt; recommendations that amend reporting standards may result in unnecessary duplication of text. We confirm that the PRISMA 2020 statement addresses SE reporting problems, but is mainly oriented to quantitative reviews, mixed-methods reviews and meta-analyses. However, we show that the PRISMA 2020 item definitions can be extended to cover the information needed to report mapping studies and qualitative reviews. &lt;italic&gt;Conclusions&lt;/italic&gt;: In this paper and its Supplementary Material, we present and illustrate an integrated set of guidelines called SEGRESS (Software Engineering Guidelines for REporting Secondary Studies), suitable for quantitative systematic reviews (building upon PRISMA 2020), mapping studies (PRISMA-ScR), and qualitative reviews (ENTREQ and RAMESES), that addresses reporting problems found in current SE SRs.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1273\u20131298",
        "numpages": "26"
    },
    "Towards Scalable Model Checking of Reflective Systems via Labeled Transition Systems": {
        "type": "article",
        "key": "10.1109/TSE.2022.3174408",
        "author": "Tei, Kenji and Tahara, Yasuyuki and Ohsuga, Akihiko",
        "title": "Towards Scalable Model Checking of Reflective Systems via Labeled Transition Systems",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3174408",
        "doi": "10.1109/TSE.2022.3174408",
        "abstract": "Reflection is a technique that enables a system to inspect or change its structure and/or behavior at runtime. It is a key enabler of many techniques for developing systems that have to function despite rapidly changing requirements and environments. A crucial issue in developing reflective systems is to ensure the correctness of their behaviors, because object-level behaviors are affected by metalevel behaviors. In this paper, we present an extended labeled transition system (LTS), which we call a metalevel LTS (MLTS), that supports data representation of another LTS for use in modeling a reflective tower. We show that two of the existing state reduction techniques for an LTS (&lt;italic&gt;symmetry reduction&lt;/italic&gt; and &lt;italic&gt;divergence-sensitive stutter bisimulation&lt;/italic&gt;) are also applicable to an MLTS. Then, we introduce two strategies for implementing an MLTS model in Promela, thereby enabling verification with the SPIN model checker. We also present case studies of applying MLTSs to two reflection applications: self-adaptation of a reconnaissance robot system, and dynamic evolution of an Internet-of-things (IoT) system. The case studies demonstrate the applicability of our approach and its scalability improvement through the state reduction techniques.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1299\u20131322",
        "numpages": "24"
    },
    "Annotative Software Product Line Analysis Using Variability-Aware Datalog": {
        "type": "article",
        "key": "10.1109/TSE.2022.3175752",
        "author": "Shahin, Ramy and Akhundov, Murad and Chechik, Marsha",
        "title": "Annotative Software Product Line Analysis Using Variability-Aware Datalog",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3175752",
        "doi": "10.1109/TSE.2022.3175752",
        "abstract": "Applying program analyses to Software Product Lines (SPLs) has been a fundamental research problem at the intersection of Product Line Engineering and software analysis. Different attempts have been made to \u201clift\u201d particular product-level analyses to run on the entire product line. In this paper, we tackle the class of Datalog-based analyses (e.g., pointer and taint analyses), study the theoretical aspects of lifting Datalog inference, and implement a lifted inference algorithm inside the Souffl\u00e9 Datalog engine. We evaluate our implementation on a set of Java and C-language benchmark annotative software product lines. We show significant savings in processing time and fact database size (billions of times faster on one of the benchmarks) compared to brute-force analysis of each product individually.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1323\u20131341",
        "numpages": "19"
    },
    "Towards Reliable Online Just-in-Time Software Defect Prediction": {
        "type": "article",
        "key": "10.1109/TSE.2022.3175789",
        "author": "Cabral, George G. and Minku, Leandro L.",
        "title": "Towards Reliable Online Just-in-Time Software Defect Prediction",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3175789",
        "doi": "10.1109/TSE.2022.3175789",
        "abstract": "Throughout its development period, a software project experiences different phases, comprises modules with different complexities and is touched by many different developers. Hence, it is natural that problems such as Just-in-Time Software Defect Prediction (JIT-SDP) are affected by changes in the defect generating process (concept drifts), potentially hindering predictive performance. JIT-SDP also suffers from delays in receiving the labels of training examples (verification latency), potentially exacerbating the challenges posed by concept drift and further hindering predictive performance. However, little is known about what types of concept drift affect JIT-SDP and how they affect JIT-SDP classifiers in view of verification latency. This work performs the first detailed analysis of that. Among others, it reveals that different types of concept drift together with verification latency significantly impair the stability of the predictive performance of existing JIT-SDP approaches, drastically affecting their reliability over time. Based on the findings, a new JIT-SDP approach is proposed, aimed at providing higher and more stable predictive performance (i.e., reliable) over time. Experiments based on ten GitHub open source projects show that our approach was capable of produce significantly more stable predictive performances in all investigated datasets while maintaining or improving the predictive performance obtained by state-of-art methods.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1342\u20131358",
        "numpages": "17"
    },
    "Software Updates Strategies: A Quantitative Evaluation Against Advanced Persistent Threats": {
        "type": "article",
        "key": "10.1109/TSE.2022.3176674",
        "author": "Tizio, Giorgio Di and Armellini, Michele and Massacci, Fabio",
        "title": "Software Updates Strategies: A Quantitative Evaluation Against Advanced Persistent Threats",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3176674",
        "doi": "10.1109/TSE.2022.3176674",
        "abstract": "Software updates reduce the opportunity for exploitation. However, since updates can also introduce breaking changes, enterprises face the problem of balancing the need to secure software with updates with the need to support operations. We propose a methodology to quantitatively investigate the effectiveness of software updates strategies against attacks of Advanced Persistent Threats (APTs). We consider strategies where the vendor updates are the only limiting factors to cases in which enterprises delay updates from 1 to 7 months based on SANS data. Our manually curated dataset of APT attacks covers 86 APTs and 350 campaigns from 2008 to 2020. It includes information about attack vectors, exploited vulnerabilities (e.g., 0-days versus public vulnerabilities), and affected software and versions. Contrary to common belief, most APT campaigns employed publicly known vulnerabilities. If an enterprise could theoretically update as soon as an update is released, it would face lower odds of being compromised than those waiting one (4.9x) or three (9.1x) months. However, if attacked, it could still be compromised from 14% to 33% of the times. As in practice enterprises must do regression testing before applying an update, our major finding is that one could perform 12% of all possible updates restricting oneself only to versions fixing publicly known vulnerabilities without significant changes to the odds of being compromised compared to a company that updates for all versions.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1359\u20131373",
        "numpages": "15"
    },
    "Construct Validity in Software Engineering": {
        "type": "article",
        "key": "10.1109/TSE.2022.3176725",
        "author": "Sj\\o{}berg, Dag I. K. and Bergersen, Gunnar Rye",
        "title": "Construct Validity in Software Engineering",
        "year": "2023",
        "issue_date": "March 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "3",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3176725",
        "doi": "10.1109/TSE.2022.3176725",
        "abstract": "Empirical research aims to establish generalizable claims from data. Such claims may involve concepts that must be measured indirectly by using indicators. Construct validity is concerned with whether one can justifiably make claims at the conceptual level that are supported by results at the operational level. We report a quantitative analysis of the awareness of construct validity in the software engineering literature between 2000 and 2019 and a qualitative review of 83 articles about human-centric experiments published in five high-quality journals between 2015 and 2019. Over the two decades, the appearance in the literature of the term construct validity increased sevenfold. Some of the reviewed articles we reviewed employed various ways to ensure that the indicators span the concept in an unbiased manner. We also found articles that reuse formerly validated constructs. However, the articles disagree about how to define construct validity. Several interpret construct validity excessively by including threats to internal, external, or statistical conclusion validity. A few articles also include fundamental challenges of a study, such as cheating and misunderstanding of experiment material. The diversity of topics included as threats to construct validity calls for a more minimalist approach. Based on the review, we propose seven guidelines to improve how construct validity is handled and reported in software engineering.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1374\u20131396",
        "numpages": "23"
    },
    "Differential Testing of Machine Translators Based on Compositional Semantics": {
        "type": "article",
        "key": "10.1109/TSE.2023.3323969",
        "author": "Liu, Shuang and Dou, Shujie and Chen, Junjie and Zhang, Zhirun and Lu, Ye",
        "title": "Differential Testing of Machine Translators Based on Compositional Semantics",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3323969",
        "doi": "10.1109/TSE.2023.3323969",
        "abstract": "Powered by the advances of deep neural networks, machine translation software has achieved rapid progresses recently. Machine translators are widely adopted in people's daily lives, e.g., for information consumption, medical consumption and online shopping. However, machine translators are far from robust, and may produce wrong translations, which could potentially cause misunderstandings or even serious consequences. It is thus critical to detect errors in machine translators, and provide informative feedback for developers. In this work, we adopt the differential testing method to test machine translators. In particular, we use mature commercial translators as reference machine translation engines. Based on the principle of compositionality, which specifies that the meaning of a complex expression is determined by the meanings of its constituent expressions and the syntactic rules used to combine them, we design the oracle which conducts similarity comparison guided by syntactic structure and semantic encoding. In particular, we employ the constituency parsing to obtain the part-whole structure relation between a sentence and one of its component. Then we compute the semantic similarity of each sentence part with pre-trained language model and expert knowledge. We implement our approach into a tool named DCS, conduct experiments on three popular machine translators, i.e., Google translate, Baidu translate and Microsoft Bing translate, and compare DCS with two state-of-the-art approaches, i.e., CIT and CAT. The experiment results show that DCS achieves 8.6\\% and 35.4\\% higher precision, respectively. Moreover, the errors reported by DCS have the lowest redundancy in terms of the duplicated error locations in the source sentence. DCS can be used in complement with existing approaches and achieve higher detection precision. It also shows comparable efficiency with state-of-the-art approaches.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "oct",
        "pages": "5046\u20135059",
        "numpages": "14"
    },
    "FeatRacer: Locating Features Through Assisted Traceability": {
        "type": "article",
        "key": "10.1109/TSE.2023.3324719",
        "author": "Mukelabai, Mukelabai and Hermann, Kevin and Berger, Thorsten and Stegh\\\"{o}fer, Jan-Philipp",
        "title": "FeatRacer: Locating Features Through Assisted Traceability",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3324719",
        "doi": "10.1109/TSE.2023.3324719",
        "abstract": "Locating features is one of the most common software development activities. It is typically done during maintenance and evolution, when developers need to identify the exact places in a codebase where specific features are implemented. Unfortunately, locating features is laborious and error-prone, since feature knowledge fades, projects are developed by different developers, and features are often scattered across the codebase. Recognizing the need, many &lt;italic&gt;automated feature location techniques&lt;/italic&gt; have been proposed, which try to retroactively recover features, i.e., very domain-specific information from the codebase. Unfortunately, such techniques require large training datasets, only recover coarse-grained locations and produce too many false positives to be useful in practice. An alternative is &lt;italic&gt;recording features during development&lt;/italic&gt;, when they are still fresh in a developer's mind. However, recording is easily forgotten and also costly, especially when the software evolves and such recordings need to be updated. We address the infamous &lt;italic&gt;feature location problem&lt;/italic&gt; (a.k.a., &lt;italic&gt;concern location&lt;/italic&gt; or &lt;italic&gt;concept assignment problem&lt;/italic&gt;) differently. We present FeatRacer, which combines feature recording and automated feature location in a way that allows developers to proactively and continuously record features and their locations during development, while addressing the shortcomings of both strategies. Specifically, FeatRacer relies on embedded code annotations and a machine-learning-based recommender system. When a developer forgets to annotate, FeatRacer reminds the developer about potentially missing features, which it learned from the feature recording practices in the project at hand. FeatRacer also facilitates fine-grained locations as decided by the developer. Our evaluation shows that FeatRacer outperforms traditional automated feature location based on Latent Semantic Indexing (LSI) and Linear Discriminant Analysis (LDA)\u2014two of the most common methods to realize such techniques\u2014when predicting features for 4,650 commit changesets from the histories of 16 open-source projects spanning an average of three years between 1985 and 2015. Compared to the traditional techniques, FeatRacer showed a 3x higher precision and a 4.5x higher recall, with an average precision and recall of 89.6\\% among all 16 projects. It can accurately predict feature locations within the first five commits of our evaluation projects, being effective already for small datasets. FeatRacer takes on average 1.9ms to learn from past code fragments of a project, and 0.002ms to predict forgotten feature annotations in new code.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "oct",
        "pages": "5060\u20135083",
        "numpages": "24"
    },
    "Simulating Operational Memory Models Using Off-the-Shelf Program Analysis Tools": {
        "type": "article",
        "key": "10.1109/TSE.2023.3326056",
        "author": "Iorga, Dan and Wickerson, John and Donaldson, Alastair F.",
        "title": "Simulating Operational Memory Models Using Off-the-Shelf Program Analysis Tools",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3326056",
        "doi": "10.1109/TSE.2023.3326056",
        "abstract": "Memory models allow reasoning about the correctness of multithreaded programs. Constructing and using such models is facilitated by &lt;italic&gt;simulators&lt;/italic&gt; that reveal which behaviours of a given program are allowed. While extensive work has been done on simulating &lt;italic&gt;axiomatic&lt;/italic&gt; memory models, there has been less work on simulation of &lt;italic&gt;operational&lt;/italic&gt; models. Operational models are often considered more intuitive than axiomatic models, but are challenging to simulate due to the vast number of paths through the model's transition system. Observing that a similar path-explosion problem is tackled by program analysis tools, we investigate the idea of reducing the decision problem of \u201cwhether a given memory model allows a given behaviour\u201d to the decision problem of \u201cwhether a given C program is safe\u201d, which can be handled by a variety of off-the-shelf tools. We report on our experience using multiple program analysis tools for C for this purpose\u2014a model checker (CBMC), a symbolic execution tool (KLEE), and three coverage-guided fuzzers (libFuzzer, Centipede and AFL++)\u2014presenting two case-studies. First, we evaluate the performance and scalability of these tools in the context of the x86 memory model, showing that fuzzers offer performance competitive with that of RMEM, a state-of-the-art bespoke memory model simulator. Second, we study a more complex, recently developed memory model for hybrid CPU/FPGA devices for which no bespoke simulator is available. We highlight how different encoding strategies can aid the various tools and show how our approach allows us to simulate the CPU/FPGA model twice as deeply as in prior work, leading to us finding and fixing several infidelities in the model. We also experimented with applying three analysis tools that won the \u201cfalsification\u201d category in the 2023 Annual Software Verification Competition (SV-COMP). We found that these tools do not scale to our use cases, motivating us to submit example C programs arising from our work for inclusion in the set of SV-COMP benchmarks, so that they can serve as challenge examples.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "oct",
        "pages": "5084\u20135102",
        "numpages": "19"
    },
    "An Empirical Study of Refactoring Rhythms and Tactics in the Software Development Process": {
        "type": "article",
        "key": "10.1109/TSE.2023.3326775",
        "author": "Noei, Shayan and Li, Heng and Georgiou, Stefanos and Zou, Ying",
        "title": "An Empirical Study of Refactoring Rhythms and Tactics in the Software Development Process",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3326775",
        "doi": "10.1109/TSE.2023.3326775",
        "abstract": "It is critical for developers to develop high-quality software to reduce maintenance cost. While often, developers apply refactoring practices to make source code readable and maintainable without impacting the software functionality. Existing studies identify development rhythms (i.e., weekly development patterns) and their relationship with various metrics, such as productivity. However, existing studies focus entirely on development rhythms. There is no study on refactoring rhythms and their relationship with code quality. Moreover, the existing studies categorize the refactoring tactics (i.e., long-term refactoring patterns) into two general concepts of consistent and inconsistent refactoring. Nevertheless, the existence of other tactics and their relationship with code quality is not explored. In this paper, we conduct an empirical study on the refactoring practices of 196 Apache projects in the early, middle, and late stages of development. We aim to identify (1) existing refactoring rhythms, (2) further refactoring tactics, and (3) the relationship between the identified tactics and rhythms with code quality. The recognition of existing refactoring strategies and their relationship with code quality can assist practitioners in recognizing and applying the appropriate and high-quality refactoring rhythms or tactics to deliver a higher quality of software. We find two frequently used refactoring rhythms: work-day refactoring and all-day refactoring. We also identify two deviations of floss and root canal refactoring tactics as: intermittent root canal, intermittent spiked floss, frequent spiked floss, and frequent root canal. We find that root canal-based tactics are correlated with less increase in the code smells (i.e., higher quality code) compared to floss-based tactics. Moreover, we find that refactoring rhythms are not significantly correlated with the quality of the code. Furthermore, we provide detailed information on the relationship of each refactoring tactic to each code smell type.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "5103\u20135119",
        "numpages": "17"
    },
    "Identifying the Hazard Boundary of ML-Enabled Autonomous Systems Using Cooperative Coevolutionary Search": {
        "type": "article",
        "key": "10.1109/TSE.2023.3327575",
        "author": "Sharifi, Sepehr and Shin, Donghwan and Briand, Lionel C. and Aschbacher, Nathan",
        "title": "Identifying the Hazard Boundary of ML-Enabled Autonomous Systems Using Cooperative Coevolutionary Search",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3327575",
        "doi": "10.1109/TSE.2023.3327575",
        "abstract": "In Machine Learning (ML)-enabled autonomous systems (MLASs), it is essential to identify the &lt;italic&gt;hazard boundary&lt;/italic&gt; of ML Components (MLCs) in the MLAS under analysis. Given that such boundary captures the conditions in terms of MLC behavior and system context that can lead to hazards, it can then be used to, for example, build a safety monitor that can take any predefined fallback mechanisms at runtime when reaching the hazard boundary. However, determining such &lt;italic&gt;hazard boundary&lt;/italic&gt; for an ML component is challenging. This is due to the problem space combining system contexts (i.e., scenarios) and MLC behaviors (i.e., inputs and outputs) being far too large for exhaustive exploration and even to handle using conventional metaheuristics, such as genetic algorithms. Additionally, the high computational cost of simulations required to determine any MLAS safety violations makes the problem even more challenging. Furthermore, it is unrealistic to consider a region in the problem space deterministically safe or unsafe due to the uncontrollable parameters in simulations and the non-linear behaviors of ML models (e.g., deep neural networks) in the MLAS under analysis. To address the challenges, we propose MLCSHE (ML Component Safety Hazard Envelope), a novel method based on a Cooperative Co-Evolutionary Algorithm (CCEA), which aims to tackle a high-dimensional problem by decomposing it into two lower-dimensional search subproblems. Moreover, we take a &lt;italic&gt;probabilistic&lt;/italic&gt; view of safe and unsafe regions and define a novel fitness function to measure the distance from the probabilistic hazard boundary and thus drive the search effectively. We evaluate the effectiveness and efficiency of MLCSHE on a complex Autonomous Vehicle (AV) case study. Our evaluation results show that MLCSHE is significantly more effective and efficient compared to a standard genetic algorithm and random search.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "5120\u20135138",
        "numpages": "19"
    },
    "&lt;sc&gt;Prevent&lt;/sc&gt;: An Unsupervised Approach to Predict Software Failures in Production": {
        "type": "article",
        "key": "10.1109/TSE.2023.3327583",
        "author": "Denaro, Giovanni and Heydarov, Rahim and Mohebbi, Ali and Pezz\\`{e}, Mauro",
        "title": "&lt;sc&gt;Prevent&lt;/sc&gt;: An Unsupervised Approach to Predict Software Failures in Production",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3327583",
        "doi": "10.1109/TSE.2023.3327583",
        "abstract": "This paper presents &lt;sc&gt;Prevent&lt;/sc&gt;, a fully unsupervised approach to predict and localize failures in distributed enterprise applications. Software failures in production are unavoidable. Predicting failures and locating failing components online are the first steps to proactively manage faults in production. Many techniques predict failures from anomalous combinations of system metrics with supervised, weakly supervised, and semi-supervised learning models. Supervised approaches require large sets of labelled data not commonly available in large enterprise applications, and address failure types that can be either captured with predefined rules or observed while training supervised models. &lt;sc&gt;Prevent&lt;/sc&gt; integrates the core ingredients of unsupervised approaches into a novel fully unsupervised approach to predict failures and localize failing resources. The results of experimenting with &lt;sc&gt;Prevent&lt;/sc&gt; on a commercially-compliant distributed cloud system indicate that &lt;sc&gt;Prevent&lt;/sc&gt; provides more stable, reliable and timely predictions than supervised learning approaches, without requiring the often impractical training with labeled data.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "5139\u20135153",
        "numpages": "15"
    },
    "Mitigating False Positive Static Analysis Warnings: Progress, Challenges, and Opportunities": {
        "type": "article",
        "key": "10.1109/TSE.2023.3329667",
        "author": "Guo, Zhaoqiang and Tan, Tingting and Liu, Shiran and Liu, Xutong and Lai, Wei and Yang, Yibiao and Li, Yanhui and Chen, Lin and Dong, Wei and Zhou, Yuming",
        "title": "Mitigating False Positive Static Analysis Warnings: Progress, Challenges, and Opportunities",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3329667",
        "doi": "10.1109/TSE.2023.3329667",
        "abstract": "Static analysis (SA) tools can generate useful static warnings to reveal the problematic code snippets in a software system without dynamically executing the corresponding source code. In the literature, static warnings are of paramount importance because they can easily indicate specific types of software defects in the early stage of a software development process, which accordingly reduces the maintenance costs by a substantial margin. Unfortunately, due to the conservative approximations of such SA tools, a large number of false positive (FP for short) warnings (i.e., they do not indicate real bugs) are generated, making these tools less effective. During the past two decades, therefore, many false positive mitigation (FPM for short) approaches have been proposed so that more accurate and critical warnings can be delivered to developers. This paper offers a detailed survey of research achievements on the topic of FPM. Given the collected 130 surveyed papers, we conduct a comprehensive investigation from five different perspectives. First, we reveal the research trends of this field. Second, we classify the existing FPM approaches into five different types and then present the concrete research progress. Third, we analyze the evaluation system applied to examine the performance of the proposed approaches in terms of studied SA tools, evaluation scenarios, performance indicators, and collected datasets, respectively. Fourth, we summarize the four types of empirical studies relating to SA warnings to exploit the insightful findings that are helpful to reduce FP warnings. Finally, we sum up 10 challenges unresolved in the literature from the aspects of systematicness, effectiveness, completeness, and practicability and outline possible research opportunities based on three emerging techniques in the future.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "5154\u20135188",
        "numpages": "35"
    },
    "Studying the Influence and Distribution of the Human Effort in a Hybrid Fitness Function for Search-Based Model-Driven Engineering": {
        "type": "article",
        "key": "10.1109/TSE.2023.3329730",
        "author": "Casamayor, Rodrigo and Cetina, Carlos and Pastor, \\'{O}scar and P\\'{e}rez, Francisca",
        "title": "Studying the Influence and Distribution of the Human Effort in a Hybrid Fitness Function for Search-Based Model-Driven Engineering",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3329730",
        "doi": "10.1109/TSE.2023.3329730",
        "abstract": "Search-Based Software Engineering (SBSE) offers solutions that efficiently explore large complex problem spaces. To obtain more favorable solutions, human participation in the search process is needed. However, humans cannot handle the same number of solutions as an algorithm. We propose the first hybrid fitness function that combines human effort with human simulations. Human effort refers to human participation for providing evaluations of candidate solutions during the search process, whereas human simulations refer to recreations of a scenario in a specific situation for automatically obtaining the evaluation of candidate solutions. We also propose three variants for the hybrid fitness function that vary in the distribution of human effort in order to study whether the variants influence the performance in terms of solution quality. Specifically, we leverage our hybrid fitness function to locate bugs in software models for the video games of game software engineering. Video games are a fertile domain for these hybrid functions because simulated players are naturally developed as part of the video games (e.g., bots in First-Person Shooters). Our evaluation is at the scale of industrial settings with a commercial video game (Play Station 4 and Steam) and 29 professional video game developers. Hybridizing the fitness function outperforms the results of the best baseline by 33.46\\% in F-measure. A focus group confirms the acceptance of the hybrid fitness function. Hybridizing the fitness function significantly improves the bug localization process by reducing the amount of tedious manual work and by minimizing the number of bugs that go unnoticed. Furthermore, the variant that obtains the best results is a counter-intuitive result that was under the radar of the interactive SBSE community. These results can help not only video game developers to locate bugs, but they can also inspire SBSE researchers to bring hybrid fitness functions to other software engineering tasks.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "5189\u20135202",
        "numpages": "14"
    },
    "Confirmation Bias and Time Pressure: A Family of Experiments in Software Testing": {
        "type": "article",
        "key": "10.1109/TSE.2023.3330400",
        "author": "Salman, Iflaah and Turhan, Burak and Rama\\v{c}, Robert and Mandi\\'{c}, Vladimir",
        "title": "Confirmation Bias and Time Pressure: A Family of Experiments in Software Testing",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3330400",
        "doi": "10.1109/TSE.2023.3330400",
        "abstract": "&lt;bold&gt;Background&lt;/bold&gt;: Software testers manifest confirmation bias (the cognitive tendency) when they design relatively more specification consistent test cases than specification inconsistent test cases. Time pressure may influence confirmation bias of testers per the research in the psychology discipline. &lt;bold&gt;Objective:&lt;/bold&gt; We examine the manifestation of confirmation bias of software testers while designing functional test cases, and the effect of time pressure on confirmation bias in the same context. &lt;bold&gt;Method:&lt;/bold&gt; We executed one internal and two external experimental replications concerning the original experimentation in Oulu. We analyse individual replications and meta-analyse our family of experiments (the original and replications) for joint results on the phenomena. &lt;bold&gt;Results:&lt;/bold&gt; Our findings indicate a significant manifestation of confirmation bias by software testers during the designing of functional test cases. Time pressure significantly promoted confirmation bias among testers per the joint results of the family. The different experimental sites affected the results; however, we did not detect any effects of site-specific variables. &lt;bold&gt;Conclusion:&lt;/bold&gt; Software testers should develop an outside-of-the-box thinking attitude to counter the manifestation of confirmation bias. Time pressure can be manoeuvred by centring manual suites on the designing and consequently the execution of inconsistent test cases, while automated testing focuses on consistent ones.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "5203\u20135222",
        "numpages": "20"
    },
    "The &lt;italic&gt;Why&lt;/italic&gt;, &lt;italic&gt;When&lt;/italic&gt;, &lt;italic&gt;What,&lt;/italic&gt; and &lt;italic&gt;How&lt;/italic&gt; About Predictive Continuous Integration: A Simulation-Based Investigation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3330510",
        "author": "Liu, Bohan and Zhang, He and Ma, Weigang and Li, Gongyuan and Li, Shanshan and Shen, Haifeng",
        "title": "The &lt;italic&gt;Why&lt;/italic&gt;, &lt;italic&gt;When&lt;/italic&gt;, &lt;italic&gt;What,&lt;/italic&gt; and &lt;italic&gt;How&lt;/italic&gt; About Predictive Continuous Integration: A Simulation-Based Investigation",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3330510",
        "doi": "10.1109/TSE.2023.3330510",
        "abstract": "Continuous Integration (CI) enables developers to detect defects early and thus reduce lead time. However, the high frequency and long duration of executing CI have a detrimental effect on this practice. Existing studies have focused on using CI outcome predictors to reduce frequency. Since there is no reported project using predictive CI, it is difficult to evaluate its economic impact. This research aims to investigate predictive CI from a process perspective, including why and when to adopt predictors, what predictors to be used, and how to practice predictive CI in real projects. We innovatively employ Software Process Simulation to simulate a predictive CI process with a Discrete-Event Simulation (DES) model and conduct simulation-based experiments. We develop the Rollback-based Identification of Defective Commits (RIDEC) method to account for the negative effects of false predictions in simulations. Experimental results show that: 1) using predictive CI generally improves the effectiveness of CI, reducing time costs by up to 36.8\\% and the average waiting time before executing CI by 90.5\\%; 2) the time-saving varies across projects, with higher commit frequency projects benefiting more; and 3) predictor performance does not strongly correlate with time savings, but the precision of both failed and passed predictions should be paid more attention. Simulation-based evaluation helps identify overlooked aspects in existing research. Predictive CI saves time and resources, but improved prediction performance has limited cost-saving benefits. The primary value of predictive CI lies in providing accurate and quick feedback to developers, aligning with the goal of CI.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "5223\u20135249",
        "numpages": "27"
    },
    "Robust Test Selection for Deep Neural Networks": {
        "type": "article",
        "key": "10.1109/TSE.2023.3330982",
        "author": "Sun, Weifeng and Yan, Meng and Liu, Zhongxin and Lo, David",
        "title": "Robust Test Selection for Deep Neural Networks",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3330982",
        "doi": "10.1109/TSE.2023.3330982",
        "abstract": "Deep Neural Networks (DNNs) have been widely used in various domains, such as computer vision and software engineering. Although many DNNs have been deployed to assist various tasks in the real world, similar to traditional software, they also suffer from defects that may lead to severe outcomes. DNN testing is one of the most widely used methods to ensure the quality of DNNs. Such method needs rich test inputs with oracle information (expected output) to reveal the incorrect behaviors of a DNN model. However, manually labeling all the collected test inputs is a labor-intensive task, which delays the quality assurance process. &lt;italic&gt;Test selection&lt;/italic&gt; tackles this problem by carefully selecting a small, more suspicious set of test inputs to label, enabling the failure detection of a DNN model with reduced effort. Researchers have proposed different test selection methods, including neuron-coverage-based and uncertainty-based methods, where the uncertainty-based method is arguably the most popular technique. Unfortunately, existing uncertainty-based selection methods meet the performance bottleneck due to one or several limitations: 1) they ignore noisy data in real scenarios; 2) they wrongly exclude many &lt;italic&gt;failure-revealing test inputs&lt;/italic&gt; but rather include many &lt;italic&gt;successful test inputs&lt;/italic&gt; (referring to those test inputs that are correctly predicted by the model); 3) they ignore the diversity of the selected test set. In this paper, we propose RTS, a Robust Test Selection method for deep neural networks to overcome the limitations mentioned above. First, RTS divides all unlabeled candidate test inputs into noise set, successful set, and suspicious set and assigns different selection prioritization to divided sets, which effectively alleviates the impact of noise and improves the ability to identify suspect test inputs. Subsequently, RTS leverages a probability-tier-matrix-based test metric for prioritizing the test inputs in each divided set (i.e., suspicious, successful, and noise set). As a result, RTS can select more suspicious test inputs within a limited selection size. We evaluate RTS by comparing it with 14 baseline methods under 5 widely-used DNN models and 6 widely-used datasets. The experimental results demonstrate that RTS can significantly outperform all test selection methods in failure detection capability and the test suites selected by RTS have the best model optimization capability. For example, when selecting 2.5\\% test input, RTS achieves an improvement of 9.37\\%-176.75\\% over baseline methods in terms of failure detection.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "5250\u20135278",
        "numpages": "29"
    },
    "PatchDiscovery: Patch Presence Test for Identifying Binary Vulnerabilities Based on Key Basic Blocks": {
        "type": "article",
        "key": "10.1109/TSE.2023.3332732",
        "author": "Xu, Xi and Zheng, Qinghua and Yan, Zheng and Fan, Ming and Jia, Ang and Zhou, Zhaohui and Wang, Haijun and Liu, Ting",
        "title": "PatchDiscovery: Patch Presence Test for Identifying Binary Vulnerabilities Based on Key Basic Blocks",
        "year": "2023",
        "issue_date": "Dec. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "12",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3332732",
        "doi": "10.1109/TSE.2023.3332732",
        "abstract": "Software vulnerabilities are easily propagated through code reuses, which pose dire threats to software system security. Automatic patch presence test offers an effective way to detect whether vulnerabilities have been patched, which is significant for large-scale software system maintenance. However, most existing approaches cannot handle binary codes. They suffer from low accuracy and poor efficiency. None of them are resilient to version gap, function size, and patch size. To tackle the above problems, we propose &lt;italic&gt;PatchDiscovery&lt;/italic&gt;, a patch presence test approach to identify binary vulnerabilities by extracting key basic blocks of patch and vulnerability as their signatures for patch discovery. We propose an efficient and accurate basic block matching method over the normalized and simplified control flow graphs (CFGs) of a vulnerable function (VF) and its patched function (PF) to precisely locate a vulnerability and a patch. Then, we conduct fine-grained patch-level analysis on the patch and the vulnerability to gain their key basic blocks as the signatures of PF and VF for patch presence test. Concretely, the key basic blocks of PF and VF are separately searched in a target function (TF) to identify whether the TF is more similar to PF or VF, i.e., patched or not. Extensive experiments based on two real-world binary datasets that contain 524 common vulnerabilities and exposures (CVEs) with 11607 target functions reveal that &lt;italic&gt;PatchDiscovery&lt;/italic&gt; is very effective and efficient. It achieves &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$92.2\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mn&gt;92.2&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"yan-ieq1-3332732.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; F-measure and takes only 0.091s on average to test a target function. It is also resilient to version gap, patch size, and function size to a good extent. Moreover, it is outperforming the state-of-the-art works and has a much faster testing speed for large-scale patch detection. Moreover, &lt;italic&gt;PatchDiscovery&lt;/italic&gt; achieves good performance in firmware vulnerability discovery scenario.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "5279\u20135294",
        "numpages": "16"
    }
}