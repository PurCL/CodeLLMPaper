@article{10.1145/3689711,
author = {Li, Yichuan and Song, Wei and Huang, Jeff},
title = {VarLifter: Recovering Variables and Types from Bytecode of Solidity Smart Contracts},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689711},
doi = {10.1145/3689711},
abstract = {Since funds or tokens in smart contracts are maintained through specific state variables, contract audit, an effective means for security assurance, particularly focuses on these variables and their related operations. However, the absence of publicly accessible source code for numerous contracts, with only bytecode exposed, hinders audit efforts. Recovering variables and their types from Solidity bytecode is thus a critical task in smart contract analysis and audit, yet this is a challenging task because the bytecode loses variable and type information, only with low-level data operated by stack manipulations and untyped memory/storage accesses. The state-of-the-art smart contract decompilers miss identifying many variables and incorrectly infer the types for many identified variables. To this end, we propose VarLifter, a lifter dedicated to the precise and efficient recovery of typed variables. VarLifter interprets every read or written field of a data region as at least one potential variable, and after discarding falsely identified variables, it progressively refines the variable types based on the variable behaviors in the form of operation sequences. We evaluate VarLifter on 34,832 real-world Solidity smart contracts. VarLifter attains a precision of 97.48\% and a recall of 91.84\% for typed variable recovery. Moreover, VarLifter finishes analyzing 77\% of smart contracts in around 10 seconds per contract. If VarLifter is used to replace the variable recovery modules of the two state-of-the-art Solidity bytecode decompilers, 52.4\%, and 74.6\% more typed variables will be correctly recovered, respectively. The applications of VarLifter to contract decompilation, contract audit, and contract bytecode fuzzing illustrate that the recovered variable information improves many contract analysis tasks.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {271},
numpages = {29},
keywords = {Blockchain, EVM, Solidity bytecode, smart contract, variable recovery}
}

@software{10.5281/zenodo.12671461,
author = {Li, Yichuan and Song, Wei and Huang, Jeff},
title = {VarLifter: Recovering Variables and Types from Bytecode of Solidity Smart Contracts},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12671461},
abstract = {
    <p>The artifact presented here is VarLifter, a tool designed to lift
variables and their types from compiled Solidity smart contract runtime
bytecode. VarLifter provides precise variable and type information,
freeing researchers from the burden of dealing with intricate low-level
details, and is proven to assist downstream tasks related to smart
contracts. VarLifter supports both command-line interface (CLI) and
graphical user interface (GUI) for ease of use.</p>

},
keywords = {Blockchain, EVM, smart contract, Solidity bytecode, variable recovery}
}

@article{10.1145/3689712,
author = {Simonnet, Julien and Lemerre, Matthieu and Sighireanu, Mihaela},
title = {A Dependent Nominal Physical Type System for Static Analysis of Memory in Low Level Code},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689712},
doi = {10.1145/3689712},
abstract = {We tackle the problem of checking non-proof-carrying code, i.e. automatically proving type-safety (implying in our type system spatial memory safety) of low-level C code or of machine code resulting from its compilation without modification. This requires a precise static analysis that we obtain by having a type system which (i) is expressive enough to encode common low-level idioms, like pointer arithmetic, discriminating variants by bit-stealing on aligned pointers, storing the size and the base address of a buffer in distinct parts of the memory, or records with flexible array members, among others; and (ii) can be embedded in an abstract interpreter. We propose a new type system that meets these criteria. The distinguishing feature of this type system is a nominal organization of contiguous memory regions, which (i) allows nesting, concatenation, union, and sharing parameters between regions; (ii) induces a lattice over sets of addresses from the type definitions; and (iii) permits updates to memory cells that change their type without requiring one to control aliasing. We provide a semantic model for our type system, which enables us to derive sound type checking rules by abstract interpretation, then to integrate these rules as an abstract domain in a standard flow-sensitive static analysis. Our experiments on various challenging benchmarks show that semantic type-checking using this expressive type system generally succeeds in proving type safety and spatial memory safety of C and machine code programs without modification, using only user-provided function prototypes.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {272},
numpages = {30},
keywords = {Abstract interpretation, Dependent types, Spatial memory safety, Type checking, Typed C}
}

@software{10.5281/zenodo.13383433,
author = {Simonnet, Julien and Lemerre, Matthieu and Sighireanu, Mihaela},
title = {Artifact for the paper 'A Dependent Nominal Physical Type System for the Static Analysis of Memory in Low Level Code'},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13383433},
abstract = {
    <p>The artifact includes the sources of the analyser Codex, the set of
benchmarks used in experiments, and the utilities (makefiles, scripts)
to reproduce the results presented in this paper.</p>

},
keywords = {Abstract interpretation, Dependent types, Spatial memory safety, Type checking, Typed C}
}

@article{10.1145/3689713,
author = {Klopp, David and Erdweg, Sebastian and Pacak, Andr\'{e}},
title = {Object-Oriented Fixpoint Programming with Datalog},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689713},
doi = {10.1145/3689713},
abstract = {Modern usages of Datalog exceed its original design purpose in scale and complexity.
 
In particular, Datalog lacks abstractions for code organization and reuse, making programs hard to maintain.
 
Is it possible to exploit abstractions and design patterns from object-oriented programming (OOP) while retaining a Datalog-like fixpoint semantics?
 
To answer this question, we design a new OOP language called OODL with common OOP features: dynamic object allocation, object identity, dynamic dispatch, and mutation.
 
However, OODL has a Datalog-like fixpoint semantics, such that recursive computations iterate until their result becomes stable.
 
We develop two semantics for OODL: a fixpoint interpreter and a compiler that translates OODL to Datalog.
 
Although the side effects found in OOP (object allocation and mutation) conflict with Datalog's fixpoint semantics, we can mostly resolve these incompatibilities through extensions of OODL.
 
Within fixpoint computations, we employ immutable algebraic data structures (e.g. case classes in Scala), rather than relying on object allocation, and we introduce monotonically mutable data types (mono types) to enable a relaxed form of mutation.
 
Our performance evaluation shows that the interpreter fails to solve fixpoint problems efficiently, whereas the compiled code exploits Datalog's semi-na\"{\i}ve evaluation.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {273},
numpages = {27},
keywords = {Datalog, Fixpoint, objectoriented}
}

@article{10.1145/3689714,
author = {Palmer, Zachary and Filardo, Nathaniel Wesley and Wu, Ke},
title = {Intensional Functions},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689714},
doi = {10.1145/3689714},
abstract = {Functions in functional languages have a single elimination form — application — and cannot be compared, hashed, or subjected to other non-application operations. These operations can be approximated via defunctionalization: functions are replaced with first-order data and calls are replaced with invocations of a dispatch function. Operations such as comparison may then be implemented for these first-order data to approximate e.g. deduplication of continuations in algorithms such as unbounded searches. Unfortunately, this encoding is tedious, imposes a maintenance burden, and obfuscates the affected code.                We introduce an alternative in intensional functions, a language feature which supports the definition of non-application operations in terms of a function’s definition site and closure-captured values. First-order data operations may be defined on intensional functions without burdensome code transformation. We give an operational semantics and type system and prove their formal properties. We further define intensional monads, whose Kleisli arrows are intensional functions, enabling monadic values to be similarly subjected to additional operations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {274},
numpages = {26},
keywords = {closure, continuation, defunctionalization, function, intensional}
}

@software{10.5281/zenodo.13381352,
author = {Palmer, Zachary and Filardo, Nathaniel Wesley and Wu, Ke},
title = {Intensional Functions Virtual Machine Image},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13381352},
abstract = {
    <p>This archive contains a virtual machine image with a precompiled
version of the artifact from the paper Intensional Functions, which
appears in OOPSLA 2024 / PACMPL.</p>

},
keywords = {function, image, intensional, machine, virtual}
}

@article{10.1145/3689715,
author = {Nagy, Shaan and Kim, Jinwoo and Reps, Thomas and D’Antoni, Loris},
title = {Automating Unrealizability Logic: Hoare-Style Proof Synthesis for Infinite Sets of Programs},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689715},
doi = {10.1145/3689715},
abstract = {Automated verification of all members of a (potentially infinite) set of programs has the potential to be useful in program synthesis, as well as in verification of dynamically loaded code, concurrent code, and language properties. Existing techniques for verification of sets of programs are limited in scope and unable to create or use interpretable or reusable information about sets of programs. The consequence is that one cannot learn anything from one verification problem that can be used in another. Unrealizability logic (UL), proposed by Kim et al. as the first Hoare-style proof system to prove properties over sets of programs (defined by a regular tree grammar), presents a theoretical framework that can express and use reusable insight. In particular, UL features nonterminal summaries---inductive facts that characterize recursive nonterminals (analogous to procedure summaries in Hoare logic). In this work, we design the first UL proof synthesis algorithm, implemented as Wuldo. Specifically, we decouple the problem of deciding how to apply UL rules from the problem of synthesizing/checking nonterminal summaries by computing proof structure in a fully syntax-directed fashion. We show that Wuldo, when provided nonterminal summaries, can express and prove verification problems beyond the reach of existing tools, including establishing how infinitely many programs behave on infinitely many inputs. In some cases, Wuldo can even synthesize the necessary nonterminal summaries. Moreover, Wuldo can reuse previously proven nonterminal summaries across verification queries, making verification 1.96 times as fast as when summaries are instead proven from scratch.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {275},
numpages = {27},
keywords = {Unrealizability logic, automated reasoning, infinite sets of programs}
}

@software{10.5281/zenodo.12627576,
author = {Nagy, Shaan and Kim, Jinwoo and Reps, Thomas and D’Antoni, Loris},
title = {Wuldo Unrealizability Logic Proof Synthesizer},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12627576},
abstract = {
    <p>Automated verification of all members of a (potentially infinite) set
of programs has the potential to be useful in program synthesis, as well
as in verification of dynamically loaded code, concurrent code, and
language properties. Existing techniques for verification of sets of
programs are limited in scope and unable to create or use interpretable
or reusable information about sets of programs. The consequence is that
one cannot learn anything from one verification problem that can be used
in another. Unrealizability logic (UL), proposed by Kim et al.&nbsp;as the
first Hoare-style proof system to prove properties over sets of programs
(defined by a regular tree grammar), presents a theoretical framework
that can express and use reusable insight. In particular, UL features
nonterminal summaries—inductive facts that characterize recursive
nonterminals (analogous to procedure summaries in Hoare logic). In this
work, we design the first UL proof synthesis algorithm, implemented as
Wuldo. Specifically, we decouple the problem of deciding how to apply UL
rules from the problem of synthesizing/checking nonterminal summaries by
computing proof structure in a fully syntax-directed fashion. We show
that Wuldo, when provided nonterminal summaries, can express and prove
verification problems beyond the reach of existing tools, including
establishing how infinitely many programs behave on infinitely many
inputs. In some cases, Wuldo can even synthesize the necessary
nonterminal summaries. Moreover, Wuldo can reuse previously proven
nonterminal summaries across verification queries, making verification
1.96 times as fast as when summaries are instead proven from
scratch.</p>

},
keywords = {automated reasoning, infinite sets of programs, Unrealizability logic}
}

@article{10.1145/3689716,
author = {Kang, Chan Gu and Lee, Joonghoon and Oh, Hakjoo},
title = {Statistical Testing of Quantum Programs via Fixed-Point Amplitude Amplification},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689716},
doi = {10.1145/3689716},
abstract = {We present a new technique for accelerating quantum program testing. Given a quantum circuit with an input/output specification, our goal is to check whether executing the program on the input state produces the expected output. In quantum computing, however, it is impossible to directly check the equivalence of the two quantum states. Instead, we rely on statistical testing, which involves repeated program executions, state measurements, and subsequent comparisons with the specified output. To guarantee a high level of assurance, however, this method requires an extensive number of measurements. In this paper, we propose a solution to alleviate this challenge by adapting Fixed-Point Amplitude Amplification (FPAA) for quantum program testing. We formally present our technique, demonstrate its ability to reduce the required number of measurements as well as runtime cost without sacrificing the original statistical guarantee, and showcase its runtime effectiveness through case studies.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {276},
numpages = {25},
keywords = {Quantum Computing, Quantum Programming, Testing, Verification}
}

@software{10.5281/zenodo.13370788,
author = {Kang, Chan Gu and Lee, Joonghoon and Oh, Hakjoo},
title = {Artifact for Paper "Statistical Testing of Quantum Programs via Fixed-Point Amplitude Amplification" in OOPSLA 2024},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13370788},
abstract = {
    <p>This is Zenodo repoistory for Software Artifact of Paper “Statistical
Testing of Quantum Programs via Fixed-Point Amplitude Amplification”
appeared in in OOPSLA 2024.</p>
<p>This repository contains a PDF file of artifact manual and runnable
software artifact. Running the artifact will give :</p>
<ul>
<li>Numerical calculation shown in the Examples in the paper</li>
<li>Validation of depth cost expression (appeared in Section 6 and 7 of
the paper)</li>
<li>Producing Speed-up plots in Case Study (appeared in Section 7 of the
paper)</li>
<li>Implementation of Testing Algorithms</li>
</ul>
<p>For the further details, see the attached artifact manual. For any
questions, contact changukang@korea.ac.kr. The artifact may be continued
through following github repo : https://github.com/kupl/FpaaTestArtifact
.</p>

},
keywords = {Quantum Computing, Quantum Programming, Testing, Verification}
}

@article{10.1145/3689717,
author = {Bowman, William J.},
title = {A Low-Level Look at A-Normal Form},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689717},
doi = {10.1145/3689717},
abstract = {A-normal form (ANF) is a widely studied intermediate form in which local control and data flow is made explicit in syntax, and a normal form in which many programs with equivalent control-flow graphs have a single normal syntactic representation. However, ANF is difficult to implement effectively and, as we formalize, difficult to extend with new lexically scoped constructs such as scoped region-based allocation. The problem, as has often been observed, is that normalization of commuting conversions is hard.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
This traditional view of ANF that normalizing commuting conversions is hard, found in formal models and informed by high-level calculi, is wrong. By studying the low-level intensional aspects of ANF, we can derive a normal form in which normalizing commuting conversion is easy, does not require join points, or code duplication, or renormalization after inlining, and is easily extended with new lexically scoped effects. We formalize the connection between ANF and monadic form and their intensional properties, derive an imperative ANF, and design a compiler pipeline from an untyped -calculus with scoped regions, to monadic form, to a low-level imperative monadic form in which A-normalization is trivial and safe for regions. We prove that any such compiler preserves, or optimizes, stack and memory behaviour compared to ANF. Our formalization reconstructs and systematizes pragmatic choices found in practice, including current production-ready compilers.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The main take-away from this work is that, in general, monadic form should be preferred over ANF, and A-normalization should only be done in a low-level imperative intermediate form. This maximizes the advantages of each form, and avoids all the standard problems with ANF.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {277},
numpages = {27},
keywords = {A-normal form, CPS, Compilers, Intermediate Representation, Monadic Form, Normal Form, Normalization, Optimization}
}

@software{10.5281/zenodo.13376916,
author = {Bowman, William J.},
title = {A Low-Level Look at A-Normal Form (artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13376916},
abstract = {
    <p>This artifact accompanies the paper A low-level look at A-normal form
(https://doi.org/10.1145/3689717). The <code>materials.tar.gz</code>
contain source files, documentation, Docker files, and assorted build
scripts. The file <code>oopsla2024-159-artifact-amd64.qcow2</code>
contains the same files, bundled as a QEMU VM image; instructions for
running it are in the gzip file.</p>

},
keywords = {A-normal form, Compilers, CPS, Intermediate Representation, Monadic Form, Normal Form, Normalization, Optimization}
}

@article{10.1145/3689718,
author = {Zhou, Litao and Wan, Qianyong and Oliveira, Bruno C. d. S.},
title = {Full Iso-Recursive Types},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689718},
doi = {10.1145/3689718},
abstract = {There are two well-known formulations of recursive types: iso-recursive and equi-recursive types. Abadi and Fiore [LICS 1996] have shown that iso- and equi-recursive types have the same expressive power. However, their encoding of equi-recursive types in terms of iso-recursive types requires explicit coercions. These coercions come with significant additional computational overhead, and complicate reasoning about the equivalence of the two formulations of recursive types.  This paper proposes a generalization of iso-recursive types called full iso-recursive types. Full iso-recursive types allow encoding all programs with equi-recursive types without computational overhead. Instead of explicit term coercions, all type transformations are captured by computationally irrelevant casts, which can be erased at runtime without affecting the semantics of the program. Consequently, reasoning about the equivalence between the two approaches can be greatly simplified. We present a calculus called λFiµ, which extends the simply typed lambda calculus (STLC) with full iso-recursive types. The λFiµ calculus is proved to be type sound, and shown to have the same expressive power as a calculus with equi-recursive types. We also extend our results to subtyping, and show that equi-recursive subtyping can be expressed in terms of iso-recursive subtyping with cast operators.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {278},
numpages = {30},
keywords = {Recursive types, Subtyping, Type system}
}

@software{10.5281/zenodo.12669929,
author = {Zhou, Litao and Wan, Qianyong and Oliveira, Bruno C. d. S.},
title = {Artifact for "Full Iso-Recursive Types"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12669929},
abstract = {
    <p>This artifact accompanies the OOPSLA’24 paper titled Full
Iso-recursive Types. It provides the Coq formalization of the proofs
discussed in the paper, including a pre-configured Docker image for easy
verification and the original source code for those who wish to build
the proofs manually. The artifact includes all necessary scripts, proof
files, and dependencies to reproduce the results presented in the paper,
with a focus on the formalization of iso-recursive types and their
extensions.</p>

},
keywords = {Coq, Mechanized Proof, Recursive types, Subtyping, Type system}
}

@article{10.1145/3689719,
author = {Zhang, Chengyu and Su, Zhendong},
title = {SMT2Test: From SMT Formulas to Effective Test Cases},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689719},
doi = {10.1145/3689719},
abstract = {One of the primary challenges in software testing is generating high-quality test inputs and obtaining corresponding test oracles. This paper introduces a novel methodology to mitigate this challenge in testing program verifiers by employing SMT (Satisfiability Modulo Theories) formulas as a universal test case generator. The key idea is to transform SMT formulas into programs and link the satisfiability of the formulas with the safety property of the programs, allowing the satisfiability of the formulas to act as a test oracle for program verifiers. This method was implemented as a framework named SMT2Test, which enables the transformation of SMT formulas into Dafny and C programs. An intermediate representation was designed to augment the flexibility of this framework, streamlining the transformation for other programming languages and fostering modular transformation strategies. We evaluated the effectiveness of SMT2Test by finding defects in two program verifiers: the Dafny verifier and CPAchecker. Utilizing the SMT2Test framework with the SMT formulas from the SMT competition and SMT solver fuzzers, we discovered and reported a total of 14 previously unknown defects in these program verifiers that were not found by previous methods. After reporting, all of them have been confirmed, and 6 defects have been fixed. These findings show the effectiveness of our method and imply its potential application in testing other programming language infrastructures.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {279},
numpages = {24},
keywords = {Program Verification, SMT Solving, Software Testing}
}

@software{10.5281/zenodo.12669638,
author = {Zhang, Chengyu and Su, Zhendong},
title = {SMT2Test: From SMT Formulas to Effective Test Cases},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12669638},
abstract = {
    <p>The main focus of this artifact is to help people 1) validate the
functionality of the bugfinding tool SMT2Test, 2) check the bug
triggering test cases reported in the paper, 3) reproduce the
performance evaluation. This artifact includes the implementation of
SMT2Test, bug-triggering test cases, seed formulas, and scripts for
reproducing the experiments.</p>

},
keywords = {program verification, SMT solving, software testing}
}

@article{10.1145/3689720,
author = {Raad, Azalea and Vanegue, Julien and O’Hearn, Peter},
title = {Non-termination Proving at Scale},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689720},
doi = {10.1145/3689720},
abstract = {Program termination is a classic non-safety property whose falsification cannot in general be witnessed by a finite trace. This makes testing for non-termination challenging, and also a natural target for symbolic proof. Several works in the literature apply non-termination proving to small, self-contained benchmarks, but     it has not been developed for large, real-world projects; as such, despite its allure, non-termination proving has had limited practical impact.     We develop a compositional theory for non-termination proving, paving the way for its scalable application to large codebases.     Discovering non-termination is an under-approximate problem, and we present UNTer, a sound and complete under-approximate logic for proving non-termination. We then extend UNTer with separation logic and develop UNTerSL for heap-manipulating programs, yielding a compositional proof method amenable to automation via under-approximation and bi-abduction.     We extend the Pulse analyser from Meta and develop Pulse∞, an automated, compositional prover for non-termination based on UNTerSL. We have run Pulse∞ on large codebases and libraries, each comprising hundreds of thousands of lines of code, including OpenSSL, libxml2, libxpm and CryptoPP; it discovered several previously-unknown non-termination bugs and have reported them to developers of these libraries.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {280},
numpages = {29},
keywords = {Divergence, incorrectness logic, non-termination, under-approximation}
}

@software{10.5281/zenodo.12637589,
author = {Raad, Azalea and Vanegue, Julien and O’Hearn, Peter},
title = {Non Termination Proving At Scale: Artifact (Pulse Infinite)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12637589},
abstract = {
    <p>This package contains the Pulse-Infinite project source code
distributed as part of the Infer framework as of 7/6/24.</p>
<p>For the most recent version of Pulse-Infinite, please consult:
https://github.com/jvanegue/infer/</p>

},
keywords = {incorrectness, infer, logic, separation, termination}
}

@article{10.1145/3689721,
author = {Liu, Peiming and Root, Alexander J and Xu, Anlun and Li, Yinying and Kjolstad, Fredrik and Bik, Aart J.C.},
title = {Compiler Support for Sparse Tensor Convolutions},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689721},
doi = {10.1145/3689721},
abstract = {This paper extends prior work on sparse tensor algebra compilers to generate asymptotically efficient code for tensor expressions with affine subscript expressions. Our technique enables compiler support for a wide range of sparse computations, including sparse convolutions and pooling that are widely used in ML and graphics applications. We propose an approach that gradually rewrites compound subscript expressions to simple subscript expressions with loops that exploit the sparsity pattern of the input sparse tensors. As a result, the time complexity of the generated kernels is bounded by the number of stored elements and not by the shape of the tensors. Our approach seamlessly integrates into existing frameworks and is compatible with recent advances in compilers for sparse computations, including the flexibility to efficiently handle arbitrary combinations of different sparse tensor formats. The implementation of our algorithm is open source and upstreamed to the MLIR sparse compiler. Experimental results show that our method achieves 19.5x speedup when compared with the state-of-the-art compiler-based method at 99.9\% sparsity. The generated sparse kernels start to outperform dense convolution implementations at about 80\% sparsity.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {281},
numpages = {29},
keywords = {code generation, convolution, iteration graphs, merge lattices, performance, sparse data structures, sparse tensor algebra, sparse tensors}
}

@article{10.1145/3689722,
author = {Legoupil, Maxime and Rousseau, June and Georges, A\"{\i}na Linn and Pichon-Pharabod, Jean and Birkedal, Lars},
title = {Iris-MSWasm: Elucidating and Mechanising the Security Invariants of Memory-Safe WebAssembly},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689722},
doi = {10.1145/3689722},
abstract = {WebAssembly offers coarse-grained encapsulation guarantees via its module system, but does not support fine-grained sharing of its linear memory. MSWasm is a recent proposal which extends WebAssembly with fine-grained memory sharing via handles, a type of capability that guarantees spatial and temporal safety, and thus enables an expressive yet safe style of programming with flexible sharing.
 
 
 
 
 
 
 
In this paper, we formally validate the pen-and-paper design of MSWasm. To do so, we first define MSWasmCert, a mechanisation of MSWasm that makes it a fully-defined, conservative extension of WebAssembly 1.0, including the module system. We then develop Iris-MSWasm, a foundational reasoning framework for MSWasm composed of a separation logic to reason about known code, and a logical relation to reason about unknown, potentially adversarial code. Iris-MSWasm thereby makes explicit a key aspect of the implicit universal contract of MSWasm: robust capability safety. We apply Iris-MSWasm to reason about key use cases of handles, in which the effect of calling an unknown function is bounded by robust capability safety. Iris-MSWasm thus works as a framework to prove complex security properties of MSWasm programs, and provides a foundation to evaluate the language-level guarantees of MSWasm.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {282},
numpages = {29},
keywords = {Capabilities, Encapsulation, Logical Relation, MSWasm, Memory Safety, Wasm, WebAssembly}
}

@software{10.5281/zenodo.13383121,
author = {Legoupil, Maxime and Rousseau, June and Georges, A\"{\i}na Linn and Pichon-Pharabod, Jean and Birkedal, Lars},
title = {Artifact and Appendix of 'Iris-MSWasm: elucidating and mechanising the security invariants of Memory-Safe WebAssembly'},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13383121},
abstract = {
    <p>This is the artifact and appendix of the OOPSLA ‘24 paper
’Iris-MSWasm: elucidating and mechanising the security invariants of
Memory-Safe WebAssembly’. The artifact contains the coq source code as
well as a readme.md file that explains how to build the project and
where to find the different parts of the paper. The appendix is a pdf
that contains figures that were elided in the paper for space
constraints.</p>

},
keywords = {Capabilities, Coq, Encapsulation, Logical Relation, Mechanised Proofs, Memory Safety, MSWasm, Wasm, WebAssembly}
}

@article{10.1145/3689723,
author = {Craaijo, Jos and Verbeek, Freek and Ravindran, Binoy},
title = {libLISA: Instruction Discovery and Analysis on x86-64},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689723},
doi = {10.1145/3689723},
abstract = {Even though heavily researched, a full formal model of the x86-64 instruction set is still not available. We present libLISA, a tool for automated discovery and analysis of the ISA of a CPU. This produces the most extensive formal x86-64 model to date, with over 118000 different instruction groups. The process requires as little human specification as possible: specifically, we do not rely on a human-written (dis)assembler to dictate which instructions are executable on a given CPU, or what their in- and outputs are. The generated model is CPU-specific: behavior that is "undefined" is synthesized for the current machine. Producing models for five different x86-64 machines, we mutually compare them, discover undocumented instructions, and generate instruction sequences that are CPU-specific. Experimental evaluation shows that we enumerate virtually all instructions within scope, that the instructions' semantics are correct w.r.t. existing work, and that we improve existing work by exposing bugs in their handwritten models.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {283},
numpages = {29},
keywords = {instruction enumeration, instruction semantics, synthesis}
}

@software{10.5281/zenodo.13380062,
author = {Craaijo, Jos and Verbeek, Freek and Ravindran, Binoy},
title = {Reproduction package (Docker container) for the OOPSLA 2024 article "libLISA: Instruction Discovery and Analysis on x86-64"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13380062},
abstract = {
    <p>These files accompany the OOPSLA 2024 paper ‘libLISA - Instruction
Discovery and Analysis on x86-64’.</p>
<p>This artifact consists of the following:</p>
<ul>
<li>The source code for libLISA, the tool presented in the paper.</li>
<li>The generated semantics for the five CPU architectures we
analyzed.</li>
<li>The source code for all additional tools that we used to generate
the data in Table 5, 6, 7, 8 and 9.</li>
</ul>
<p>This is all relevant source code and data necessary to reproduce all
results in the paper.</p>
<p>Please see the full guide in artifact-evaluation-guide.7z.</p>

},
keywords = {instruction enumeration, instruction semantics, synthesis}
}

@article{10.1145/3689724,
author = {Liu, Jiangyi and Murphy, Charlie and Grover, Anvay and Johnson, Keith J.C. and Reps, Thomas and D’Antoni, Loris},
title = {Synthesizing Formal Semantics from Executable Interpreters},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689724},
doi = {10.1145/3689724},
abstract = {Program verification and synthesis frameworks that allow one to customize the language in which one is interested typically require the user to provide a formally defined semantics for the language.
 
Because writing a formal semantics can be a daunting and error-prone task, this requirement stands in the way of such frameworks being adopted by non-expert users.
 
We present an algorithm that can automatically synthesize inductively defined syntax-directed semantics when given (i) a grammar describing the syntax of a language and (ii) an executable (closed-box) interpreter for computing the semantics of programs in the language of the grammar.
 
Our algorithm synthesizes the semantics in the form of Constrained-Horn Clauses (CHCs), a natural, extensible, and formal logical framework for specifying inductively defined relations that has recently received widespread adoption in program verification and synthesis.
 
The key innovation of our synthesis algorithm is a Counterexample-Guided Synthesis (CEGIS) approach that breaks the hard problem of synthesizing a set of constrained Horn clauses into small, tractable expression-synthesis problems that can be dispatched to existing SyGuS synthesizers.
 
Our tool Synantic synthesized inductively-defined formal semantics from 14 interpreters for languages used in program-synthesis applications.
 
When synthesizing formal semantics for one of our benchmarks, Synantic unveiled an inconsistency in the semantics computed by the interpreter for a language of regular expressions; fixing the inconsistency resulted in a more efficient semantics and, for some cases, in a 1.2x speedup for a synthesizer solving synthesis problems over such a language.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {284},
numpages = {27},
keywords = {Program Synthesis, SMT, SemGuS, Semantics, SyGuS}
}

@software{10.5281/zenodo.13368062,
author = {Liu, Jiangyi and Murphy, Charlie and Grover, Anvay and Johnson, Keith J.C. and Reps, Thomas and D’Antoni, Loris},
title = {Artifact of paper "Synthesizing Formal Semantics from Executable Interpreters"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13368062},
abstract = {
    <p>This is the artifact of paper “Synthesizing Formal Semantics from
Executable Interpreters”. It contains code for the tool Synantic as well
as benchmarks used to support our claims in the paper.</p>

},
keywords = {program synthesis, semantics, SemGuS, SMT solvers, SyGuS}
}

@article{10.1145/3689725,
author = {Rajani, Vineet and Barthe, Gilles and Garg, Deepak},
title = {A Modal Type Theory of Expected Cost in Higher-Order Probabilistic Programs},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689725},
doi = {10.1145/3689725},
abstract = {The design of online learning algorithms typically aims to optimise the incurred loss or cost, e.g., the number of classification mistakes made by the algorithm. The goal of this paper is to build a type-theoretic framework to prove that a certain algorithm achieves its stated bound on the cost.    Online learning algorithms often rely on randomness, their loss functions are often defined as expectations, precise bounds are often non-polynomial (e.g., logarithmic) and proofs of optimality often rely on potential-based arguments. Accordingly, we present pλ-amor, a type-theoretic graded modal framework for analysing (expected) costs of higher-order probabilistic programs with recursion. pλ-amor is an effect-based framework which uses graded modal types to represent potentials, cost and probability at the type level. It extends prior work (λ-amor) on cost analysis for deterministic programs. We prove pλ-amor sound relative to a Kripke step-indexed model which relates potentials with probabilistic coupling. We use pλ-amor to prove cost bounds of several examples from the online machine learning literature. Finally, we describe an extension of pλ-amor with a graded comonad and describe the relationship between the different modalities.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {285},
numpages = {26},
keywords = {expected cost, graded modal types, higher-order programs, potentials, probabilistic coupling}
}

@article{10.1145/3689726,
author = {Lobo-Vesga, Elisabet and Russo, Alejandro and Gaboardi, Marco and Corti\~{n}as, Carlos Tom\'{e}},
title = {Sensitivity by Parametricity},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689726},
doi = {10.1145/3689726},
abstract = {The work of Fuzz has pioneered the use of functional programming languages where types allow reasoning about the sensitivity of programs. Fuzz and subsequent work (e.g., DFuzz and Duet) use advanced technical devices like linear types, modal types, and partial evaluation. These features usually require the design of a new programming language from scratch—a significant task on its own! While these features are part of the classical toolbox of programming languages, they are often unfamiliar to non-experts in this field. Fortunately, recent studies (e.g., Solo) have shown that linear and complex types in general, are not strictly needed for the task of determining programs’ sensitivity since this can be achieved by annotating base types with static sensitivity information. In this work, we take a different approach. We propose to enrich base types with information about the metric relation between values, and we present the novel idea of applying parametricity to derive direct proofs for the sensitivity of functions. A direct consequence of our result is that calculating and proving the sensitivity of functions is reduced to simply type-checking in a programming language with support for polymorphism and type-level naturals. We formalize our main result in a calculus, prove its soundness, and implement a software library in the programming language Haskell–where we reason about the sensitivity of canonical examples. We show that the simplicity of our approach allows us to exploit the type inference of the host language to support a limited form of sensitivity inference. Furthermore, we extend the language with a privacy monad to showcase how our library can be used in practical scenarios such as the implementation of differentially private programs, where the privacy guarantees depend on the sensitivity of user-defined functions. Our library, called Spar, is implemented in less than 500 lines of code.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {286},
numpages = {27},
keywords = {Haskell, differential privacy, functional programming languages, sensitivity}
}

@software{10.5281/zenodo.13622515,
author = {Lobo-Vesga, Elisabet and Russo, Alejandro and Gaboardi, Marco and Corti\~{n}as, Carlos Tom\'{e}},
title = {Paper Artifact: Sensitivity by Parametricity},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13622515},
abstract = {
    <p>The official artifact accompanying the OOPSLA 2024 paper “Sensitivity
by Parametricity” for the Spar library. The paper explores the use of
parametricity to perform sensitivity analysis on user-defined functions,
additionally, it introduces a Haskell library called Spar that
implements this technique. Spar encodes value distances as type-level
naturals, proving the sensitivity of a function is reduced to
type-checking! This artifact is distributed as a Docker image where the
Spar library is built and ready to use. Instructions for building the
image are provided in README.</p>

},
keywords = {differential privacy, functional programming languages, Haskell, sensitivity}
}

@article{10.1145/3689727,
author = {Geeson, Luke and Brotherston, James and Dijkstra, Wilco and Donaldson, Alastair F. and Smith, Lee and Sorensen, Tyler and Wickerson, John},
title = {Mix Testing: Specifying and Testing ABI Compatibility of C/C++ Atomics Implementations},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689727},
doi = {10.1145/3689727},
abstract = {The correctness of complex software depends on the correctness of both the source code and the compilers that generate corresponding binary code. Compilers must do more than preserve the semantics of a single source file: they must ensure that generated binaries can be composed with other binaries to form a final executable. The compatibility of composition is ensured using an Application Binary Interface (ABI), which specifies details of calling conventions, exception handling, and so on. Unfortunately, there are no official ABIs for concurrent programs, so different atomics mappings, although correct in isolation, may induce bugs when composed. Indeed, today, mixing binaries generated by different compilers can lead to an erroneous resulting binary.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We present mix testing: a new technique designed to find compiler bugs when the instructions of a C/C++ test are separately compiled for multiple compatible architectures and then mixed together. We define a class of compiler bugs, coined mixing bugs, that arise when parts of a program are compiled separately using different mappings from C/C++ atomic operations to assembly sequences. To demonstrate the generality of mix testing, we have designed and implemented a tool, atomic-mixer, which we have used: (a) to reproduce one existing non-mixing bug that state-of-the-art concurrency testing tools are limited to being able to find (showing that atomic-mixer at least meets the capabilities of these tools), and (b) to find four previously-unknown mixing bugs in LLVM and GCC, and one prospective mixing bug in mappings proposed for the Java Virtual Machine. Lastly, we have worked with engineers at Arm to specify, for the first time, an atomics ABI for Armv8, and have used atomic-mixer to validate the LLVM and GCC compilers against it.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {287},
numpages = {26},
keywords = {Compiler Testing, Concurrency, Interoperability}
}

@software{10.5281/zenodo.13625822,
author = {Geeson, Luke and Brotherston, James and Dijkstra, Wilco and Donaldson, Alastair F. and Smith, Lee and Sorensen, Tyler and Wickerson, John},
title = {Artifact for "Mix Testing: Specifying and Testing ABI Compatibility Of C/C++ Atomics Implementations"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13625822},
abstract = {
    <p>This is the artifact accompanying our paper “Mix Testing: Specifying
and Testing ABI Compatibility Of C/C++ Atomics Implementations”,
conditionally accepted for publication at OOPSLA2024</p>
<p>The artifact consists of scripts to reproduce the figures in the
paper paper. We aim for all badges. For comments please contact
luke.geeson@cs.ucl.ac.uk.</p>

},
keywords = {Compiler Testing, Concurrency, Interoperability}
}

@article{10.1145/3689728,
author = {Blinn, Andrew and Li, Xiang and Kim, June Hyung and Omar, Cyrus},
title = {Statically Contextualizing Large Language Models with Typed Holes},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689728},
doi = {10.1145/3689728},
abstract = {Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate code context, particularly when working with definitions that are neither in the training data nor near the cursor. This paper demonstrates that tighter integration with the type and binding structure of the programming language in use, as exposed by its language server, can help address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server is able to identify the type and typing context of the hole that the programmer is filling, with Hazel's total syntax and type error correction ensuring that a meaningful program sketch is available whenever the developer requests a completion. This allows the system to prompt the LLM with codebase-wide contextual information that is not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server, which provides error localization and error messages. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications with accompanying unit tests that have been written from scratch to avoid data contamination, and that can easily be ported to new languages because they do not have large external library dependencies. These applications serve as challenge problems due to their extensive reliance on application-specific data structures. Through an ablation study, we examine the impact of contextualization with type definitions, function headers, and errors messages, individually and in combination. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel, a low-resource language, we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource mainstream languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {288},
numpages = {31},
keywords = {Large Language Models, Program Repair, Program Synthesis}
}

@software{10.5281/zenodo.12669479,
author = {Blinn, Andrew and Li, Xiang and Kim, June Hyung and Omar, Cyrus},
title = {Artifact for Statically Contextualizing Large Language Models with Typed Holes},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12669479},
abstract = {
    <p>Artifact for the paper ‘Artifact for Statically Contextualizing Large
Language Models with Typed Holes’, to be published at OOPSLA2024. The
artifact consists of raw data from the experiments, the testing harness
used, the source for the Hazel Editor and Langauge Server, and a copy of
the StarCoder2 LLM weights. The archive is password protected to prevent
the benchmark data set from being automatically scrapped; see the Zenodo
description for the password.</p>

},
keywords = {Large Language Models, Program Synthesis, Programming Languages, Types}
}

@article{10.1145/3689729,
author = {Lin, Zhengyao and Gancher, Joshua and Parno, Bryan},
title = {FlowCert: Translation Validation for Asynchronous Dataflow via Dynamic Fractional Permissions},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689729},
doi = {10.1145/3689729},
abstract = {Coarse-grained reconfigurable arrays (CGRAs) have gained attention in recent years due to their 
 
 
 
 
 
 
 
promising power efficiency compared to traditional von Neumann architectures.
 
 
 
 
 
 
 
To program these architectures using ordinary languages such as C, 
 
 
 
 
 
 
 
a dataflow compiler must transform the original sequential, imperative program
 
 
 
 
 
 
 
into an equivalent dataflow graph, composed of dataflow operators running
 
 
 
 
 
 
 
in parallel.
 
 
 
 
 
 
 
This transformation is challenging since the asynchronous nature of dataflow
 
 
 
 
 
 
 
graphs allows out-of-order execution of operators, leading to behaviors not present in the original imperative programs.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
We address this challenge by developing a translation validation technique for dataflow compilers
 
 
 
 
 
 
 
to ensure that the dataflow program has the same behavior as the original imperative program 
 
 
 
 
 
 
 
on all possible inputs and schedules of execution.
 
 
 
 
 
 
 
We apply this method to a state-of-the-art dataflow compiler targeting the RipTide CGRA architecture.
 
 
 
 
 
 
 
Our tool uncovers 8 compiler bugs where the compiler outputs incorrect dataflow
 
 
 
 
 
 
 
graphs, including a data race that is otherwise hard to discover via testing.
 
 
 
 
 
 
 
After repairing these bugs, our tool verifies the correct compilation of all programs in the RipTide benchmark suite.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {289},
numpages = {28},
keywords = {Asynchronous Dataflow, Coarse-Grained Reconfigurable Arrays, Translation Validation}
}

@software{10.5281/zenodo.12552491,
author = {Lin, Zhengyao and Gancher, Joshua and Parno, Bryan},
title = {FlowCert: Translation Validation for Asynchronous Dataflow Programs via Dynamic Fractional Permissions},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12552491},
abstract = {
    <p>Implementation of FlowCert and evaluation data packaged in a Docker
image.</p>

},
keywords = {Asynchronous Dataflow, Coarse-Grained Reconfigurable Arrays, Translation Validation}
}

@article{10.1145/3689730,
author = {Dias, Adhitha and Anderson, Logan and Sundararajah, Kirshanthan and Pelenitsyn, Artem and Kulkarni, Milind},
title = {SparseAuto: An Auto-scheduler for Sparse Tensor Computations using Recursive Loop Nest Restructuring},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689730},
doi = {10.1145/3689730},
abstract = {Automated code generation and performance enhancements for sparse tensor algebra have become essential in many real-world applications, such as quantum computing, physical simulations, computational chemistry, and machine learning. General sparse tensor algebra compilers are not always versatile enough to generate asymptotically optimal code for sparse tensor contractions. This paper shows how to generate asymptotically better schedules for complex sparse tensor expressions using kernel fission and fusion. We present generalized loop restructuring transformations to reduce asymptotic time complexity and memory footprint.
 
 
 
Furthermore, we present an auto-scheduler that uses a partially ordered set (poset)-based cost model that uses both time and auxiliary memory complexities to prune the search space of schedules. In addition, we highlight the use of Satisfiability Module Theory (SMT) solvers in sparse auto-schedulers to approximate the Pareto frontier of better schedules to the smallest number of possible schedules, with user-defined constraints available at compile-time. Finally, we show that our auto-scheduler can select better-performing schedules and generate code for them. Our results show that the auto-scheduler provided schedules achieve orders-of-magnitude speedup compared to the code generated by the Tensor Algebra Compiler (TACO) for several computations on different real-world tensors.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {290},
numpages = {30},
keywords = {Asymptotic Analysis, Automatic Scheduling, Fusion, Loop Transformations, Sparse Tensor Algebra}
}

@software{10.5281/zenodo.12764370,
author = {Dias, Adhitha and Anderson, Logan and Sundararajah, Kirshanthan and Pelenitsyn, Artem and Kulkarni, Milind},
title = {Reproduction package of article "SparseAuto: An Auto-Scheduler for Sparse Tensor Computations Using Recursive Loop Nest Restructuring"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12764370},
abstract = {
    <p>The artifact contains a self contained docker image with the dataset
and the software package that was used to generate the graphs in the
paper. The artifact also contains the source files without the docker
image. The artifact contains two main directories containing the
auto-scheduler (written in Python) described in Section 06 in the paper,
and the language extension to SparseLNR/TACO described in the Section 05
in the paper.</p>

},
keywords = {Asymptotic Analysis., Automatic Scheduling, Fusion, Loop Transformations, Sparse Tensor Algebra}
}

@article{10.1145/3689731,
author = {Jeon, Seungmin and Cho, Kyeongmin and Kang, Chan Gu and Lee, Janggun and Oh, Hakjoo and Kang, Jeehoon},
title = {Quantum Probabilistic Model Checking for Time-Bounded Properties},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689731},
doi = {10.1145/3689731},
abstract = {Probabilistic model checking (PMC) is a verification technique for analyzing the properties of probabilistic systems. However, existing techniques face challenges in verifying large systems with high accuracy. PMC struggles with state explosion, where the number of states grows exponentially with the size of the system, making large system verification infeasible. While statistical model checking (SMC) avoids PMC’s state explosion problem by using a simulation approach, it suffers from runtime explosion, requiring numerous samples for high accuracy. 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
To address these limitations in verifying large systems with high accuracy, we present quantum probabilistic model checking (QPMC), the first method leveraging quantum computing for PMC with respect to time-bounded properties. QPMC addresses state explosion by encoding PMC problems into quantum circuits that superpose states within qubits. Additionally, QPMC resolves runtime explosion through Quantum Amplitude Estimation, efficiently estimating the probabilities of specified properties. We prove that QPMC correctly solves PMC problems and achieves a quadratic speedup in time complexity compared to SMC.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {291},
numpages = {31},
keywords = {bounded reachability, probabilistic model checking, quantum computing}
}

@software{10.5281/zenodo.13377564,
author = {Jeon, Seungmin and Cho, Kyeongmin and Kang, Chan Gu and Lee, Janggun and Oh, Hakjoo and Kang, Jeehoon},
title = {Artifact for "Quantum Probabilistic Model Checking for Time-Bounded Properties"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13377564},
abstract = {
    <p>This is the artifact for OOPSLA 2024 paper: “Quantum Probabilistic
Model Checking for Time-Bounded Properties”.</p>
<p>This artifact includes the following files: - qpmc.zip: Contains the
implementation and evaluation program used to generate the results
presented in the paper. - qpmc-evaluation-results.zip: Includes the
output files generated by the evaluation program.</p>
<p>For detailed information about this artifact, please refer to the
README.md file within the qpmc.zip archive. The artifact also includes
the full paper with appendices (paper-full.pdf).</p>

},
keywords = {bounded reachability, probabilistic model checking, quantum computing}
}

@article{10.1145/3689732,
author = {Somers, Thomas and Krebbers, Robbert},
title = {Verified Lock-Free Session Channels with Linking},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689732},
doi = {10.1145/3689732},
abstract = {Type systems and program logics based on session types provide powerful high-level reasoning principles for message-passing concurrency. Modern versions employ bidirectional session channels that (1) are asynchronous so that send operations do not block, (2) have buffers in both directions so that both parties can send messages in parallel, and (3) feature a link operation (also called forward) to concisely write programs in process style. These features complicate a low-level lock-free implementation of channels and therefore increase the gap between the meta theory of prior work—which is verified w.r.t. a high-level semantics of channels (e.g., π-calculus)—and the code that runs on an actual computer.  We address this problem by verifying a low-level lock-free implementation of session channels w.r.t. a high-level specification based on session types. We carry out our verification in a layered manner by employing the Iris framework for concurrent separation logic. We start with an abstract specification of (unidirectional) queues—of which we provide a linked-list and array-segment based implementation—and gradually build up to session channels with all of the aforementioned features. To make a layered verification possible we develop two logical abstractions—queues with ghost linking and pairing invariants—to reason about the atomicity and changing endpoints due to linking, respectively. All our results are mechanized in the Coq proof assistant.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {292},
numpages = {30},
keywords = {Coq, Iris, Message passing, separation logic, session types}
}

@software{10.5281/zenodo.13599952,
author = {Somers, Thomas and Krebbers, Robbert},
title = {Artifact of 'Verified Lock-Free Session Channels with Linking'},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13599952},
abstract = {
    <p>This artifact contains the Coq mechanization of the OOPSLA 2024
paper: ‘Verified Lock-Free Session Channels with Linking’. It contains
the source code, instructions for evaluating the artifact, and the
correspondence between the artifact and paper.</p>

},
keywords = {Coq, Iris, Message passing, separation logic, session types}
}

@article{10.1145/3689733,
author = {Takashima, Yoshiki and Cho, Chanhee and Martins, Ruben and Jia, Limin and P\u{a}s\u{a}reanu, Corina S.},
title = {Crabtree: Rust API Test Synthesis Guided by Coverage and Type},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689733},
doi = {10.1145/3689733},
abstract = {Rust type system constrains pointer operations, preventing bugs such        as use-after-free. However, these constraints may be too strict for        programming tasks such as implementing cyclic data structures. For        such tasks, programmers can temporarily suspend checks using the        unsafe keyword. Rust libraries wrap unsafe code blocks        and expose higher-level APIs. They need to be extensively tested to        uncover memory-safety bugs that can only be triggered by unexpected        API call sequences or inputs. While prior works have attempted to        automatically test Rust library APIs, they fail to test APIs with        common Rust features, such as polymorphism, traits, and higher-order        functions, or they have scalability issues and can only generate tests        for a small number of combined APIs.                We propose Crabtree, a testing tool for Rust library APIs that can        automatically synthesize test cases with native support for Rust        traits and higher-order functions. Our tool improves upon the test synthesis algorithms of prior        works by combining synthesis and fuzzing through a coverage- and type-guided        search algorithm that intelligently grows test programs and input        corpus towards testing more code. To the best of our knowledge, our tool        is the first to generate well-typed tests for libraries that make use        of higher-order trait functions. Evaluation of Crabtree on 30        libraries found four previously unreported memory-safety bugs, all of which were accepted by the respective authors.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {293},
numpages = {30},
keywords = {API testing, Rust, fuzzing, program synthesis}
}

@software{10.5281/zenodo.13626235,
author = {Takashima, Yoshiki and Cho, Chanhee and Martins, Ruben and Jia, Limin and P\u{a}s\u{a}reanu, Corina S.},
title = {Artifact Package for Paper "Crabtree: Rust API Test Synthesis Guided by Coverage and Type"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13626235},
abstract = {
    <p>Download the tarball and use the README inside.</p>

},
keywords = {API testing, fuzzing, program synthesis, Rust}
}

@article{10.1145/3689734,
author = {Ye, Wenjia and Oliveira, Bruno C. d. S. and Toro, Mat\'{\i}as},
title = {Merging Gradual Typing},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689734},
doi = {10.1145/3689734},
abstract = {Programming language mechanisms with a type-directed semantics are nowadays common and widely used. Such mechanisms include gradual typing, type classes, implicits and intersection types with a merge operator. While sharing common challenges in their design and having complementary strengths, type-directed mechanisms have been mostly independently studied.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
This paper studies a new calculus, called λM⋆, which combines two type-directed mechanisms: gradual typing and a merge operator based on intersection types. Gradual typing enables a smooth transition between dynamically and statically typed code, and is available in languages such as TypeScript or Flow. The merge operator generalizes record concatenation to allow merges of values of any two types. Recent work has shown that the merge operator enables modelling expressive OOP features like first-class traits/classes and dynamic inheritance with static type-checking. These features are not found in mainstream statically typed OOP languages, but they can be found in dynamically or gradually typed languages such as JavaScript or TypeScript. In λM⋆, by exploiting the complementary strengths of gradual typing and the merge operator, we obtain a foundation for modelling gradually typed languages with both first-class classes and dynamic inheritance. We study a static variant of λM⋆ (called λM); prove the type-soundness of λM⋆; show that λM⋆ can encode gradual rows and all well-typed terms in the GTFL≲ calculus; and show that λM⋆ satisfies gradual typing criteria. The dynamic gradual guarantee (DGG) is challenging due to the possibility of ambiguity errors. We establish a variant of the DGG using a semantic notion of precision based on a step-indexed logical relation.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {294},
numpages = {29},
keywords = {Bidirectional Typing, Gradual Typing, Merge Operator, Type-Directed Semantics}
}

@software{10.5281/zenodo.13378311,
author = {Ye, Wenjia and Oliveira, Bruno C. d. S. and Toro, Mat\'{\i}as},
title = {Merging Gradual Typing (Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13378311},
abstract = {
    <p>This is the artifact of OOPSLA2024 research paper: Merging Gradual
Typing.</p>

},
keywords = {Bidirectional Typing, Gradual Typing, Merge Operator, Type-Directed Semantics}
}

@article{10.1145/3689735,
author = {Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Freeman, Anders and Anderson, Carolyn Jane and Feldman, Molly Q and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun},
title = {Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689735},
doi = {10.1145/3689735},
abstract = {Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as building blocks for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming language. Code LLMs produce impressive results on high-resource programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages that have limited training data available (e.g., OCaml, Racket, and several others).
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach, called MultiPL-T, generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. MultiPL-T translates training data from high-resource languages into training data for low-resource languages in the following way. 1) We use a Code LLM to synthesize unit tests for commented code from a high-resource source language, filtering out faulty tests and code with low test coverage. 2) We use a Code LLM to translate the code from the high-resource source language to a target low-resource language. This gives us a corpus of candidate training data in the target language, but many of these translations are wrong. 3) We use a lightweight compiler to compile the test cases generated in (1) from the source language to the target language, which allows us to filter our obviously wrong translations. The result is a training corpus in the target low-resource language where all items have been validated with test cases. We apply this approach to generate tens of thousands of new, validated training items for five low-resource languages: Julia, Lua, OCaml, R, and Racket, using Python as the source high-resource language. Furthermore, we use an open Code LLM (StarCoderBase) with open training data (The Stack), which allows us to decontaminate benchmarks, train models without violating licenses, and run experiments that could not otherwise be done.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Using datasets generated with MultiPL-T, we present fine-tuned versions of StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket that outperform other fine-tunes of these base models on the natural language to code task. We also present Racket fine-tunes for two very recent models, DeepSeek Coder and StarCoder2, to show that MultiPL-T continues to outperform other fine-tuning approaches for low-resource languages. The MultiPL-T approach is easy to apply to new languages, and is significantly more efficient and effective than alternatives such as training longer.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {295},
numpages = {32},
keywords = {Large Language Models trained on Code}
}

@software{10.5281/zenodo.12453932,
author = {Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Freeman, Anders and Anderson, Carolyn Jane and Feldman, Molly Q and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun},
title = {Artifact: Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12453932},
abstract = {
    <p>The artifact contains the code to fully reproduce the results in the
paper.</p>

},
keywords = {large language models trained on code}
}

@article{10.1145/3689736,
author = {Yang, Chenyuan and Deng, Yinlin and Lu, Runyu and Yao, Jiayi and Liu, Jiawei and Jabbarvand, Reyhaneh and Zhang, Lingming},
title = {WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689736},
doi = {10.1145/3689736},
abstract = {Compiler correctness is crucial, as miscompilation can falsify program behaviors, leading to serious consequences over the software supply chain. In the literature, fuzzing has been extensively studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates test programs without sufficient understanding of internal compiler behaviors. As such, they often fail to construct test programs to exercise intricate optimizations. Meanwhile, traditional white-box techniques, such as symbolic execution, are computationally inapplicable to the giant codebase of compiler systems. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks and even have achieved state-of-the-art performance in black-box fuzzing. Nonetheless, guiding LLMs with compiler source-code information remains a missing piece of research in compiler testing.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization, with a spotlight on detecting deep logic bugs in the emerging deep learning (DL) compilers. WhiteFox adopts a multi-agent framework: (i) an LLM-based analysis agent examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; (ii) an LLM-based generation agent produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are also used as feedback to further enhance the test generation prompt on the fly. Our evaluation on the three most popular DL compilers (i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows that WhiteFox can generate high-quality test programs to exercise deep optimizations requiring intricate conditions, practicing up to 8 times more optimizations than state-of-the-art fuzzers. To date, WhiteFox has found in total 101 bugs for the compilers under test, with 92 confirmed as previously unknown and 70 already fixed. Notably, WhiteFox has been recently acknowledged by the PyTorch team, and is in the process of being incorporated into its development workflow. Finally, beyond DL compilers, WhiteFox can also be adapted for compilers in different domains, such as LLVM, where WhiteFox has already found multiple bugs.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {296},
numpages = {27},
keywords = {Code Analysis, Fuzzing, Large Language Models, White-box Testing}
}

@article{10.1145/3689737,
author = {Jungmair, Michael and Engelke, Alexis and Giceva, Jana},
title = {HiPy: Extracting High-Level Semantics from Python Code for Data Processing},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689737},
doi = {10.1145/3689737},
abstract = {Data science workloads frequently include Python code, but Python's dynamic nature makes efficient execution hard. Traditional approaches either treat Python as a black box, missing out on optimization potential, or are limited to a narrow domain. However, a deep and efficient integration of user-defined Python code into data processing systems requires extracting the semantics of the entire Python code.
 

 
In this paper, we propose a novel approach for extracting the high-level semantics by transforming general Python functions into program generators that generate a statically-typed IR when executed.
 
The extracted IR then allows for high-level, domain-specific optimizations and the generation of efficient C++ code. With our prototype implementation, HiPy, we achieve single-threaded speedups of 2-20x for many workloads. Furthermore, HiPy is also capable of accelerating Python code in other domains like numerical data, where it can sometimes even outperform specialized compilers.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {297},
numpages = {27},
keywords = {Data Processing, High-Level Optimizations, Program Generation, Python}
}

@software{10.5281/zenodo.13323059,
author = {Jungmair, Michael and Engelke, Alexis and Giceva, Jana},
title = {Artifact for "HiPy: Extracting High-Level Semantics From Python Code For Data Processing"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13323059},
abstract = {
    <p>This is the artifact to the paper “HiPy: Extracting High-Level
Semantics From Python Code For Data Processing” which provides the
source code of HiPy as well as tooling, benchmarks, and Docker images to
reproduce the experiments shown in the paper.</p>

},
keywords = {Data Processing, High-Level Optimizations, Program Generation, Python}
}

@article{10.1145/3689738,
author = {D’Antoni, Loris and Ding, Shuo and Goel, Amit and Ramesh, Mathangi and Rungta, Neha and Sung, Chungha},
title = {Automatically Reducing Privilege for Access Control Policies},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689738},
doi = {10.1145/3689738},
abstract = {Access control policies are programs used to secure cloud resources. These polices should only grant the necessary permissions that a given application needs. However, it is challenging to write and maintain policies as applications and their required permissions change over time.
 
 
 
In this paper, we focus on the Amazon Web Services (AWS) IAM policy language and present an approach that, given a policy, synthesizes a modified policy that is more restrictive and better abides to the principle of least privilege. Our approach looks at the actual access history (e.g., access logs) used by an application and computes the least permissive local modification of the user-given policy that still provides the same permissions that were observed in the access history. We treat the problem of computing the least permissive policy as a generalization problem in a lattice of possible policies (i.e., the set of local modifications). We show that our synthesis algorithm comes with correctness guarantees and is amendable to an efficient implementation that is easy to parallelize. We implement our algorithm in a tool IAM-PolicyRefiner and evaluate it on policies attached to AWS roles with access logs. For each role, IAM-PolicyRefiner can compute easy-to-inspect refined policies in less than 1 minute, and the refined policies do not overfit to the requests in the log---i.e., the policies also allow requests in a left-out test set of requests.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {298},
numpages = {28},
keywords = {Access Control, Authorization, Formal Methods}
}

@article{10.1145/3689739,
author = {Yang, Ziteng and Shirako, Jun and Sarkar, Vivek},
title = {Fully Verified Instruction Scheduling},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689739},
doi = {10.1145/3689739},
abstract = {CompCert project, the state-of-the-art compiler that achieves the first end-to-end formally verified C compiler, does not support fully verified instruction scheduling. Instead, existing research that works on such topics only implements translation validation. This means they do not have direct formal proof that the scheduling algorithm is correct, but only a posterior validation to check each compiling case. Using such a method, CompCert accepts a valid C program and compiles correctly only when the untrusted scheduler generates a correct result. However, it does not guarantee the complete correctness of the scheduler. It also causes compile-time validation overhead in the view of runtime performance.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In this work, we present the first achievement in developing a mechanized library for fully verified instruction scheduling while keeping the proof workload acceptably lightweight. The idea to reduce the proof length is to exploit a simple property that the topological reordering of a topological sorted list is equal to a sequence of swapping adjacent unordered elements. Together with the transitivity of semantic simulation relation, the only burden will become proving the semantic preservation of a transition that only swaps two adjacent independent instructions inside one block. After successfully proving this result, proving the correctness of any new instruction scheduling algorithm only requires proof that it preserved the syntax-level dependence among instructions, instead of reasoning about semantics details every time. We implemented a mechanized library of such methods in the Coq proof assistant based on CompCert's library as a framework and used the list scheduling algorithm as a case study to show the correctness can be formally proved using our theory.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We show that with our method that abstracts away the semantics details, it is flexible to implement any scheduler that reorders instructions with little extra proof burden. Our scheduler in the case study also abstracts away the outside scheduling heuristic as a universal parameter so it is flexible to modify without touching any correctness proof.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {299},
numpages = {26},
keywords = {CompCert, Compiler Verification, Coq Proof Assistant, Instruction-level Parallelism}
}

@software{10.5281/zenodo.13625830,
author = {Yang, Ziteng and Shirako, Jun and Sarkar, Vivek},
title = {Artifact for "Fully Verified Instruction Scheduling"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13625830},
abstract = {
    <p>This is the artifact of our project in our paper Fully Verified
Instruction Scheduling: a lightweight and flexible approach.</p>
<p>The artifacts consists of two parts: mechanized proofs and
performance experiments. Evaluating the mechanized proofs only requires
software dependencies on Linux machine and the use of proof assistant
Coq. Evaluating the experiments requires an in-order Risc-V
hardware.</p>
<p>The documentations contains step-by-step building guides and a
detailed paper-to-artifact correspondence guide that matches every
lemma/theorems in our submitted paper with the mechanized proofs.</p>

},
keywords = {CompCert, Compiler Verification, Coq Proof Assistant, Instruction-level
Parallelism}
}

@article{10.1145/3689740,
author = {Zhang, Linpeng and Zilberstein, Noam and Kaminski, Benjamin Lucien and Silva, Alexandra},
title = {Quantitative Weakest Hyper Pre: Unifying Correctness and Incorrectness Hyperproperties via Predicate Transformers},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689740},
doi = {10.1145/3689740},
abstract = {We present a novel weakest pre calculus for reasoning about quantitative hyperproperties over nondeterministic and probabilistic programs. Whereas existing calculi allow reasoning about the expected value that a quantity assumes after program termination from a single initial state, we do so for initial sets of states or initial probability distributions.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We thus (i) obtain a weakest pre calculus for hyper Hoare logic and (ii) enable reasoning about so-called hyperquantities which include expected values but also quantities (e.g. variance) out of scope of previous work. As a byproduct, we obtain a novel strongest post for weighted programs that extends both existing strongest and strongest liberal post calculi. Our framework reveals novel dualities between forward and backward transformers, correctness and incorrectness, as well as nontermination and unreachability.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {300},
numpages = {29},
keywords = {hyperproperties, nondeterminism, probabilistic verification, quantitative software verification, strongest postcondition, weakest precondition}
}

@article{10.1145/3689741,
author = {Chen, Ethan and Chang, Jiwon and Zhu, Yuhao},
title = {CoolerSpace: A Language for Physically Correct and Computationally Efficient Color Programming},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689741},
doi = {10.1145/3689741},
abstract = {Color programmers manipulate lights, materials, and the resulting colors from light-material interactions. Existing libraries for color programming provide only a thin layer of abstraction around matrix operations. Color programs are, thus, vulnerable to bugs arising from mathematically permissible but physically meaningless matrix computations. Correct implementations are difficult to write and optimize. We introduce CoolerSpace to facilitate physically correct and computationally efficient color programming. CoolerSpace raises the level of abstraction of color programming by allowing programmers to focus on describing the logic of color physics. Correctness and efficiency are handled by CoolerSpace. The type system in CoolerSpace assigns physical meaning and dimensions to user-defined objects. The typing rules permit only legal computations informed by color physics and perception. Along with type checking, CoolerSpace also generates performance-optimized programs using equality saturation. CoolerSpace is implemented as a Python library and compiles to ONNX, a common intermediate representation for tensor computations. CoolerSpace not only prevents common errors in color programming, but also does so without run-time overhead: even unoptimized CoolerSpace programs out-perform existing Python-based color programming systems by up to 5.7 times; our optimizations provide up to an additional 1.4 times speed-up.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {301},
numpages = {30},
keywords = {color science, language design, type systems}
}

@software{10.5281/zenodo.13621721,
author = {Chen, Ethan and Chang, Jiwon and Zhu, Yuhao},
title = {CoolerSpace Artifacts},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13621721},
abstract = {
    <p>This artifact contains four repositories.</p>
<p>The first repository, CoolerSpace, is the main repository for the
CoolerSpace library.</p>
<p>The second repository, eggwrap, is a wrapper for the egg equality
saturation framework.</p>
<p>The third repository, onneggs, is an optimization tool for
CoolerSpace programs.</p>
<p>The fourth repository, CoolerSpaceBenchmarker, contains a
benchmarking suite for CoolerSpace and several example CoolerSpace
programs.</p>
<p>Each repository has a readme.md file with more information.</p>

},
keywords = {domain-specific languages, graphics}
}

@article{10.1145/3689742,
author = {Liu, Si and Gu, Long and Wei, Hengfeng and Basin, David},
title = {Plume: Efficient and Complete Black-Box Checking of Weak Isolation Levels},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689742},
doi = {10.1145/3689742},
abstract = {Modern databases embrace weak isolation levels to cater for highly available transactions. However, weak isolation bugs have recently manifested in many production databases. This raises the concern of whether database implementations actually deliver their promised isolation guarantees in practice. In this paper we present Plume, the first efficient, complete, black-box checker for weak isolation levels. Plume builds on modular, fine-grained, transactional anomalous patterns, with which we establish sound and complete characterizations of representative weak isolation levels, including read committed, read atomicity, and transactional causal consistency. Plume leverages a novel combination of two techniques, vectors and tree clocks, to accelerate isolation checking. Our extensive assessment shows that Plume can reproduce all known violations in a large collection of anomalous database execution histories, detect new isolation bugs in three production databases along with informative counterexamples, find more weak isolation anomalies than the state-of-the-art checkers, and efficiently validate isolation guarantees under a wide variety of workloads.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {302},
numpages = {29},
keywords = {black-box testing, formal specification, weak isolation levels}
}

@software{10.5281/zenodo.13618225,
author = {Liu, Si and Gu, Long and Wei, Hengfeng and Basin, David},
title = {Artifact for "Plume: Efficient and Complete Black-Box Checking of Weak Isolation Levels"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13618225},
abstract = {
    <p>This artifact provides a Docker image that includes all the necessary
tools and experimental data used in the paper titled “Plume: Efficient
and Complete Black-Box Checking of Weak Isolation Levels.” The Docker
image ensures a fully configured environment to facilitate the
reproducibility of the results presented in the paper.</p>
<p>The Docker image includes:</p>
<ul>
<li>Comparison Tools: A suite of tools required for the
experiments.</li>
<li>Experimental Data: Complete datasets used in the experiments.</li>
<li>Reproduction Scripts: Scripts for reproducing the experimental
results.</li>
</ul>

},
keywords = {black-box testing, formal specification, weak isolation levels}
}

@article{10.1145/3689743,
author = {Campbell, Eric Hayden and Hojjat, Hossein and Foster, Nate},
title = {Computing Precise Control Interface Specifications},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689743},
doi = {10.1145/3689743},
abstract = {Verifying network programs is challenging because of how they divide labor: the control plane computes high level routes through the network and compiles them to device configurations, while the data plane uses these configurations to realize the desired forwarding behavior. In practice, the correctness of the data plane often assumes that the configurations generated by the control plane will satisfy complex specifications. Consequently, validation tools such as program verifiers, runtime monitors, fuzzers, and test-case generators must be aware of these control interface specifications (ci-specs) to avoid raising false alarms.  In this paper, we propose the first algorithm for computing precise ci-specs for network data planes. Our specifications are designed to be efficiently monitorable—concretely, checking that a fixed configuration satisfies a ci-spec can be done in polynomial time. Our algorithm, based on modular program instrumentation, quantifier elimination, and a path-based analysis, is more expressive than prior work, and is applicable to practical network programs. We describe an implementation and show that ci-specs computed by our tool are useful for finding real bugs in real-world data plane programs.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {303},
numpages = {30},
keywords = {deductive synthesis, programmable networks, quantifier elimination}
}

@software{10.5281/zenodo.12785373,
author = {Campbell, Eric Hayden and Hojjat, Hossein and Foster, Nate},
title = {Capisce Source Code for paper ``Computing Precise Control Interface Specifications''},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12785373},
abstract = {
    <h2 id="capisce">Capisce</h2>
<p>Capisce is described in the OOPSLA paper entitled <em>Computing
Precise Control Interface Specifications</em>.</p>
<p>The Capisce library comprises two key pieces:</p>
<ul>
<li><p>an OCAML library for writing down and specifying data plane
programs called <code>GPL</code>.</p>
<ul>
<li><p>Example programs can be seen in the
<code>capisce/programs</code>.</p></li>
<li><p>The core interface for writing programs can be found in
<code>capisce/lib/ASTs.ml</code>.</p></li>
</ul></li>
<li><p>an instrumentation algorithm to translate <code>GPL</code>
programs into programs in the guarded command language</p>
<ul>
<li>the instrumentation algorithm <code>GPL.encode_tables</code> van be
found in <code>capisce/lib/AST.ml</code></li>
</ul></li>
<li><p>an inference algorithm to infer control interface specifications
for data plane programs</p>
<ul>
<li>The core algorithm (<code>cegqe</code>) can be found in
<code>capisce/lib/Qe.ml</code>.</li>
</ul></li>
</ul>
<p>Here we list the claims in the paper and how they are supported by
the artifact:</p>
<ol type="1">
<li><p><em>Capisce compute control interface specifications for real
programs.</em> This artifact supports this via its survey of real world
programs that have been implemented in our library. We have provided
scripts to automatically generate the tex for Figures 5 and 6 from the
paper.</p></li>
<li><p><em>A small proportion of paths suffice to compute control
interface specifications for real programs</em>. This can be seen in the
final column of Figures 5 and 6, as well as qualitatively in Figure 7.
We have provided scripts to automatically generate the graphs in Figure
7.</p></li>
</ol>
<h3 id="hardware-dependencies">Hardware Dependencies</h3>
<p>We ran our experiments on an Ubuntu 22.04.4 server with the following
specs: - CPU: Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz - RAM: 0.5
TB</p>
<h3 id="getting-started">Getting started</h3>
<p>There are two ways to get started with Docker: using Docker, and
building from source. We have tested <code>Capisce</code> on Ubuntu
20.04 and 22.04. We recommend using docker to get started quickly.</p>
<p>Either way, the first step is to download the source code, by either
cloning the repository using git, or unpacking the downloaded .zip file.
Then change into the newly created directory. For instance:</p>
<pre><code>git clone https://github.com/cornell-netlab/capisce.git
cd capisce</code></pre>
<h4 id="docker">Docker</h4>
<p>The easiest way to get Capisce running is using Docker.</p>
<ul>
<li>First install Docker.</li>
<li>Finally, <code>build</code> and <code>run</code> the docker
image</li>
</ul>
<pre><code>docker build -t capisce .
docker run -it capisce</code></pre>
<p>Once this succeeds (it may take a while), you should be greeted with
a shell prompt similar to the following:</p>
<pre><code>opam@1497723f4b4d:~/capisce/capisce$</code></pre>
<p>Now to build Capisce, run <code>make</code>.</p>
<p>Verify your build by running <code>./capisce exp -help</code></p>
<h6 id="known-issue">Known Issue</h6>
<p>On M1 Macs there may be an issue regarding the a missing
<code>/lib64/ld-linux-x86-64.so.2</code> file. If you get such an error
try building with the flag <code>--platform linux/amd64</code></p>
<h4 id="installing-from-source">Installing from source</h4>
<p>Capiscelib is an ocaml library, so we first need to install
<code>opam</code>. Then, <code>switch</code> to the supported ocaml
compiler version</p>
<pre><code>opam switch create 4.14.0
eval $(opam env)</code></pre>
<p>Now install some basic ocaml tooling</p>
<pre><code>opam install dune
opam install menhir
opam install utop</code></pre>
<p>As well as a system dependency:</p>
<pre><code>sudo apt install libgmp-dev -y</code></pre>
<p>Now, change into the nested <code>capisce</code> directory
(i.e.&nbsp;<code>/path/to/repo/capisce/capisce</code>), and install the
dependencies in the <code>capisce.opam</code> file:</p>
<pre><code>opam install . --deps-only</code></pre>
<p>Now you should be ready to build <code>capisce</code> by running
<code>make</code></p>
<pre><code>make</code></pre>
<p>Verify your installation by running
<code>./capisce exp -help</code></p>
<h5 id="dependencies-for-processing-the-experimental-results">Dependencies
for processing the experimental results</h5>
<p>The experimental results are processed using some python scripts.
They have their own dependencies that need to be installed:</p>
<pre><code>sudo apt install python3 -y
sudo apt install python3-pip -y
pip3 install sigfig
pip3 install matplotlib
pip3 install ipython</code></pre>
<h4 id="hello-world-arp">Hello World: ARP</h4>
<p>Once you’ve installed <code>Capisce</code>, you can verify it works,
by computing a specification for the <code>arp</code> program, which can
be found in <code>programs/Arp.ml</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># wd: capisce/capisce</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> ./survey_data_oopsla</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="ex">./capisce</span> exp arp <span class="at">-out</span> ./survey_data_oopsla <span class="at">-hv</span></span></code></pre></div>
<p><code>capisce</code> will spit out a collection of SMT formulae whose
conjunction corresponds to the control interface specification (spec)
that enforces there are no invalid header reads (<code>-hv</code>). It
should take about 5 seconds.</p>
<p>If the above command fails, with an error complaining about not being
able to find <code>../solvers/z3.4.8.13</code> or
<code>../solvers/princess</code>, you can specify the path to these
solvers (in the <code>solvers</code> directory) manually by using the
<code>-z3</code> and <code>-princess</code> flags. For instance:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./capisce</span> exp <span class="at">-name</span> arp <span class="at">-hv</span> <span class="at">-out</span> ./survey_data_oopsla <span class="at">-z3</span> /path/to/z3 <span class="at">-princess</span> /path/to/princess</span></code></pre></div>
<p>In the sequel we will omit the explicit paths flags, but they may
always be added if needed.</p>
<h3 id="step-by-step-instructions">Step By Step Instructions</h3>
<p>We’ve provided instructions for automatically exercising the
experiments using our scripts, and for running them manually.</p>
<h4 id="exercising-the-experiments">Exercising the Experiments</h4>
<p>Now you can run the experiments from the paper. These will take
several days. Note that because path selection is done by Z3, there may
be some variation in the precise numbers generated by this step.</p>
<ul>
<li><p><code>make survey</code> runs the experiments described in
Figures 5 and 6. The output can be seen in
<code>./survey_data_oopsla</code>. Running the experiments takes about 5
days of compute.</p></li>
<li><p><code>make survey-tex</code> generates the TeX for Figures 5 and
6. Note. Run this while <code>make survey</code> is running to see the
results computed so far.</p></li>
<li><p><code>make coverage</code> generates the graphs in Figure 7. This
will take 2-3 hours</p></li>
</ul>
<h4 id="running-experiments-one-by-one">Running Experiments
One-by-One</h4>
<p>To run the experiments individually, execute the following command
once for each pipeline:</p>
<pre><code>./capisce exp NAME -out ./survey_data_oopsla</code></pre>
<p>where <code>OUT</code> is the output directory in which you wish to
store the results, and <code>NAME</code> is the name of the example
program. The valid names can be seen by typing
<code>./capisce exp</code>.</p>
<p>Most of these will finish in minutes, but several will take nearly
two days. For more-precise timing expectations, consult Figures 5 and
6.</p>
<p>Once you’ve done this, you can generate Figures 5 and 6 using the
script <code>./scripts/survey-to-tex.py</code>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> ./scripts/survey-to-tex.py</span></code></pre></div>
<p>You may run this script after any number of examples have been run,
and you will get partial results. Those results that haven’t finished
running yet will be indicated by an infinity symbol in the “Result”
column.</p>
<p>To reproduce Figure 7, re-run the relevant programs with the
<code>-replay</code> flag. This will generate the additional data
required to generate Figure 7. The coverage analysis is slow, and may
take several hours. To generate the data run the following commands:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./capisce</span> exp arp <span class="at">-replay</span> <span class="at">-out</span> survey_data_oopsla <span class="at">-hv</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="ex">./capisce</span> exp heavy_hitter_2 <span class="at">-replay</span> <span class="at">-out</span> survey_data_oopsla <span class="at">-hv</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="ex">./capisce</span> exp heavy_hitter_1 <span class="at">-replay</span> <span class="at">-out</span> survey_data_oopsla <span class="at">-hv</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="ex">./capisce</span> exp flowlet <span class="at">-replay</span> <span class="at">-out</span> survey_data_oopsla <span class="at">-hv</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="ex">./capisce</span> exp 07-multiprotocol <span class="at">-replay</span> <span class="at">-out</span> survey_data_oopsla <span class="at">-hv</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="ex">./capisce</span> exp simple_nat <span class="at">-replay</span> <span class="at">-out</span> survey_data_oopsla <span class="at">-hv</span></span></code></pre></div>
<p>Then, to produce the graphs as seen in Figure 7, run the following
script:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> scripts/graphs.py</span></code></pre></div>
<p>Running the script will output the relative paths to the pdfs it
generates.</p>
<h4 id="common-issues">Common Issues</h4>
<p>The Capisce repo ships with solver executables in the
<code>solvers</code> directory. It is common to have are issues with the
solvers—Capisce prints an inscrutable error message followed by
<code>(Failure "found false")</code>. For instance,</p>
<pre><code>Error: Cannot find or load the main class ap.CmdlMain
Cause: java.lang.ClassNotFoundException: ap.CmdlMain
Uncaught exception:

  (Failure "found false")</code></pre>
<p>If these occur, please install <a href="https://github.com/Z3Prover/z3">Z3</a> and <a href="http://www.philipp.ruemmer.org/princess.shtml">princess</a>. You
can either place the executables in capisce’s <code>solvers</code>
directory, or pass the locations of the executables to
<code>capisce</code> using the <code>-z3</code> and
<code>-princess</code> flags.</p>
<h3 id="reusability-guide">Reusability Guide</h3>
<p>Our artifact supports three key pieces for reusability.</p>
<ul>
<li><p>The pipeline specification IR <code>GPL</code>, which can be
found in <code>ASTs.GPL</code>. This AST can be used as a compiler
backend for related dataplane analysis tools like <code>petr4</code>,
<code>p4cub</code>, <code>p4k</code>, <code>p4-constraints</code>, or
<code>PI4</code>.</p></li>
<li><p>The compiler infrastructure for <code>GPL.t</code> allows for
programmers to easily extend the core set of primitives, in a way that
supports efficient reuse.</p></li>
<li><p>The Counterexample-guided inductive quantifier elimination
algorithm <code>QE.cegqe</code> is succinctly stated, and can be
reimplemented or adapted to as new algorithms are discovered.</p></li>
</ul>
<h4 id="tutorial">Tutorial</h4>
<p>Now that you’ve build <code>Capisce</code>, we’ll show you how it
works.</p>
<p>First run <code>dune utop</code>. This will load <code>Capisce</code>
into a REPL. Now, open the <code>Capisce</code> module:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="kw">open</span> Capisce;;</span></code></pre></div>
<p>In this Hello-World tutorial, we’ll write a program in our IR
<code>GPL</code>, which represents the guarded pipeline language
described in the paper. Then we’ll write a specification that it must
satisfy. Finally, we’ll infer a ccontrol interface spec that will ensure
the assertion is satisfied.</p>
<h5 id="part-1-writing-a-program-in-gpl">Part 1: Writing a program in
GPL</h5>
<p>First, let open the Modules for the program syntax
(<code>GPL</code>), including Bitvector Expressions (<code>Expr</code>)
and Boolean Expressions (<code>BExpr</code>).</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="kw">open</span> ASTs.GPL;; <span class="kw">open</span> Expr;; <span class="kw">open</span> BExpr;;</span></code></pre></div>
<p>We’ll now write a simple GPL program that uses a single forwarding
table to set a single 9-bit field <code>port</code> based on the value
of a 32-bit destination address <code>dst</code>. First, we can define
the variables <code>port</code>and <code>dst</code>:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="kw">let</span> port = Var.make <span class="st">"port"</span> <span class="dv">9</span>;;</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> port : Var.t = &lt;abstr&gt;</span></code></pre></div>
<div class="sourceCode" id="cb20"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="kw">let</span> dst = Var.make <span class="st">"dst"</span> <span class="dv">32</span>;;</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> dst : Var.t = &lt;abstr&gt;</span></code></pre></div>
<p>We’ll use these variables to construct our table. Lets see how we
might do that by inspecting the type of the constructor
<code>table</code>:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>utop # table;;</span></code></pre></div>
<p>should produce</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>- : <span class="dt">string</span> -&gt;</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    [&gt; `Exact <span class="kw">of</span> Var.t | `Maskable <span class="kw">of</span> Var.t | `MaskableDegen <span class="kw">of</span> Var.t ] <span class="dt">list</span> -&gt;</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    (Var.t <span class="dt">list</span> * Capiscelib.Primitives.Action.t <span class="dt">list</span>) <span class="dt">list</span> -&gt;</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    Pack.t</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>= &lt;<span class="kw">fun</span>&gt;</span></code></pre></div>
<p>This tells us that to construct a <code>table</code>, we need 3
arguments: a <code>string</code> name, a list variable keys tagged with
their matchkind (<code>Exact</code>, <code>Maskable</code>, and
<code>MaskableDegen</code>), then a list of
<code>Var.t list * Primitives.Action.t list</code>. Each pair
<code>(xs, as)</code> in this list corresponds to an anonymous function
where <code>xs</code> occur free in a list of primitive actions. This
list should be understood as sequential composition. Lets construct our
first action.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="kw">let</span> nop : Var.t <span class="dt">list</span> * Primitives.Action.t <span class="dt">list</span> = [],[];;</span></code></pre></div>
<p>This action is the trivial action. It takes no arguments
<code>[],</code> and executes noactions <code>[]</code>.</p>
<p>Stepping it up a notch in complexity. We will define a action that
takes in a single argument, indicated by parameter <code>p</code>, and
assigns <code>p</code> to our previously-defined variable
<code>port</code>:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="kw">let</span> setport =</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>     <span class="kw">let</span> p = Var.make <span class="st">"p"</span> <span class="dv">9</span> <span class="kw">in</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>     [p], Primitives.Action.[</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>          assign port (Expr.var p)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>     ];;</span></code></pre></div>
<p>The first line constructs the AST node for parameter <code>p</code>.
Then the <code>[p],</code> says that <code>p</code> is an argument for
the action, which is defined by the subsequent action list.</p>
<p>Now we can define a table, called <code>simpletable</code> that reads
the vpalue of <code>port</code>, and then either execute the
<code>setport</code> action with some parameter, or take no action.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="kw">let</span> simpletable =</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    table <span class="st">"simpletable"</span> [`Exact port] [setport; nop];;</span></code></pre></div>
<p>The <code>Exact</code> matchkind tells us that
<code>simpletable</code> must precisely read the bits of
<code>port</code>. Using <code>Maskable</code> corresponds unifies the
notions of <code>ternary</code>, <code>lpm</code> and
<code>optional</code>, all of which allow the the table to skip reading
that specific key. <code>MaskableDegen</code> is semantically equivalent
to <code>Exact</code>, but allows us to differentiate between truly
maskable match data and degenerate cases described in the paper</p>
<h5 id="part-2-writing-a-specification">Part 2: Writing a
Specification</h5>
<p>Now, as an example specification, we can exclude a specific port
value. Perhaps to indicate that this port value, say <code>47</code> is
disabled. So we never want to forward a packet out on port
<code>47</code>. We define a spec that ensures this as follows:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="kw">let</span> port_not_47 =</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> prohibited = Expr.bvi <span class="dv">47</span> <span class="dv">9</span> <span class="kw">in</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    BExpr.(not_ (eq_ (Expr.var port) prohibited));;</span></code></pre></div>
<p>Now we can use assertions to specify that our table must satisfy this
spec:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="kw">let</span> program = sequence [</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    simpletable;</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    assert_ port_not_47</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>];;</span></code></pre></div>
<h5 id="part-3-inferring-a-spec">Part 3: Inferring A Spec</h5>
<p>Inferring the control interface spec for the table requires two
steps. First we encode the tables using the instrumentation strategy
described in the paper, and then we run our inference algorithm.</p>
<p>The encoding step eliminates tables, and converts a
<code>GPL.t</code> program into a <code>GCL.t</code> program.
<code>GCL</code> here stands for Dijkstra’s <em>guarded command
language</em>. We run this using the <code>GPL.encode_tables</code>
function:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="kw">let</span> gcl = ASTs.GPL.encode_tables program;;</span></code></pre></div>
<p>To see a pretty printed version of the table-free program, run the
following:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="dt">Printf</span>.printf <span class="st">"\%s"</span> @@ ASTs.GCL.to_string gcl;;</span></code></pre></div>
<p>Now we can infer the specification for this program by running the
<code>CEGQE</code> algorithm:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="kw">let</span> cis = Qe.cegqe gcl;;</span></code></pre></div>
<p>To pretty print the result in SMTLIB format, run the following
command:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>utop # <span class="dt">Printf</span>.printf <span class="st">"\%s"</span> @@ BExpr.to_smtlib cis;;</span></code></pre></div>
<p>The resulting specification has two conjucts. The first, shown below,
says that whenever the action is has index <code>0</code>, that is when
it corresponds to <code>setport</code>, the argument to
<code>setport</code> must not be <code>47</code>.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode lisp"><code class="sourceCode commonlisp"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">not</span> (<span class="kw">and</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>          (<span class="op">=</span> _symb$simpletable$action$_$<span class="dv">0</span> (_ bv1 <span class="dv">1</span>))</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>          (<span class="op">=</span> _symb$simpletable$<span class="dv">1</span>$p$_$<span class="dv">0</span> (_ bv47 <span class="dv">9</span>))))</span></code></pre></div>
<p>The second conjunct, replicated below, says that whenever the action
is <code>setport</code>, the key that was matched by
<code>simpletable</code> must not be equal to <code>47</code>.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode lisp"><code class="sourceCode commonlisp"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">not</span> (<span class="kw">and</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>          (<span class="op">=</span> _symb$simpletable$action$_$<span class="dv">0</span> (_ bv0 <span class="dv">1</span>))</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>          (<span class="op">=</span> _symb$simpletable$match_0$_$<span class="dv">0</span> (_ bv47 <span class="dv">9</span>))))</span></code></pre></div>
<h4 id="guarded-pipeline-language-gpl">Guarded Pipeline Language
<code>GPL</code></h4>
<p>Here we provide documentation of the core interface for writing
<code>GPL.t</code> programs.</p>
<p>The <code>GPL</code> module, defined in <code>ASTs.ml</code> defines
programs.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> t</span></code></pre></div>
<p>It has a type <code>t</code> that corresponds to <code>GPL</code>
programs themselves.</p>
<p>We can construct trivial programs</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> skip : t</span></code></pre></div>
<p>Sequential compositions of programs;</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> sequence : t <span class="dt">list</span> -&gt; t</span></code></pre></div>
<p>Nondeterministic choice between programs:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> choice : t <span class="dt">list</span> -&gt; ts</span></code></pre></div>
<p>We can also construct variable assignments</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> assign_ : Var.t -&gt; Expr.t -&gt; t</span></code></pre></div>
<p>and the most important constuct, tables:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> table : <span class="dt">string</span> </span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>  -&gt; [&gt; `Exact <span class="kw">of</span> Var.t <span class="dt">list</span> | `Maskable <span class="kw">of</span> Var.t <span class="dt">list</span> | `MaskableDegen <span class="kw">of</span> Var.t ] <span class="dt">list</span> </span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>  -&gt; ( Var.t * Primtives.Action.t <span class="dt">list</span>) <span class="dt">list</span> -&gt; t</span></code></pre></div>
<p>As described above <code>table name keys actions</code> constructs a
table named <code>name</code> that chooses an action <code>a</code> in
<code>actions</code> by inspecting the variables in
<code>keys</code>.</p>
<p>More about <code>Primitives.Action</code> can be found in the next
section.</p>
<p>To specify desired behaviors we have two primitives,
<code>assume</code> and <code>assert</code>.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> assume : BExpr.t -&gt; t</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> assert_ : BExpr.t -&gt; t</span></code></pre></div>
<p>The <code>assume</code> primitive is angelic—if it can be satisfied
the program assumes it is. Conversely, <code>assert_</code> is
demonic—if it can ba falsified the program assumes it is.</p>
<h5 id="building-the-documentation">Building the Documentation</h5>
<p>The full documentation can be viewed using the following command. It
may prompt you to install <code>odoc</code>. Please do so using
<code>opam install odoc</code>.</p>
<pre><code>make doc -B</code></pre>
<p>This will open the documentation in your systems default web browser.
If you do not have a web browser installed the terminal
<code>xdg-open</code> command will fail. Feel free to browse the
documenation some other way. In the docker container the docs will be
opened in <code>w3m</code> (press <code>enter</code> to follow links and
<code>q</code> to quit).</p>
<p>The documentation for the core modules can be found by clicking on
<code>capisce</code> and then navigating to modules <code>Cmd</code>,
<code>ASTs</code>, and <code>Qe</code>.</p>
<h4 id="instrumentation-and-compiler">Instrumentation and Compiler</h4>
<p><code>GPL.t</code> is constructed using a functor
<code>Cmd.Make</code> that allows users to produce simple loop-free
imperative programs with demonic nondeterminism.</p>
<p><code>Cmd.Make</code> has a single module argument which must have
module type <code>Primitive</code> shown below:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">module</span> <span class="kw">type</span> Primitive = <span class="kw">sig</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">type</span> t [@@deriving quickcheck, eq, hash, sexp, <span class="dt">compare</span>]</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">val</span> assume : BExpr.t -&gt; t</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">val</span> assert_ : BExpr.t -&gt; t</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">val</span> contra : t -&gt; t -&gt; <span class="dt">bool</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">val</span> to_smtlib : t -&gt; <span class="dt">string</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">val</span> size : t -&gt; <span class="dt">int</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">val</span> vars : t -&gt; Var.t <span class="dt">list</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p>The <code>assume</code> and <code>assert_</code> functions construct
assumptions and assertions as before; <code>contra</code> describes when
two assumptions and/or assertions are contradictory;
<code>to_smtlib</code> converts <code>t</code> into an string that uses
smtlib syntax for expressions and variables; <code>size</code> computes
the size of a primitive, and <code>vars</code> computes the variables
used in a primitive.</p>
<p>This structure is extremely extensible and facilitates easy reuse of
our code. The file <code>Primitives.ml</code> serves as a great tutorial
for how to build up a higherarchical set of Primitives. Then
<code>ASTs.ml</code> uses these Primtives to build a set of IRs and a
compiler pipeline between them. We summarize it here.</p>
<p>Our compiler pipeline starts with <code>GPL</code> and then uses
<code>encode_tables</code> to produce a <code>GCL</code> program. Then
passify produces a program in the passive form <code>Psv.t</code> as
defined in Section 3.4 of the paper. Then we use standard verification
generation technqiues to produce formulae in SMTLIB.</p>
<p>Each of these passes is a fundamentally a catamorphism over the core
structure of the programs, and eliminates a single primitive at a time:
<code>encode_tables</code> eliminates tables, and <code>passify</code>
eliminates assignments. This is captured in the types.</p>
<p>Starting from the bottom, <code>Psv.t = Cmd.Make(Passive).t</code>,
where <code>Passive.t</code> is either an <code>Assume</code> or an
<code>Assert</code>. Then <code>GCL = Cmd.Make(Active).t</code>, where
<code>Active.t</code> is either a <code>Passive.t</code> or an
<code>Assign</code>ment. Finally
<code>GPL = Cmd.Make(Pipeline).t</code>, where <code>Pipeline.t</code>
is either a <code>Table</code> or an <code>Active</code>. The
transformation functions <code>encode_tables</code> and
<code>passify</code> defined in <code>ASTs.ml</code> define this
clearly.</p>
<p>With this compiler infrastructure in place, it would be easy for
future researchers to extend our work with additional features. Just as
writing the compiler for <code>GPL</code> leverages the existing
compiler for <code>GCL</code>, futurue work could extend GPL add
primitives for multiple-assignment, hash functions, or stateful
operations. Simply by writing elimination passes, researchers could make
ready use of our existing verification generation and specification
inference code.</p>
<h4 id="specification-inference-and-modelling">Specification Inference
and Modelling</h4>
<p>The algorithm <code>Qe.ceqge</code> in the <code>Qe.ml</code> file is
straightfoward and easy to modify. The experimental setup defined in
<code>bin/Main.ml</code> supports swapping in different algorithms for
performing the inference, which will allow future researchers to
directly measure their improvements over CegQe.</p>

},
keywords = {deductive synthesis, programmable networks, quantifier elimination}
}

@article{10.1145/3689744,
author = {Johnson, Keith J.C. and Krishnan, Rahul and Reps, Thomas and D’Antoni, Loris},
title = {Automating Pruning in Top-Down Enumeration for Program Synthesis Problems with Monotonic Semantics},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689744},
doi = {10.1145/3689744},
abstract = {In top-down enumeration for program synthesis, abstraction-based pruning uses an abstract domain to approximate the set of possible values that a partial program, when completed, can output on a given input. If the set does not contain the desired output, the partial program and all its possible completions can be pruned. In its general form, abstraction-based pruning requires manually designed, domain-specific abstract domains and semantics, and thus has only been used in domain-specific synthesizers. 
 
 
 
 
 
 
 
This paper provides sufficient conditions under which a form of abstraction-based pruning can be automated for arbitrary synthesis problems in the general-purpose Semantics-Guided Synthesis (SemGuS) framework without requiring manually-defined abstract domains. We show that if the semantics of the language for which we are synthesizing programs exhibits some monotonicity properties, one can obtain an abstract interval-based semantics for free from the concrete semantics of the programming language, and use such semantics to effectively prune the search space. We also identify a condition that ensures such abstract semantics can be used to compute a precise abstraction of the set of values that a program derivable from a given hole in a partial program can produce. These precise abstractions make abstraction-based pruning more effective. 
 
 
 
 
 
 
 
We implement our approach in a tool, Moito, which can tackle synthesis problems defined in the SemGuS framework. Moito can automate interval-based pruning without any a-priori knowledge of the problem domain, and solve synthesis problems that previously required domain-specific, abstraction-based synthesizers— e.g., synthesis of regular expressions, CSV file schema, and imperative programs from examples.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {304},
numpages = {27},
keywords = {abstract interpretation, grammar flow analysis, program synthesis}
}

@software{10.5281/zenodo.12669773,
author = {Johnson, Keith J.C. and Krishnan, Rahul and Reps, Thomas and D’Antoni, Loris},
title = {Automating Pruning in Top-Down Enumeration for Program Synthesis Problems with Monotonic Semantics},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12669773},
abstract = {
    <p>This is the artifact submission for OOPSLA’24 (R2) submission #377:
Automating Pruning in Top-Down Enumeration for Program Synthesis
Problems with Monotonic Semantics</p>

},
keywords = {abstract interpretation, grammar flow analysis, program synthesis}
}

@article{10.1145/3689745,
author = {Webbers, Robin and von Gleissenthall, Klaus and Jhala, Ranjit},
title = {Refinement Type Refutations},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689745},
doi = {10.1145/3689745},
abstract = {Refinement types combine SMT decidable constraints with a compositional, syntax-directed type system to provide a convenient way to statically and automatically check properties of programs. However, when type checking fails, programmers must use cryptic error messages that, at best, point out the code location where a subtyping constraint failed to determine the root cause of the failure. In this paper, we introduce refinement type refutations, a new approach to explaining why refinement type checking fails, which mirrors the compositional way in which refinement type checking is carried out. First, we show how to systematically transform standard bidirectional type checking rules to obtain refutations. Second, we extend the approach to account for global constraint-based refinement inference via the notion of a must-instantiation: a set of concrete inhabitants of the types of subterms that suffice to demonstrate why typing fails. Third, we implement our method in HayStack—an extension to LiqidHaskell which automatically finds type-refutations when refinement type checking fails, and helps users understand refutations via an interactive user-interface. Finally, we present an empirical evaluation of HayStack using the regression benchmark-set of LiqidHaskell, and the benchmark set of G2, a previous method that searches for (non-compositional) counterexample traces by symbolically executing Haskell source. We show that HayStack can find refutations for 99.7\% of benchmarks, including those with complex typing constructs (e.g., abstract and bounded refinements, and reflection), and does so, an order of magnitude faster than G2.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {305},
numpages = {26},
keywords = {Counterexamples, Refinement Types, Type Refutations}
}

@article{10.1145/3689746,
author = {Cheng, Luyu and Parreaux, Lionel},
title = {The Ultimate Conditional Syntax},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689746},
doi = {10.1145/3689746},
abstract = {Functional programming languages typically support expressive pattern-matching syntax allowing programmers
 
to write concise and type-safe code, especially appropriate for manipulating algebraic data types. Many
 
features have been proposed to enhance the expressiveness of stock pattern-matching syntax, such as pattern
 
bindings, pattern alternatives (a.k.a. disjunction), pattern conjunction, view patterns, pattern guards, pattern
 
synonyms, active patterns, ‘if-let’ patterns, multi-way if-expressions, etc. In this paper, we propose a new
 
pattern-matching syntax that is both more expressive and (we argue) simpler and more readable than previous
 
alternatives. Our syntax supports parallel and nested matches interleaved with computations and intermediate
 
bindings. This is achieved through a form of nested multi-way if-expressions with a condition-splitting mechanism
 
to factor common conditional prefixes as well as a binding technique we call conditional pattern flowing.
 
We motivate this new syntax with many examples in the setting of MLscript, a new ML-family programming
 
language. We describe a straightforward desugaring pass from our rich source syntax into a minimal core
 
syntax that only supports flat patterns and has an intuitive small-step semantics. We then provide a translation
 
from the core syntax into a normalized syntax without backtracking, which is more amenable to coverage
 
checking and compilation, and formally prove that our translation is semantics-preserving. We view this work
 
as a step towards rethinking pattern matching to make it more powerful and natural to use. Our syntax can
 
easily be integrated, in part or in whole, into existing as well as future programming language designs.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {306},
numpages = {30},
keywords = {pattern matching, programming language design, syntax}
}

@software{10.5281/zenodo.13621222,
author = {Cheng, Luyu and Parreaux, Lionel},
title = {The Ultimate Conditional Syntax (Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13621222},
abstract = {
    <p>This is the artifact of OOPSLA paper titled <em>The Ultimate
Conditional Syntax</em>. The latest version of the artifact can be found
at <a href="https://github.com/hkust-taco/ucs" class="uri">https://github.com/hkust-taco/ucs</a>. A online web demo can
be found at <a href="https://ucs.mlscript.dev" class="uri">https://ucs.mlscript.dev</a>. A comprehensive manual of the
artifact is located at <code>manual/manual.pdf</code> in the
archive.</p>

},
keywords = {compiler, MLscript, pattern matching, semantics, syntax, transformation, web demo}
}

@article{10.1145/3689747,
author = {Bittner, Paul Maximilian and Schulthei\ss{}, Alexander and Moosherr, Benjamin and Young, Jeffrey M. and Teixeira, Leopoldo and Walkingshaw, Eric and Ataei, Parisa and Th\"{u}m, Thomas},
title = {On the Expressive Power of Languages for Static Variability},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689747},
doi = {10.1145/3689747},
abstract = {Variability permeates software development to satisfy ever-changing requirements and mass-customization needs. A prime example is the Linux kernel, which employs the C preprocessor to specify a set of related but distinct kernel variants. To study, analyze, and verify variational software, several formal languages have been proposed. For example, the choice calculus has been successfully applied for type checking and symbolic execution of configurable software, while other formalisms have been used for variational model checking, change impact analysis, among other use cases. Yet, these languages have not been formally compared, hence, little is known about their relationships. Crucially, it is unclear to what extent one language subsumes another, how research results from one language can be applied to other languages, and which language is suitable for which purpose or domain. In this paper, we propose a formal framework to compare the expressive power of languages for static (i.e., compile-time) variability. By establishing a common semantic domain to capture a widely used intuition of explicit variability, we can formulate the basic, yet to date neglected, properties of soundness, completeness, and expressiveness for variability languages. We then prove the (un)soundness and (in)completeness of a range of existing languages, and relate their ability to express the same variational systems. We implement our framework as an extensible open source Agda library in which proofs act as correct compilers between languages or differencing algorithms. We find different levels of expressiveness as well as complete and incomplete languages w.r.t. our unified semantic domain, with the choice calculus being among the most expressive languages.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {307},
numpages = {33},
keywords = {configuration, language semantics, software product lines, variation}
}

@software{10.5281/zenodo.13502454,
author = {Bittner, Paul Maximilian and Schulthei\ss{}, Alexander and Moosherr, Benjamin and Young, Jeffrey M. and Teixeira, Leopoldo and Walkingshaw, Eric and Ataei, Parisa and Th\"{u}m, Thomas},
title = {Vatras - Artifact for the Paper "On the Expressive Power of Languages for Static Variability"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13502454},
abstract = {
    <p>Vatras is an Agda library and formalizes all results in our
paper:</p>
<ul>
<li>All formal languages for static software variability presented in
our survey (<strong>Section 3 + Table 1</strong>) are formalized as
algebraic datatypes.</li>
<li>The library implements our formal framework for language
comparisons, including necessary data structures, theorems, and proofs
(<strong>Section 4</strong>).</li>
<li>This library contains all theorems and proofs to establish the map
of variability languages we find by comparing the languages from our
survey with our framework (<strong>Section 5</strong>).</li>
</ul>
<p>Additionally, our library comes with a small demo. When run in a
terminal, our demo will show a translation roundtrip, showcasing the
circle of compilers developed for identifying the map of variability
languages (Section 5).</p>

},
keywords = {expressive power, formalization, proofs, software product lines, software variability}
}

@article{10.1145/3689748,
author = {Pham, Long and Wang, Di and Saad, Feras A. and Hoffmann, Jan},
title = {Programmable MCMC with Soundly Composed Guide Programs},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689748},
doi = {10.1145/3689748},
abstract = {Probabilistic programming languages (PPLs) provide language support                                for expressing flexible probabilistic models and solving Bayesian                                inference problems.                                PPLs with programmable inference make it possible for users                                to obtain improved results by customizing inference engines using                                guide programs that are tailored to a corresponding                                model program.                                However, errors in guide programs can compromise the statistical                                soundness of the inference.                                This article introduces a novel coroutine-based framework for verifying                                the correctness of user-written guide programs for a broad class of Markov                                chain Monte Carlo (MCMC) inference algorithms.                                Our approach rests on a novel type system for describing communication                                protocols between a model program and a sequence of guides that each                                update only a subset of random variables.                                We prove that, by translating guide types to context-free processes with finite                                norms, it is possible to check structural type equality                                between models and guides in polynomial time.                                This connection gives rise to an efficient type-inference                                algorithm for probabilistic programs with flexible constructs such as                                general recursion and branching.                                We also contribute a coverage-checking algorithm that verifies                                the support of sequentially composed guide programs agrees                                with that of the model program, which is a key soundness condition for                                MCMC inference with multiple guides.                                Evaluations on diverse benchmarks show that our type-inference and                                coverage-checking algorithms efficiently infer types and detect sound                                and unsound guides for programs that existing static analyses cannot handle.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {308},
numpages = {30},
keywords = {Bayesian inference, context-free types, coroutines, probabilistic programming, type systems}
}

@software{10.5281/zenodo.12669572,
author = {Pham, Long and Wang, Di and Saad, Feras A. and Hoffmann, Jan},
title = {Artifact for Programmable MCMC with Soundly Composed Guide Programs},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12669572},
abstract = {
    <p>In probabilistic programming with a newly proposed coroutine-based
programmable inference framework, the user provides (i) a model
coroutine and (ii) a sequential composition of guide coroutines. The
model coroutine specifies a probabilistic model for Bayesian inference.
Meanwhile, the sequential composition of guide coroutines customizes the
Block Metropolis-Hastings (BMH) algorithm, where we successively run the
guide coroutines, each of which is followed by an MH acceptance routine.
Each guide coroutine only updates a subset (i.e., block) of random
variables. The model and guide coroutines communicate with one another
by message passing, and their communication protocols are described by
guide types.</p>
<p>This artifact is a program analysis tool for statically checking the
soundness of a probabilistic program in this coroutine-based framework.
The artifact offers three functionalities: - Type-equality checking:
check structural type equality of guide types. - Type inference: infer
guide types of model and guide coroutines (using the first functionality
for structural-type-equality checking) - Coverage checking: check
whether the support of sequentially composed guide coroutines coincides
with the support of a model coroutine.</p>

},
keywords = {Bayesian inference, context-free types, coroutines, probabilistic programming, type systems}
}

@article{10.1145/3689749,
author = {de Vilhena, Paulo Em\'{\i}lio and Lahav, Ori and Vafeiadis, Viktor and Raad, Azalea},
title = {Extending the C/C++ Memory Model with Inline Assembly},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689749},
doi = {10.1145/3689749},
abstract = {Programs written in C/C++ often include inline assembly: a snippet of architecture-specific assembly code used to access low-level functionalities that are impossible or expensive to simulate in the source language. Although inline assembly is widely used, its semantics has not yet been formally studied.
 
In this paper, we overcome this deficiency by investigating the effect of inline assembly on the consistency semantics of C/C++ programs. We propose the first memory model of the C++ Programming Language with support for inline assembly for Intel's x86 including non-temporal stores and store fences. We argue that previous provably correct compiler optimizations and correct compiler mappings should remain correct under such an extended model and we prove that this requirement is met by our proposed model.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {309},
numpages = {27},
keywords = {Concurrency, Semantics of Programming Languages, Weak Memory Models}
}

@article{10.1145/3689750,
author = {Torczon, Cassia and Su\'{a}rez Acevedo, Emmanuel and Agrawal, Shubh and Velez-Ginorio, Joey and Weirich, Stephanie},
title = {Effects and Coeffects in Call-by-Push-Value},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689750},
doi = {10.1145/3689750},
abstract = {Effect and coeffect tracking integrate many types of compile-time
 
 
 
 analysis, such as cost, liveness, or dataflow, directly into a language's type
 
 
 
 system. In this paper, we investigate the addition of effect and coeffect
 
 
 
 tracking to the type system of call-by-push-value (CBPV), a computational
 
 
 
 model useful in compilation for its isolation of effects and for its ability
 
 
 
 to cleanly express both call-by-name and call-by-value computations. Our
 
 
 
 main result is effect-and-coeffect soundness, which asserts that the
 
 
 
 type system accurately bounds the effects that the program may trigger
 
 
 
 during execution and accurately tracks the demands that the program may make
 
 
 
 on its environment. This result holds for two different dynamic semantics: a
 
 
 
 generic one that can be adapted for different coeffects and one that is
 
 
 
 adapted for reasoning about resource usage. In particular, the second 
 
 
 
 semantics discards the evaluation of unused values and pure computations
 
 
 
 while ensuring that effectful computations are always evaluated, even if 
 
 
 
 their results are not required. Our results have been mechanized using 
 
 
 
 the Coq proof assistant.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {310},
numpages = {27},
keywords = {CBPV, Coeffects, Effects, Types}
}

@software{10.5281/zenodo.12654518,
author = {Torczon, Cassia and Su\'{a}rez Acevedo, Emmanuel and Agrawal, Shubh and Velez-Ginorio, Joey and Weirich, Stephanie},
title = {Artifact Associated with "Effects and Coeffects in Call-by-Push-Value"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12654518},
abstract = {
    <p>Coq mechanization of definitions and theorems</p>

},
keywords = {CBPV, Coeffects, Coq, Effects}
}

@article{10.1145/3689751,
author = {Boruch-Gruszecki, Aleksander and Ghosn, Adrien and Payer, Mathias and Pit-Claudel, Cl\'{e}ment},
title = {Gradient: Gradual Compartmentalization via Object Capabilities Tracked in Types},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689751},
doi = {10.1145/3689751},
abstract = {Modern software needs fine-grained compartmentalization, i.e., intra-process isolation. A particularly important reason for it are supply-chain attacks, the need for which is aggravated by modern applications depending on hundreds or even thousands of libraries. Object capabilities are a particularly salient approach to compartmentalization, but they require the entire program to assume a lack of ambient authority. Most of existing code was written under no such assumption; effectively, existing applications need to undergo a rewrite-the-world migration to reap the advantages of ocap. We propose gradual compartmentalization, an approach which allows gradually migrating an application to object capabilities, component by component in arbitrary order, all the while continuously enjoying security guarantees. The approach relies on runtime authority enforcement and tracking the authority of objects the type system. We present Gradient, a proof-of-concept gradual compartmentalization extension to Scala which uses Enclosures and Capture Tracking as its key components. We evaluate our proposal by migrating the standard XML library of Scala to Gradient.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {311},
numpages = {27},
keywords = {compartmentalization, object capabilities, security, type systems}
}

@article{10.1145/3689752,
author = {Root, Alexander J and Yan, Bobby and Liu, Peiming and Gyurgyik, Christophe and Bik, Aart J.C. and Kjolstad, Fredrik},
title = {Compilation of Shape Operators on Sparse Arrays},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689752},
doi = {10.1145/3689752},
abstract = {We show how to build a compiler for a sparse array language that supports shape operators such as reshaping or concatenating arrays, in addition to compute operators. Existing sparse array programming systems implement generic shape operators for only some sparse data structures, reduce shape operators on other data structures to those, and do not support fusion. Our system compiles sparse array expressions to code that efficiently iterates over reshaped views of irregular sparse data structures, without needing to materialize temporary storage for intermediates. Our evaluation shows that our approach generates sparse array code competitive with popular sparse array libraries: our generated shape operators achieve geometric mean speed-ups of 1.66\texttimes{}–15.3\texttimes{} when compared to hand-written kernels in scipy.sparse and 1.67\texttimes{}–651\texttimes{} when compared to generic implementations in pydata/sparse. For operators that require data structure conversions in these libraries, our generated code achieves geometric mean speed-ups of 7.29\texttimes{}–13.0\texttimes{} when compared to scipy.sparse and 21.3\texttimes{}–511\texttimes{} when compared to pydata/sparse. Finally, our evaluation demonstrates that fusing shape and compute operators improves the performance of several expressions by geometric mean speed-ups of 1.22\texttimes{}–2.23\texttimes{}.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {312},
numpages = {27},
keywords = {sparse array programming, sparse data structures, sparse iteration theory}
}

@software{10.5281/zenodo.13381305,
author = {Root, Alexander J and Yan, Bobby and Liu, Peiming and Gyurgyik, Christophe and Bik, Aart J.C. and Kjolstad, Fredrik},
title = {Artifact for OOPSLA 2024 Paper: Compilation of Shape Operators on Sparse Arrays},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13381305},
abstract = {
    <p>This artifact contains the prototype Burrito compiler, all benchmarks
from the paper, and graphing scripts used to produce the figures in the
paper.</p>

},
keywords = {sparse array programming, sparse data structures, sparse iteration theory}
}

@article{10.1145/3689753,
author = {Haselwarter, Philipp G. and Li, Kwing Hei and de Medeiros, Markus and Gregersen, Simon Oddershede and Aguirre, Alejandro and Tassarotti, Joseph and Birkedal, Lars},
title = {Tachis: Higher-Order Separation Logic with Credits for Expected Costs},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689753},
doi = {10.1145/3689753},
abstract = {We present Tachis, a higher-order separation logic to reason about the expected cost of probabilistic programs. Inspired by the uses of time credits for reasoning about the running time of deterministic programs, we introduce a novel notion of probabilistic cost credit. Probabilistic cost credits are a separation logic resource that can be used to pay for the cost of operations in programs, and that can be distributed across all possible branches of sampling instructions according to their weight, thus enabling us to reason about expected cost. The representation of cost credits as separation logic resources gives Tachis a great deal of flexibility and expressivity. In particular, it permits reasoning about amortized expected cost by storing excess credits as potential into data structures to pay for future operations. Tachis further supports a range of cost models, including running time and entropy usage. We showcase the versatility of this approach by applying our techniques to prove upper bounds on the expected cost of a variety of probabilistic algorithms and data structures, including randomized quicksort, hash tables, and meldable heaps. All of our results have been mechanized using Coq, Iris, and the Coquelicot real analysis library.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {313},
numpages = {30},
keywords = {expected time complexity, probabilistic programs, resource analysis}
}

@software{10.5281/zenodo.12659527,
author = {Haselwarter, Philipp G. and Li, Kwing Hei and de Medeiros, Markus and Gregersen, Simon Oddershede and Aguirre, Alejandro and Tassarotti, Joseph and Birkedal, Lars},
title = {Tachis: Higher-Order Separation Logic with Credits for Expected Costs - Coq Artifact},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12659527},
abstract = {
    <p>Formalization of the results of the paper “Tachis: Higher-Order
Separation Logic with Credits for Expected Costs” in the Coq proof
assistant.</p>

},
keywords = {expected time complexity, probabilistic programs, resource analysis}
}

@article{10.1145/3689754,
author = {Bembenek, Aaron and Greenberg, Michael and Chong, Stephen},
title = {Making Formulog Fast: An Argument for Unconventional Datalog Evaluation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689754},
doi = {10.1145/3689754},
abstract = {With its combination of Datalog, SMT solving, and functional programming, the language Formulog provides an appealing mix of features for implementing SMT-based static analyses (e.g., refinement type checking, symbolic execution) in a natural, declarative way. At the same time, the performance of its custom Datalog solver can be an impediment to using Formulog beyond prototyping—a common problem for Datalog variants that aspire to solve large problem instances. In this work we speed up Formulog evaluation, with some surprising results: while 2.2\texttimes{} speedups can be obtained by using the conventional techniques for high-performance Datalog (e.g., compilation, specialized data structures), the big wins come by abandoning the central assumption in modern performant Datalog engines, semi-naive Datalog evaluation. In the place of semi-naive evaluation, we develop eager evaluation, a concurrent Datalog evaluation algorithm that explores the logical inference space via a depth-first traversal order. In practice, eager evaluation leads to an advantageous distribution of Formulog’s SMT workload to external SMT solvers and improved SMT solving times: our eager evaluation extensions to the Formulog interpreter and Souffl\'{e}’s code generator achieve mean 5.2\texttimes{} and 7.6\texttimes{} speedups, respectively, over the optimized code generated by off-the-shelf Souffl\'{e} on SMT-heavy Formulog benchmarks.                                All in all, using compilation and eager evaluation (as appropriate), Formulog implementations of refinement type checking, bottom-up pointer analysis, and symbolic execution achieve speedups on 20 out of 23 benchmarks over previously published, hand-tuned analyses written in F♯, Java, and C++, providing strong evidence that Formulog can be the basis of a realistic platform for SMT-based static analysis. Moreover, our experience adds nuance to the conventional wisdom that traditional semi-naive evaluation is the one-size-fits-all best Datalog evaluation algorithm for static analysis workloads.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {314},
numpages = {30},
keywords = {Datalog, Formulog, SMT solving, compilation, parallel evaluation}
}

@software{10.5281/zenodo.13372573,
author = {Bembenek, Aaron and Greenberg, Michael and Chong, Stephen},
title = {Making Formulog Fast: An Argument for Unconventional Datalog Evaluation (OOPSLA'24 Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13372573},
abstract = {
    <p>Welcome to the artifact for the paper “Making Formulog Fast: An
Argument for Unconventional Datalog Evaluation” (OOPSLA’24) by Aaron
Bembenek, Michael Greenberg, and Stephen Chong. This artifact was
reviewed by the OOPSLA’24 Artifact Evaluation Committee. The artifact
includes: our extensions to Formulog and Souffl\'{e}; benchmarks and
experimental infrastructure; the data from our experiments and our data
analysis scripts; and documentation on how to build on top of our
software. To use the artifact, download the archive, extract it, and
follow the instructions in the README. If you just want to run Formulog,
you can get Formulog directly from the GitHub repo
(https://github.com/HarvardPL/formulog).</p>

},
keywords = {compilation, Datalog, Formulog, parallel evaluation, SMT solving}
}

@article{10.1145/3689755,
author = {Wagner, Andrew and Eisbach, Zachary and Ahmed, Amal},
title = {Realistic Realizability: Specifying ABIs You Can Count On},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689755},
doi = {10.1145/3689755},
abstract = {The Application Binary Interface (ABI) for a language defines the interoperability rules for its target platforms, including data layout and calling conventions, such that compliance with the rules ensures “safe” execution and perhaps certain resource usage guarantees. These rules are relied upon by compilers, libraries, and foreign-function interfaces. Unfortunately, ABIs are typically specified in prose, and while type systems for source languages have evolved, ABIs have comparatively stalled, lacking advancements in expressivity and safety.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
We propose a vision for richer, semantic ABIs to improve interoperability and library integration, supported by a methodology for formally specifying ABIs using realizability models. These semantic ABIs connect abstract, high-level types to unwieldy, but well-behaved, low-level code. We illustrate our approach with a case study formalizing the ABI of a functional source language in terms of a reference-counting implementation in a C-like target language. A key contribution supporting this case study is a graph-based model of separation logic that captures the ownership and accessibility of reference-counted resources using modalities inspired by hybrid logic. To highlight the flexibility of our methodology, we show how various design decisions can be interpreted into the semantic ABI. Finally, we provide the first formalization of library evolution, a distinguishing feature of Swift’s ABI.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {315},
numpages = {30},
keywords = {application binary interfaces, logical relations, program logics, reference counting, semantics, separation logic, type soundness}
}

@article{10.1145/3689756,
author = {Dardinier, Thibault and Li, Anqi and M\"{u}ller, Peter},
title = {Hypra: A Deductive Program Verifier for Hyper Hoare Logic},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689756},
doi = {10.1145/3689756},
abstract = {Hyperproperties relate multiple executions of a program and are useful to express common correctness properties (such as determinism) and security properties (such as non-interference). While there are a number of powerful program logics for the deductive verification of hyperproperties, their automation falls behind. Most existing deductive verification tools are limited to safety properties, but cannot reason about the existence of executions, for instance, to prove the violation of a safety property. Others support more flexible hyperproperties such as generalized non-interference, but have limitations in terms of the programs and proof structures they support.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In this paper, we present the first deductive verification technique for arbitrary hyperproperties over multiple executions of the same program. Our technique automates the generation of verification conditions for Hyper Hoare Logic. Our key insight is that arbitrary hyperproperties and the corresponding proof rules can be encoded into a standard intermediate verification language by representing sets of states of the input program explicitly in the states of the intermediate program. Verification is then automated using an existing SMT-based verifier for the intermediate language. We implement our technique in a tool called Hypra and demonstrate that it can reliably verify complex hyperproperties.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {316},
numpages = {30},
keywords = {Deductive Verification, Hyperproperties, Incorrectness Logic}
}

@software{10.5281/zenodo.12671562,
author = {Dardinier, Thibault and Li, Anqi and M\"{u}ller, Peter},
title = {Hypra: A Deductive Program Verifier for Hyperproperties (artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12671562},
abstract = {
    <p>This artifact consists of: - Our tool Hypra. - Our evaluation, with
instructions to replicate it. - An Isabelle/HOL proof of the soundness
of the novel loop rule described in section 4.2 (Theorem 1), as well as
Lemma 1.</p>
<p>The artifact is a VirtualBox VM image with Ubuntu 24.04 LTS that
contains our tool Hypra, all benchmarks used in our evaluation, Isabelle
2024, and our Isabelle/HOL formalization. It uses 8GB of RAM and two
cores by default.</p>

},
keywords = {automated reasoning, deductive verification, hyperproperties, incorrectness logic}
}

@article{10.1145/3689757,
author = {Zhou, Chijin and Qian, Bingzhou and Go, Gwihwan and Zhang, Quan and Li, Shanshan and Jiang, Yu},
title = {PolyJuice: Detecting Mis-compilation Bugs in Tensor Compilers with Equality Saturation Based Rewriting},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689757},
doi = {10.1145/3689757},
abstract = {Tensor compilers are essential for deploying deep learning applications across various hardware platforms. While powerful, they are inherently complex and present significant challenges in ensuring correctness. This paper introduces PolyJuice, an automatic detection tool for identifying mis-compilation bugs in tensor compilers. Its basic idea is to construct semantically-equivalent computation graphs to validate the correctness of tensor compilers. The main challenge is to construct equivalent graphs capable of efficiently exploring the diverse optimization logic during compilation. We approach it from two dimensions. First, we propose arithmetic and structural equivalent rewrite rules to modify the dataflow of a tensor program. Second, we design an efficient equality saturation based rewriting framework to identify the most simplified and the most complex equivalent computation graphs for an input graph. After that, the outcome computation graphs have different dataflow and will likely experience different optimization processes during compilation. We applied it to five well-tested industrial tensor compilers, namely PyTorch Inductor, OnnxRuntime, TVM, TensorRT, and XLA, as well as two well-maintained academic tensor compilers, EinNet and Hidet. In total, PolyJuice detected 84 non-crash mis-compilation bugs, out of which 49 were confirmed with 20 fixed.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {317},
numpages = {27},
keywords = {Equality Saturation, Fuzzing, ML System, Tensor Compiler Testing}
}

@software{10.5281/zenodo.12671619,
author = {Zhou, Chijin and Qian, Bingzhou and Go, Gwihwan and Zhang, Quan and Li, Shanshan and Jiang, Yu},
title = {PolyJuice: Detecting Mis-Compilation Bugs in Tensor Compilers with Equality Saturation Based Rewriting},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12671619},
abstract = {
    <p>This is the artifact for “PolyJuice: Detecting Mis-Compilation Bugs
in Tensor Compilers with Equality Saturation Based Rewriting”, published
in SPLASH/OOPSLA 2024. Reproducibility instructions can be found in the
“oopsla24-polyjuice-artifact.pdf” file.</p>

},
keywords = {Equality Saturation, Fuzzing, ML System, Tensor Compiler Testing}
}

@article{10.1145/3689758,
author = {Guan, Zhichao and Cao, Yiyuan and Yu, Tailai and Wang, Ziheng and Wang, Di and Hu, Zhenjiang},
title = {Semantics Lifting for Syntactic Sugar},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689758},
doi = {10.1145/3689758},
abstract = {Syntactic sugar plays a crucial role in engineering programming languages. It offers convenient syntax and higher-level of abstractions, as witnessed by its pervasive use in both general-purpose and domain-specific contexts. Unfortunately, the traditional approach of translating programs containing syntactic sugars into the host language can lead to abstraction leakage, breaking the promise of convenience and hindering program comprehension. To address this challenge, we introduce the idea of semantics lifting that aims to statically derive self-contained evaluation rules for syntactic sugars. More specifically, we propose a semantics-lifting framework that consists of (i) a general algorithm for deriving host-independent semantics of syntactic sugars from the semantics of the host language and the desugaring rules, (ii) a formulation of the correctness and abstraction properties for a lifted semantics, and (iii) a systematic investigation of sufficient conditions that ensure a lifted semantics is provably correct and abstract. To evaluate our semantics-lifting framework, we have implemented a system named Osazone and conducted several case studies, demonstrating that our approach is flexible, effective, and practical for implementing domain-specific languages.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {318},
numpages = {26},
keywords = {Domain-Specific Languages, Programming Languages, Syntactic Sugar}
}

@software{10.5281/zenodo.13626469,
author = {Guan, Zhichao and Cao, Yiyuan and Yu, Tailai and Wang, Ziheng and Wang, Di and Hu, Zhenjiang},
title = {Artifact for OOPSLA'24: Semantics Lifting for Syntactic Sugar},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13626469},
abstract = {
    <p>Artifact for OOPSLA’24: Semantics Lifting for Syntactic Sugar. This
project will be maintained at
https://github.com/vbcpascal/Osazone-oopsla24.</p>

},
keywords = {Domain-specific Languages, Programming Language, Syntactic Sugar}
}

@article{10.1145/3689759,
author = {Ding, Boyao and Li, Qingwei and Zhang, Yu and Tang, Fugen and Chen, Jinbao},
title = {MEA2: A Lightweight Field-Sensitive Escape Analysis with Points-to Calculation for Golang},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689759},
doi = {10.1145/3689759},
abstract = {Escape analysis plays a crucial role in garbage-collected languages as it enables the allocation of non-escaping variables on the stack by identifying the dynamic lifetimes of objects and pointers. This helps in reducing heap allocations and alleviating garbage collection pressure. However, Go, as a garbage-collected language, employs a fast yet conservative escape analysis, which is field-insensitive and omits point-to-set calculation to expedite compilation. This results in more variables being allocated on the heap. Empirical statistics reveal that field access and indirect memory access are prevalent in real-world Go programs, suggesting potential opportunities for escape analysis to enhance program performance. In this paper, we propose MEA2, an escape analysis framework atop GoLLVM (an LLVM-based Go compiler), which combines field sensitivity and points-to analysis. Moreover, a novel generic function summary representation is designed to facilitate fast inter-procedural analysis. We evaluated it by using MEA2 to perform stack allocation in 12 wildly-use open-source projects. The results show that, compared to Go’s escape analysis, MEA2 can reduce heap allocation sites by 7.9},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {319},
numpages = {28},
keywords = {Golang, escape analysis, field sensitive}
}

@article{10.1145/3689760,
author = {Sistla, Meghana and Chaudhuri, Swarat and Reps, Thomas},
title = {Weighted Context-Free-Language Ordered Binary Decision Diagrams},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689760},
doi = {10.1145/3689760},
abstract = {This paper presents a new data structure, called Weighted Context-Free-Language Ordered BDDs (WCFLOBDDs), which are a hierarchically structured decision diagram, akin to Weighted BDDs (WBDDs) enhanced with a procedure-call mechanism. For some functions, WCFLOBDDs are exponentially more succinct than WBDDs. They are potentially beneficial for representing functions of type Bn → D, when a function’s image V ⊆ D has many different values. We apply WCFLOBDDs in quantum-circuit simulation, and find that they perform better than WBDDs on certain benchmarks.                With a 15-minute timeout, the number of qubits that can be handled by WCFLOBDDs is 1-64\texttimes{} that of WBDDs(and 1-128\texttimes{} that of CFLOBDDs, which are an unweighted version of WCFLOBDDs). These results support the conclusion that for this application—from the standpoint of problem size, measured as the number of qubits—WCFLOBDDs provide the best of both worlds: performance roughly matches whichever of WBDDs and CFLOBDDs is better.(From the standpoint of running time, the results are more nuanced.)},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {320},
numpages = {30},
keywords = {Weighted decision diagram, best-case double-exponential compression, matched paths, quantum simulation}
}

@software{10.5281/zenodo.12670155,
author = {Sistla, Meghana and Chaudhuri, Swarat and Reps, Thomas},
title = {Weighted CFLOBDDs},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12670155},
abstract = {
    <p>The artifact contains source code and experiments to run Weighted
CFLOBDDs (WCFLOBDDs)</p>

},
keywords = {quantum simulation, Weighted decision diagrams}
}

@article{10.1145/3689761,
author = {Correnson, Arthur and Nie\ss{}en, Tobias and Finkbeiner, Bernd and Weissenbacher, Georg},
title = {Finding ∀∃ Hyperbugs using Symbolic Execution},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689761},
doi = {10.1145/3689761},
abstract = {Many important hyperproperties, such as refinement and generalized non-interference, fall into the class of ∀∃ hyperproperties and require, for each execution trace of a system, the existence of another trace relating to the first one in a certain way. The alternation of quantifiers renders ∀∃ hyperproperties extremely difficult to verify, or even just to test. Indeed, contrary to trace properties, where it suffices to find a single counterexample trace, refuting a ∀∃ hyperproperty requires not only to find a trace, but also a proof that no second trace satisfies the specified relation with the first trace. As a consequence, automated testing of ∀∃ hyperproperties falls out of the scope of existing automated testing tools. In this paper, we present a fully automated approach to detect violations of ∀∃ hyperproperties in software systems. Our approach extends bug-finding techniques based on symbolic execution with support for trace quantification. We provide a prototype implementation of our approach, and demonstrate its effectiveness on a set of challenging examples.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {321},
numpages = {26},
keywords = {Bounded model checking, Hyperproperties, Infinite-state systems, Symbolic execution}
}

@article{10.1145/3689762,
author = {Hinrichsen, Jonas Kastberg and Jacobs, Jules and Krebbers, Robbert},
title = {Multris: Functional Verification of Multiparty Message Passing in Separation Logic},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689762},
doi = {10.1145/3689762},
abstract = {We introduce Multris, a separation logic for verifying functional correctness of programs that combine multiparty message-passing communication with shared-memory concurrency. The foundation of our work is a novel concept of multiparty protocol consistency, which guarantees safe communication among a set of parties, provided each party adheres to its prescribed protocol. Our concept of protocol consistency is inspired by the bottom-up approach for multiparty session types. However, by considering it in the context of separation logic instead of a type system, we go further in terms of generality by supporting new notions of implicit transfer of knowledge and implicit transfer of resources. We develop tactics for automatically verifying protocol consistency and for reasoning about message-passing operations in Multris. We evaluate Multris on a range of examples, including the well-known two- and three-buyer protocols, as well as a new verification benchmark based on Chang and Roberts's ring leader election protocol. To ensure the reliability of our work, we prove soundness of Multris w.r.t. a low-level channel semantics using the Iris framework in Coq.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {322},
numpages = {29},
keywords = {Coq, Iris, Message passing, multiparty, separation logic, session types}
}

@software{10.5281/zenodo.13380561,
author = {Hinrichsen, Jonas Kastberg and Jacobs, Jules and Krebbers, Robbert},
title = {Multris: Functional Verification of Multiparty Message Passing in Separation Logic - Coq Mechanization},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13380561},
abstract = {
    <p>Coq mechanization artifact for the OOPSLA’24 paper: “Multris:
Functional Verification of Multiparty Message Passing in Separation
Logic”.</p>
<p>See included README for more details.</p>

},
keywords = {Coq, Mechanization, Separation Logic}
}

@article{10.1145/3689763,
author = {Saffrich, Hannes and Nishida, Yuki and Thiemann, Peter},
title = {Law and Order for Typestate with Borrowing},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689763},
doi = {10.1145/3689763},
abstract = {Typestate systems are notoriously complex as they require
 
 
 
sophisticated machinery for tracking aliasing.
 
 
 
We propose a new, transition-oriented foundation for typestate in the
 
 
 
setting of impure functional programming.
 
 
 
Our approach relies on ordered types for simple alias tracking and
 
 
 
its formalization draws on work on bunched implications. 
 
 
 
Yet, we support a flexible notion of borrowing in the presence of typestate.
 
 
 

 
 
 
Our core calculus comes with a notion of resource types indexed by an
 
 
 
ordered partial monoid that models abstract state transitions. 
 
 
 
We prove syntactic type soundness with respect to a
 
 
 
resource-instrumented semantics.
 
 
 
We give an algorithmic version of our type system and prove its
 
 
 
soundness. Algorithmic typing facilitates a
 
 
 
simple surface language that does not expose tedious details of
 
 
 
ordered types. We implemented a typechecker for the surface language
 
 
 
along with an interpreter for the core language.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {323},
numpages = {29},
keywords = {borrowing, ownership, resource typing, substructural typing, typestate}
}

@article{10.1145/3689764,
author = {Yi, Xin and Yu, Hengbiao and Chen, Liqian and Mao, Xiaoguang and Wang, Ji},
title = {FPCC: Detecting Floating-Point Errors via Chain Conditions},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689764},
doi = {10.1145/3689764},
abstract = {Floating-point arithmetic is notorious for its rounding errors, which can propagate and accumulate, leading to unacceptable results. Detecting inputs that can trigger significant floating-point errors is crucial for enhancing the reliability of numerical programs. Existing methods for generating error-triggering inputs often rely on costly shadow executions that involve high-precision computations or suffer from false positives. This paper introduces chain conditions to capture the propagation and accumulation of floating-point errors, using them to guide the search for error-triggering inputs. We have implemented a tool named FPCC and evaluated it on 88 functions from the GNU Scientific Library, as well as 21 functions with multiple inputs from previous research. The experimental results demonstrate the effectiveness and efficiency of our approach: (1) FPCC achieves 100\% accuracy in detecting significant errors for the reported rank-1 inputs, while 72.69\% rank-1 inputs from the state-of-the-art tool ATOMU can trigger significant errors. Overall, 99.64\% (1049/1053) of the inputs reported by FPCC can trigger significant errors, whereas only 19.45\% (141/723) of the inputs reported by ATOMU can trigger significant errors; (2) FPCC exhibits a 2.17x speedup over ATOMU in detecting significant errors; (3) FPCC also excels in supporting functions with multiple inputs, outperforming the state-of-the-art technique. To facilitate further research in the community, we have made FPCC available on GitHub at https://github.com/DataReportRe/FPCC.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {324},
numpages = {28},
keywords = {accuracy, chain condition, error-triggering input, floating-point error}
}

@software{10.5281/zenodo.13618683,
author = {Yi, Xin and Yu, Hengbiao and Chen, Liqian and Mao, Xiaoguang and Wang, Ji},
title = {FPCC: Detecting Floating-Point Errors via Chain Conditions (Paper Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13618683},
abstract = {
    <p>This is the artifact for “FPCC: Detecting Floating-Point Errors via
Chain Conditions”, published in SPLASH/OOPSLA 2024. All instructions can
be found in the zip file.</p>

},
keywords = {accuracy, chain condition, error-triggering input, floating-point error}
}

@article{10.1145/3689765,
author = {Yan, Zhenyu and Zhang, Xin and Di, Peng},
title = {Scaling Abstraction Refinement for Program Analyses in Datalog using Graph Neural Networks},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689765},
doi = {10.1145/3689765},
abstract = {Counterexample-guided abstraction refinement (CEGAR) is a popular approach for automatically selecting abstractions with high precision and low time costs. Existing works cast abstraction refinements as constraint-solving problems. Due to the complexity of these problems, they cannot be scaled to large programs or complex analyses. We propose a novel approach that applies graph neural networks to improve the scalability of CEGAR for Datalog-based program analyses. By constructing graphs directly from the Datalog solver’s calculations, our method then uses a neural network to score abstraction parameters based on the information in these graphs. Then we reform the constraint problems such that the constraint solver ignores parameters with low scores. This in turn reduces the solution space and the size of the constraint problems. Since our graphs are directly constructed from Datalog computation without human effort, our approach can be applied to a broad range of parametric static analyses implemented in Datalog. We evaluate our approach on a pointer analysis and a typestate analysis and our approach can answer 2.83\texttimes{} and 1.5\texttimes{} as many queries as the baseline approach on large programs for the pointer analysis and the typestate analysis, respectively.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {325},
numpages = {29},
keywords = {abstraction refinement, graph neural networks, program analysis}
}

@software{10.5281/zenodo.12663344,
author = {Yan, Zhenyu and Zhang, Xin and Di, Peng},
title = {Scaling Abstraction Refinement for Program Analyses in Datalog Using Graph Neural Networks (Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12663344},
abstract = {
    <p>This is the artifact of the paper Scaling Abstraction Refinement for
Program Analyses in Datalog Using Graph Neural Networks to appear in
OOPSLA 2024.</p>

},
keywords = {abstraction refinement, graph neural networks, program analysis}
}

@article{10.1145/3689766,
author = {Liu, Zhengyang and Mada, Stefan and Regehr, John},
title = {Minotaur: A SIMD-Oriented Synthesizing Superoptimizer},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689766},
doi = {10.1145/3689766},
abstract = {A superoptimizing compiler—-one that performs a meaningful search of the program space as part of the optimization process—-can find optimization opportunities that are missed by even the best existing optimizing compilers. We created Minotaur: a superoptimizer for LLVM that uses program synthesis to improve its code generation, focusing on integer and floating-point SIMD code. On an Intel Cascade Lake processor, Minotaur achieves an average speedup of 7.3\% on the GNU Multiple Precision library (GMP)’s benchmark suite, with a maximum speedup of 13\%. On SPEC CPU 2017, our superoptimizer produces an average speedup of 1.5\%, with a maximum speedup of 4.5\% for 638.imagick. Every optimization produced by Minotaur has been formally verified, and several optimizations that it has discovered have been implemented in LLVM as a result of our work.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {326},
numpages = {25},
keywords = {SIMD, peephole optimization, program synthesis, superoptimization}
}

@article{10.1145/3689767,
author = {Klopp, David and Erdweg, Sebastian and Pacak, Andr\'{e}},
title = {A Typed Multi-level Datalog IR and Its Compiler Framework},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689767},
doi = {10.1145/3689767},
abstract = {The resurgence of Datalog in the last two decades has led to a multitude of new Datalog systems.
 
These systems explore novel ideas for improving Datalog's programmability and performance, making important contributions to the field. 
 
Unfortunately, the individual systems progress at a much slower pace than the overall field, because improvements in one system are rarely ported to other systems. 
 
The reason for this rift is that each system provides its own Datalog dialect with specific notation, language features, and invariants, enabling specific optimization and execution strategies.
 
 
 
This paper presents the first compiler framework for Datalog that can be used to support any Datalog frontend language and to target any Datalog backend.
 
The centerpiece of our framework is a novel typed multi-level Datalog IR that supports IR extensions and guarantees executability.
 
Existing Datalog systems can provide a compiler frontend that translates their Datalog dialect to the extended IR.
 
The IR is then progressively lowered toward core Datalog, allowing optimizations at each level.
 
At last, compiler backends can target different Datalog solvers.
 
We have implemented the compiler framework and integrated 4 Datalog frontends and 3 Datalog backends, using 16 IR extensions.
 
We also formalize the IR's flexible type system, which is bidirectional, flow-sensitive, bipolar, and uses three-valued typing contexts.
 
The type system simultaneously validates type compatibility and precisely tracks bindings of logic variables while permitting IR extensions.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {327},
numpages = {29},
keywords = {Datalog, compiler framework, multi-level IR, type system}
}

@article{10.1145/3689768,
author = {Zhang, Yiyu and Liu, Tianyi and Wang, Yueyang and Qi, Yun and Ji, Kai and Tang, Jian and Wang, Xiaoliang and Li, Xuandong and Zuo, Zhiqiang},
title = {HardTaint: Production-Run Dynamic Taint Analysis via Selective Hardware Tracing},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689768},
doi = {10.1145/3689768},
abstract = {Dynamic taint analysis (DTA), as a fundamental analysis technique, is widely used in security, privacy, and diagnosis, etc. As DTA demands to collect and analyze massive taint data online, it suffers extremely high runtime overhead. Over the past decades, numerous attempts have been made to lower the overhead of DTA. Unfortunately, the reductions they achieved are marginal, causing DTA only applicable to the debugging/testing scenarios. In this paper, we propose and implement HardTaint, a system that can realize production-run dynamic taint tracking. HardTaint adopts a hybrid and systematic design which combines static analysis, selective hardware tracing and parallel graph processing techniques. The comprehensive evaluations demonstrate that HardTaint introduces only around 8\% runtime overhead which is an order of magnitude lower than the state-of-the-arts, while without sacrificing any taint detection capability.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {328},
numpages = {26},
keywords = {Dynamic Taint Analysis, Intel PT, Selective Tracing}
}

@software{10.5281/zenodo.13117983,
author = {Zhang, Yiyu and Liu, Tianyi and Wang, Yueyang and Qi, Yun and Ji, Kai and Tang, Jian and Wang, Xiaoliang and Li, Xuandong and Zuo, Zhiqiang},
title = {Artifact Package for Article 'HardTaint: Production-Run Dynamic Taint Analysis via Selective Hardware Tracing'},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13117983},
abstract = {
    <p>The artifact is used to claim that the main components proposed in
the paper “HardTaint: Production-Run Dynamic Taint Analysis via
Selective Hardware Tracing” are functional.</p>

},
keywords = {Dynamic Taint Analysis, Intel PT, Selective Tracing}
}

@article{10.1145/3689769,
author = {Schwartz, David and Kowshik, Ankith and Pina, Lu\'{\i}s},
title = {Jmvx: Fast Multi-threaded Multi-version Execution and Record-Replay for Managed Languages},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689769},
doi = {10.1145/3689769},
abstract = {Multi-Version eXecution (MVX) is a technique that deploys many equivalent versions of the same program — variants — as a single program, with direct applications in important fields such as: security, reliability, analysis, and availability. MVX can be seen as “online Record/Replay (RR)”, as RR captures a program’s execution as a log stored on disk that can later be replayed to observe the same execution. Unfortunately, current MVX techniques target programs written in C/C++ and do not support programs written in managed languages, which are the vast majority of code written nowadays.
 
 
 
This paper presents the design, implementation, and evaluation of Jmvx— a novel system for performing MVX and RR on programs written in managed languages. Jmvx supports programs written in Java by intercepting automatically identified non-deterministic methods, via a novel dynamic analysis technique, and ensuring that all variants execute the same methods and obtain the same data. 
 
 
 
Jmvx supports multi-threaded programs, by capturing synchronization operations in one variant, and ensuring all other variants follow the same ordering. We validated that Jmvx supports MVX and RR by applying it to a suite of benchmarks representative of programs written in Java. Internally, Jmvx uses a circular buffer located in shared memory between JVMs to enable fast communication between all variants, averaging 5\% |47\% performance overhead when performing MVX with multithreading support disabled|enabled, 8\% |25\% when recording, and 13\% |73\% when replaying.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {329},
numpages = {29},
keywords = {Multi-version execution, deterministic replay, record replay, reproducible debugging}
}

@software{10.5281/zenodo.12637140,
author = {Schwartz, David and Kowshik, Ankith and Pina, Lu\'{\i}s},
title = {Artifact for Jmvx: Fast Multi-threaded Multi-Version eXecution and Record-Replay for Managed Languages},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12637140},
abstract = {
    <p>Java Multi-Version Execution (JMVX) is a tool for performing
Multi-Version Execution (MVX) and Record Replay (RR) in Java.</p>
<p>Most tools for MVX and RR observe the behavior of a program at a low
level, e.g., by looking at system calls.</p>
<p>Unfortunately, this approach fails for high level language virtual
machines due to benign divergences (differences in behavior that
accomplish that same result) introduced by the virtual machine –
particularly by garbage collection and just-in-time compilation.</p>
<p>In other words, the management of the virtual machines creates
differing sequences of system calls that lead existing tools to believe
a program has diverged, when in practice, the application running on top
of the VM has not.</p>
<p>JMVX takes a different approach, opting instead to add MVX and RR
logic into the bytecode of compiled programs running in the VM to avoid
benign divergences related to VM management.</p>
<p>This artifact is a docker image that will create a container holding
our source code, compiled system, and experiments with JMVX.</p>
<p>The image allows you to run the experiments we used to address the
research questions from the paper (from Section 4).</p>
<p>This artifact is desiged to show:</p>
<p>[Supported] JMVX performs MVX for Java [Supported] JMVX performs RR
for Java [Supported] JMVX is performant</p>

},
keywords = {deterministic replay, Multi-version execution, record replay, reproducible debugging}
}

@article{10.1145/3689770,
author = {Ma, Cong and Ge, Zhaoyi and Lee, Edward and Zhang, Yizhou},
title = {Lexical Effect Handlers, Directly},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689770},
doi = {10.1145/3689770},
abstract = {Lexically scoping effect handlers is a language-design idea that equips algebraic effects with a modular semantics: it enables local-reasoning principles without giving up on the control-flow expressiveness that makes effect handlers powerful. However, we observe that existing implementations risk incurring costs akin to the run-time search for dynamically scoped handlers. This paper presents a compilation strategy for lexical effect handlers, adhering to the lexical scoping principle and targeting a language with low-level control over stack layout. Key aspects of this approach are formalized and proven correct. We embody the ideas in a language called Lexa: the Lexa compiler translates high-level effect handling to low-level stack switching. We evaluate the Lexa compiler on a set of benchmarks; the results suggest that it generates efficient code, reducing running-time complexity from quadratic to linear in some cases.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {330},
numpages = {29},
keywords = {Algebraic effects, Lexa, Salt, compiler correctness, continuations, effect handlers, stack switching}
}

@software{10.5281/zenodo.13770453,
author = {Ma, Cong and Ge, Zhaoyi and Lee, Edward and Zhang, Yizhou},
title = {Lexical Effect Handlers, Directly (artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13770453},
abstract = {
    <p>This is the artifact accompanying the paper
<code>Lexical Effect Handlers, Directly</code>.</p>

},
keywords = {Algebraic effects, compiler correctness, continuations, effect handlers, Lexa, Salt, stack switching}
}

@article{10.1145/3689771,
author = {Le Glaunec, Alexis and Kong, Lingkun and Mamouras, Konstantinos},
title = {HybridSA: GPU Acceleration of Multi-pattern Regex Matching using Bit Parallelism},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689771},
doi = {10.1145/3689771},
abstract = {Multi-pattern matching is widely used in modern software for applications requiring high throughput such as protein search, network traffic inspection, virus or spam detection. Graphics Processor Units (GPUs) excel at executing massively parallel workloads. Regular expression (regex) matching is typically performed by simulating the execution of deterministic finite automata (DFAs) or nondeterministic finite automata (NFAs). The natural implementations of these automata simulation algorithms on GPUs are highly inefficient because they give rise to irregular memory access patterns.
 

 
This paper presents HybridSA, a heterogeneous CPU-GPU parallel engine for multi-pattern matching. HybridSA uses bit parallelism to efficiently simulate NFAs on GPUs, thus reducing the number of memory accesses and increasing the throughput. Our bit-parallel algorithms extend the classical shift-and algorithm for string matching to a large class of regular expressions and reduce automata simulation to a small number of bitwise operations. We have developed a compiler to translate regular expressions into bit masks, perform optimizations, and choose the best algorithms to run on the GPU. The majority of the regular expressions are accelerated on the GPU, while the patterns that exhibit random memory accesses are executed on the CPU in parallel. We evaluate HybridSA against state-of-the-art CPU and GPU engines, as well as a hybrid combination of the two. HybridSA achieves between 4 and 60 times higher throughput than the state-of-the-art CPU engine and between 4 and 233 times better than the state-of-the-art GPU engine across a collection of real-world benchmarks.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {331},
numpages = {30},
keywords = {CUDA, bit parallelism, regex matching, regular expressions, shift-and algorithm}
}

@article{10.1145/3689772,
author = {Eymer, Jeff and Dexter, Philip and Raskind, Joseph and Liu, Yu David},
title = {A Runtime System for Interruptible Query Processing: When Incremental Computing Meets Fine-Grained Parallelism},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689772},
doi = {10.1145/3689772},
abstract = {Online data services have stringent performance requirement and must tolerate workload fluctuation. This paper introduces PitStop, a new query language runtime design built on the idea of interruptible query processing: the time-consuming task of data inspection for processing each query or update may be interrupted and resumed later at the boundary of fine-grained data partitions. This counter-intuitive idea enables a novel form of fine-grained concurrency while preserving sequential consistency. We build PitStop through modifying the language runtime of Cypher, the query language of a state-of-the-art graph database, Neo4j. Our evaluation on the Google Cloud shows that PitStop can outperform unmodified Neo4j during workload fluctuation, with reduced latency and increased throughput.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {332},
numpages = {28},
keywords = {fine-grained parallelism, interruptible query processing, query language runtime design}
}

@software{10.5281/zenodo.13372050,
author = {Eymer, Jeff and Dexter, Philip and Raskind, Joseph and Liu, Yu David},
title = {A Runtime System for Interruptible Query Processing: When Incremental Computing Meets Fine-Grained Parallelism - Artifact},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13372050},
abstract = {
    <p>The goal of PitStop is to use lazy evaluation to improve both
throughput and operation latency for data processing systems. PitStop
allows for queries and updates to be temporarily halted mid-traversal
and continued at a later time. Though it may seem counter intuitive,
this pausing allows for a fine-grained parallelism that helps to balance
workload and alleviate workload fluctuation. This design also allows for
further query optimization through the use of techniques such as
batching and fusion.</p>

},
keywords = {fine-grained parallelism, interruptible query processing, query language runtime design}
}

@article{10.1145/3689773,
author = {Reitz, Antonin and Fromherz, Aymeric and Protzenko, Jonathan},
title = {StarMalloc: Verifying a Modern, Hardened Memory Allocator},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689773},
doi = {10.1145/3689773},
abstract = {We present StarMalloc, a verified, efficient, security-oriented, and concurrent memory allocator. Using the Steel separation logic framework, we show how to specify and verify a multitude of low-level patterns and delicate security mechanisms, by relying on a combination of dependent types, SMT, and modular abstractions to enable efficient verification. We produce a verified artifact, in C, that implements the entire API surface of an allocator, and as such works as a drop-in replacement for real-world projects, notably the Firefox browser.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
As part of StarMalloc, we develop several generic datastructures and proof libraries directly reusable in future systems verification projects. We also extend the Steel toolchain to express several low-level idioms that were previously missing. Finally, we show that StarMalloc exhibits competitive performance by evaluating it against 10 state-of-the-art memory allocators, and against a variety of real-world projects, such as Redis, the Lean compiler, and the Z3 SMT solver.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {333},
numpages = {30},
keywords = {Formal Verification, Memory Allocators, Separation Logic}
}

@software{10.5281/zenodo.12670476,
author = {Reitz, Antonin and Fromherz, Aymeric and Protzenko, Jonathan},
title = {Artifact for OOPSLA 2024 paper: StarMalloc: Verifying a Modern, Hardened Memory Allocator},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12670476},
abstract = {
    <p>This is the artifact for the OOPSLA 2024 paper: StarMalloc: Verifying
a Modern, Hardened Memory Allocator. It includes the F* sources for
StarMalloc, the generated C code, and the experimental setup to
reproduce experiments presented in the paper.</p>

},
keywords = {Formal Verification, Memory Allocators, Separation Logic}
}

@article{10.1145/3689774,
author = {Schenck, Robert and Hinnerskov, Nikolaj Hey and Henriksen, Troels and Madsen, Magnus and Elsman, Martin},
title = {AUTOMAP: Inferring Rank-Polymorphic Function Applications with Integer Linear Programming},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689774},
doi = {10.1145/3689774},
abstract = {Dynamically typed array languages such as Python, APL, and Matlab lift scalar operations to arrays and replicate scalars to fit applications. We present a mechanism for automatically inferring map and replicate operations in a statically-typed language in a way that resembles the programming experience of a dynamically-typed language while preserving the static typing guarantees. Our type system---which supports parametric polymorphism, higher-order functions, and top-level let-generalization---makes use of integer linear programming in order to find the minimum number of operations needed to elaborate to a well-typed program. We argue that the inference system provides useful and unsurprising guarantees to the programmer. We demonstrate important theoretical properties of the mechanism and report on the implementation of the mechanism in the statically-typed array programming language Futhark.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {334},
numpages = {27},
keywords = {array programming, constraint-based type systems, data parallelism}
}

@software{10.5281/zenodo.12775308,
author = {Schenck, Robert and Hinnerskov, Nikolaj Hey and Henriksen, Troels and Madsen, Magnus and Elsman, Martin},
title = {futhark-oopsla24},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12775308},
abstract = {
    <p>Artifact for the paper AUTOMAP: Inferring Rank-Polymorphic Function
Applications with Integer Linear Programming submitted to OOPSLA 24.</p>

},
keywords = {array programming, constraint-based type systems, data parallelism}
}

@article{10.1145/3689775,
author = {Le, Callista and Gopinathan, Kiran and Lee, Koon Wen and Gilbert, Seth and Sergey, Ilya},
title = {Concurrent Data Structures Made Easy},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689775},
doi = {10.1145/3689775},
abstract = {Design of an efficient thread-safe concurrent data structure is a balancing act between its implementation complexity and performance. Lock-based concurrent data structures, which are relatively easy to derive from their sequential counterparts and to prove thread-safe, suffer from poor throughput under even light multi-threaded workload. At the same time, lock-free concurrent structures allow for high throughput, but are notoriously difficult to get right and require careful reasoning to formally establish their correctness.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
In this work, we explore a solution to this conundrum based on a relatively old idea of batch parallelism---an approach for designing high-throughput concurrent data structures via a simple insight: efficiently processing a batch of a priori known operations in parallel is easier than optimising performance for a stream of arbitrary asynchronous requests. Alas, batch-parallel structures have not seen wide practical adoption due to (i) the inconvenience of having to structure multi-threaded programs to explicitly group operations and (ii) the lack of a systematic methodology to implement batch-parallel structures as simply as lock-based ones.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
We present OBatcher---a Multicore OCaml library that streamlines the design, implementation, and usage of batch-parallel structures. OBatcher solves the first challenge (how to use) by suggesting a new lightweight implicit batching design pattern that is built on top of generic asynchronous programming mechanisms. The second challenge (how to implement) is addressed by identifying a family of strategies for converting common sequential structures into the corresponding efficient batch-parallel versions, and by providing a library of functors that embody those strategies. We showcase OBatcher with a diverse set of benchmarks ranging from Red-Black and AVL trees to van Emde Boas trees, skip lists, and a thread-safe implementation of a Datalog solver. Our evaluation of all the implementations on large asynchronous workloads shows that (a) they consistently outperform the corresponding coarse-grained lock-based implementations---the only ones available in OCaml to date, and that (b) their throughput scales reasonably with the number of processors.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {335},
numpages = {29},
keywords = {Multicore OCaml, batch parallelism, shared-memory concurrency}
}

@software{10.5281/zenodo.12604575,
author = {Le, Callista and Gopinathan, Kiran and Lee, Koon Wen and Gilbert, Seth and Sergey, Ilya},
title = {OBatcher: Implementation, Data Structures, and Experiments (OOPSLA'24 Artefact) Creators},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12604575},
abstract = {
    <p>This release snapshots the functionality of the submitted artefact
for the OBatcher framework and data structures described in the OOPSLA
24 paper “Concurrent Data Structures Made Easy”:</p>
<ul>
<li><p>Docker file with reproducible build environment</p></li>
<li><p>Readme with getting started and step-by-step
instructions</p></li>
<li><p>Source code and build files for OBatcher</p></li>
<li><p>Instantiation of the OBatcher framework in Rust</p></li>
<li><p>Benchmark code and scripts to reproduce the 33 graphs presented
in the paper</p></li>
</ul>

},
keywords = {batched data structures, batching, concurrency, OCaml, programming languages, Rust}
}

@article{10.1145/3689776,
author = {Li, Ningke and Li, Yuekang and Liu, Yi and Shi, Ling and Wang, Kailong and Wang, Haoyu},
title = {Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689776},
doi = {10.1145/3689776},
abstract = {Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations — coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges: 1) Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases. 2) Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations.    To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth.    Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7\% to 59.8\%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations.    To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {336},
numpages = {30},
keywords = {Hallucination, Large Language Model, Software Testing}
}

@article{10.1145/3689777,
author = {Pimpalkhare, Nikhil and Kincaid, Zachary},
title = {Monotone Procedure Summarization via Vector Addition Systems and Inductive Potentials},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689777},
doi = {10.1145/3689777},
abstract = {This paper presents a technique for summarizing recursive procedures operating on integer variables. The motivation of our work is to create more predictable program analyzers, and in particular to formally guarantee compositionality and monotonicity of procedure summarization.  To summarize a procedure, we compute its best abstraction as a vector addition system with resets (VASR) and exactly summarize the executions of this VASR over the context-free language of syntactic paths through the procedure.   We improve upon this technique by refining the language of syntactic paths using (automatically synthesized) linear potential functions that bound the number of recursive calls within valid executions of the input program.   We implemented our summarization technique in an automated program verification tool; our experimental evaluation demonstrates that our technique computes more precise summaries than existing abstract interpreters and that our tool’s verification capabilities are comparable with state-of-the-art software model checkers.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {337},
numpages = {27},
keywords = {Formal Methods, Invariant Generation, Program Analysis}
}

@software{10.5281/zenodo.12786846,
author = {Pimpalkhare, Nikhil and Kincaid, Zachary},
title = {Virtual Machine Artifact for "Monotone Procedure Summarization via Vector Addition Systems and Inductive Potentials"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12786846},
abstract = {
    <p>UTM Virtual Machine for replicating Evaluation of “Monotone Procedure
Summarization via Vector Addition Systems and Inductive Potentials”</p>

},
keywords = {Abstract Machines, Program Analysis}
}

@article{10.1145/3689778,
author = {Enea, Constantin and Giannakopoulou, Dimitra and Kokologiannakis, Michalis and Majumdar, Rupak},
title = {Model Checking Distributed Protocols in Must},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689778},
doi = {10.1145/3689778},
abstract = {We describe the design and implementation of Must, a framework for
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 modeling and automatically verifying distributed systems. Must
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 provides a concurrency API that supports multiple communication
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 models, on top of a mainstream programming language, such as Rust.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Given a program using this API, Must verifies it by means of a
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 novel, optimal dynamic partial order reduction algorithm that
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 maintains completeness and optimality for all communication models
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 supported by the API.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 We use Must to design and verify models of distributed systems in
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 an industrial context. We demonstrate the usability of Must's API
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 by modeling high-level system idioms (e.g., timeouts, leader
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 election, versioning) as abstractions over the core API, and
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 demonstrate Must's scalability by verifying systems employed in
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 production (e.g., replicated logs, distributed transaction management
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 protocols), the verification of which lies beyond the capacity of
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 previous model checkers.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {338},
numpages = {28},
keywords = {Distributed Systems, Model Checking}
}

@article{10.1145/3689779,
author = {Borgarelli, Andrea and Enea, Constantin and Majumdar, Rupak and Nagendra, Srinidhi},
title = {Reward Augmentation in Reinforcement Learning for Testing Distributed Systems},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689779},
doi = {10.1145/3689779},
abstract = {Bugs in popular distributed protocol implementations have been the source of many downtimes in popular internet services. We describe a randomized testing approach for distributed protocol implementations based on reinforcement learning. Since the natural reward structure is very sparse, the key to successful exploration in reinforcement learning is reward augmentation. We show two different techniques that build on one another. First, we provide a decaying exploration bonus based on the discovery of new states---the reward decays as the same state is visited multiple times. The exploration bonus captures the intuition from coverage-guided fuzzing of prioritizing new coverage points; in contrast to other schemes, we show that taking the maximum of the bonus and the Q-value leads to more effective exploration. Second, we provide waypoints to the algorithm as a sequence of predicates that capture interesting semantic scenarios. Waypoints exploit designer insight about the protocol and guide the exploration to "interesting" parts of the state space. Our reward structure ensures that new episodes can reliably get to deep interesting states even without execution caching. We have implemented our algorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and RSL) shows that our algorithm can significantly outperform baseline approaches in terms of coverage and bug finding.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {339},
numpages = {27},
keywords = {Distributed Systems, Reactive Systems Testing, Reinforcement Learning}
}

@software{10.5281/zenodo.13166254,
author = {Borgarelli, Andrea and Enea, Constantin and Majumdar, Rupak and Nagendra, Srinidhi},
title = {Artifact for Reward Augmentation in Reinforcement Learning for Testing Distributed Systems &nbsp;Creators},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13166254},
abstract = {
    <p>The artifact provides a framework - WaypointRL - for testing
distributed protocol implementations using Reinforcement learning
methods and strategies. The framework expects an instrumented
implementation as input. The instrumentation will allow the framework to
simulate the network and pick the order of messages delivered.
Additionally, the framework will control the nodes of the distributed
system to introduce failures (stops and restarts). Apart from RL based
strategies when testing, the framework provides a generic interface to
implement any strategy as an Agent that interacts with the distributed
system Environment.</p>

},
keywords = {distributed systems, reactive systems testing, reinforcement learning}
}

@article{10.1145/3689780,
author = {Wang, Qian and Jung, Ralf},
title = {Rustlantis: Randomized Differential Testing of the Rust Compiler},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689780},
doi = {10.1145/3689780},
abstract = {Compilers are at the core of all computer architecture.  Their middle-end and back-end are full of subtle code that is easy to get wrong.  At the same time, the consequences of compiler bugs can be severe.  Therefore, it is important that we develop techniques to increase our confidence in compiler correctness, and to help find the bugs that inevitably happen.  One promising such technique that has successfully found many compiler bugs in the past is randomized differential testing, a fuzzing approach whereby the same program is executed with different compilers or different compiler settings to detect any unexpected differences in behavior.    We present Rustlantis, the first fuzzer for the Rust programming language that is able to find new correctness bugs in the official Rust compiler.  To avoid having to deal with Rust’s strict type and borrow checker, Rustlantis directly generates MIR, the central IR of the Rust compiler for optimizations.  The program generation strategy of Rustlantis is a combination of statically tracking the state of the program, obscuring the program state for the compiler, and decoy blocks to lead optimizations astray.  This has allowed us to identify 22 previously unknown bugs in the Rust compiler, most of which have been fixed.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {340},
numpages = {27},
keywords = {Compiler testing, Differential fuzzing, Rust}
}

@software{10.5281/zenodo.12670660,
author = {Wang, Qian and Jung, Ralf},
title = {Reproduction Image for Article 'Rustlantis: Randomized Differential Testing of the Rust Compiler'},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12670660},
abstract = {
    <p>Docker image to reproduce evaluations in OOPSLA 2024 paper
Rustlantis: Randomized Differential Testing of the Rust Compiler.</p>

},
keywords = {Compiler testing, Differential fuzzing, Rust}
}

@article{10.1145/3689781,
author = {Ambal, Guillaume and Dongol, Brijesh and Eran, Haggai and Klimis, Vasileios and Lahav, Ori and Raad, Azalea},
title = {Semantics of Remote Direct Memory Access: Operational and Declarative Models of RDMA on TSO Architectures},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689781},
doi = {10.1145/3689781},
abstract = {Remote direct memory access (RDMA) is a modern technology enabling networked machines to exchange information without involving the operating system of either side, and thus significantly speeding up data transfer in computer clusters. While RDMA is extensively used in practice and studied in various research papers, a formal underlying model specifying the allowed behaviours of concurrent RDMA programs running in modern multicore architectures is still missing. This paper aims to close this gap and provide semantic foundations of RDMA on x86-TSO machines. We propose three equivalent formal models, two operational models in different levels of abstraction and one declarative model, and prove that the three characterisations are equivalent. To gain confidence in the proposed semantics, the more concrete operational model has been reviewed by NVIDIA experts, a major vendor of RDMA systems, and we have empirically validated the declarative formalisation on various subtle litmus tests by extensive testing. We believe that this work is a necessary initial step for formally addressing RDMA-based systems by proposing language-level models, verifying their mapping to hardware, and developing reasoning techniques for concurrent RDMA programs.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {341},
numpages = {28},
keywords = {Declarative Semantics, Operational Semantics, RDMA, x86-TSO}
}

@article{10.1145/3689782,
author = {Ye, Wenjia and Sun, Yaozhu and Oliveira, Bruno C. d. S.},
title = {Imperative Compositional Programming: Type Sound Distributive Intersection Subtyping with References via Bidirectional Typing},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689782},
doi = {10.1145/3689782},
abstract = {Compositional programming is a programming paradigm that emphasizes modularity and is implemented in the CP programming language. The foundations for compositional programming are based on a purely functional variant of System F with intersection types, called Fi+, which includes distributivity rules for subtyping.
 
 
 
This paper shows how to extend compositional programming and CP with mutable references, enabling a modular, imperative compositional programming style. A technical obstacle solved in our work is the interaction between distributive intersection subtyping and mutable references. Davies and Pfenning [2000] studied this problem in standard formulations of intersection type systems and argued that, when combined with references, distributive subtyping rules lead to type unsoundness. To recover type soundness, they proposed dropping distributivity rules in subtyping. CP cannot adopt this solution, since it fundamentally relies on distributivity for modularity. Therefore, we revisit the problem and show that, by adopting bidirectional typing, a more lightweight and type sound restriction is possible: we can simply restrict the typing rule for references. This solution retains distributivity and an unrestricted intersection introduction rule. We present a first calculus, based on Davies and Pfenning's work, which illustrates the generality of our solution. Then we present an extension of Fi+ with references, which adopts our restriction and enables imperative compositional programming. We implement an extension of CP with references and show how to model a modular live-variable analysis in CP. Both calculi and their proofs are formalized in the Coq proof assistant.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {342},
numpages = {30},
keywords = {Bidirectional Typing, Compositional Programming, Distributive Subtyping, Intersection Types, Mutable References, Type Soundness}
}

@software{10.5281/zenodo.13373228,
author = {Ye, Wenjia and Sun, Yaozhu and Oliveira, Bruno C. d. S.},
title = {Imperative Compositional Programming (Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13373228},
abstract = {
    <p>This is the artifact of the OOPSLA2024 research paper : Imperative
Compositional Programming.</p>

},
keywords = {Bidirectional Typing, Compositional Programming, Distributive Subtyping, Intersection Types, Mutable References, Type Soundness}
}

@article{10.1145/3689783,
author = {Wu, Jifeng and Lemieux, Caroline},
title = {QuAC: Quick Attribute-Centric Type Inference for Python},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689783},
doi = {10.1145/3689783},
abstract = {Python’s dynamic typing facilitates rapid prototyping and underlies its popularity in many domains. However, dynamic typing reduces the power of many static checking and bug-finding tools. Python type annotations can make these tools more useful. Type inference tools aim to reduce developers’ burden of adding them. However, existing type inference tools struggle to support dynamic features, infer correct types (especially container type parameters and non-builtin types), and run in reasonable time. Inspired by Python’s duck typing, where the attributes accessed on Python expressions characterize their implicit interfaces, we propose QuAC (Quick Attribute-Centric Type Inference for Python). At its core, QuAC collects attribute sets for Python expressions and leverages information retrieval techniques to predict classes from these attribute sets. It also recursively predicts container type parameters. We evaluate QuAC’s performance on popular Python projects. Compared to state-of-the-art non-LLM baselines, QuAC predicts types with high accuracy complementary to those predicted by the baselines while not sacrificing coverage. It also demonstrates clear advantages in predicting container type parameters and non-builtin types and reduces run times. Furthermore, QuAC is nearly two orders of magnitude faster than an LLM-based method while covering nearly half of its errorless non-trivial type predictions. It is also significantly more consistent at predicting container type parameters and non-builtin types than the LLM-based method, regardless of whether the project has ground-truth type annotations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {343},
numpages = {30},
keywords = {Gradual Typing, Python, Static Analysis, Type Inference}
}

@software{10.5281/zenodo.13367665,
author = {Wu, Jifeng and Lemieux, Caroline},
title = {Reproduction Package for Article `QuAC: Quick Attribute-Centric Type Inference for Python`},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13367665},
abstract = {
    <p>This artifact supports the paper “QuAC: Quick Attribute-Centric Type
Inference for Python.” The artifact includes the code and data used to
generate the results presented in the paper. It aims to reproduce the
main scientific claims and facilitate future research by making the
software publicly available.</p>

},
keywords = {Gradual Typing, Python, Static Analysis, Type Inference}
}

@article{10.1145/3689784,
author = {Saioc, Georgian-Vlad and Lange, Julien and M\o{}ller, Anders},
title = {Automated Verification of Parametric Channel-Based Process Communication},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689784},
doi = {10.1145/3689784},
abstract = {A challenge of writing concurrent message passing programs is ensuring the absence of partial deadlocks, which can cause severe memory leaks in long running systems. Several static analysis techniques have been proposed for automatically detecting partial deadlocks in Go programs. For a large enterprise code base, we found these tools too imprecise to reason about process communication that is parametric, i.e., where the number of channel communication operations or the channel capacities are determined at runtime.
 
We present a novel approach to automatically verify the absence of partial deadlocks in Go program fragments with such parametric process communication. The key idea is to translate Go fragments to a core language that is sufficiently expressive to represent real-world parametric communication patterns and can be encoded into Dafny programs annotated with postconditions enforcing partial deadlock freedom. In situations where a fragment is partial deadlock free only when the concurrency parameters satisfy certain conditions, a suitable precondition can often be inferred.
 
Experimental results on a real-world code base containing 583 program fragments that are beyond the reach of existing techniques have shown that the approach can verify the absence of partial deadlocks in 145 cases. For an additional 228 cases, a nontrivial precondition is inferred that the surrounding code must satisfy to ensure partial deadlock freedom.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {344},
numpages = {27},
keywords = {Go, automated verification, invariant discovery, message passing concurrency, partial deadlocks, static analysis}
}

@software{10.5281/zenodo.13825844,
author = {Saioc, Georgian-Vlad and Lange, Julien and M\o{}ller, Anders},
title = {Artifact Submission For "Automated Verification of Parametric Channel-Based Process Communication"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13825844},
abstract = {
    <p>Public release of OOPSLA 2024 Ginger artifact. Includes a small set
of example program fragments. Experimental data used in the evaluation
section of the original paper is proprietary.</p>

},
keywords = {automated verification, Go, invariant discovery, message passing concurrency, partial deadlocks, static analysis}
}

@article{10.1145/3689785,
author = {Venev, Hristo and Gehr, Timon and Dimitrov, Dimitar and Vechev, Martin},
title = {Modular Synthesis of Efficient Quantum Uncomputation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689785},
doi = {10.1145/3689785},
abstract = {A key challenge of quantum programming is uncomputation: the reversible deallocation of qubits. And while there has been much recent progress on automating uncomputation, state-of-the-art methods are insufficient for handling today’s expressive quantum programming languages. A core reason is that they operate on primitive quantum circuits, while quantum programs express computations beyond circuits, for instance, they can capture families of circuits defined recursively in terms of uncomputation and adjoints.
 

 
In this paper, we introduce the first modular automatic approach to synthesize correct and efficient uncomputation for expressive quantum programs. Our method is based on two core technical contributions: (i) an intermediate representation (IR) that can capture expressive quantum programs and comes with support for uncomputation, and (ii) modular algorithms over that IR for synthesizing uncomputation and adjoints.
 

 
We have built a complete end-to-end implementation of our method, including an implementation of the IR and the synthesis algorithms, as well as a translation from an expressive fragment of the Silq programming language to our IR and circuit generation from the IR. Our experimental evaluation demonstrates that we can handle programs beyond the capabilities of existing uncomputation approaches, while being competitive on the benchmarks they can handle. More broadly, we show that it is possible to benefit from the greater expressivity and safety offered by high-level quantum languages without sacrificing efficiency.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {345},
numpages = {28},
keywords = {intermediate representations, quantum programming languages}
}

@software{10.5281/zenodo.13487216,
author = {Venev, Hristo and Gehr, Timon and Dimitrov, Dimitar and Vechev, Martin},
title = {Artifact for "Modular Synthesis of Efficient Quantum Uncomputation"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13487216},
abstract = {
    <p>This archive contains the artifact for the paper “Modular Synthesis
of Efficient Quantum Uncomputation”. &nbsp;It can be used to reproduce the
results in Tables 1-3.</p>

},
keywords = {intermediate representations, quantum programming languages}
}

@article{10.1145/3689786,
author = {Carnier, Denis and Pottier, Fran\c{c}ois and Keuchel, Steven},
title = {Type Inference Logics},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689786},
doi = {10.1145/3689786},
abstract = {Type inference is essential for statically-typed languages such as OCaml and Haskell.
 
It can be decomposed into two (possibly interleaved) phases: a generator converts programs to constraints; a solver decides whether a constraint is satisfiable.
 
Elaboration, the task of decorating a program with explicit type annotations, can also be structured in this way.
 
Unfortunately, most machine-checked implementations of type inference do not follow this phase-separated, constraint-based approach.
 
Those that do are rarely executable, lack effectful abstractions, and do not include elaboration.
 
 
 
To close the gap between common practice in real-world implementations and mechanizations inside proof assistants, we propose an approach that enables modular reasoning about monadic constraint generation in the presence of elaboration.
 
Our approach includes a domain-specific base logic for reasoning about metavariables and a program logic that allows us to reason abstractly about the meaning of constraints.
 
To evaluate it, we report on a machine-checked implementation of our techniques inside the Coq proof assistant.
 
As a case study, we verify both soundness and completeness for three elaborating type inferencers for the simply typed lambda calculus with Booleans.
 
Our results are the first demonstration that type inference algorithms can be verified in the same form as they are implemented in practice: in an imperative style, modularly decomposed into constraint generation and solving, and delivering elaborated terms to the remainder of the compiler chain.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {346},
numpages = {31},
keywords = {elaboration, program verification, type inference}
}

@software{10.5281/zenodo.13625874,
author = {Carnier, Denis and Pottier, Fran\c{c}ois and Keuchel, Steven},
title = {Type Inference Logics - Artifact},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13625874},
abstract = {
    <p>This artifact contains both sources and a prebuilt Docker image. The
sources file contains a <code>README.md</code> for navigating the source
code with instructions on how to get started. The image file contains a
Docker image compiled for AMD64 with all the necessary dependencies to
check the code with the Coq proof assistant and GHC.</p>

},
keywords = {elaboration, program verification, type inference}
}

@article{10.1145/3689787,
author = {Baek, Doehyun and Getz, Jakob and Sim, Yusung and Lehmann, Daniel and Titzer, Ben L. and Ryu, Sukyoung and Pradel, Michael},
title = {Wasm-R3: Record-Reduce-Replay for Realistic and Standalone WebAssembly Benchmarks},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689787},
doi = {10.1145/3689787},
abstract = {WebAssembly (Wasm for short) brings a new, powerful capability to the web as well as Edge, IoT, and embedded systems. Wasm is a portable, compact binary code format with high performance and robust sandboxing properties. As Wasm applications grow in size and importance, the complex performance characteristics of diverse Wasm engines demand robust, representative benchmarks for proper tuning. Stopgap benchmark suites, such as PolyBenchC and libsodium, continue to be used in the literature, though they are known to be unrepresentative. Porting of more complex suites remains difficult because Wasm lacks many system APIs and extracting real-world Wasm benchmarks from the web is difficult due to complex host interactions. To address this challenge, we introduce Wasm-R3, the first record and replay technique for Wasm. Wasm-R3 transparently injects instrumentation into Wasm modules to record an execution trace from inside the module, then reduces the execution trace via several optimizations, and finally produces a replay module that is executable standalone without any host environment-on any engine. The benchmarks created by our approach are (i) realistic, because the approach records real-world web applications, (ii) faithful to the original execution, because the replay benchmark includes the unmodified original code, only adding emulation of host interactions, and (iii) standalone, because the replay benchmarks run on any engine. Applying Wasm-R3 to web-based Wasm applications in the wild demonstrates the correctness of our approach as well as the effectiveness of our optimizations, which reduce the recorded traces by 99.53\% and the size of the replay benchmark by 9.98\%. We release the resulting benchmark suite of 27 applications, called Wasm-R3-Bench, to the community, to inspire a new generation of realistic and standalone Wasm benchmarks.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {347},
numpages = {27},
keywords = {Benchmarking, WebAssembly, record and replay}
}

@software{10.5281/zenodo.13382344,
author = {Baek, Doehyun and Getz, Jakob and Sim, Yusung and Lehmann, Daniel and Titzer, Ben L. and Ryu, Sukyoung and Pradel, Michael},
title = {Reproduction Package for Article `Wasm-R3: Record-Reduce-Replay for Realistic and Standalone WebAssembly Benchmarks`},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13382344},
abstract = {
    <p>This artifact contains supplementary material for the paper “Wasm-R3:
Record-Reduce-Replay for Realistic and Standalone WebAssembly
Benchmarks” (OOPSLA’24).</p>
<p>There are two main components to this artifact, which are Wasm-R3, a
record-and-replay framework for Webassembly, and Wasm-R3-Bench, a
benchmark suite of 27 real-world web applications generated by
Wasm-R3.</p>

},
keywords = {Benchmarking, record and replay, WebAssembly}
}

@article{10.1145/3689788,
author = {Qian, Kelvin and Smith, Scott and Stride, Brandon and Weng, Shiwei and Wu, Ke},
title = {Semantic-Type-Guided Bug Finding},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689788},
doi = {10.1145/3689788},
abstract = {In recent years, there has been an increased interest in tools that establish incorrectness rather than correctness of program properties. In this work we build on this approach by developing a novel methodology to prove incorrectness of semantic typing properties of functional programs, extending the incorrectness approach to the model theory of functional program typing. We define a semantic type refuter which refutes semantic typings for a simple functional language. We prove our refuter is co-recursively enumerable, and that it is sound and complete with respect to a semantic typing notion. An initial implementation is described which uses symbolic evaluation to efficiently find type errors over a functional language with a rich type system.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {348},
numpages = {28},
keywords = {Incorrectness, Semantic Typing, Symbolic Execution, Test Generation}
}

@software{10.5281/zenodo.13393058,
author = {Qian, Kelvin and Smith, Scott and Stride, Brandon and Weng, Shiwei and Wu, Ke},
title = {Software Artifact for Semantic-Type-Guided Bug Finding},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13393058},
abstract = {
    <p>This is the codebase for the languages BlueJay, Jay, and JayIL, as
well as the languages’ semantic-type-guided type checker. This code is
developed by the JHU Programming Languages Lab. It is a pipeline of
functional languages that fits for research at each layer.</p>
<p>This snapshot is the artifact for Semantic-Type-Guided Bug
Finding.</p>
<p>With this artifact, the user can recreate all benchmarks seen in the
paper.</p>

},
keywords = {Incorrectness, OCaml, Semantic Typing, Symbolic Execution, Test Generation}
}

@article{10.1145/3689789,
author = {Mariano, Benjamin and Wang, Ziteng and Pailoor, Shankara and Collberg, Christian and Dillig, I\c{s}il},
title = {Control-Flow Deobfuscation using Trace-Informed Compositional Program Synthesis},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689789},
doi = {10.1145/3689789},
abstract = {Code deobfuscation, which attempts to simplify code that has been intentionally obfuscated to prevent understanding, is a critical technique for downstream security analysis tasks like malware detection. While there has been significant prior work on code deobfuscation, most techniques either do not handle control flow obfuscations that modify control flow or they target specific classes of control flow obfuscations, making them unsuitable for handling new types of obfuscations or combinations of existing ones. In this paper, we study a new deobfuscation technique that is based on program synthesis and that can handle a broad class of control flow obfuscations. Given an obfuscated program P, our approach aims to synthesize a smallest program that is a control-flow reduction of P and that is semantically equivalent. Since our method does not assume knowledge about the types of obfuscations that have been applied to the original program, the underlying synthesis problem ends up being very challenging. To address this challenge, we propose a novel trace-informed compositional synthesis algorithm that leverages hints present in dynamic traces of the obfuscated program to decompose the synthesis problem into a set of simpler subproblems. In particular, we show how dynamic traces can be useful for inferring a suitable control-flow skeleton of the deobfuscated program and performing independent synthesis of each basic block. We have implemented this approach in a tool called Chisel and evaluate it on 546 benchmarks that have been obfuscated using combinations of six different obfuscation techniques. Our evaluation shows that our approach is effective and that it produces code that is almost identical (modulo variable renaming) to the original (non-obfuscated) program in 86\% of cases. Our evaluation also shows that Chisel significantly outperforms existing techniques.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {349},
numpages = {31},
keywords = {Deobfuscation, Obfuscation, Program Synthesis}
}

@software{10.5281/zenodo.13863694,
author = {Mariano, Benjamin and Wang, Ziteng and Pailoor, Shankara and Collberg, Christian and Dillig, I\c{s}il},
title = {Software Artifact for "Control-Flow Deobfuscation using Trace-Informed Compositional Program Synthesis"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13863694},
abstract = {
    <p>Software artifact for the paper “Control-Flow Deobfuscation using
Trace-Informed Compositional Program Synthesis”.</p>

},
keywords = {Deobfuscation, Program Synthesis}
}

@article{10.1145/3689790,
author = {Kim, Caleb and Li, Pai and Mohan, Anshuman and Butt, Andrew and Sampson, Adrian and Nigam, Rachit},
title = {Unifying Static and Dynamic Intermediate Languages for Accelerator Generators},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689790},
doi = {10.1145/3689790},
abstract = {Compilers for accelerator design languages (ADLs) translate high-level languages into application-specific hardware. ADL compilers rely on a hardware control interface to compose hardware units. There are two choices: static control, which relies on cycle-level timing;  or dynamic control, which uses explicit signalling to avoid depending on timing details. Static control is efficient but brittle; dynamic control incurs hardware costs to support compositional reasoning.    Piezo is an ADL compiler that unifies static and dynamic control in a single intermediate language (IL). Its key insight is that the IL’s static fragment is a refinement of its dynamic fragment:  static code admits a subset of the run-time behaviors of the dynamic equivalent. Piezo can optimize code by combining facts from static and dynamic submodules, and it opportunistically converts code from dynamic to static control styles. We implement Piezo as an extension to an existing dynamic ADL compiler, Calyx. We use Piezo to implement a frontend for an existing ADL, a systolic array generator, and a packet-scheduling hardware generator to demonstrate its optimizations and the static–dynamic interactions it enables.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {350},
numpages = {26},
keywords = {Accelerator Design Language, Intermediate Language}
}

@software{10.5281/zenodo.13388204,
author = {Kim, Caleb and Li, Pai and Mohan, Anshuman and Butt, Andrew and Sampson, Adrian and Nigam, Rachit},
title = {Reproduction Package for "Unifying Static and Dynamic Intermediate Languages for Accelerator Generators"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13388204},
abstract = {
    <p>The artifact is a VirtualBox Image to reproduce the results for
“Unifying Static and Dynamic Intermediate Languages for Accelerator
Generators”. In particular, the artifact can be used to reproduce the
graphs and quantitative claims made in section 6 and 7 of the paper.
More detailed instructions can be found in the REAMDE.md of the
following repository: https://github.com/cucapra/calyx-resource-eval,
and more general documentation of Piezo can be found here:
https://docs.calyxir.org/.</p>
<p>The VirtualBox image consists of: - The Calyx/Piezo compiler - fud,
the compiler driver - The Dahlia compiler - The evaluation code. In
particular, it contains the scripts necessary to generate the data and
graphs used in the paper. It also contains the source code for the
benchmarks used in the paper: in particular Polybench benchmarks written
in Dahlia and PIFO tree benchmarks written in Piezo. For the systolic
arrays, fud uses a Python script to generate systolic arrays written in
Piezo. More details about fud’s systolic array generation can be found
here: https://docs.calyxir.org/frontends/systolic-array.html.</p>

},
keywords = {Accelerator Design Language, Intermediate Language}
}

@article{10.1145/3689791,
author = {Norlinder, Jonas and \"{O}sterlund, Erik and Black-Schaffer, David and Wrigstad, Tobias},
title = {Mark–Scavenge: Waiting for Trash to Take Itself Out},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689791},
doi = {10.1145/3689791},
abstract = {Moving garbage collectors (GCs) typically free memory by evacuating live objects in order to reclaim contiguous 
memory regions. Evacuation is typically done either during tracing (scavenging), or after tracing when 
identification of live objects is complete (mark–evacuate). Scavenging typically requires more memory (memory 
for all objects to be moved), but performs less work in sparse memory areas (single pass). This makes it 
attractive for collecting young objects. Mark–evacuate typically requires less memory and performs less 
work in memory areas with dense object clusters, by focusing relocation around sparse regions, making it 
attractive for collecting old objects. Mark–evacuate also completes identification of live objects faster, making 
it attractive for concurrent GCs that can reclaim memory immediately after identification of live objects 
finishes (as opposed to when evacuation finishes), at the expense of more work compared to scavenging, for 
young objects. 

We propose an alternative approach for concurrent GCs to combine the benefits of scavenging with the 
benefits of mark–evacuate, for young objects. The approach is based on the observation that by the time 
young objects are relocated by a concurrent GC, they are likely to already be unreachable. By performing 
relocation lazily, most of the relocations in the defragmentation phase of mark–evacuate can typically be 
eliminated. Similar to scavenging, objects are relocated during tracing with the proposed approach. However, 
instead of relocating all objects that are live in the current GC cycle, it lazily relocates profitable sparse object 
clusters that survived from the previous GC cycle. This turns the memory headroom that concurrent GCs 
typically “waste” in order to safely avoid running out of memory before GC finishes, into an asset used to 
eliminate much of the relocation work, which constitutes a significant portion of the GC work. 

We call this technique mark–scavenge and implement it on-top of ZGC in OpenJDK in a collector we call 
MS-ZGC. We perform a performance evaluation that compares MS-ZGC against ZGC. The most striking result 
is (up to) 91\% reduction in relocation of dead objects (depending on machine-dependent factors).},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {351},
numpages = {28},
keywords = {concurrent, garbage collection, mark-evacuate, scavenging}
}

@software{10.5281/zenodo.13361333,
author = {Norlinder, Jonas and \"{O}sterlund, Erik and Black-Schaffer, David and Wrigstad, Tobias},
title = {[OOPSLA'24 Artefact] Mark–Scavenge: Waiting for Trash to Take Itself Out},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13361333},
abstract = {
    <p>This artefact contains contains MS-ZGC, an implementation of
mark-scavenge, presented in the paper Mark–Scavenge: Waiting for Trash
to Take Itself Out.</p>

},
keywords = {concurrent, garbage collection, mark-evacuate, mark-scavenge, scavenging}
}

@article{10.1145/3689792,
author = {Oliveira Vale, Arthur and Wang, Zhongye and Chen, Yixuan and You, Peixin and Shao, Zhong},
title = {Compositionality and Observational Refinement for Linearizability with Crashes},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689792},
doi = {10.1145/3689792},
abstract = {Crash-safety is an important property of real systems, as the main functionality of some systems is resilience to crashes. Toward a compositional verification approach for crash-safety under full-system crashes, one observes that crashes propagate instantaneously to all components across all levels of abstraction, even to unspecified components, hindering compositionality. Furthermore, in the presence of concurrency, a correctness criterion that addresses both crashes and concurrency proves necessary. For this, several adaptations of linearizability have been suggested, each featuring different trade-offs between complexity and expressiveness. The recently proposed compositional linearizability framework shows that to achieve compositionality with linearizability, both a locality and observational refinement property are necessary. Despite that, no linearizability criterion with crashes has been proven to support an observational refinement property.        In this paper, we define a compositional model of concurrent computation with full-system crashes. We use this model to develop a compositional theory of linearizability with crashes, which reveals a criterion, crash-aware linearizability, as its inherent notion of linearizability and supports both locality and observational refinement. We then show that strict linearizability and durable linearizability factor through crash-aware linearizability as two different ways of translating between concurrent computation with and without crashes, enabling simple proofs of locality and observational refinement for a generalization of these two criteria. Then, we show how the theory can be connected with a program logic for durable and crash-aware linearizability, which gives the first program logic that verifies a form of linearizability with crashes. We showcase the advantages of compositionality by verifying a library facilitating programming persistent data structures and a fragment of a transactional interface for a file system.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {352},
numpages = {29},
keywords = {Compositional Linearizability, Crash-Aware Linearizability, Durable Linearibility, Strict Linearizability}
}

@article{10.1145/3689793,
author = {Wong, Augustine and Bucci, Paul and Beschastnikh, Ivan and Fedorova, Alexandra},
title = {Making Sense of Multi-threaded Application Performance at Scale with NonSequitur},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689793},
doi = {10.1145/3689793},
abstract = {Modern multi-threaded systems are highly complex. This makes their behavior difficult to understand. Developers frequently capture behavior in the form of program traces and then manually inspect these traces. Existing tools, however, fail to scale to traces larger than a million events.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In this paper we present an approach to compress multi-threaded traces in order to allow developers to visually explore these traces at scale. Our approach is able to compress traces that contain millions of events down to a few hundred events. We use this approach to design and implement a tool called NonSequitur. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We present three case studies which demonstrate how we used NonSequitur to analyze real-world performance issues with Meta's storage engine RocksDB and MongoDB's storage engine WiredTiger, two complex database backends. We also evaluate NonSequitur with 42 participants on traces from RocksDB and WiredTiger. We demonstrate that, in some cases, participants on average scored 11 times higher when performing performance analysis tasks on large execution traces. Additionally, for some performance analysis tasks, the participants spent on average three times longer with other tools than with NonSequitur.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {353},
numpages = {30},
keywords = {Multi-threaded Applications, Outlier events, Performance debugging, Runtime trace visualization}
}

@software{10.5281/zenodo.13446443,
author = {Wong, Augustine and Bucci, Paul and Beschastnikh, Ivan and Fedorova, Alexandra},
title = {NonSequitur Source Code and User Study Result Data for the paper "Making Sense of Multi-Threaded Application Performance at Scale with NonSequitur."},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13446443},
abstract = {
    <p>The artifact accompanying the paper “Making Sense of Multi-Threaded
Application Performance at Scale with NonSequitur,” which contains: The
source code for the NonSequitur visualization tool. Some additional
Python scripts and data collected/used during the user study described
in the paper.</p>
<p>This paper was accepted in the OOPSLA 2024 conference held in
Pasadena, California. The artifact is contained in the zip file
“oopsla24-ns-artifact.zip.” A detailed description of the artifact is in
the file “OOPSLA 2024 NonSequitur Artifact Doc.pdf.”</p>

},
keywords = {Multi-threaded Applications, Outlier events, Performance debugging, Runtime trace visualization}
}

@article{10.1145/3689794,
author = {Yang, Chen and Chen, Junjie and Jiang, Jiajun and Huang, Yuliang},
title = {Dependency-Aware Code Naturalness},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689794},
doi = {10.1145/3689794},
abstract = {Code naturalness, which captures repetitiveness and predictability in programming languages, has proven valuable for various code-related tasks in software engineering. However, precisely measuring code naturalness remains a fundamental challenge. Existing methods measure code naturalness over individual lines of code while ignoring the deep semantic relations among different lines, e.g., program dependency, which may negatively affect the precision of the measure. Despite the intuitive appeal of extending the code naturalness measure to the code dependency domain (as there are some work that have initiated the utilization of code dependency for diverse code-related tasks), this assumption remains unexplored and warrants direct investigation. In this study, we aim to perform the first empirical study to investigate whether incorporating code dependency, instead of analyzing individual lines, can enhance the precision of measuring code naturalness.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To achieve that, we first propose a new method named DAN for measuring code naturalness by incorporating the rich dependency information in the code. Specifically, DAN extracts multiple sequences of code lines by traversing the program dependency graph, where different code lines are connected by dependencies in each sequence, and then the code naturalness will be measured by taking each sequence as a whole. In this way, the dependency information can be well captured. Finally, we have conducted an extensive study to evaluate the influence of code dependency for measuring code naturalness with DAN, and compared it with the state-of-the-art methods under three emerging application scenarios of code naturalness. The results demonstrate that DAN can not only better distinguish natural and unnatural code, but also substantially boost two important downstream applications of code naturalness, i.e., distinguishing buggy and non-buggy code lines and data cleansing for training better code models, reflecting the significance of code dependency in measuring code naturalness.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {354},
numpages = {23},
keywords = {Code Entropy, Naturalness, Program Dependency}
}

@software{10.5281/zenodo.12783666,
author = {Yang, Chen and Chen, Junjie and Jiang, Jiajun and Huang, Yuliang},
title = {Dependency-aware code naturalness},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12783666},
abstract = {
    <p>Docker image and data for the artifact of DAN (dependency-aware code
naturalness)</p>

},
keywords = {Code Entropy, Naturalness, Program Dependency}
}

@article{10.1145/3689795,
author = {Winterer, Dominik and Su, Zhendong},
title = {Validating SMT Solvers for Correctness and Performance via Grammar-Based Enumeration},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689795},
doi = {10.1145/3689795},
abstract = {We introduce ET, a grammar-based enumerator for validating SMT solver correctness and performance. By compiling grammars of the SMT theories to algebraic datatypes, ET leverages the functional enumerator FEAT. ET is highly effective at bug finding and has many complimentary benefits. Despite the extensive and continuous testing of the state-of-the-art SMT solvers Z3 and cvc5, ET found 102 bugs, out of which 76 were confirmed and 32 were fixed. Moreover, ET can be used to understand the evolution of solvers. We derive eight grammars realizing all major SMT theories including the booleans, integers, reals, realints, bit-vectors, arrays, floating points, and strings. Using ET, we test all consecutive releases of the SMT solvers Z3 and CVC4/cvc5 from the last six years (61 versions) on 8 million formulas, and 488 million solver calls. Our results suggest improved correctness in recent versions of both solvers but decreased performance in newer releases of Z3 on small timeouts (since z3-4.8.11) and regressions in early cvc5 releases on larger timeouts. Due to its systematic testing and efficiency, we further advocate ET's use for continuous integration.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {355},
numpages = {24},
keywords = {Fuzz testing, Grammar-based enumeration, SMT solvers}
}

@article{10.1145/3689796,
author = {Grossman, Shelly and Toman, John and Bakst, Alexander and Arora, Sameer and Sagiv, Mooly and Nandi, Chandrakana},
title = {Practical Verification of Smart Contracts using Memory Splitting},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689796},
doi = {10.1145/3689796},
abstract = {SMT-based verification of low-level code requires modeling and reasoning about memory operations. Prior work has shown that optimizing memory representations is beneficial for scaling verification—pointer analysis, for example can be used to split memory into disjoint regions leading to faster SMT solving. However, these techniques are mostly designed for C and C++ programs with explicit operations for memory allocation which are not present in all languages. For instance, on the Ethereum virtual machine, memory is simply a monolithic array of bytes which can be freely accessed by Ethereum bytecode, and there is no allocation primitive.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
In this paper, we present a memory splitting transformation guided by a conservative memory analysis for Ethereum bytecode generated by the Solidity compiler. The analysis consists of two phases: recovering memory allocation and memory regions, followed by a pointer analysis. The goal of the analysis is to enable memory splitting which in turn speeds up verification. We implemented both the analysis and the memory splitting transformation as part of a verification tool, CertoraProver, and show that the transformation speeds up SMT solving by up to 120x and additionally mitigates 16 timeouts when used on 229 real-world smart contract verification tasks.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {356},
numpages = {32},
keywords = {SMT solving, pointer analysis, smart contracts, verification}
}

@software{10.1145/3580439,
author = {Grossman, Shelly and Toman, John and Bakst, Alexander and Arora, Sameer and Sagiv, Mooly and Nandi, Chandrakana},
title = {Artifact for our paper titled "Practical Verification Of Smart Contracts using Memory Splitting"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3580439},
abstract = {
    <p>This is the artifact for our paper “Practical Verification Of Smart
Contracts using Memory Splitting”. In our paper, we present a novel
memory splitting transformation and a memory analysis for EVM (Ethereum
Virtual Machine) bytecode. The key insight of the paper is that applying
memory splitting transformation during a pre-processing step leads to
faster SMT solving times and mitigates SMT timeouts when performing
automated verification of real-world programs. We apply this idea in the
domain of smart contracts. This artifact is developed by Certora:
https://www.certora.com/.</p>

},
keywords = {pointer analysis, smart contracts, SMT solving, verification}
}

@article{10.1145/3689797,
author = {Liew, Dennis and Cogumbreiro, Tiago and Lange, Julien},
title = {Sound and Partially-Complete Static Analysis of Data-Races in GPU Programs},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689797},
doi = {10.1145/3689797},
abstract = {GPUs are progressively being integrated into modern society, playing a pivotal role in Artificial Intelligence
 
and High-Performance Computing. Programmers need a deep understanding of the GPU programming model
 
to avoid subtle data-races in their codes. Static verification that is sound and incomplete can guarantee
 
data-race freedom, but the alarms it raises may be spurious and need to be validated.
 

 
In this paper, we establish a True Positive Theorem for a static data-race detector for GPU programs, i.e., a
 
result that identifies a class of programs for which our technique only raises true alarms. Our work builds on
 
the formalism of memory access protocols, that models the concurrency operations of CUDA programs. The
 
crux of our approach is an approximation analysis that can correctly identify true alarms, and pinpoint the
 
conditions that make an alarm imprecise. Our approximation analysis detects when the reported locations are
 
reachable (control independence, or CI), and when the reported locations are precise (data independence, or
 
DI), as well identify inexact values in an alarm. In addition to a True Positive result for programs that are CI
 
and DI, we establish the root causes of spurious alarms depending on whether CI or DI are present.
 

 
We apply our theory to introduce FaialAA, the first sound and partially complete data-race detector. We
 
evaluate FaialAA in three experiments. First, in a comparative study with the state-of-the-art tools, we show that
 
FaialAA confirms more DRF programs than others while emitting 1.9\texttimes{} fewer potential alarms. Importantly, the
 
approximation analysis of FaialAA detects 10 undocumented data-races. Second, in an experiment studying 6
 
commits of data-race fixes in open source projects OpenMM and Nvidia’s MegaTron, FaialAA confirmed the
 
buggy and fixed versions of 5 commits, while others were only able to confirm 2. Third, we show that 59.5\% of
 
2,770 programs are CI and DI, quantifying when the approximation analysis of FaialAA is complete.
 

 
This paper is accompanied by the mechanized proofs of the theoretical results presented therein and a tool
 
(FaialAA) implementing of our theory.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {357},
numpages = {28},
keywords = {GPU programming, data-race detection, static analysis, true positives}
}

@software{10.5281/zenodo.12666682,
author = {Liew, Dennis and Cogumbreiro, Tiago and Lange, Julien},
title = {Sound and partially-complete static analysis of data-races in GPU programs (Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12666682},
abstract = {
    <p>An artifact with all the tools and datasets presented in Section 6,
and mechanized proofs of the theoretical results mentioned in Section
4.4.</p>

},
keywords = {data-race detection, GPU programming, static analysis, true positives}
}

@article{10.1145/3689798,
author = {Alvarez-Picallo, Mario and Freund, Teodoro and Ghica, Dan R. and Lindley, Sam},
title = {Effect Handlers for C via Coroutines},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689798},
doi = {10.1145/3689798},
abstract = {Effect handlers provide a structured means for implementing user-defined, composable,  and customisable computational effects, ranging from exceptions to generators to  lightweight threads.  We introduce libseff, a novel effect handlers library for C, based on coroutines.  Whereas prior effect handler libraries for C are intended primarily as compilation  targets, libseff is intended to be used directly from C programs.  As such, the design of libseff parts ways from traditional effect handler  implementations, both by using mutable coroutines as the main representation of  pending computations, and by avoiding closures as handlers by way of reified effects.  We show that the performance of libseff is competitive across a range of platforms  and benchmarks.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {358},
numpages = {28},
keywords = {C, Coroutines, Effect Handlers}
}

@software{10.5281/zenodo.13485897,
author = {Alvarez-Picallo, Mario and Freund, Teodoro and Ghica, Dan R. and Lindley, Sam},
title = {Effect Handlers for C via Coroutines - Artifact},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13485897},
abstract = {
    <p>Artifact for the library and benchmarks described in the Effect
Handlers for C via Coroutines paper published at OOPSLA 2024.</p>

},
keywords = {C, Coroutines, Effect Handlers}
}

@article{10.1145/3689799,
author = {Drosos, Georgios-Petros and Sotiropoulos, Thodoris and Alexopoulos, Georgios and Mitropoulos, Dimitris and Su, Zhendong},
title = {When Your Infrastructure Is a Buggy Program: Understanding Faults in Infrastructure as Code Ecosystems},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689799},
doi = {10.1145/3689799},
abstract = {Modern applications have become increasingly complex and their manual installation and configuration is
 
no longer practical. Instead, IT organizations heavily rely on Infrastructure as Code (IaC) technologies, to automate the provisioning, configuration, and maintenance of computing infrastructures and systems. IaC systems typically offer declarative, domain-specific languages (DSLs) that allow system administrators and developers to write high-level programs that specify the desired state of their infrastructure in a reliable, predictable, and documented fashion. Just like traditional programs, IaC software is not immune to faults, with issues ranging from deployment failures to critical misconfigurations that often impact production systems used by millions of end users. Surprisingly, despite its crucial role in global infrastructure management, the tooling and techniques for ensuring IaC reliability still have room for improvement. 
 
 
 
In this work, we conduct a comprehensive analysis of 360 bugs identified in IaC software within prominent IaC ecosystems including Ansible, Puppet, and Chef. Our work is the first in-depth exploration of bug characteristics in these widely-used IaC environments. Through our analysis we aim to understand: (1) how these bugs manifest, (2) their underlying root causes, (3) their reproduction requirements in terms of system state (e.g., operating system versions) or input characteristics, and (4) how these bugs are fixed. Based on our findings, we evaluate the state-of-the-art techniques for IaC reliability, identify their limitations, and provide a set of recommendations for future research. We believe that our study helps researchers to (1) better understand the complexity and peculiarities of IaC software, and (2) develop advanced tooling for more reliable and robust system configurations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {359},
numpages = {31},
keywords = {Ansible, Chef, IaC, Puppet, bug, deployment, infrastructure as code, testing}
}

@software{10.5281/zenodo.12668895,
author = {Drosos, Georgios-Petros and Sotiropoulos, Thodoris and Alexopoulos, Georgios and Mitropoulos, Dimitris and Su, Zhendong},
title = {Reproduction Package for Article: "When Your Infrastructure Is a Buggy Program: Understanding Faults in Infrastructure as Code Ecosystems},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.12668895},
abstract = {
    <p>The purpose of this artifact is (1) to reproduce the results
presented in the OOPSLA 2024 paper titled “When Your Infrastructure Is a
Buggy Program: Understanding Faults in Infrastructure as Code
Ecosystems”, and (2) to document the dataset and the proposed
categorization in order to facilitate further research. Specifically,
the artifact has the following structure:</p>
<ul>
<li><code>scripts/</code>: This directory contains the scripts necessary
to replicate the findings, figures, and tables introduced in our
study.</li>
<li><code>scripts/fetch/</code>: This directory contains the scripts
required to assemble the dataset of IaC bugs as outlined in Section 3.1
of our study (i.e., this directory includes the code for our repository
collection gathering and bug collection stages).</li>
<li><code>data/</code>: This directory contains the initial bug dataset
of the data collection phase as well as the “pre-baked” dataset of the
360 IaC bugs under study.</li>
<li><code>figures/</code>: A directory which is going to store the
produced paper figures.</li>
<li><code>requirements.txt</code>: A textual file declaring the required
PyPI libraries to run our analysis.</li>
</ul>

},
keywords = {Ansible, bug, Chef, deployment, IaC, infrastructure as code, Puppet}
}

@article{10.1145/3689800,
author = {Tan, Jinhao and Oliveira, Bruno C. d. S.},
title = {A Case for First-Class Environments},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689800},
doi = {10.1145/3689800},
abstract = {Formalizations of programming languages typically adopt the
 
 
 
substitution model from the lambda calculus. However, substitution
 
 
 
creates notorious complications for reasoning and implementation. Furthermore,
 
 
 
it is disconnected from practical implementations, which normally adopt
 
 
 
environments and closures.In this paper we advocate for formalizing programming languages using
 
 
 
a novel style of small-step environment-based semantics, which avoids
 
 
 
substitution and is closer to implementations.
 
 
 
We present a call-by-value statically typed calculus, called λE, using
 
 
 
our small-step environment semantics. With our
 
 
 
alternative environment semantics
 
 
 
programming language constructs for first-class environments arise naturally, without creating significant additional
 
 
 
complexity. Therefore, λE also adopts first-class environments,
 
 
 
adding expressive power that is not available in conventional lambda
 
 
 
calculi. λE is a conservative
 
 
 
extension of the call-by-value Simply Typed Lambda Calculus (STLC),
 
 
 
and employs de Bruijn indices for its formalization, which fit
 
 
 
naturally with the environment-based semantics.
 
 
 
Reasoning about λE is simple, and in many cases
 
 
 
simpler than reasoning about the traditional STLC. We show an abstract
 
 
 
machine that implements the semantics of λE, and has an easy
 
 
 
correctness proof. We also extend λE with references. We show that 
 
 
 
λE can model a simple form of first-class modules, and suggest using first-class
 
 
 
environments as an alternative to objects for modelling capabilities.
 
 
 
All technical results are formalized in the Coq proof assistant. In summary, our work shows that the small-step environment semantics that we adopt has three main and orthogonal benefits: 1) it simplifies the notorious binding problem in
 
 
 
formalizations and proof assistants; 2) it is closer to implementations;
 
 
 
and 3) additional
 
 
 
expressive power is obtained from first-class environments almost for free.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {360},
numpages = {30},
keywords = {First-class Environments, Mechanical Formalization, Semantics}
}

@software{10.5281/zenodo.13370814,
author = {Tan, Jinhao and Oliveira, Bruno C. d. S.},
title = {A Case for First-Class Environments (Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13370814},
abstract = {
    <p>The artifact includes the Coq formalization of the paper “A Case for
First-Class Environments”.</p>

},
keywords = {First-class Environments, Mechanical Formalization, Semantics}
}

@article{10.1145/3689801,
author = {Goharshady, Amir Kafshdar and Lam, Chun Kit and Parreaux, Lionel},
title = {Fast and Optimal Extraction for Sparse Equality Graphs},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689801},
doi = {10.1145/3689801},
abstract = {Equality graphs (e-graphs) are used to compactly represent equivalence classes of terms in symbolic reasoning systems. Beyond their original roots in automated theorem proving, e-graphs have been used in a variety of applications. They have become particularly important as the key ingredient in the popular technique of equality saturation, which has notable applications in compiler optimization, program synthesis, program verification, and symbolic execution, among others. In a typical equality saturation workflow, an e-graph is used to store a large number of equalities that are generated by local rewrites during a saturation phase, after which an optimal term is extracted from the e-graph as the output of the technique. However, despite its crucial role in equality saturation, e-graph extraction has received relatively little attention in the literature, which we seek to start addressing in this paper. Extraction is a challenging problem and is notably known to be NP-hard in general, so current equality saturation tools rely either on slow optimal extraction algorithms based on integer linear programming (ILP) or on heuristics that may not always produce the optimal result. In fact, in this paper, we show that e-graph extraction is hard to approximate within any constant ratio. Thus, any such heuristic will produce wildly suboptimal results in the worst case. Fortunately, we show that the problem becomes tractable when the e-graph is sparse, which is the case in many practical applications. We present a novel parameterized algorithm for extracting optimal terms from e-graphs with low treewidth, a measure of how “tree-like” a graph is, and prove its correctness. We also present an efficient Rust implementation of our algorithm and evaluate it against ILP on a number of benchmarks extracted from the Cranelift benchmark suite, a real-world compiler optimization library based on equality saturation. Our algorithm optimally extracts e-graphs with treewidths of up to 10 in a fraction of the time taken by ILP. These results suggest that our algorithm can be a valuable tool for equality saturation users who need to extract optimal terms from sparse e-graphs.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {361},
numpages = {27},
keywords = {e-graphs, equality saturation, extraction, treewidth}
}

@software{10.5281/zenodo.13624896,
author = {Goharshady, Amir Kafshdar and Lam, Chun Kit and Parreaux, Lionel},
title = {Fast and Optimal Extraction for Sparse Equality Graphs},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13624896},
abstract = {
    <p>This artifact contains the implementation of our optimal extraction
algorithm, as well as experiment with the cranelift (wasmtime)
compiler.</p>

},
keywords = {e-graphs, optimal extraction, treewidth}
}

@article{10.1145/3689802,
author = {Nagar, Kartik and Sahoo, Anmol and Chowdhury, Romit Roy and Jagannathan, Suresh},
title = {Automated Robustness Verification of Concurrent Data Structure Libraries against Relaxed Memory Models},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689802},
doi = {10.1145/3689802},
abstract = {Clients reason about the behavior of concurrent data structure                                libraries such as sets, queues, or stacks using specifications that                                capture well-understood correctness conditions, such as                                linearizability. The implementation of these libraries, however,                                focused as they are on performance, may additionally exploit relaxed                                memory behavior allowed by the language or underlying hardware that                                weaken the strong ordering and visibility constraints on shared-memory                                accesses that would otherwise be imposed by a sequentially consistent                                (SC) memory model. As an alternative to developing new specification                                and verification mechanisms for reasoning about libraries under                                relaxed memory model, we instead consider the orthogonal problem of                                library robustness, a property that holds when all possible                                behaviors of a library implementation under relaxed memory model are                                also possible under SC. In this paper, we develop a new                                automated technique for verifying robustness of library                                implementations in the context of a C11-style memory model. This task                                is challenging because a most-general client may invoke an unbounded                                number of concurrently executing library operations that can                                manipulate an unbounded number of shared locations. We establish a                                novel inductive technique for verifying library robustness that                                leverages prior work on the robustness problem for the C11 memory                                model based on the search for a non-robustness witness under SC                                executions. We crucially rely on the fact that this search is carried                                out over SC executions, and use high-level SC specifications                                (including linearizability) of the library to verify the absence of a                                non-robustness witness. Our technique is compositional - we show how                                we can safely preserve robustness of multiple interacting library                                implementations and clients using additional SC fences to guarantee                                robustness of entire executions. Experimental results on a number of                                complex realistic library implementations demonstrate the feasibility                                of our approach.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {362},
numpages = {28},
keywords = {Concurrent Library implementations, Relaxed Memory Models, Robustness}
}

@software{10.5281/zenodo.13626195,
author = {Nagar, Kartik and Sahoo, Anmol and Chowdhury, Romit Roy and Jagannathan, Suresh},
title = {Artifact - Automated Robustness Verification of Concurrent Data Structure Libraries Against Relaxed Memory Models},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.13626195},
abstract = {
    <p>The artifact for our submission contains the implementation of the
tool (Robocop) and various library implementations tested for
robustness. Robocop is an executable program, written in Python, that
takes as input a library implementation in C and library specifications
in a text format. Internally, it parses the C code, performs the
necessary analysis to generate constraints and generates SMT calls to
Z3, to discharge whether the library is robust or not.</p>

},
keywords = {verification, weak-memory}
}

@article{10.1145/3689803,
author = {Halalingaiah, Shashin and Sundaresan, Vijay and Maier, Daryl and Nandivada, V. Krishna},
title = {The ART of Sharing Points-to Analysis: Reusing Points-to Analysis Results Safely and Efficiently},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689803},
doi = {10.1145/3689803},
abstract = {Data-flow analyses like points-to analysis can vastly improve the precision of other analyses, and enable powerful code optimizations. However, whole-program points-to analysis of large Java programs tends to be expensive – both in terms of time and memory. Consequently, many compilers (both static and JIT) and program-analysis tools tend to employ faster – but more conservative – points-to analyses to improve usability. As an alternative to such trading of precision for performance, various techniques have been proposed to perform precise yet expensive fixed-point points-to analyses ahead of time in a static analyzer, store the results, and then transmit them to independent compilation/program-analysis stages that may need them. However, an underlying concern of safety affects all such techniques – can a compiler (or program analysis tool) trust the points-to analysis results generated by another compiler/tool?
 
 
 
In this work, we address this issue of trust in the context of Java, while accounting for the issue of performance. We propose ART: Analysis-results Representation Template – a novel scheme to efficiently and concisely encode results of flow-sensitive, context-insensitive points-to analysis computed by a static analyzer for use in any independent system that may benefit from such a precise points-to analysis. ART also allows for fast regeneration of the encoded sound analysis results in such systems. Our scheme has two components: (i) a producer that can statically perform expensive points-to analysis and encode the same concisely, (ii) a consumer that, on receiving such encoded results (called artwork), can regenerate the points-to analysis results encoded by the artwork if it is deemed “safe”. The regeneration scheme completely avoids fixed-point computations and thus can help consumers like static analyzers and JIT compilers to obtain precise points-to information without paying a prohibitively high cost. We demonstrate the usage of ART by implementing a producer (in Soot) and two consumers (in Soot and the Eclipse OpenJ9 JIT compiler). We have evaluated our implementation over various benchmarks from the DaCapo and SPECjvm2008 suites. Our results demonstrate that using ART, a consumer can obtain precise flow-sensitive, context-insensitive points-to analysis results in less than (average) 1\% of the time taken by a static analyzer to perform the same analysis, with the storage overhead of ART representing a small fraction of the program size (average around 4\%).},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {363},
numpages = {27},
keywords = {IDFA, Java program analysis, points-to analysis, staged analysis}
}

@article{10.1145/3689804,
author = {Li, Haofeng and Shi, Chenghang and Lu, Jie and Li, Lian and Xue, Jingling},
title = {Boosting the Performance of Alias-Aware IFDS Analysis with CFL-Based Environment Transformers},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689804},
doi = {10.1145/3689804},
abstract = {The IFDS algorithm is pivotal in solving field-sensitive data-flow problems. However, its conventional use of access paths for field sensitivity leads to the generation of a large number of data-flow facts. This causes scalability challenges in larger programs, limiting its practical application in extensive codebases. In response, we propose a new field-sensitive technique that reinterprets the generation of access paths as a Context-Free Language (CFL) for field-sensitivity and formulates it as an IDE problem. This approach significantly reduces the number of data-flow facts generated and handled during the analysis, which is a major factor in performance degradation.    To demonstrate the effectiveness of this approach, we developed a taint analysis tool, IDEDroid, in the IFDS/IDE framework. IDEDroid outperforms FlowDroid, an established IFDS-based taint analysis tool, in the analysis of 24 major Android apps while improving its precision (guaranteed theoretically). The speed improvement ranges from 2.1\texttimes{} to 2,368.4\texttimes{}, averaging at 222.0\texttimes{}, with precision gains reaching up to 20.0\% (in terms of false positives reduced). This performance indicates that IDEDroid is substantially more effective in detecting information-flow leaks, making it a potentially superior tool for mobile app vetting in the market.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {364},
numpages = {29},
keywords = {Alias Analysis, CFL-Reachability, IDE, IFDS, Taint Analysis}
}

@software{10.6084/m9.figshare.26105056.v1,
author = {Li, Haofeng and Shi, Chenghang and Lu, Jie and Li, Lian and Xue, Jingling},
title = {Boosting the Performance of Alias-Aware IFDS Analysis with CFL-Based Environment Transformers (Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.6084/m9.figshare.26105056.v1},
abstract = {
    <p>The IFDS algorithm is pivotal in solving field-sensitive data-flow
problems. However, its conventional use of access paths for field
sensitivity leads to the generation of a large number of data-flow
facts. This causes scalability challenges in larger programs, limiting
its practical application in extensive codebases. In response, we
propose a new field-sensitive technique that reinterprets the generation
of access paths as a CFL for field-sensitivity and formulates it as an
IDE problem. This approach significantly reduces the number of data-flow
facts generated and handled during the analysis, which is a major factor
in performance degradation.</p>
<p>To demonstrate the effectiveness of this approach, we developed a
taint analysis tool, IDEDroid, in the IFDS/IDE framework. IDEDroid
outperforms FlowDroid, an established IFDS-based taint analysis tool, in
the analysis of 24 major Android apps while improving its precision
(guaranteed theoretically). The speed improvement ranges from 2.1\texttimes{} to
2,368.4\texttimes{}, averaging at 222.0\texttimes{}, with precision gains reaching up to 20.0\%
(in terms of false positives reduced). This performance indicates that
IDEDroid is substantially more effective in detecting information-flow
leaks, making it a potentially superior tool for mobile app vetting in
the market.</p>

},
keywords = {Alias Analysis, CFL-Reachability, IDE, IFDS, Taint Analysis}
}

@article{10.1145/3689805,
author = {Sekiyama, Taro and Unno, Hiroshi},
title = {Higher-Order Model Checking of Effect-Handling Programs with Answer-Type Modification},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689805},
doi = {10.1145/3689805},
abstract = {Model checking is one of the successful program verification methodologies. Since the seminal work by Ong, the model checking of higher-order programs―called higher-order model checking, or HOMC for short―has gained attention. It is also crucial for making HOMC applicable to real-world software to address programs involving computational effects. Recently, Dal Lago and Ghyselen considered an extension of HOMC to algebraic effect handlers, which enable programming the semantics of effects. They showed a negative result for HOMC with algebraic effect handlers―it is undecidable. In this work, we explore a restriction on programs with algebraic effect handlers which ensures the decidability of HOMC while allowing implementations of various effects. We identify the crux of the undecidability as the use of an unbounded number of algebraic effect handlers being active at the same time. To prevent it, we introduce answer-type modification (ATM), which can bound the number of algebraic effect handlers that can be active at the same time. We prove that ATM can ensure the decidability of HOMC and show that it accommodates a wide range of effects. To evaluate our approach, we implemented an automated verifier EffCaml based on the presented techniques and confirmed that the program examples discussed in this paper can be automatically verified.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {365},
numpages = {30},
keywords = {algebraic effect handlers, answer-type modification, model checking}
}

@article{10.1145/3649811,
author = {Yuan, Charles and Villanyi, Agnes and Carbin, Michael},
title = {Quantum Control Machine: The Limits of Control Flow in Quantum Programming},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649811},
doi = {10.1145/3649811},
abstract = {Quantum algorithms for tasks such as factorization, search, and simulation rely on control flow such as branching and iteration that depends on the value of data in superposition. High-level programming abstractions for control flow, such as switches, loops, higher-order functions, and continuations, are ubiquitous in classical languages. By contrast, many quantum languages do not provide high-level abstractions for control flow in superposition, and instead require the use of hardware-level logic gates to implement such control flow.  

The reason for this gap is that whereas a classical computer supports control flow abstractions using a program counter that can depend on data, the typical architecture of a quantum computer does not analogously provide a program counter that can depend on data in superposition. As a result, the complete set of control flow abstractions that can be correctly realized on a quantum computer has not yet been established.  

In this work, we provide a complete characterization of the properties of control flow abstractions that are correctly realizable on a quantum computer. First, we prove that even on a quantum computer whose program counter exists in superposition, one cannot correctly realize control flow in quantum algorithms by lifting the classical conditional jump instruction to work in superposition. This theorem denies the ability to directly lift general abstractions for control flow such as the λ-calculus from classical to quantum programming.  

In response, we present the necessary and sufficient conditions for control flow to be correctly realizable on a quantum computer. We introduce the quantum control machine, an instruction set architecture featuring a conditional jump that is restricted to satisfy these conditions. We show how this design enables a developer to correctly express control flow in quantum algorithms using a program counter in place of logic gates.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {94},
numpages = {28},
keywords = {quantum instruction set architectures, quantum programming languages}
}

@software{10.5281/zenodo.10452601,
author = {Yuan, Charles and Villanyi, Agnes and Carbin, Michael},
title = {Artifact for Quantum Control Machine: The Limits of Control Flow in Quantum Programming},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10452601},
abstract = {
    <p>This artifact contains an implementation of a simulator for the quantum control machine and the programs from the case study as presented in the paper.</p>

},
keywords = {quantum instruction set architectures, quantum programming languages}
}

@article{10.1145/3649812,
author = {Crichton, Will and Krishnamurthi, Shriram},
title = {Profiling Programming Language Learning},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649812},
doi = {10.1145/3649812},
abstract = {This paper documents a year-long experiment to “profile” the process of learning a programming language: gathering data to understand what makes a language hard to learn, and using that data to improve the learning process. We added interactive quizzes to The Rust Programming Language, the official textbook for learning Rust. Over 13 months, 62,526 readers answered questions 1,140,202 times. First, we analyze the trajectories of readers. We find that many readers drop-out of the book early when faced with difficult language concepts like Rust’s ownership types. Second, we use classical test theory and item response theory to analyze the characteristics of quiz questions. We find that better questions are more conceptual in nature, such as asking why a program does not compile vs. whether a program compiles. Third, we performed 12 interventions into the book to help readers with difficult questions. We find that on average, interventions improved quiz scores on the targeted questions by +20},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {95},
numpages = {26},
keywords = {digital textbooks, item response theory, rust education}
}

@software{10.5281/zenodo.10798571,
author = {Crichton, Will and Krishnamurthi, Shriram},
title = {Artifact for "Profiling Programming Language Learning"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10798571},
abstract = {
    <p>These are Docker images that contain the codebase, data, and analysis scripts for our OOPSLA 2024 paper “Profiling Programming Language Learning”.</p>

},
keywords = {digital textbooks, item response theory, rust education}
}

@article{10.1145/3649813,
author = {Paradis, Anouk and Dekoninck, Jasper and Bichsel, Benjamin and Vechev, Martin},
title = {Synthetiq: Fast and Versatile Quantum Circuit Synthesis},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649813},
doi = {10.1145/3649813},
abstract = {To implement quantum algorithms on quantum computers it is crucial to decompose their operators into the limited gate set supported by those computers. Unfortunately, existing works automating this essential task are generally slow and only applicable to narrow use cases.We present Synthetiq, a method to synthesize quantum circuits implementing a given specification over arbitrary finite gate sets, which is faster and more versatile than existing works. Synthetiq utilizes Simulated Annealing instantiated with a novel, domain-specific energy function that allows developers to leverage partial specifications for better efficiency. Synthetiq further couples this synthesis method with a custom simplification pass, to ensure efficiency of the found circuits. We experimentally demonstrate that Synthetiq can generate better implementations than were previously known for multiple relevant quantum operators including RCCCX, CCT, CCiSWAP, C√SWAP, and C√iSWAP. Our extensive evaluation also demonstrates Synthetiq frequently outperforms a wide variety of more specialized tools in their own domains, including (i) ‍the well-studied task of synthesizing fully specified operators in the Clifford+T gate set, (ii) ‍є-approximate synthesis of multi-qubit operators in the same gate set, and (iii) ‍synthesis tasks with custom gate sets. On all those tasks, Synthetiq is typically one to two orders of magnitude faster than previous state-of-the-art and can tackle problems that were previously out of the reach of any synthesis tool.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {96},
numpages = {28},
keywords = {Clifford+T, Quantum Circuits, Synthesis}
}

@software{10.5281/zenodo.10777503,
author = {Paradis, Anouk and Dekoninck, Jasper and Bichsel, Benjamin and Vechev, Martin},
title = {Reproduction Package for the Article "Synthetiq: Fast and Versatile Quantum Circuit Synthesis"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10777503},
abstract = {
    <p>This artifact contains the code for the paper ‘Synthetiq: Fast and Versatile Quantum Circuit Synthesis’. Synthetiq is a tool to synthesize quantum circuits implementing a given (partial) specification over arbitrary finite gate sets and is faster and more versatile than existing works.</p>
<p>This artifact contains: - the code of our tool Synthetiq and installation instructions; - precise instructions to reproduce our evaluation; - usage guide to use Synthetiq on new operators; - all quantum circuits found by Synthetiq and mentioned in the paper.</p>

},
keywords = {Clifford+T, Quantum Circuits, Synthesis}
}

@article{10.1145/3649814,
author = {Yadavally, Aashish and Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {A Learning-Based Approach to Static Program Slicing},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649814},
doi = {10.1145/3649814},
abstract = {Traditional program slicing techniques are crucial for early bug detection and manual/automated debugging of online code snippets. Nevertheless, their inability to handle incomplete code hinders their real-world applicability in such scenarios. To overcome these challenges, we present NS-Slicer, a novel learning-based approach that predicts static program slices for both complete and partial code Our tool leverages a pre-trained language model to exploit its understanding of fine-grained variable-statement dependencies within source code. With this knowledge, given a variable at a specific location and a statement in a code snippet, NS-Slicer determines whether the statement belongs to the backward slice or forward slice, respectively. We conducted a series of experiments to evaluate NS-Slicer's performance. On complete code, it predicts the backward and forward slices with an F1-score of 97.41\% and 95.82\%, respectively, while achieving an overall F1-score of 96.77\%. Notably, in 85.20\% of the cases, the static program slices predicted by NS-Slicer exactly match entire slices from the oracle. For partial programs, it achieved an F1-score of 96.77\%–97.49\% for backward slicing, 92.14\%–95.40\% for forward slicing, and an overall F1-score of 94.66\%–96.62\%. Furthermore, we demonstrate NS-Slicer's utility in vulnerability detection (VD), integrating its predicted slices into an automated VD tool. In this setup, the tool detected vulnerabilities in Java code with a high F1-score of 73.38\%. We also include the analyses studying NS-Slicer’s promising performance and limitations, providing insights into its understanding of intrinsic code properties such as variable aliasing, leading to better slicing.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {97},
numpages = {27},
keywords = {AI4SE, Debugging, Neural Networks, Pre-Trained Language Models, Static Slicing, Vulnerability Detection}
}

@software{10.5281/zenodo.10463878,
author = {Yadavally, Aashish and Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {Artifact for "A Learning-Based Approach to Static Program Slicing"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10463878},
abstract = {
    <p>NS-Slicer is a learning-based static program slicing tool, which extends such an analysis to partial Java programs. The source code, data, and model artifacts are publicly available on GitHub (https://github.com/aashishyadavally/ns-slicer), and Zenodo (https://zenodo.org/records/10463878).</p>

},
keywords = {AI4SE, Debugging, Neural Networks, Pre-Trained Language Models, Static Slicing, Vulnerability Detection}
}

@article{10.1145/3649815,
author = {Zhang, Chi and Wang, Linzhang and Rigger, Manuel},
title = {Finding Cross-Rule Optimization Bugs in Datalog Engines},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649815},
doi = {10.1145/3649815},
abstract = {Datalog is a popular and widely-used declarative logic programming language. Datalog engines apply many cross-rule optimizations; bugs in them can cause incorrect results. To detect such optimization bugs, we propose an automated testing approach called Incremental Rule Evaluation (IRE), which synergistically tackles the test oracle and test case generation problem. The core idea behind the test oracle is to compare the results of an optimized program and a program without cross-rule optimization; any difference indicates a bug in the Datalog engine. Our core insight is that, for an optimized, incrementally-generated Datalog program, we can evaluate all rules individually by constructing a reference program to disable the optimizations that are performed among multiple rules. Incrementally generating test cases not only allows us to apply the test oracle for every new rule generated—we also can ensure that every newly added rule generates a non-empty result with a given probability and eschew recomputing already-known facts. We implemented IRE as a tool named Deopt, and evaluated Deopt on four mature Datalog engines, namely Souffl\'{e}, CozoDB, μZ, and DDlog, and discovered a total of 30 bugs. Of these, 13 were logic bugs, while the remaining were crash and error bugs. Deopt can detect all bugs found by queryFuzz, a state-of-the-art approach. Out of the bugs identified by Deopt, queryFuzz might be unable to detect 5. Our incremental test case generation approach is efficient; for example, for test cases containing 60 rules, our incremental approach can produce 1.17\texttimes{} (for DDlog) to 31.02\texttimes{} (for Souffl\'{e}) as many valid test cases with non-empty results as the naive random method. We believe that the simplicity and the generality of the approach will lead to its wide adoption in practice.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {98},
numpages = {27},
keywords = {Datalog engine testing, cross-rule optimization bugs, test oracle}
}

@software{10.5281/zenodo.10609061,
author = {Zhang, Chi and Wang, Linzhang and Rigger, Manuel},
title = {Artifact for "Finding Cross-rule Optimization Bugs in Datalog Engines"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10609061},
abstract = {
    <p>The artifact consists of two main components:</p>
<p>1、The source code of the tool Datalog Engine Optimization Tester (Deopt), which we used to find all of the bugs presented in our paper. 2、The data and reproduce documents for the results of the evaluation in the paper.</p>

},
keywords = {cross-rule optimization bugs, Datalog engine testing, test oracle}
}

@article{10.1145/3649816,
author = {Liu, Jie and Zhao, Zhongyuan and Ding, Zijian and Brock, Benjamin and Rong, Hongbo and Zhang, Zhiru},
title = {UniSparse: An Intermediate Language for General Sparse Format Customization},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649816},
doi = {10.1145/3649816},
abstract = {The ongoing trend of hardware specialization has led to a growing use of custom data formats when processing sparse workloads, which are typically memory-bound. These formats facilitate optimized software/hardware implementations by utilizing sparsity pattern- or target-aware data structures and layouts to enhance memory access latency and bandwidth utilization. However, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. Additionally, because these frameworks represent formats using a limited set of per-dimension attributes, they lack the flexibility to accommodate numerous new variations of custom sparse data structures and layouts.  
To overcome this deficiency, we propose UniSparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. Unlike the existing attribute-based frameworks, UniSparse decouples the logical representation of the sparse tensor (i.e., the data structure) from its low-level memory layout, enabling the customization of both. As a result, a rich set of format customizations can be succinctly expressed in a small set of well-defined query, mutation, and layout primitives. We also develop a compiler leveraging the MLIR infrastructure, which supports adaptive customization of formats, and automatic code generation of format conversion and compute operations for heterogeneous architectures. We demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with specialized formats on multiple different hardware targets, including an Intel CPU, an NVIDIA GPU, an AMD Xilinx FPGA, and a simulated processing-in-memory (PIM) device.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {99},
numpages = {29},
keywords = {compilers, heterogeneous systems, programming languages, sparse data formats}
}

@software{10.5281/zenodo.10464500,
author = {Liu, Jie and Zhao, Zhongyuan and Ding, Zijian and Brock, Benjamin and Rong, Hongbo and Zhang, Zhiru},
title = {Reproduction Package for Article `UniSparse: An Intermediate LanguageforGeneralSparseFormatCustomization'},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10464500},
abstract = {
    <p>UniSparse is an intermediate language for general sparse format customization. UniSparse automates code generation for custom sparse format conversion and compute operations targeting heterogeneous architectures.</p>

},
keywords = {compilers, heterogeneous systems, programming languages, sparse data formats}
}

@article{10.1145/3649817,
author = {Lamba, Ada and Taylor, Max and Beardsley, Vincent and Bambeck, Jacob and Bond, Michael D. and Lin, Zhiqiang},
title = {Cocoon: Static Information Flow Control in Rust},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649817},
doi = {10.1145/3649817},
abstract = {Information flow control (IFC) provides confidentiality by enforcing noninterference, which ensures that high-secrecy values cannot affect low-secrecy values. Prior work introduces fine-grained IFC approaches that modify the programming language and use non-standard compilation tools, impose run-time overhead, or report false secrecy leaks—all of which hinder adoption.  
		  
This paper presents Cocoon, a Rust library for static type-based IFC that uses the unmodified Rust language and compiler. The key insight of Cocoon lies in leveraging Rust’s type system and procedural macros to establish an effect system that enforces noninterference. A performance evaluation shows that using Cocoon increases compile time but has no impact on application performance. To demonstrate Cocoon’s utility, we retrofitted two popular Rust programs, the Spotify TUI client and Mozilla’s Servo browser engine, to use Cocoon to enforce limited confidentiality policies},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {100},
numpages = {28},
keywords = {Rust, information flow control, type and effect systems}
}

@software{10.5281/zenodo.10798978,
author = {Lamba, Ada and Taylor, Max and Beardsley, Vincent and Bambeck, Jacob and Bond, Michael D. and Lin, Zhiqiang},
title = {Implementation for "Cocoon: Static Information Flow Control in Rust"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10798978},
abstract = {
    <p>Cocoon is a Rust library that provides types and mechanisms for statically enforcing information flow control in Rust programs. Cocoon is currently intended to prevent programmer errors such as accidentally leaking a “private” value to an untrusted function or other value. Cocoon does not currently address dynamic labels, integrity labels, OS integration, or leaks caused by other means such as side-channel attacks.</p>
<p>This artifact contains the Cocoon library itself, all examples presented in the paper, and evaluation scripts.</p>

},
keywords = {information flow control, Rust, type and effect systems}
}

@article{10.1145/3649818,
author = {Clement, Blaudeau and R\'{e}my, Didier and Radanne, Gabriel},
title = {Fulfilling OCaml Modules with Transparency},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649818},
doi = {10.1145/3649818},
abstract = {ML modules come as an additional layer on top of a core language to offer  
large-scale notions of composition and abstraction. They largely  
contributed to the success of OCaml and SML. While modules are easy to write  
for common cases, their advanced use may become tricky. Additionally,  
despite a long line of works, their meta-theory remains difficult to  
comprehend, with involved soundness proofs. In fact, the module layer of  
OCaml does not currently have a formal specification and its implementation  
has some surprising behaviors.  

Building on previous translations from ML modules to Fω, we propose a type  
system, called Mω, that covers a large subset of OCaml modules, including  
both applicative and generative functors, and extended with transparent  
ascription. This system produces signatures in an OCaml-like syntax extended  
with Fω quantifiers. We provide a reverse translation from Mω signatures to  
path-based source signatures along with a characterization of signature  
avoidance cases, making Mω signatures well suited to serve as a new internal  
representation for a typechecker.  

The soundness of the type system is shown by elaboration in Fω. We improve  
over previous encodings of sealing within applicative functors, by the  
introduction of transparent existential types, a weaker form of existential  
types that can be lifted out of universal and arrow types. This shines a new  
light on the form of abstraction provided by applicative functors and brings  
their treatment much closer to those of generative functors.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {101},
numpages = {29},
keywords = {F-omega, ML, applicative functors, existential types, signature avoidance}
}

@article{10.1145/3649819,
author = {Alshnakat, Anoud and Lundberg, Didrik and Guanciale, Roberto and Dam, Mads},
title = {HOL4P4: Mechanized Small-Step Semantics for P4},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649819},
doi = {10.1145/3649819},
abstract = {We present the first semantics of the network data plane programming language P4 able to adequately capture all key features of P416, the most recent version of P4, including external functions (externs) and concurrency. These features are intimately related since, in P4, extern invocations are the only points at which one execution thread can affect another. Reflecting P4’s lack of a general-purpose memory and the presence of multithreading the semantics is given in small-step style and eschews the use of a heap. In addition to the P4 language itself, we provide an architectural level semantics, which allows the composition of P4-programmed blocks, models end-to-end packet processing, and can take into account features such as arbitration and packet recirculation. A corresponding type system is provided with attendant progress, preservation, and type-soundness theorems. Semantics, type system, and meta-theory are formalized in the HOL4 theorem prover. From this formalization, we derive a HOL4 executable semantics that supports verified execution of programs with partially symbolic packets able to validate simple end-to-end program properties.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {102},
numpages = {27},
keywords = {P4, formal verification, interactive theorem proving, programming language semantics}
}

@software{10.5281/zenodo.10796440,
author = {Alshnakat, Anoud and Lundberg, Didrik and Guanciale, Roberto and Dam, Mads},
title = {OOPSLA 2024 Artifact: HOL4P4: Mechanized Small-Step Semantics for P4},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10796440},
abstract = {
    <p>For a detailed description of this artifact, see OVERVIEW.md or OVERVIEW.pdf among the files.</p>
<p>HOL4P4-OOPSLA2024-source.tar.gz is a compressed directory with the source code, and hol4p4-amd64.tar.gz and hol4p4-aarch64.tar.gz are compressed Docker images for x86_64-based and ARM64-based CPUs, respectively.</p>

},
keywords = {formal verification, interactive theorem proving, P4, programming language semantics}
}

@article{10.1145/3649820,
author = {Sundram, Shiv and Tariq, Muhammad Usman and Kjolstad, Fredrik},
title = {Compiling Recurrences over Dense and Sparse Arrays},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649820},
doi = {10.1145/3649820},
abstract = {We present a framework for compiling recurrence equations into native code. In our framework, users specify a system of recurrences, the types of data structures that store inputs and outputs, and scheduling commands for optimization. Our compiler then lowers these specifications into native code that respects the dependencies in the recurrence equations. Our compiler can generate code over both sparse and dense data structures, and determines if the recurrence system is solvable with the provided scheduling primitives. We evaluate the performance and correctness of the generated code on several recurrences, from domains as diverse as dense and sparse matrix solvers, dynamic programming, graph problems, and sparse tensor algebra. We demonstrate that the generated code has competitive performance to hand-optimized implementations in libraries. However, these handwritten libraries target specific recurrences, specific data structures, and specific optimizations. Our system, on the other hand, automatically generates implementations from recurrences, data formats, and schedules, giving our system more generality than library approaches.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {103},
numpages = {26},
keywords = {dynamic programming, linear algebra, recurrences, sparse tensor algebra}
}

@software{10.5281/zenodo.10774458,
author = {Sundram, Shiv and Tariq, Muhammad Usman and Kjolstad, Fredrik},
title = {Artifact for OOPSLA 2024 Paper: Compiling Recurrences over Dense and Sparse Arrays (version 1)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10774458},
abstract = {
    <p>This is a docker container containing the artifact for the OOPSLA 2024 accepted paper “Compiling Recurrences over Dense and Sparse Arrays” by Shiv Sundram, Muhammad Usman Tariq, Fredrik Kjolstad</p>
<p>Directions for running artifact and reproducing the paper’s figures can be found in the Getting Started Guide:</p>
<p>https://docs.google.com/document/d/1YCC8AskQYFQfUQ1_jtW2OOqWrT9-qpizTR4BUmbYBXQ/edit?usp=sharing</p>

},
keywords = {Domain specific languages, Recurrences, Software and its engineering, Source code generation}
}

@article{10.1145/3649821,
author = {Zilberstein, Noam and Saliling, Angelina and Silva, Alexandra},
title = {Outcome Separation Logic: Local Reasoning for Correctness and Incorrectness with Computational Effects},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649821},
doi = {10.1145/3649821},
abstract = {Separation logic’s compositionality and local reasoning properties have led to significant advances in scalable static analysis. But program analysis has new challenges—many programs display computational effects and, orthogonally, static analyzers must handle incorrectness too. We present Outcome Separation Logic (OSL), a program logic that is sound for both correctness and incorrectness reasoning in programs with varying effects. OSL has a frame rule—just like separation logic—but uses different underlying assumptions that open up local reasoning to a larger class of properties than can be handled by any single existing logic. Building on this foundational theory, we also define symbolic execution algorithms that use bi-abduction to derive specifications for programs with effects. This involves a new tri-abduction procedure to analyze programs whose execution branches due to effects such as nondeterministic or probabilistic choice. This work furthers the compositionality promised by separation logic by opening up the possibility for greater reuse of analysis tools across two dimensions: bug-finding vs verification in programs with varying effects.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {104},
numpages = {29},
keywords = {Incorrectness, Outcome Logic, Separation Logic}
}

@article{10.1145/3649822,
author = {Wang, Di and Reps, Thomas},
title = {Newtonian Program Analysis of Probabilistic Programs},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649822},
doi = {10.1145/3649822},
abstract = {Due to their quantitative nature, probabilistic programs pose non-trivial challenges for designing compositional and efficient program analyses. Many analyses for probabilistic programs rely on iterative approximation. This article presents an interprocedural dataflow-analysis framework, called NPA-PMA, for designing and implementing (partially) non-iterative program analyses of probabilistic programs with unstructured control-flow, nondeterminism, and general recursion. NPA-PMA is based on Newtonian Program Analysis (NPA), a generalization of Newton's method to solve equation systems over semirings. The key challenge for developing NPA-PMA is to handle multiple kinds of confluences in both the algebraic structures that specify analyses and the equation systems that encode control flow: semirings support a single confluence operation, whereas NPA-PMA involves three confluence operations (conditional, probabilistic, and nondeterministic).  

Our work introduces ω-continuous pre-Markov algebras (ωPMAs) to factor out common parts of different analyses; adopts regular infinite-tree expressions to encode probabilistic programs with unstructured control-flow; and presents a linearization method that makes Newton's method applicable to the setting of regular-infinite-tree equations over ωPMAs. NPA-PMA allows analyses to supply a non-iterative strategy to solve linearized equations. Our experimental evaluation demonstrates that (i) NPA-PMA holds considerable promise for outperforming Kleene iteration, and (ii) provides great generality for designing program analyses.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {105},
numpages = {29},
keywords = {Algebraic Program Analysis, Interprocedural Program Analysis, Newton's Method, Probabilistic Programs}
}

@software{10.5281/zenodo.10791709,
author = {Wang, Di and Reps, Thomas},
title = {Newtonian Program Analysis of Probabilistic Programs (Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10791709},
abstract = {
    <p>This artifact provides a prototype implementation of the framework of Newtonian Program Analysis with Pre-Markov Algebras (NPA-PMA). NPA-PMA is an interprocedural dataflow-analysis framework for designing and implementing (partially) non-iterative program analyses of probabilistic programs with unstructured control-flow, nondeterminism, and general recursion. To demonstrate the usage of NPA-PMA, this artifact also includes five instantiations for four analyses: Bayesian-inference analysis, higher-moment analysis of accumulated rewards, expectation-invariant analysis, and expectation-recurrence analysis.</p>

},
keywords = {Algebraic Program Analysis, Interprocedural Program Analysis, Newton's Method, Probabilistic Programs}
}

@article{10.1145/3649823,
author = {Lu, Kuang-Chen and Krishnamurthi, Shriram},
title = {Identifying and Correcting Programming Language Behavior Misconceptions},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649823},
doi = {10.1145/3649823},
abstract = {Misconceptions about core linguistic concepts like mutable  
variables, mutable compound data, and their interaction with scope  
and higher-order functions seem to be widespread.  
But  
how do we detect  
them, given that experts have blind spots and may not realize the  
myriad ways in which students can misunderstand programs?  
Furthermore, once identified, what can we do to correct them?  

In this paper, we present a curated list of misconceptions, and an  
instrument to detect them. These are distilled from student work  
over several years and match and extend prior research. We also  
present an automated, self-guided tutoring system. The tutor builds  
on strategies in the education literature and is explicitly designed  
around identifying and correcting misconceptions.  

We have tested the tutor in multiple settings.  
Our data consistently show that (a) the misconceptions we tackle are  
widespread, and (b) the tutor appears to improve understanding.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {106},
numpages = {28},
keywords = {automated interactive tutors, misconceptions, program behavior/semantics}
}

@software{10.1145/3580432,
author = {Lu, Kuang-Chen and Krishnamurthi, Shriram},
title = {Reproduction Package for Article `Identifying and Correcting Programming Language Behavior Misconceptions'},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3580432},
abstract = {
    <p>We claim in the paper (Section 7) that “we provide all the misinterpreters in the artifact.” To support this claim, this artifact provides the source code of all misinterpreters in ./Misinterpreters. In addition, we provide the source code of the reference interpreter ./Misinterpreters/smol-referential.rkt, which represents the correct conception. ./Misinterpreters contains a few other files:</p>
<ul>
<li>Metadata that makes the folder a Racket package: ./Misinterpreters/info.rkt</li>
<li>The definitions of AST and a parser (from S-expressions to AST): ./Misinterpreters/smol-syntax.rkt and ./Misinterpreters/parse.rkt</li>
<li>Shared helper functions for all (mis)interpreters: ./Misinterpreters/utilities.rkt</li>
</ul>
<p>We present statistical results in the paper. Though not promised in the paper, to support all those claims, this artifact also provides an R Markdown ./Paper.Rmd that computes (most of) the numbers, generates the figures, and performs the hypothesis tests. For numbers that cannot be computed, the R Markdown gives justification.</p>
<p>./Paper.Rmd depends on the ./SMoL Tutor folder, which includes</p>
<ul>
<li>Metadata about SMoL Tutor: ./SMoL Tutor/Tasks.csv and ./SMoL Tutor/Choices.csv</li>
<li>Data collected by SMoL Tutor: ./SMoL Tutor/Datasets/</li>
<li>A script that tags wrong answers with misinterpreters: ./SMoL Tutor/Tag_Answers.rkt</li>
</ul>

},
keywords = {automated interactive tutors, misconceptions, program behavior/semantics}
}

@article{10.1145/3649824,
author = {Chatterjee, Krishnendu and Goharshady, Amir Kafshdar and Meggendorfer, Tobias and \v{Z}ikeli\'{c}, undefinedor\dj{}e},
title = {Quantitative Bounds on Resource Usage of Probabilistic Programs},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649824},
doi = {10.1145/3649824},
abstract = {Cost analysis, also known as resource usage analysis, is the task of finding bounds on the total cost of a program and is a well-studied problem in static analysis. In this work, we consider two classical quantitative problems in cost analysis for probabilistic programs. The first problem is to find a bound on the expected total cost of the program. This is a natural measure for the resource usage of the program and can also be directly applied to average-case runtime analysis. The second problem asks for a tail bound, i.e. ‍given a threshold t the goal is to find a probability bound p such that ℙ[total cost ≥ t] ≤ p. Intuitively, given a threshold t on the resource, the problem is to find the likelihood that the total cost exceeds this threshold. First, for expectation bounds, a major obstacle in previous works on cost analysis is that they can handle only non-negative costs or bounded variable updates. In contrast, we provide a new variant of the standard notion of cost martingales, that allows us to find expectation bounds for a class of programs with general positive or negative costs and no restriction on the variable updates. More specifically, our approach is applicable as long as there is a lower bound on the total cost incurred along every path. Second, for tail bounds, all previous methods are limited to programs in which the expected total cost is finite. In contrast, we present a novel approach, based on a combination of our martingale-based method for expectation bounds with a quantitative safety analysis, to obtain a solution to the tail bound problem that is applicable even to programs with infinite expected cost. Specifically, this allows us to obtain runtime tail bounds for programs that do not terminate almost-surely. In summary, we provide a novel combination of martingale-based cost analysis and quantitative safety analysis that is able to find expectation and tail cost bounds for probabilistic programs, without the restrictions of non-negative costs, bounded updates, or finiteness of the expected total cost. Finally, we provide experimental results showcasing that our approach can solve instances that were beyond the reach of previous methods.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {107},
numpages = {30},
keywords = {Cost Analysis, Martingales, Probabilistic Programming, Quantitative Bounds, Static Analysis}
}

@software{10.5281/zenodo.10457566,
author = {Chatterjee, Krishnendu and Goharshady, Amir Kafshdar and Meggendorfer, Tobias and \v{Z}ikeli\'{c}, undefinedor\dj{}e},
title = {Artefact for: Quantitative Bounds on Resource Usage of Probabilistic Programs},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10457566},
abstract = {
    <p>The artefact for the OOPSLA 2024 paper “Quantitative Bounds on Resource Usage of Probabilistic Programs”.</p>

},
keywords = {Cost Analysis, Martingales, Probabilistic Programming, Quantitative Bounds, Static Analysis}
}

@article{10.1145/3649825,
author = {Ding, Yangruibo and Min, Marcus J. and Kaiser, Gail and Ray, Baishakhi},
title = {CYCLE: Learning to Self-Refine the Code Generation},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649825},
doi = {10.1145/3649825},
abstract = {Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction. For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well. In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate CYCLE on three popular code generation benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE successfully maintains, sometimes improves, the quality of one-time code generation, while significantly improving the self-refinement capability of code LMs. We implement four variants of CYCLE with varied numbers of parameters across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently boosts the code generation performance, by up to 63.5},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {108},
numpages = {27},
keywords = {Code Generation, Code Language Models, Iterative Programming, Source Code Modeling}
}

@article{10.1145/3649826,
author = {Honor\'{e}, Wolf and Qiu, Longfei and Kim, Yoonseung and Shin, Ji-Yong and Kim, Jieung and Shao, Zhong},
title = {AdoB: Bridging Benign and Byzantine Consensus with Atomic Distributed Objects},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649826},
doi = {10.1145/3649826},
abstract = {Achieving consensus is a challenging and ubiquitous problem in distributed systems that is only made harder by the introduction of malicious byzantine servers. While significant effort has been devoted to the benign and byzantine failure models individually, no prior work has considered the mechanized verification of both in a generic way. We claim this is due to the lack of an appropriate abstraction that is capable of representing both benign and byzantine consensus without either losing too much detail or becoming impractically complex. We build on recent work on the atomic distributed object model to fill this void with a novel abstraction called AdoB. In addition to revealing important insights into the essence of consensus, this abstraction has practical benefits for easing distributed system verification. As a case study, we proved safety and liveness properties for AdoB in Coq, which are the first such mechanized proofs to handle benign and byzantine consensus in a unified manner. We also demonstrate that AdoB faithfully models real consensus protocols by proving it is refined by standard network-level specifications of Fast Paxos and a variant of Jolteon.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {109},
numpages = {30},
keywords = {byzantine, consensus protocols, distributed systems, formal verification, liveness, proof assistants, refinement}
}

@software{10.5281/zenodo.10727570,
author = {Honor\'{e}, Wolf and Qiu, Longfei and Kim, Yoonseung and Shin, Ji-Yong and Kim, Jieung and Shao, Zhong},
title = {Artifact For "AdoB: Bridging Benign and Byzantine Consensus with Atomic Distributed Objects"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10727570},
abstract = {
    <p>This artifact includes the Coq definitions and proofs to support the claims in the paper. It also includes the OCaml shim layer needed to evaluate the GenJolteon implementation.</p>

},
keywords = {byzantine, consensus protocols, distributed systems, formal verification, liveness, proof assistants, refinement}
}

@article{10.1145/3649827,
author = {Xu, Ziyang and Chon, Yebin and Su, Yian and Tan, Zujun and Apostolakis, Sotiris and Campanoni, Simone and August, David I.},
title = {PROMPT: A Fast and Extensible Memory Profiling Framework},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649827},
doi = {10.1145/3649827},
abstract = {Memory profiling captures programs’ dynamic memory behavior, assisting programmers in debugging, tuning, and enabling advanced compiler optimizations like speculation-based automatic parallelization. As each use case demands its unique program trace summary, various memory profiler types have been developed. Yet, designing practical memory profilers often requires extensive compiler expertise, adeptness in program optimization, and significant implementation effort. This often results in a void where aspirations for fast and robust profilers remain unfulfilled. To bridge this gap, this paper presents PROMPT, a framework for streamlined development of fast memory profilers. With PROMPT, developers need only specify profiling events and define the core profiling logic, bypassing the complexities of custom instrumentation and intricate memory profiling components and optimizations. Two state-of-the-art memory profilers were ported with PROMPT where all features preserved. By focusing on the core profiling logic, the code was reduced by more than 65\% and the profiling overhead was improved by 5.3\texttimes{} and 7.1\texttimes{} respectively. To further underscore PROMPT’s impact, a tailored memory profiling workflow was constructed for a sophisticated compiler optimization client. In 570 lines of code, this redesigned workflow satisfies the client’s memory profiling needs while achieving more than 90\% reduction in profiling overhead and improved robustness compared to the original profilers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {110},
numpages = {25},
keywords = {compiler optimizations, memory profiling, profiler framework}
}

@software{10.5281/zenodo.10783906,
author = {Xu, Ziyang and Chon, Yebin and Su, Yian and Tan, Zujun and Apostolakis, Sotiris and Campanoni, Simone and August, David I.},
title = {Artifact for Paper "PROMPT: A Fast and Extensible Memory Profiling Framework"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10783906},
abstract = {
    <p>This repository contains the artifact evaluation for the PROMPT paper. PROMPT is a fast and extensible memory profiling framework.</p>

},
keywords = {compiler optimizations, memory profiling, profiler framework}
}

@article{10.1145/3649828,
author = {Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun},
title = {Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649828},
doi = {10.1145/3649828},
abstract = {While static analysis is instrumental in uncovering software bugs, its precision in analyzing large and intricate codebases remains challenging. The emerging prowess of Large Language Models (LLMs) offers a promising avenue to address these complexities. In this paper, we present LLift, a pioneering framework that synergizes static analysis and LLMs, with a spotlight on identifying use-before-initialization (UBI) bugs within the Linux kernel. Drawing from our insights into variable usage conventions in Linux, we enhance path analysis using post-constraint guidance. This approach, combined with our methodically crafted procedures, empowers LLift to adeptly handle the challenges of bug-specific modeling, extensive codebases, and the unpredictable nature of LLMs. Our real-world evaluations identified four previously undiscovered UBI bugs in the mainstream Linux kernel, which the Linux community has acknowledged. This study reaffirms the potential of marrying static analysis with LLMs, setting a compelling direction for future research in this area.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {111},
numpages = {26},
keywords = {Static analysis, bug detection, large language model}
}

@software{10.5281/zenodo.10780591,
author = {Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun},
title = {Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach (Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10780591},
abstract = {
    <p>This repo contains all the code and test cases for the paper “Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach”. LLift is an automated framework enhancing static analysis in bug detection with LLMs.</p>

},
keywords = {bug detection, large language model, Static analysis}
}

@article{10.1145/3649829,
author = {Chen, Qian and Yu, Chenyang and Liu, Ruyan and Zhang, Chi and Wang, Yu and Wang, Ke and Su, Ting and Wang, Linzhang},
title = {Evaluating the Effectiveness of Deep Learning Models for Foundational Program Analysis Tasks},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649829},
doi = {10.1145/3649829},
abstract = {While deep neural networks provide state-of-the-art solutions to a wide range of programming language tasks, their effectiveness in dealing with foundational program analysis tasks remains under explored. In this paper, we present an empirical study that evaluates four prominent models of code (i.e., CuBERT, CodeBERT, GGNN, and Graph Sandwiches) in two such foundational tasks: (1) alias prediction, in which models predict whether two pointers must alias, may alias or must not alias; and (2) equivalence prediction, in which models predict whether or not two programs are semantically equivalent. At the core of this study is CodeSem, a dataset built upon the source code of real-world flagship software (e.g., Linux Kernel, GCC, MySQL) and manually validated for the two prediction tasks.  
Results show that all models are accurate in both prediction tasks, especially CuBERT with an accuracy of 89\% and 84\% in alias prediction and equivalence prediction, respectively. We also conduct a comprehensive, in-depth analysis of the results of all models in both tasks, concluding that deep learning models are generally capable of performing foundational tasks in program analysis even though in specific cases their weaknesses are also evident.  

Our code and evaluation data are publicly available at https://github.com/CodeSemDataset/CodeSem.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {112},
numpages = {29},
keywords = {Alias Analysis, Deep Learning, Equivalence Checking}
}

@article{10.1145/3649830,
author = {Castello, Jonathan and Redmond, Patrick and Kuper, Lindsey},
title = {Inductive Diagrams for Causal Reasoning},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649830},
doi = {10.1145/3649830},
abstract = {The Lamport diagram is a pervasive and intuitive tool for informal reasoning about “happens-before” relationships in a concurrent system. However, traditional axiomatic formalizations of Lamport diagrams can be painful to work with in a mechanized setting like Agda. We propose an alternative, inductive formalization — the causal separation diagram (CSD) — that takes inspiration from string diagrams and concurrent separation logic, but enjoys a graphical syntax similar to Lamport diagrams. Critically, CSDs are based on the idea that causal relationships between events are witnessed by the paths that information follows between them. To that end, we model “happens-before” as a dependent type of paths between events. The inductive formulation of CSDs enables their interpretation into a variety of semantic domains. We demonstrate the interpretability of CSDs with a case study on properties of logical clocks, widely-used mechanisms for reifying causal relationships as data. We carry out this study by implementing a series of interpreters for CSDs, culminating in a generic proof of Lamport’s clock condition that is parametric in a choice of clock. We instantiate this proof on Lamport’s scalar clock, on Mattern’s vector clock, and on the matrix clocks of Raynal et al. and of Wuu and Bernstein, yielding verified implementations of each. The CSD formalism and our case study are mechanized in the Agda proof assistant.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {113},
numpages = {26},
keywords = {causality, concurrent systems, mechanized reasoning}
}

@article{10.1145/3649831,
author = {Li, Zikun and Peng, Jinjun and Mei, Yixuan and Lin, Sina and Wu, Yi and Padon, Oded and Jia, Zhihao},
title = {Quarl: A Learning-Based Quantum Circuit Optimizer},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649831},
doi = {10.1145/3649831},
abstract = {Optimizing quantum circuits is challenging due to the very large search space of functionally equivalent circuits and the necessity of applying transformations that temporarily decrease performance to achieve a final performance improvement. This paper presents Quarl, a learning-based quantum circuit optimizer. Applying reinforcement learning (RL) to quantum circuit optimization raises two main challenges: the large and varying action space and the non-uniform state representation. Quarl addresses these issues with a novel neural architecture and RL-training procedure. Our neural architecture decomposes the action space into two parts and leverages graph neural networks in its state representation, both of which are guided by the intuition that optimization decisions can be mostly guided by local reasoning while allowing global circuit-wide reasoning. Our evaluation shows that Quarl significantly outperforms existing circuit optimizers on almost all benchmark circuits. Surprisingly, Quarl can learn to perform rotation merging—a complex, non-local circuit optimization implemented as a separate pass in existing optimizers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {114},
numpages = {28},
keywords = {Compilers, Quantum Computation, Reinforcement Learning}
}

@software{10.5281/zenodo.10463907,
author = {Li, Zikun and Peng, Jinjun and Mei, Yixuan and Lin, Sina and Wu, Yi and Padon, Oded and Jia, Zhihao},
title = {Reproduction Package for "Quarl: A learning-based quantum circuit optimizer"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10463907},
abstract = {
    <p>This package contains the source code and instructions for the reproduction of the evaluation results in the paper “Quarl: A learning-based quantum circuit optimizer”.</p>

},
keywords = {Compilers, Quantum Computation, Reinforcement Learning}
}

@article{10.1145/3649832,
author = {Lee, Edward and Zhao, Yaoyu and Lhot\'{a}k, Ond\v{r}ej and You, James and Satheeskumar, Kavin and Brachth\"{a}user, Jonathan Immanuel},
title = {Qualifying System F&lt;:: Some Terms and Conditions May Apply},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649832},
doi = {10.1145/3649832},
abstract = {Type qualifiers offer a lightweight mechanism for enriching existing type systems to enforce additional, desirable, program invariants. 
 
They do so by offering a restricted but effective form of subtyping. 
 
While the theory of type qualifiers is well understood and present in many programming languages today, polymorphism over type qualifiers remains an area less well examined. 
 
We explore how such a polymorphic system could arise by constructing a calculus, System F-sub-Q, which combines the higher-rank bounded polymorphism of System F-sub with the theory of type qualifiers. We explore how the ideas used to construct System F-sub-Q can be reused in situations where type qualifiers naturally arise---in reference immutability, function colouring, and capture checking. Finally, we re-examine other qualifier systems in the literature in light of the observations presented while developing System F-sub-Q.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {115},
numpages = {30},
keywords = {System F-sub, Type Qualifiers, Type Systems}
}

@software{10.1145/3580431,
author = {Lee, Edward and Zhao, Yaoyu and Lhot\'{a}k, Ond\v{r}ej and You, James and Satheeskumar, Kavin and Brachth\"{a}user, Jonathan Immanuel},
title = {Artifact for the OOPSLA 2024 paper ’Qualifying System F-sub’},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3580431},
abstract = {
    <p>Mechanized proofs for the calculi described in the paper ’Qualifying System F-sub’.</p>

},
keywords = {Coq, Mechanized proofs, System F-sub-Q}
}

@article{10.1145/3649833,
author = {Nelson, Tim and Greenman, Ben and Prasad, Siddhartha and Dyer, Tristan and Bove, Ethan and Chen, Qianfan and Cutting, Charles and Del Vecchio, Thomas and LeVine, Sidney and Rudner, Julianne and Ryjikov, Ben and Varga, Alexander and Wagner, Andrew and West, Luke and Krishnamurthi, Shriram},
title = {Forge: A Tool and Language for Teaching Formal Methods},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649833},
doi = {10.1145/3649833},
abstract = {This paper presents the design of Forge, a tool for teaching formal methods gradually. Forge is based on the widely-used Alloy language and analysis tool, but contains numerous improvements based on more than a decade of experience teaching Alloy to students. Although our focus has been on the classroom, many of the ideas in Forge likely also apply to training in industry. Forge offers a progression of languages that improve the learning experience by only gradually increasing in expressive power. Forge supports custom visualization of its outputs, enabling the use of widely-understood domain-specific representations. Finally, Forge provides a variety of testing features to ease the transition from programming to formal modeling. We present the motivation for and design of these aspects of Forge, and then provide a substantial evaluation based on multiple years of classroom use.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {116},
numpages = {29},
keywords = {formal-methods education, language levels, lightweight formal-methods}
}

@software{10.5281/zenodo.10463960,
author = {Nelson, Tim and Greenman, Ben and Prasad, Siddhartha and Dyer, Tristan and Bove, Ethan and Chen, Qianfan and Cutting, Charles and Del Vecchio, Thomas and LeVine, Sidney and Rudner, Julianne and Ryjikov, Ben and Varga, Alexander and Wagner, Andrew and West, Luke and Krishnamurthi, Shriram},
title = {Artifact for Forge: A Tool and Language for Teaching Formal Methods},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10463960},
abstract = {
    <p>The purpose of this artifact is to show that Forge works as advertised. To that end, we provide instructions for installing Forge (similar to what our students see), links to the documentation, and code from the paper.</p>

},
keywords = {formal-methods education, language levels, lightweight formal-methods}
}

@article{10.1145/3649834,
author = {Chen, Zhe and Zhu, Yunlong and Wang, Zhemin},
title = {Design and Implementation of an Aspect-Oriented C Programming Language},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649834},
doi = {10.1145/3649834},
abstract = {Aspect-Oriented Programming (AOP) is a programming paradigm that implements crosscutting concerns in a modular way. People have witnessed the prosperity of AOP languages for Java and C++, such as AspectJ and AspectC++, which has propelled AOP to become an important programming paradigm with many interesting application scenarios, e.g., runtime verification. In contrast, the AOP languages for C are still poor and lack compiler support. In this paper, we design a new general-purpose and expressive aspect-oriented C programming language, namely Aclang, and implement a compiler for it, which brings fully-fledged AOP support into the C domain. We have evaluated the effectiveness and performance of our compiler against two state-of-the-art tools, ACC and AspectC++. In terms of effectiveness, Aclang outperforms ACC and AspectC++. In terms of performance, Aclang outperforms ACC in execution time and outperforms AspectC++ in both execution time and memory consumption.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {117},
numpages = {28},
keywords = {C language, aspect-oriented programming, compiler, instrumentation, semantics, transformation}
}

@software{10.5281/zenodo.10775922,
author = {Chen, Zhe and Zhu, Yunlong and Wang, Zhemin},
title = {Reproduction Package for Article `Design and Implementation of an Aspect-Oriented C Programming Language'},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10775922},
abstract = {
    <p>The artifact contains the Movec compiler for Aclang and all the benchmarks used in our experiments. The purpose of the artifact is to reproduce the experiments in Section 7 and support the main claims in the paper.</p>

},
keywords = {aspect-oriented programming, C language, compiler, instrumentation, semantics, transformation}
}

@article{10.1145/3649835,
author = {Cutler, Joseph W. and Disselkoen, Craig and Eline, Aaron and He, Shaobo and Headley, Kyle and Hicks, Michael and Hietala, Kesha and Ioannidis, Eleftherios and Kastner, John and Mamat, Anwar and McAdams, Darin and McCutchen, Matt and Rungta, Neha and Torlak, Emina and Wells, Andrew M.},
title = {Cedar: A New Language for Expressive, Fast, Safe, and Analyzable Authorization},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649835},
doi = {10.1145/3649835},
abstract = {Cedar is a new authorization policy language designed to be ergonomic, fast, safe, and analyzable. Rather than embed authorization logic in an application’s code, developers can write that logic as Cedar policies and delegate access decisions to Cedar’s evaluation engine. Cedar’s simple and intuitive syntax supports common authorization use-cases with readable policies, naturally leveraging concepts from role-based, attribute-based, and relation-based access control models. Cedar’s policy structure enables access requests to be decided quickly. Cedar’s policy validator leverages optional typing to help policy writers avoid mistakes, but not get in their way. Cedar’s design has been finely balanced to allow for a sound and complete logical encoding, which enables precise policy analysis, e.g., to ensure that when refactoring a set of policies, the authorized  
permissions do not change. We have modeled Cedar in the Lean programming language, and used Lean’s proof assistant to prove important properties of Cedar’s design. We have implemented Cedar in Rust, and released it open-source. Comparing Cedar to two open-source languages, OpenFGA and Rego, we find (subjectively) that Cedar has equally or more readable policies, but (objectively) performs far better.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {118},
numpages = {28},
keywords = {Authorization, Formal models, Policies as code}
}

@article{10.1145/3649836,
author = {Kravchuk-Kirilyuk, Anastasiya and Feng, Gary and Iskander, Jonas and Zhang, Yizhou and Amin, Nada},
title = {Persimmon: Nested Family Polymorphism with Extensible Variant Types},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649836},
doi = {10.1145/3649836},
abstract = {Many obstacles stand in the way of modular, extensible code. Some language constructs, such as pattern matching, are not easily extensible. Inherited code may not be type safe in the presence of extended types. The burden of setting up design patterns can discourage users, and parameter clutter can make the code less readable. Given these challenges, it is no wonder that extensibility often gives way to code duplication. We present our solution: Persimmon, a functional system with nested family polymorphism, extensible variant types, and extensible pattern matching. Most constructs in our language are built-in "extensibility hooks," cutting down on the parameter clutter and user burden associated with extensible code. Persimmon preserves the relationships between nested families upon inheritance, enabling extensibility at a large scale. Since nested family polymorphism can express composable extensions, Persimmon supports mixins via an encoding. We show how Persimmon can be compiled into a functional language without extensible variants with our translation to Scala. Finally, we show that our system is sound by proving the properties of progress and preservation.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {119},
numpages = {27},
keywords = {Persimmon, composable extensions, extensibility, family polymorphism, nested inheritance}
}

@software{10.5281/zenodo.10798266,
author = {Kravchuk-Kirilyuk, Anastasiya and Feng, Gary and Iskander, Jonas and Zhang, Yizhou and Amin, Nada},
title = {Persimmon: Nested Family Polymorphism with Extensible Variant Types (Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10798266},
abstract = {
    <p>Our implementation consists of the Persimmon type checker and our prototype compiler to Scala.</p>

},
keywords = {composable extensions, extensibility, family polymorphism, nested inheritance, Persimmon}
}

@article{10.1145/3649837,
author = {Mukherjee, Manasij and Regehr, John},
title = {Hydra: Generalizing Peephole Optimizations with Program Synthesis},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649837},
doi = {10.1145/3649837},
abstract = {Optimizing compilers rely on peephole optimizations to simplify  
combinations of instructions and remove redundant instructions.  
Typically, a new peephole optimization is added when a compiler  
developer notices an optimization opportunity---a collection of  
dependent instructions that can be improved---and manually derives a  
more general rewrite rule that optimizes not only the original code,  
but also other, similar collections of instructions.  
In this paper, we present Hydra, a tool that automates the process of  
generalizing peephole optimizations using a collection of techniques  
centered on program synthesis.  
One of the most important problems we have solved is finding a version  
of each optimization that is independent of the bitwidths of the  
optimization's inputs (when this version exists).  
We show that Hydra can generalize 75\% of the ungeneralized missed  
peephole optimizations that LLVM developers have posted to the LLVM  
project's issue tracker.  
All of Hydra's generalized peephole optimizations have been formally  
verified, and furthermore we can automatically turn them into C++ code  
that is suitable for inclusion in an LLVM pass.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {120},
numpages = {29},
keywords = {alive2, generalization, hydra, llvm, peephole optimization, program synthesis, souper, superoptimization}
}

@article{10.1145/3649838,
author = {Sato, Shigeyuki and Nakamaru, Tomoki},
title = {Multiverse Notebook: Shifting Data Scientists to Time Travelers},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649838},
doi = {10.1145/3649838},
abstract = {Computational notebook environments are popular and de facto standard  
tools for programming in data science, whereas computational notebooks are notorious in  
software engineering. The criticism there stems from the characteristic  
of facilitating unrestricted dynamic patching of running programs, which  
makes exploratory coding quick but the resultant code messy and  
inconsistent. In this work, we first reveal that dynamic patching is a  
natural demand rather than a mere bad practice in data science  
programming on Kaggle. We then develop Multiverse Notebook, a  
computational notebook engine for time-traveling exploration. It  
enables users to time-travel to any past state and restart with new code  
from there under state isolation. We present an approach to efficiently  
implementing time-traveling exploration. We empirically evaluate  
Multiverse Notebook on ten real-world tasks from Kaggle. Our experiments  
show that time-traveling exploration on Multiverse Notebook is  
reasonably efficient.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {121},
numpages = {30},
keywords = {Computational notebook, Exploratory programming, Memory management}
}

@article{10.1145/3649839,
author = {Avanzini, Martin and Barthe, Gilles and Gr\'{e}goire, Benjamin and Moser, Georg and Vanoni, Gabriele},
title = {Hopping Proofs of Expectation-Based Properties: Applications to Skiplists and Security Proofs},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649839},
doi = {10.1145/3649839},
abstract = {We propose, implement, and evaluate a hopping proof approach for  
 proving expectation-based properties of probabilistic programs. Our  
 approach combines EHL, a syntax-directed proof system for reducing  
 proof goals of a program to proof goals of simpler programs, with a  
 "hopping" proof rule for reducing proof goals of an  
 original program to proof goal of a different program which is  
 suitably related (by means of pRHL, a relational program logic for  
 probabilistic program) to the original program. We prove that EHL  
 is sound for a core language with procedure calls and adversarial  
 computations, and complete for the adversary-free fragment of the  
 language. We also provide an implementation of EHL into  
 EasyCrypt, a proof assistant tailored for reasoning about  
 relational properties of probabilistic programs. We provide a tight  
 integration of EHL with other program logics supported by  
 EasyCrypt, and in particular probabilistic Relational Hoare Logic  
 (pRHL). Using this tight integration, we give mechanized proofs of  
 expected complexity of in-place implementations of randomized  
 quickselect and skip lists. We also sketch applications of our  
 approach to cryptographic proofs and discuss the broader impact of  
 EHL in the EasyCrypt proof assistant.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {122},
numpages = {26},
keywords = {Hoare logic, formal verification, probabilistic programs}
}

@software{10.5281/zenodo.10517828,
author = {Avanzini, Martin and Barthe, Gilles and Gr\'{e}goire, Benjamin and Moser, Georg and Vanoni, Gabriele},
title = {ehoare},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10517828},
abstract = {
    <p>This artifact provides a docker image that contains a working installation of Easycrypt together with three proof scripts related to the examples given in the paper.</p>
<ul>
<li><code>qselect.ec</code> contains the formalisation of quickselect from Section 3. It relies on an auxiliary library <code>partition.eca</code> concerned with properties of the partitioning scheme.</li>
<li><code>skip_list.ec</code> contains the formalisation of skip-lists outlined in Section 6</li>
<li><code>adversary.ec</code> contains the prototypical cryptography proof example outlined in Section 7</li>
</ul>

},
keywords = {expectation logic, skip list}
}

@article{10.1145/3649840,
author = {Ryan, Gabriel and Cetin, Burcu and Lim, Yongwhan and Jana, Suman},
title = {Accurate Data Race Prediction in the Linux Kernel through Sparse Fourier Learning},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649840},
doi = {10.1145/3649840},
abstract = {Testing for data races in the Linux OS kernel is challenging because there is an exponentially large space of system calls and thread interleavings that can potentially lead to concurrent executions with races. In this work, we introduce a new approach for modeling execution trace feasibility and apply it to Linux OS Kernel race prediction. To address the fundamental scalability challenge posed by the exponentially large domain of possible execution traces, we decompose the task of predicting trace feasibility into independent prediction subtasks encoded as learning Boolean indicator functions for specific memory accesses, and apply a sparse fourier learning approach to learning each feasibility subtask.  

Boolean functions that are sparse in their fourier domain can be efficiently learned by estimating the coefficients of their fourier expansion. Since the feasibility of each memory access depends on only a few other relevant memory accesses or system calls (e.g., relevant inter-thread communications), we observe that trace feasibility functions often have this sparsity property and can be learned efficiently. We use learned trace feasibility functions in conjunction with conservative alias analysis to implement a kernel race-testing system, HBFourier, that uses sparse fourier learning to efficiently model feasibility when making predictions. We evaluate our approach on a recent Linux development kernel and show it finds 44 more races with 15.7\% more accurate race predictions than the next best performing system in our evaluation, in addition to identifying 5 new race bugs confirmed by kernel developers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {123},
numpages = {23},
keywords = {Data Race Prediction, Linux Kernel Testing, Sparse Fourier Learning}
}

@software{10.6084/m9.figshare.25365340.v1,
author = {Ryan, Gabriel and Cetin, Burcu and Lim, Yongwhan and Jana, Suman},
title = {HBFourier Artifact},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.6084/m9.figshare.25365340.v1},
abstract = {
    <p>Replication artifact for paper “Accurate Data Race Prediction in the Linux Kernel through Sparse Fourier Learning” to appear in OOPSLA 2024. The artifact contains HBFourier’s core implementation as a well as scripts for reproducing all results presented in the paper.</p>

},
keywords = {Data Race Prediction, Linux Kernel, Machine Learning, Spectral Method}
}

@article{10.1145/3649841,
author = {Naik, Aaditya and Stein, Adam and Wu, Yinjun and Naik, Mayur and Wong, Eric},
title = {TorchQL: A Programming Framework for Integrity Constraints in Machine Learning},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649841},
doi = {10.1145/3649841},
abstract = {Finding errors in machine learning applications requires a thorough exploration of their behavior over data. Existing approaches used by practitioners are often ad-hoc and lack the abstractions needed to scale this process. We present TorchQL, a programming framework to evaluate and improve the correctness of machine learning applications. TorchQL allows users to write queries to specify and check integrity constraints over machine learning models and datasets. It seamlessly integrates relational algebra with functional programming to allow for highly expressive queries using only eight intuitive operators. We evaluate TorchQL on diverse use-cases including finding critical temporal inconsistencies in objects detected across video frames in autonomous driving, finding data imputation errors in time-series medical records, finding data labeling errors in real-world images, and evaluating biases and constraining outputs of language models. Our experiments show that TorchQL enables up to 13x faster query executions than baselines like Pandas and MongoDB, and up to 40\% shorter queries than native Python. We also conduct a user study and find that TorchQL is natural enough for developers familiar with Python to specify complex integrity constraints.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {124},
numpages = {31},
keywords = {Integrity Constraints, Machine Learning, Query Languages}
}

@software{10.5281/zenodo.10723160,
author = {Naik, Aaditya and Stein, Adam and Wu, Yinjun and Naik, Mayur and Wong, Eric},
title = {Artifact for `TorchQL: A Programming Framework for Integrity Constraints in Machine Learning`},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10723160},
abstract = {
    <p>This artifact contains the queries in the paper and reproduces the results. Follow the README.md file for more information.</p>

},
keywords = {integrity constraints, machine learning, query language}
}

@article{10.1145/3649842,
author = {Gierczak, Olek and Menon, Lucy and Dimoulas, Christos and Ahmed, Amal},
title = {Gradually Typed Languages Should Be Vigilant!},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649842},
doi = {10.1145/3649842},
abstract = {In gradual typing, different languages perform different dynamic type checks for the same program even 
though the languages have the same static type system. This raises the question of whether, given a gradually 
typed language, the combination of the translation that injects checks in well-typed terms and the dynamic 
semantics that determines their behavior sufficiently enforce the static type system of the language. Neither 
type soundness, nor complete monitoring, nor any other meta-theoretic property of gradually typed languages 
to date provides a satisfying answer. 

In response, we present vigilance, a semantic analytical instrument that defines when the check-injecting 
translation and dynamic semantics of a gradually typed language are adequate for its static type system. 
Technically, vigilance asks if a given translation-and-semantics combination enforces the complete run-time 
typing history of a value, which consists of all of the types associated with the value. We show that the standard 
combination for so-called Natural gradual typing is vigilant for the standard simple type system, but the 
standard combination for Transient gradual typing is not. At the same time, the standard combination for 
Transient is vigilant for a tag type system but the standard combination for Natural is not. Hence, we clarify 
the comparative type-level reasoning power between the two most studied approaches to sound gradual typing. 
Furthermore, as an exercise that demonstrates how vigilance can guide design, we introduce and examine 
a new theoretical static gradual type system, dubbed truer, that is stronger than tag typing and more faithfully 
reflects the type-level reasoning power that the dynamic semantics of Transient gradual typing can guarantee.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {125},
numpages = {29},
keywords = {gradual typing, logical relations, natural, semantics, transient}
}

@article{10.1145/3649843,
author = {Michel, Jesse and Mu, Kevin and Yang, Xuanda and Bangaru, Sai Praveen and Collins, Elias Rojas and Bernstein, Gilbert and Ragan-Kelley, Jonathan and Carbin, Michael and Li, Tzu-Mao},
title = {Distributions for Compositionally Differentiating Parametric Discontinuities},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649843},
doi = {10.1145/3649843},
abstract = {Computations in physical simulation, computer graphics, and probabilistic inference often require the differentiation of discontinuous processes due to contact, occlusion, and changes at a point in time. Popular differentiable programming languages, such as PyTorch and JAX, ignore discontinuities during differentiation. This is incorrect for parametric discontinuities—conditionals containing at least one real-valued parameter and at least one variable of integration. We introduce Potto, the first differentiable first-order programming language to soundly differentiate parametric discontinuities. We present a denotational semantics for programs and program derivatives and show the two accord. We describe the implementation of Potto, which enables separate compilation of programs. Our prototype implementation overcomes previous compile-time bottlenecks achieving an 88.1x and 441.2x speed up in compile time and a 2.5x and 7.9x speed up in runtime, respectively, on two increasingly large image stylization benchmarks. We showcase Potto by implementing a prototype differentiable renderer with separately compiled shaders.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {126},
numpages = {30},
keywords = {Denotational Semantics, Differentiable Programming, Differentiable Rendering, Distribution Theory, Probabilistic Programming}
}

@article{10.1145/3649844,
author = {Klinkenberg, Lutz and Blumenthal, Christian and Chen, Mingshuai and Haase, Darion and Katoen, Joost-Pieter},
title = {Exact Bayesian Inference for Loopy Probabilistic Programs using Generating Functions},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649844},
doi = {10.1145/3649844},
abstract = {We present an exact Bayesian inference method for inferring posterior distributions encoded by probabilistic programs featuring possibly unbounded loops. Our method is built on a denotational semantics represented by probability generating functions, which resolves semantic intricacies induced by intertwining discrete probabilistic loops with conditioning (for encoding posterior observations). We implement our method in a tool called Prodigy; it augments existing computer algebra systems with the theory of generating functions for the (semi-)automatic inference and quantitative verification of conditioned probabilistic programs. Experimental results show that Prodigy can handle various infinite-state loopy programs and exhibits comparable performance to state-of-the-art exact inference tools over loop-free benchmarks.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {127},
numpages = {31},
keywords = {Bayesian inference, conditioning, denotational semantics, generating functions, non-termination, probabilistic programs, quantitative verification}
}

@software{10.5281/zenodo.10782412,
author = {Klinkenberg, Lutz and Blumenthal, Christian and Chen, Mingshuai and Haase, Darion and Katoen, Joost-Pieter},
title = {Exact Bayesian Inference for Loopy Probabilistic Programs using Generating Functions - Artifact},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10782412},
abstract = {
    <p>The artifact includes a Docker image with the Prodigy tool and program examples that were used for the benchmarks in the paper. It includes scripts and documentation to allow for easy replication of the presented benchmark results.</p>

},
keywords = {Bayesian inference, conditioning, denotational semantics, generating functions, non-termination, probabilistic programs, quantitative verification}
}

@article{10.1145/3649845,
author = {Zhang, Yifan and Shi, Yuanfeng and Zhang, Xin},
title = {Learning Abstraction Selection for Bayesian Program Analysis},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649845},
doi = {10.1145/3649845},
abstract = {We propose a learning-based approach to select abstractions for Bayesian program analysis. Bayesian program analysis converts a program analysis into a Bayesian model by attaching probabilities to analysis rules. It computes probabilities of analysis results and can update them by learning from user feedback, test runs, and other information. Its abstraction heavily affects how well it learns from such information. There exists a long line of works in selecting abstractions for conventional program analysis but they are not effective for Bayesian program analysis. This is because they do not optimize for generalization ability. We propose a data-driven framework to solve this problem by learning from labeled programs. Starting from an abstraction, it decides how to change the abstraction based on analysis derivations. To be general, it considers graph properties of analysis derivations; to be effective, it considers the derivations before and after changing the abstraction. We demonstrate the effectiveness of our approach using a datarace analysis and a thread-escape analysis.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {128},
numpages = {29},
keywords = {Bayesian network, Static analysis, abstract interpretation, alarm ranking, machine learning for program analysis}
}

@software{10.5281/zenodo.10897277,
author = {Zhang, Yifan and Shi, Yuanfeng and Zhang, Xin},
title = {Learning Abstraction Selection for Bayesian Program Analysis (Paper Artifact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10897277},
abstract = {
    <p>Our artifact includes all code, scripts, data, and statistics in our experiments. It supports the following things: 1. Reproduction of all results in our experiments automatically. 2. Transformation from the results to Tables 7-11 and Figures 8-10 in our paper automatically. 3. Reusability guide for applying BinGraph framework to other settings and extensions.</p>

},
keywords = {abstract interpretation, alarm ranking, Bayesian network, machine learning for program analysis, Static analysis}
}

@article{10.1145/3649846,
author = {Binder, David and Skupin, Ingo and S\"{u}berkr\"{u}b, Tim and Ostermann, Klaus},
title = {Deriving Dependently-Typed OOP from First Principles},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649846},
doi = {10.1145/3649846},
abstract = {The expression problem describes how most types can easily be extended with new ways to produce the type or new ways to consume the type, but not both. When abstract syntax trees are defined as an algebraic data type, for example, they can easily be extended with new consumers, such as print or eval, but adding a new constructor requires the modification of all existing pattern matches. The expression problem is one way to elucidate the difference between functional or data-oriented programs (easily extendable by new consumers) and object-oriented programs (easily extendable by new producers). This difference between programs which are extensible by new producers or new consumers also exists for dependently typed programming, but with one core difference: Dependently-typed programming almost exclusively follows the functional programming model and not the object-oriented model, which leaves an interesting space in the programming language landscape unexplored. In this paper, we explore the field of dependently-typed object-oriented programming by deriving it from first principles using the principle of duality. That is, we do not extend an existing object-oriented formalism with dependent types in an ad-hoc fashion, but instead start from a familiar data-oriented language and derive its dual fragment by the systematic use of defunctionalization and refunctionalization. Our central contribution is a dependently typed calculus which contains two dual language fragments. We provide type- and semantics-preserving transformations between these two language fragments: defunctionalization and refunctionalization. We have implemented this language and these transformations and use this implementation to explain the various ways in which constructions in dependently typed programming can be explained as special instances of the general phenomenon of duality.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {129},
numpages = {27},
keywords = {Codata Types, Defunctionalization, Dependent Types, Expression Problem}
}

@software{10.5281/zenodo.10779424,
author = {Binder, David and Skupin, Ingo and S\"{u}berkr\"{u}b, Tim and Ostermann, Klaus},
title = {Artifact for the article "Deriving Dependently-Typed OOP from First Principles"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10779424},
abstract = {
    <p>Contained in the artifact is a Rust implementation of a dependently-typed programming language. This implementation contains a typechecker as well as an LSP server, VScode plugin and the infrastructure necessary to produce a static website in which snippets from the programming language can be typechecked in the browser. The language server provides a code action which can transform any codata type into a data type using defunctionalization, and any data type into a codata type using refunctionalization.</p>

},
keywords = {algebraic data types, codata, defunctionalization, dependent types, refunctionalization}
}

@article{10.1145/3649847,
author = {Kabaha, Anan and Cohen, Dana Drachsler},
title = {Verification of Neural Networks’ Global Robustness},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649847},
doi = {10.1145/3649847},
abstract = {Neural networks are successful in various applications but are also susceptible to adversarial attacks. To show the safety of network classifiers, many verifiers have been introduced to reason about the local robustness of a given input to a given perturbation. While successful, local robustness cannot generalize to unseen inputs. Several works analyze global robustness properties, however, neither can provide a precise guarantee about the cases where a network classifier does not change its classification. In this work, we propose a new global robustness property for classifiers aiming at finding the minimal globally robust bound, which naturally extends the popular local robustness property for classifiers. We introduce VHAGaR, an anytime verifier for computing this bound. VHAGaR relies on three main ideas: encoding the problem as a mixed-integer programming and pruning the search space by identifying dependencies stemming from the perturbation or the network's computation and generalizing adversarial attacks to unknown inputs. We evaluate VHAGaR on several datasets and classifiers and show that, given a three hour timeout, the average gap between the lower and upper bound on the minimal globally robust bound computed by VHAGaR is 1.9, while the gap of an existing global robustness verifier is 154.7. Moreover, VHAGaR is 130.6x faster than this verifier. Our results further indicate that leveraging dependencies and adversarial attacks makes VHAGaR 78.6x faster.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {130},
numpages = {30},
keywords = {Constrained Optimization, Global Robustness, Neural Network Verification}
}

@article{10.1145/3649848,
author = {Marshall, Daniel and Orchard, Dominic},
title = {Functional Ownership through Fractional Uniqueness},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649848},
doi = {10.1145/3649848},
abstract = {Ownership and borrowing systems, designed to enforce safe memory management without the need for garbage collection, have been brought to the fore by the Rust programming language. Rust also aims to bring some guarantees offered by functional programming into the realm of performant systems code, but the type system is largely separate from the ownership model, with type and borrow checking happening in separate compilation phases. Recent models such as RustBelt and Oxide aim to formalise Rust in depth, but there is less focus on integrating the basic ideas into more traditional type systems. An approach designed to expose an essential core for ownership and borrowing would open the door for functional languages to borrow concepts found in Rust and other ownership frameworks, so that more programmers can enjoy their benefits.  

One strategy for managing memory in a functional setting is through uniqueness types, but these offer a coarse-grained view: either a value has exactly one reference, and can be mutated safely, or it cannot, since other references may exist. Recent work demonstrates that linear and uniqueness types can be combined in a single system to offer restrictions on program behaviour and guarantees about memory usage. We develop this connection further, showing that just as graded type systems like those of Granule and Idris generalise linearity, a Rust-like ownership model arises as a graded generalisation of uniqueness. We combine fractional permissions with grading to give the first account of ownership and borrowing that smoothly integrates into a standard type system alongside linearity and graded types, and extend Granule accordingly with these ideas.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {131},
numpages = {31},
keywords = {borrowing, fractional permissions, graded modal types, ownership}
}

@software{10.5281/zenodo.10797791,
author = {Marshall, Daniel and Orchard, Dominic},
title = {Functional Ownership through Fractional Uniqueness (Artefact)},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10797791},
abstract = {
    <p>Artifact for the paper of the same title that appears at OOPSLA 2024. Includes code examples in both Granule and Rust. See overview.pdf for more information.</p>

},
keywords = {borrowing, fractional permissions, graded modal types, ownership}
}

@article{10.1145/3649849,
author = {He, Yang and Zhao, Pinhan and Wang, Xinyu and Wang, Yuepeng},
title = {VeriEQL: Bounded Equivalence Verification for Complex SQL Queries with Integrity Constraints},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649849},
doi = {10.1145/3649849},
abstract = {The task of SQL query equivalence checking is important in various real-world applications (including query rewriting and automated grading) that involve complex queries with integrity constraints; yet, state-of-the-art techniques are very limited in their capability of reasoning about complex features (e.g., those that involve sorting, case statement, rich integrity constraints, etc.) in real-life queries. To the best of our knowledge, we propose the first SMT-based approach and its implementation, VeriEQL, capable of proving and disproving bounded equivalence of complex SQL queries. VeriEQL is based on a new logical encoding that models query semantics over symbolic tuples using the theory of integers with uninterpreted functions. It is simple yet highly practical -- our comprehensive evaluation on over 20,000 benchmarks shows that VeriEQL outperforms all state-of-the-art techniques by more than one order of magnitude in terms of the number of benchmarks that can be proved or disproved. VeriEQL can also generate counterexamples that facilitate many downstream tasks (such as finding serious bugs in systems like MySQL and Apache Calcite).},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {132},
numpages = {29},
keywords = {Equivalence Checking, Program Verification, Relational Databases}
}

@software{10.5281/zenodo.10795614,
author = {He, Yang and Zhao, Pinhan and Wang, Xinyu and Wang, Yuepeng},
title = {Artifact Evaluation VeriEQL: Bounded Equivalence Verification for Complex SQL Queries with Integrity Constraints},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10795614},
abstract = {
    <p>The artifact aims to show that our claims and conclusions in the paper (VeriEQL: Bounded Equivalence Verification for Complex SQL Queries with Integrity Constraints) are well-founded and that others can reproduce the experimental results. It employs a SQL encoder and an SMT solver to encode and check equivalence of two SQL queries on relational schemas.</p>

},
keywords = {Equivalence Checking, Program Verification, Relational Databases}
}

@article{10.1145/3649850,
author = {Zhang, Jialu and Cambronero, Jos\'{e} Pablo and Gulwani, Sumit and Le, Vu and Piskac, Ruzica and Soares, Gustavo and Verbruggen, Gust},
title = {PyDex: Repairing Bugs in Introductory Python Assignments using LLMs},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649850},
doi = {10.1145/3649850},
abstract = {Students often make mistakes in their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex (a version of GPT), to build an APR system -- PyDex -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate PyDex on 286 real student programs and compare to three baselines, including one that combines a state-of-the-art Python syntax repair engine, BIFI, and a state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that PyDex can fix more programs and produce smaller patches on average.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {133},
numpages = {25},
keywords = {AI for programming education, automated program repair, large language models}
}

@article{10.1145/3649851,
author = {Santos, Joanna C. S. and Mirakhorli, Mehdi and Shokri, Ali},
title = {Seneca: Taint-Based Call Graph Construction for Java Object Deserialization},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649851},
doi = {10.1145/3649851},
abstract = {Object serialization and deserialization are widely used for storing and preserving objects in files, memory, or database as well as for transporting them across machines, enabling remote interaction among processes and many more. This mechanism relies on reflection, a dynamic language that introduces serious challenges for static analyses. Current state-of-the-art call graph construction algorithms do not fully support object serialization/deserialization, i.e., they are unable to uncover the callback methods that are invoked when objects are serialized and deserialized. Since call graphs are a core data structure for multiple types of analysis (e.g., vulnerability detection), an appropriate analysis cannot be performed since the call graph does not capture hidden (vulnerable) paths that occur via callback methods. In this paper, we present Seneca, an approach for handling serialization with improved soundness in the context of call graph construction. Our approach relies on taint analysis and API modeling to construct sound call graphs. We evaluated our approach with respect to soundness, precision, performance, and usefulness in detecting untrusted object deserialization vulnerabilities. Our results show that Seneca can create sound call graphs with respect to serialization features. The resulting call graphs do not incur significant runtime overhead and were shown to be useful for performing identification of vulnerable paths caused by untrusted object deserialization.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {134},
numpages = {29},
keywords = {call graphs, object serialization, taint analysis, untrusted object deserialization}
}

@article{10.1145/3649852,
author = {Smith, Scott and Zhang, Robert},
title = {A Pure Demand Operational Semantics with Applications to Program Analysis},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649852},
doi = {10.1145/3649852},
abstract = {This paper develops a novel minimal-state operational semantics for higher-order functional languages that uses only the call stack and a source program point or a lexical level as the complete state information: there is no environment, no substitution, no continuation, etc. We prove this form of operational semantics equivalent to standard presentations. We then show how this approach can open the door to potential new applications: we define a program analysis as a direct finitization of this operational semantics. The program analysis that naturally emerges has a number of novel and interesting properties compared to standard program analyses for higher-order programs: for example, it can infer recurrences and does not need value widening. We both give a formal definition of the analysis and describe our current implementation.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {135},
numpages = {27},
keywords = {Demand-Driven Operational Semantics}
}

@software{10.5281/zenodo.10794350,
author = {Smith, Scott and Zhang, Robert},
title = {Software Artifact for A Pure Demand Operational Semantics with Applications to Program Analysis},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10794350},
abstract = {
    <p>This artifact facilitates building, testing, benchmarking, and evolving the interpreter and program analyses presented in the paper.</p>

},
keywords = {Higher-Order Functional Programming, Operational Semantics, Program Analysis}
}

@article{10.1145/3649853,
author = {Xu, Yichen and Boruch-Gruszecki, Aleksander and Odersky, Martin},
title = {Degrees of Separation: A Flexible Type System for Safe Concurrency},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649853},
doi = {10.1145/3649853},
abstract = {Data races have long been a notorious problem in concurrent programming. They are hard to detect, and lead to non-deterministic behaviours. There has been a lot of interest in type systems that statically guarantee data race freedom. Significant progress has been made in this area, and these type systems are increasingly usable and practical. However, their adoption in mainstream programming languages is still limited, which is largely attributed to their strict alias prevention principles that obstruct the usage of existing programming patterns. This is a deterrent to the migration of existing code bases. To tackle this problem, we propose Capture Separation Calculus (System CSC), a calculus that models fork-join parallelism and statically prevents data races while being compatible with established programming patterns. It follows a control-as-you-need philosophy: by default, aliases are allowed, but they are tracked in the type system. When data races are a concern, the tracked aliases are controlled to prevent data-race-prone patterns. We study the formal properties of System CSC. Type soundness is proven via the standard progress and preservation theorems. Additionally, we formally verify the data race freedom property of System CSC by proving that the reduction of a well-typed program is confluent.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {136},
numpages = {27},
keywords = {Scala, capture checking, data race freedom, safe concurrency, type systems}
}

@article{10.1145/3649854,
author = {Zheng, Mingwei and Shi, Qingkai and Liu, Xuwei and Xu, Xiangzhe and Yu, Le and Liu, Congyu and Wei, Guannan and Zhang, Xiangyu},
title = {ParDiff: Practical Static Differential Analysis of Network Protocol Parsers},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649854},
doi = {10.1145/3649854},
abstract = {Countless devices all over the world are connected by networks and communicated via network protocols. Just like common software, protocol implementations suffer from bugs, many of which only cause silent data corruption instead of crashes. Hence, existing automated bug-finding techniques focused on memory safety, such as fuzzing, can hardly detect them. In this work, we propose a static differential analysis called ParDiff to find protocol implementation bugs, especially silent ones hidden in message parsers. Our key observation is that a network protocol often has multiple implementations and any semantic discrepancy between them may indicate bugs. However, different implementations are often written in disparate styles, e.g., using different data structures or written with different control structures, making it challenging to directly compare two implementations of even the same protocol. To exploit this observation and effectively compare multiple protocol implementations, ParDiff (1) automatically extracts finite state machines from programs to represent protocol format specifications, and (2) then leverages bisimulation and SMT solvers to find fine-grained and  
semantic inconsistencies between them. We have extensively evaluated ParDiff using 14 network protocols. The results show that ParDiff outperforms both differential symbolic execution and differential fuzzing tools. To date, we have detected 41 bugs with 25 confirmed by developers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {137},
numpages = {27},
keywords = {Network protocol, differential analysis, protocol format specification, static program analysis}
}

@article{10.1145/3649855,
author = {Stjerna, Amanda and R\"{u}mmer, Philipp},
title = {A Constraint Solving Approach to Parikh Images of Regular Languages},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649855},
doi = {10.1145/3649855},
abstract = {A common problem in string constraint solvers is computing the Parikh image, a linear arithmetic formula that describes all possible combinations of character counts in strings of a given language. Automata-based string solvers frequently need to compute the Parikh image of products (or intersections) of finite-state automata, in particular when solving string constraints that also include the integer data-type due to operations like string length and indexing. In this context, the computation of Parikh images often turns out to be both prohibitively slow and memory-intensive. This paper contributes a new understanding of how the reasoning about Parikh images can be cast as a constraint solving problem, and questions about Parikh images be answered without explicitly computing the product automaton or the exact Parikh image. The paper shows how this formulation can be efficiently implemented as a calculus, PC*, embedded in an automated theorem prover supporting Presburger logic. The resulting standalone tool Catra is evaluate on constraints produced by the Ostrich+ string solver when solving standard string constraint benchmarks involving integer operations. The experiments show that PC* strictly outperforms the standard approach by Verma et al. to extract Parikh images from finite-state automata, as well as the over-approximating method recently described by Jank\r{u} and Turo\v{n}ov\'{a} by a wide margin, and for realistic timeouts (under 60 s) also the nuXmv model checker. When added as the Parikh image backend of Ostrich+ to the Ostrich string constraint solver’s portfolio, it boosts its results on the quantifier-free strings with linear integer algebra track of SMT-COMP 2023 (QF_SLIA) enough to solve the most Unsat instances in that track of all competitors.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {138},
numpages = {29},
keywords = {Parikh images, model checking, string solvers}
}

@software{10.5281/zenodo.10796555,
author = {Stjerna, Amanda and R\"{u}mmer, Philipp},
title = {Reproduction Package for `A Constraint Solving Approach to Parikh Images of Regular Languages'},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10796555},
abstract = {
    <p>This is a reproduction and software package for A Constraint Solving Approach to Parikh Images of Regular Languages, published in OOPSLA 2024. For more information, see Artifact Overview.md.</p>
<p>It contains all software developed for the paper, as well as Jupyter notebooks for analysing experiments and relevant scripts for building them.</p>

},
keywords = {automata, automated theorem proving, parikh automata, parikh images, regular languages, SMT, string solving}
}

@article{10.1145/3649856,
author = {Wang, Zhaoyu and Ma, Pingchuan and Wang, Huaijin and Wang, Shuai},
title = {PP-CSA: Practical Privacy-Preserving Software Call Stack Analysis},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649856},
doi = {10.1145/3649856},
abstract = {Software call stack is a sequence of function calls that are executed during the runtime of a software program. Software call stack analysis (CSA) is widely used in software engineering to analyze the runtime behavior of software, which can be used to optimize the software performance, identify bugs, and profile the software. Despite the benefits of CSA, it has recently come under scrutiny due to concerns about privacy. To date, software is often deployed at user-side devices like mobile phones and smart watches. The collected call stacks may thus contain privacy-sensitive information, such as healthy information or locations, depending on the software functionality. Leaking such information to third parties may cause serious privacy concerns such as discrimination and targeted advertisement. This paper presents PP-CSA, a practical and privacy-preserving CSA framework that can be deployed in real-world scenarios. Our framework leverages local differential privacy (LDP) as a principled privacy guarantee, to mutate the collected call stacks and protect the privacy of individual users. Furthermore, we propose several key design principles and optimizations in the technical pipeline of PP-CSA, including an encoder-decoder scheme to properly enforce LDP over software call stacks, and several client/server-side optimizations to largely improve the efficiency of PP-CSA. Our evaluation over real-world Java and Android programs shows that our privacy-preserving CSA pipeline can achieve high utility and privacy guarantees while maintaining high efficiency. We have released our implementation of PP-CSA as an open-source project at https://github.com/wangzhaoyu07/PP-CSA for results reproducibility. We will provide more detailed documents to support and the usage and extension of the community.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {139},
numpages = {30},
keywords = {call stack analysis, differential privacy}
}

@article{10.1145/3649857,
author = {Enea, Constantin and Koskinen, Eric},
title = {Scenario-Based Proofs for Concurrent Objects},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649857},
doi = {10.1145/3649857},
abstract = {Concurrent objects form the foundation of many applications that exploit multicore architectures and their importance has lead to informal correctness arguments, as well as formal proof systems. Correctness arguments (as found in the distributed computing literature) give intuitive descriptions of a few canonical executions or "scenarios" often each with only a few threads, yet it remains unknown as to whether these intuitive arguments have a formal grounding and extend to arbitrary interleavings over unboundedly many threads.  

We present a novel proof technique for concurrent objects, based around identifying a small set of scenarios (representative, canonical interleavings), formalized as the commutativity quotient of a concurrent object. We next give an expression language for defining abstractions of the quotient in the form of regular or context-free languages that enable simple proofs of linearizability. These quotient expressions organize unbounded interleavings into a form more amenable to reasoning and make explicit the relationship between implementation-level contention/interference and ADT-level transitions.  

We evaluate our work on numerous non-trivial concurrent objects from the literature (including the Michael-Scott queue, Elimination stack, SLS reservation queue, RDCSS and Herlihy-Wing queue). We show that quotients capture the diverse features/complexities of these algorithms, can be used even when linearization points are not straight-forward, correspond to original authors' correctness arguments, and provide some new scenario-based arguments. Finally, we show that discovery of some object's quotients reduces to two-thread reasoning and give an implementation that can derive candidate quotients expressions from source code.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {140},
numpages = {30},
keywords = {commutativity quotient, concurrent objects, linearizability, verification}
}

@software{10.5281/zenodo.10814650,
author = {Enea, Constantin and Koskinen, Eric},
title = {CION: Concurrent Trace Reductions},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10814650},
abstract = {
    <p>A proof-of-concept implementation to automatically generate candidate layer quotient automata directly form the source code of concurrent object implementations. See f OOPSLA 2024 paper, “Scenario-Based Proofs for Concurrent Objects”.</p>

},
keywords = {concurrent objects, layer quotients, linearizability, proofs of concurrent objects, Quotients}
}

@article{10.1145/3649858,
author = {Li, Yongjian and Zhan, Bohua and Pang, Jun},
title = {Mechanizing the CMP Abstraction for Parameterized Verification},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649858},
doi = {10.1145/3649858},
abstract = {Parameterized verification is a challenging problem that is known to be undecidable in the general case.  ‍is a widely-used method for parameterized verification, originally proposed by Chou, Mannava and Park in 2004. It involves abstracting the protocol to a small fixed number of nodes, and strengthening by auxiliary invariants to refine the abstraction. In most of the existing applications of CMP, the abstraction and strengthening procedures are carried out manually, which can be tedious and error-prone. Existing theoretical justification of the  ‍method is also done at a high level, without detailed descriptions of abstraction and strengthening rules. In this paper, we present a formally verified theory of  ‍in Isabelle/HOL, with detailed, syntax-directed procedure for abstraction and strengthening that is proven correct. The formalization also includes correctness of symmetry reduction and assume-guarantee reasoning. We also describe a tool AutoCMP for automatically carrying out abstraction and strengthening in , as well as generating Isabelle proof scripts showing their correctness. We applied the tool to a number of parameterized protocols, and discovered some inaccuracies in previous manual applications of  ‍to the FLASH cache coherence protocol.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {141},
numpages = {27},
keywords = {Isabelle/HOL, Parameterized verification, cache coherence protocols, invariants, model checking, theorem proving}
}

@article{10.1145/3649859,
author = {Kavanagh, Ryan and Pientka, Brigitte},
title = {Message-Observing Sessions},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649859},
doi = {10.1145/3649859},
abstract = {We present Most, a process language with message-observing session types. Message-observing session types extend binary session types with type-level computation to specify communication protocols that vary based on messages observed on other channels. Hence, Most allows us to express global invariants about processes, rather than just local invariants, in a bottom-up, compositional way. We give Most a semantic foundation using traces with binding, a semantic approach for compositionally reasoning about traces in the presence of name generation. We use this semantics to prove type soundness and compositionality for Most processes. We see this as a significant step towards capturing message-dependencies and providing more precise guarantees about processes.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {142},
numpages = {29},
keywords = {dependent types, program specification, session types, trace semantics}
}

@article{10.1145/3649860,
author = {Lu, Yifei and Hou, Weidong and Pan, Minxue and Li, Xuandong and Su, Zhendong},
title = {Understanding and Finding Java Decompiler Bugs},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649860},
doi = {10.1145/3649860},
abstract = {Java decompilers are programs that perform the reverse process of Java compilers, i.e., they translate Java bytecode to Java source code. They are essential for reverse engineering purposes and have become more sophisticated and reliable over the years. However, it remains challenging for modern Java decompilers to reliably perform correct decompilation on real-world programs. To shed light on the key challenges of Java decompilation, this paper provides the first systematic study on the characteristics and causes of bugs in mature, widely-used Java decompilers. We conduct the study by investigating 333 unique bugs from three popular Java decompilers. Our key findings and observations include: (1) Although most of the reported bugs were found when decompiling large, real-world code, 40.2\% of them have small test cases for bug reproduction; (2) Over 80\% of the bugs manifest as exceptions, syntactic errors, or semantic errors, and bugs with source code artifacts are very likely semantic errors; (3) 57.7\%, 39.0\%, and 41.1\% of the bugs respectively are attributed to three stages of decompilers—loading structure entities from bytecode, optimizing these entities, and generating source code from these entities; (4) Bugs in decompilers’ type inference are the most complex to fix; and (5) Region restoration for structures like loop, sugaring for special structures like switch, and type inference of variables of generic types or indistinguishable types are the three most significant challenges in Java decompilation, which to some extent explains our findings in (3) and (4). Based on these findings, we present JD-Tester, a differential testing framework for Java decompilers, and our experience of using it in testing the three popular Java decompilers. JD-Testerutilizes different Java program generators to construct executable Java tests and finds exceptions, syntactic, and semantic inconsistencies (i.e. bugs) between a generated test and its compiled-decompiled version (through compilation and execution). In total, we have found 62 bugs in the three decompilers, demonstrating both the effectiveness of JD-Tester, and the importance of testing and validating Java decompilers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {143},
numpages = {27},
keywords = {Decompiler, Differential Testing, Reverse Engineering}
}

@article{10.1145/3649861,
author = {Ye, Qianchuan and Delaware, Benjamin},
title = {Taypsi: Static Enforcement of Privacy Policies for Policy-Agnostic Oblivious Computation},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649861},
doi = {10.1145/3649861},
abstract = {Secure multiparty computation (MPC) techniques enable multiple parties to compute joint functions over their private data without sharing that data with other parties, typically by employing powerful cryptographic protocols to protect individual's data. One challenge when writing such functions is that most MPC languages force users to intermix programmatic and privacy concerns in a single application, making it difficult to change or audit a program's underlying privacy policy. Prior policy-agnostic MPC languages relied on dynamic enforcement to decouple privacy requirements from program logic. Unfortunately, the resulting overhead makes it difficult to scale MPC applications that manipulate structured data. This work proposes to eliminate this overhead by instead transforming programs into semantically equivalent versions that statically enforce user-provided privacy policies. We have implemented this approach in a new MPC language, called Taypsi; our experimental evaluation demonstrates that the resulting system features considerable performance improvements on a variety of MPC applications involving structured data and complex privacy policies.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {144},
numpages = {30},
keywords = {Algebraic Data Types, Dependent Types, Oblivious Computation}
}

@software{10.5281/zenodo.10701642,
author = {Ye, Qianchuan and Delaware, Benjamin},
title = {Taypsi: Static Enforcement of Privacy Policies for Policy-Agnostic Oblivious Computation: OOPSLA24 Artifact},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10701642},
abstract = {
    <p>This is the artifact for the OOPSLA24 paper “Taypsi: Static Enforcement of Privacy Policies for Policy-Agnostic Oblivious Computation”. It contains:</p>
<ul>
<li>README.md: artifact instructions in markdown format</li>
<li>README.pdf: artifact instructions in pdf format</li>
<li>taypsi-image-amd64.tar.xz: docker image for amd64 (x86_64) architecture</li>
<li>taypsi-image-arm64.tar.xz: docker image for arm64 architecture</li>
<li>Dockerfile: docker file used to generate the docker images</li>
<li>taypsi.tar.xz: source code of the Taypsi type checker, compiler, examples and benchmarks</li>
<li>taype-pldi.tar.xz: source code of the Taype type checker and compiler (PLDI23), extended with additional benchmarks for comparison with Taypsi</li>
<li>taype-sa.tar.xz: source code of a version of Taype with an additional optimization (smart array) for a fairer comparison with Taypsi</li>
<li>taype-drivers.tar.xz: source code of drivers that implement the cryptographic primitives and oblivious array, used by taypsi and taype-sa</li>
<li>taype-drivers-legacy.tar.xz: source code of the drivers used by taype-pldi</li>
<li>taypsi-theories.tar.xz: Coq formalization of the Taypsi core calculus</li>
<li>taype-vscode.tar.xz: source code of a VS Code extension that provides basic syntax highlighting for Taypsi programs</li>
</ul>
<p>To evaluate this artifact, you only need to download the docker image for your architecture. Other tarballs provide clean versions of the source code, but you do not need them for evaluation. See README.md / README.pdf for details about this artifact and evaluation instructions. The same README.md is also available in the docker images.</p>

},
keywords = {Algebraic Data types, Coq Proof Assistant, Dependent Types, Oblivious Computation, Secure Multiparty Computation}
}

@article{10.1145/3649862,
author = {Xu, Pei and Lei, Yuxiang and Sui, Yulei and Xue, Jingling},
title = {Iterative-Epoch Online Cycle Elimination for Context-Free Language Reachability},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649862},
doi = {10.1145/3649862},
abstract = {Context-free language reachability (CFL-reachability) is a fundamental framework for implementing various static analyses. CFL-reachability utilizes context-free grammar (CFG) to extend the expressiveness of ordinary graph reachability from an unlabeled graph to an edge-labeled graph. Solving CFL-reachability requires a (sub)cubic time complexity with respect to the graph size, which limits its scalability in practice. Thus, an approach that can effectively reduce the graph size while maintaining the reachability result is highly desirable. Most of the existing graph simplification techniques for CFL-reachability work during the preprocessing stage, i.e., before the dynamic CFL-reachability solving process. However, in real-world CFL-reachability analyses, there is a large number of reducible nodes and edges that can only be discovered during dynamic solving, leaving significant room for on-the-fly improvements.  

This paper aims to reduce the graph size of CFL-reachability dynamically via online cycle elimination. We propose a simple yet effective approach to detect collapsible cycles in the graph based on the input context-free grammar. Our key insight is that symbols with particular forms of production rules in the grammar are the essence of transitivity of reachability relations in the graph. Specifically, in the graph, a reachability relation to a node v_i can be "transited" to another node v_j if there is a transitive relation from v_i to v_j, and cycles formed by transitive relations are collapsible. In this paper, we present an approach to identify the transitive symbols in a context-free grammar and propose an iterative-epoch framework for online cycle elimination. From the perspective of non-parallelized CFL-reachability solving, our iterative-epoch framework is well compatible with both the standard (unordered) solver and the recent ordered solver, and can significantly improve their performance. Our experiment on context-sensitive value-flow analysis for C/C++ and field-sensitive alias analysis for Java demonstrates promising performance improvement by our iterative-epoch cycle elimination technique. By collapsing cycles online, our technique accelerates the standard solver by 17.17\texttimes{} and 13.94\texttimes{} for value-flow analysis and alias analysis, respectively, with memory reductions of 48.8\% and 45.0\%. Besides, our technique can also accelerate the ordered solver by 14.32\texttimes{} and 8.36\texttimes{} for value-flow analysis and alias analysis, respectively, with memory reductions of 55.2\% and 57.8\%.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {145},
numpages = {26},
keywords = {CFL-reachability, online graph simplification, performance}
}

@article{10.1145/3649863,
author = {Rose, Abhishek and Bansal, Sorav},
title = {Modeling Dynamic (De)Allocations of Local Memory for Translation Validation},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649863},
doi = {10.1145/3649863},
abstract = {End-to-End Translation Validation is the problem of verifying the executable code generated by a compiler against the corresponding input source code for a single compilation. This becomes particularly hard in the presence of dynamically-allocated local memory where addresses of local memory may be observed by the program. In the context of validating the translation of a C procedure to executable code, a validator needs to tackle constant-length local arrays, address-taken local variables, address-taken formal parameters, variable-length local arrays, procedure-call arguments (including variadic arguments), and the alloca() operator. We provide an execution model, a definition of refinement, and an algorithm to soundly convert a refinement check into first-order logic queries that an off-the-shelf SMT solver can handle efficiently. In our experiments, we perform blackbox translation validation of C procedures (with up to 100+ SLOC), involving these local memory allocation constructs, against their corresponding assembly implementations (with up to 200+ instructions) generated by an optimizing compiler with complex loop and vectorizing transformations.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {146},
numpages = {30},
keywords = {Certified compilation, Equivalence checking, Translation validation}
}

@software{10.5281/zenodo.10797459,
author = {Rose, Abhishek and Bansal, Sorav},
title = {Artifact for paper "Modeling Dynamic (De)Allocations of Local Memory for Translation Validation"},
year = {2024},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.10797459},
abstract = {
    <p>This artifact contains a prototype implementation of the Dynamo algorithm described in the paper “Modeling Dynamic (De)Allocations of Local Memory for Translation Validation”. The artifact is packaged as a Docker application and is tested to run on Ubuntu 20.04 operating system. Please see README.pdf (packaged inside the archive) for more details.</p>

},
keywords = {Certified compilation, Equivalence checking, Translation validation}
}

