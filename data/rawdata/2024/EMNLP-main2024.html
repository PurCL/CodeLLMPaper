<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing - ACL Anthology</title><meta name=generator content="Hugo 0.118.2"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-light bg-light bg-gradient-light shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-inline pl-2">ACL Anthology</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav flex-grow-1 pr-md-2"><li class=nav-item><a class=nav-link href=/posts/>News<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/contrib/>Submissions<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/acl-org/acl-anthology/><i class="fab fa-github pr-1"></i>Github</a></li></ul><form class="form-inline my-2 my-lg-0 flex-nowrap" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-primary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</h2><p class=lead><a href=/people/y/yaser-al-onaizan/>Yaser Al-Onaizan</a>,
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>,
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2024.emnlp-main</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2024</dd><dt>Address:</dt><dd>Miami, Florida, USA</dd><dt>Venue:</dt><dd><a href=/venues/emnlp/>EMNLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2024.emnlp-main>https://aclanthology.org/2024.emnlp-main</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row><a class="btn btn-secondary btn-sm" href=/volumes/2024.emnlp-main.bib>BibTeX</a>
<a class="btn btn-secondary btn-sm" href=/volumes/2024.emnlp-main.xml>MODS XML</a>
<a class="btn btn-secondary btn-sm" href=/volumes/2024.emnlp-main.endf>EndNote</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href=/volumes/2024.emnlp-main.bib title="Export 'Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing' to bib format"><i class="fas fa-file-export"></i><span class="pl-2 transform-lower-sm">Bib</span><span class="d-none d-sm-inline">TeX</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2024+Conference+on+Empirical+Methods+in+Natural+Language+Processing" title="Search for 'Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><button class="btn btn-sm btn-info d-block mb-3" id=toggle-all-abstracts data-toggle-state=hide disabled>
<span class=on-toggle-state-hide>Show all abstracts<i class="ml-2 fas fa-angle-double-down"></i></span><span class=on-toggle-state-show>Hide all abstracts<i class="ml-2 fas fa-angle-double-up"></i></span></button><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.0/>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</a></strong><br><a href=/people/y/yaser-al-onaizan/>Yaser Al-Onaizan</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1/><span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>G</span>en: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation</a></strong><br><a href=/people/j/juhwan-choi/>Juhwan Choi</a>
|
<a href=/people/y/yeonghwa-kim/>Yeonghwa Kim</a>
|
<a href=/people/s/seunguk-yu/>Seunguk Yu</a>
|
<a href=/people/j/jungmin-yun/>JungMin Yun</a>
|
<a href=/people/y/youngbin-kim/>YoungBin Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1><div class="card-body p-3 small">Although pre-trained language models have exhibited great flexibility and versatility with prompt-based few-shot learning, they suffer from the extensive parameter size and limited applicability for inference. Recent studies have suggested that PLMs be used as dataset generators and a tiny task-specific model be trained to achieve efficient inference. However, their applicability to various domains is limited because they tend to generate domain-specific datasets. In this work, we propose a novel approach to universal domain generalization that generates a dataset regardless of the target domain. This allows for generalization of the tiny task model to any domain that shares the label space, thus enhancing the real-world applicability of the dataset generation paradigm. Our experiments indicate that the proposed method accomplishes generalizability across various domains while using a parameter set that is orders of magnitude smaller than PLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.2/>Multi-News+: Cost-efficient Dataset Cleansing via <span class=acl-fixed-case>LLM</span>-based Data Annotation</a></strong><br><a href=/people/j/juhwan-choi/>Juhwan Choi</a>
|
<a href=/people/j/jungmin-yun/>JungMin Yun</a>
|
<a href=/people/k/kyohoon-jin/>Kyohoon Jin</a>
|
<a href=/people/y/youngbin-kim/>YoungBin Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--2><div class="card-body p-3 small">The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models. However, datasets often contain noisy data inadvertently included during the construction process. Numerous attempts have been made to correct this issue through human annotators. However, hiring and managing human annotators is expensive and time-consuming. As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation.In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy. Specifically, we leverage approaches such as chain-of-thought and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task. Through our proposed cleansing method, we introduce an enhanced Multi-News+. By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.3.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.3/><span class=acl-fixed-case>FIZZ</span>: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document</a></strong><br><a href=/people/j/joonho-yang/>Joonho Yang</a>
|
<a href=/people/s/seunghyun-yoon/>Seunghyun Yoon</a>
|
<a href=/people/b/byeongjeong-kim/>ByeongJeong Kim</a>
|
<a href=/people/h/hwanhee-lee/>Hwanhee Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--3><div class="card-body p-3 small">Through the advent of pre-trained language models, there have been notable advancements in abstractive summarization systems. Simultaneously, a considerable number of novel methods for evaluating factual consistency in abstractive summarization systems has been developed. But these evaluation approaches incorporate substantial limitations, especially on refinement and interpretability. In this work, we propose highly effective and interpretable factual inconsistency detection method FIZZ (Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document) for abstractive summarization systems that is based on fine-grained atomic facts decomposition. Moreover, we align atomic facts decomposed from the summary with the source document through adaptive granularity expansion. These atomic facts represent a more fine-grained unit of information, facilitating detailed understanding and interpretability of the summary’s factual inconsistency. Experimental results demonstrate that our proposed factual consistency checking system significantly outperforms existing systems. We release the code at https://github.com/plm3332/FIZZ.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.4.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.4.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.4/>Prompts have evil twins</a></strong><br><a href=/people/r/rimon-melamed/>Rimon Melamed</a>
|
<a href=/people/l/lucas-hurley-mccabe/>Lucas Hurley McCabe</a>
|
<a href=/people/t/tanay-wakhare/>Tanay Wakhare</a>
|
<a href=/people/y/yejin-kim/>Yejin Kim</a>
|
<a href=/people/h/h-howie-huang/>H. Howie Huang</a>
|
<a href=/people/e/enric-boix-adsera/>Enric Boix-Adserà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--4><div class="card-body p-3 small">We discover that many natural-language prompts can be replaced by corresponding prompts that are unintelligible to humans but that provably elicit similar behavior in language models. We call these prompts “evil twins” because they are obfuscated and uninterpretable (evil), but at the same time mimic the functionality of the original natural-language prompts (twins). Remarkably, evil twins transfer between models. We find these prompts by solving a maximum-likelihood problem which has applications of independent interest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.5.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.5.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.5/>Table Question Answering for Low-resourced <span class=acl-fixed-case>I</span>ndic Languages</a></strong><br><a href=/people/v/vaishali-pal/>Vaishali Pal</a>
|
<a href=/people/e/evangelos-kanoulas/>Evangelos Kanoulas</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/m/maarten-de-rijke/>Maarten de Rijke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--5><div class="card-body p-3 small">TableQA is the task of answering questions over tables of structured information, returning individual cells or tables as output. TableQA research has focused primarily on high-resource languages, leaving medium- and low-resource languages with little progress due to scarcity of annotated data and neural models. We address this gap by introducing a fully automatic large-scale tableQA data generation process for low-resource languages with limited budget. We incorporate our data generation method on two Indic languages, Bengali and Hindi, which have no tableQA datasets or models. TableQA models trained on our large-scale datasets outperform state-of-the-art LLMs. We further study the trained models on different aspects, including mathematical reasoning capabilities and zero-shot cross-lingual transfer. Our work is the first on low-resource tableQA focusing on scalable data generation and evaluation procedures. Our proposed data generation method can be applied to any low-resource language with a web presence. We release datasets, models, and code (https://github.com/kolk/Low-Resource-TableQA-Indic-languages).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.6/><span class=acl-fixed-case>I</span>mage<span class=acl-fixed-case>I</span>n<span class=acl-fixed-case>W</span>ords: Unlocking Hyper-Detailed Image Descriptions</a></strong><br><a href=/people/r/roopal-garg/>Roopal Garg</a>
|
<a href=/people/a/andrea-burns/>Andrea Burns</a>
|
<a href=/people/b/burcu-karagol-ayan/>Burcu Karagol Ayan</a>
|
<a href=/people/y/yonatan-bitton/>Yonatan Bitton</a>
|
<a href=/people/c/ceslee-montgomery/>Ceslee Montgomery</a>
|
<a href=/people/y/yasumasa-onoe/>Yasumasa Onoe</a>
|
<a href=/people/a/andrew-bunner/>Andrew Bunner</a>
|
<a href=/people/r/ranjay-krishna/>Ranjay Krishna</a>
|
<a href=/people/j/jason-michael-baldridge/>Jason Michael Baldridge</a>
|
<a href=/people/r/radu-soricut/>Radu Soricut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--6><div class="card-body p-3 small">Despite the longstanding adage ”an image is worth a thousand words,” generating accurate hyper-detailed image descriptions remains unsolved. Trained on short web-scraped image-text, vision-language models often generate incomplete descriptions with visual inconsistencies. We address this via a novel data-centric approach with ImageInWords (IIW), a carefully designed human-in-the-loop framework for curating hyper-detailed image descriptions. Human evaluations on IIW data show major gains compared to recent datasets (+66%) and GPT-4V (+48%) across comprehensiveness, specificity, hallucinations, and more. We also show that fine-tuning with IIW data improves these metrics by +31% against models trained with prior work, even with only 9k samples. Lastly, we evaluate IIW models with text-to-image generation and vision-language reasoning tasks. Our generated descriptions result in the highest fidelity images, and boost compositional reasoning by up to 6% on ARO, SVO-Probes, and Winoground datasets. We release the IIW-Eval benchmark with human judgement labels, object and image-level annotations from our framework, and existing image caption datasets enriched via IIW-model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.7/><span class=acl-fixed-case>LLM</span>-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay</a></strong><br><a href=/people/y/yihuai-lan/>Yihuai Lan</a>
|
<a href=/people/z/zhiqiang-hu/>Zhiqiang Hu</a>
|
<a href=/people/l/lei-wang/>Lei Wang</a>
|
<a href=/people/y/yang-wang/>Yang Wang</a>
|
<a href=/people/d/deheng-ye/>Deheng Ye</a>
|
<a href=/people/p/peilin-zhao/>Peilin Zhao</a>
|
<a href=/people/e/ee-peng-lim/>Ee-Peng Lim</a>
|
<a href=/people/h/hui-xiong/>Hui Xiong</a>
|
<a href=/people/h/hao-wang/>Hao Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--7><div class="card-body p-3 small">This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents’ social behaviors. Results affirm the framework’s effectiveness in creating adaptive agents and suggest LLM-based agents’ potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field’s research and applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.8/>When <span class=acl-fixed-case>LLM</span>s Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection</a></strong><br><a href=/people/x/xiangyu-zhang/>Xiangyu Zhang</a>
|
<a href=/people/h/hexin-liu/>Hexin Liu</a>
|
<a href=/people/k/kaishuai-xu/>Kaishuai Xu</a>
|
<a href=/people/q/qiquan-zhang/>Qiquan Zhang</a>
|
<a href=/people/d/daijiao-liu/>Daijiao Liu</a>
|
<a href=/people/b/beena-ahmed/>Beena Ahmed</a>
|
<a href=/people/j/julien-epps/>Julien Epps</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--8><div class="card-body p-3 small">Depression is a critical concern in global mental health, prompting extensive research into AI-based detection methods. Among various AI technologies, Large Language Models (LLMs) stand out for their versatility in healthcare applications. However, the application of LLMs in the identification and analysis of depressive states remains relatively unexplored, presenting an intriguing avenue for future research. In this paper, we present an innovative approach to employ an LLM in the realm of depression detection, integrating acoustic speech information into the LLM framework for this specific application. We investigate an efficient method for automatic depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. This approach is not only valuable for the detection of depression but also represents a new perspective in enhancing the ability of LLMs to comprehend and process speech signals. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration also provides insights into the unique speech patterns of individuals, revealing the potential mental states of individuals. By encoding acoustic landmarks information into LLMs, evaluations of the proposed approach on the DAIC-WOZ dataset reveal state-of-the-art results when compared with existing Audio-Text baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.9/>Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model</a></strong><br><a href=/people/x/xiangyu-zhang/>Xiangyu Zhang</a>
|
<a href=/people/d/daijiao-liu/>Daijiao Liu</a>
|
<a href=/people/h/hexin-liu/>Hexin Liu</a>
|
<a href=/people/q/qiquan-zhang/>Qiquan Zhang</a>
|
<a href=/people/h/hanyu-meng/>Hanyu Meng</a>
|
<a href=/people/l/leibny-paola-garcia-perera/>Leibny Paola Garcia Perera</a>
|
<a href=/people/e/engsiong-chng/>EngSiong Chng</a>
|
<a href=/people/l/lina-yao/>Lina Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--9><div class="card-body p-3 small">Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their prolonged training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training—a key factor in the costs associated with adding or customizing voices—often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves comparable or superior performance to the original model in speech synthesis tasks but also demonstrates its versatility. By investigating and utilizing different wavelet bases, our approach proves effective not just in speech synthesis, but also in speech enhancement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.10/>Hateful Word in Context Classification</a></strong><br><a href=/people/s/sanne-hoeken/>Sanne Hoeken</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/o/ozge-alacam/>Özge Alacam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--10><div class="card-body p-3 small">Hate speech detection is a prevalent research field, yet it remains underexplored at the level of word meaning. This is significant, as terms used to convey hate often involve non-standard or novel usages which might be overlooked by commonly leveraged LMs trained on general language use. In this paper, we introduce the Hateful Word in Context Classification (<b>HateWiC</b>) task and present a dataset of ~4000 WiC-instances, each labeled by three annotators. Our analyses and computational exploration focus on the interplay between the subjective nature (context-dependent connotations) and the descriptive nature (as described in dictionary definitions) of hateful word senses. HateWiC annotations confirm that hatefulness of a word in context does not always derive from the sense definition alone. We explore the prediction of both majority and individual annotator labels, and we experiment with modeling context- and sense-based inputs. Our findings indicate that including definitions proves effective overall, yet not in cases where hateful connotations vary. Conversely, including annotator demographics becomes more important for mitigating performance drop in subjective hate prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.11/>Eyes Don’t Lie: Subjective Hate Annotation and Detection with Gaze</a></strong><br><a href=/people/o/ozge-alacam/>Özge Alacam</a>
|
<a href=/people/s/sanne-hoeken/>Sanne Hoeken</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--11><div class="card-body p-3 small">Hate speech is a complex and subjective phenomenon. In this paper, we present a dataset (GAZE4HATE) that provides gaze data collected in a hate speech annotation experiment. We study whether the gaze of an annotator provides predictors of their subjective hatefulness rating, and how gaze features can improve Hate Speech Detection (HSD). We conduct experiments on statistical modeling of subjective hate ratings and gaze and analyze to what extent rationales derived from hate speech models correspond to human gaze and explanations in our data. Finally, we introduce MEANION, a first gaze-integrated HSD model. Our experiments show that particular gaze features like dwell time or fixation counts systematically correlate with annotators’ subjective hate ratings and improve predictions of text-only hate speech models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.12/><span class=acl-fixed-case>N</span>umero<span class=acl-fixed-case>L</span>ogic: Number Encoding for Enhanced <span class=acl-fixed-case>LLM</span>s’ Numerical Reasoning</a></strong><br><a href=/people/e/eli-schwartz/>Eli Schwartz</a>
|
<a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/j/joseph-shtok/>Joseph Shtok</a>
|
<a href=/people/s/sivan-doveh/>Sivan Doveh</a>
|
<a href=/people/l/leonid-karlinsky/>Leonid Karlinsky</a>
|
<a href=/people/a/assaf-arbelle/>Assaf Arbelle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--12><div class="card-body p-3 small">Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of “42”, we suggest using “2:42” as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.13/>“Thinking” Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models</a></strong><br><a href=/people/s/shaz-furniturewala/>Shaz Furniturewala</a>
|
<a href=/people/s/surgan-jandial/>Surgan Jandial</a>
|
<a href=/people/a/abhinav-java/>Abhinav Java</a>
|
<a href=/people/p/pragyan-banerjee/>Pragyan Banerjee</a>
|
<a href=/people/s/simra-shahid/>Simra Shahid</a>
|
<a href=/people/s/sumit-bhatia/>Sumit Bhatia</a>
|
<a href=/people/k/kokil-jaidka/>Kokil Jaidka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--13><div class="card-body p-3 small">Existing debiasing techniques are typically training-based or require access to the model’s internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine whether structured prompting techniques can offer opportunities for fair text generation. We evaluate a comprehensive end-user-focused iterative framework of debiasing that applies System 2 thinking processes for prompts to induce logical, reflective, and critical text generation, with single, multi-step, instruction, and role-based variants. By systematically evaluating many LLMs across many datasets and different prompting strategies, we show that the more complex System 2-based Implicative Prompts significantly improve over other techniques demonstrating lower mean bias in the outputs with competitive performance on the downstream tasks. Our work offers research directions for the design and the potential of end-user-focused evaluative frameworks for LLM use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.14/>A Usage-centric Take on Intent Understanding in <span class=acl-fixed-case>E</span>-Commerce</a></strong><br><a href=/people/w/wendi-zhou/>Wendi Zhou</a>
|
<a href=/people/t/tianyi-li/>Tianyi Li</a>
|
<a href=/people/p/pavlos-vougiouklis/>Pavlos Vougiouklis</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a>
|
<a href=/people/j/jeff-z-pan/>Jeff Z. Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--14><div class="card-body p-3 small">Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its essential role in product recommendation and business user profiling analysis, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predicative user intents as “how a customer uses a product”, and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph: category-rigidity and property-ambiguity. They limit its ability to strongly align user intents with products having the most desirable property, and to recommend useful products across diverse categories. Following these observations, we introduce a Product Recovery Benchmark featuring a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this benchmark. Our code and dataset are available at https://github.com/stayones/Usgae-Centric-Intent-Understanding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.15/>Fine-Tuning or Retrieval? Comparing Knowledge Injection in <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/o/oded-ovadia/>Oded Ovadia</a>
|
<a href=/people/m/menachem-brief/>Menachem Brief</a>
|
<a href=/people/m/moshik-mishaeli/>Moshik Mishaeli</a>
|
<a href=/people/o/oren-elisha/>Oren Elisha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--15><div class="card-body p-3 small">Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.16/>Systematic Biases in <span class=acl-fixed-case>LLM</span> Simulations of Debates</a></strong><br><a href=/people/a/amir-taubenfeld/>Amir Taubenfeld</a>
|
<a href=/people/y/yaniv-dover/>Yaniv Dover</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/a/ariel-goldstein/>Ariel Goldstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--16><div class="card-body p-3 small">The emergence of Large Language Models (LLMs), has opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. Current research suggests that LLM-based agents become increasingly human-like in their performance, sparking interest in using these AI agents as substitutes for human participants in behavioral studies. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. Hence, it is crucial to study and pinpoint the key behavioral distinctions between humans and LLM-based agents. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs’ ability to simulate political debates on topics that are important aspects of people’s day-to-day lives and decision-making processes. Our findings indicate a tendency for LLM agents to conform to the model’s inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.17/>Studying and Mitigating Biases in Sign Language Understanding Models</a></strong><br><a href=/people/k/katherine-atwell/>Katherine Atwell</a>
|
<a href=/people/d/danielle-bragg/>Danielle Bragg</a>
|
<a href=/people/m/malihe-alikhani/>Malihe Alikhani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--17><div class="card-body p-3 small">Ensuring that the benefits of sign language technologies are distributed equitably among all community members is crucial. Thus, it is important to address potential biases and inequities that may arise from the design or use of these resources. Crowd-sourced sign language datasets, such as the ASL Citizen dataset, are great resources for improving accessibility and preserving linguistic diversity, but they must be used thoughtfully to avoid reinforcing existing biases.In this work, we utilize the rich information about participant demographics and lexical features present in the ASL Citizen dataset to study and document the biases that may result from models trained on crowd-sourced sign datasets. Further, we apply several bias mitigation techniques during model training, and find that these techniques reduce performance disparities without decreasing accuracy. With the publication of this work, we release the demographic information about the participants in the ASL Citizen dataset to encourage future bias mitigation work in this space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.18.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.18/>Uncertainty in Language Models: Assessment through Rank-Calibration</a></strong><br><a href=/people/x/xinmeng-huang/>Xinmeng Huang</a>
|
<a href=/people/s/shuo-li/>Shuo Li</a>
|
<a href=/people/m/mengxin-yu/>Mengxin Yu</a>
|
<a href=/people/m/matteo-sesia/>Matteo Sesia</a>
|
<a href=/people/h/hamed-hassani/>Hamed Hassani</a>
|
<a href=/people/i/insup-lee/>Insup Lee</a>
|
<a href=/people/o/osbert-bastani/>Osbert Bastani</a>
|
<a href=/people/e/edgar-dobriban/>Edgar Dobriban</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--18><div class="card-body p-3 small">Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures (e.g., semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges (e.g., <span class=tex-math>[0,∞)</span> or <span class=tex-math>[0,1]</span>). In this work, we address this issue by developing a novel and practical framework, termed *Rank-Calibration*, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score (e.g., ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.19.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.19.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.19/><span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>TB</span>ench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning</a></strong><br><a href=/people/j/junjie-ye/>Junjie Ye</a>
|
<a href=/people/y/yilong-wu/>Yilong Wu</a>
|
<a href=/people/s/songyang-gao/>Songyang Gao</a>
|
<a href=/people/c/caishuang-huang/>Caishuang Huang</a>
|
<a href=/people/s/sixian-li/>Sixian Li</a>
|
<a href=/people/g/guanyu-li/>Guanyu Li</a>
|
<a href=/people/x/xiaoran-fan/>Xiaoran Fan</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--19><div class="card-body p-3 small">Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs’ capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce *RoTBench*, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model’s resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.20/>Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing</a></strong><br><a href=/people/f/fangkai-jiao/>Fangkai Jiao</a>
|
<a href=/people/c/chengwei-qin/>Chengwei Qin</a>
|
<a href=/people/z/zhengyuan-liu/>Zhengyuan Liu</a>
|
<a href=/people/n/nancy-chen/>Nancy F. Chen</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--20><div class="card-body p-3 small">Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.21/>Scaling Properties of Speech Language Models</a></strong><br><a href=/people/s/santiago-cuervo/>Santiago Cuervo</a>
|
<a href=/people/r/ricard-marxer/>Ricard Marxer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--21><div class="card-body p-3 small">Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.22.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.22/>“We Demand Justice!”: Towards Social Context Grounding of Political Texts</a></strong><br><a href=/people/r/rajkumar-pujari/>Rajkumar Pujari</a>
|
<a href=/people/c/chengfei-wu/>Chengfei Wu</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--22><div class="card-body p-3 small">Political discourse on social media often contains similar language with opposing intended meanings. For example, the phrase thoughts and prayers, is used to express sympathy for mass shooting victims, as well as satirically criticize the lack of legislative action on gun control. Understanding such discourse fully by reading only the text is difficult. However, knowledge of the social context information makes it easier. We characterize the social context required to fully understand such ambiguous discourse, by grounding the text in real-world entities, actions, and attitudes. We propose two datasets that require understanding social context and benchmark them using large pre-trained language models and several novel structured models. We show that structured models, explicitly modeling social context, outperform larger models on both tasks, but still lag significantly behind human performance. Finally, we perform an extensive analysis, to obtain further insights into the language understanding challenges posed by our social grounding tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.23/>An Experimental Analysis on Evaluating Patent Citations</a></strong><br><a href=/people/r/rabindra-nath-nandi/>Rabindra Nath Nandi</a>
|
<a href=/people/s/suman-maity/>Suman Maity</a>
|
<a href=/people/b/brian-uzzi/>Brian Uzzi</a>
|
<a href=/people/s/sourav-medya/>Sourav Medya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--23><div class="card-body p-3 small">The patent citation count is a good indicator of patent quality. This often generates monetary value for the inventors and organizations. However, the factors that influence a patent receiving high citations over the year are still not well understood. With the patents over the past two decades, we study the problem of patent citation prediction and formulate this as a binary classification problem. We create a semantic graph of patents based on their semantic similarities, enabling the use of Graph Neural Network (GNN)-based approaches for predicting citations. Our experimental results demonstrate the effectiveness of our GNN-based methods when applied to the semantic graph, showing that they can accurately predict patent citations using only patent text. More specifically, these methods produce up to 94% recall for patents with high citations and outperform existing baselines. Furthermore, we leverage this constructed graph to gain insights and explanations for the predictions made by the GNNs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.24/>Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?</a></strong><br><a href=/people/d/dawei-zhu/>Dawei Zhu</a>
|
<a href=/people/p/pinzhen-chen/>Pinzhen Chen</a>
|
<a href=/people/m/miaoran-zhang/>Miaoran Zhang</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--24><div class="card-body p-3 small">Traditionally, success in multilingual machine translation can be attributed to three key factors in training data: large volume, diverse translation directions, and high quality. In the current practice of fine-tuning large language models (LLMs) for translation, we revisit the importance of these factors. We find that LLMs display strong translation capability after being fine-tuned on as few as 32 parallel sentences and that fine-tuning on a single translation direction enables translation in multiple directions. However, the choice of direction is critical: fine-tuning LLMs with only English on the target side can lead to task misinterpretation, which hinders translation into non-English languages. Problems also arise when noisy synthetic data is placed on the target side, especially when the target language is well-represented in LLM pre-training. Yet interestingly, synthesized data in an under-represented language has a less pronounced effect. Our findings suggest that when adapting LLMs to translation, the requirement on data quantity can be eased but careful considerations are still crucial to prevent an LLM from exploiting unintended data biases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.25.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.25/>Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing</a></strong><br><a href=/people/l/le-yan/>Le Yan</a>
|
<a href=/people/z/zhen-qin/>Zhen Qin</a>
|
<a href=/people/h/honglei-zhuang/>Honglei Zhuang</a>
|
<a href=/people/r/rolf-jagerman/>Rolf Jagerman</a>
|
<a href=/people/x/xuanhui-wang/>Xuanhui Wang</a>
|
<a href=/people/m/michael-bendersky/>Michael Bendersky</a>
|
<a href=/people/h/harrie-oosterhuis/>Harrie Oosterhuis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--25><div class="card-body p-3 small">The powerful generative abilities of large language models (LLMs) show potential in generating relevance labels for search applications. Previous work has found that directly asking about relevancy, such as "*How relevant is document A to query Q?*”, results in suboptimal ranking. Instead, the pairwise-ranking prompting (PRP) approach produces promising ranking performance through asking about pairwise comparisons, e.g., "*Is document A more relevant than document B to query Q?*”. Thus, while LLMs are effective at their ranking ability, this is not reflected in their relevance label generation.In this work, we propose a post-processing method to consolidate the relevance labels generated by an LLM with its powerful ranking abilities. Our method takes both LLM generated relevance labels and pairwise preferences. The labels are then altered to satisfy the pairwise preferences of the LLM, while staying as close to the original values as possible. Our experimental results indicate that our approach effectively balances label accuracy and ranking performance. Thereby, our work shows it is possible to combine both the ranking and labeling abilities of LLMs through post-processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.26.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.26/>Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation</a></strong><br><a href=/people/t/tong-zhang/>Tong Zhang</a>
|
<a href=/people/c/chen-huang/>Chen Huang</a>
|
<a href=/people/y/yang-deng/>Yang Deng</a>
|
<a href=/people/h/hongru-liang/>Hongru Liang</a>
|
<a href=/people/j/jia-liu/>Jia Liu</a>
|
<a href=/people/z/zujie-wen/>Zujie Wen</a>
|
<a href=/people/w/wenqiang-lei/>Wenqiang Lei</a>
|
<a href=/people/t/tat-seng-chua/>Tat-Seng Chua</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--26><div class="card-body p-3 small">We investigate non-collaborative dialogue agents, which are expected to engage in strategic conversations with diverse users, for securing a mutual agreement that leans favorably towards the system’s objectives. This poses two main challenges for existing dialogue agents: 1) The inability to integrate user-specific characteristics into the strategic planning, and 2) The difficulty of training strategic planners that can be generalized to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.27.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.27/>Impeding <span class=acl-fixed-case>LLM</span>-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation</a></strong><br><a href=/people/s/saiful-islam-salim/>Saiful Islam Salim</a>
|
<a href=/people/r/rubin-yuchan-yang/>Rubin Yuchan Yang</a>
|
<a href=/people/a/alexander-cooper/>Alexander Cooper</a>
|
<a href=/people/s/suryashree-ray/>Suryashree Ray</a>
|
<a href=/people/s/saumya-debray/>Saumya Debray</a>
|
<a href=/people/s/sazzadur-rahaman/>Sazzadur Rahaman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--27><div class="card-body p-3 small">While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at measuring the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77%, ii) the drop in correctness caused by these perturbations was affected based on their detectability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.28.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.28/>Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation</a></strong><br><a href=/people/y/yuan-ge/>Yuan Ge</a>
|
<a href=/people/y/yilun-liu/>Yilun Liu</a>
|
<a href=/people/c/chi-hu/>Chi Hu</a>
|
<a href=/people/w/weibin-meng/>Weibin Meng</a>
|
<a href=/people/s/shimin-tao/>Shimin Tao</a>
|
<a href=/people/x/xiaofeng-zhao/>Xiaofeng Zhao</a>
|
<a href=/people/m/mahong-xia/>Mahong Xia</a>
|
<a href=/people/z/zhang-li/>Zhang Li</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/b/bei-li/>Bei Li</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>JingBo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--28><div class="card-body p-3 small">With contributions from the open-source community, a vast amount of instruction tuning (IT) data has emerged. Given the significant resource allocation required by training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data. However, existing methods for instruction data selection have limitations such as relying on fragile external APIs, being affected by biases in GPT models, or reducing the diversity of the selected instruction dataset. In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved instruction data selection method: Clustering and Ranking (CaR). CaR consists of two steps. The first step involves ranking instruction pairs using a scoring model that is well aligned with expert preferences (achieving an accuracy of 84.25%). The second step involves preserving dataset diversity through a clustering process. In our experiment, CaR selected a subset containing only 1.96% of Alpaca’s IT data, yet the underlying AlpaCaR model trained on this subset outperforms Alpaca by an average of 32.1% in GPT-4 evaluations. Furthermore, our method utilizes small models (550M parameters) and requires only 11.2% of the monetary cost compared to existing methods, making it easily deployable in industrial scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.29.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.29/>On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models</a></strong><br><a href=/people/a/abhilasha-sancheti/>Abhilasha Sancheti</a>
|
<a href=/people/h/haozhe-an/>Haozhe An</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--29><div class="card-body p-3 small">We study the presence of heteronormative biases and prejudice against interracial romantic relationships in large language models by performing controlled name-replacement experiments for the task of relationship prediction. We show that models are less likely to predict romantic relationships for (a) same-gender character pairs than different-gender pairs; and (b) intra/inter-racial character pairs involving Asian names as compared to Black, Hispanic, or White names. We examine the contextualized embeddings of first names and find that gender for Asian names is less discernible than non-Asian names. We discuss the social implications of our findings, underlining the need to prioritize the development of inclusive and equitable technology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.30.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.30.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.30/><span class=acl-fixed-case>E</span>mph<span class=acl-fixed-case>A</span>ssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models</a></strong><br><a href=/people/m/maureen-de-seyssel/>Maureen de Seyssel</a>
|
<a href=/people/a/antony-davirro/>Antony D’Avirro</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--30><div class="card-body p-3 small">We introduce EmphAssess, a prosodic benchmark designed to evaluate the capability of speech-to-speech models to encode and reproduce prosodic emphasis. We apply this to two tasks: speech resynthesis and speech-to-speech translation. In both cases, the benchmark evaluates the ability of the model to encode emphasis in the speech input and accurately reproduce it in the output, potentially across a change of speaker and language. As part of the evaluation pipeline, we introduce EmphaClass, a new model that classifies emphasis at the frame or word level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.31.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.31/>On Fake News Detection with <span class=acl-fixed-case>LLM</span> Enhanced Semantics Mining</a></strong><br><a href=/people/x/xiaoxiao-ma/>Xiaoxiao Ma</a>
|
<a href=/people/y/yuchen-zhang/>Yuchen Zhang</a>
|
<a href=/people/k/kaize-ding/>Kaize Ding</a>
|
<a href=/people/j/jian-yang/>Jian Yang</a>
|
<a href=/people/j/jia-wu/>Jia Wu</a>
|
<a href=/people/h/hao-fan/>Hao Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--31><div class="card-body p-3 small">Large language models (LLMs) have emerged as valuable tools for enhancing textual features in various text-related tasks. Despite their superiority in capturing the lexical semantics between tokens for text analysis, our preliminary study on two popular LLMs, i.e., ChatGPT and Llama2, showcases that simply applying the news embeddings from LLMs is ineffective for fake news detection. Such embeddings only encapsulate the language styles between tokens. Meanwhile, the high-level semantics among named entities and topics, which reveal the deviating patterns of fake news, have been ignored. Therefore, we propose a topic model together with a set of specially designed prompts to extract topics and real entities from LLMs and model the relations among news, entities, and topics as a heterogeneous graph to facilitate investigating news semantics. We then propose a Generalized Page-Rank model and a consistent learning criteria for mining the local and global semantics centered on each news piece through the adaptive propagation of features across the graph. Our model shows superior performance on five benchmark datasets over seven baseline methods and the efficacy of the key ingredients has been thoroughly validated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.32.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.32/>On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices</a></strong><br><a href=/people/b/branislav-pecher/>Branislav Pecher</a>
|
<a href=/people/i/ivan-srba/>Ivan Srba</a>
|
<a href=/people/m/maria-bielikova/>Maria Bielikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--32><div class="card-body p-3 small">While learning with limited labelled data can effectively deal with a lack of labels, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (i.e., non-deterministic decisions such as choice or order of samples). We propose and formalise a method to systematically investigate the effects of individual randomness factors while taking the interactions (dependence) between them into consideration. To this end, our method mitigates the effects of other factors while observing how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works led to inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consistent sensitivity of in-context learning to sample order even with random sample selection; and 2) besides mutual interactions, the effects of randomness factors, especially sample order, are also dependent on more systematic choices unexplored in existing works, such as number of classes, samples per class or choice of prompt format.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.33.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.33/>Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection</a></strong><br><a href=/people/z/zekun-li/>Zekun Li</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/p/pengcheng-he/>Pengcheng He</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--33><div class="card-body p-3 small">Large Language Models (LLMs) have demonstrated exceptional proficiency in instruction-following, making them increasingly integral to various applications. However, this capability introduces the risk of prompt injection attacks, where malicious instructions are embedded in the input to trigger unintended actions or content. Understanding the robustness of LLMs against such attacks is critical for ensuring their safe deployment. In this work, we establish a benchmark to evaluate the robustness of instruction-following LLMs against prompt injection attacks, assessing their ability to discern which instructions to follow and which to disregard. Through extensive experiments with leading instruction-following LLMs, we reveal significant vulnerabilities, particularly in models that mis-follow injected instructions. Our results show that certain models are excessively inclined to prioritize embedded instructions in prompts, often focusing on the latter parts of the prompt without fully understanding the overall context. Conversely, models that exhibit stronger contextual understanding and instruction-following capabilities tend to be more easily compromised by injected instructions. These findings highlight the need to balance improving LLMs’ instruction-following abilities with enhancing their overall comprehension of prompts, to prevent mis-following inappropriate instructions. We hope our analysis provides valuable insights into these vulnerabilities, contributing to the development of more robust solutions in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.34.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.34/>A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers</a></strong><br><a href=/people/v/valentin-barriere/>Valentin Barriere</a>
|
<a href=/people/s/sebastian-cifuentes/>Sebastian Cifuentes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--34><div class="card-body p-3 small">In this paper, we apply a method to quantify biases associated with named entities from various countries. We create counterfactual examples with small perturbations on target-domain data instead of relying on templates or specific datasets for bias detection. On widely used classifiers for subjectivity analysis, including sentiment, emotion, hate speech, and offensive text using Twitter data, our results demonstrate positive biases related to the language spoken in a country across all classifiers studied. Notably, the presence of certain country names in a sentence can strongly influence predictions, up to a 23% change in hate speech detection and up to a 60% change in the prediction of negative emotions such as anger. We hypothesize that these biases stem from the training data of pre-trained language models (PLMs) and find correlations between affect predictions and PLMs likelihood in English and unknown languages like Basque and Maori, revealing distinct patterns with exacerbate correlations. Further, we followed these correlations in-between counterfactual examples from a same sentence to remove the syntactical component, uncovering interesting results suggesting the impact of the pre-training data was more important for English-speaking-country names.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.35.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.35/>Mitigating the Alignment Tax of <span class=acl-fixed-case>RLHF</span></a></strong><br><a href=/people/y/yong-lin/>Yong Lin</a>
|
<a href=/people/h/hangyu-lin/>Hangyu Lin</a>
|
<a href=/people/w/wei-xiong/>Wei Xiong</a>
|
<a href=/people/s/shizhe-diao/>Shizhe Diao</a>
|
<a href=/people/j/jianmeng-liu/>Jianmeng Liu</a>
|
<a href=/people/j/jipeng-zhang/>Jipeng Zhang</a>
|
<a href=/people/r/rui-pan/>Rui Pan</a>
|
<a href=/people/h/haoxiang-wang/>Haoxiang Wang</a>
|
<a href=/people/w/wenbin-hu/>Wenbin Hu</a>
|
<a href=/people/h/hanning-zhang/>Hanning Zhang</a>
|
<a href=/people/h/hanze-dong/>Hanze Dong</a>
|
<a href=/people/r/renjie-pi/>Renjie Pi</a>
|
<a href=/people/h/han-zhao/>Han Zhao</a>
|
<a href=/people/n/nan-jiang/>Nan Jiang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/y/yuan-yao/>Yuan Yao</a>
|
<a href=/people/t/tong-zhang/>Tong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--35><div class="card-body p-3 small">LLMs acquire a wide range of abilities during pre-training, but aligning LLMs under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting pretrained abilities, which is also known as the alignment tax. To investigate alignment tax, we conducted experiments with existing RLHF algorithms using OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. Whereas, despite various techniques to mitigate forgetting, they are often at odds with the RLHF performance, leading to a trade-off between alignment performance and forgetting mitigation, leading to an alignment-forgetting trade-off. In this paper we show that model averaging, which simply interpolates between pre and post RLHF model weights, surprisingly achieves the most strongest alignment-forgetting Pareto front among a wide range of competing methods. To understand its effectiveness, we offer theoretical insights into model averaging, revealing that it enhances performance Pareto front by increasing feature diversity on the layers where tasks share overlapped feature spaces. Empirical evidence corroborates our analysis by showing the benefits of averaging low-level transformer layers. Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different alignment-forgetting trade-offs, we propose Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination ratios of model layers. HMA seeks to maximize the alignment performance while incurring minimal alignment tax. Moreover, we validate HMA’s performance across a range of RLHF algorithms over OpenLLaMA-3B and further extend our findings to Mistral-7B which is evaluated by open-sourced preference model and GPT4. Code available here.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.36.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.36/>Evaluating Readability and Faithfulness of Concept-based Explanations</a></strong><br><a href=/people/m/meng-li/>Meng Li</a>
|
<a href=/people/h/haoran-jin/>Haoran Jin</a>
|
<a href=/people/r/ruixuan-huang/>Ruixuan Huang</a>
|
<a href=/people/z/zhihao-xu/>Zhihao Xu</a>
|
<a href=/people/d/defu-lian/>Defu Lian</a>
|
<a href=/people/z/zijia-lin/>Zijia Lin</a>
|
<a href=/people/d/di-zhang/>Di Zhang</a>
|
<a href=/people/x/xiting-wang/>Xiting Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--36><div class="card-body p-3 small">With the growing popularity of general-purpose Large Language Models (LLMs), comes a need for more global explanations of model behaviors. Concept-based explanations arise as a promising avenue for explaining high-level patterns learned by LLMs. Yet their evaluation poses unique challenges, especially due to their non-local nature and high dimensional representation in a model’s hidden space. Current methods approach concepts from different perspectives, lacking a unified formalization. This makes evaluating the core measures of concepts, namely faithfulness or readability, challenging. To bridge the gap, we introduce a formal definition of concepts generalizing to diverse concept-based explanations’ settings. Based on this, we quantify the faithfulness of a concept explanation via perturbation. We ensure adequate perturbation in the high-dimensional space for different concepts via an optimization problem. Readability is approximated via an automatic and deterministic measure, quantifying the coherence of patterns that maximally activate a concept while aligning with human understanding. Finally, based on measurement theory, we apply a meta-evaluation method for evaluating these measures, generalizable to other types of explanations or tasks as well. Extensive experimental analysis has been conducted to inform the selection of explanation evaluation measures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.37.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.37/>Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems</a></strong><br><a href=/people/z/zhengyuan-liu/>Zhengyuan Liu</a>
|
<a href=/people/s/stella-xin-yin/>Stella Xin Yin</a>
|
<a href=/people/g/geyu-lin/>Geyu Lin</a>
|
<a href=/people/n/nancy-chen/>Nancy F. Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--37><div class="card-body p-3 small">Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student’s persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher’s adaptive scaffolding strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.38.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.38/><span class=acl-fixed-case>MSI</span>-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making</a></strong><br><a href=/people/d/dayuan-fu/>Dayuan Fu</a>
|
<a href=/people/b/biqing-qi/>Biqing Qi</a>
|
<a href=/people/y/yihuai-gao/>Yihuai Gao</a>
|
<a href=/people/c/che-jiang/>Che Jiang</a>
|
<a href=/people/g/guanting-dong/>Guanting Dong</a>
|
<a href=/people/b/bowen-zhou/>Bowen Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--38><div class="card-body p-3 small">Insight gradually becomes a crucial form of long-term memory for an agent. However, the emergence of irrelevant insight and the lack of general insight can greatly undermine the effectiveness of insight. To solve this problem, in this paper, we introduce **M**ulti-**S**cale **I**nsight Agent (MSI-Agent), an embodied agent designed to improve LLMs’ planning and decision-making ability by summarizing and utilizing insight effectively across different scales. MSI achieves this through the experience selector, insight generator, and insight selector. Leveraging a three-part pipeline, MSI can generate task-specific and high-level insight, store it in a database, and then use relevant insight from it to aid in decision-making. Our experiments show that MSI outperforms another insight strategy when planning by GPT3.5. Moreover, We delve into the strategies for selecting seed experience and insight, aiming to provide LLM with more useful and relevant insight for better decision-making. Our observations also indicate that MSI exhibits better robustness when facing domain-shifting scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.39.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.39.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.39/><span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>F</span>a: A Dataset of News Comments with Common Logical Fallacies Written by <span class=acl-fixed-case>LLM</span>-Assisted Crowds</a></strong><br><a href=/people/m/min-hsuan-yeh/>Min-Hsuan Yeh</a>
|
<a href=/people/r/ruyuan-wan/>Ruyuan Wan</a>
|
<a href=/people/t/ting-hao-huang/>Ting-Hao Kenneth Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--39><div class="card-body p-3 small">Detecting logical fallacies in texts can help users spot argument flaws, but automating this detection is not easy. Manually annotating fallacies in large-scale, real-world text data to create datasets for developing and validating detection models is costly. This paper introduces CoCoLoFa, the largest known logical fallacy dataset, containing 7,706 comments for 648 news articles, with each comment labeled for fallacy presence and type. We recruited 143 crowd workers to write comments embodying specific fallacy types (e.g., slippery slope) in response to news articles. Recognizing the complexity of this writing task, we built an LLM-powered assistant into the workers’ interface to aid in drafting and refining their comments. Experts rated the writing quality and labeling validity of CoCoLoFa as high and reliable. BERT-based models fine-tuned using CoCoLoFa achieved the highest fallacy detection (F1=0.86) and classification (F1=0.87) performance on its test set, outperforming the state-of-the-art LLMs. Our work shows that combining crowdsourcing and LLMs enables us to more effectively construct datasets for complex linguistic phenomena that crowd workers find challenging to produce on their own.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.40.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.40/>Tokenization Is More Than Compression</a></strong><br><a href=/people/c/craig-w-schmidt/>Craig W Schmidt</a>
|
<a href=/people/v/varshini-reddy/>Varshini Reddy</a>
|
<a href=/people/h/haoran-zhang/>Haoran Zhang</a>
|
<a href=/people/a/alec-alameddine/>Alec Alameddine</a>
|
<a href=/people/o/omri-uzan/>Omri Uzan</a>
|
<a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/c/chris-tanner/>Chris Tanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--40><div class="card-body p-3 small">Tokenization is a foundational step in natural language processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document’s text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.41.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.41/><span class=acl-fixed-case>FLIRT</span>: Feedback Loop In-context Red Teaming</a></strong><br><a href=/people/n/ninareh-mehrabi/>Ninareh Mehrabi</a>
|
<a href=/people/p/palash-goyal/>Palash Goyal</a>
|
<a href=/people/c/christophe-dupuy/>Christophe Dupuy</a>
|
<a href=/people/q/qian-hu/>Qian Hu</a>
|
<a href=/people/s/shalini-ghosh/>Shalini Ghosh</a>
|
<a href=/people/r/richard-zemel/>Richard Zemel</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a>
|
<a href=/people/r/rahul-gupta/>Rahul Gupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--41><div class="card-body p-3 small">Warning: this paper contains content that may be inappropriate or offensive.As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. In this work, we propose an automatic red teaming framework that evaluates a given black-box model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. In particular, taking text-to-image models as target models, we explore different feedback mechanisms to automatically learn effective and diverse adversarial prompts. Our experiments demonstrate that even with enhanced safety features, Stable Diffusion (SD) models are vulnerable to our adversarial prompts, raising concerns on their robustness in practical uses. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.42.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.42/>Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections</a></strong><br><a href=/people/l/lingjun-zhao/>Lingjun Zhao</a>
|
<a href=/people/k/khanh-xuan-nguyen/>Khanh Xuan Nguyen</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé Iii</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--42><div class="card-body p-3 small">Language models will inevitably err in situations with which they are unfamiliar. However, by effectively communicating uncertainties, they can still guide humans toward making sound decisions in those contexts. We demonstrate this idea by developing HEAR, a system that can successfully guide humans in simulated residential environments despite generating potentially inaccurate instructions. Diverging from systems that provide users with only the instructions they generate, HEAR warns users of potential errors in its instructions and suggests corrections. This rich uncertainty information effectively prevents misguidance and reduces the search space for users. Evaluation with 80 users shows that HEAR achieves a 13% increase in success rate and a 29% reduction in final location error distance compared to only presenting instructions to users. Interestingly, we find that offering users possibilities to explore, HEAR motivates them to make more attempts at the task, ultimately leading to a higher success rate. To our best knowledge, this work is the first to show the practical benefits of uncertainty communication in a long-horizon sequential decision-making problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.43.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.43/>Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks</a></strong><br><a href=/people/h/haoyuan-wu/>Haoyuan Wu</a>
|
<a href=/people/h/haisheng-zheng/>Haisheng Zheng</a>
|
<a href=/people/z/zhuolun-he/>Zhuolun He</a>
|
<a href=/people/b/bei-yu/>Bei Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--43><div class="card-body p-3 small">Large language models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across general tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce parameter-efficient sparsity crafting (PESC), which crafts dense models into sparse models using the mixture-of-experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal parameter increase when guaranteeing the quality of approximation in function space compared to original sparse upcycling. Our empirical evaluation demonstrates the effectiveness of the PESC method. Using PESC during instruction tuning, our best sparse model outperforms other sparse and dense models and exhibits superior general capabilities compared to GPT-3.5.Our code is available at https://github.com/wuhy68/Parameter-Efficient-MoE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.44.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.44/><span class=acl-fixed-case>G</span>eo<span class=acl-fixed-case>GPT</span>4<span class=acl-fixed-case>V</span>: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation</a></strong><br><a href=/people/s/shihao-cai/>Shihao Cai</a>
|
<a href=/people/k/keqin-bao/>Keqin Bao</a>
|
<a href=/people/h/hangyu-guo/>Hangyu Guo</a>
|
<a href=/people/j/jizhi-zhang/>Jizhi Zhang</a>
|
<a href=/people/j/jun-song/>Jun Song</a>
|
<a href=/people/b/bo-zheng/>Bo Zheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--44><div class="card-body p-3 small">Large language models have seen widespread adoption in math problem-solving, yet for geometry problems, which often necessitate visual aids even for humans, the most advanced multi-modal models still struggle to effectively utilize image information. High-quality data is crucial for enhancing the geometric capabilities of multi-modal models, yet existing open-source datasets and related efforts are either too challenging for direct model learning or suffer from misalignment between text and images. To overcome this issue, we introduce a novel pipeline that leverages GPT-4 and GPT-4V to generate relatively basic geometry problems with aligned text and images, facilitating model learning. We have produced a dataset of 4.9K geometry problems and combined it with 19K open-source data to form our GeoGPT4V dataset. Experimental results demonstrate that the GeoGPT4V dataset significantly improves the geometry performance of various models on the MathVista and MathVision benchmarks. The code is available at https://anonymous.4open.science/r/GeoGPT4V-08B2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.45.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.45/><span class=acl-fixed-case>D</span>y<span class=acl-fixed-case>V</span>o: Dynamic Vocabularies for Learned Sparse Retrieval with Entities</a></strong><br><a href=/people/t/thong-nguyen/>Thong Nguyen</a>
|
<a href=/people/s/shubham-chatterjee/>Shubham Chatterjee</a>
|
<a href=/people/s/sean-macavaney/>Sean MacAvaney</a>
|
<a href=/people/i/iain-mackie/>Iain Mackie</a>
|
<a href=/people/j/jeff-dalton/>Jeff Dalton</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--45><div class="card-body p-3 small">Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained transformers, which often split entities into nonsensical fragments. Splitting entities diminishes retrieval accuracy and limits the model’s ability to incorporate up-to-date world knowledge not included in the training data. In this work, we enhance the LSR vocabulary with Wikipedia concepts and entities, enabling the model to resolve ambiguities more effectively and stay current with evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo) head, which leverages existing entity embeddings and an entity retrieval component that identifies entities relevant to a query or document. We use the DyVo head to generate entity weights, which are then merged with word piece weights to create joint representations for efficient indexing and retrieval using an inverted index. In experiments across three entity-rich document ranking datasets, the resulting DyVo model substantially outperforms several state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.46.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.46/>Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models</a></strong><br><a href=/people/z/zihan-wang/>Zihan Wang</a>
|
<a href=/people/d/deli-chen/>Deli Chen</a>
|
<a href=/people/d/damai-dai/>Damai Dai</a>
|
<a href=/people/r/runxin-xu/>Runxin Xu</a>
|
<a href=/people/z/zhuoshu-li/>Zhuoshu Li</a>
|
<a href=/people/y/yu-wu/>Yu Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--46><div class="card-body p-3 small">Parameter-efficient fine-tuning (<b>PEFT</b>) is crucial for customizing Large Language Models (LLMs) with constrained resource. Although there have been various PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture LLMs is still underexplored. In this work, we study the PEFT method for LLMs with the Mixture-of-Experts (MoE) architecture and the contents of this work are mainly threefold: (1) We investigate the dispersion degree of the activated experts in customized tasks, and found that the routing distribution for specific task tend to be highly concentrated, while the distribution of activated experts varies significantly across different tasks. (2) We propose the expert-specialized fine-tuning method, which tunes the experts most relevant to downstream tasks while freezing the other experts; experimental results demonstrate that our method not only improves the tuning efficiency, but also matches or even surpasses the performance of full-parameter fine-tuning. (3) We further analyze the impact of the MoE architecture on expert-specialized fine-tuning. We find that MoE models with finer-grained experts are more advantageous in selecting the combination of experts that are most relevant to downstream tasks, thereby enhancing the both the training efficiency and effectiveness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.47.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.47.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--47 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.47 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.47/><span class=acl-fixed-case>L</span>ong<span class=acl-fixed-case>E</span>mbed: Extending Embedding Models for Long Context Retrieval</a></strong><br><a href=/people/d/dawei-zhu/>Dawei Zhu</a>
|
<a href=/people/l/liang-wang/>Liang Wang</a>
|
<a href=/people/n/nan-yang/>Nan Yang</a>
|
<a href=/people/y/yifan-song/>Yifan Song</a>
|
<a href=/people/w/wenhao-wu/>Wenhao Wu</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--47><div class="card-body p-3 small">Embedding models play a pivotal role in modern NLP applications such as document retrieval. However, existing embedding models are limited to encoding short documents of typically 512 tokens, restrained from application scenarios requiring long inputs. This paper explores context window extension of existing embedding models, pushing their input length to a maximum of 32,768. We begin by evaluating the performance of existing embedding models using our newly constructed LongEmbed benchmark, which includes two synthetic and four real-world tasks, featuring documents of varying lengths and dispersed target information. The benchmarking results highlight huge opportunities for enhancement in current models. Via comprehensive experiments, we demonstrate that training-free context window extension strategies can effectively increase the input length of these models by several folds. Moreover, comparison of models using Absolute Position Encoding (APE) and Rotary Position Encoding (RoPE) reveals the superiority of RoPE-based embedding models in context window extension, offering empirical guidance for future models. Our benchmark, code and trained models will be released to advance the research in long context embedding models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.48.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.48.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--48 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.48 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.48/>Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences</a></strong><br><a href=/people/x/xiangyang-liu/>Xiangyang Liu</a>
|
<a href=/people/j/junliang-he/>Junliang He</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--48><div class="card-body p-3 small">Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps using chain-of-thought prompting under zero-shot or few-shot settings. However, zero-shot prompting always encounters low performance, and the superior performance of few-shot prompting hinges on the manual-crafting of task-specific demonstrations one by one. In this paper, we present **RoSE** (**R**easoning with **O**rchestrated **S**treaming **E**xperiences), a general framework for solving reasoning tasks that can self-improve as it answers various reasoning questions. To enable RoSE, we describe an architecture that extends an LLM to store all answered reasoning questions and their reasoning steps in a streaming experience pool and orchestrate helpful questions from the pool to assist itself in answering new questions. To set up a question-aware orchestration mechanism, RoSE first calculates the similarity of each question in the pool with the question to be answered. Since the solution to each question in the experience pool is not always correct, RoSE will sort the questions according to their similarity with the question to be answered, and then uniformly divide them into multiple buckets. It finally extracts one question from each bucket to make the extracted questions more diverse. To make the extracted questions help RoSE answer new questions as much as possible, we introduce two other attributes of uncertainty and complexity for each question. RoSE will preferentially select the questions with low uncertainty and high complexity from each bucket. We evaluate the versatility of RoSE in various complex reasoning tasks and LLMs, such as arithmetic and commonsense reasoning, and find that it can achieve excellent performance without any labeled data and pre-set unlabeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.49.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.49.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.49/>Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue</a></strong><br><a href=/people/x/xianlong-luo/>Xianlong Luo</a>
|
<a href=/people/m/meng-yang/>Meng Yang</a>
|
<a href=/people/y/yihao-wang/>Yihao Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--49><div class="card-body p-3 small">Dialogue Aspect-based Sentiment Quadruple analysis (DiaASQ) extends ABSA to more complex real-world scenarios (i.e., dialogues), which makes existing generation methods encounter heightened noise and order bias challenges, leading to decreased robustness and accuracy.To address these, we propose the Segmentation-Aided multi-grained Denoising and Debiasing (SADD) method. For noise, we propose the Multi-Granularity Denoising Generation model (MGDG), achieving word-level denoising via sequence labeling and utterance-level denoising via topic-aware dialogue segmentation. Denoised Attention in MGDG integrates multi-grained denoising information to help generate denoised output.For order bias, we first theoretically analyze its direct cause as the gap between ideal and actual training objectives and propose a distribution-based solution. Since this solution introduces a one-to-many learning challenge, our proposed Segmentation-aided Order Bias Mitigation (SOBM) method utilizes dialogue segmentation to supplement order diversity, concurrently mitigating this challenge and order bias.Experiments demonstrate SADD’s effectiveness, achieving state-of-the-art results with a 6.52% F1 improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.50.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.50/>Integrating <span class=acl-fixed-case>P</span>lutchik’s Theory with Mixture of Experts for Enhancing Emotion Classification</a></strong><br><a href=/people/d/dongjun-lim/>Dongjun Lim</a>
|
<a href=/people/y/yun-gyung-cheong/>Yun-Gyung Cheong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--50><div class="card-body p-3 small">Emotion significantly influences human behavior and decision-making processes. We propose a labeling methodology grounded in Plutchik’s Wheel of Emotions theory for emotion classification. Furthermore, we employ a Mixture of Experts (MoE) architecture to evaluate the efficacy of this labeling approach, by identifying the specific emotions that each expert learns to classify. Experimental results reveal that our methodology improves the performance of emotion classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.51.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.51/>In-context Contrastive Learning for Event Causality Identification</a></strong><br><a href=/people/l/liang-chao/>Liang Chao</a>
|
<a href=/people/w/wei-xiang/>Wei Xiang</a>
|
<a href=/people/b/bang-wang/>Bang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--51><div class="card-body p-3 small">Event Causality Identification (ECI) aims at determining the existence of a causal relation between two events. Although recent prompt learning-based approaches have shown promising improvements on the ECI task, their performance are often subject to the delicate design of multiple prompts and the positive correlations between the main task and derivate tasks. The in-context learning paradigm provides explicit guidance for label prediction in the prompt learning paradigm, alleviating its reliance on complex prompts and derivative tasks. However, it does not distinguish between positive and negative demonstrations for analogy learning. Motivated from such considerations, this paper proposes an **I**n-**C**ontext **C**ontrastive **L**earning (ICCL) model that utilizes contrastive learning to enhance the effectiveness of both positive and negative demonstrations. Additionally, we apply contrastive learning to event pairs to better facilitate event causality identification. Our ICCL is evaluated on the widely used corpora, including the EventStoryLine and Causal-TimeBank, and results show significant performance improvements over the state-of-the-art algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.52.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.52.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.52/>What’s Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs</a></strong><br><a href=/people/a/anna-wegmann/>Anna Wegmann</a>
|
<a href=/people/t/tijs-a-van-den-broek/>Tijs A. Van Den Broek</a>
|
<a href=/people/d/dong-nguyen/>Dong Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--52><div class="card-body p-3 small">Best practices for high conflict conversations like counseling or customer support almost always include recommendations to paraphrase the previous speaker. Although paraphrase classification has received widespread attention in NLP, paraphrases are usually considered independent from context, and common models and datasets are not applicable to dialog settings. In this work, we investigate paraphrases across turns in dialog (e.g., Speaker 1: “That book is mine.” becomes Speaker 2: “That book is yours.”). We provide an operationalization of context-dependent paraphrases, and develop a training for crowd-workers to classify paraphrases in dialog. We introduce ContextDeP, a dataset with utterance pairs from NPR and CNN news interviews annotated for context-dependent paraphrases. To enable analyses on label variation, the dataset contains 5,581 annotations on 600 utterance pairs. We present promising results with in-context learning and with token classification models for automatic paraphrase detection in dialog.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.53.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.53.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.53.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.53/>Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing <span class=acl-fixed-case>AANN</span>s</a></strong><br><a href=/people/k/kanishka-misra/>Kanishka Misra</a>
|
<a href=/people/k/kyle-mahowald/>Kyle Mahowald</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--53><div class="card-body p-3 small">Language models learn rare syntactic phenomena, but the extent to which this is attributable to generalization vs. memorization is a major open question. To that end, we iteratively trained transformer language models on systematically manipulated corpora which were human-scale in size, and then evaluated their learning of a rare grammatical phenomenon: the English Article+Adjective+Numeral+Noun (AANN) construction (“a beautiful five days”). We compared how well this construction was learned on the default corpus relative to a counterfactual corpus in which AANN sentences were removed. We found that AANNs were still learned better than systematically perturbed variants of the construction. Using additional counterfactual corpora, we suggest that this learning occurs through generalization from related constructions (e.g., “a few days”). An additional experiment showed that this learning is enhanced when there is more variability in the input. Taken together, our results provide an existence proof that LMs can learn rare grammatical phenomena by generalization from less rare phenomena. Data and code: https://github.com/kanishkamisra/aannalysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.54.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.54/>Large Language Models for Data Annotation and Synthesis: A Survey</a></strong><br><a href=/people/z/zhen-tan/>Zhen Tan</a>
|
<a href=/people/d/dawei-li/>Dawei Li</a>
|
<a href=/people/s/song-wang/>Song Wang</a>
|
<a href=/people/a/alimohammad-beigi/>Alimohammad Beigi</a>
|
<a href=/people/b/bohan-jiang/>Bohan Jiang</a>
|
<a href=/people/a/amrita-bhattacharjee/>Amrita Bhattacharjee</a>
|
<a href=/people/m/mansooreh-karami/>Mansooreh Karami</a>
|
<a href=/people/j/jundong-li/>Jundong Li</a>
|
<a href=/people/l/lu-cheng/>Lu Cheng</a>
|
<a href=/people/h/huan-liu/>Huan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--54><div class="card-body p-3 small">Data annotation and synthesis generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to automate the complicated process of data annotation and synthesis. While existing surveys have extensively covered LLM architecture, training, and general applications, we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment, and LLM-Generated Annotations Utilization. Furthermore, this survey includes an in-depth taxonomy of data types that LLMs can annotate, a comprehensive review of learning strategies for models utilizing LLM-generated annotations, and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation and synthesis. Serving as a key guide, this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation, thereby fostering future advancements in this critical field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.55.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.55/>Chain-of-Dictionary Prompting Elicits Translation in Large Language Models</a></strong><br><a href=/people/h/hongyuan-lu/>Hongyuan Lu</a>
|
<a href=/people/h/haoran-yang/>Haoran Yang</a>
|
<a href=/people/h/haoyang-huang/>Haoyang Huang</a>
|
<a href=/people/d/dongdong-zhang/>Dongdong Zhang</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--55><div class="card-body p-3 small">Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even if not being trained explicitly for translation. Yet, they still struggle with translating low-resource languages. As supported by our experiments, a bilingual dictionary between the source and the target language could help. Motivated by the fact that multilingual training effectively improves cross-lingual performance, we show that a chained multilingual dictionary with words expressed in more languages can provide more information to better enhance the LLM translation. To this end, we present a novel framework, CoD, Chain-of-Dictionary Prompting, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Experiments indicate that ChatGPT and InstructGPT still have room for improvement in translating many language pairs. And CoD elicits large gains by up to 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We demonstrate the importance of chaining the multilingual dictionaries, as well as the superiority of CoD to few-shot in-context learning for low-resource languages. Using CoD helps ChatGPT to obviously surpass the SOTA translator NLLB 3.3B.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.56.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.56.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.56/><span class=acl-fixed-case>A</span>da<span class=acl-fixed-case>Z</span>eta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning</a></strong><br><a href=/people/y/yifan-yang/>Yifan Yang</a>
|
<a href=/people/k/kai-zhen/>Kai Zhen</a>
|
<a href=/people/e/ershad-banijamali/>Ershad Banijamali</a>
|
<a href=/people/a/athanasios-mouchtaris/>Athanasios Mouchtaris</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--56><div class="card-body p-3 small">Fine-tuning large language models (LLMs) has achieved remarkable performance across various natural language processing tasks, yet it demands more and more memory as model sizes keep growing. To address this issue, the recently proposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs using only forward passes, thereby avoiding the need for a backpropagation graph. However, significant performance drops and a high risk of divergence have limited their widespread adoption. In this paper, we propose the Adaptive Zeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed to improve the performance and convergence of the ZO methods. To enhance dimension-dependent ZO estimation accuracy, we introduce a fast-forward, low-parameter tensorized adapter. To tackle the frequently observed divergence issue in large-scale ZO fine-tuning tasks, we propose an adaptive query number schedule that guarantees convergence. Detailed theoretical analysis and extensive experimental results on Roberta-Large and Llama-2-7B models substantiate the efficacy of our AdaZeta framework in terms of accuracy, memory efficiency, and convergence speed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.57.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.57/><span class=acl-fixed-case>R</span>ose<span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>RA</span>: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning</a></strong><br><a href=/people/h/haoyu-wang/>Haoyu Wang</a>
|
<a href=/people/t/tianci-liu/>Tianci Liu</a>
|
<a href=/people/r/ruirui-li/>Ruirui Li</a>
|
<a href=/people/m/monica-xiao-cheng/>Monica Xiao Cheng</a>
|
<a href=/people/t/tuo-zhao/>Tuo Zhao</a>
|
<a href=/people/j/jing-gao/>Jing Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--57><div class="card-body p-3 small">Pre-trained language models, trained on large-scale corpora, demonstrate strong generalizability across various NLP tasks. Fine-tuning these models for specific tasks typically involves updating all parameters, which is resource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the popular LoRA family, introduce low-rank matrices to learn only a few parameters efficiently. However, during inference, the product of these matrices updates all pre-trained parameters, complicating tasks like knowledge editing that require selective updates. We propose a novel PEFT method, which conducts <b>r</b>ow and c<b>o</b>lumn-wise spar<b>se</b> <b>lo</b>w-<b>r</b>ank <b>a</b>daptation (RoseLoRA), to address this challenge. RoseLoRA identifies and updates only the most important parameters for a specific task, maintaining efficiency while preserving other model knowledge. By adding a sparsity constraint on the product of low-rank matrices and converting it to row and column-wise sparsity, we ensure efficient and precise model updates. Our theoretical analysis guarantees the lower bound of the sparsity with respective to the matrix product. Extensive experiments on five benchmarks across twenty datasets demonstrate that RoseLoRA outperforms baselines in both general fine-tuning and knowledge editing tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.58.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.58/><span class=acl-fixed-case>B</span>lend<span class=acl-fixed-case>F</span>ilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering</a></strong><br><a href=/people/h/haoyu-wang/>Haoyu Wang</a>
|
<a href=/people/r/ruirui-li/>Ruirui Li</a>
|
<a href=/people/h/haoming-jiang/>Haoming Jiang</a>
|
<a href=/people/j/jinjin-tian/>Jinjin Tian</a>
|
<a href=/people/z/zhengyang-wang/>Zhengyang Wang</a>
|
<a href=/people/c/chen-luo/>Chen Luo</a>
|
<a href=/people/x/xianfeng-tang/>Xianfeng Tang</a>
|
<a href=/people/m/monica-xiao-cheng/>Monica Xiao Cheng</a>
|
<a href=/people/t/tuo-zhao/>Tuo Zhao</a>
|
<a href=/people/j/jing-gao/>Jing Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--58><div class="card-body p-3 small">Retrieval-augmented Large Language Models (LLMs) offer substantial benefits in enhancing performance across knowledge-intensive scenarios. However, these methods often struggle with complex inputs and encounter difficulties due to noisy knowledge retrieval, notably hindering model effectiveness. To address this issue, we introduce BlendFilter, a novel approach that elevates retrieval-augmented LLMs by integrating query generation blending with knowledge filtering. BlendFilter proposes the blending process through its query generation method, which integrates both external and internal knowledge augmentation with the original query, ensuring comprehensive information gathering. Additionally, our distinctive knowledge filtering module capitalizes on the intrinsic capabilities of the LLM, effectively eliminating extraneous data. We conduct extensive experiments on three open-domain question answering benchmarks, and the findings clearly indicate that our innovative BlendFilter surpasses state-of-the-art baselines significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.59.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.59/><span class=acl-fixed-case>HEART</span>-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/j/jocelyn-j-shen/>Jocelyn J Shen</a>
|
<a href=/people/j/joel-mire/>Joel Mire</a>
|
<a href=/people/h/hae-won-park/>Hae Won Park</a>
|
<a href=/people/c/cynthia-breazeal/>Cynthia Breazeal</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--59><div class="card-body p-3 small">Empathy serves as a cornerstone in enabling prosocial behaviors, and can be evoked through sharing of personal experiences in stories. While empathy is influenced by narrative content, intuitively, people respond to the way a story is told as well, through narrative style. Yet the relationship between empathy and narrative style is not fully understood. In this work, we empirically examine and quantify this relationship between style and empathy using LLMs and large-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy, HEART (Human Empathy and Narrative Taxonomy) that delineates elements of narrative style that can lead to empathy with the narrator of a story. We establish the performance of LLMs in extracting narrative elements from HEART, showing that prompting with our taxonomy leads to reasonable, human-level annotations beyond what prior lexicon-based methods can do. To show empirical use of our taxonomy, we collect a dataset of empathy judgments of stories via a large-scale crowdsourcing study with <span class=tex-math>N=2,624</span> participants. We show that narrative elements extracted via LLMs, in particular, vividness of emotions and plot volume, can elucidate the pathways by which narrative style cultivates empathy towards personal stories. Our work suggests that such models can be used for narrative analyses that lead to human-centered social and behavioral insights.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.60.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.60/>Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled <span class=acl-fixed-case>KL</span> Divergence</a></strong><br><a href=/people/j/junru-lu/>Junru Lu</a>
|
<a href=/people/j/jiazheng-li/>Jiazheng Li</a>
|
<a href=/people/s/siyu-an/>Siyu An</a>
|
<a href=/people/m/meng-zhao/>Meng Zhao</a>
|
<a href=/people/y/yulan-he/>Yulan He</a>
|
<a href=/people/d/di-yin/>Di Yin</a>
|
<a href=/people/x/xing-sun/>Xing Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--60><div class="card-body p-3 small">Direct Preference Optimization (DPO) has emerged as a prominent algorithm for the direct and robust alignment of Large Language Models (LLMs) with human preferences, offering a more straightforward alternative to the complex Reinforcement Learning from Human Feedback (RLHF). Despite its promising efficacy, DPO faces a notable drawback: “verbosity”, a common over-optimization phenomenon also observed in RLHF. While previous studies mainly attributed verbosity to biased labels within the data, we propose that the issue also stems from an inherent algorithmic length reliance in DPO. Specifically, we suggest that the discrepancy between sequence-level Kullback–Leibler (KL) divergences between chosen and rejected sequences, used in DPO, results in overestimated or underestimated rewards due to varying token lengths. Empirically, we utilize datasets with different label lengths to demonstrate the presence of biased rewards. We then introduce an effective downsampling approach, named SamPO, to eliminate potential length reliance. Our experimental evaluations, conducted across three LLMs of varying scales and a diverse array of conditional and open-ended benchmarks, highlight the efficacy of SamPO in mitigating verbosity, achieving improvements of 5% to 12% over DPO through debaised rewards. Our code can be accessed at: https://github.com/LuJunru/SamPO/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.61.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.61.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.61.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.61/>Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval</a></strong><br><a href=/people/t/tianyi-hu/>Tianyi Hu</a>
|
<a href=/people/m/maria-maistro/>Maria Maistro</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--61><div class="card-body p-3 small">The cross-cultural adaptation of recipes is an important application of identifying and bridging cultural differences in language. The challenge lies in retaining the essence of the original recipe while also aligning with the writing and dietary habits of the target culture. Information Retrieval (IR) offers a way to address the challenge because it retrieves results from the culinary practices of the target culture while maintaining relevance to the original recipe. We introduce a novel task about cross-cultural recipe retrieval and present a unique Chinese-English cross-cultural recipe retrieval benchmark. Our benchmark is manually annotated under limited resource, utilizing various retrieval models to generate a pool of candidate results for manual annotation. The dataset provides retrieval samples that are culturally adapted but textually diverse, presenting greater challenges. We propose CARROT, a plug-and-play cultural-aware recipe information retrieval framework that incorporates cultural-aware query rewriting and re-ranking methods and evaluate it both on our benchmark and intuitive human judgments. The results show that our framework significantly enhances the preservation of the original recipe and its cultural appropriateness for the target culture. We believe these insights will significantly contribute to future research on cultural adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.62.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.62/><span class=acl-fixed-case>RULE</span>: Reliable Multimodal <span class=acl-fixed-case>RAG</span> for Factuality in Medical Vision Language Models</a></strong><br><a href=/people/p/peng-xia/>Peng Xia</a>
|
<a href=/people/k/kangyu-zhu/>Kangyu Zhu</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/h/hongtu-zhu/>Hongtu Zhu</a>
|
<a href=/people/y/yun-li/>Yun Li</a>
|
<a href=/people/g/gang-li/>Gang Li</a>
|
<a href=/people/l/linjun-zhang/>Linjun Zhang</a>
|
<a href=/people/h/huaxiu-yao/>Huaxiu Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--62><div class="card-body p-3 small">The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical facts. Retrieval-Augmented Generation (RAG), which utilizes external knowledge, can improve the factual accuracy of these models but introduces two major challenges. First, limited retrieved contexts might not cover all necessary information, while excessive retrieval can introduce irrelevant and inaccurate references, interfering with the model’s generation. Second, in cases where the model originally responds correctly, applying RAG can lead to an over-reliance on retrieved contexts, resulting in incorrect answers. To address these issues, we propose RULE, which consists of two components. First, we introduce a provably effective strategy for controlling factuality risk through the calibrated selection of the number of retrieved contexts. Second, based on samples where over-reliance on retrieved contexts led to errors, we curate a preference dataset to fine-tune the model, balancing its dependence on inherent knowledge and retrieved contexts for generation. We demonstrate the effectiveness of RAFE on three medical VQA datasets, achieving an average improvement of 20.8% in factual accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.63.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.63/><span class=acl-fixed-case>C</span>rypto<span class=acl-fixed-case>T</span>rade: A Reflective <span class=acl-fixed-case>LLM</span>-based Agent to Guide Zero-shot Cryptocurrency Trading</a></strong><br><a href=/people/y/yuan-li/>Yuan Li</a>
|
<a href=/people/b/bingqiao-luo/>Bingqiao Luo</a>
|
<a href=/people/q/qian-wang/>Qian Wang</a>
|
<a href=/people/n/nuo-chen/>Nuo Chen</a>
|
<a href=/people/x/xu-liu/>Xu Liu</a>
|
<a href=/people/b/bingsheng-he/>Bingsheng He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--63><div class="card-body p-3 small">The utilization of Large Language Models (LLMs) in financial trading has primarily been concentrated within the stock market, aiding in economic and financial decisions. Yet, the unique opportunities presented by the cryptocurrency market, noted for its on-chain data’s transparency and the critical influence of off-chain signals like news, remain largely untapped by LLMs. This work aims to bridge the gap by developing an LLM-based trading agent, CryptoTrade, which uniquely combines the analysis of on-chain and off-chain data. This approach leverages the transparency and immutability of on-chain data, as well as the timeliness and influence of off-chain signals, providing a comprehensive overview of the cryptocurrency market. CryptoTrade incorporates a reflective mechanism specifically engineered to refine its daily trading decisions by analyzing the outcomes of prior trading decisions. This research makes two significant contributions. Firstly, it broadens the applicability of LLMs to the domain of cryptocurrency trading. Secondly, it establishes a benchmark for cryptocurrency trading strategies. Through extensive experiments, CryptoTrade has demonstrated superior performance in maximizing returns compared to time-series baselines, but not compared to traditional trading signals, across various cryptocurrencies and market conditions. Our code and data are available at <a href=https://github.com/Xtra-Computing/CryptoTrade class=acl-markup-url>https://github.com/Xtra-Computing/CryptoTrade</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.64.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.64.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--64 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.64 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.64/>A Survey on In-context Learning</a></strong><br><a href=/people/q/qingxiu-dong/>Qingxiu Dong</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/d/damai-dai/>Damai Dai</a>
|
<a href=/people/c/ce-zheng/>Ce Zheng</a>
|
<a href=/people/j/jingyuan-ma/>Jingyuan Ma</a>
|
<a href=/people/r/rui-li/>Rui Li</a>
|
<a href=/people/h/heming-xia/>Heming Xia</a>
|
<a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/z/zhiyong-wu/>Zhiyong Wu</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--64><div class="card-body p-3 small">With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.65.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.65/><span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>H</span>ie<span class=acl-fixed-case>N</span>et: A Large and Diverse Dataset for Document Hierarchy Parsing</a></strong><br><a href=/people/h/hangdi-xing/>Hangdi Xing</a>
|
<a href=/people/c/changxu-cheng/>Changxu Cheng</a>
|
<a href=/people/f/feiyu-gao/>Feiyu Gao</a>
|
<a href=/people/z/zirui-shao/>Zirui Shao</a>
|
<a href=/people/z/zhi-yu/>Zhi Yu</a>
|
<a href=/people/j/jiajun-bu/>Jiajun Bu</a>
|
<a href=/people/q/qi-zheng/>Qi Zheng</a>
|
<a href=/people/c/cong-yao/>Cong Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--65><div class="card-body p-3 small">Parsing documents from pixels, such as pictures and scanned PDFs, into hierarchical structures is extensively demanded in the daily routines of data storage, retrieval and understanding. However, previously the research on this topic has been largely hindered since most existing datasets are small-scale, or contain documents of only a single type, which are characterized by a lack of document diversity. Moreover, there is a significant discrepancy in the annotation standards across datasets. In this paper, we introduce a large and diverse document hierarchy parsing (DHP) dataset to compensate for the data scarcity and inconsistency problem. We aim to set a new standard as a more practical, long-standing benchmark. Meanwhile, we present a new DHP framework designed to grasp both fine-grained text content and coarse-grained pattern at layout element level, enhancing the capacity of pre-trained text-layout models in handling the multi-page and multi-level challenges in DHP. Through exhaustive experiments, we validate the effectiveness of our proposed dataset and method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.66.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.66.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.66/><span class=acl-fixed-case>AMR</span>-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation</a></strong><br><a href=/people/z/ziyang-luo/>Ziyang Luo</a>
|
<a href=/people/x/xin-li/>Xin Li</a>
|
<a href=/people/h/hongzhan-lin/>Hongzhan Lin</a>
|
<a href=/people/j/jing-ma/>Jing Ma</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--66><div class="card-body p-3 small">The impressive performance of proprietary LLMs like GPT4 in code generation has led to a trend to replicate these capabilities in open-source models through knowledge distillation (e.g. Code Evol-Instruct). However, these efforts often neglect the crucial aspect of response quality, relying heavily on teacher models for direct response distillation. This paradigm, especially for complex instructions, can degrade the quality of synthesized data, compromising the knowledge distillation process. To this end, our study introduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which employs a two-stage process to refine response distillation. The first stage, modular decomposition, breaks down the direct response into more manageable sub-modules. The second stage, adaptive response evolution, automatically evolves the response with the related function modules. Our experiments with three popular code benchmarks—HumanEval, MBPP, and EvalPlus—attests to the superiority of the AMR-Evol framework over baseline response distillation methods. By comparing with the open-source Code LLMs trained on a similar scale of data, we observed performance enhancements: more than +3.0 points on HumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the effectiveness of our framework. Our codes are available at https://github.com/ChiYeungLaw/AMR-Evol.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.67.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.67.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.67/><span class=acl-fixed-case>EFUF</span>: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models</a></strong><br><a href=/people/s/shangyu-xing/>Shangyu Xing</a>
|
<a href=/people/f/fei-zhao/>Fei Zhao</a>
|
<a href=/people/z/zhen-wu/>Zhen Wu</a>
|
<a href=/people/t/tuo-an/>Tuo An</a>
|
<a href=/people/w/weihao-chen/>Weihao Chen</a>
|
<a href=/people/c/chunhui-li/>Chunhui Li</a>
|
<a href=/people/j/jianbing-zhang/>Jianbing Zhang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--67><div class="card-body p-3 small">Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we propose an efficient fine-grained unlearning framework (EFUF), which performs gradient ascent utilizing three tailored losses to eliminate hallucinations without paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.68.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.68/>Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization</a></strong><br><a href=/people/s/sungbin-shin/>Sungbin Shin</a>
|
<a href=/people/w/wonpyo-park/>Wonpyo Park</a>
|
<a href=/people/j/jaeho-lee/>Jaeho Lee</a>
|
<a href=/people/n/namhoon-lee/>Namhoon Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--68><div class="card-body p-3 small">This work suggests fundamentally rethinking the current practice of pruning large language models (LLMs). The way it is done is by divide and conquer: split the model into submodels, sequentially prune them, and reconstruct predictions of the dense counterparts on small calibration data one at a time; the final model is obtained simply by putting the resulting sparse submodels together. While this approach enables pruning under memory constraints, it generates high reconstruction errors. In this work, we first present an array of reconstruction techniques that can significantly reduce this error by more than 90%. Unwittingly, however, we discover that minimizing reconstruction error is not always ideal and can overfit the given calibration data, resulting in rather increased language perplexity and poor performance at downstream tasks. We find out that a strategy of self-generating calibration data can mitigate this trade-off between reconstruction and generalization, suggesting new directions in the presence of both benefits and pitfalls of reconstruction for pruning LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.69.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.69/><span class=acl-fixed-case>LLM</span>s Are Zero-Shot Context-Aware Simultaneous Translators</a></strong><br><a href=/people/r/roman-koshkin/>Roman Koshkin</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--69><div class="card-body p-3 small">The advent of transformers has fueled progress in machine translation. More recently large language models (LLMs) have come to the spotlight thanks to their generality and strong performance in a wide range of language tasks, including translation. Here we show that open-source LLMs perform on par with or better than some state-of-the-art baselines in simultaneous machine translation (SiMT) tasks, zero-shot. We also demonstrate that injection of minimal background information, which is easy with an LLM, brings further performance gains, especially on challenging technical subject-matter. This highlights LLMs’ potential for building next generation of massively multilingual, context-aware and terminologically accurate SiMT systems that require no resource-intensive training or fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.70.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.70/><span class=acl-fixed-case>A</span>gent<span class=acl-fixed-case>R</span>eview: Exploring Peer Review Dynamics with <span class=acl-fixed-case>LLM</span> Agents</a></strong><br><a href=/people/y/yiqiao-jin/>Yiqiao Jin</a>
|
<a href=/people/q/qinlin-zhao/>Qinlin Zhao</a>
|
<a href=/people/y/yiyang-wang/>Yiyang Wang</a>
|
<a href=/people/h/hao-chen/>Hao Chen</a>
|
<a href=/people/k/kaijie-zhu/>Kaijie Zhu</a>
|
<a href=/people/y/yijia-xiao/>Yijia Xiao</a>
|
<a href=/people/j/jindong-wang/>Jindong Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--70><div class="card-body p-3 small">Peer review is fundamental to the integrity and advancement of scientific publication. Traditional methods of peer review analyses often rely on exploration and statistics of existing peer review data, which do not adequately address the multivariate nature of the process, account for the latent variables, and are further constrained by privacy concerns due to the sensitive nature of the data. We introduce AgentReview, the first large language model (LLM) based peer review simulation framework, which effectively disentangles the impacts of multiple latent factors and addresses the privacy issue. Our study reveals significant insights, including a notable 37.1% variation in paper decisions due to reviewers’ biases, supported by sociological theories such as the social influence theory, altruism fatigue, and authority bias. We believe that this study could offer valuable insights to improve the design of peer review mechanisms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.71.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.71/><span class=acl-fixed-case>C</span>hat<span class=acl-fixed-case>R</span>etriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval</a></strong><br><a href=/people/k/kelong-mao/>Kelong Mao</a>
|
<a href=/people/c/chenlong-deng/>Chenlong Deng</a>
|
<a href=/people/h/haonan-chen/>Haonan Chen</a>
|
<a href=/people/f/fengran-mo/>Fengran Mo</a>
|
<a href=/people/z/zheng-liu/>Zheng Liu</a>
|
<a href=/people/t/tetsuya-sakai/>Tetsuya Sakai</a>
|
<a href=/people/z/zhicheng-dou/>Zhicheng Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--71><div class="card-body p-3 small">Conversational search requires accurate interpretation of user intent from complex multi-turn contexts. This paper presents ChatRetriever, which inherits the strong generalization capability of large language models to robustly represent complex conversational sessions for dense retrieval. To achieve this, we propose a simple and effective dual-learning approach that adapts LLM for retrieval via contrastive learning while enhancing the complex session understanding through masked instruction tuning on high-quality conversational instruction tuning data. Extensive experiments on five conversational search benchmarks demonstrate that ChatRetriever significantly outperforms existing conversational dense retrievers, achieving state-of-the-art performance on par with LLM-based rewriting approaches. Furthermore, ChatRetriever exhibits superior robustness in handling diverse conversational contexts. Our work highlights the potential of adapting LLMs for retrieval with complex inputs like conversational search sessions and proposes an effective approach to advance this research direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.72.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.72.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--72 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.72 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.72/>Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments</a></strong><br><a href=/people/h/han-zhou/>Han Zhou</a>
|
<a href=/people/x/xingchen-wan/>Xingchen Wan</a>
|
<a href=/people/y/yinhong-liu/>Yinhong Liu</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--72><div class="card-body p-3 small">Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.73.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.73/>Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation</a></strong><br><a href=/people/c/chenlong-deng/>Chenlong Deng</a>
|
<a href=/people/k/kelong-mao/>Kelong Mao</a>
|
<a href=/people/z/zhicheng-dou/>Zhicheng Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--73><div class="card-body p-3 small">Legal case retrieval for sourcing similar cases is critical in upholding judicial fairness. Different from general web search, legal case retrieval involves processing lengthy, complex, and highly specialized legal documents. Existing methods in this domain often overlook the incorporation of legal expert knowledge, which is crucial for accurately understanding and modeling legal cases, leading to unsatisfactory retrieval performance. This paper introduces KELLER, a legal knowledge-guided case reformulation approach based on large language models (LLMs) for effective and interpretable legal case retrieval. By incorporating professional legal knowledge about crimes and law articles, we enable large language models to accurately reformulate the original legal case into concise sub-facts of crimes, which contain the essential information of the case. Extensive experiments on two legal case retrieval benchmarks demonstrate superior retrieval performance and robustness on complex legal case queries of KELLER over existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.74.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.74.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.74/>Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process</a></strong><br><a href=/people/p/peng-wang/>Peng Wang</a>
|
<a href=/people/x/xiaobin-wang/>Xiaobin Wang</a>
|
<a href=/people/c/chao-lou/>Chao Lou</a>
|
<a href=/people/s/shengyu-mao/>Shengyu Mao</a>
|
<a href=/people/p/pengjun-xie/>Pengjun Xie</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--74><div class="card-body p-3 small">In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language Models (LLMs), existing works are highly dependent on large-scale labeled support sets, not always feasible in practical scenarios. To refine this approach, we focus primarily on an innovative selective annotation mechanism, which precedes the standard demonstration retrieval. We introduce the Language Model-based Determinant Point Process (LM-DPP) that simultaneously considers the uncertainty and diversity of unlabeled instances for optimal selection. Consequently, this yields a subset for annotation that strikes a trade-off between the two factors. We apply LM-DPP to various language models, including GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2 Generation datasets demonstrate that LM-DPP can effectively select canonical examples. Further analysis reveals that LLMs benefit most significantly from subsets that are both low uncertainty and high diversity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.75.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.75/>Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation</a></strong><br><a href=/people/y/yuhui-zhang/>Yuhui Zhang</a>
|
<a href=/people/b/brandon-mckinzie/>Brandon McKinzie</a>
|
<a href=/people/z/zhe-gan/>Zhe Gan</a>
|
<a href=/people/v/vaishaal-shankar/>Vaishaal Shankar</a>
|
<a href=/people/a/alexander-t-toshev/>Alexander T Toshev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--75><div class="card-body p-3 small">Recent advances in image tokenizers, such as VQ-VAE, have enabled text-to-image generation using auto-regressive methods, similar to language modeling. However, these methods have yet to leverage pre-trained language models, despite their adaptability to various downstream tasks. In this work, we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help. We provide a two-fold explanation by analyzing tokens from each modality. First, we demonstrate that image tokens possess significantly different semantics compared to text tokens, rendering pre-trained language models no more effective in modeling them than randomly initialized ones. Second, the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation of language models’ capability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.76.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.76/><span class=acl-fixed-case>QUDSELECT</span>: Selective Decoding for Questions Under Discussion Parsing</a></strong><br><a href=/people/a/ashima-suvarna/>Ashima Suvarna</a>
|
<a href=/people/x/xiao-liu/>Xiao Liu</a>
|
<a href=/people/t/tanmay-parekh/>Tanmay Parekh</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--76><div class="card-body p-3 small">Question Under Discussion (QUD) is a discourse framework that uses implicit questions to reveal discourse relationships between sentences. In QUD parsing, each sentence is viewed as an answer to a question triggered by an anchor sentence in prior context. The resulting QUD structure is required to conform to several theoretical criteria like answer compatibility(how well the question is answered), making QUD parsing a challenging task. Previous works construct QUD parsers in a pipelined manner (i.e. detect the trigger sentence in context and then generate the question). However, these parsers lack a holistic view of the task and can hardly satisfy all the criteria. In this work, we introduce QUDSELECT, a joint-training framework that selectively decodes the QUD dependency structures considering the QUD criteria criteria. Using instruction-tuning, we train models to simultaneously predict the anchor sentence and generate the associated question. To explicitly incorporate the criteria, we adopt a selective decoding strategy of sampling multiple QUD candidates during inference, followed by selecting the best one with criteria scorers. Our method outperforms the state-of-the-art baseline models by 9% in human evaluation and 4% in automatic evaluation, demonstrating the effectiveness of our framework. Code and data are in https://github.com/asuvarna31/qudselect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.77.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.77/>Mitigating Language Bias of <span class=acl-fixed-case>LMM</span>s in Social Intelligence Understanding with Virtual Counterfactual Calibration</a></strong><br><a href=/people/p/peng-chen/>Peng Chen</a>
|
<a href=/people/x/xiao-yu-guo/>Xiao-Yu Guo</a>
|
<a href=/people/y/yuan-fang-li/>Yuan-Fang Li</a>
|
<a href=/people/x/xiaowang-zhang/>Xiaowang Zhang</a>
|
<a href=/people/z/zhiyong-feng/>Zhiyong Feng</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.78.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.78.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--78 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.78 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.78.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.78/>Model Balancing Helps Low-data Training and Fine-tuning</a></strong><br><a href=/people/z/zihang-liu/>Zihang Liu</a>
|
<a href=/people/y/yuanzhe-hu/>Yuanzhe Hu</a>
|
<a href=/people/t/tianyu-pang/>Tianyu Pang</a>
|
<a href=/people/y/yefan-zhou/>Yefan Zhou</a>
|
<a href=/people/p/pu-ren/>Pu Ren</a>
|
<a href=/people/y/yaoqing-yang/>Yaoqing Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--78><div class="card-body p-3 small">Recent advances in foundation models have emphasized the need to align pre-trained models with specialized domains using small, curated datasets. Studies on these foundation models underscore the importance of low-data training and fine-tuning. This topic, well-known in natural language processing (NLP), has also gained increasing attention in the emerging field of scientific machine learning (SciML). To address the limitations of low-data training and fine-tuning, we draw inspiration from Heavy-Tailed Self-Regularization (HT-SR) theory, analyzing the shape of empirical spectral densities (ESDs) and revealing an imbalance in training quality across different model layers. To mitigate this issue, we adapt a recently proposed layer-wise learning rate scheduler, TempBalance, which effectively balances training quality across layers and enhances low-data training and fine-tuning for both NLP and SciML tasks. Notably, TempBalance demonstrates increasing performance gains as the amount of available tuning data decreases. Comparative analyses further highlight the effectiveness of TempBalance and its adaptability as an “add-on” method for improving model performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.79.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.79/>Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment</a></strong><br><a href=/people/z/zhaofeng-wu/>Zhaofeng Wu</a>
|
<a href=/people/a/ananth-balashankar/>Ananth Balashankar</a>
|
<a href=/people/y/yoon-kim/>Yoon Kim</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a>
|
<a href=/people/a/ahmad-beirami/>Ahmad Beirami</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--79><div class="card-body p-3 small">Aligning language models (LMs) based on human-annotated preference data is a crucial step in obtaining practical and performant LM-based systems. However, multilingual human preference data are difficult to obtain at scale, making it challenging to extend this framework to diverse languages. In this work, we evaluate a simple approach for zero-shot cross-lingual alignment, where a reward model is trained on preference data in one source language and directly applied to other target languages. On summarization and open-ended dialog generation, we show that this method is consistently successful under comprehensive evaluation settings, including human evaluation: cross-lingually aligned models are preferred by humans over unaligned models on up to >70% of evaluation instances. We moreover find that a different-language reward model sometimes yields better aligned models than a same-language reward model. We also identify best practices when there is no language-specific data for even supervised finetuning, another component in alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.80.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.80/>Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment</a></strong><br><a href=/people/k/kun-luo/>Kun Luo</a>
|
<a href=/people/m/minghao-qin/>Minghao Qin</a>
|
<a href=/people/z/zheng-liu/>Zheng Liu</a>
|
<a href=/people/s/shitao-xiao/>Shitao Xiao</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--80><div class="card-body p-3 small">Pre-trained language models like BERT and T5 serve as crucial backbone encoders for dense retrieval. However, these models often exhibit limited generalization capabilities and face challenges in improving in-domain accuracy. Recent research has explored using large language models (LLMs) as retrievers, achieving state-of-the-art performance across various tasks. Despite these advancements, the specific benefits of LLMs over traditional retrievers and the impact of different LLM configurations—such as parameter sizes, pre-training duration, and alignment processes—on retrieval tasks remain unclear. In this work, we conduct a comprehensive empirical study on a wide range of retrieval tasks, including in-domain accuracy, data efficiency, zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. We evaluate over 15 different backbone LLMs and non-LLMs. Our findings reveal that larger models and extensive pre-training consistently enhance in-domain accuracy and data efficiency. Additionally, larger models demonstrate significant potential in zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. These results underscore the advantages of LLMs as versatile and effective backbone encoders in dense retrieval, providing valuable insights for future research and development in this field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.81.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.81.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.81.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.81/>A New Pipeline for Knowledge Graph Reasoning Enhanced by Large Language Models Without Fine-Tuning</a></strong><br><a href=/people/z/zhongwu-chen/>Zhongwu Chen</a>
|
<a href=/people/l/long-bai/>Long Bai</a>
|
<a href=/people/z/zixuan-li/>Zixuan Li</a>
|
<a href=/people/z/zhen-huang/>Zhen Huang</a>
|
<a href=/people/x/xiaolong-jin/>Xiaolong Jin</a>
|
<a href=/people/y/yong-dou/>Yong Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--81><div class="card-body p-3 small">Conventional Knowledge Graph Reasoning (KGR) models learn the embeddings of KG components over the structure of KGs, but their performances are limited when the KGs are severely incomplete. Recent LLM-enhanced KGR models input KG structural information into LLMs. However, they require fine-tuning on open-source LLMs and are not applicable to closed-source LLMs. Therefore, in this paper, to leverage the knowledge in LLMs without fine-tuning to assist and enhance conventional KGR models, we propose a new three-stage pipeline, including knowledge alignment, KG reasoning and entity reranking. Specifically, in the alignment stage, we propose three strategies to align the knowledge in LLMs to the KG schema by explicitly associating unconnected nodes with semantic relations. Based on the enriched KGs, we train structure-aware KGR models to integrate aligned knowledge to original knowledge existing in KGs. In the reranking stage, after obtaining the results of KGR models, we rerank the top-scored entities with LLMs to recall correct answers further. Experiments show our pipeline can enhance the KGR performance in both incomplete and general situations. Code and datasets are available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.82.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.82/>Towards Tool Use Alignment of Large Language Models</a></strong><br><a href=/people/z/zhi-yuan-chen/>Zhi-Yuan Chen</a>
|
<a href=/people/s/shiqi-shen/>Shiqi Shen</a>
|
<a href=/people/g/guangyao-shen/>Guangyao Shen</a>
|
<a href=/people/g/gong-zhi/>Gong Zhi</a>
|
<a href=/people/x/xu-chen/>Xu Chen</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--82><div class="card-body p-3 small">Recently, tool use with LLMs has become one of the primary research topics as it can help LLM generate truthful and helpful responses. Existing studies on tool use with LLMs primarily focus on enhancing the tool-calling ability of LLMs. In practice, like chat assistants, LLMs are also required to align with human values in the context of tool use. Specifically, LLMs should refuse to answer unsafe tool use relevant instructions and insecure tool responses to ensure their reliability and harmlessness. At the same time, LLMs should demonstrate autonomy in tool use to reduce the costs associated with tool calling. To tackle this issue, we first introduce the principle that LLMs should follow in tool use scenarios: H2A. The goal of H2A is to align LLMs with **helpfulness**, **harmlessness**, and **autonomy**. In addition, we propose ToolAlign, a dataset comprising instruction-tuning data and preference data to align LLMs with the H2A principle for tool use. Based on ToolAlign, we develop LLMs by supervised fine-tuning and preference learning, and experimental results demonstrate that the LLMs exhibit remarkable tool-calling capabilities, while also refusing to engage with harmful content, and displaying a high degree of autonomy in tool utilization. The code and datasets are available at: https://github.com/zhiyuanc2001/ToolAlign.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.83.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.83.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.83/><span class=acl-fixed-case>D</span>ecorate<span class=acl-fixed-case>LM</span>: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models</a></strong><br><a href=/people/r/ranchi-zhao/>Ranchi Zhao</a>
|
<a href=/people/z/zhen-leng-thai/>Zhen Leng Thai</a>
|
<a href=/people/y/yifan-zhang/>Yifan Zhang</a>
|
<a href=/people/s/shengding-hu/>Shengding Hu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/y/yunqi-ba/>Yunqi Ba</a>
|
<a href=/people/j/jie-cai/>Jie Cai</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--83><div class="card-body p-3 small">The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the quality of this data is challenging due to its sheer volume and the absence of sample-level quality annotations and enhancements. In this paper, we introduce DecorateLM, a data engineering method designed to refine the pretraining corpus through data rating, tagging and editing. Specifically, DecorateLM rates texts against quality criteria, tags texts with hierarchical labels, and edits texts into a more formalized format. Due to the massive size of the pretraining corpus, adopting an LLM for decorating the entire corpus is less efficient. Therefore, to balance performance with efficiency, we curate a meticulously annotated training corpus for DecorateLM using a large language model and distill data engineering expertise into a compact 1.2 billion parameter small language model (SLM). We then apply DecorateLM to enhance 100 billion tokens of the training corpus, selecting 45 billion tokens that exemplify high quality and diversity for the further training of another 1.2 billion parameter LLM. Our results demonstrate that employing such high-quality data can significantly boost model performance, showcasing a powerful approach to enhance the quality of the pretraining corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.84.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.84.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.84/>Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps</a></strong><br><a href=/people/y/yung-sung-chuang/>Yung-Sung Chuang</a>
|
<a href=/people/l/linlu-qiu/>Linlu Qiu</a>
|
<a href=/people/c/cheng-yu-hsieh/>Cheng-Yu Hsieh</a>
|
<a href=/people/r/ranjay-krishna/>Ranjay Krishna</a>
|
<a href=/people/y/yoon-kim/>Yoon Kim</a>
|
<a href=/people/j/james-glass/>James R. Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--84><div class="card-body p-3 small">When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such **contextual hallucinations**. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these _lookback ratio_ features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector—**Lookback Lens**—is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.85.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.85.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.85/>Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment</a></strong><br><a href=/people/y/yiju-guo/>Yiju Guo</a>
|
<a href=/people/g/ganqu-cui/>Ganqu Cui</a>
|
<a href=/people/l/lifan-yuan/>Lifan Yuan</a>
|
<a href=/people/n/ning-ding/>Ning Ding</a>
|
<a href=/people/z/zexu-sun/>Zexu Sun</a>
|
<a href=/people/b/bowen-sun/>Bowen Sun</a>
|
<a href=/people/h/huimin-chen/>Huimin Chen</a>
|
<a href=/people/r/ruobing-xie/>Ruobing Xie</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--85><div class="card-body p-3 small">Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the ”alignment tax”–a compromise where enhancements in alignment within one objective (e.g., harmlessness) can diminish performance in others (e.g., helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the ”3H” (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving improvements in multi-objective alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.86.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.86/>Mitigating Matthew Effect: Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation</a></strong><br><a href=/people/y/yongsen-zheng/>Yongsen Zheng</a>
|
<a href=/people/r/ruilin-xu/>Ruilin Xu</a>
|
<a href=/people/g/guohua-wang/>Guohua Wang</a>
|
<a href=/people/l/liang-lin/>Liang Lin</a>
|
<a href=/people/k/kwok-yan-lam/>Kwok-Yan Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--86><div class="card-body p-3 small">The Matthew effect is a big challenge in Recommender Systems (RSs), where popular items tend to receive increasing attention, while less popular ones are often overlooked, perpetuating existing disparities. Although many existing methods attempt to mitigate Matthew effect in the static or quasi-static recommendation scenarios, such issue will be more pronounced as users engage with the system over time. To this end, we propose a novel framework, Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation (HiCore), aiming to address Matthew effect in the Conversational Recommender System (CRS) involving the dynamic user-system feedback loop. It devotes to learn multi-level user interests by building a set of hypergraphs (i.e., item-, entity-, word-oriented multiple-channel hypergraphs) to alleviate the Matthew effec. Extensive experiments on four CRS-based datasets showcase that HiCore attains a new state-of-the-art performance, underscoring its superiority in mitigating the Matthew effect effectively. Our code is available at https://github.com/zysensmile/HiCore.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.87.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.87/>Advancing Event Causality Identification via Heuristic Semantic Dependency Inquiry Network</a></strong><br><a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/q/qiang-gao/>Qiang Gao</a>
|
<a href=/people/h/hongmei-wu/>Hongmei Wu</a>
|
<a href=/people/l/li-huang/>Li Huang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.88.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.88/>Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors</a></strong><br><a href=/people/w/wenjian-ding/>Wenjian Ding</a>
|
<a href=/people/y/yao-zhang/>Yao Zhang</a>
|
<a href=/people/j/jun-wang/>Jun Wang</a>
|
<a href=/people/a/adam-jatowt/>Adam Jatowt</a>
|
<a href=/people/z/zhenglu-yang/>Zhenglu Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--88><div class="card-body p-3 small">Multiple-choice visual question answering (VQA) is to automatically choose a correct answer from a set of choices after reading an image. Existing efforts have been devoted to a separate generation of an image-related question, a correct answer, or challenge distractors. By contrast, we turn to a holistic generation and optimization of questions, answers, and distractors (QADs) in this study. This integrated generation strategy eliminates the need for human curation and guarantees information consistency. Furthermore, we first propose to put the spotlight on different image regions to diversify QADs. Accordingly, a novel framework ReBo is formulated in this paper. ReBo cyclically generates each QAD based on a recurrent multimodal encoder, and each generation is focusing on a different area of the image compared to those already concerned by the previously generated QADs. In addition to traditional VQA comparisons with state-of-the-art approaches, we also validate the capability of ReBo in generating augmented data to benefit VQA models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.89.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.89.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.89/><span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>F</span>ashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation</a></strong><br><a href=/people/x/xiangyu-zhao/>Xiangyu Zhao</a>
|
<a href=/people/y/yuehan-zhang/>Yuehan Zhang</a>
|
<a href=/people/w/wenlong-zhang/>Wenlong Zhang</a>
|
<a href=/people/x/xiao-ming-wu/>Xiao-Ming Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--89><div class="card-body p-3 small">The fashion domain encompasses a variety of real-world multimodal tasks, including multimodal retrieval and multimodal generation. The rapid advancements in artificial intelligence generated content, particularly in technologies like large language models for text generation and diffusion models for visual generation, have sparked widespread research interest in applying these multimodal models in the fashion domain. However, tasks that use embeddings, such as image-to-text or text-to-image retrieval, have been largely ignored from this perspective due to the diverse nature of the multimodal fashion domain. And current research on multi-task single models lack focus on image generation. In this work, we present UniFashion, a unified framework that simultaneously tackles the challenges of multimodal generation and retrieval tasks within the fashion domain, integrating image generation with retrieval tasks and text generation tasks. UniFashion unifies embedding and generative tasks by integrating a diffusion model and LLM, enabling controllable and high-fidelity generation. Our model significantly outperforms previous single-task state-of-the-art models across diverse fashion tasks, and can be readily adapted to manage complex vision-language tasks. This work demonstrates the potential learning synergy between multimodal generation and retrieval, offering a promising direction for future research in the fashion domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.90.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.90/>Tracking the perspectives of interacting language models</a></strong><br><a href=/people/h/hayden-helm/>Hayden Helm</a>
|
<a href=/people/b/brandon-duderstadt/>Brandon Duderstadt</a>
|
<a href=/people/y/youngser-park/>Youngser Park</a>
|
<a href=/people/c/carey-priebe/>Carey Priebe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--90><div class="card-body p-3 small">Large language models (LLMs) are capable of producing high quality information at unprecedented rates. As these models continue to entrench themselves in society, the content they produce will become increasingly pervasive in databases that are, in turn, incorporated into the pre-training data, fine-tuning data, retrieval data, etc. of other language models. In this paper we formalize the idea of a communication network of LLMs and introduce a method for representing the perspective of individual models within a collection of LLMs. Given these tools we systematically study information diffusion in the communication network of LLMs in various simulated settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.91.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.91/><span class=acl-fixed-case>MAR</span>: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering</a></strong><br><a href=/people/z/zhengxuan-zhang/>Zhengxuan Zhang</a>
|
<a href=/people/y/yin-wu/>Yin Wu</a>
|
<a href=/people/y/yuyu-luo/>Yuyu Luo</a>
|
<a href=/people/n/nan-tang/>Nan Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--91><div class="card-body p-3 small">A multimodal large language model MLLMs may struggle with answering visual-based (personal) entity questions (VEQA), such as ”who is A?” or ”who is A that B is talking to?” for various reasons, e.g., the absence of the name of A in the caption or the inability of MLLMs to recognize A, particularly for less common entities. Furthermore, even if the MLLMs can identify A, it may refrain from answering due to privacy concerns. In this paper, we introduce a novel method called Matching-Augmented Reasoning (MAR) to enhance VEQA. Given a collection of visual objects with captions, MAR preprocesses each object individually, identifying faces, names, and their alignments within the object. It encodes this information and stores their vector representations in vector databases. When handling VEQA, MAR retrieves matching faces and names and organizes these entities into a matching graph. MAR then derives the answer to the query by reasoning over this matching graph. Extensive experiments show that MAR significantly improves VEQA compared with the state-of-the-art methods using MLLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.92.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.92/>Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?</a></strong><br><a href=/people/z/zhe-yang/>Zhe Yang</a>
|
<a href=/people/y/yichang-zhang/>Yichang Zhang</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/j/jian-yang/>Jian Yang</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/c/chang-zhou/>Chang Zhou</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--92><div class="card-body p-3 small">Large language models (LLMs) have demonstrated impressive capabilities, but still suffer from inconsistency issues (e.g. LLMs can react differently to disturbances like rephrasing or inconsequential order change). In addition to these inconsistencies, we also observe that LLMs, while capable of solving hard problems, can paradoxically fail at easier ones. To evaluate this hard-to-easy inconsistency, we develop the ConsisEval benchmark, where each entry comprises a pair of questions with a strict order of difficulty. Furthermore, we introduce the concept of consistency score to quantitatively measure this inconsistency and analyze the potential for improvement in consistency by relative consistency score. Based on comprehensive experiments across a variety of existing models, we find: (1) GPT-4 achieves the highest consistency score of 92.2% but is still inconsistent to specific questions due to distraction by redundant information, misinterpretation of questions, etc.; (2) models with stronger capabilities typically exhibit higher consistency, but exceptions also exist; (3) hard data enhances consistency for both fine-tuning and in-context learning. Our data and code will be publicly available on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.93.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.93/>Watch Every Step! <span class=acl-fixed-case>LLM</span> Agent Learning via Iterative Step-level Process Refinement</a></strong><br><a href=/people/w/weimin-xiong/>Weimin Xiong</a>
|
<a href=/people/y/yifan-song/>Yifan Song</a>
|
<a href=/people/x/xiutian-zhao/>Xiutian Zhao</a>
|
<a href=/people/w/wenhao-wu/>Wenhao Wu</a>
|
<a href=/people/x/xun-wang/>Xun Wang</a>
|
<a href=/people/k/ke-wang/>Ke Wang</a>
|
<a href=/people/c/cheng-li/>Cheng Li</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--93><div class="card-body p-3 small">Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the **I**terative step-level **P**rocess **R**efinement **(IPR)** framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical finds highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.94.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.94.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.94.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.94/>Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation</a></strong><br><a href=/people/j/joseph-marvin-imperial/>Joseph Marvin Imperial</a>
|
<a href=/people/g/gail-forey/>Gail Forey</a>
|
<a href=/people/h/harish-tayyar-madabushi/>Harish Tayyar Madabushi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--94><div class="card-body p-3 small">Domain experts across engineering, healthcare, and education follow strict standards for producing quality content such as technical manuals, medication instructions, and children’s reading materials. However, current works in controllable text generation have yet to explore using these standards as references for control. Towards this end, we introduce Standardize, a retrieval-style in-context learning-based framework to guide large language models to align with expert-defined standards. Focusing on English language standards in the education domain as a use case, we consider the Common European Framework of Reference for Languages (CEFR) and Common Core Standards (CCS) for the task of open-ended content generation. Our findings show that models can gain 45% to 100% increase in precise accuracy across open and commercial LLMs evaluated, demonstrating that the use of knowledge artifacts extracted from standards and integrating them in the generation process can effectively guide models to produce better standard-aligned content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.95.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.95/>Cross-domain <span class=acl-fixed-case>NER</span> with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective</a></strong><br><a href=/people/z/zhihao-zhang/>Zhihao Zhang</a>
|
<a href=/people/s/sophia-yat-mei-lee/>Sophia Yat Mei Lee</a>
|
<a href=/people/j/junshuang-wu/>Junshuang Wu</a>
|
<a href=/people/d/dong-zhang/>Dong Zhang</a>
|
<a href=/people/s/shoushan-li/>Shoushan Li</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--95><div class="card-body p-3 small">Cross-domain Named Entity Recognition (CDNER) is crucial for Knowledge Graph (KG) construction and natural language processing (NLP), enabling learning from source to target domains with limited data. Previous studies often rely on manually collected entity-relevant sentences from the web or attempt to bridge the gap between tokens and entity labels across domains. These approaches are time-consuming and inefficient, as these data are often weakly correlated with the target task and require extensive pre-training.To address these issues, we propose automatically generating task-oriented knowledge (GTOK) using large language models (LLMs), focusing on the reasoning process of entity extraction. Then, we employ task-oriented pre-training (TOPT) to facilitate domain adaptation. Additionally, current cross-domain NER methods often lack explicit explanations for their effectiveness. Therefore, we introduce the concept of information density to better evaluate the model’s effectiveness before performing entity recognition.We conduct systematic experiments and analyses to demonstrate the effectiveness of our proposed approach and the validity of using information density for model evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.96.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.96.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--96 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.96 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.96/>Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models</a></strong><br><a href=/people/z/zhen-tan/>Zhen Tan</a>
|
<a href=/people/c/chengshuai-zhao/>Chengshuai Zhao</a>
|
<a href=/people/r/raha-moraffah/>Raha Moraffah</a>
|
<a href=/people/y/yifan-li/>Yifan Li</a>
|
<a href=/people/s/song-wang/>Song Wang</a>
|
<a href=/people/j/jundong-li/>Jundong Li</a>
|
<a href=/people/t/tianlong-chen/>Tianlong Chen</a>
|
<a href=/people/h/huan-liu/>Huan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--96><div class="card-body p-3 small">Retrieval-Augmented Generative (RAG) models enhance Large Language Models (LLMs) by integrating external knowledge bases, improving their performance in applications like fact-checking and information searching. In this paper, we demonstrate a security threat where adversaries can exploit the openness of these knowledge bases by injecting deceptive content into the retrieval database, intentionally changing the model’s behavior. This threat is critical as it mirrors real-world usage scenarios where RAG systems interact with publicly accessible knowledge bases, such as web scrapings and user-contributed data pools. To be more realistic, we target a realistic setting where the adversary has no knowledge of users’ queries, knowledge base data, and the LLM parameters. We demonstrate that it is possible to exploit the model successfully through crafted content uploads with access to the retriever. Our findings emphasize an urgent need for security measures in the design and deployment of RAG systems to prevent potential manipulation and ensure the integrity of machine-generated content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.97.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.97.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--97 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.97 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.97/>Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement</a></strong><br><a href=/people/y/yuxuan-wang/>Yuxuan Wang</a>
|
<a href=/people/x/xiaoyuan-liu/>Xiaoyuan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--97><div class="card-body p-3 small">Scene Graph Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between objects. This complexity and diversity in SGG leads to underrepresentation, where parts of triplet labels are rare or even unseen during training, resulting in imprecise predictions. To tackle this, we propose integrating the pretrained Vision-language Models to enhance representation. However, due to the gap between pretraining and SGG, direct inference of pretrained VLMs on SGG leads to severe bias, which stems from the imbalanced predicates distribution in the pretraining language set. To alleviate the bias, we introduce a novel LM Estimation to approximate the unattainable predicates distribution. Finally, we ensemble the debiased VLMs with SGG models to enhance the representation, where we design a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights. Our training-free method effectively addresses the predicates bias in pretrained VLMs, enhances SGG’s representation, and significantly improve the performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.98.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.98.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.98.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.98/><span class=acl-fixed-case>SHIELD</span>: Evaluation and Defense Strategies for Copyright Compliance in <span class=acl-fixed-case>LLM</span> Text Generation</a></strong><br><a href=/people/x/xiaoze-liu/>Xiaoze Liu</a>
|
<a href=/people/t/ting-sun/>Ting Sun</a>
|
<a href=/people/t/tianyang-xu/>Tianyang Xu</a>
|
<a href=/people/f/feijie-wu/>Feijie Wu</a>
|
<a href=/people/c/cunxiang-wang/>Cunxiang Wang</a>
|
<a href=/people/x/xiaoqian-wang/>Xiaoqian Wang</a>
|
<a href=/people/j/jing-gao/>Jing Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--98><div class="card-body p-3 small">Large Language Models (LLMs) have transformed machine learning but raised significant legal concerns due to their potential to produce text that infringes on copyrights, resulting in several high-profile lawsuits. The legal landscape is struggling to keep pace with these rapid advancements, with ongoing debates about whether generated text might plagiarize copyrighted materials. Current LLMs may infringe on copyrights or overly restrict non-copyrighted texts, leading to these challenges: (i) the need for a comprehensive evaluation benchmark to assess copyright compliance from multiple aspects; (ii) evaluating robustness against safeguard bypassing attacks; and (iii) developing effective defenses targeted against the generation of copyrighted text.To tackle these challenges, we introduce a curated dataset to evaluate methods, test attack strategies, and propose a lightweight, real-time defense mechanism to prevent the generation of copyrighted text, ensuring the safe and lawful use of LLMs. Our experiments demonstrate that current LLMs frequently output copyrighted text, and that jailbreaking attacks can significantly increase the volume of copyrighted output. Our proposed defense mechanism substantially reduces the volume of copyrighted text generated by LLMs by effectively refusing malicious requests.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.99.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.99/><span class=acl-fixed-case>M</span>atch<span class=acl-fixed-case>T</span>ime: Towards Automatic Soccer Game Commentary Generation</a></strong><br><a href=/people/j/jiayuan-rao/>Jiayuan Rao</a>
|
<a href=/people/h/haoning-wu/>Haoning Wu</a>
|
<a href=/people/c/chang-liu/>Chang Liu</a>
|
<a href=/people/y/yanfeng-wang/>Yanfeng Wang</a>
|
<a href=/people/w/weidi-xie/>Weidi Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--99><div class="card-body p-3 small">Soccer is a globally popular sport with a vast audience, in this paper, we consider constructing an automatic soccer game commentary model to improve the audiences’ viewing experience. In general, we make the following contributions: *First*, observing the prevalent video-text misalignment in existing datasets, we manually annotate timestamps for 49 matches, establishing a more robust benchmark for soccer game commentary generation, termed as *SN-Caption-test-align*; *Second*, we propose a multi-modal temporal alignment pipeline to automatically correct and filter the existing dataset at scale, creating a higher-quality soccer game commentary dataset for training, denoted as *MatchTime*; *Third*, based on our curated dataset, we train an automatic commentary generation model, named **MatchVoice**. Extensive experiments and ablation studies have demonstrated the effectiveness of our alignment pipeline, and training model on the curated datasets achieves state-of-the-art performance for commentary generation, showcasing that better alignment can lead to significant performance improvements in downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.100.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--100 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.100.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.100/>Rethinking Token Reduction for State Space Models</a></strong><br><a href=/people/z/zheng-zhan/>Zheng Zhan</a>
|
<a href=/people/y/yushu-wu/>Yushu Wu</a>
|
<a href=/people/z/zhenglun-kong/>Zhenglun Kong</a>
|
<a href=/people/c/changdi-yang/>Changdi Yang</a>
|
<a href=/people/y/yifan-gong/>Yifan Gong</a>
|
<a href=/people/x/xuan-shen/>Xuan Shen</a>
|
<a href=/people/x/xue-lin/>Xue Lin</a>
|
<a href=/people/p/pu-zhao/>Pu Zhao</a>
|
<a href=/people/y/yanzhi-wang/>Yanzhi Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--100><div class="card-body p-3 small">Recent advancements in State Space Models (SSMs) have attracted significant interest, particularly in models optimized for parallel training and handling long-range dependencies. Architectures like Mamba have scaled to billions of parameters with selective SSM. To facilitate broader applications using Mamba, exploring its efficiency is crucial. While token reduction techniques offer a straightforward post-training strategy, we find that applying existing methods directly to SSMs leads to substantial performance drops. Through insightful analysis, we identify the reasons for this failure and the limitations of current techniques. In response, we propose a tailored, unified post-training token reduction method for SSMs. Our approach integrates token importance and similarity, thus taking advantage of both pruning and merging, to devise a fine-grained intra-layer token reduction strategy. Extensive experiments show that our method improves the average accuracy by 5.7% to 13.1% on six benchmarks with Mamba-2 compared to existing methods, while significantly reducing computational demands and memory requirements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.101.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.101/>Triad: A Framework Leveraging a Multi-Role <span class=acl-fixed-case>LLM</span>-based Agent to Solve Knowledge Base Question Answering</a></strong><br><a href=/people/c/chang-zong/>Chang Zong</a>
|
<a href=/people/y/yuchen-yan/>Yuchen Yan</a>
|
<a href=/people/w/weiming-lu/>Weiming Lu</a>
|
<a href=/people/j/jian-shao/>Jian Shao</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/h/heng-chang/>Heng Chang</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--101><div class="card-body p-3 small">Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with multiple roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent’s multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.102.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.102/><span class=acl-fixed-case>M</span>eta<span class=acl-fixed-case>GPT</span>: Merging Large Language Models Using Model Exclusive Task Arithmetic</a></strong><br><a href=/people/y/yuyan-zhou/>Yuyan Zhou</a>
|
<a href=/people/l/liang-song/>Liang Song</a>
|
<a href=/people/b/bingning-wang/>Bingning Wang</a>
|
<a href=/people/w/weipeng-chen/>Weipeng Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--102><div class="card-body p-3 small">The advent of large language models (LLMs) like GPT-4 has catalyzed the exploration of multi-task learning (MTL), in which a single model demonstrates proficiency across diverse tasks. Task arithmetic has emerged as a cost-effective approach for MTL. It enables performance enhancement across multiple tasks by adding their corresponding task vectors to a pre-trained model. However, the current lack of a method that can simultaneously achieve optimal performance, computational efficiency, and data privacy limits their application to LLMs. In this paper, we propose <b>M</b>odel <b>E</b>xclusive <b>T</b>ask <b>A</b>rithmetic for merging <b>GPT</b>-scale models (MetaGPT) which formalizes the objective of model merging into a multi-task learning framework, aiming to minimize the average loss difference between the merged model and each individual task model. Since data privacy limits the use of multi-task training data, we leverage LLMs’ local linearity and task vectors’ orthogonality to separate the data term and scaling coefficients term and derive a model-exclusive task arithmetic method. Our proposed MetaGPT is data-agnostic and bypasses the heavy search process, making it cost-effective and easy to implement for LLMs. Extensive experiments demonstrate that MetaGPT leads to improvement of task arithmetic and achieves state-of-the-art performance on multiple tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.103.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.103/>Event Causality Identification with Synthetic Control</a></strong><br><a href=/people/h/haoyu-wang/>Haoyu Wang</a>
|
<a href=/people/f/fengze-liu/>Fengze Liu</a>
|
<a href=/people/j/jiayao-zhang/>Jiayao Zhang</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--103><div class="card-body p-3 small">Event causality identification (ECI), a process that extracts causal relations between events from text, is crucial for distinguishing causation from correlation. Traditional approaches to ECI have primarily utilized linguistic patterns and multi-hop relational inference, risking false causality identification due to informal usage of causality and specious graphical inference. In this paper, we adopt the Rubin Causal Model to identify event causality: given two temporally ordered events, we see the first event as the treatment and the second one as the observed outcome. Determining their causality involves manipulating the treatment and estimating the resultant change in the likelihood of the outcome. Given that it is only possible to implement manipulation conceptually in the text domain, as a work-around, we try to find a twin for the protagonist from existing corpora. This twin should have identical life experiences with the protagonist before the treatment but undergoes an intervention of treatment. However, the practical difficulty of locating such a match limits its feasibility. Addressing this issue, we use the synthetic control method to generate such a twin’ from relevant historical data, leveraging text embedding synthesis and inversion techniques. This approach allows us to identify causal relations more robustly than previous methods, including GPT-4, which is demonstrated on a causality benchmark, COPES-hard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.104.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.104/>Retrieved Sequence Augmentation for Protein Representation Learning</a></strong><br><a href=/people/c/chang-ma/>Chang Ma</a>
|
<a href=/people/h/haiteng-zhao/>Haiteng Zhao</a>
|
<a href=/people/l/lin-zheng/>Lin Zheng</a>
|
<a href=/people/j/jiayi-xin/>Jiayi Xin</a>
|
<a href=/people/q/qintong-li/>Qintong Li</a>
|
<a href=/people/l/lijun-wu/>Lijun Wu</a>
|
<a href=/people/z/zhi-hong-deng/>Zhihong Deng</a>
|
<a href=/people/y/yang-young-lu/>Yang Young Lu</a>
|
<a href=/people/q/qi-liu/>Qi Liu</a>
|
<a href=/people/s/sheng-wang/>Sheng Wang</a>
|
<a href=/people/l/lingpeng-kong/>Lingpeng Kong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--104><div class="card-body p-3 small">Protein Language Models traditionally depend on Multiple Sequence Alignments (MSA) to incorporate evolutionary knowledge. However, MSA-based approaches suffer from substantial computational overhead and generally underperform in generalizing to de novo proteins. This study reevaluates the role of MSA, proposing it as a retrieval augmentation method and questioning the necessity of sequence alignment. We show that a simple alternative, Retrieved Sequence Augmentation (RSA), can enhance protein representation learning without the need for alignment and cumbersome preprocessing. RSA surpasses MSA Transformer by an average of 5% in both structural and property prediction tasks while being 373 times faster. Additionally, RSA demonstrates enhanced transferability for predicting de novo proteins. This methodology addresses a critical need for efficiency in protein prediction and can be rapidly employed to identify homologous sequences, improve representation learning, and enhance the capacity of Large Language Models to interpret protein structures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.105.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--105 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.105/><span class=acl-fixed-case>HELPD</span>: Mitigating Hallucination of <span class=acl-fixed-case>LVLM</span>s by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding</a></strong><br><a href=/people/f/fan-yuan/>Fan Yuan</a>
|
<a href=/people/c/chi-qin/>Chi Qin</a>
|
<a href=/people/x/xiaogang-xu/>Xiaogang Xu</a>
|
<a href=/people/p/piji-li/>Piji Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--105><div class="card-body p-3 small">Large Vision-Language Models (LVLMs) have shown remarkable performance on many visual-language tasks. However, these models still suffer from <i>multimodal hallucination</i>, which means the generation of objects or content that violates the images. Many existing work detects hallucination by directly judging whether an object exists in an image, overlooking the association between the object and semantics. To address this issue, we propose Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding (HELPD). This framework incorporates hallucination feedback at both object and sentence semantic levels. Remarkably, even with a marginal degree of training, this approach can alleviate over 15% of hallucination. Simultaneously, HELPD penalizes the output logits according to the image attention window to avoid being overly affected by generated text. HELPD can be seamlessly integrated with any LVLMs. Our experiments demonstrate that the proposed framework yields favorable results across multiple hallucination benchmarks. It effectively mitigates hallucination for different LVLMs and concurrently improves their text generation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.106.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--106 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.106/><span class=acl-fixed-case>T</span>op<span class=acl-fixed-case>V</span>iew<span class=acl-fixed-case>RS</span>: Vision-Language Models as Top-View Spatial Reasoners</a></strong><br><a href=/people/c/chengzu-li/>Chengzu Li</a>
|
<a href=/people/c/caiqi-zhang/>Caiqi Zhang</a>
|
<a href=/people/h/han-zhou/>Han Zhou</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--106><div class="card-body p-3 small">Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of ‘non-human’ agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs in this setup remain unattested and underexplored. In this work, we study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.107.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--107 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.107.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.107/><span class=acl-fixed-case>DA</span><span class=tex-math><sup>3</sup></span>: A Distribution-Aware Adversarial Attack against Language Models</a></strong><br><a href=/people/y/yibo-wang/>Yibo Wang</a>
|
<a href=/people/x/xiangjue-dong/>Xiangjue Dong</a>
|
<a href=/people/j/james-caverlee/>James Caverlee</a>
|
<a href=/people/p/philip-s-yu/>Philip S. Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--107><div class="card-body p-3 small">Language models can be manipulated by adversarial attacks, which introduce subtle perturbations to input data. While recent attack methods can achieve a relatively high attack success rate (ASR), we’ve observed that the generated adversarial examples have a different data distribution compared with the original examples. Specifically, these adversarial examples exhibit reduced confidence levels and greater divergence from the training data distribution. Consequently, they are easy to detect using straightforward detection methods, diminishing the efficacy of such attacks. To address this issue, we propose a Distribution-Aware Adversarial Attack (DA<span class=tex-math><sup>3</sup></span>) method. DA<span class=tex-math><sup>3</sup></span> considers the distribution shifts of adversarial examples to improve attacks’ effectiveness under detection methods. We further design a novel evaluation metric, the Non-detectable Attack Success Rate (NASR), which integrates both ASR and detectability for the attack task. We conduct experiments on four widely used datasets to validate the attack effectiveness and transferability of adversarial examples generated by DA<span class=tex-math><sup>3</sup></span> against both the white-box BERT-base and RoBERTa-base models and the black-box LLaMA2-7b model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.108.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.108/>Evaluating Psychological Safety of Large Language Models</a></strong><br><a href=/people/x/xingxuan-li/>Xingxuan Li</a>
|
<a href=/people/y/yutong-li/>Yutong Li</a>
|
<a href=/people/l/lin-qiu/>Lin Qiu</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--108><div class="card-body p-3 small">In this work, we designed unbiased prompts to systematically evaluate the psychological safety of large language models (LLMs). First, we tested five different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT models. Following these observations, we showed that fine-tuning Llama-2-chat-7B with responses from BFI using direct preference optimization could effectively reduce the psychological toxicity of the model. Based on the findings, we recommended the application of systematic and comprehensive psychological metrics to further evaluate and improve the safety of LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.109.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.109/>An Effective Deployment of Diffusion <span class=acl-fixed-case>LM</span> for Data Augmentation in Low-Resource Sentiment Classification</a></strong><br><a href=/people/z/zhuowei-chen/>Zhuowei Chen</a>
|
<a href=/people/l/lianxi-wang/>Lianxi Wang</a>
|
<a href=/people/y/yuben-wu/>Yuben Wu</a>
|
<a href=/people/x/xinfeng-liao/>Xinfeng Liao</a>
|
<a href=/people/y/yujia-tian/>Yujia Tian</a>
|
<a href=/people/j/junyang-zhong/>Junyang Zhong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--109><div class="card-body p-3 small">Sentiment classification (SC) often suffers from low-resource challenges such as domain-specific contexts, imbalanced label distributions, and few-shot scenarios. The potential of the diffusion language model (LM) for textual data augmentation (DA) remains unexplored, moreover, textual DA methods struggle to balance the diversity and consistency of new samples. Most DA methods either perform logical modifications or rephrase less important tokens in the original sequence with the language model. In the context of SC, strong emotional tokens could act critically on the sentiment of the whole sequence. Therefore, contrary to rephrasing less important context, we propose DiffusionCLS to leverage a diffusion LM to capture in-domain knowledge and generate pseudo samples by reconstructing strong label-related tokens. This approach ensures a balance between consistency and diversity, avoiding the introduction of noise and augmenting crucial features of datasets. DiffusionCLS also comprises a Noise-Resistant Training objective to help the model generalize. Experiments demonstrate the effectiveness of our method in various low-resource scenarios including domain-specific and domain-general problems. Ablation studies confirm the effectiveness of our framework’s modules, and visualization studies highlight optimal deployment conditions, reinforcing our conclusions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.110.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.110/>Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering</a></strong><br><a href=/people/d/dongze-hao/>Dongze Hao</a>
|
<a href=/people/q/qunbo-wang/>Qunbo Wang</a>
|
<a href=/people/l/longteng-guo/>Longteng Guo</a>
|
<a href=/people/j/jie-jiang/>Jie Jiang</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--110><div class="card-body p-3 small">While large pre-trained visual-language models have shown promising results on traditional visual question answering benchmarks, it is still challenging for them to answer complex VQA problems which requires diverse world knowledge. Motivated by the research of retrieval-augmented generation in the field of natural language processing, we use Dense Passage Retrieval (DPR) to retrieve related knowledge to help the model answer questions. However, DPR conduct retrieving in natural language space, which may not ensure comprehensive acquisition of image information. Thus, the retrieved knowledge is not truly conducive to helping answer the question, affecting the performance of the overall system. To address this issue, we propose a novel framework that leverages the visual-language model to select the key knowledge retrieved by DPR and answer questions. The framework consists of two modules: Selector and Answerer, where both are initialized by the MLLM and parameter-efficiently finetuned by self-bootstrapping: find key knowledge in the retrieved knowledge documents using the Selector, and then use them to finetune the Answerer to predict answers; obtain the pseudo-labels of key knowledge documents based on the predictions of the Answerer and weak supervision labels, and then finetune the Selector to select key knowledge; repeat. Our framework significantly enhances the performance of the baseline on the challenging open-domain Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of 62.83%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.111.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.111/><span class=acl-fixed-case>P</span>s<span class=acl-fixed-case>F</span>uture: A Pseudo-Future-based Zero-Shot Adaptive Policy for Simultaneous Machine Translation</a></strong><br><a href=/people/l/libo-zhao/>Libo Zhao</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/z/ziqian-zeng/>Ziqian Zeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--111><div class="card-body p-3 small">Simultaneous Machine Translation (SiMT) requires target tokens to be generated in real-time as streaming source tokens are consumed. Traditional approaches to SiMT typically require sophisticated architectures and extensive parameter configurations for training adaptive read/write policies, which in turn demand considerable computational power and memory. We propose PsFuture, the first zero-shot adaptive read/write policy for SiMT, enabling the translation model to independently determine read/write actions without the necessity for additional training. Furthermore, we introduce a novel training strategy, Prefix-to-Full (P2F), specifically tailored to adjust offline translation models for SiMT applications, exploiting the advantages of the bidirectional attention mechanism inherent in offline models. Experiments across multiple benchmarks demonstrate that our zero-shot policy attains performance on par with strong baselines and the P2F method can further enhance performance, achieving an outstanding trade-off between translation quality and latency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.112.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--112 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.112/><span class=acl-fixed-case>T</span>iny<span class=acl-fixed-case>C</span>hart: Efficient Chart Understanding with Program-of-Thoughts Learning and Visual Token Merging</a></strong><br><a href=/people/l/liang-zhang/>Liang Zhang</a>
|
<a href=/people/a/anwen-hu/>Anwen Hu</a>
|
<a href=/people/h/haiyang-xu/>Haiyang Xu</a>
|
<a href=/people/m/ming-yan/>Ming Yan</a>
|
<a href=/people/y/yichen-xu/>Yichen Xu</a>
|
<a href=/people/q/qin-jin/>Qin Jin</a>
|
<a href=/people/j/ji-zhang/>Ji Zhang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--112><div class="card-body p-3 small">Charts are important for presenting and explaining complex data relationships. Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in chart understanding. However, the sheer size of these models limits their use in resource-constrained environments. In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters. TinyChart overcomes two key challenges in efficient chart understanding: (1) reduce the burden of learning numerical computations through Program-of-Thoughts (PoT) learning, which trains the model to generate Python programs for numerical calculations, and (2) reduce lengthy vision feature sequences through Vision Token Merging, which gradually merges most similar vision tokens. Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on various chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart-understanding MLLMs with up to 13B parameters, and close-sourced MLLM GPT-4V on ChartQA, with higher throughput during inference due to a smaller model scale and more efficient vision encoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.113.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--113 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.113/>Do We Need Language-Specific Fact-Checking Models? The Case of <span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/c/caiqi-zhang/>Caiqi Zhang</a>
|
<a href=/people/z/zhijiang-guo/>Zhijiang Guo</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--113><div class="card-body p-3 small">This paper investigates the potential benefits of language-specific fact-checking models, focusing on the case of Chinese using CHEF dataset. To better reflect real-world fact-checking, we first develop a novel Chinese document-level evidence retriever, achieving state-of-the-art performance. We then demonstrate the limitations of translation-based methods and multilingual language models, highlighting the need for language-specific systems. To better analyze token-level biases in different systems, we construct an adversarial dataset based on the CHEF dataset, where each instance has a large word overlap with the original one but holds the opposite veracity label. Experimental results on the CHEF dataset and our adversarial dataset show that our proposed method outperforms translation-based methods and multilingual language models and is more robust toward biases, emphasizing the importance of language-specific fact-checking systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.114.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.114.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.114/>Enhancing Advanced Visual Reasoning Ability of Large Language Models</a></strong><br><a href=/people/z/zhiyuan-li/>Zhiyuan Li</a>
|
<a href=/people/d/dongnan-liu/>Dongnan Liu</a>
|
<a href=/people/c/chaoyi-zhang/>Chaoyi Zhang</a>
|
<a href=/people/h/heng-wang/>Heng Wang</a>
|
<a href=/people/t/tengfei-xue/>Tengfei Xue</a>
|
<a href=/people/w/weidong-cai/>Weidong Cai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--114><div class="card-body p-3 small">Recent advancements in Vision-Language (VL) research have sparked new benchmarks for complex visual reasoning, challenging models’ advanced reasoning ability. Traditional Vision-Language models (VLMs) perform well in visual perception tasks while struggling with complex reasoning scenarios. Conversely, Large Language Models (LLMs) demonstrate robust text reasoning capabilities; however, they lack visual acuity. To bridge this gap, we propose **C**omplex **V**isual **R**easoning **L**arge **L**anguage **M**odels (**CVR-LLM**), capitalizing on VLMs’ visual perception proficiency and LLMs’ extensive reasoning capability. Unlike recent multimodal large language models (MLLMs) that require a projection layer, our approach transforms images into detailed, context-aware descriptions using an iterative self-refinement loop and leverages LLMs’ text knowledge for accurate predictions without extra training. We also introduce a novel multi-modal in-context learning (ICL) methodology to enhance LLMs’ contextual understanding and reasoning. Additionally, we introduce Chain-of-Comparison (CoC), a step-by-step comparison technique enabling contrasting various aspects of predictions. Our CVR-LLM presents the first comprehensive study across a wide array of complex visual reasoning tasks and achieves SOTA performance among all.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.115.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.115/><span class=acl-fixed-case>CMD</span>: a framework for Context-aware Model self-Detoxification</a></strong><br><a href=/people/z/zecheng-tang/>Zecheng Tang</a>
|
<a href=/people/k/keyan-zhou/>Keyan Zhou</a>
|
<a href=/people/j/juntao-li/>Juntao Li</a>
|
<a href=/people/y/yuyang-ding/>Yuyang Ding</a>
|
<a href=/people/p/pinzheng-wang/>Pinzheng Wang</a>
|
<a href=/people/y/yan-bowen/>Yan Bowen</a>
|
<a href=/people/r/renjie-hua/>Renjie Hua</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--115><div class="card-body p-3 small">Text detoxification aims to minimize the risk of language models producing toxic content. Existing detoxification methods of directly constraining the model output or further training the model on the non-toxic corpus fail to achieve a decent balance between detoxification effectiveness and generation quality. This issue stems from the neglect of constrain imposed by the context since language models are designed to generate output that closely matches the context while detoxification methods endeavor to ensure the safety of the output even if it semantically deviates from the context. In view of this, we introduce a Context-aware Model self-Detoxification (CMD) framework that pays attention to both the context and the detoxification process, i.e., first detoxifying the context and then making the language model generate along the safe context. Specifically, CMD framework involves two phases: utilizing language models to synthesize data and applying these data for training. We also introduce a toxic contrastive loss that encourages the model generation away from the negative toxic samples. Experiments on various LLMs have verified the effectiveness of our MSD framework, which can yield the best performance compared to baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.116.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.116/>Embedding and Gradient Say Wrong: A White-Box Method for Hallucination Detection</a></strong><br><a href=/people/x/xiaomeng-hu/>Xiaomeng Hu</a>
|
<a href=/people/y/yiming-zhang/>Yiming Zhang</a>
|
<a href=/people/r/ru-peng/>Ru Peng</a>
|
<a href=/people/h/haozhe-zhang/>Haozhe Zhang</a>
|
<a href=/people/c/chenwei-wu/>Chenwei Wu</a>
|
<a href=/people/g/gang-chen/>Gang Chen</a>
|
<a href=/people/j/junbo-zhao/>Junbo Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--116><div class="card-body p-3 small">In recent years, large language models (LLMs) have achieved remarkable success in the field of natural language generation. Compared to previous small-scale models, they are capable of generating fluent output based on the provided prefix or prompt. However, one critical challenge — the *hallucination* problem — remains to be resolved. Generally, the community refers to the undetected hallucination scenario where the LLMs generate text unrelated to the input text or facts. In this study, we intend to model the distributional distance between the regular conditional output and the unconditional output, which is generated without a given input text. Based upon Taylor Expansion for this distance at the output probability space, our approach manages to leverage the embedding and first-order gradient information. The resulting approach is plug-and-play that can be easily adapted to any autoregressive LLM. On the hallucination benchmarks HADES and other datasets, our approach achieves state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.117.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--117 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.117.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.117/><span class=acl-fixed-case>TCS</span>inger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control</a></strong><br><a href=/people/y/yu-zhang/>Yu Zhang</a>
|
<a href=/people/z/ziyue-jiang/>Ziyue Jiang</a>
|
<a href=/people/r/ruiqi-li/>Ruiqi Li</a>
|
<a href=/people/c/changhao-pan/>Changhao Pan</a>
|
<a href=/people/j/jinzheng-he/>Jinzheng He</a>
|
<a href=/people/r/rongjie-huang/>Rongjie Huang</a>
|
<a href=/people/c/chuxin-wang/>Chuxin Wang</a>
|
<a href=/people/z/zhou-zhao/>Zhou Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--117><div class="card-body p-3 small">Zero-shot singing voice synthesis (SVS) with style transfer and style control aims to generate high-quality singing voices with unseen timbres and styles (including singing method, emotion, rhythm, technique, and pronunciation) from audio and text prompts. However, the multifaceted nature of singing styles poses a significant challenge for effective modeling, transfer, and control. Furthermore, current SVS models often fail to generate singing voices rich in stylistic nuances for unseen singers. To address these challenges, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control. Specifically, TCSinger proposes three primary modules: 1) the clustering style encoder employs a clustering vector quantization model to stably condense style information into a compact latent space; 2) the Style and Duration Language Model (S&amp;D-LM) concurrently predicts style information and phoneme duration, which benefits both; 3) the style adaptive decoder uses a novel mel-style adaptive normalization method to generate singing voices with enhanced details. Experimental results show that TCSinger outperforms all baseline models in synthesis quality, singer similarity, and style controllability across various tasks, including zero-shot style transfer, multi-level style control, cross-lingual style transfer, and speech-to-singing style transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.118.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.118/>Be Helpful but Don’t Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support</a></strong><br><a href=/people/j/junlin-li/>Junlin Li</a>
|
<a href=/people/b/bo-peng/>Bo Peng</a>
|
<a href=/people/y/yu-yin-hsu/>Yu-Yin Hsu</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--118><div class="card-body p-3 small">For a conversation to help and support, speakers should maintain an “effect-effort” tradeoff. As outlined in the gist of “Cognitive Relevance Principle”, helpful speakers should optimize the “cognitive relevance” through maximizing the “cognitive effects” and minimizing the “processing effort” imposed on listeners. Although preference learning methods have given rise a boon of studies in pursuit of“effect-optimization”, none have delved into the critical “effort-optimiazation” to fully cultivate the awareness of “optimal relevance” into thecognition of conversation agents. To address this gap, we integrate the “Cognitive Relevance Principle” into emotional support agents in the environment of multi-turn conversation. The results demonstrate a significant and robust improvement against the baseline systems with respect to response quality, human-likedness and supportivenss. This study offers compelling evidence for the effectiveness of the “Relevance Principle” in generating human-like, helpful, and harmless emotional support conversations. The source code will be available at https://github.com/CN-Eyetk/VLESA-ORL.git</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.119.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--119 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.119/>Aligning Language Models to Explicitly Handle Ambiguity</a></strong><br><a href=/people/h/hyuhng-joon-kim/>Hyuhng Joon Kim</a>
|
<a href=/people/y/youna-kim/>Youna Kim</a>
|
<a href=/people/c/cheonbok-park/>Cheonbok Park</a>
|
<a href=/people/j/junyeob-kim/>Junyeob Kim</a>
|
<a href=/people/c/choonghyun-park/>Choonghyun Park</a>
|
<a href=/people/k/kang-min-yoo/>Kang Min Yoo</a>
|
<a href=/people/s/sang-goo-lee/>Sang-goo Lee</a>
|
<a href=/people/t/taeuk-kim/>Taeuk Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--119><div class="card-body p-3 small">In interactions between users and language model agents, user utterances frequently exhibit ellipsis (omission of words or phrases) or imprecision (lack of exactness) to prioritize efficiency. This can lead to varying interpretations of the same input based on different assumptions or background knowledge. It is thus crucial for agents to adeptly handle the inherent ambiguity in queries to ensure reliability. However, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge. To address these issues, we propose Alignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to manage ambiguous queries by leveraging their own assessment of ambiguity (i.e., perceived ambiguity). Experimental results on question-answering datasets demonstrate that APA empowers LLMs to explicitly detect and manage ambiguous queries while retaining the ability to answer clear questions. Furthermore, our finding proves that APA excels beyond training with gold-standard labels, especially in out-of-distribution scenarios. The data and code are available at https://github.com/heyjoonkim/APA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.120.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.120/>Tag-grounded Visual Instruction Tuning with Retrieval Augmentation</a></strong><br><a href=/people/d/daiqing-qi/>Daiqing Qi</a>
|
<a href=/people/h/handong-zhao/>Handong Zhao</a>
|
<a href=/people/z/zijun-wei/>Zijun Wei</a>
|
<a href=/people/s/sheng-li/>Sheng Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--120><div class="card-body p-3 small">Despite recent advances in the general visual instruction-following ability of Multimodal Large Language Models (MLLMs), they still struggle with critical problems when required to provide a precise and detailed response to a visual instruction: (1) failure to identify novel objects or entities, (2) mention of non-existent objects, and (3) neglect of object’s attributed details. Intuitive solutions include improving the size and quality of data or using larger foundation models. They show effectiveness in mitigating these issues, but at an expensive cost of collecting a vast amount of new data and introducing a significantly larger model. Standing at the intersection of these approaches, we examine the three object-oriented problems from the perspective of the image-to-text mapping process by the multimodal connector. In this paper, we first identify the limitations of multimodal connectors stemming from insufficient training data. Driven by this, we propose to enhance the mapping with retrieval-augmented tag tokens, which contain rich object-aware information such as object names and attributes. With our Tag-grounded visual instruction tuning with retrieval Augmentation (TUNA), we outperform baselines that share the same language model and training data on 12 benchmarks. Furthermore, we show the zero-shot capability of TUNA when provided with specific datastores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.121.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--121 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.121/><span class=acl-fixed-case>GL</span>a<span class=acl-fixed-case>PE</span>: Gold Label-agnostic Prompt Evaluation for Large Language Models</a></strong><br><a href=/people/x/xuanchang-zhang/>Xuanchang Zhang</a>
|
<a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--121><div class="card-body p-3 small">Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders its generality. To overcome the limitation, this work proposes GLaPE, a gold label-agnostic prompt evaluation method to alleviate dependence on gold labels. GLaPE is composed of two critical aspects: self-consistency evaluation of a single prompt and mutual-consistency refinement across multiple prompts. Experimental results on 8 widely-recognized reasoning tasks demonstrate that GLaPE can produce more effective prompts, achieving performance comparable to those derived from manually annotated gold labels. Analysis shows that GLaPE provides reliable evaluations aligned with accuracy, even in the absence of gold labels. Code is publicly available at **Anonymous**.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.122.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.122/>Decoding the Echoes of Vision from f<span class=acl-fixed-case>MRI</span>: Memory Disentangling for Past Semantic Information</a></strong><br><a href=/people/r/runze-xia/>Runze Xia</a>
|
<a href=/people/c/congchi-yin/>Congchi Yin</a>
|
<a href=/people/p/piji-li/>Piji Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.123.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--123 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.123/>Optimizing Code Retrieval: High-Quality and Scalable Dataset Annotation through Large Language Models</a></strong><br><a href=/people/r/rui-li/>Rui Li</a>
|
<a href=/people/q/qi-liu/>Qi Liu</a>
|
<a href=/people/l/liyang-he/>Liyang He</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/h/hao-zhang/>Hao Zhang</a>
|
<a href=/people/s/shengyu-ye/>Shengyu Ye</a>
|
<a href=/people/j/junyu-lu/>Junyu Lu</a>
|
<a href=/people/z/zhenya-huang/>Zhenya Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--123><div class="card-body p-3 small">Code retrieval aims to identify code from extensive codebases that semantically aligns with a given query code snippet. Collecting a broad and high-quality set of query and code pairs is crucial to the success of this task. However, existing data collection methods struggle to effectively balance scalability and annotation quality. In this paper, we first analyze the factors influencing the quality of function annotations generated by Large Language Models (LLMs). We find that the invocation of intra-repository functions and third-party APIs plays a significant role. Building on this insight, we propose a novel annotation method that enhances the annotation context by incorporating the content of functions called within the repository and information on third-party API functionalities. Additionally, we integrate LLMs with a novel sorting method to address the multi-level function call relationships within repositories. Furthermore, by applying our proposed method across a range of repositories, we have developed the Query4Code dataset. The quality of this synthesized dataset is validated through both model training and human evaluation, demonstrating high-quality annotations. Moreover, cost analysis confirms the scalability of our annotation method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.124.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.124.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.124/>Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models</a></strong><br><a href=/people/y/yongjin-yang/>Yongjin Yang</a>
|
<a href=/people/j/jongwoo-ko/>Jongwoo Ko</a>
|
<a href=/people/s/se-young-yun/>Se-Young Yun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--124><div class="card-body p-3 small">Vision-language models (VLMs) like CLIP have demonstrated remarkable applicability across a variety of downstream tasks, including zero-shot image classification. Recently, the use of prompts or adapters for efficient transfer learning (ETL) has gained significant attention for effectively adapting to downstream tasks. However, previous studies have overlooked the challenge of varying transfer difficulty of downstream tasks. In this paper, we empirically analyze how each ETL method behaves with respect to transfer difficulty. Our observations indicate that utilizing vision prompts and text adapters is crucial for adaptability and generalizability in domains with high difficulty. Also, by applying an adaptive ensemble approach that integrates task-adapted VLMs with pre-trained VLMs and strategically leverages more general knowledge in low-difficulty and less in high-difficulty domains, we consistently enhance performance across both types of domains. Based on these observations, we propose an adaptive ensemble method that combines visual prompts and text adapters with pre-trained VLMs, tailored by transfer difficulty, to achieve optimal performance for any target domain. Upon experimenting with extensive benchmarks, our method consistently outperforms all baselines, particularly on unseen tasks, demonstrating its effectiveness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.125.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.125.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.125/>Advancing Process Verification for Large Language Models via Tree-Based Preference Learning</a></strong><br><a href=/people/m/mingqian-he/>Mingqian He</a>
|
<a href=/people/y/yongliang-shen/>Yongliang Shen</a>
|
<a href=/people/w/wenqi-zhang/>Wenqi Zhang</a>
|
<a href=/people/z/zeqi-tan/>Zeqi Tan</a>
|
<a href=/people/w/weiming-lu/>Weiming Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--125><div class="card-body p-3 small">Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales. Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% → 82.79%), MATH (17.00% → 26.80%), CSQA (68.14% → 72.97%), and StrategyQA (82.86% → 83.25%). Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.126.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--126 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.126.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.126/>An Inversion Attack Against Obfuscated Embedding Matrix in Language Model Inference</a></strong><br><a href=/people/y/yu-lin/>Yu Lin</a>
|
<a href=/people/q/qizhi-zhang/>Qizhi Zhang</a>
|
<a href=/people/q/quanwei-cai/>Quanwei Cai</a>
|
<a href=/people/j/jue-hong/>Jue Hong</a>
|
<a href=/people/w/wu-ye/>Wu Ye</a>
|
<a href=/people/h/huiqi-liu/>Huiqi Liu</a>
|
<a href=/people/b/bing-duan/>Bing Duan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--126><div class="card-body p-3 small">With the rapidly-growing deployment of large language model (LLM) inference services, privacy concerns have arisen regarding to the user input data. Recent studies are exploring transforming user inputs to obfuscated embedded vectors, so that the data will not be eavesdropped by service provides. However, in this paper we show that again, without a solid and deliberate security design and analysis, such embedded vector obfuscation failed to protect users’ privacy. We demonstrate the conclusion via conducting a novel inversion attack called Element-wise Differential Nearest Neighbor (EDNN) on the glide-reflection proposed in (CITATION), and the result showed that the original user input text can be 100% recovered from the obfuscated embedded vectors. We further analyze security requirements on embedding obfuscation and present several remedies to our proposed attack.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.127.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.127/><span class=acl-fixed-case>V</span>ideo<span class=acl-fixed-case>S</span>core: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation</a></strong><br><a href=/people/x/xuan-he/>Xuan He</a>
|
<a href=/people/d/dongfu-jiang/>Dongfu Jiang</a>
|
<a href=/people/g/ge-zhang/>Ge Zhang</a>
|
<a href=/people/m/max-ku/>Max Ku</a>
|
<a href=/people/a/achint-soni/>Achint Soni</a>
|
<a href=/people/s/sherman-siu/>Sherman Siu</a>
|
<a href=/people/h/haonan-chen/>Haonan Chen</a>
|
<a href=/people/a/abhranil-chandra/>Abhranil Chandra</a>
|
<a href=/people/z/ziyan-jiang/>Ziyan Jiang</a>
|
<a href=/people/a/aaran-arulraj/>Aaran Arulraj</a>
|
<a href=/people/k/kai-wang/>Kai Wang</a>
|
<a href=/people/q/quy-duc-do/>Quy Duc Do</a>
|
<a href=/people/y/yuansheng-ni/>Yuansheng Ni</a>
|
<a href=/people/b/bohan-lyu/>Bohan Lyu</a>
|
<a href=/people/y/yaswanth-narsupalli/>Yaswanth Narsupalli</a>
|
<a href=/people/r/rongqi-fan/>Rongqi Fan</a>
|
<a href=/people/z/zhiheng-lyu/>Zhiheng Lyu</a>
|
<a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--127><div class="card-body p-3 small">The recent years have witnessed great advances in video generation. However, the development of automatic video metrics is lagging significantly behind. None of the existing metric is able to provide reliable scores over generated videos. The main barrier is the lack of large-scale human-annotated dataset. In this paper, we release VideoFeedback, the first large-scale dataset containing human-provided multi-aspect score over 37.6K synthesized videos from 11 existing video generative models. We train VideoScore (initialized from Mantis)based on VideoFeedback to enable automatic video quality assessment. Experiments show that the Spearman’s correlation betweenVideoScore and humans can reach 77.1 on VideoFeedback-test, beating the prior best metrics by about 50 points. Further result onother held-out EvalCrafter, GenAI-Bench, and VBench show that VideoScore has consistently much higher correlation with humanjudges than other metrics. Due to these results, we believe VideoScore can serve as a great proxy for human raters to (1) rate different video models to track progress (2) simulate fine-grained human feedback in Reinforcement Learning with Human Feedback (RLHF) to improve current video generation models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.128.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.128/><span class=acl-fixed-case>L</span>ogic<span class=acl-fixed-case>A</span>sker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models</a></strong><br><a href=/people/y/yuxuan-wan/>Yuxuan Wan</a>
|
<a href=/people/w/wenxuan-wang/>Wenxuan Wang</a>
|
<a href=/people/y/yiliu-yang/>Yiliu Yang</a>
|
<a href=/people/y/youliang-yuan/>Youliang Yuan</a>
|
<a href=/people/j/jen-tse-huang/>Jen-tse Huang</a>
|
<a href=/people/p/pinjia-he/>Pinjia He</a>
|
<a href=/people/w/wenxiang-jiao/>Wenxiang Jiao</a>
|
<a href=/people/m/michael-lyu/>Michael Lyu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--128><div class="card-body p-3 small">We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT-4. Despite LLMs’ prowess in tasks like writing assistance, code generation, and machine translation, assessing their ability to reason has been challenging. Traditional evaluations often prioritize accuracy on downstream tasks over direct assessments of reasoning processes. LogicAsker addresses this gap by employing a set of atomic reasoning skills grounded in propositional and predicate logic to systematically examine and improve the reasoning prowess of LLMs. Our methodology reveals significant gaps in LLMs’ learning of logical rules, with identified reasoning failures ranging from 29% to 90% across different models. Moreover, we leverage these findings to construct targeted demonstration examples and fine-tune data, notably enhancing logical reasoning in models like GPT-4o by up to 5%. To our knowledge, this is the first effort to utilize test case outcomes to effectively refine LLMs’ formal reasoning capabilities. We make our code, data, and results publicly available(https://github.com/yxwan123/LogicAsker) to facilitate further research and replication of our findings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.129.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.129/>Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training</a></strong><br><a href=/people/x/xiaoyang-yi/>Xiaoyang Yi</a>
|
<a href=/people/y/yuru-bao/>Yuru Bao</a>
|
<a href=/people/j/jian-zhang/>Jian Zhang</a>
|
<a href=/people/y/yifang-qin/>Yifang Qin</a>
|
<a href=/people/f/faxin-lin/>Faxin Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--129><div class="card-body p-3 small">Information Extraction (IE), aiming to extract structured information from unstructured natural language texts, can significantly benefit from pre-trained language models. However, existing pre-training methods solely focus on exploiting the textual knowledge, relying extensively on annotated large-scale datasets, which is labor-intensive and thus limits the scalability and versatility of the resulting models. To address these issues, we propose SKIE, a novel pre-training framework tailored for IE that integrates structural semantic knowledge via contrastive learning, effectively alleviating the annotation burden. Specifically, SKIE utilizes Abstract Meaning Representation (AMR) as a low-cost supervision source to boost model performance without human intervention. By enhancing the topology of AMR graphs, SKIE derives high-quality cohesive subgraphs as additional training samples, providing diverse multi-level structural semantic knowledge. Furthermore, SKIE refines the graph encoder to better capture cohesive information and edge relation information, thereby improving the pre-training efficacy. Extensive experimental results demonstrate that SKIE outperforms state-of-the-art baselines across multiple IE tasks and showcases exceptional performance in few-shot and zero-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.130.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.130.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.130/><span class=acl-fixed-case>F</span>use<span class=acl-fixed-case>G</span>en: <span class=acl-fixed-case>PLM</span> Fusion for Data-generation based Zero-shot Learning</a></strong><br><a href=/people/t/tianyuan-zou/>Tianyuan Zou</a>
|
<a href=/people/y/yang-liu/>Yang Liu</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/j/jianqing-zhang/>Jianqing Zhang</a>
|
<a href=/people/j/jingjing-liu/>Jingjing Liu</a>
|
<a href=/people/y/ya-qin-zhang/>Ya-Qin Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--130><div class="card-body p-3 small">Data-generation based zero-shot learning, although effective in training Small Task-specific Models (STMs) via synthetic datasets generated by Pre-trained Language Models (PLMs), is often limited by the low quality of such synthetic datasets. Previous solutions have primarily focused on single PLM settings, where synthetic datasets are typically restricted to specific sub-spaces and often deviate from real-world distributions, leading to severe distribution bias. To mitigate such bias, we propose FuseGen, a novel data-generation based zero-shot learning framework that introduces a new criteria for subset selection from synthetic datasets via utilizing multiple PLMs and trained STMs. The chosen subset provides in-context feedback to each PLM, enhancing dataset quality through iterative data generation. Trained STMs are then used for sample re-weighting as well, further improving data quality. Extensive experiments across diverse tasks demonstrate that FuseGen substantially outperforms existing methods, highly effective in boosting STM performance in a PLM-agnostic way. The code is available at https://github.com/LindaLydia/FuseGen.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.131.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--131 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.131/><span class=acl-fixed-case>I</span> Need Help! Evaluating <span class=acl-fixed-case>LLM</span>’s Ability to Ask for Users’ Support: A Case Study on Text-to-<span class=acl-fixed-case>SQL</span> Generation</a></strong><br><a href=/people/c/cheng-kuang-wu/>Cheng-Kuang Wu</a>
|
<a href=/people/z/zhi-rui-tam/>Zhi Rui Tam</a>
|
<a href=/people/c/chao-chung-wu/>Chao-Chung Wu</a>
|
<a href=/people/c/chieh-yen-lin/>Chieh-Yen Lin</a>
|
<a href=/people/h/hung-yi-lee/>Hung-yi Lee</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--131><div class="card-body p-3 small">This study explores the proactive ability of LLMs to seek user support. We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability. Our experiments show that without external feedback, many LLMs struggle to recognize their need for user support. The findings highlight the importance of external signals and provide insights for future research on improving support-seeking strategies. Source code: https://github.com/appier-research/i-need-help</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.132.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.132.data.tgz data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.132/>Oddballs and Misfits: Detecting Implicit Abuse in Which Identity Groups are Depicted as Deviating from the Norm</a></strong><br><a href=/people/m/michael-wiegand/>Michael Wiegand</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--132><div class="card-body p-3 small">We address the task of detecting abusive sentences in which identity groups are depicted as deviating from the norm (e.g. Gays sprinkle flour over their gardens for good luck). These abusive utterances need not be stereotypes or negative in sentiment. We introduce the first dataset for this task. It is created via crowdsourcing and includes 7 identity groups. We also report on classification experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.133.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.133.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.133/>By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting</a></strong><br><a href=/people/h/hyungjun-yoon/>Hyungjun Yoon</a>
|
<a href=/people/b/biniyam-aschalew-tolera/>Biniyam Aschalew Tolera</a>
|
<a href=/people/t/taesik-gong/>Taesik Gong</a>
|
<a href=/people/k/kimin-lee/>Kimin Lee</a>
|
<a href=/people/s/sung-ju-lee/>Sung-Ju Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--133><div class="card-body p-3 small">Large language models (LLMs) have demonstrated exceptional abilities across various domains. However, utilizing LLMs for ubiquitous sensing applications remains challenging as existing text-prompt methods show significant performance degradation when handling long sensor data sequences. In this paper, we propose a visual prompting approach for sensor data using multimodal LLMs (MLLMs). Specifically, we design a visual prompt that directs MLLMs to utilize visualized sensor data alongside descriptions of the target sensory task. Additionally, we introduce a visualization generator that automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge. We evaluated our approach on nine sensory tasks involving four sensing modalities, achieving an average of 10% higher accuracy compared to text-based prompts and reducing token costs by 15.8 times. Our findings highlight the effectiveness and cost-efficiency of using visual prompts with MLLMs for various sensory tasks. The source code is available at https://github.com/diamond264/ByMyEyes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.134.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.134/>Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization</a></strong><br><a href=/people/s/seungwoo-son/>Seungwoo Son</a>
|
<a href=/people/w/wonpyo-park/>Wonpyo Park</a>
|
<a href=/people/w/woohyun-han/>Woohyun Han</a>
|
<a href=/people/k/kyuyeun-kim/>Kyuyeun Kim</a>
|
<a href=/people/j/jaeho-lee/>Jaeho Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--134><div class="card-body p-3 small">Despite recent advances in LLM quantization, activation quantization remains to be challenging due to the activation outliers. Conventional remedies, e.g., mixing precisions for different channels, introduce extra overhead and reduce the speedup. In this work, we develop a simple yet effective strategy to facilitate per-tensor activation quantization by preventing the generation of problematic tokens. Precisely, we propose a method to find a set of key-value cache, coined _CushionCache_, which mitigates outliers in subsequent tokens when inserted as a prefix. CushionCache works in two steps: First, we greedily search for a prompt token sequence that minimizes the maximum activation values in subsequent tokens. Then, we further tune the token cache to regularize the activations of subsequent tokens to be more quantization-friendly. The proposed method successfully addresses activation outliers of LLMs, providing a substantial performance boost for per-tensor activation quantization methods. We thoroughly evaluate our method over a wide range of models and benchmarks and find that it significantly surpasses the established baseline of per-tensor W8A8 quantization and can be seamlessly integrated with the recent activation quantization method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.135.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--135 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.135/><span class=acl-fixed-case>CHIQ</span>: Contextual History Enhancement for Improving Query Rewriting in Conversational Search</a></strong><br><a href=/people/f/fengran-mo/>Fengran Mo</a>
|
<a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/k/kelong-mao/>Kelong Mao</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/j/jian-yun-nie/>Jian-Yun Nie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--135><div class="card-body p-3 small">In this paper, we study how open-source large language models (LLMs) can be effectively deployed for improving query rewriting in conversational search, especially for ambiguous queries. We introduce CHIQ, a two-step method that leverages the capabilities of LLMs to resolve ambiguities in the conversation history before query rewriting. This approach contrasts with prior studies that predominantly use closed-source LLMs to directly generate search queries from conversation history. We demonstrate on five well-established benchmarks that CHIQ leads to state-of-the-art results across most settings, showing highly competitive performances with systems leveraging closed-source LLMs. Our study provides a first step towards leveraging open-source LLMs in conversational search, as a competitive alternative to the prevailing reliance on commercial LLMs. Data, models, and source code will be publicly available upon acceptance at https://github.com/fengranMark/CHIQ.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.136.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.136/>Towards Low-Resource Harmful Meme Detection with <span class=acl-fixed-case>LMM</span> Agents</a></strong><br><a href=/people/j/jianzhao-huang/>Jianzhao Huang</a>
|
<a href=/people/h/hongzhan-lin/>Hongzhan Lin</a>
|
<a href=/people/l/liu-ziyan/>Liu Ziyan</a>
|
<a href=/people/z/ziyang-luo/>Ziyang Luo</a>
|
<a href=/people/g/guang-chen/>Guang Chen</a>
|
<a href=/people/j/jing-ma/>Jing Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--136><div class="card-body p-3 small">The proliferation of Internet memes in the age of social media necessitates effective identification of harmful ones. Due to the dynamic nature of memes, existing data-driven models may struggle in low-resource scenarios where only a few labeled examples are available. In this paper, we propose an agency-driven framework for low-resource harmful meme detection, employing both outward and inward analysis with few-shot annotated samples. Inspired by the powerful capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first retrieve relative memes with annotations to leverage label information as auxiliary signals for the LMM agent. Then, we elicit knowledge-revising behavior within the LMM agent to derive well-generalized insights into meme harmfulness. By combining these strategies, our approach enables dialectical reasoning over intricate and implicit harm-indicative patterns. Extensive experiments conducted on three meme datasets demonstrate that our proposed approach achieves superior performance than state-of-the-art methods on the low-resource harmful meme detection task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.137.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--137 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.137.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.137/><span class=acl-fixed-case>VIVA</span>: A Benchmark for Vision-Grounded Decision-Making with Human Values</a></strong><br><a href=/people/z/zhe-hu/>Zhe Hu</a>
|
<a href=/people/y/yixiao-ren/>Yixiao Ren</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/y/yu-yin/>Yu Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--137><div class="card-body p-3 small">This paper introduces VIVA, a benchmark for VIsion-grounded decision-making driven by human VA. While most large vision-language models (VLMs) focus on physical-level skills, our work is the first to examine their multimodal capabilities in leveraging human values to make decisions under a vision-depicted situation. VIVA contains 1,062 images depicting diverse real-world situations and the manually annotated decisions grounded in them. Given an image there, the model should select the most appropriate action to address the situation and provide the relevant human values and reason underlying the decision. Extensive experiments based on VIVA show the limitation of VLMs in using human values to make multimodal decisions. Further analyses indicate the potential benefits of exploiting action consequences and predicted human values.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.138.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--138 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.138/>Direct Multi-Turn Preference Optimization for Language Agents</a></strong><br><a href=/people/w/wentao-shi/>Wentao Shi</a>
|
<a href=/people/m/mengqi-yuan/>Mengqi Yuan</a>
|
<a href=/people/j/junkang-wu/>Junkang Wu</a>
|
<a href=/people/q/qifan-wang/>Qifan Wang</a>
|
<a href=/people/f/fuli-feng/>Fuli Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--138><div class="card-body p-3 small">Adapting Large Language Models (LLMs) for agent tasks is critical in developing language agents. Direct Preference Optimization (DPO) is a promising technique for this adaptation with the alleviation of compounding errors, offering a means to directly optimize Reinforcement Learning (RL) objectives. However, applying DPO to multi-turn tasks presents challenges due to the inability to cancel the partition function. Overcoming this obstacle involves making the partition function independent of the current state and addressing length disparities between preferred and dis-preferred trajectories. In this light, we replace the policy constraint with the state-action occupancy measure constraint in the RL objective and add length normalization to the Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn agent tasks with theoretical explanations. Extensive experiments on three multi-turn agent task datasets confirm the effectiveness and superiority of the DMPO loss.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.139.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--139 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.139.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.139.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.139/>Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models</a></strong><br><a href=/people/l/leonardo-ranaldi/>Leonardo Ranaldi</a>
|
<a href=/people/a/andre-freitas/>Andre Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--139><div class="card-body p-3 small">The alignment of reasoning abilities between smaller and larger Language Models are largely conducted via supervised fine-tuning using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations.In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-improve their abilities.Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on synthetic demonstrations provided by LLMs, and then the instructed models self-improve their abilities through preference optimization strategies.In particular, the second phase operates refinement heuristics based on Direct Preference Optimization, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs.Results obtained on commonsense and math reasoning tasks show that this approach consistently outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.140.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.140.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.140/>In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search</a></strong><br><a href=/people/h/huihan-li/>Huihan Li</a>
|
<a href=/people/y/yuting-ning/>Yuting Ning</a>
|
<a href=/people/z/zeyi-liao/>Zeyi Liao</a>
|
<a href=/people/s/siyuan-wang/>Siyuan Wang</a>
|
<a href=/people/x/xiang-lorraine-li/>Xiang Lorraine Li</a>
|
<a href=/people/x/ximing-lu/>Ximing Lu</a>
|
<a href=/people/w/wenting-zhao/>Wenting Zhao</a>
|
<a href=/people/f/faeze-brahman/>Faeze Brahman</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--140><div class="card-body p-3 small">To effectively use large language models (LLMs) for real-world queries, it is imperative that they generalize to the long-tail distribution, i.e. rare examples where models exhibit low confidence. In this work, we take the first step towards evaluating LLMs in the long-tail distribution of inferential knowledge. We exemplify long-tail evaluation on the Natural Language Inference task. First, we introduce Logic-Induced-Knowledge-Search (LINK), a systematic long-tail data generation framework, to obtain factually-correct yet long-tail inferential statements. LINK uses variable-wise prompting grounded on symbolic rules to seek low-confidence statements while ensuring factual correctness. We then use LINK to curate Logic-Induced-Long-Tail (LINT), a large-scale long-tail inferential knowledge dataset that contains 108K statements spanning four domains. We evaluate popular LLMs on LINT; we find that state-of-the-art LLMs show significant performance drop (21% relative drop for GPT4) on long-tail data as compared to on head distribution data, and smaller models show even more generalization weakness. These results further underscore the necessity of long-tail evaluation in developing generalizable LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.141.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.141.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.141.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.141/><span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>S</span>craper: A Progressive Understanding Web Agent for Web Scraper Generation</a></strong><br><a href=/people/w/wenhao-huang/>Wenhao Huang</a>
|
<a href=/people/z/zhouhong-gu/>Zhouhong Gu</a>
|
<a href=/people/c/chenghao-peng/>Chenghao Peng</a>
|
<a href=/people/j/jiaqing-liang/>Jiaqing Liang</a>
|
<a href=/people/z/zhixu-li/>Zhixu Li</a>
|
<a href=/people/y/yanghua-xiao/>Yanghua Xiao</a>
|
<a href=/people/l/liqian-wen/>Liqian Wen</a>
|
<a href=/people/z/zulong-chen/>Zulong Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--141><div class="card-body p-3 small">Web scraping is a powerful technique that extracts data from websites, enabling automated data collection, enhancing data analysis capabilities, and minimizing manual data entry efforts. Existing methods, wrappers-based methods suffer from limited adaptability and scalability when faced with a new website, while language agents, empowered by large language models (LLMs), exhibit poor reusability in diverse web environments. In this work, we introduce the paradigm of generating web scrapers with LLMs and propose AutoScraper, a two-stage framework that can handle diverse and changing web environments more efficiently. AutoScraper leverages the hierarchical structure of HTML and similarity across different web pages for generating web scrapers. Besides, we propose a new executability metric for better measuring the performance of web scraper generation tasks. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Our work is now open-source.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.142.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--142 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.142.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.142/>Backward Lens: Projecting Language Model Gradients into the Vocabulary Space</a></strong><br><a href=/people/s/shahar-katz/>Shahar Katz</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/m/mor-geva/>Mor Geva</a>
|
<a href=/people/l/lior-wolf/>Lior Wolf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--142><div class="card-body p-3 small">Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models’ vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs’ backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes’ inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs’ neurons.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.143.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.143.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.143.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.143/>Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding</a></strong><br><a href=/people/j/jiwan-chung/>Jiwan Chung</a>
|
<a href=/people/s/sungjae-lee/>Sungjae Lee</a>
|
<a href=/people/m/minseo-kim/>Minseo Kim</a>
|
<a href=/people/s/seungju-han/>Seungju Han</a>
|
<a href=/people/a/ashkan-yousefpour/>Ashkan Yousefpour</a>
|
<a href=/people/j/jack-hessel/>Jack Hessel</a>
|
<a href=/people/y/youngjae-yu/>Youngjae Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--143><div class="card-body p-3 small">Visual arguments, often used in advertising or social causes, rely on images to persuade viewers to do or believe something. Understanding these arguments requires selective vision: only specific visual stimuli within an image are relevant to the argument, and relevance can only be understood within the context of a broader argumentative structure. While visual arguments are readily appreciated by human audiences, we ask: are today’s AI capable of similar understanding?We present VisArgs, a dataset of 1,611 images annotated with 5,112 visual premises (with regions), 5,574 commonsense premises, and reasoning trees connecting them into structured arguments. We propose three tasks for evaluating visual argument understanding: premise localization, premise identification, and conclusion deduction.Experiments show that 1) machines struggle to capture visual cues: GPT-4-O achieved 78.5% accuracy, while humans reached 98.0%. Models also performed 19.5% worse when distinguishing between irrelevant objects within the image compared to external objects. 2) Providing relevant visual premises improved model performance significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.144.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--144 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.144.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.144.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.144/>Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!</a></strong><br><a href=/people/j/jiwan-chung/>Jiwan Chung</a>
|
<a href=/people/s/seungwon-lim/>Seungwon Lim</a>
|
<a href=/people/j/jaehyun-jeon/>Jaehyun Jeon</a>
|
<a href=/people/s/seungbeen-lee/>Seungbeen Lee</a>
|
<a href=/people/y/youngjae-yu/>Youngjae Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--144><div class="card-body p-3 small">Humans possess multimodal literacy, allowing them to actively integrate information from various modalities to form reasoning. Faced with challenges like lexical ambiguity in text, we supplement this with other modalities, such as thumbnail images or textbook illustrations. Is it possible for machines to achieve a similar multimodal understanding capability?In response, we present Understanding Pun with Image Explanations (UNPIE), a novel benchmark designed to assess the impact of multimodal inputs in resolving lexical ambiguities. Puns serve as the ideal subject for this evaluation due to their intrinsic ambiguity. Our dataset includes 1,000 puns, each accompanied by an image that explains both meanings. We pose three multimodal challenges with the annotations to assess different aspects of multimodal literacy; Pun Grounding, Disambiguation, and Reconstruction. The results indicate that various Socratic Models and Visual-Language Models improve over the text-only models when given visual context, particularly as the complexity of the tasks increases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.145.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.145/>Reusing Transferable Weight Increments for Low-resource Style Generation</a></strong><br><a href=/people/c/chunzhen-jin/>Chunzhen Jin</a>
|
<a href=/people/e/eliot-huang/>Eliot Huang</a>
|
<a href=/people/h/heng-chang/>Heng Chang</a>
|
<a href=/people/y/yaqi-wang/>Yaqi Wang</a>
|
<a href=/people/p/peng-cao/>Peng Cao</a>
|
<a href=/people/o/osmar-r-zaiane/>Osmar Zaiane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--145><div class="card-body p-3 small">Text style transfer (TST) is crucial in natural language processing, aiming to endow text with a new style without altering its meaning. In real-world scenarios, not all styles have abundant resources. This work introduces TWIST (reusing Transferable Weight Increments for Style Text generation), a novel framework to mitigate data scarcity by utilizing style features in weight increments to transfer low-resource styles effectively. During target style learning, we derive knowledge via a specially designed weight pool and initialize the parameters for the unseen style. To enhance the effectiveness of merging, the target style weight increments are often merged from multiple source style weight increments through singular vectors. Considering the diversity of styles, we also designed a multi-key memory network that simultaneously focuses on task- and instance-level information to derive the most relevant weight increments. Results from multiple style transfer datasets show that TWIST demonstrates remarkable performance across different backbones, achieving particularly effective results in low-resource scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.146.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--146 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.146/>Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course</a></strong><br><a href=/people/c/cheng-han-chiang/>Cheng-Han Chiang</a>
|
<a href=/people/w/wei-chih-chen/>Wei-Chih Chen</a>
|
<a href=/people/c/chun-yi-kuan/>Chun-Yi Kuan</a>
|
<a href=/people/c/chienchou-yang/>Chienchou Yang</a>
|
<a href=/people/h/hung-yi-lee/>Hung-yi Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--146><div class="card-body p-3 small">Using large language models (LLMs) for automatic evaluation has become an important evaluation method in NLP research. However, it is unclear whether these LLM-based evaluators can be effectively applied in real-world classrooms to assess student assignments. This empirical report shares how we use GPT-4 as an automatic assignment evaluator in a university course with over 1000 students. Based on student responses, we found that LLM-based assignment evaluators are generally acceptable to students when they have free access to these tools. However, students also noted that the LLM sometimes fails to adhere to the evaluation instructions, resulting in unreasonable assessments. Additionally, we observed that students can easily manipulate the LLM to output specific strings, allowing them to achieve high scores without meeting the assignment rubric. Based on student feedback and our experience, we offer several recommendations for effectively integrating LLMs into future classroom evaluations. Our observation also highlights potential directions for improving LLM-based evaluators, including their instruction-following ability and vulnerability to prompt hacking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.147.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.147.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.147.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.147/>Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?</a></strong><br><a href=/people/n/neeladri-bhuiya/>Neeladri Bhuiya</a>
|
<a href=/people/v/viktor-schlegel/>Viktor Schlegel</a>
|
<a href=/people/s/stefan-winkler/>Stefan Winkler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--147><div class="card-body p-3 small">State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on multi-hop reasoning—the ability to identify and integrate information from multiple textual sources.Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. We propose a challenging multi-hop reasoning benchmark by generating seemingly plausible multi-hop reasoning chains that ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs and show that their multi-hop reasoning performance is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives. We also find that—while LLMs tend to ignore misleading lexical cues—misleading reasoning paths indeed present a significant challenge. The code and data are made available at https://github.com/zawedcvg/Are-Large-Language-Models-Attentive-Readers</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.148.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.148/>Instruction Pre-Training: Language Models are Supervised Multitask Learners</a></strong><br><a href=/people/d/daixuan-cheng/>Daixuan Cheng</a>
|
<a href=/people/y/yuxian-gu/>Yuxian Gu</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/j/junyu-bi/>Junyu Bi</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--148><div class="card-body p-3 small">Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better generalization. In this paper, we explore supervised multitask pre-training by proposing Instruction Pre-training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In our experiments, we synthesize 200M instruction response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-training. In pre-training from scratch, Instruction Pre-training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-training enables Llama3-8B to be comparable to or even outperform Llama3-70B. Our model, code, and data are available at https://github.com/microsoft/LMOps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.149.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--149 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.149/><span class=acl-fixed-case>LEM</span>o<span class=acl-fixed-case>E</span>: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models</a></strong><br><a href=/people/r/renzhi-wang/>Renzhi Wang</a>
|
<a href=/people/p/piji-li/>Piji Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--149><div class="card-body p-3 small">Large language models (LLMs) require continual knowledge updates to stay abreast of the ever-changing world facts, prompting the formulation of lifelong model editing task. While recent years have witnessed the development of various techniques for single and batch editing, these methods either fail to apply or perform sub-optimally when faced with lifelong editing. In this paper, we introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong model editing. We first analyze the factors influencing the effectiveness of conventional MoE adaptor in lifelong editing, including catastrophic forgetting, inconsistent routing and order sensitivity. Based on these insights, we propose a tailored module insertion method to achieve lifelong editing, incorporating a novel KV anchor routing to enhance routing consistency between training and inference stage, along with a concise yet effective clustering-based editing order planning. Experimental results demonstrate the effectiveness of our method in lifelong editing, surpassing previous model editing techniques while maintaining outstanding performance in batch editing task. Our code will be available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.150.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--150 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.150/>Collaborative Performance Prediction for Large Language Models</a></strong><br><a href=/people/q/qiyuan-zhang/>Qiyuan Zhang</a>
|
<a href=/people/f/fuyuan-lyu/>Fuyuan Lyu</a>
|
<a href=/people/x/xue-liu/>Xue Liu</a>
|
<a href=/people/c/chen-ma/>Chen Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--150><div class="card-body p-3 small">Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works demonstrated intrinsic similarities within model families and utilized such similarities for performance prediction. However, they tend to overlook the similarities between model families and only consider design factors listed in the original scaling law. To overcome these limitations, we introduce a novel framework, Collaborative Performance Prediction (CPP), which significantly enhances prediction accuracy by leveraging the historical performance of various models on downstream tasks and other design factors for both model and task. We also collect a collaborative data sourced from online platforms containing both historical performance and additional design factors. With the support of the collaborative data, CPP not only surpasses traditional scaling laws in predicting the performance of scaled LLMs but also facilitates a detailed analysis of factor importance, an area previously overlooked.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.151.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.151.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.151/>Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (<span class=acl-fixed-case>CCR</span>) for Classical <span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/y/yuqi-chen/>Yuqi Chen</a>
|
<a href=/people/s/sixuan-li/>Sixuan Li</a>
|
<a href=/people/y/ying-li/>Ying Li</a>
|
<a href=/people/m/mohammad-atari/>Mohammad Atari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--151><div class="card-body p-3 small">In this work, we develop a pipeline for historical-psychological text analysis in classical Chinese. Humans have produced texts in various languages for thousands of years; however, most of the computational literature is focused on contemporary languages and corpora. The emerging field of historical psychology relies on computational techniques to extract aspects of psychology from historical corpora using new methods developed in natural language processing (NLP). The present pipeline, called Contextualized Construct Representations (CCR), combines expert knowledge in psychometrics (i.e., psychological surveys) with text representations generated via Transformer-based language models to measure psychological constructs such as traditionalism, norm strength, and collectivism in classical Chinese corpora. Considering the scarcity of available data, we propose an indirect supervised contrastive learning approach and build the first Chinese historical psychology corpus (C-HI-PSY) to fine-tune pre-trained models. We evaluate the pipeline to demonstrate its superior performance compared with other approaches. The CCR method outperforms word-embedding-based approaches across all of our tasks and exceeds prompting with GPT-4 in most tasks. Finally, we benchmark the pipeline against objective, external data to further verify its validity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.152.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.152/>Knowledge Verification to Nip Hallucination in the Bud</a></strong><br><a href=/people/f/fanqi-wan/>Fanqi Wan</a>
|
<a href=/people/x/xinting-huang/>Xinting Huang</a>
|
<a href=/people/l/leyang-cui/>Leyang Cui</a>
|
<a href=/people/x/xiaojun-quan/>Xiaojun Quan</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--152><div class="card-body p-3 small">While large language models (LLMs) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation LLMs. Specifically, we propose a novel approach called Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs. To address knowledge inconsistencies in the alignment data, KCA implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of KCA in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data are openly accessible at <a href=https://github.com/fanqiwan/KCA class=acl-markup-url>https://github.com/fanqiwan/KCA</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.153.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--153 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.153.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.153/><span class=acl-fixed-case>QUITE</span>: Quantifying Uncertainty in Natural Language Text in <span class=acl-fixed-case>B</span>ayesian Reasoning Scenarios</a></strong><br><a href=/people/t/timo-pierre-schrader/>Timo Pierre Schrader</a>
|
<a href=/people/l/lukas-lange/>Lukas Lange</a>
|
<a href=/people/s/simon-razniewski/>Simon Razniewski</a>
|
<a href=/people/a/annemarie-friedrich/>Annemarie Friedrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--153><div class="card-body p-3 small">Reasoning is key to many decision making processes. It requires consolidating a set of rule-like premises that are often associated with degrees of uncertainty and observations to draw conclusions. In this work, we address both the case where premises are specified as numeric probabilistic rules and situations in which humans state their estimates using words expressing degrees of certainty. Existing probabilistic reasoning datasets simplify the task, e.g., by requiring the model to only rank textual alternatives, by including only binary random variables, or by making use of a limited set of templates that result in less varied text.In this work, we present QUITE, a question answering dataset of real-world Bayesian reasoning scenarios with categorical random variables and complex relationships. QUITE provides high-quality natural language verbalizations of premises together with evidence statements and expects the answer to a question in the form of an estimated probability. We conduct an extensive set of experiments, finding that logic-based models outperform out-of-the-box large language models on all reasoning types (causal, evidential, and explaining-away). Our results provide evidence that neuro-symbolic models are a promising direction for improving complex reasoning. We release QUITE and code for training and experiments on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.154.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--154 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.154/><span class=acl-fixed-case>A</span>frican or <span class=acl-fixed-case>E</span>uropean Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification</a></strong><br><a href=/people/g/gregor-geigle/>Gregor Geigle</a>
|
<a href=/people/r/radu-timofte/>Radu Timofte</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--154><div class="card-body p-3 small">Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities on numerous image understanding and reasoning tasks. The task of fine-grained object classification (e.g., distinction between <i>animal species</i>), however, has been probed insufficiently, despite its downstream importance. We fill this evaluation gap by creating FOCI (<b>F</b>ine-grained <b>O</b>bject <b>C</b>lass<b>I</b>fication), a difficult multiple-choice benchmark for fine-grained object classification, from existing object classification datasets: (1) multiple-choice avoids ambiguous answers associated with casting classification as open-ended QA task; (2) we retain classification difficulty by mining negative labels with a CLIP model. FOCI complements five popular classification datasets with four domain-specific subsets from ImageNet-21k. We benchmark 12 public LVLMs on and show that it tests for a <i>complementary skill</i> to established image understanding and reasoning benchmarks. Crucially, CLIP models exhibit dramatically better performance than LVLMs. Since the image encoders of LVLMs come from these CLIP models, this points to inadequate alignment for fine-grained object distinction between the encoder and the LLM and warrants (pre)training data with more fine-grained annotation. We release our code at <a href=ANONYMIZED class=acl-markup-url>ANONYMIZED</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.155.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.155/>Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models</a></strong><br><a href=/people/h/hongbang-yuan/>Hongbang Yuan</a>
|
<a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/z/zhuoran-jin/>Zhuoran Jin</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/d/daojian-zeng/>Daojian Zeng</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--155><div class="card-body p-3 small">Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose <b>FAITH</b> (<b>F</b>alse premise <b>A</b>ttention head constra<b>I</b>ining for mi<b>T</b>igating <b>H</b>allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately 1% of the attention heads in the model yields a notable increase of nearly 20% of model performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.156.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--156 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.156/>To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models</a></strong><br><a href=/people/b/bastien-lietard/>Bastien Liétard</a>
|
<a href=/people/p/pascal-denis/>Pascal Denis</a>
|
<a href=/people/m/mikaela-keller/>Mikaela Keller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--156><div class="card-body p-3 small">Polysemy and synonymy are two crucial interrelated facets of lexicalambiguity. While both phenomena are widely documented in lexical resources and have been studied extensively in NLP,leading to dedicated systems, they are often being consideredindependently in practictal problems. While many tasks dealing with polysemy (e.g. Word SenseDisambiguiation or Induction) highlight the role of word’s senses,the study of synonymy is rooted in the study of concepts, i.e. meaningsshared across the lexicon. In this paper, we introduce ConceptInduction, the unsupervised task of learning a soft clustering amongwords that defines a set of concepts directly from data. This taskgeneralizes Word Sense Induction. We propose a bi-levelapproach to Concept Induction that leverages both a locallemma-centric view and a global cross-lexicon view to induceconcepts. We evaluate the obtained clustering on SemCor’s annotateddata and obtain good performance (BCubed F1 above0.60). We find that the local and the global levels are mutuallybeneficial to induce concepts and also senses in our setting. Finally,we create static embeddings representing our induced concepts and usethem on the Word-in-Context task, obtaining competitive performancewith the State-of-the-Art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.157.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.157/><span class=acl-fixed-case>ASETF</span>: A Novel Method for Jailbreak Attack on <span class=acl-fixed-case>LLM</span>s through Translate Suffix Embeddings</a></strong><br><a href=/people/h/hao-wang/>Hao Wang</a>
|
<a href=/people/h/hao-li/>Hao Li</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/l/lei-sha/>Lei Sha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--157><div class="card-body p-3 small">The safety defense methods of Large language models (LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. However, similar to traditional text adversarial attacks, this approach, while effective, is limited by the challenge of the discrete tokens. This gradient based discrete optimization attack requires over 100,000 LLM calls, and due to the unreadable of adversarial suffixes, it can be relatively easily penetrated by common defense methods such as perplexity filters.To cope with this challenge, in this paper, we propose an Adversarial Suffix Embedding Translation Framework (ASETF), aimed at transforming continuous adversarial suffix embeddings into coherent and understandable text. This method greatly reduces the computational overhead during the attack process and helps to automatically generate multiple adversarial samples, which can be used as data to strengthen LLM’s security defense. Experimental evaluations were conducted on Llama2, Vicuna, and other prominent LLMs, employing harmful directives sourced from the Advbench dataset.The results indicate that our method significantly reduces the computation time of adversarial suffixes and achieves a much better attack success rate than existing techniques, while significantly enhancing the textual fluency of the prompts. In addition, our approach can be generalized into a broader method for generating transferable adversarial suffixes that can successfully attack multiple LLMs, even black-box LLMs, such as ChatGPT and Gemini.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.158.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--158 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.158.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.158/>An Electoral Approach to Diversify <span class=acl-fixed-case>LLM</span>-based Multi-Agent Collective Decision-Making</a></strong><br><a href=/people/x/xiutian-zhao/>Xiutian Zhao</a>
|
<a href=/people/k/ke-wang/>Ke Wang</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--158><div class="card-body p-3 small">Modern large language models (LLMs) have exhibited cooperative synergy on complex task-solving, and collective decision-making (CDM) is a pivotal component in LLM-based multi-agent collaboration frameworks. Our survey on 52 recent such systems uncovers a severe lack of diversity, with a heavy reliance on dictatorial and plurality voting for CDM. Through the lens of social choice theory, we scrutinize widely-adopted CDM methods and identify their limitations. To enrich current landscape of LLM-based CDM, we present GEDI, an electoral CDM module that incorporates various ordinal preferential voting mechanisms. Our empirical case study across three benchmarks shows that the integration of certain CDM methods can markedly improve the reasoning capabilities and robustness of some leading LLMs, all without requiring intricate system designs. Additionally, we find that some CDM mechanisms generate positive synergies even with as few as three agents. The voting-based methods also demonstrate robustness against single points of failure, as well as diversity in terms of hit-rate@k and subject-wise impacts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.159.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--159 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.159/>Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?</a></strong><br><a href=/people/g/gregor-geigle/>Gregor Geigle</a>
|
<a href=/people/r/radu-timofte/>Radu Timofte</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--159><div class="card-body p-3 small">Large vision-language models (LVLMs) have recently dramatically pushed the state of the art in image captioning and many image understanding tasks (e.g., visual question answering). LVLMs, however, often <i>hallucinate</i> and produce captions that mention concepts that cannot be found in the image. These hallucinations erode the trustworthiness of LVLMs and are arguably among the main obstacles to their ubiquitous adoption. Recent work suggests that addition of grounding objectives—those that explicitly align image regions or objects to text spans—reduces the amount of LVLM hallucination. Although intuitive, this claim is not empirically justified as the reduction effects have been established, we argue, with flawed evaluation protocols that (i) rely on data (i.e., MSCOCO) that has been extensively used in LVLM training and (ii) measure hallucination via question answering rather than open-ended caption generation.In this work, in contrast, we offer the first systematic analysis of the effect of fine-grained object grounding on LVLM hallucination under an evaluation protocol that more realistically captures LVLM hallucination in open generation. Our extensive experiments over three backbone LLMs reveal that grounding objectives have little to no effect on object hallucination in open caption generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.160.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.160/>Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment</a></strong><br><a href=/people/z/zhenyu-liu/>Zhenyu Liu</a>
|
<a href=/people/d/dongfang-li/>Dongfang Li</a>
|
<a href=/people/x/xinshuo-hu/>Xinshuo Hu</a>
|
<a href=/people/x/xinping-zhao/>Xinping Zhao</a>
|
<a href=/people/y/yibin-chen/>Yibin Chen</a>
|
<a href=/people/b/baotian-hu/>Baotian Hu</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--160><div class="card-body p-3 small">Recent studies have explored the working mechanisms of In-Context Learning (ICL). However, they mainly focus on classification and simple generation tasks, limiting their broader application to more complex generation tasks in practice. To address this gap, we investigate the impact of demonstrations on token representations within the practical alignment tasks. We find that the transformer embeds the task function learned from demonstrations into the separator token representation, which plays an important role in the generation of prior response tokens. Once the prior response tokens are determined, the demonstrations become redundant. Motivated by this finding, we propose an efficient Progressive In-Context Alignment (PICA) method consisting of two stages. In the first few-shot stage, the model generates several prior response tokens via standard ICL while concurrently extracting the ICL vector that stores the task function from the separator token representation. In the following zero-shot stage, this ICL vector guides the model to generate responses without further demonstrations. Extensive experiments demonstrate that our PICA not only surpasses vanilla ICL but also achieves comparable performance to other alignment tuning methods. The proposed training-free method reduces the time cost (e.g., 5.45×) with improved alignment performance (e.g., 6.57+). Consequently, our work highlights the application of ICL for alignment and calls for a deeper understanding of ICL for complex generations. The code will be available at https://github.com/HITsz-TMG/PICA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.161.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--161 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.161/><span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>DULA</span>: Mixture of Domain-Specific and Universal <span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>RA</span> for Multi-Task Learning</a></strong><br><a href=/people/y/yufei-ma/>Yufei Ma</a>
|
<a href=/people/z/zihan-liang/>Zihan Liang</a>
|
<a href=/people/h/huangyu-dai/>Huangyu Dai</a>
|
<a href=/people/b/ben-chen/>Ben Chen</a>
|
<a href=/people/d/dehong-gao/>Dehong Gao</a>
|
<a href=/people/z/zhuoran-ran/>Zhuoran Ran</a>
|
<a href=/people/w/wang-zihan/>Wang Zihan</a>
|
<a href=/people/l/linbo-jin/>Linbo Jin</a>
|
<a href=/people/w/wen-jiang/>Wen Jiang</a>
|
<a href=/people/g/guannan-zhang/>Guannan Zhang</a>
|
<a href=/people/x/xiaoyan-cai/>Xiaoyan Cai</a>
|
<a href=/people/l/libin-yang/>Libin Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--161><div class="card-body p-3 small">The growing demand for larger-scale models in the development of Large Language Models (LLMs) poses challenges for efficient training within limited computational resources. Traditional fine-tuning methods often exhibit instability in multi-task learning and rely heavily on extensive training resources. Here, we propose MoDULA (Mixture of Domain-Specific and Universal LoRA), a novel Parameter Efficient Fine-Tuning (PEFT) Mixture-of-Expert (MoE) paradigm for improved fine-tuning and parameter efficiency in multi-task learning. The paradigm effectively improves the multi-task capability of the model by training universal experts, domain-specific experts, and routers separately. MoDULA-Res is a new method within the MoDULA paradigm, which maintains the model’s general capability by connecting universal and task-specific experts through residual connections. The experimental results demonstrate that the overall performance of the MoDULA-Flan and MoDULA-Res methods surpasses that of existing fine-tuning methods on various LLMs. Notably, MoDULA-Res achieves more significant performance improvements in multiple tasks while reducing training costs by over 80% without losing general capability. Moreover, MoDULA displays flexible pluggability, allowing for the efficient addition of new tasks without retraining existing experts from scratch. This progressive training paradigm circumvents data balancing issues, enhancing training efficiency and model stability. Overall, MoDULA provides a scalable, cost-effective solution for fine-tuning LLMs with enhanced parameter efficiency and generalization capability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.162.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.162.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.162/>Message Passing on Semantic-Anchor-Graphs for Fine-grained Emotion Representation Learning and Classification</a></strong><br><a href=/people/p/pinyi-zhang/>Pinyi Zhang</a>
|
<a href=/people/j/jingyang-chen/>Jingyang Chen</a>
|
<a href=/people/j/junchen-shen/>Junchen Shen</a>
|
<a href=/people/z/zijie-zhai/>Zijie Zhai</a>
|
<a href=/people/p/ping-li/>Ping Li</a>
|
<a href=/people/j/jie-zhang/>Jie Zhang</a>
|
<a href=/people/k/kai-zhang/>Kai Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--162><div class="card-body p-3 small">Emotion classification has wide applications in education, robotics, virtual reality, etc. However, identifying subtle differences between fine-grained emotion categories remains challenging. Current methods typically aggregate numerous token embeddings of a sentence into a single vector, which, while being an efficient compressor, may not fully capture complex semantic and temporal distributions. To solve this problem, we propose SEmantic ANchor Graph Neural Networks (SEAN-GNN) for fine-grained emotion classification. It learns a group of representative, multi-faceted semantic anchors in the token embedding space: using these anchors as a global reference, any sentence can be projected onto them to form a “semantic-anchor graph”, with node attributes and edge weights quantifying the semantic and temporal information respectively. The graph structure is well aligned across sentences and, importantly, allows for generating comprehensive emotion representations regarding <span class=tex-math>K</span> different anchors. Message passing on this graph can further integrate and refine the learned features. Empirically, SEAN-GNN can generate meaningful semantic anchors and discriminative graph patterns for different emotion, with promising classification results on 6 popular benchmark datasets against state-of-the-arts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.163.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--163 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.163/><span class=acl-fixed-case>P</span>hilo<span class=acl-fixed-case>GPT</span>: A Philology-Oriented Large Language Model for <span class=acl-fixed-case>A</span>ncient <span class=acl-fixed-case>C</span>hinese Manuscripts with Dunhuang as Case Study</a></strong><br><a href=/people/y/yuqing-zhang/>Yuqing Zhang</a>
|
<a href=/people/b/baoyi-he/>Baoyi He</a>
|
<a href=/people/y/yihan-chen/>Yihan Chen</a>
|
<a href=/people/h/hangqi-li/>Hangqi Li</a>
|
<a href=/people/h/han-yue/>Han Yue</a>
|
<a href=/people/s/shengyu-zhang/>Shengyu Zhang</a>
|
<a href=/people/h/huaiyong-dou/>Huaiyong Dou</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a>
|
<a href=/people/z/zemin-liu/>Zemin Liu</a>
|
<a href=/people/y/yongquan-zhang/>Yongquan Zhang</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--163><div class="card-body p-3 small">Philology, the study of ancient manuscripts, demands years of professional training in ex-tensive knowledge memorization and manual textual retrieval. Despite these requirements align closely with strengths of recent successful Large Language Models (LLMs), the scarcity of high-quality, specialized training data has hindered direct applications. To bridge this gap, we curated the PhiloCorpus-ZH, a rich collec-tion of ancient Chinese texts spanning a millen-nium with 30 diverse topics, including firsthand folk copies. This corpus facilitated the develop-ment of PhiloGPT, the first LLM tailored for discovering ancient Chinese manuscripts. To effectively tackle complex philological tasks like restoration, attribution, and linguistic anal-ysis, we introduced the PhiloCoP framework. Modeled on the analytical patterns of philol-ogists, PhiloCoP enhances LLM’s handling of historical linguistic peculiarities such as phonetic loans, polysemy, and syntactic inver-sions. We further integrated these tasks into the PhiloBenchmark, establishing a new standard for evaluating ancient Chinese LLMs address-ing philology tasks. Deploying PhiloGPT in practical scenarios has enabled Dunhuang spe-cialists to resolve philology tasks, such as iden-tifying duplication of copied text and assisting archaeologists with text completion, demon-strating its potential in real-world applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.164.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.164/>Alignment-Enhanced Decoding: Defending Jailbreaks via Token-Level Adaptive Refining of Probability Distributions</a></strong><br><a href=/people/q/quan-liu/>Quan Liu</a>
|
<a href=/people/z/zhenhong-zhou/>Zhenhong Zhou</a>
|
<a href=/people/l/longzhu-he/>Longzhu He</a>
|
<a href=/people/y/yi-liu/>Yi Liu</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/s/sen-su/>Sen Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--164><div class="card-body p-3 small">Large language models are susceptible to jailbreak attacks, which can result in the generation of harmful content. While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures. In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues. We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits. Then, AED adaptively combines Competitive Index and post-alignment logits with the original logits to obtain harmless and helpful distributions. Consequently, our method enhances safety alignment while maintaining helpfulness. We conduct experiments across five models and four common jailbreaks, with the results validating the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.165.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--165 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.165.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.165/><span class=acl-fixed-case>M</span>ini<span class=acl-fixed-case>C</span>on<span class=acl-fixed-case>GTS</span>: A Near Ultimate Minimalist Contrastive Grid Tagging Scheme for Aspect Sentiment Triplet Extraction</a></strong><br><a href=/people/q/qiao-sun/>Qiao Sun</a>
|
<a href=/people/l/liujia-yang/>Liujia Yang</a>
|
<a href=/people/m/minghao-ma/>Minghao Ma</a>
|
<a href=/people/n/nanyang-ye/>Nanyang Ye</a>
|
<a href=/people/q/qinying-gu/>Qinying Gu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--165><div class="card-body p-3 small">Aspect Sentiment Triplet Extraction (ASTE) aims to co-extract the sentiment triplets in a given corpus. Existing approaches within the pretraining-finetuning paradigm tend to either meticulously craft complex tagging schemes and classification heads, or incorporate external semantic augmentation to enhance performance. In this study, we, for the first time, re-evaluate the redundancy in tagging schemes and the internal enhancement in pretrained representations. We propose a method to improve and utilize pretrained representations by integrating a minimalist tagging scheme and a novel token-level contrastive learning strategy. The proposed approach demonstrates comparable or superior performance compared to state-of-the-art techniques while featuring a more compact design and reduced computational overhead. Additionally, we are the first to formally evaluate GPT-4’s performance in few-shot learning and Chain-of-Thought scenarios for this task. The results demonstrate that the pretraining-finetuning paradigm remains highly effective even in the era of large language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.166.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--166 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.166.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.166/>Evaluating Large Language Models via Linguistic Profiling</a></strong><br><a href=/people/a/alessio-miaschi/>Alessio Miaschi</a>
|
<a href=/people/f/felice-dellorletta/>Felice Dell’Orletta</a>
|
<a href=/people/g/giulia-venturi/>Giulia Venturi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--166><div class="card-body p-3 small">Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models’ linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs’ sentence generation abilities under specific linguistic constraints. Drawing on the ‘linguistic profiling’ approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.167.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--167 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.167/>With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models</a></strong><br><a href=/people/t/tyler-loakman/>Tyler Loakman</a>
|
<a href=/people/y/yucheng-li/>Yucheng Li</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--167><div class="card-body p-3 small">Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have demonstrated aptitude as potential substitutes for human participants in experiments testing psycholinguistic phenomena. However, an understudied question is to what extent models that only have access to vision and text modalities are able to implicitly understand sound-based phenomena via abstract reasoning from orthography and imagery alone. To investigate this, we analyse the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise a non-arbitrary link between sounds and concepts) as well as their ability to “hear” via the interplay of the language and vision modules of open and closed-source multimodal models. We perform multiple experiments, including replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism tasks and comparing human judgements of linguistic iconicity with that of LLMs. Our results show that VLMs demonstrate varying levels of agreement with human labels, and more task information may be required for VLMs versus their human counterparts for <i>in silico</i> experimentation. We additionally see through higher maximum agreement levels that Magnitude Symbolism is an easier pattern for VLMs to identify than Shape Symbolism, and that an understanding of linguistic iconicity is highly dependent on model size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.168.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--168 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.168/><span class=acl-fixed-case>KB</span>-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases</a></strong><br><a href=/people/j/jiajie-zhang/>Jiajie Zhang</a>
|
<a href=/people/s/shulin-cao/>Shulin Cao</a>
|
<a href=/people/l/linmei-hu/>Linmei Hu</a>
|
<a href=/people/l/ling-feng/>Ling Feng</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--168><div class="card-body p-3 small">Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of a given KB, and is thus challenging for many low-resourced KBs that lack annotated data. To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB. Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin. Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize the information to induce programs over this KB. Experiments show that KB-Plugin outperforms SoTA low-resourced PI methods with 25x smaller backbone LLM on both large-scale and domain-specific KBs, and even approaches the performance of supervised methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.169.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.169/>Understanding Higher-Order Correlations Among Semantic Components in Embeddings</a></strong><br><a href=/people/m/momose-oyama/>Momose Oyama</a>
|
<a href=/people/h/hiroaki-yamagiwa/>Hiroaki Yamagiwa</a>
|
<a href=/people/h/hidetoshi-shimodaira/>Hidetoshi Shimodaira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--169><div class="card-body p-3 small">Independent Component Analysis (ICA) offers interpretable semantic components of embeddings.While ICA theory assumes that embeddings can be linearly decomposed into independent components, real-world data often do not satisfy this assumption. Consequently, non-independencies remain between the estimated components, which ICA cannot eliminate. We quantified these non-independencies using higher-order correlations and demonstrated that when the higher-order correlation between two components is large, it indicates a strong semantic association between them, along with many words sharing common meanings with both components. The entire structure of non-independencies was visualized using a maximum spanning tree of semantic components. These findings provide deeper insights into embeddings through ICA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.170.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.170/><span class=acl-fixed-case>DGLF</span>: A Dual Graph-based Learning Framework for Multi-modal Sarcasm Detection</a></strong><br><a href=/people/z/zhihong-zhu/>Zhihong Zhu</a>
|
<a href=/people/k/kefan-shen/>Kefan Shen</a>
|
<a href=/people/z/zhaorun-chen/>Zhaorun Chen</a>
|
<a href=/people/y/yunyan-zhang/>Yunyan Zhang</a>
|
<a href=/people/y/yuyan-chen/>Yuyan Chen</a>
|
<a href=/people/x/xiaoqi-jiao/>Xiaoqi Jiao</a>
|
<a href=/people/z/zhongwei-wan/>Zhongwei Wan</a>
|
<a href=/people/s/shaorong-xie/>Shaorong Xie</a>
|
<a href=/people/w/wei-liu/>Wei Liu</a>
|
<a href=/people/x/xian-wu/>Xian Wu</a>
|
<a href=/people/y/yefeng-zheng/>Yefeng Zheng</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.171.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.171.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.171/>Evaluating <span class=acl-fixed-case>D</span>-<span class=acl-fixed-case>MERIT</span> of Partial-annotation on Information Retrieval</a></strong><br><a href=/people/r/royi-rassin/>Royi Rassin</a>
|
<a href=/people/y/yaron-fairstein/>Yaron Fairstein</a>
|
<a href=/people/o/oren-kalinsky/>Oren Kalinsky</a>
|
<a href=/people/g/guy-kushilevitz/>Guy Kushilevitz</a>
|
<a href=/people/n/nachshon-cohen/>Nachshon Cohen</a>
|
<a href=/people/a/alexander-libov/>Alexander Libov</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.172.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--172 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.172.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.172/>Verification and Refinement of Natural Language Explanations through <span class=acl-fixed-case>LLM</span>-Symbolic Theorem Proving</a></strong><br><a href=/people/x/xin-quan/>Xin Quan</a>
|
<a href=/people/m/marco-valentino/>Marco Valentino</a>
|
<a href=/people/l/louise-a-dennis/>Louise A. Dennis</a>
|
<a href=/people/a/andre-freitas/>Andre Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--172><div class="card-body p-3 small">Natural language explanations represent a proxy for evaluating explanation-based and multi-step Natural Language Inference (NLI) models. However, assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets, a process that is time-consuming and prone to logical errors. To address existing limitations, this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs). Specifically, we present a neuro-symbolic framework, named Explanation-Refiner, that integrates TPs with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI. In turn, the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning, autoformalisation, and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of explanations of variable complexity in different domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.173.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--173 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.173/>Calibrating the Confidence of Large Language Models by Eliciting Fidelity</a></strong><br><a href=/people/m/mozhi-zhang/>Mozhi Zhang</a>
|
<a href=/people/m/mianqiu-huang/>Mianqiu Huang</a>
|
<a href=/people/r/rundong-shi/>Rundong Shi</a>
|
<a href=/people/l/linsen-guo/>Linsen Guo</a>
|
<a href=/people/c/chong-peng/>Chong Peng</a>
|
<a href=/people/p/peng-yan/>Peng Yan</a>
|
<a href=/people/y/yaqian-zhou/>Yaqian Zhou</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--173><div class="card-body p-3 small">Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the <i>Uncertainty</i> about the question and the <i>Fidelity</i> to the answer generated by language models. Then, we propose a plug-and-play method, <i>UF Calibration</i>, to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on <i>Truly Well-Calibrated Confidence</i> for large language models. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.174.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--174 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.174/>The Accuracy Paradox in <span class=acl-fixed-case>RLHF</span>: When Better Reward Models Don’t Yield Better Language Models</a></strong><br><a href=/people/y/yanjun-chen/>Yanjun Chen</a>
|
<a href=/people/d/dawei-zhu/>Dawei Zhu</a>
|
<a href=/people/y/yirong-sun/>Yirong Sun</a>
|
<a href=/people/x/xinghao-chen/>Xinghao Chen</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--174><div class="card-body p-3 small">Reinforcement Learning from Human Feedback significantly enhances Natural Language Processing by aligning language models with human expectations. A critical factor in this alignment is the strength of reward models used during training. This study explores whether stronger reward models invariably lead to better language models. In this paper, through experiments on relevance, factuality, and completeness tasks using the QA-FEEDBACK dataset and reward models based on Longformer, we uncover a surprising paradox: language models trained with moderately accurate reward models outperform those guided by highly accurate ones. This challenges the widely held belief that stronger reward models always lead to better language models, and opens up new avenues for future research into the key factors driving model performance and how to choose the most suitable reward models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.175.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--175 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.175/>How Hard is this Test Set? <span class=acl-fixed-case>NLI</span> Characterization by Exploiting Training Dynamics</a></strong><br><a href=/people/a/adrian-cosma/>Adrian Cosma</a>
|
<a href=/people/s/stefan-ruseti/>Stefan Ruseti</a>
|
<a href=/people/m/mihai-dascalu/>Mihai Dascalu</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--175><div class="card-body p-3 small">Natural Language Inference (NLI) evaluation is crucial for assessing language understanding models; however, popular datasets suffer from systematic spurious correlations that artificially inflate actual model performance. To address this, we propose a method for the automated creation of a challenging test set without relying on the manual construction of artificial and unrealistic examples. We categorize the test set of popular NLI datasets into three difficulty levels by leveraging methods that exploit training dynamics. This categorization significantly reduces spurious correlation measures, with examples labeled as having the highest difficulty showing markedly decreased performance and encompassing more realistic and diverse linguistic phenomena. When our characterization method is applied to the training set, models trained with only a fraction of the data achieve comparable performance to those trained on the full dataset, surpassing other dataset characterization techniques. Our research addresses limitations in NLI dataset construction, providing a more authentic evaluation of model performance with implications for diverse NLU applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.176.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.176/>Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection</a></strong><br><a href=/people/g/gaetan-lopez-latouche/>Gaetan Lopez Latouche</a>
|
<a href=/people/m/marc-andre-carbonneau/>Marc-André Carbonneau</a>
|
<a href=/people/b/ben-swanson/>Benjamin Swanson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--176><div class="card-body p-3 small">Grammatical Error Detection (GED) methods rely heavily on human annotated error corpora. However, these annotations are unavailable in many low-resource languages. In this paper, we investigate GED in this context. Leveraging the zero-shot cross-lingual transfer capabilities of multilingual pre-trained language models, we train a model using data from a diverse set of languages to generate synthetic errors in other languages. These synthetic error corpora are then used to train a GED model. Specifically we propose a two-stage fine-tuning pipeline where the GED model is first fine-tuned on multilingual synthetic data from target languages followed by fine-tuning on human-annotated GED corpora from source languages. This approach outperforms current state-of-the-art annotation-free GED methods. We also analyse the errors produced by our method and other strong baselines, finding that our approach produces errors that are more diverse and more similar to human errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.177.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--177 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.177/><span class=acl-fixed-case>CUTE</span>: Measuring <span class=acl-fixed-case>LLM</span>s’ Understanding of Their Tokens</a></strong><br><a href=/people/l/lukas-edman/>Lukas Edman</a>
|
<a href=/people/h/helmut-schmid/>Helmut Schmid</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--177><div class="card-body p-3 small">Large Language Models (LLMs) show remarkable performance on a wide variety of tasks. Most LLMs split text into multi-character tokens and process them as atomic units without direct access to individual characters. This raises the question: To what extent can LLMs learn orthographic information? To answer this, we propose a new benchmark, CUTE, which features a collection of tasks designed to test the orthographic knowledge of LLMs. We evaluate popular LLMs on CUTE, finding that most of them seem to know the spelling of their tokens, yet fail to use this information effectively to manipulate text, calling into question how much of this knowledge is generalizable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.178.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--178 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.178/><span class=acl-fixed-case>SEER</span>: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</a></strong><br><a href=/people/x/xinping-zhao/>Xinping Zhao</a>
|
<a href=/people/d/dongfang-li/>Dongfang Li</a>
|
<a href=/people/y/yan-zhong/>Yan Zhong</a>
|
<a href=/people/b/boren-hu/>Boren Hu</a>
|
<a href=/people/y/yibin-chen/>Yibin Chen</a>
|
<a href=/people/b/baotian-hu/>Baotian Hu</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--178><div class="card-body p-3 small">Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at https://github.com/HITsz-TMG/SEER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.179.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--179 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.179.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.179.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.179/>On the Role of Context in Reading Time Prediction</a></strong><br><a href=/people/a/andreas-opedal/>Andreas Opedal</a>
|
<a href=/people/e/eleanor-chodroff/>Eleanor Chodroff</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/e/ethan-wilcox/>Ethan Wilcox</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--179><div class="card-body p-3 small">We present a new perspective on how readers integrate context during real-time language comprehension. Our proposals build on surprisal theory, which posits that the processing effort of a linguistic unit (e.g., a word) is an affine function of its in-context information content. We first observe that surprisal is only one out of many potential ways that a contextual predictor can be derived from a language model. Another one is the pointwise mutual information (PMI) between a unit and its context, which turns out to yield the same predictive power as surprisal when controlling for unigram frequency. Moreover, both PMI and surprisal are correlated with frequency. This means that neither PMI nor surprisal contains information about context alone. In response to this, we propose a technique where we project surprisal onto the orthogonal complement of frequency, yielding a new contextual predictor that is uncorrelated with frequency. Our experiments show that the proportion of variance in reading times explained by context is a lot smaller when context is represented by the orthogonalized predictor. From an interpretability standpoint, this indicates that previous studies may have overstated the role that context has in predicting reading times.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.180.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--180 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.180.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.180.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.180/><span class=acl-fixed-case>BC</span>-Prover: Backward Chaining Prover for Formal Theorem Proving</a></strong><br><a href=/people/y/yuhang-he/>Yuhang He</a>
|
<a href=/people/j/jihai-zhang/>Jihai Zhang</a>
|
<a href=/people/j/jianzhu-bao/>Jianzhu Bao</a>
|
<a href=/people/f/fangquan-lin/>Fangquan Lin</a>
|
<a href=/people/c/cheng-yang/>Cheng Yang</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a>
|
<a href=/people/w/wotao-yin/>Wotao Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--180><div class="card-body p-3 small">Despite the remarkable progress made by large language models in mathematical reasoning, interactive theorem proving in formal logic still remains a prominent challenge. Previous methods resort to neural models for proofstep generation and search. However, they suffer from exploring possible proofsteps empirically in a large search space. Moreover, they directly use a less rigorous informal proof for proofstep generation, neglecting the incomplete reasoning within. In this paper, we propose BC-Prover, a backward chaining framework guided by pseudo steps. Specifically, BC-Prover prioritizes pseudo steps to proofstep generation. The pseudo steps boost the proof construction in two aspects: (1) Backward Chaining that decomposes the proof into sub-goals for goal-oriented exploration. (2) Step Planning that makes a fine-grained planning to bridge the gap between informal and formal proofs. Experiments on the miniF2F benchmark show significant performance gains by our framework over the state-of-the-art approaches. Our framework is also compatible with existing provers and further improves their performance with the backward chaining technique.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.181.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.181/>From Insights to Actions: The Impact of Interpretability and Analysis Research on <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/m/marius-mosbach/>Marius Mosbach</a>
|
<a href=/people/v/vagrant-gautam/>Vagrant Gautam</a>
|
<a href=/people/t/tomas-vergara-browne/>Tomás Vergara Browne</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/m/mor-geva/>Mor Geva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--181><div class="card-body p-3 small">Interpretability and analysis (IA) research is a growing subfield within NLP with the goal of developing a deeper understanding of the behavior or inner workings of NLP systems and methods. Despite growing interest in the subfield, a criticism of this work is that it lacks actionable insights and therefore has little impact on NLP. In this paper, we seek to quantify the impact of IA research on the broader field of NLP. We approach this with a mixed-methods analysis of: (1) a citation graph of 185K+ papers built from all papers published at ACL and EMNLP conferences from 2018 to 2023, and their references and citations, and (2) a survey of 138 members of the NLP community. Our quantitative results show that IA work is well-cited outside of IA, and central in the NLP citation graph. Through qualitative analysis of survey responses and manual annotation of 556 papers, we find that NLP researchers build on findings from IA work and perceive it as important for progress in NLP, multiple subfields, and rely on its findings and terminology for their own work. Many novel methods are proposed based on IA findings and highly influenced by them, but highly influential non-IA work cites IA findings without being driven by them. We end by summarizing what is missing in IA work today and provide a call to action, to pave the way for a more impactful future of IA research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.182.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--182 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.182/>Autoregressive Pre-Training on Pixels and Texts</a></strong><br><a href=/people/y/yekun-chai/>Yekun Chai</a>
|
<a href=/people/q/qingyi-liu/>Qingyi Liu</a>
|
<a href=/people/j/jingwu-xiao/>Jingwu Xiao</a>
|
<a href=/people/s/shuohuan-wang/>Shuohuan Wang</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--182><div class="card-body p-3 small">The integration of visual and textual information represents a promising direction in the advancement of language models. In this paper, we explore the dual modality of language—both visual and textual—within an autoregressive framework, pre-trained on both document images and texts. Our method employs a multimodal training strategy, utilizing visual data through next patch prediction with a regression head and/or textual data through next token prediction with a classification head. We focus on understanding the interaction between these two modalities and their combined impact on model performance. Our extensive evaluation across a wide range of benchmarks shows that incorporating both visual and textual data significantly improves the performance of pixel-based language models. Remarkably, we find that a unidirectional pixel-based model trained solely on visual data can achieve comparable results to state-of-the-art bidirectional models on several language understanding tasks. This work uncovers the untapped potential of integrating visual and textual modalities for more effective language modeling. We release our code, data, and model checkpoints at https://github.com/ernie-research/pixelgpt.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.183.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--183 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.183/>On Training Data Influence of <span class=acl-fixed-case>GPT</span> Models</a></strong><br><a href=/people/y/yekun-chai/>Yekun Chai</a>
|
<a href=/people/q/qingyi-liu/>Qingyi Liu</a>
|
<a href=/people/s/shuohuan-wang/>Shuohuan Wang</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a>
|
<a href=/people/q/qiwei-peng/>Qiwei Peng</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--183><div class="card-body p-3 small">Amidst the rapid advancements in generative language models, the investigation of how training data shapes the performance of GPT models is still emerging. This paper presents GPTfluence, a novel approach that leverages a featurized simulation to assess the impact of training examples on the training dynamics of GPT models. Our approach not only traces the influence of individual training instances on performance trajectories, such as loss and other key metrics, on targeted test points but also enables a comprehensive comparison with existing methods across various training scenarios in GPT models, ranging from 14 million to 2.8 billion parameters, across a range of downstream tasks. Contrary to earlier methods that struggle with generalization to new data, GPTfluence introduces a parameterized simulation of training dynamics, demonstrating robust generalization capabilities to unseen training data. This adaptability is evident across both fine-tuning and instruction-tuning scenarios, spanning tasks in natural language understanding and generation. We make our code and data publicly available at https://github.com/ernie-research/gptfluence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.184.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.184/>Understanding “Democratization” in <span class=acl-fixed-case>NLP</span> and <span class=acl-fixed-case>ML</span> Research</a></strong><br><a href=/people/a/arjun-subramonian/>Arjun Subramonian</a>
|
<a href=/people/v/vagrant-gautam/>Vagrant Gautam</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/z/zeerak-talat/>Zeerak Talat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--184><div class="card-body p-3 small">Recent improvements in natural language processing (NLP) and machine learning (ML) and increased mainstream adoption have led to researchers frequently discussing the “democratization” of artificial intelligence. In this paper, we seek to clarify how democratization is understood in NLP and ML publications, through large-scale mixed-methods analyses of papers using the keyword “democra*” published in NLP and adjacent venues. We find that democratization is most frequently used to convey (ease of) access to or use of technologies, without meaningfully engaging with theories of democratization, while research using other invocations of “democra*” tends to be grounded in theories of deliberation and debate. Based on our findings, we call for researchers to enrich their use of the term democratization with appropriate theory, towards democratic technologies beyond superficial access.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.185.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.185/><span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>KD</span>: Knowledge Distillation from <span class=acl-fixed-case>LLM</span>s for Open-World Document Understanding Models</a></strong><br><a href=/people/s/sungnyun-kim/>Sungnyun Kim</a>
|
<a href=/people/h/haofu-liao/>Haofu Liao</a>
|
<a href=/people/s/srikar-appalaraju/>Srikar Appalaraju</a>
|
<a href=/people/p/peng-tang/>Peng Tang</a>
|
<a href=/people/z/zhuowen-tu/>Zhuowen Tu</a>
|
<a href=/people/r/ravi-kumar-satzoda/>Ravi Kumar Satzoda</a>
|
<a href=/people/r/r-manmatha/>R. Manmatha</a>
|
<a href=/people/v/vijay-mahadevan/>Vijay Mahadevan</a>
|
<a href=/people/s/stefano-soatto/>Stefano Soatto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--185><div class="card-body p-3 small">Visual document understanding (VDU) is a challenging task that involves understanding documents across various modalities (text and image) and layouts (forms, tables, etc.). This study aims to enhance generalizability of small VDU models by distilling knowledge from LLMs. We identify that directly prompting LLMs often fails to generate informative and useful data. In response, we present a new framework (called DocKD) that enriches the data generation process by integrating external document knowledge. Specifically, we provide an LLM with various document elements like key-value pairs, layouts, and descriptions, to elicit open-ended answers. Our experiments show that DocKD produces high-quality document annotations and surpasses the direct knowledge distillation approach that does not leverage external document knowledge. Moreover, student VDU models trained with solely DocKD-generated data is not only comparable to those trained with human-annotated data on in-domain tasks but also significantly excel them on out-of-domain tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.186.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--186 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.186.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.186.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.186/>Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages</a></strong><br><a href=/people/s/seonjeong-hwang/>Seonjeong Hwang</a>
|
<a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/g/gary-lee/>Gary Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--186><div class="card-body p-3 small">Automatic question generation (QG) serves a wide range of purposes, such as augmenting question-answering (QA) corpora, enhancing chatbot systems, and developing educational materials. Despite its importance, most existing datasets predominantly focus on English, resulting in a considerable gap in data availability for other languages. Cross-lingual transfer for QG (XLT-QG) addresses this limitation by allowing models trained on high-resource language datasets to generate questions in low-resource languages. In this paper, we propose a simple and efficient XLT-QG method that operates without the need for monolingual, parallel, or labeled data in the target language, utilizing a small language model. Our model, trained solely on English QA datasets, learns interrogative structures from a limited set of question exemplars, which are then applied to generate questions in the target language. Experimental results show that our method outperforms several XLT-QG baselines and achieves performance comparable to GPT-3.5-turbo across different languages. Additionally, the synthetic data generated by our model proves beneficial for training multilingual QA models. With significantly fewer parameters than large language models and without requiring additional training for target languages, our approach offers an effective solution for QG and QA tasks across various languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.187.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--187 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.187/><span class=acl-fixed-case>S</span>caling<span class=acl-fixed-case>F</span>ilter: Assessing Data Quality through Inverse Utilization of Scaling Laws</a></strong><br><a href=/people/r/ruihang-li/>Ruihang Li</a>
|
<a href=/people/y/yixuan-wei/>Yixuan Wei</a>
|
<a href=/people/m/miaosen-zhang/>Miaosen Zhang</a>
|
<a href=/people/n/nenghai-yu/>Nenghai Yu</a>
|
<a href=/people/h/han-hu/>Han Hu</a>
|
<a href=/people/h/houwen-peng/>Houwen Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--187><div class="card-body p-3 small">High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In this paper, we propose ScalingFilter, a novel approach that evaluates text quality based on the perplexity difference between two language models trained on the same data, thereby eliminating the influence of the reference dataset in the filtering process. An theoretical analysis shows that ScalingFilter is equivalent to an inverse utilization of scaling laws. Through training models with 1.3B parameters on the same data source processed by various quality filters, we find ScalingFilter can improve zero-shot performance of pre-trained models in downstream tasks. To assess the bias introduced by quality filtering, we introduce semantic diversity, a metric of utilizing text embedding models for semantic representations. Extensive experiments reveal that semantic diversity is a reliable indicator of dataset diversity, and ScalingFilter achieves an optimal balance between downstream performance and semantic diversity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.188.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--188 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.188/>Word Alignment as Preference for Machine Translation</a></strong><br><a href=/people/q/qiyu-wu/>Qiyu Wu</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a>
|
<a href=/people/z/zhongtao-miao/>Zhongtao Miao</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--188><div class="card-body p-3 small">The problem of hallucination and omission, a long-standing problem in machine translation (MT), is more pronounced when a large language model (LLM) is used in MT because an LLM itself is susceptible to these phenomena. In this work, we mitigate the problem in an LLM-based MT model by guiding it to better word alignment. We first study the correlation between word alignment and the phenomena of hallucination and omission in MT. Then we propose to utilize word alignment as preference to optimize the LLM-based MT model. The preference data are constructed by selecting chosen and rejected translations from multiple MT tools. Subsequently, direct preference optimization is used to optimize the LLM-based model towards the preference signal. Given the absence of evaluators specifically designed for hallucination and omission in MT, we further propose selecting hard instances and utilizing GPT-4 to directly evaluate the performance of the models in mitigating these issues. We verify the rationality of these designed evaluation methods by experiments, followed by extensive results demonstrating the effectiveness of word alignment-based preference optimization to mitigate hallucination and omission. On the other hand, although it shows promise in mitigating hallucination and omission, the overall performance of MT in different language directions remains mixed, with slight increases in BLEU and decreases in COMET.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.189.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.189/>Improving Multi-party Dialogue Generation via Topic and Rhetorical Coherence</a></strong><br><a href=/people/y/yaxin-fan/>Yaxin Fan</a>
|
<a href=/people/p/peifeng-li/>Peifeng Li</a>
|
<a href=/people/q/qiaoming-zhu/>Qiaoming Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--189><div class="card-body p-3 small">Previous studies on multi-party dialogue generation predominantly concentrated on modeling the reply-to structure of dialogue histories, always overlooking the coherence between generated responses and target utterances. To address this issue, we propose a Reinforcement Learning approach emphasizing both Topic and Rhetorical Coherence (RL-TRC). In particular, the topic- and rhetorical-coherence tasks are designed to enhance the model’s perception of coherence with the target utterance. Subsequently, an agent is employed to learn a coherence policy, which guides the generation of responses that are topically and rhetorically aligned with the target utterance. Furthermore, three discourse-aware rewards are developed to assess the coherence between the generated response and the target utterance, with the objective of optimizing the policy. The experimental results and in-depth analyses on two popular datasets demonstrate that our RL-TRC significantly outperforms the state-of-the-art baselines, particularly in generating responses that are more coherent with the target utterances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.190.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--190 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.190/><span class=acl-fixed-case>SEEKR</span>: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models</a></strong><br><a href=/people/j/jinghan-he/>Jinghan He</a>
|
<a href=/people/h/haiyun-guo/>Haiyun Guo</a>
|
<a href=/people/k/kuan-zhu/>Kuan Zhu</a>
|
<a href=/people/z/zihan-zhao/>Zihan Zhao</a>
|
<a href=/people/m/ming-tang/>Ming Tang</a>
|
<a href=/people/j/jinqiao-wang/>Jinqiao Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--190><div class="card-body p-3 small">Continual learning (CL) is crucial for language models to dynamically adapt to the evolving real-world demands. To mitigate the catastrophic forgetting problem in CL, data replay has been proven a simple and effective strategy, and the subsequent data-replay-based distillation can further enhance the performance. However, existing methods fail to fully exploit the knowledge embedded in models from previous tasks, resulting in the need for a relatively large number of replay samples to achieve good results. In this work, we first explore and emphasize the importance of attention weights in knowledge retention, and then propose a SElective attEntion-guided Knowledge Retention method (SEEKR) for data-efficient replay-based continual learning of large language models (LLMs). Specifically, SEEKR performs attention distillation on the selected attention heads for finer-grained knowledge retention, where the proposed forgettability-based and task-sensitivity-based measures are used to identify the most valuable attention heads. Experimental results on two continual learning benchmarks for LLMs demonstrate the superiority of SEEKR over the existing methods on both performance and efficiency. Explicitly, SEEKR achieves comparable or even better performance with only 1/10 of the replayed data used by other methods, and reduces the proportion of replayed data to 1%. The code is available at https://github.com/jinghan1he/SEEKR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.191.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.191/>Neuron-Level Knowledge Attribution in Large Language Models</a></strong><br><a href=/people/z/zeping-yu/>Zeping Yu</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--191><div class="card-body p-3 small">Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify “value neurons” directly contributing to the final prediction, we propose a method for identifying “query neurons” which activate these “value neurons”. Finally, we apply our methods to analyze six types of knowledge across both attention and feed-forward network (FFN) layers. Our method and analysis are helpful for understanding the mechanisms of knowledge storage and set the stage for future research in knowledge editing. The code is available on https://github.com/zepingyu0512/neuron-attribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.192.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--192 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.192/>How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning</a></strong><br><a href=/people/z/zeping-yu/>Zeping Yu</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--192><div class="card-body p-3 small">We investigate the mechanism of in-context learning (ICL) on sentence classification tasks with semantically-unrelated labels (“foo”/“bar”). We find intervening in only 1% heads (named “in-context heads”) significantly affects ICL accuracy from 87.6% to 24.4%. To understand this phenomenon, we analyze the value-output vectors in these heads and discover that the vectors at each label position contain substantial information about the corresponding labels. Furthermore, we observe that the prediction shift from “foo” to “bar” is due to the respective reduction and increase in these heads’ attention scores at “foo” and “bar” positions. Therefore, we propose a hypothesis for ICL: in in-context heads, the value-output matrices extract label features, while the query-key matrices compute the similarity between the features at the last position and those at each label position. The query and key matrices can be considered as two towers that learn the similarity metric between the last position’s features and each demonstration at label positions. Using this hypothesis, we explain the majority label bias and recency bias in ICL and propose two methods to reduce these biases by 22% and 17%, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.193.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--193 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.193/>Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis</a></strong><br><a href=/people/z/zeping-yu/>Zeping Yu</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--193><div class="card-body p-3 small">We find arithmetic ability resides within a limited number of attention heads, with each head specializing in distinct operations. To delve into the reason, we introduce the Comparative Neuron Analysis (CNA) method, which identifies an internal logic chain consisting of four distinct stages from input to prediction: feature enhancing with shallow FFN neurons, feature transferring by shallow attention layers, feature predicting by arithmetic heads, and prediction enhancing among deep FFN neurons. Moreover, we identify the human-interpretable FFN neurons within both feature-enhancing and feature-predicting stages. These findings lead us to investigate the mechanism of LoRA, revealing that it enhances prediction probabilities by amplifying the coefficient scores of FFN neurons related to predictions. Finally, we apply our method in model pruning for arithmetic tasks and model editing for reducing gender bias. Code is on https://github.com/zepingyu0512/arithmetic-mechanism.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.194.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--194 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.194/>Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models</a></strong><br><a href=/people/k/kushal-tatariya/>Kushal Tatariya</a>
|
<a href=/people/v/vladimir-araujo/>Vladimir Araujo</a>
|
<a href=/people/t/thomas-bauwens/>Thomas Bauwens</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--194><div class="card-body p-3 small">Pixel-based language models have emerged as a compelling alternative to subword-based language modelling, particularly because they can represent virtually any script. PIXEL, a canonical example of such a model, is a vision transformer that has been pre-trained on rendered text. While PIXEL has shown promising cross-script transfer abilities and robustness to orthographic perturbations, it falls short of outperforming monolingual subword counterparts like BERT in most other contexts. This discrepancy raises questions about the amount of linguistic knowledge learnt by these models and whether their performance in language tasks stems more from their visual capabilities than their linguistic ones. To explore this, we probe PIXEL using a variety of linguistic and visual tasks to assess its position on the vision-to-language spectrum. Our findings reveal a substantial gap between the model’s visual and linguistic understanding. The lower layers of PIXEL predominantly capture superficial visual features, whereas the higher layers gradually learn more syntactic and semantic abstractions. Additionally, we examine variants of PIXEL trained with different text rendering strategies, discovering that introducing certain orthographic constraints at the input level can facilitate earlier learning of surface-level features. With this study, we hope to provide insights that aid the further development of pixel-based language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.195.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.195.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.195/><span class=acl-fixed-case>G</span>old<span class=acl-fixed-case>C</span>oin: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory</a></strong><br><a href=/people/w/wei-fan/>Wei Fan</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/z/zheye-deng/>Zheye Deng</a>
|
<a href=/people/w/weiqi-wang/>Weiqi Wang</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--195><div class="card-body p-3 small">Privacy issues arise prominently during the inappropriate transmission of information between entities. Existing research primarily studies privacy by exploring various privacy attacks, defenses, and evaluations within narrowly predefined patterns, while neglecting that privacy is not an isolated, context-free concept limited to traditionally sensitive data (e.g., social security numbers), but intertwined with intricate social contexts that complicate the identification and analysis of potential privacy violations. The advent of Large Language Models (LLMs) offers unprecedented opportunities for incorporating the nuanced scenarios outlined in privacy laws to tackle these complex privacy issues. However, the scarcity of open-source relevant case studies restricts the efficiency of LLMs in aligning with specific legal statutes. To address this challenge, we introduce a novel framework, GoldCoin, designed to efficiently ground LLMs in privacy laws for judicial assessing privacy violations. Our framework leverages the theory of contextual integrity as a bridge, creating numerous synthetic scenarios grounded in relevant privacy statutes (e.g., HIPAA), to assist LLMs in comprehending the complex contexts for identifying privacy risks in the real world. Extensive experimental results demonstrate that GoldCoin markedly enhances LLMs’ capabilities in recognizing privacy risks across real court cases, surpassing the baselines on different judicial tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.196.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.196.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--196 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.196 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.196/>Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in <span class=acl-fixed-case>D</span>anish and <span class=acl-fixed-case>N</span>orwegian Literature</a></strong><br><a href=/people/a/ali-al-laith/>Ali Al-Laith</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/j/jens-bjerring-hansen/>Jens Bjerring-Hansen</a>
|
<a href=/people/j/jakob-ingemann-parby/>Jakob Ingemann Parby</a>
|
<a href=/people/a/alexander-conroy/>Alexander Conroy</a>
|
<a href=/people/t/timothy-r-tangherlini/>Timothy R Tangherlini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--196><div class="card-body p-3 small">We present a framework for detecting and categorizing noise in literary texts, demonstrated through its application to Danish and Norwegian literature from the late 19-th century. Noise, understood as “aberrant sonic behaviour,” is not only an auditory phenomenon but also a cultural construct tied to the processes of civilization and urbanization.We begin by utilizing topic modeling techniques to identify noise-related documents, followed by fine-tuning BERT-based language models trained on Danish and Norwegian texts to analyze a corpus of over 800 novels.We identify and track the prevalence of noise in these texts, offering insights into the literary perceptions of noise during the Scandinavian “Modern Breakthrough” period (1870-1899). Our contributions include the development of a comprehensive dataset annotated for noise-related segments and their categorization into human-made, non-human-made, and musical noises. This study illustrates the framework’s potential for enhancing the understanding of the relationship between noise and its literary representations, providing a deeper appreciation of the auditory elements in literary works, including as sources for cultural history.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.197.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.197/><span class=acl-fixed-case>QUIK</span>: Towards End-to-end 4-Bit Inference on Generative Large Language Models</a></strong><br><a href=/people/s/saleh-ashkboos/>Saleh Ashkboos</a>
|
<a href=/people/i/ilia-markov/>Ilia Markov</a>
|
<a href=/people/e/elias-frantar/>Elias Frantar</a>
|
<a href=/people/t/tingxuan-zhong/>Tingxuan Zhong</a>
|
<a href=/people/x/xincheng-wang/>Xincheng Wang</a>
|
<a href=/people/j/jie-ren/>Jie Ren</a>
|
<a href=/people/t/torsten-hoefler/>Torsten Hoefler</a>
|
<a href=/people/d/dan-alistarh/>Dan Alistarh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--197><div class="card-body p-3 small">Large Language Models (LLMs) from the GPT family have become extremely popular, leading to a race towards reducing their inference costs to allow for efficient local computation. However, the vast majority of existing work focuses on weight-only quantization, which can reduce runtime costs in the memory-bound one-token-at-a-time generative setting, but does not address costs in compute-bound scenarios, such as batched inference or prompt processing.In this paper, we address the general quantization problem, where <i>both weights and activations</i> should be quantized, which leads to computational improvements in general. We show that the majority of inference computations for large generative models can be performed with both weights and activations being cast to 4 bits, while at the same time maintaining good accuracy. We achieve this via a hybrid quantization strategy called QUIK that compresses most of the weights and activations to 4-bit, while keeping a small fraction of “outlier” weights and activations in higher-precision. QUIK is that it is designed with computational efficiency in mind: we provide GPU kernels matching the QUIK format with highly-efficient layer-wise runtimes, which lead to practical end-to-end throughput improvements of up to 3.4x relative to FP16 execution. We provide detailed studies for models from the OPT, LLaMA-2 and Falcon families, as well as a first instance of accurate inference using quantization plus 2:4 sparsity.Anonymized code is available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.198.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--198 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.198.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.198/>Fine-Grained Prediction of Reading Comprehension from Eye Movements</a></strong><br><a href=/people/o/omer-shubi/>Omer Shubi</a>
|
<a href=/people/y/yoav-meiri/>Yoav Meiri</a>
|
<a href=/people/c/cfir-avraham-hadar/>Cfir Avraham Hadar</a>
|
<a href=/people/y/yevgeni-berzak/>Yevgeni Berzak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--198><div class="card-body p-3 small">Can human reading comprehension be assessed from eye movements in reading? In this work, we address this longstanding question using large-scale eyetracking data. We focus on a cardinal and largely unaddressed variant of this question: predicting reading comprehension of a single participant for a single question from their eye movements over a single paragraph. We tackle this task using a battery of recent models from the literature, and three new multimodal language models. We evaluate the models in two different reading regimes: ordinary reading and information seeking, and examine their generalization to new textual items, new participants, and the combination of both. The evaluations suggest that the task is highly challenging, and highlight the importance of benchmarking against a strong text-only baseline. While in some cases eye movements provide improvements over such a baseline, they tend to be small. This could be due to limitations of current modelling approaches, limitations of the data, or because eye movement behavior does not sufficiently pertain to fine-grained aspects of reading comprehension processes. Our study provides an infrastructure for making further progress on this question.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.199.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.199/><span class=acl-fixed-case>E</span>fficient<span class=acl-fixed-case>RAG</span>: Efficient Retriever for Multi-Hop Question Answering</a></strong><br><a href=/people/z/ziyuan-zhuang/>Ziyuan Zhuang</a>
|
<a href=/people/z/zhiyang-zhang/>Zhiyang Zhang</a>
|
<a href=/people/s/sitao-cheng/>Sitao Cheng</a>
|
<a href=/people/f/fangkai-yang/>Fangkai Yang</a>
|
<a href=/people/j/jia-liu/>Jia Liu</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/q/qingwei-lin/>Qingwei Lin</a>
|
<a href=/people/s/saravan-rajmohan/>Saravan Rajmohan</a>
|
<a href=/people/d/dongmei-zhang/>Dongmei Zhang</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--199><div class="card-body p-3 small">Retrieval-augmented generation (RAG) methods encounter difficulties when addressing complex questions like multi-hop queries.While iterative retrieval methods improve performance by gathering additional information, current approaches often rely on multiple calls of large language models (LLMs).In this paper, we introduce EfficientRAG, an efficient retriever for multi-hop question answering.EfficientRAG iteratively generates new queries without the need for LLM calls at each iteration and filters out irrelevant information.Experimental results demonstrate that EfficientRAG surpasses existing RAG methods on three open-domain multi-hop question-answering datasets.The code is available in [aka.ms/efficientrag](https://github.com/NIL-zhuang/EfficientRAG-official).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.200.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--200 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.200.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.200.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.200/>Unsupervised Human Preference Learning</a></strong><br><a href=/people/s/sumuk-shashidhar/>Sumuk Shashidhar</a>
|
<a href=/people/a/abhinav-chinta/>Abhinav Chinta</a>
|
<a href=/people/v/vaibhav-sahai/>Vaibhav Sahai</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--200><div class="card-body p-3 small">Large language models demonstrate impressive reasoning abilities but struggle to provide personalized content due to their lack of individual user preference information. Existing methods, such as in-context learning and parameter-efficient fine-tuning, fall short in capturing the complexity of human preferences, especially given the small, personal datasets individuals possess. In this paper, we propose a novel approach utilizing small parameter models as preference agents to generate natural language rules that guide a larger, pre-trained model, enabling efficient personalization. Our method involves a small, local “steering wheel” model that directs the outputs of a much larger foundation model, producing content tailored to an individual’s preferences while leveraging the extensive knowledge and capabilities of the large model. Importantly, this personalization is achieved without the need to fine-tune the large model. Experimental results on email and article datasets, demonstrate that our technique significantly outperforms baseline personalization methods. By allowing foundation models to adapt to individual preferences in a data and compute-efficient manner, our approach paves the way for highly personalized language model applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.201.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--201 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.201.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.201/>Is Safer Better? The Impact of Guardrails on the Argumentative Strength of <span class=acl-fixed-case>LLM</span>s in Hate Speech Countering</a></strong><br><a href=/people/h/helena-bonaldi/>Helena Bonaldi</a>
|
<a href=/people/g/greta-damo/>Greta Damo</a>
|
<a href=/people/n/nicolas-benjamin-ocampo/>Nicolás Benjamín Ocampo</a>
|
<a href=/people/e/elena-cabrio/>Elena Cabrio</a>
|
<a href=/people/s/serena-villata/>Serena Villata</a>
|
<a href=/people/m/marco-guerini/>Marco Guerini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--201><div class="card-body p-3 small">The potential effectiveness of counterspeech as a hate speech mitigation strategy is attracting increasing interest in the NLG research community, particularly towards the task of automatically producing it. However, automatically generated responses often lack the argumentative richness which characterises expert-produced counterspeech. In this work, we focus on two aspects of counterspeech generation to produce more cogent responses. First, by investigating the tension between helpfulness and harmlessness of LLMs, we test whether the presence of safety guardrails hinders the quality of the generations. Secondly, we assess whether attacking a specific component of the hate speech results in a more effective argumentative strategy to fight online hate. By conducting an extensive human and automatic evaluation, we show how the presence of safety guardrails can be detrimental also to a task that inherently aims at fostering positive social interactions. Moreover, our results show that attacking a specific component of the hate speech, and in particular its implicit negative stereotype and its hateful parts, leads to higher-quality generations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.202.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.202/>Leading Whitespaces of Language Models’ Subword Vocabulary Pose a Confound for Calculating Word Probabilities</a></strong><br><a href=/people/b/byung-doh-oh/>Byung-Doh Oh</a>
|
<a href=/people/w/william-schuler/>William Schuler</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.203.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.203/><span class=acl-fixed-case>LLM</span>4<span class=acl-fixed-case>D</span>ecompile: Decompiling Binary Code with Large Language Models</a></strong><br><a href=/people/h/hanzhuo-tan/>Hanzhuo Tan</a>
|
<a href=/people/q/qi-luo/>Qi Luo</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/y/yuqun-zhang/>Yuqun Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--203><div class="card-body p-3 small">Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.204.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.204/>From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning</a></strong><br><a href=/people/j/jihao-gu/>Jihao Gu</a>
|
<a href=/people/z/zelin-wang/>Zelin Wang</a>
|
<a href=/people/y/yibo-zhang/>Yibo Zhang</a>
|
<a href=/people/z/ziji-zhang/>Ziji Zhang</a>
|
<a href=/people/p/ping-gong/>Ping Gong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--204><div class="card-body p-3 small">With the proliferation of large language models, Parameter Efficient Fine-Tuning (PEFT) method, which freeze pre-trained parameters and only fine-tune a few task-specific parameters, are playing an increasingly important role. However, previous work primarily applied uniform operations across all layers of the model, overlooking the fact that different layers in a transformer store different information. In the process of exploration, We find that there is a significant differences in fine-tuning strategies between different layers, and fine-tuning only a subset of layers can even achieve comparable performance. Based on this, we propose the Hybrid LoRA-Prefix Tuning(HLPT) method, which uses enhanced LoRA and Prefix-tuning methods with learnable adaptive mechanism separately for the bottom and top layers, and the Half Hybrid LoRA-Prefix Tuning(<span class=tex-math>H<sup>2</sup></span>LPT) method, which goes a step further, reducing the parameter count to nearly half by omitting fine-tuning in the middle layers. Extensive experiments with large language models on various downstream tasks provide strong evidence for the potential of PEFT focusing on different layers’ interactions and the effectiveness of our methods. Furthermore, we validate the robustness of these methods and their advantages in speeding up training convergence, reducing inference time requirements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.205.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.205.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.205.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.205/><span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>TKR</span>: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering</a></strong><br><a href=/people/y/yike-wu/>Yike Wu</a>
|
<a href=/people/y/yi-huang/>Yi Huang</a>
|
<a href=/people/n/nan-hu/>Nan Hu</a>
|
<a href=/people/y/yuncheng-hua/>Yuncheng Hua</a>
|
<a href=/people/g/guilin-qi/>Guilin Qi</a>
|
<a href=/people/j/jiaoyan-chen/>Jiaoyan Chen</a>
|
<a href=/people/j/jeff-z-pan/>Jeff Z. Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--205><div class="card-body p-3 small">Recent studies have explored the use of Large Language Models (LLMs) with Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering (KGQA). They typically require rewriting retrieved subgraphs into natural language formats comprehensible to LLMs. However, when tackling complex questions, the knowledge rewritten by existing methods may include irrelevant information, omit crucial details, or fail to align with the question’s semantics. To address them, we propose a novel rewriting method CoTKR, Chain- of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces and corresponding knowledge in an interleaved manner, thereby mitigating the limitations of single-step knowledge rewriting. Additionally, to bridge the preference gap between the knowledge rewriter and the question answering (QA) model, we propose a training strategy PAQAF, Preference Alignment from Question Answering Feedback, for leveraging feedback from the QA model to further optimize the knowledge rewriter. We conduct experiments using various LLMs across several KGQA benchmarks. Experimental results demonstrate that, compared with previous knowledge rewriting methods, CoTKR generates the most beneficial knowledge representation for QA models, which significantly improves the performance of LLMs in KGQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.206.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--206 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.206/><span class=acl-fixed-case>MTLS</span>: Making Texts into Linguistic Symbols</a></strong><br><a href=/people/w/wenlong-fei/>Wenlong Fei</a>
|
<a href=/people/x/xiaohua-wang/>Xiaohua Wang</a>
|
<a href=/people/m/min-hu/>Min Hu</a>
|
<a href=/people/q/qingyu-zhang/>Qingyu Zhang</a>
|
<a href=/people/h/hongbo-li/>Hongbo Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--206><div class="card-body p-3 small">In linguistics, all languages can be considered as symbolic systems, with each language relying on symbolic processes to associate specific symbols with meanings. In the same language, there is a fixed correspondence between linguistic symbol and meaning. In different languages, universal meanings follow varying rules of symbolization in one-to-one correspondence with symbols. Most work overlooks the properties of languages as symbol systems. In this paper, we shift the focus to the symbolic properties and introduce MTLS: a pre-training method to improve the multilingual capability of models by Making Texts into Linguistic Symbols. Initially, we replace the vocabulary in pre-trained language models by mapping relations between linguistic symbols and semantics. Subsequently, universal semantics within the symbolic system serve as bridges, linking symbols from different languages to the embedding space of the model, thereby enabling the model to process linguistic symbols. To evaluate the effectiveness of MTLS, we conducted experiments on multilingual tasks using BERT and RoBERTa, respectively, as the backbone. The results indicate that despite having just over 12,000 pieces of English data in pre-training, the improvement that MTLS brings to multilingual capabilities is remarkably significant.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.207.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.207/><span class=acl-fixed-case>D</span>2<span class=acl-fixed-case>R</span>: Dual-Branch Dynamic Routing Network for Multimodal Sentiment Detection</a></strong><br><a href=/people/y/yifan-chen/>Yifan Chen</a>
|
<a href=/people/k/kuntao-li/>Kuntao Li</a>
|
<a href=/people/w/weixing-mai/>Weixing Mai</a>
|
<a href=/people/q/qiaofeng-wu/>Qiaofeng Wu</a>
|
<a href=/people/y/yun-xue/>Yun Xue</a>
|
<a href=/people/f/fenghuan-li/>Fenghuan Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.208.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.208/>A Generic Method for Fine-grained Category Discovery in Natural Language Texts</a></strong><br><a href=/people/c/chang-tian/>Chang Tian</a>
|
<a href=/people/m/matthew-b-blaschko/>Matthew B. Blaschko</a>
|
<a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/m/mingzhe-xing/>Mingzhe Xing</a>
|
<a href=/people/y/yinliang-yue/>Yinliang Yue</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--208><div class="card-body p-3 small">Fine-grained category discovery using only coarse-grained supervision is a cost-effective yet challenging task. Previous training methods focus on aligning query samples with positive samples and distancing them from negatives. They often neglect intra-category and inter-category semantic similarities of fine-grained categories when navigating sample distributions in the embedding space. Furthermore, some evaluation techniques that rely on pre-collected test samples are inadequate for real-time applications. To address these shortcomings, we introduce a method that successfully detects fine-grained clusters of semantically similar texts guided by a novel objective function. The method uses semantic similarities in a logarithmic space to guide sample distributions in the Euclidean space and to form distinct clusters that represent fine-grained categories. We also propose a centroid inference mechanism to support real-time applications. The efficacy of the method is both theoretically justified and empirically confirmed on three benchmark tasks. The proposed objective function is integrated in multiple contrastive learning based neural models. Its results surpass existing state-of-the-art approaches in terms of Accuracy, Adjusted Rand Index and Normalized Mutual Information of the detected fine-grained categories. Code and data are publicly available at https://github.com/changtianluckyforever/F-grained-STAR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.209.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--209 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.209/>Toxicity Detection is <span class=acl-fixed-case>NOT</span> all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method</a></strong><br><a href=/people/y/yang-trista-cao/>Yang Trista Cao</a>
|
<a href=/people/l/lovely-frances-domingo/>Lovely-Frances Domingo</a>
|
<a href=/people/s/sarah-gilbert/>Sarah Gilbert</a>
|
<a href=/people/m/michelle-l-mazurek/>Michelle L. Mazurek</a>
|
<a href=/people/k/katie-shilton/>Katie Shilton</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé Iii</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--209><div class="card-body p-3 small">Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators’ needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey study with volunteer moderators to gain insight into their perspectives on useful moderation models. Overall, we observe a non trivial gap, as missing developed models and LLMs exhibit moderate to low performance on a significant portion of the rules. Moderators’ reports provide guides for future work on developing moderation assistant models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.210.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--210 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.210.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.210/>A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models</a></strong><br><a href=/people/j/jiayin-wang/>Jiayin Wang</a>
|
<a href=/people/f/fengran-mo/>Fengran Mo</a>
|
<a href=/people/w/weizhi-ma/>Weizhi Ma</a>
|
<a href=/people/p/peijie-sun/>Peijie Sun</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/j/jian-yun-nie/>Jian-Yun Nie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--210><div class="card-body p-3 small">Large language models (LLMs) are essential tools that users employ across various scenarios, so evaluating their performance and guiding users in selecting the suitable service is important. Although many benchmarks exist, they mainly focus on specific predefined model abilities, such as world knowledge, reasoning, etc. Based on these ability scores, it is hard for users to determine which LLM best suits their particular needs. To address these issues, we propose to evaluate LLMs from a user-centric perspective and design this benchmark to measure their efficacy in satisfying user needs under distinct intents. Firstly, we collect 1,846 real-world use cases from a user study with 712 participants from 23 countries. This first-hand data helps us understand actual user intents and needs in LLM interactions, forming the User Reported Scenarios (URS) dataset, which is categorized with six types of user intents. Secondly, based on this authentic dataset, we benchmark 10 LLM services with GPT-4-as-Judge. Thirdly, we show that benchmark scores align well with human preference in both real-world experience and pair-wise annotations, achieving Pearson correlations of 0.95 and 0.94, respectively. This alignment confirms that the URS dataset and our evaluation method establish an effective user-centric benchmark. The dataset, code, and process data are publicly available at https://github.com/Alice1998/URS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.211.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--211 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.211/>Decompose and Compare Consistency: Measuring <span class=acl-fixed-case>VLM</span>s’ Answer Reliability via Task-Decomposition Consistency Comparison</a></strong><br><a href=/people/q/qian-yang/>Qian Yang</a>
|
<a href=/people/w/weixiang-yan/>Weixiang Yan</a>
|
<a href=/people/a/aishwarya-agrawal/>Aishwarya Agrawal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--211><div class="card-body p-3 small">Despite tremendous advancements, current state-of-the-art Vision-Language Models (VLMs) are still far from perfect. They tend to hallucinate and may generate biased responses. In such circumstances, having a way to assess the reliability of a given response generated by a VLM is quite useful. Existing methods, such as estimating uncertainty using answer likelihoods or prompt-based confidence generation, often suffer from overconfidence. Other methods use self-consistency comparison but are affected by confirmation biases. To alleviate these, we propose Decompose and Compare Consistency (DeCC) for reliability measurement. By comparing the consistency between the direct answer generated using the VLM’s internal reasoning process, and the indirect answers obtained by decomposing the question into sub-questions and reasoning over the sub-answers produced by the VLM, DeCC measures the reliability of VLM’s direct answer. Experiments across six vision-language tasks with three VLMs show DeCC’s reliability estimation achieves better correlation with task accuracy compared to the existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.212.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--212 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.212.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.212.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.212/>Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism</a></strong><br><a href=/people/l/lang-cao/>Lang Cao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--212><div class="card-body p-3 small">Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM’s understanding of the world, enabling it to provide traceable gold knowledge. This knowledge base is separate from the LLM and initially empty. It can be filled with validated knowledge and progressively expanded. When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently. Additionally, we introduce a method for automatically and efficiently expanding the knowledge base of LLMs. Through qualitative and quantitative analysis, we demonstrate that our approach enhances the controllability and reliability of LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.213.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.213.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.213.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.213/><span class=acl-fixed-case>VGB</span>ench: Evaluating Large Language Models on Vector Graphics Understanding and Generation</a></strong><br><a href=/people/b/bocheng-zou/>Bocheng Zou</a>
|
<a href=/people/m/mu-cai/>Mu Cai</a>
|
<a href=/people/j/jianrui-zhang/>Jianrui Zhang</a>
|
<a href=/people/y/yong-jae-lee/>Yong Jae Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--213><div class="card-body p-3 small">In the realm of vision models, the primary mode of representation is using pixels to rasterize the visual world. Yet this is not always the best or unique way to represent visual content, especially for designers and artists who depict the world using geometry primitives such as polygons. Vector graphics (VG), on the other hand, offer a textual representation of visual content, which can be more concise and powerful for content like cartoons, sketches and scientific figures. Recent studies have shown promising results on processing vector graphics with capable Large Language Models (LLMs). However, such works focus solely on qualitative results, understanding, or a specific type of vector graphics. We propose VGBench, a comprehensive benchmark for LLMs on handling vector graphics through diverse aspects, including (a) both visual understanding and generation, (b) evaluation of various vector graphics formats, (c) diverse question types, (d) wide range of prompting techniques, (e) under multiple LLMs and (f) comparison with VLMs on rasterized representations. Evaluating on our collected 4279 understanding and 5845 generation samples, we find that LLMs show strong capability on both aspects while exhibiting less desirable performance on low-level formats (SVG). Both data and evaluation pipeline will be open-sourced.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.214.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--214 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.214/>What do Large Language Models Need for Machine Translation Evaluation?</a></strong><br><a href=/people/s/shenbin-qian/>Shenbin Qian</a>
|
<a href=/people/a/archchana-sindhujan/>Archchana Sindhujan</a>
|
<a href=/people/m/minnie-kabra/>Minnie Kabra</a>
|
<a href=/people/d/diptesh-kanojia/>Diptesh Kanojia</a>
|
<a href=/people/c/constantin-orasan/>Constantin Orasan</a>
|
<a href=/people/t/tharindu-ranasinghe/>Tharindu Ranasinghe</a>
|
<a href=/people/f/fred-blain/>Fred Blain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--214><div class="card-body p-3 small">Leveraging large language models (LLMs) for various natural language processing tasks has led to superlative claims about their performance. For the evaluation of machine translation (MT), existing research shows that LLMs are able to achieve results comparable to fine-tuned multilingual pre-trained language models. In this paper, we explore what translation information, such as the source, reference, translation errors and annotation guidelines, is needed for LLMs to evaluate MT quality. In addition, we investigate prompting techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for eight language pairs covering high-, medium- and low-resource languages, leveraging varying LLM variants. Our findings indicate the importance of reference translations for an LLM-based evaluation. While larger models do not necessarily fare better, they tend to benefit more from CoT prompting, than smaller models. We also observe that LLMs do not always provide a numerical score when generating evaluations, which poses a question on their reliability for the task. Our work presents a comprehensive analysis for resource-constrained and training-less LLM-based evaluation of machine translation. We release the accrued prompt templates, code and data publicly for reproducibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.215.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--215 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.215/>Performance-Guided <span class=acl-fixed-case>LLM</span> Knowledge Distillation for Efficient Text Classification at Scale</a></strong><br><a href=/people/f/flavio-di-palo/>Flavio Di Palo</a>
|
<a href=/people/p/prateek-singhi/>Prateek Singhi</a>
|
<a href=/people/b/bilal-h-fadlallah/>Bilal H Fadlallah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--215><div class="card-body p-3 small">Large Language Models (LLMs) face significant challenges at inference time due to their high computational demands. To address this, we present Performance-Guided Knowledge Distillation (PGKD), a cost-effective and high-throughput solution for production text classification applications. PGKD utilizes teacher-student Knowledge Distillation to distill the knowledge of LLMs into smaller, task-specific models. PGKD establishes an active learning routine between the student model and the LLM; the LLM continuously generates new training data leveraging hard-negative mining, student model validation performance, and early-stopping protocols to inform the data generation. By employing a cyclical, performance-aware approach tailored for highly multi-class, sparsely annotated datasets prevalent in industrial text classification, PGKD effectively addresses training challenges and outperforms traditional BERT-base models and other knowledge distillation methods on several multi-class classification datasets. Additionally, cost and latency benchmarking reveals that models fine-tuned with PGKD are up to 130X faster and 25X less expensive than LLMs for inference on the same classification task. While PGKD is showcased for text classification tasks, its versatile framework can be extended to any LLM distillation task, including language generation, making it a powerful tool for optimizing performance across a wide range of AI applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.216.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--216 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.216/>External Knowledge-Driven Argument Mining: Leveraging Attention-Enhanced Multi-Network Models</a></strong><br><a href=/people/d/debela-gemechu/>Debela Gemechu</a>
|
<a href=/people/c/chris-reed/>Chris Reed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--216><div class="card-body p-3 small">Argument mining (AM) involves the identification of argument relations (AR) between Argumentative Discourse Units (ADUs). The essence of ARs among ADUs is context-dependent and lies in maintaining a coherent flow of ideas, often centered around the relations between discussed entities, topics, themes or concepts. However, these relations are not always explicitly stated; rather, inferred from implicit chains of reasoning connecting the concepts addressed in the ADUs. While humans can infer such background knowledge, machines face challenges when the contextual cues are not explicitly provided. This paper leverages external resources, including WordNet, ConceptNet, and Wikipedia to identify semantic paths (knowledge paths) connecting the concepts discussed in the ADUs to obtain the implicit chains of reasoning. To effectively leverage these paths for AR prediction, we propose attention-based Multi-Network architectures. Various architecture are evaluated on the external resources, and the Wikipedia based configuration attains F-scores of 0.85, 0.84, 0.70, and 0.87, respectively, on four diverse datasets, showing strong performance over the baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.217.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--217 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.217/><span class=acl-fixed-case>C</span>3<span class=acl-fixed-case>PA</span>: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits</a></strong><br><a href=/people/m/maaz-bin-musa/>Maaz Bin Musa</a>
|
<a href=/people/s/steven-m-winston/>Steven M. Winston</a>
|
<a href=/people/g/garrison-allen/>Garrison Allen</a>
|
<a href=/people/j/jacob-schiller/>Jacob Schiller</a>
|
<a href=/people/k/kevin-moore/>Kevin Moore</a>
|
<a href=/people/s/sean-quick/>Sean Quick</a>
|
<a href=/people/j/johnathan-melvin/>Johnathan Melvin</a>
|
<a href=/people/p/padmini-srinivasan/>Padmini Srinivasan</a>
|
<a href=/people/m/mihailis-e-diamantis/>Mihailis E. Diamantis</a>
|
<a href=/people/r/rishab-nithyanand/>Rishab Nithyanand</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--217><div class="card-body p-3 small">The development of tools and techniques to analyze and extract organizations’ data habits from privacy policies are critical for scalable regulatory compliance audits. Unfortunately, these tools are becoming increasingly limited in their ability to identify compliance issues and fixes. After all, most were developed using regulation-agnostic datasets of annotated privacy policies obtained from a time before the introduction of landmark privacy regulations such as EU’s GDPR and California’s CCPA. In this paper, we describe the first open regulation-aware dataset of expert-annotated privacy policies, C3PA (CCPA Privacy Policy Provision Annotations), aimed to address this challenge. C3PA contains over 48K expert-labeled privacy policy text segments associated with responses to CCPA-specific disclosure mandates from 411 unique organizations. We demonstrate that the C3PA dataset is uniquely suited for aiding automated audits of compliance with CCPA-related disclosure mandates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.218.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--218 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.218/><span class=acl-fixed-case>M</span><span class=tex-math><sup>2</sup></span><span class=acl-fixed-case>PT</span>: Multimodal Prompt Tuning for Zero-shot Instruction Learning</a></strong><br><a href=/people/t/taowen-wang/>Taowen Wang</a>
|
<a href=/people/y/yiyang-liu/>Yiyang Liu</a>
|
<a href=/people/j/james-chenhao-liang/>James Chenhao Liang</a>
|
<a href=/people/j/junhan-zhao/>Junhan Zhao</a>
|
<a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/y/yuning-mao/>Yuning Mao</a>
|
<a href=/people/s/shaoliang-nie/>Shaoliang Nie</a>
|
<a href=/people/j/jiahao-liu/>Jiahao Liu</a>
|
<a href=/people/f/fuli-feng/>Fuli Feng</a>
|
<a href=/people/z/zenglin-xu/>Zenglin Xu</a>
|
<a href=/people/c/cheng-han/>Cheng Han</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/q/qifan-wang/>Qifan Wang</a>
|
<a href=/people/d/dongfang-liu/>Dongfang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--218><div class="card-body p-3 small">Multimodal Large Language Models (MLLMs) demonstrate remarkable performance across a wide range of domains, with increasing emphasis on enhancing their zero-shot generalization capabilities for unseen tasks across various modalities. Instruction tuning has emerged as an effective strategy for achieving zero-shot generalization by finetuning pretrained models on diverse multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient finetuning becomes increasingly critical. However, most existing parameter-efficient approaches focus only on single modalities and often overlook the multimodal characteristics during finetuning. In this work, we introduce a novel Multimodal Prompt Tuning (M<span class=tex-math><sup>2</sup></span>PT) approach for efficient instruction tuning of MLLMs. M<span class=tex-math><sup>2</sup></span>PT effectively integrates visual and textual prompts into the vision encoder and language processor respectively during finetuning, facilitating the extraction and alignment of features across modalities. Empirical results on various multimodal evaluation datasets demonstrate the superior performance of our approach compared to several state-of-the-art baselines. A comprehensive set of ablation studies validates the effectiveness of our prompt design and the efficiency of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.219.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--219 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.219/>Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification</a></strong><br><a href=/people/l/letian-peng/>Letian Peng</a>
|
<a href=/people/y/yi-gu/>Yi Gu</a>
|
<a href=/people/c/chengyu-dong/>Chengyu Dong</a>
|
<a href=/people/z/zihan-wang/>Zihan Wang</a>
|
<a href=/people/j/jingbo-shang/>Jingbo Shang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--219><div class="card-body p-3 small">For extremely weak-supervised text classification, pioneer research generates pseudo labels by mining texts similar to the class names from the raw corpus, which may end up with very limited or even no samples for the minority classes. Recent works have started to generate the relevant texts by prompting LLMs using the class names or definitions; however, there is a high risk that LLMs cannot generate in-distribution (i.e., similar to the corpus where the text classifier will be applied) data, leading to ungeneralizable classifiers. In this paper, we combine the advantages of these two approaches and propose to bridge the gap via a novel framework, <i>text grafting</i>, which aims to obtain clean and near-distribution weak supervision for minority classes. Specifically, we first use LLM-based logits to mine masked templates from the raw corpus, which have a high potential for data synthesis into the target minority class. Then, the templates are filled by state-of-the-art LLMs to synthesize near-distribution texts falling into minority classes. Text grafting shows significant improvement over direct mining or synthesis on minority classes. We also use analysis and case studies to comprehend the property of text grafting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.220.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.220/>Incubating Text Classifiers Following User Instruction with Nothing but <span class=acl-fixed-case>LLM</span></a></strong><br><a href=/people/l/letian-peng/>Letian Peng</a>
|
<a href=/people/z/zilong-wang/>Zilong Wang</a>
|
<a href=/people/j/jingbo-shang/>Jingbo Shang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--220><div class="card-body p-3 small">In this paper, we aim to generate text classification data given arbitrary class definitions (i.e., user instruction), so one can train a text classifier without any human annotation or raw corpus. Recent advances in large language models (LLMs) lead to pioneer attempts to individually generate texts for each class via prompting. In this paper, we propose Incubator, the first framework that can handle complicated and even mutually dependent classes (e.g., "<i>TED Talk given by Educator</i>" and "<i>Other</i>"). Specifically, our Incubator is a fine-tuned LLM that takes the instruction of all class definitions as input, and in each inference, it can jointly generate one sample for every class. First, we tune Incubator on the instruction-to-data mappings that we obtained from classification datasets and descriptions on Hugging Face together with in-context augmentation by GPT-4. To emphasize the uniformity and diversity in generations, we refine Incubator by fine-tuning with the cluster centers of semantic textual embeddings of the generated samples. We compare Incubator on various classification tasks with strong baselines such as direct LLM-based inference and training data generation by prompt engineering. Experiments show Incubator is able to (1) outperform previous methods on traditional benchmarks, (2) take label interdependency and user preference into consideration, and (3) enable logical text mining by incubating multiple classifiers</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.221.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--221 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.221.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.221.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.221/><span class=acl-fixed-case>PTD</span>-<span class=acl-fixed-case>SQL</span>: Partitioning and Targeted Drilling with <span class=acl-fixed-case>LLM</span>s in Text-to-<span class=acl-fixed-case>SQL</span></a></strong><br><a href=/people/r/ruilin-luo/>Ruilin Luo</a>
|
<a href=/people/l/liyuan-wang/>Liyuan Wang</a>
|
<a href=/people/b/binghuai-lin/>Binghuai Lin</a>
|
<a href=/people/z/zicheng-lin/>Zicheng Lin</a>
|
<a href=/people/y/yujiu-yang/>Yujiu Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--221><div class="card-body p-3 small">Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problem and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-the-art (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress. Code is available at https://github.com/lrlbbzl/PTD-SQL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.222.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--222 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.222.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.222/>Conditional and Modal Reasoning in Large Language Models</a></strong><br><a href=/people/w/wesley-h-holliday/>Wesley H. Holliday</a>
|
<a href=/people/m/matthew-mandelkern/>Matthew Mandelkern</a>
|
<a href=/people/c/cedegao-e-zhang/>Cedegao E. Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--222><div class="card-body p-3 small">The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. In this paper, we probe the extent to which twenty-nine LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., '*If* Ann has a queen, *then* Bob has a jack’) and epistemic modals (e.g., ‘Ann *might* have an ace’, ‘Bob *must* have a king’). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental human ability to reason about distal possibilities. Assessing LLMs on these inferences is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. All the LLMs we tested make some basic mistakes with conditionals or modals, though zero-shot chain-of-thought prompting helps them make fewer mistakes. Even the best performing LLMs make basic errors in modal reasoning, display logically inconsistent judgments across inference patterns involving epistemic modals and conditionals, and give answers about complex conditional inferences that do not match reported human judgments. These results highlight gaps in basic logical reasoning in today’s LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.223.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--223 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.223/>Advancing Large Language Model Attribution through Self-Improving</a></strong><br><a href=/people/l/lei-huang/>Lei Huang</a>
|
<a href=/people/x/xiaocheng-feng/>Xiaocheng Feng</a>
|
<a href=/people/w/weitao-ma/>Weitao Ma</a>
|
<a href=/people/l/liang-zhao/>Liang Zhao</a>
|
<a href=/people/y/yuchun-fan/>Yuchun Fan</a>
|
<a href=/people/w/weihong-zhong/>Weihong Zhong</a>
|
<a href=/people/d/dongliang-xu/>Dongliang Xu</a>
|
<a href=/people/q/qing-yang/>Qing Yang</a>
|
<a href=/people/h/hongtao-liu/>Hongtao Liu</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--223><div class="card-body p-3 small">Teaching large language models (LLMs) to generate text with citations to evidence sources can mitigate hallucinations and enhance verifiability in information-seeking systems. However, improving this capability requires high-quality attribution data, which is costly and labor-intensive. Inspired by recent advances in self-improvement that enhance LLMs without manual annotation, we present START, a Self-Taught AttRibuTion framework for iteratively improving the attribution capability of LLMs. First, to prevent models from stagnating due to initially insufficient supervision signals, START leverages the model to self-construct synthetic training data for warming up. To further self-improve the model’s attribution ability, START iteratively utilizes fine-grained preference supervision signals constructed from its sampled responses to encourage robust, comprehensive, and attributable generation. Experiments on three open-domain question-answering datasets, covering long-form QA and multi-step reasoning, demonstrate significant performance gains of 25.13% on average without relying on human annotations and more advanced models. Further analysis reveals that START excels in aggregating information across multiple sources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.224.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.224/><span class=acl-fixed-case>A</span>lign<span class=acl-fixed-case>C</span>ap: Aligning Speech Emotion Captioning to Human Preferences</a></strong><br><a href=/people/z/ziqi-liang/>Ziqi Liang</a>
|
<a href=/people/h/haoxiang-shi/>Haoxiang Shi</a>
|
<a href=/people/h/hanhui-chen/>Hanhui Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--224><div class="card-body p-3 small">Speech Emotion Captioning (SEC) has gradually become an active research task. The emotional content conveyed through human speech are often complex, and classifying them into fixed categories may not be enough to fully capture speech emotions. Describing speech emotions through natural language may be a more effective approach. However, existing SEC methods often produce hallucinations and lose generalization on unseen speech. To overcome these problems, we propose AlignCap, which Aligning Speech Emotion Captioning to Human Preferences based on large language model (LLM) with two properties: 1) Speech-Text Alignment, which minimizing the divergence between the LLM’s response prediction distributions for speech and text inputs using knowledge distillation (KD) Regularization. 2) Human Preference Alignment, where we design Preference Optimization (PO) Regularization to eliminate factuality and faithfulness hallucinations. We also extract emotional clues as a prompt for enriching fine-grained information under KD-Regularization. Experiments demonstrate that AlignCap presents stronger performance to other state-of-the-art methods on Zero-shot SEC task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.225.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--225 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.225/>Interpretability-based Tailored Knowledge Editing in Transformers</a></strong><br><a href=/people/y/yihuai-hong/>Yihuai Hong</a>
|
<a href=/people/a/aldo-lipani/>Aldo Lipani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--225><div class="card-body p-3 small">Language models recognized as a new form of knowledge bases, face challenges of outdated, erroneous, and privacy-sensitive information, necessitating knowledge editing to rectify errors without costly retraining. Existing methods, spanning model’s parameters modification, external knowledge integration, and in-context learning, lack in-depth analysis from a model interpretability perspective. Our work explores the instability in in-context learning outcomes, providing insights into its reasons and distinctions from other methods. Leveraging findings on the critical role of feed-forward MLPs in decoder-only models, we propose a tailored knowledge editing method, TailoredKE, that considers the unique information flow of each sample. Model interpretability reveals diverse attribute recall across transformer layers, guiding edits to specific features at different depths and mitigating over-editing issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.226.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--226 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.226.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.226/><span class=acl-fixed-case>PR</span>ompt Optimization in Multi-Step Tasks (<span class=acl-fixed-case>PROMST</span>): Integrating Human Feedback and Heuristic-based Sampling</a></strong><br><a href=/people/y/yongchao-chen/>Yongchao Chen</a>
|
<a href=/people/j/jacob-arkin/>Jacob Arkin</a>
|
<a href=/people/y/yilun-hao/>Yilun Hao</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a>
|
<a href=/people/n/nicholas-roy/>Nicholas Roy</a>
|
<a href=/people/c/chuchu-fan/>Chuchu Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--226><div class="card-body p-3 small">Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework PROMST that incorporates human-designed feedback rules to automatically offer direct suggestions for improvement. We also use an extra learned heuristic model that predicts prompt performance to efficiently sample from prompt candidates. This approach significantly outperforms both human-engineered prompts and several other prompt optimization methods across 11 representative multi-step tasks (an average 10.6%-29.3% improvement to current best methods on five LLMs respectively). We believe our work can serve as a benchmark for automatic prompt optimization for LLM-driven multi-step tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.227.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.227/>Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting</a></strong><br><a href=/people/c/chen-cai/>Chen Cai</a>
|
<a href=/people/z/zheng-wang/>Zheng Wang</a>
|
<a href=/people/j/jianjun-gao/>Jianjun Gao</a>
|
<a href=/people/w/wenyang-liu/>Wenyang Liu</a>
|
<a href=/people/y/ye-lu/>Ye Lu</a>
|
<a href=/people/r/runzhong-zhang/>Runzhong Zhang</a>
|
<a href=/people/k/kim-hui-yap/>Kim-Hui Yap</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--227><div class="card-body p-3 small">In recent years, the rapid increase in online video content has underscored the limitations of static Video Question Answering (VideoQA) models trained on fixed datasets, as they struggle to adapt to new questions or tasks posed by newly available content. In this paper, we explore the novel challenge of VideoQA within a continual learning framework, and empirically identify a critical issue: fine-tuning a large language model (LLM) for a sequence of tasks often results in catastrophic forgetting. To address this, we propose Collaborative Prompting (ColPro), which integrates specific question constraint prompting, knowledge acquisition prompting, and visual temporal awareness prompting. These prompts aim to capture textual question context, visual content, and video temporal dynamics in VideoQA, a perspective underexplored in prior research. Experimental results on the NExT-QA and DramaQA datasets show that ColPro achieves superior performance compared to existing approaches, achieving 55.14% accuracy on NExT-QA and 71.24% accuracy on DramaQA, highlighting its practical relevance and effectiveness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.228.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.228/>Dissecting Fine-Tuning Unlearning in Large Language Models</a></strong><br><a href=/people/y/yihuai-hong/>Yihuai Hong</a>
|
<a href=/people/y/yuelin-zou/>Yuelin Zou</a>
|
<a href=/people/l/lijie-hu/>Lijie Hu</a>
|
<a href=/people/z/ziqian-zeng/>Ziqian Zeng</a>
|
<a href=/people/d/di-wang/>Di Wang</a>
|
<a href=/people/h/haiqin-yang/>Haiqin Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--228><div class="card-body p-3 small">Fine-tuning-based unlearning methods prevail for erasing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of the methods is unclear. In this paper, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model’s knowledge retrieval process, rather than genuinely erasing the problematic knowledge embedded in the model parameters. Furthermore, behavioral tests demonstrate that the unlearning mechanisms inevitably impact the global behavior of the models, affecting unrelated knowledge or capabilities. Our work advocates the development of more resilient unlearning techniques for truly erasing knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.229.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.229/>Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models</a></strong><br><a href=/people/z/zhengxuan-wu/>Zhengxuan Wu</a>
|
<a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/p/peng-qi/>Peng Qi</a>
|
<a href=/people/y/yumo-xu/>Yumo Xu</a>
|
<a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/y/yian-zhang/>Yian Zhang</a>
|
<a href=/people/j/jifan-chen/>Jifan Chen</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/z/zhiheng-huang/>Zhiheng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--229><div class="card-body p-3 small">Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (i.e., follow open-ended instructions) and faithfulness (i.e., ground responses in given context) when training LMs with these objectives. For instance, fine-tuning LLaMA-7B on instruction following datasets renders it less faithful. Conversely, instruction-tuned Vicuna-7B shows degraded performance at following instructions when further optimized on tasks that require contextual grounding. One common remedy is multi-task learning (MTL) with data mixing, yet it remains far from achieving a synergic outcome. We propose a simple yet effective method that relies on Reject-sampling by Self-instruct with Continued Fine-tuning (ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find that less is more, as training ReSet with high-quality, yet substantially smaller data (three-fold less) yields superior results. Our findings offer a better understanding of objective discrepancies in alignment training of LMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.230.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--230 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.230/>Where is the signal in tokenization space?</a></strong><br><a href=/people/r/renato-geh/>Renato Geh</a>
|
<a href=/people/h/honghua-zhang/>Honghua Zhang</a>
|
<a href=/people/k/kareem-ahmed/>Kareem Ahmed</a>
|
<a href=/people/b/benjie-wang/>Benjie Wang</a>
|
<a href=/people/g/guy-van-den-broeck/>Guy Van Den Broeck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--230><div class="card-body p-3 small">Large Language Models (LLMs) are typically shipped with tokenizers that *deterministically* encode text into so-called *canonical* token sequences, to which the LLMs assign probability values.One common assumption is that the probability of a piece of text is the probability of its canonical token sequence.However, the tokenization of a string is not unique: e.g., the Llama2 tokenizer encodes ‘Tokens‘ as ‘[Tok,ens]‘, but ‘[Tok,en,s]‘ also represents the same text.In this paper, we study non-canonical tokenizations.We prove that, given a string, it is computationally hard to find the most likely tokenization for an autoregressive LLM, as well as to compute the marginal probability over all possible tokenizations.We then show how the marginal is, in most cases, indistinguishable from the canonical probability.Surprisingly, we then empirically demonstrate the existence of a significant amount of signal hidden within tokenization space.Notably, by simply aggregating the probabilities of non-canonical tokenizations, we achieve improvements across a range of LLM evaluation benchmarks for a variety of architectures, including transformers and state space models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.231.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--231 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.231/>Private Language Models via Truncated Laplacian Mechanism</a></strong><br><a href=/people/t/tianhao-huang/>Tianhao Huang</a>
|
<a href=/people/t/tao-yang/>Tao Yang</a>
|
<a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/l/lijie-hu/>Lijie Hu</a>
|
<a href=/people/d/di-wang/>Di Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--231><div class="card-body p-3 small">Recently it has been shown that deep learning models for NLP tasks are prone to attacks that can even reconstruct the verbatim training texts. To prevent privacy leakage, researchers have investigated word-level perturbations, relying on the formal guarantees of differential privacy (DP) in the embedding space. However, many existing approaches either achieve unsatisfactory performance in the high privacy regime when using the Laplacian or Gaussian mechanism, or resort to weaker relaxations of DP that are inferior to the canonical DP in terms of privacy strength. This raises the question of whether a new method for private word embedding can be designed to overcome these limitations. In this paper, we propose a novel private embedding method called the high dimensional truncated Laplacian mechanism. Specifically, we introduce a non-trivial extension of the truncated Laplacian mechanism, which was previously only investigated in one-dimensional space cases. Theoretically, we show that our method has a lower variance compared to the previous private word embedding methods. To further validate its effectiveness, we conduct comprehensive experiments on private embedding and downstream tasks using three datasets. Remarkably, even in the high privacy regime, our approach only incurs a slight decrease in utility compared to the non-private scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.232.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.232/>Estimating Knowledge in Large Language Models Without Generating a Single Token</a></strong><br><a href=/people/d/daniela-gottesman/>Daniela Gottesman</a>
|
<a href=/people/m/mor-geva/>Mor Geva</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.233.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--233 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.233.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.233.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.233/>Consistent Autoformalization for Constructing Mathematical Libraries</a></strong><br><a href=/people/l/lan-zhang/>Lan Zhang</a>
|
<a href=/people/x/xin-quan/>Xin Quan</a>
|
<a href=/people/a/andre-freitas/>Andre Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--233><div class="card-body p-3 small">Autoformalization is the task of automatically translating mathematical content written in natural language to a formal language expression. The growing language interpretation capabilities of Large Language Models (LLMs), including in formal languages, are lowering the barriers for autoformalization. However, LLMs alone are not capable of consistently and reliably delivering autoformalization, in particular as the complexity and specialization of the target domain grows. As the field evolves into the direction of systematically applying autoformalization towards large mathematical libraries, the need to improve syntactic, terminological and semantic control increases. This paper proposes the coordinated use of three mechanisms, most-similar retrieval augmented generation (MS-RAG), denoising steps, and auto-correction with syntax error feedback (Auto-SEF) to improve autoformalization quality. The empirical analysis, across different models, demonstrates that these mechanisms can deliver autoformalizaton results which are syntactically, terminologically and semantically more consistent. These mechanisms can be applied across different LLMs and have shown to deliver improve results across different model types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.234.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.234/>When Context Leads but Parametric Memory Follows in Large Language Models</a></strong><br><a href=/people/y/yufei-tao/>Yufei Tao</a>
|
<a href=/people/a/adam-hiatt/>Adam Hiatt</a>
|
<a href=/people/e/erik-haake/>Erik Haake</a>
|
<a href=/people/a/antonie-j-jetter/>Antonie J. Jetter</a>
|
<a href=/people/a/ameeta-agrawal/>Ameeta Agrawal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--234><div class="card-body p-3 small">Large language models (LLMs) have demonstrated remarkable progress in leveraging diverse knowledge sources. This study investigates how nine widely used LLMs allocate knowledge between local context and global parameters when answering open-ended questions in knowledge-consistent scenarios. We introduce a novel dataset, WikiAtomic, and systematically vary context sizes to analyze how LLMs prioritize and utilize the provided information and their parametric knowledge in knowledge-consistent scenarios. Additionally, we also study their tendency to hallucinate under varying context sizes. Our findings reveal consistent patterns across models, including a consistent reliance on both contextual (around 70%) and parametric (around 30%) knowledge, and a decrease in hallucinations with increasing context. These insights highlight the importance of more effective context organization and developing models that use input more deterministically for robust performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.235.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--235 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.235/>Semantic Training Signals Promote Hierarchical Syntactic Generalization in Transformers</a></strong><br><a href=/people/a/aditya-yedetore/>Aditya Yedetore</a>
|
<a href=/people/n/najoung-kim/>Najoung Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--235><div class="card-body p-3 small">Neural networks without hierarchical biases often struggle to learn linguistic rules that come naturally to humans. However, neural networks are trained primarily on form alone, while children acquiring language additionally receive data about meaning. Would neural networks generalize more like humans when trained on both form and meaning? We investigate this by examining if Transformers—neural networks without a hierarchical bias—better achieve hierarchical generalization when trained on both form and meaning compared to when trained on form alone. Our results show that Transformers trained on form and meaning do favor the hierarchical generalization more than those trained on form alone, suggesting that statistical learners without hierarchical biases can leverage semantic training signals to bootstrap hierarchical syntactic generalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.236.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--236 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.236/>When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages</a></strong><br><a href=/people/t/tyler-a-chang/>Tyler A. Chang</a>
|
<a href=/people/c/catherine-arnett/>Catherine Arnett</a>
|
<a href=/people/z/zhuowen-tu/>Zhuowen Tu</a>
|
<a href=/people/b/ben-bergen/>Ben Bergen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--236><div class="card-body p-3 small">Multilingual language models are widely used to extend NLP systems to low-resource languages. However, concrete evidence for the effects of multilinguality on language modeling performance in individual languages remains scarce. Here, we pre-train over 10,000 monolingual and multilingual language models for over 250 languages, including multiple language families that are under-studied in NLP. We assess how language modeling performance in each language varies as a function of (1) monolingual dataset size, (2) added multilingual dataset size, (3) linguistic similarity of the added languages, and (4) model size (up to 45M parameters). We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33%. Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap. However, high-resource languages consistently perform worse in multilingual pre-training scenarios. As dataset sizes increase, adding multilingual data begins to hurt performance for both low-resource and high-resource languages, likely due to limited model capacity (the “curse of multilinguality”). These results suggest that massively multilingual pre-training may not be optimal for any languages involved, but that more targeted models can significantly improve performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.237.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.237.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.237/>Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use</a></strong><br><a href=/people/j/jiajun-xi/>Jiajun Xi</a>
|
<a href=/people/y/yinong-he/>Yinong He</a>
|
<a href=/people/j/jianing-yang/>Jianing Yang</a>
|
<a href=/people/y/yinpei-dai/>Yinpei Dai</a>
|
<a href=/people/j/joyce-chai/>Joyce Chai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--237><div class="card-body p-3 small">In real-world scenarios, it is desirable for embodied agents to have the ability to leverage human language to gain explicit or implicit knowledge for learning tasks. Despite recent progress, most previous approaches adopt simple low-level instructions as language inputs, which may not reflect natural human communication. We expect human language to be informative (i.e., providing feedback on agents’ past behaviors and offering guidance on achieving their future goals) and diverse (i.e., encompassing a wide range of expressions and style nuances). To enable flexibility of language use in teaching agents tasks, this paper studies different types of language inputs in facilitating reinforcement learning (RL) embodied agents. More specifically, we examine how different levels of language informativeness and diversity impact agent learning and inference. Our empirical results based on four RL benchmarks demonstrate that agents trained with diverse and informative language feedback can achieve enhanced generalization and fast adaptation to new tasks. These findings highlight the pivotal role of language use in teaching embodied agents new tasks in an open world.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.238.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.238.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.238/><span class=acl-fixed-case>M</span>i<span class=acl-fixed-case>TT</span>en<span class=acl-fixed-case>S</span>: A Dataset for Evaluating Gender Mistranslation</a></strong><br><a href=/people/k/kevin-robinson/>Kevin Robinson</a>
|
<a href=/people/s/sneha-kudugunta/>Sneha Kudugunta</a>
|
<a href=/people/r/romina-stella/>Romina Stella</a>
|
<a href=/people/s/sunipa-dev/>Sunipa Dev</a>
|
<a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--238><div class="card-body p-3 small">Translation systems, including foundation models capable of translation, can produce errors that result in gender mistranslation, and such errors can be especially harmful. To measure the extent of such potential harms when translating into and out of English, we introduce a dataset, MiTTenS, covering 26 languages from a variety of language families and scripts, including several traditionally under-represented in digital resources. The dataset is constructed with handcrafted passages that target known failure patterns, longer synthetically generated passages, and natural passages sourced from multiple domains. We demonstrate the usefulness of the dataset by evaluating both neural machine translation systems and foundation models, and show that all systems exhibit gender mistranslation and potential harm, even in high resource languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.239.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--239 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.239/>Teaching <span class=acl-fixed-case>LLM</span>s to Abstain across Languages via Multilingual Feedback</a></strong><br><a href=/people/s/shangbin-feng/>Shangbin Feng</a>
|
<a href=/people/w/weijia-shi/>Weijia Shi</a>
|
<a href=/people/y/yike-wang/>Yike Wang</a>
|
<a href=/people/w/wenxuan-ding/>Wenxuan Ding</a>
|
<a href=/people/o/orevaoghene-ahia/>Orevaoghene Ahia</a>
|
<a href=/people/s/shuyue-stella-li/>Shuyue Stella Li</a>
|
<a href=/people/v/vidhisha-balachandran/>Vidhisha Balachandran</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--239><div class="card-body p-3 small">Multilingual LLMs often have knowledge disparities across languages, with larger gaps in under-resourced languages. Teaching LLMs to abstain in the face of knowledge gaps is thus a promising strategy to mitigate hallucinations in multilingual settings. However, previous studies on LLM abstention primarily focus on English; we find that directly applying existing solutions beyond English results in up to 20.5% performance gaps between high and low-resource languages, potentially due to LLMs’ drop in calibration and reasoning beyond a few resource-rich languages. To this end, we propose strategies to enhance LLM abstention by learning from multilingual feedback, where LLMs self-reflect on proposed answers in one language by generating multiple feedback items in related languages: we show that this helps identifying the knowledge gaps across diverse languages, cultures, and communities. Extensive experiments demonstrate that our multilingual feedback approach outperforms various strong baselines, achieving up to 9.2% improvement for low-resource languages across three black-box and open models on three datasets, featuring open-book, closed-book, and commonsense QA. Further analysis reveals that multilingual feedback is both an effective and a more equitable abstain strategy to serve diverse language speakers, and cultural factors have great impact on language selection and LLM abstention behavior, highlighting future directions for multilingual and multi-cultural reliable language modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.240.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--240 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.240/>Modular Pluralism: Pluralistic Alignment via Multi-<span class=acl-fixed-case>LLM</span> Collaboration</a></strong><br><a href=/people/s/shangbin-feng/>Shangbin Feng</a>
|
<a href=/people/t/taylor-sorensen/>Taylor Sorensen</a>
|
<a href=/people/y/yuhan-liu/>Yuhan Liu</a>
|
<a href=/people/j/jillian-fisher/>Jillian Fisher</a>
|
<a href=/people/c/chan-young-park/>Chan Young Park</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--240><div class="card-body p-3 small">While existing alignment paradigms have been integral in developing large language models (LLMs), LLMs often learn an averaged human preference and struggle to model diverse preferences across cultures, demographics, and communities. We propose Modular Pluralism, a modular framework based on multi-LLM collaboration for pluralistic alignment: it “plugs into” a base LLM a pool of smaller but specialized community LMs, where models collaborate in distinct modes to flexibility support three modes of pluralism: Overton, steerable, and distributional. Modular Pluralism is uniquely compatible with black-box LLMs and offers the modular control of adding new community LMs for previously underrepresented communities. We evaluate Modular Pluralism with six tasks and four datasets featuring questions/instructions with value-laden and perspective-informed responses. Extensive experiments demonstrate that Modular Pluralism advances the three pluralism objectives across six black-box and open-source LLMs. Further analysis reveals that LLMs are generally faithful to the inputs from smaller community LLMs, allowing seamless patching by adding a new community LM to better cover previously underrepresented communities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.241.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--241 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.241/><span class=acl-fixed-case>S</span>tyle<span class=acl-fixed-case>R</span>emix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements</a></strong><br><a href=/people/j/jillian-fisher/>Jillian Fisher</a>
|
<a href=/people/s/skyler-hallinan/>Skyler Hallinan</a>
|
<a href=/people/x/ximing-lu/>Ximing Lu</a>
|
<a href=/people/m/mitchell-l-gordon/>Mitchell L Gordon</a>
|
<a href=/people/z/zaid-harchaoui/>Zaid Harchaoui</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--241><div class="card-body p-3 small">Authorship obfuscation, rewriting a text to intentionally obscure the identity of the author, is important yet challenging. Current methods using large language models (LLMs) lack interpretability and controllability, often ignoring author-specific stylistic features, resulting in less robust performance overall.To address this, we develop StyleRemix, an adaptive and interpretable obfuscation method that perturbs specific, fine-grained style elements of the original input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA) modules to rewrite inputs along various stylistic axes (e.g., formality, length) while maintaining low computational costs. StyleRemix outperforms state-of-the-art baselines and much larger LLMs on an array of domains on both automatic and human evaluation.Additionally, we release AuthorMix, a large set of 30K high-quality, long-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a parallel corpus of 1,500 texts spanning seven style axes in 16 unique directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.242.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.242/><span class=acl-fixed-case>I</span> Could’ve Asked That: Reformulating Unanswerable Questions</a></strong><br><a href=/people/w/wenting-zhao/>Wenting Zhao</a>
|
<a href=/people/g/ge-gao/>Ge Gao</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a>
|
<a href=/people/a/alexander-m-rush/>Alexander M Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--242><div class="card-body p-3 small">When seeking information from unfamiliar documents, users frequently pose questions that cannot be answered by the documents. While existing large language models (LLMs) identify these unanswerable questions, they do not assist users in reformulating their questions, thereby reducing their overall utility. We curate CouldAsk, an evaluation benchmark composed of existing and new datasets for document-grounded question answering, specifically designed to study reformulating unanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMs on CouldAsk. The results demonstrate the limited capabilities of these models in reformulating questions. Specifically, GPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the time, respectively. Error analysis shows that 62% of the unsuccessful reformulations stem from the models merely rephrasing the questions or even generating identical questions. We publicly release the benchmark and the code to reproduce the experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.243.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--243 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.243/><span class=acl-fixed-case>STOP</span>! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions</a></strong><br><a href=/people/r/robert-morabito/>Robert Morabito</a>
|
<a href=/people/s/sangmitra-madhusudan/>Sangmitra Madhusudan</a>
|
<a href=/people/t/tyler-mcdonald/>Tyler McDonald</a>
|
<a href=/people/a/ali-emami/>Ali Emami</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--243><div class="card-body p-3 small">Mitigating explicit and implicit biases in Large Language Models (LLMs) has become a critical focus in the field of natural language processing. However, many current methodologies evaluate scenarios in isolation, without considering the broader context or the spectrum of potential biases within each situation. To address this, we introduce the Sensitivity Testing on Offensive Progressions (STOP) dataset, which includes 450 offensive progressions containing 2,700 unique sentences of varying severity that progressively escalate from less to more explicitly offensive. Covering a broad spectrum of 9 demographics and 46 sub-demographics, STOP ensures inclusivity and comprehensive coverage. We evaluate several leading closed- and open-source models, including GPT-4, Mixtral, and Llama 3. Our findings reveal that even the best-performing models detect bias inconsistently, with success rates ranging from 19.3% to 69.8%. Furthermore, we demonstrate how aligning models with human judgments on STOP can improve model answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs by up to 191%, while maintaining or even improving performance. STOP presents a novel framework for assessing the complex nature of biases in LLMs, which will enable more effective bias mitigation strategies and facilitates the creation of fairer language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.244.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.244.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--244 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.244 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.244/>Hidden Persuaders: <span class=acl-fixed-case>LLM</span>s’ Political Leaning and Their Influence on Voters</a></strong><br><a href=/people/y/yujin-potter/>Yujin Potter</a>
|
<a href=/people/s/shiyang-lai/>Shiyang Lai</a>
|
<a href=/people/j/junsol-kim/>Junsol Kim</a>
|
<a href=/people/j/james-evans/>James Evans</a>
|
<a href=/people/d/dawn-song/>Dawn Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--244><div class="card-body p-3 small">Do LLMs have political leanings and are LLMs able to shift our political views? This paper explores these questions in the context of the 2024 U.S. presidential election. Through a voting simulation, we demonstrate 18 open-weight and closed-source LLMs’ political preference for Biden over Trump. We show how Biden-leaning becomes more pronounced in instruction-tuned and reinforced models compared to their base versions by analyzing their responses to political questions related to the two nominees. We further explore the potential impact of LLMs on voter choice by recruiting 935 U.S. registered voters. Participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20% of Trump supporters reduced their support for Trump after LLM interaction. This result is noteworthy given that many studies on the persuasiveness of political campaigns have shown minimal effects in presidential elections. Many users also expressed a desire for further interaction with LLMs on political subjects. Further research on how LLMs affect users’ political views is required, as their use becomes more widespread.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.245.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.245.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--245 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.245 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.245.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.245/><span class=acl-fixed-case>SOUL</span>: Unlocking the Power of Second-Order Optimization for <span class=acl-fixed-case>LLM</span> Unlearning</a></strong><br><a href=/people/j/jinghan-jia/>Jinghan Jia</a>
|
<a href=/people/y/yihua-zhang/>Yihua Zhang</a>
|
<a href=/people/y/yimeng-zhang/>Yimeng Zhang</a>
|
<a href=/people/j/jiancheng-liu/>Jiancheng Liu</a>
|
<a href=/people/b/bharat-runwal/>Bharat Runwal</a>
|
<a href=/people/j/james-diffenderfer/>James Diffenderfer</a>
|
<a href=/people/b/bhavya-kailkhura/>Bhavya Kailkhura</a>
|
<a href=/people/s/sijia-liu/>Sijia Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--245><div class="card-body p-3 small">Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.246.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--246 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.246/>When Reasoning Meets Information Aggregation: A Case Study with Sports Narratives</a></strong><br><a href=/people/y/yebowen-hu/>Yebowen Hu</a>
|
<a href=/people/k/kaiqiang-song/>Kaiqiang Song</a>
|
<a href=/people/s/sangwoo-cho/>Sangwoo Cho</a>
|
<a href=/people/x/xiaoyang-wang/>Xiaoyang Wang</a>
|
<a href=/people/w/wenlin-yao/>Wenlin Yao</a>
|
<a href=/people/h/hassan-foroosh/>Hassan Foroosh</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a>
|
<a href=/people/f/fei-liu/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--246><div class="card-body p-3 small">Reasoning is most powerful when an LLM accurately aggregates relevant information. We examine the critical role of information aggregation in reasoning by requiring the LLM to analyze sports narratives. To succeed at this task, an LLM must infer points from actions, identify related entities, attribute points accurately to players and teams, and compile key statistics to draw conclusions. We conduct comprehensive experiments with real NBA basketball data and present SportsGen, a new method to synthesize game narratives. By synthesizing data, we can rigorously evaluate LLMs’ reasoning capabilities under complex scenarios with varying narrative lengths and density of information. Our findings show that most models, including GPT-4o, often fail to accurately aggregate basketball scores due to frequent scoring patterns. Open-source models like Llama-3 further suffer from significant score hallucinations. Finally, the effectiveness of reasoning is influenced by narrative complexity, information density, and domain-specific terms, highlighting the challenges in analytical reasoning tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.247.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--247 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.247.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.247/>An Analysis of Multilingual <span class=acl-fixed-case>FA</span>ct<span class=acl-fixed-case>S</span>core</a></strong><br><a href=/people/v/vu-trong-kim/>Vu Trong Kim</a>
|
<a href=/people/m/michael-krumdick/>Michael Krumdick</a>
|
<a href=/people/v/varshini-reddy/>Varshini Reddy</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/v/viet-dac-lai/>Viet Dac Lai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--247><div class="card-body p-3 small">FActScore has gained popularity as a metric to estimate the factuality of long-form texts generated by Large Language Models (LLMs) in English. However, there has not been any work in studying the behavior of FActScore in other languages. This paper studies the limitations of each component in the four-component pipeline of FActScore in the multilingual setting. We introduce a new dataset for FActScore on texts generated by strong multilingual LLMs. Our evaluation shows that LLMs exhibit distinct behaviors in both fact extraction and fact scoring tasks. No LLM produces consistent and reliable FActScore across languages of varying levels of resources. We also find that the knowledge source plays an important role in the quality of the estimated FActScore. Using Wikipedia as the knowledge source may hinder the true FActScore of long-form text due to its limited coverage in medium- and low-resource languages. We also incorporate 3 mitigations to our knowledge source that ultimately improve FActScore estimation across all languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.248.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--248 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.248.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.248.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.248/>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</a></strong><br><a href=/people/s/seungone-kim/>Seungone Kim</a>
|
<a href=/people/j/juyoung-suk/>Juyoung Suk</a>
|
<a href=/people/s/shayne-longpre/>Shayne Longpre</a>
|
<a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/j/jamin-shin/>Jamin Shin</a>
|
<a href=/people/s/sean-welleck/>Sean Welleck</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/m/moontae-lee/>Moontae Lee</a>
|
<a href=/people/k/kyungjae-lee/>Kyungjae Lee</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--248><div class="card-body p-3 small">Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.249.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--249 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.249.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.249/><span class=acl-fixed-case>RAG</span>-<span class=acl-fixed-case>QA</span> Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering</a></strong><br><a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/p/peng-qi/>Peng Qi</a>
|
<a href=/people/y/yumo-xu/>Yumo Xu</a>
|
<a href=/people/j/jenyuan-wang/>Jenyuan Wang</a>
|
<a href=/people/l/lan-liu/>Lan Liu</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/v/vittorio-castelli/>Vittorio Castelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--249><div class="card-body p-3 small">Question answering based on retrieval augmented generation (RAG-QA) is an important research topic in NLP and has a wide range of real-world applications. However, most existing datasets for this task are either constructed using a single source corpus or consist of short extractive answers, which fall short of evaluating large language model (LLM) based RAG-QA systems on cross-domain generalization. To address these limitations, we create Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form answers that integrate short extractive answers from multiple documents into a single, coherent narrative, covering 26K queries and large corpora across seven different domains. We further propose RAG-QA Arena by directly comparing model-generated answers against LFRQA’s answers using LLMs as evaluators. We show via extensive experiments that RAG-QA Arena and human judgments on answer quality are highly correlated. Moreover, only 41.3% of the most competitive LLM’s answers are preferred to LFRQA’s answers, demonstrating RAG-QA Arena as a challenging evaluation platform for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.250.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.250.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--250 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.250 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.250/><span class=acl-fixed-case>P</span>rompt<span class=acl-fixed-case>R</span>eps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval</a></strong><br><a href=/people/s/shengyao-zhuang/>Shengyao Zhuang</a>
|
<a href=/people/x/xueguang-ma/>Xueguang Ma</a>
|
<a href=/people/b/bevan-koopman/>Bevan Koopman</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a>
|
<a href=/people/g/guido-zuccon/>Guido Zuccon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--250><div class="card-body p-3 small">Utilizing large language models (LLMs) for zero-shot document ranking is done in one of two ways: (1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and (2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but require a large amount of paired text data for contrastive training.In this paper, we propose PromptReps, which combines the advantages of both categories: no need for training and the ability to retrieve from the whole corpus. Our method only requires prompts to guide an LLM to generate query and document representations for effective document retrieval. Specifically, we prompt the LLMs to represent a given text using a single word, and then use the last token’s hidden states and the corresponding logits associated with the prediction of the next token to construct a hybrid document retrieval system. The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM.Our experimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.251.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--251 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.251.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.251/>Voices Unheard: <span class=acl-fixed-case>NLP</span> Resources and Models for <span class=acl-fixed-case>Y</span>orùbá Regional Dialects</a></strong><br><a href=/people/o/orevaoghene-ahia/>Orevaoghene Ahia</a>
|
<a href=/people/a/anuoluwapo-aremu/>Anuoluwapo Aremu</a>
|
<a href=/people/d/diana-abagyan/>Diana Abagyan</a>
|
<a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/d/david-ifeoluwa-adelani/>David Ifeoluwa Adelani</a>
|
<a href=/people/d/daud-abolade/>Daud Abolade</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--251><div class="card-body p-3 small">Yoruba—an African language with roughly 47 million speakers—encompasses a continuum with several dialects. Recent efforts to develop NLP technologies for African languages have focused on their standard dialects, resulting in disparities for dialects and varieties for which there are little to no resources or tools. We take steps towards bridging this gap by introducing a new high-quality parallel text and speech corpus; YORULECT across three domains and four regional yoruba dialects. To develop this corpus, we engaged native speakers, traveling to communities where these dialects are spoken, to collect text and speech data. Using our newly created corpus, we conducted extensive experiments on (text) machine translation, automatic speech recognition, and speech-to-text translation. Our results reveal substantial performance disparities between standard yoruba and the other dialects across all tasks. However, we also show that with dialect-adaptive finetuning, we are able to narrow this gap. We believe our dataset and experimental analysis will contribute greatly to developing NLP tools for Yoruba and its dialects, and potentially for other African languages, by improving our understanding of existing challenges and offering a high-quality dataset for further development. We will release YORULECT dataset and models publicly under an open license.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.252.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--252 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.252.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.252/><span class=acl-fixed-case>ARES</span>: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse <span class=acl-fixed-case>AI</span> Feedback</a></strong><br><a href=/people/j/ju-seung-byun/>Ju-Seung Byun</a>
|
<a href=/people/j/jiyun-chun/>Jiyun Chun</a>
|
<a href=/people/j/jihyung-kil/>Jihyung Kil</a>
|
<a href=/people/a/andrew-perrault/>Andrew Perrault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--252><div class="card-body p-3 small">Large Multimodal Models (LMMs) excel at comprehending human instructions and demonstrate remarkable results across a broad spectrum of tasks. Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs by aligning them with specific preferences. These methods primarily use ranking-based feedback for entire generations. With advanced AI models (Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of detailed feedback that are expensive for humans to provide. We propose a two-stage algorithm ARES that Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT). First, we ask the Teacher to score how much each sentence contributes to solving the problem in a Chain-of-Thought (CoT). This sentence-level feedback allows us to consider individual valuable segments, providing more granular rewards for the RL procedure. Second, we ask the Teacher to correct wrong reasoning after the RL stage. The RL procedure requires substantial hyperparameter tuning and often generates errors such as repetitive words and incomplete sentences. With correction feedback, we stabilize the RL fine-tuned model through SFT. We conduct experiments on the multi-modal datasets ScienceQA and A-OKVQA to demonstrate the effectiveness of our proposal. The ARES rationale achieves around 70% win rate compared to baseline models judged by GPT-4o. Additionally, we observe that the improved rationale reasoning leads to a 2.5% increase in inference answer accuracy on average for the multi-modal datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.253.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.253.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--253 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.253 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.253/>Order of Magnitude Speedups for <span class=acl-fixed-case>LLM</span> Membership Inference</a></strong><br><a href=/people/r/rongting-zhang/>Rongting Zhang</a>
|
<a href=/people/m/martin-andres-bertran/>Martin Andres Bertran</a>
|
<a href=/people/a/aaron-roth/>Aaron Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--253><div class="card-body p-3 small">Large Language Models (LLMs) have the promise to revolutionize computing broadly, but their complexity and extensive training data also expose significant privacy vulnerabilities. One of the simplest privacy risks associated with LLMs is their susceptibility to membership inference attacks (MIAs), wherein an adversary aims to determine whether a specific data point was part of the model’s training set. Although this is a known risk, state of the art methodologies for MIAs rely on training multiple computationally costly ‘shadow models’, making risk evaluation prohibitive for large models. Here we adapt a recent line of work which uses quantile regression to mount membership inference attacks; we extend this work by proposing a low-cost MIA that leverages an ensemble of small quantile regression models to determine if a document belongs to the model’s training set or not. We demonstrate the effectiveness of this approach on fine-tuned LLMs of varying families (OPT, Pythia, Llama) and across multiple datasets. Across all scenarios we obtain comparable or improved accuracy compared to state of the art ‘shadow model’ approaches, with as little as 6% of their computation budget. We demonstrate increased effectiveness across multi-epoch trained target models, and architecture miss-specification robustness, that is, we can mount an effective attack against a model using a different tokenizer and architecture, without requiring knowledge on the target model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.254.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--254 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.254/><span class=acl-fixed-case>VIMI</span>: Grounding Video Generation through Multi-modal Instruction</a></strong><br><a href=/people/y/yuwei-fang/>Yuwei Fang</a>
|
<a href=/people/w/willi-menapace/>Willi Menapace</a>
|
<a href=/people/a/aliaksandr-siarohin/>Aliaksandr Siarohin</a>
|
<a href=/people/t/tsai-shien-chen/>Tsai-Shien Chen</a>
|
<a href=/people/k/kuan-chieh-wang/>Kuan-Chieh Wang</a>
|
<a href=/people/i/ivan-skorokhodov/>Ivan Skorokhodov</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/s/sergey-tulyakov/>Sergey Tulyakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--254><div class="card-body p-3 small">Existing text-to-video diffusion models rely solely on text-only encoders for their pretraining. This limitation stems from the absence of large-scale multimodal prompt video datasets, resulting in a lack of visual grounding and restricting their versatility and application in multimodal integration. To address this, we construct a large-scale multimodal prompt dataset by employing retrieval methods to pair in-context examples with the given text prompts and then utilize a two-stage training strategy to enable diverse video generation tasks within a model. In the first stage, we propose a multimodal conditional video generation framework for pretraining on these augmented datasets, establishing a foundational model for grounded video generation. Secondly, we fine-tune the model from the first stage on various video generation tasks, incorporating multimodal instructions. This process further refines the model’s ability to handle diverse inputs and tasks, ensuring seamless integration of multimodal information. After this two-stage training process, VIMI demonstrates multimodal understanding capabilities, producing contextually rich and personalized videos grounded in the provided inputs, as shown in Figure1. Compared to previous subject-driven video generation methods, our generator can synthesize consistent and temporally coherent videos with large motion while retaining the semantic control. Our generator also achieves state-of-the-art text-to-video generation results on UCF101 benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.255.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--255 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.255/><span class=acl-fixed-case>F</span><span class=tex-math><sup>2</sup></span><span class=acl-fixed-case>RL</span>: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation</a></strong><br><a href=/people/h/haiyang-wang/>Haiyang Wang</a>
|
<a href=/people/y/yuchen-pan/>Yuchen Pan</a>
|
<a href=/people/x/xin-song/>Xin Song</a>
|
<a href=/people/x/xuechen-zhao/>Xuechen Zhao</a>
|
<a href=/people/m/minghao-hu/>Minghao Hu</a>
|
<a href=/people/b/bin-zhou/>Bin Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--255><div class="card-body p-3 small">Hate speech (HS) on social media exacerbates misinformation and baseless prejudices. Evidence-supported counterspeech (CS) is crucial for correcting misinformation and reducing prejudices through facts. Existing methods for generating evidence-supported CS often lack clear guidance with a core claim for organizing evidence and do not adequately address factuality and faithfulness hallucinations in CS within anti-hate contexts. In this paper, to mitigate the aforementioned, we propose F<span class=tex-math><sup>2</sup></span>RL, a Factuality and Faithfulness Reinforcement Learning framework for generating claim-guided and evidence-supported CS. Firstly, we generate counter-claims based on hate speech and design a self-evaluation mechanism to select the most appropriate one. Secondly, we propose a coarse-to-fine evidence retrieval method. This method initially generates broad queries to ensure the diversity of evidence, followed by carefully reranking the retrieved evidence to ensure its relevance to the claim. Finally, we design a reinforcement learning method with a triplet-based factuality reward model and a multi-aspect faithfulness reward model. The method rewards the generator to encourage greater factuality, more accurate refutation of hate speech, consistency with the claim, and better utilization of evidence. Extensive experiments on three benchmark datasets demonstrate that the proposed framework achieves excellent performance in CS generation, with strong factuality and faithfulness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.256.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--256 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.256/>Deciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning</a></strong><br><a href=/people/c/chang-yang/>Chang Yang</a>
|
<a href=/people/p/peng-zhang/>Peng Zhang</a>
|
<a href=/people/h/hui-gao/>Hui Gao</a>
|
<a href=/people/j/jing-zhang/>Jing Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--256><div class="card-body p-3 small">Social networks are rife with noise and misleading information, presenting multifaceted challenges for rumor detection. In this paper, from the perspective of human cognitive subjectivity, we introduce the mining of individual latent intentions and propose a novel multi-task learning framework, the Intent-Aware Rumor Detection Network (IRDNet). IRDNet is designed to discern multi-level rumor semantic features and latent user intentions, addressing the challenges of robustness and key feature mining and alignment that plague existing models. In IRDNet, the multi-level semantic extraction module captures sequential and hierarchical features to generate robust semantic representations. The hierarchical contrastive learning module incorporates two complementary strategies, event-level and intent-level, to establish cognitive anchors that uncover the latent intentions of information disseminators. Event-level contrastive learning employs high-quality data augmentation and adversarial perturbations to enhance model robustness. Intent-level contrastive learning leverages the intent encoder to capture latent intent features and optimize consistency within the same intent while ensuring heterogeneity between different intents to clearly distinguish key features from irrelevant elements. Experimental results demonstrate that IRDNet significantly improves the effectiveness of rumor detection and effectively addresses the challenges present in the field of rumor detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.257.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.257.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--257 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.257 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.257/>Visual Prompting in <span class=acl-fixed-case>LLM</span>s for Enhancing Emotion Recognition</a></strong><br><a href=/people/q/qixuan-zhang/>Qixuan Zhang</a>
|
<a href=/people/z/zhifeng-wang/>Zhifeng Wang</a>
|
<a href=/people/d/dylan-zhang/>Dylan Zhang</a>
|
<a href=/people/w/wenjia-niu/>Wenjia Niu</a>
|
<a href=/people/s/sabrina-caldwell/>Sabrina Caldwell</a>
|
<a href=/people/t/tom-gedeon/>Tom Gedeon</a>
|
<a href=/people/y/yang-liu/>Yang Liu</a>
|
<a href=/people/z/zhenyue-qin/>Zhenyue Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--257><div class="card-body p-3 small">Vision Large Language Models (VLLMs) are transforming the intersection of computer vision and natural language processing; however, the potential of using visual prompts for emotion recognition in these models remains largely unexplored and untapped. Traditional methods in VLLMs struggle with spatial localization and often discard valuable global context. We propose a novel Set-of-Vision prompting (SoV) approach that enhances zero-shot emotion recognition by using spatial information, such as bounding boxes and facial landmarks, to mark targets precisely. SoV improves accuracy in face count and emotion categorization while preserving the enriched image context. Through comprehensive experimentation and analysis of recent commercial or open-source VLLMs, we evaluate the SoV model’s ability to comprehend facial expressions in natural environments. Our findings demonstrate the effectiveness of integrating spatial visual prompts into VLLMs for improving emotion recognition performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.258.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--258 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.258/><span class=acl-fixed-case>IDEAW</span>: Robust Neural Audio Watermarking with Invertible Dual-Embedding</a></strong><br><a href=/people/p/pengcheng-li/>Pengcheng Li</a>
|
<a href=/people/x/xulong-zhang/>Xulong Zhang</a>
|
<a href=/people/j/jing-xiao/>Jing Xiao</a>
|
<a href=/people/j/jianzong-wang/>Jianzong Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--258><div class="card-body p-3 small">The audio watermarking technique embeds messages into audio and accurately extracts messages from the watermarked audio. Traditional methods develop algorithms based on expert experience to embed watermarks into the time-domain or transform-domain of signals. With the development of deep neural networks, deep learning-based neural audio watermarking has emerged. Compared to traditional algorithms, neural audio watermarking achieves better robustness by considering various attacks during training. However, current neural watermarking methods suffer from low capacity and unsatisfactory imperceptibility. Additionally, the issue of watermark locating, which is extremely important and even more pronounced in neural audio water- marking, has not been adequately studied. In this paper, we design a dual-embedding wa- termarking model for efficient locating. We also consider the impact of the attack layer on the invertible neural network in robustness training, improving the model to enhance both its reasonableness and stability. Experiments show that the proposed model, IDEAW, can withstand various attacks with higher capacity and more efficient locating ability compared to existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.259.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--259 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.259/>Leveraging Conflicts in Social Media Posts: Unintended Offense Dataset</a></strong><br><a href=/people/c/che-wei-tsai/>Che Wei Tsai</a>
|
<a href=/people/y/yen-hao-huang/>Yen-Hao Huang</a>
|
<a href=/people/t/tsu-keng-liao/>Tsu-Keng Liao</a>
|
<a href=/people/d/didier-fernando-salazar-estrada/>Didier Fernando Salazar Estrada</a>
|
<a href=/people/r/retnani-latifah/>Retnani Latifah</a>
|
<a href=/people/y/yi-shin-chen/>Yi-Shin Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--259><div class="card-body p-3 small">In multi-person communications, conflicts often arise. Each individual may have their own perspective, which can differ. Additionally, commonly referenced offensive datasets frequently neglect contextual information and are primarily constructed with a focus on intended offenses. This study suggests that conflicts are pivotal in revealing a broader range of human interactions, including instances of unintended offensive language. This paper proposes a conflict-based data collection method to utilize inter-conflict cues in multi-person communications. By focusing on specific cue posts within conversation threads, our proposed approach effectively identifies relevant instances for analysis. Detailed analyses are provided to showcase the proposed approach efficiently gathers data on subtly offensive content. The experimental results indicate that incorporating elements of conflict into data collection significantly enhances the comprehensiveness and accuracy of detecting offensive language but also enriches our understanding of conflict dynamics in digital communication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.260.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.260.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--260 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.260 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.260/>Outcome-Constrained Large Language Models for Countering Hate Speech</a></strong><br><a href=/people/l/lingzi-hong/>Lingzi Hong</a>
|
<a href=/people/p/pengcheng-luo/>Pengcheng Luo</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a>
|
<a href=/people/x/xiaoying-song/>Xiaoying Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--260><div class="card-body p-3 small">Automatic counterspeech generation methods have been developed to assist efforts in combating hate speech. Existing research focuses on generating counterspeech with linguistic attributes such as being polite, informative, and intent-driven. However, the real impact of counterspeech in online environments is seldom considered. This study aims to develop methods for generating counterspeech constrained by conversation outcomes and evaluate their effectiveness. We experiment with large language models (LLMs) to incorporate into the text generation process two desired conversation outcomes: low conversation incivility and non-hateful hater reentry. Specifically, we experiment with instruction prompts, LLM finetuning, and LLM reinforcement learning (RL). Evaluation results show that our methods effectively steer the generation of counterspeech toward the desired outcomes. Our analyses, however, show that there are differences in the quality and style depending on the model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.261.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.261.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--261 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.261 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.261/>Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing</a></strong><br><a href=/people/c/changbing-yang/>Changbing Yang</a>
|
<a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--261><div class="card-body p-3 small">In this paper, we address the data scarcity problem in automatic data-driven glossing for low-resource languages by coordinating multiple sources of linguistic expertise. We enhance models by incorporating both token-level and sentence-level translations, utilizing the extensive linguistic capabilities of modern LLMs, and incorporating available dictionary resources. Our enhancements lead to an average absolute improvement of 5%-points in word-level accuracy over the previous state of the art on a typologically diverse dataset spanning six low-resource languages. The improvements are particularly noticeable for the lowest-resourced language Gitksan, where we achieve a 10%-point improvement. Furthermore, in a simulated ultra-low resource setting for the same six languages, training on fewer than 100 glossed sentences, we establish an average 10%-point improvement in word-level accuracy over the previous state-of-the-art system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.262.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.262.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.262.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.262/>Adaptive Immune-based Sound-Shape Code Substitution for Adversarial <span class=acl-fixed-case>C</span>hinese Text Attacks</a></strong><br><a href=/people/a/ao-wang/>Ao Wang</a>
|
<a href=/people/x/xinghao-yang/>Xinghao Yang</a>
|
<a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/b/bao-di-liu/>Bao-di Liu</a>
|
<a href=/people/w/weifeng-liu/>Weifeng Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--262><div class="card-body p-3 small">Adversarial textual examples reveal the vulnerability of natural language processing (NLP) models. Most existing text attack methods are designed for English text, while the robust implementation of the second popular language, i.e., Chinese with 1 billion users, is greatly underestimated. Although several Chinese attack methods have been presented, they either directly transfer from English attacks or adopt simple greedy search to optimize the attack priority, usually leading to unnatural sentences. To address these issues, we propose an adaptive Immune-based Sound-Shape Code (ISSC) algorithm for adversarial Chinese text attacks. Firstly, we leverage the Sound-Shape code to generate natural substitutions, which comprehensively integrate multiple Chinese features. Secondly, we employ adaptive immune algorithm (IA) to determine the replacement order, which can reduce the duplication of population to improve the search ability. Extensive experimental results validate the superiority of our ISSC in producing high-quality Chinese adversarial texts. Our code and data can be found in https://github.com/nohuma/chinese-attack-issc.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.263.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--263 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.263/>Bootstrapped Policy Learning for Task-oriented Dialogue through Goal Shaping</a></strong><br><a href=/people/y/yangyang-zhao/>Yangyang Zhao</a>
|
<a href=/people/b/ben-niu/>Ben Niu</a>
|
<a href=/people/m/mehdi-dastani/>Mehdi Dastani</a>
|
<a href=/people/s/shihan-wang/>Shihan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--263><div class="card-body p-3 small">Reinforcement learning shows promise in optimizing dialogue policies, but addressing the challenge of reward sparsity remains crucial. While curriculum learning offers a practical solution by strategically training policies from simple to complex, it hinges on the assumption of a gradual increase in goal difficulty to ensure a smooth knowledge transition across varied complexities. In complex dialogue environments without intermediate goals, achieving seamless knowledge transitions becomes tricky. This paper proposes a novel Bootstrapped Policy Learning (BPL) framework, which adaptively tailors progressively challenging subgoal curriculum for each complex goal through goal shaping, ensuring a smooth knowledge transition. Goal shaping involves goal decomposition and evolution, decomposing complex goals into subgoals with solvable maximum difficulty and progressively increasing difficulty as the policy improves. Moreover, to enhance BPL’s adaptability across various environments, we explore various combinations of goal decomposition and evolution within BPL, and identify two universal curriculum patterns that remain effective across different dialogue environments, independent of specific environmental constraints. By integrating the summarized curriculum patterns, our BPL has exhibited efficacy and versatility across four publicly available datasets with different difficulty levels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.264.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.264.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.264.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.264/><span class=acl-fixed-case>P</span>sy<span class=acl-fixed-case>GUARD</span>: An Automated System for Suicide Detection and Risk Assessment in Psychological Counseling</a></strong><br><a href=/people/h/huachuan-qiu/>Huachuan Qiu</a>
|
<a href=/people/l/lizhi-ma/>Lizhi Ma</a>
|
<a href=/people/z/zhenzhong-lan/>Zhenzhong Lan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--264><div class="card-body p-3 small">As awareness of mental health issues grows, online counseling support services are becoming increasingly prevalent worldwide. Detecting whether users express suicidal ideation in text-based counseling services is crucial for identifying and prioritizing at-risk individuals. However, the lack of domain-specific systems to facilitate fine-grained suicide detection and corresponding risk assessment in online counseling poses a significant challenge for automated crisis intervention aimed at suicide prevention. In this paper, we propose PsyGUARD, an automated system for detecting suicide ideation and assessing risk in psychological counseling. To achieve this, we first develop a detailed taxonomy for detecting suicide ideation based on foundational theories. We then curate a large-scale, high-quality dataset called PsySUICIDE for suicide detection. To evaluate the capabilities of automated systems in fine-grained suicide detection, we establish a range of baselines. Subsequently, to assist automated services in providing safe, helpful, and tailored responses for further assessment, we propose to build a suite of risk assessment frameworks. Our study not only provides an insightful analysis of the effectiveness of automated risk assessment systems based on fine-grained suicide detection but also highlights their potential to improve mental health services on online counseling platforms. Code, data, and models are available at https://github.com/qiuhuachuan/PsyGUARD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.265.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.265.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--265 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.265 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.265/>World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering</a></strong><br><a href=/people/j/jiacong-wang/>Jiacong Wang</a>
|
<a href=/people/b/bohong-wu/>Bohong Wu</a>
|
<a href=/people/h/haiyong-jiang/>Haiyong Jiang</a>
|
<a href=/people/z/zhou-xun/>Zhou Xun</a>
|
<a href=/people/x/xin-xiao/>Xin Xiao</a>
|
<a href=/people/h/haoyuan-guo/>Haoyuan Guo</a>
|
<a href=/people/j/jun-xiao/>Jun Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--265><div class="card-body p-3 small">Recent advances in Vision-Language Models (VLMs) and the scarcity of high-quality multi-modal alignment data have inspired numerous researches on synthetic VLM data generation. The conventional norm in VLM data construction uses a mixture of specialists in caption and OCR, or stronger VLM APIs and expensive human annotation.In this paper, we present World to Code (<span class=tex-math>W2C</span>), a meticulously curated multi-modal data construction pipeline that organizes the final generation output into a Python code format. The pipeline leverages the VLM itself to extract cross-modal information via different prompts and filter the generated outputs again via a consistency filtering strategy. Experiments have demonstrated the high quality of <span class=tex-math>W2C</span> by improving various existing visual question answering and visual grounding benchmarks across different VLMs. Further analysis also demonstrates that the new code parsing ability of VLMs presents better cross-modal equivalence than the commonly used detail caption ability. Our code is available at https://github.com/foundation-multimodal-models/World2Code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.266.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--266 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.266.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.266/><span class=acl-fixed-case>DVD</span>: Dynamic Contrastive Decoding for Knowledge Amplification in Multi-Document Question Answering</a></strong><br><a href=/people/j/jing-jin/>Jing Jin</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a>
|
<a href=/people/h/hao-zhang/>Hao Zhang</a>
|
<a href=/people/x/xiaoguang-li/>Xiaoguang Li</a>
|
<a href=/people/z/zhijiang-guo/>Zhijiang Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--266><div class="card-body p-3 small">Large language models (LLMs) are widely used in question-answering (QA) systems but often generate information with hallucinations. Retrieval-augmented generation (RAG) offers a potential remedy, yet the uneven retrieval quality and irrelevant contents may distract LLMs.In this work, we address these issues at the generation phase by treating RAG as a multi-document QA task.We propose a novel decoding strategy, Dynamic Contrastive Decoding, which dynamically amplifies knowledge from selected documents during the generation phase. involves constructing inputs batchwise, designing new selection criteria to identify documents worth amplifying, and applying contrastive decoding with a specialized weight calculation to adjust the final logits used for sampling answer tokens. Zero-shot experimental results on ALCE-ASQA, NQ, TQA and PopQA benchmarks show that our method outperforms other decoding strategies. Additionally, we conduct experiments to validate the effectiveness of our selection criteria, weight calculation, and general multi-document scenarios. Our method requires no training and can be integrated with other methods to improve the RAG performance. Our codes will be publicly available at https://github.com/JulieJin-km/Dynamic_Contrastive_Decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.267.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--267 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.267.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.267.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.267/>How Do Humans Write Code? Large Models Do It the Same Way Too</a></strong><br><a href=/people/l/long-li/>Long Li</a>
|
<a href=/people/x/xuzheng-he/>Xuzheng He</a>
|
<a href=/people/h/haozhe-wang/>Haozhe Wang</a>
|
<a href=/people/l/linlin-wang/>Linlin Wang</a>
|
<a href=/people/l/liang-he/>Liang He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--267><div class="card-body p-3 small">Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model’s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.268.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.268.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--268 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.268 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.268/>Retrospex: Language Agent Meets Offline Reinforcement Learning Critic</a></strong><br><a href=/people/y/yufei-xiang/>Yufei Xiang</a>
|
<a href=/people/y/yiqun-shen/>Yiqun Shen</a>
|
<a href=/people/y/yeqin-zhang/>Yeqin Zhang</a>
|
<a href=/people/n/nguyen-cam-tu/>Nguyen Cam-Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--268><div class="card-body p-3 small">Large language models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experiences for improvement. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Unlike previous approaches, Retrospex does not directly integrate experiences into the LLM’s context. Instead, it combines the LLM’s action likelihood with action values estimated by a Reinforcement Learning (RL) Critic, which is trained on past experiences through an offline “retrospection” process. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its advantages over strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.269.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.269.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--269 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.269 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.269/>Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-Context Models</a></strong><br><a href=/people/x/xinyu-liu/>Xinyu Liu</a>
|
<a href=/people/r/runsong-zhao/>Runsong Zhao</a>
|
<a href=/people/p/pengcheng-huang/>Pengcheng Huang</a>
|
<a href=/people/c/chunyang-xiao/>Chunyang Xiao</a>
|
<a href=/people/b/bei-li/>Bei Li</a>
|
<a href=/people/j/jingang-wang/>Jingang Wang</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>JingBo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--269><div class="card-body p-3 small">Numerous recent works target to extend effective context length for language models and various methods, tasks and benchmarks exist to measure model’s effective memory length. However, through thorough investigations, we find limitations for currently existing evaluations on model’s memory. We provide an extensive survey for limitations in this work and propose a new method called forgetting curve to measure the memorization capability of long-context models. We show that forgetting curve has the advantage of being robust to the tested corpus and the experimental settings, of not relying on prompt and can be applied to any model size. We apply our forgetting curve to a large variety of models involving both transformer and RNN/SSM based architectures. Our measurement provides empirical evidence for the effectiveness of transformer extension techniques while raises questions for the effective length of RNN/SSM based models. We also examine the difference between our measurement and existing benchmarks as well as popular metrics for various models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.270.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--270 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.270/>Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive <span class=acl-fixed-case>LLM</span> Generation</a></strong><br><a href=/people/y/yuanjie-lyu/>Yuanjie Lyu</a>
|
<a href=/people/z/zihan-niu/>Zihan Niu</a>
|
<a href=/people/z/zheyong-xie/>Zheyong Xie</a>
|
<a href=/people/c/chao-zhang-tu/>Chao Zhang</a>
|
<a href=/people/t/tong-xu/>Tong Xu</a>
|
<a href=/people/y/yang-wang/>Yang Wang</a>
|
<a href=/people/e/enhong-chen/>Enhong Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--270><div class="card-body p-3 small">Despite the significant progress of large language models (LLMs) in various tasks, they often produce factual errors due to their limited internal knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with external knowledge sources, offers a promising solution. However, these methods can be misled by irrelevant paragraphs in retrieved documents. Due to the inherent uncertainty in LLM generation, inputting the entire document may introduce off-topic information, causing the model to deviate from the central topic and affecting the relevance of the generated content. To address these issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates plan tokens to guide subsequent generation in the plan stage. In the answer stage, the model selects relevant fine-grained paragraphs based on the plan and uses them for further answer generation. This plan-answer process is repeated iteratively until completion, enhancing generation relevance by focusing on specific topics. To implement this framework efficiently, we utilize a simple but effective multi-task prompt-tuning method, enabling the existing LLMs to handle both planning and answering. We comprehensively compare RPG with baselines across 5 knowledge-intensive generation tasks, demonstrating the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.271.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--271 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.271.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.271.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.271/><span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>E</span>vol: Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation</a></strong><br><a href=/people/r/renhao-li/>Renhao Li</a>
|
<a href=/people/m/minghuan-tan/>Minghuan Tan</a>
|
<a href=/people/d/derek-f-wong/>Derek F. Wong</a>
|
<a href=/people/m/min-yang/>Min Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--271><div class="card-body p-3 small">In recent years, instruction fine-tuning (IFT) on large language models (LLMs) has garnered considerable attention to enhance model performance on unseen tasks. Attempts have been made on automatic construction and effective selection for IFT data. However, we posit that previous methods have not fully harnessed the potential of LLMs for enhancing data quality. The responses within IFT data could be further enhanced by leveraging the capabilities of LLMs themselves.In this paper, we propose CoEvol, an LLM-based multi-agent cooperation framework for the improvement of responses for instructions. To effectively refine the responses, we develop an iterative framework following a _debate-advise-edit-judge_ paradigm. A two-stage multi-agent debate strategy is further devised to ensure the diversity and reliability of editing suggestions within the framework. Empirically, models equipped with CoEvol outperform competitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its effectiveness in enhancing instruction-following capabilities for LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.272.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.272.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--272 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.272 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.272/>A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners</a></strong><br><a href=/people/b/bowen-jiang/>Bowen Jiang</a>
|
<a href=/people/y/yangxinyu-xie/>Yangxinyu Xie</a>
|
<a href=/people/z/zhuoqun-hao/>Zhuoqun Hao</a>
|
<a href=/people/x/xiaomeng-wang/>Xiaomeng Wang</a>
|
<a href=/people/t/tanwi-mallick/>Tanwi Mallick</a>
|
<a href=/people/w/weijie-j-su/>Weijie J Su</a>
|
<a href=/people/c/camillo-jose-taylor/>Camillo Jose Taylor</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--272><div class="card-body p-3 small">This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.273.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--273 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.273.data.tgz data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.273/><span class=acl-fixed-case>B</span>ayesian Calibration of Win Rate Estimation with <span class=acl-fixed-case>LLM</span> Evaluators</a></strong><br><a href=/people/y/yicheng-gao/>Yicheng Gao</a>
|
<a href=/people/g/gonghan-xu/>Gonghan Xu</a>
|
<a href=/people/d/daisy-zhe-wang/>Zhe Wang</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--273><div class="card-body p-3 small">Recent advances in large language models (LLMs) show the potential of using LLMs as evaluators for assessing the quality of text generations from LLMs. However, applying LLM evaluators naively to compare different systems can lead to unreliable results due to the inaccuracy and intrinsic bias of LLM evaluators. In order to mitigate this problem, we propose two calibration methods, Bayesian Win-Rate Sampling (BWRS) and Bayesian Dawid-Skene, both of which leverage Bayesian inference to more accurately infer the true win rate of generative language models. We empirically validate our methods on six datasets covering story generation, summarization, and instruction following tasks. We show that both our methods are effective in improving the accuracy of win rate estimation using LLMs as evaluators, offering a promising direction for reliable automatic text quality evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.274.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.274.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--274 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.274 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.274.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.274.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.274/><span class=acl-fixed-case>M</span>u<span class=acl-fixed-case>M</span>ath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning</a></strong><br><a href=/people/s/shuo-yin/>Shuo Yin</a>
|
<a href=/people/w/weihao-you/>Weihao You</a>
|
<a href=/people/z/zhilong-ji/>Zhilong Ji</a>
|
<a href=/people/g/guoqiang-zhong/>Guoqiang Zhong</a>
|
<a href=/people/j/jinfeng-bai/>Jinfeng Bai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--274><div class="card-body p-3 small">The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math reasoning data. However, a great method to integrate the above two research paths and combine their advantages remains to be explored. In this work, we firstly include new math questions via **mu**lti-perspective data augmenting methods and then synthesize **code**-nested solutions to them. The open LLMs (e.g., Llama-2) are finetuned on the augmented dataset to get the resulting models, **MuMath-Code** (<span class=tex-math>𝜇</span>-Math-Code). During the inference phase, our MuMath-Code generates code and interacts with the external python interpreter to get the execution results. Therefore, MuMath-Code leverages the advantages of both the external tool and data augmentation. To fully leverage the advantages of our augmented data, we propose a two-stage training strategy: In Stage-1, we finetune Llama-2 on pure CoT data to get an intermediate model, which then is trained on the code-nested data in Stage-2 to get the resulting MuMath-Code.Our MuMath-Code-7B achieves 83.8% on GSM8K and 52.4% on MATH, while MuMath-Code-70B model achieves new state-of-the-art performance among open methods—achieving 90.7% on GSM8K and 55.1% on MATH. Extensive experiments validate the combination of tool use and data augmentation, as well as our two-stage training strategy.We release the proposed dataset along with the associated code for public use: https://github.com/youweihao-tal/MuMath-Code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.275.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--275 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.275.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.275/>Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients</a></strong><br><a href=/people/w/weijun-li/>Weijun Li</a>
|
<a href=/people/q/qiongkai-xu/>Qiongkai Xu</a>
|
<a href=/people/m/mark-dras/>Mark Dras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--275><div class="card-body p-3 small">Recent studies have shown that distributed machine learning is vulnerable to gradient inversion attacks, where private training data can be reconstructed by analyzing the gradients of the models shared in training. Previous attacks established that such reconstructions are possible using gradients from all parameters in the entire models. However, we hypothesize that most of the involved modules, or even their sub-modules, are at risk of training data leakage, and we validate such vulnerabilities in various intermediate layers of language models. Our extensive experiments reveal that gradients from a single Transformer layer, or even a single linear component with 0.54% parameters, are susceptible to training data leakage. Additionally, we show that applying differential privacy on gradients during training offers limited protection against the novel vulnerability of data disclosure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.276.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.276.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--276 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.276 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.276.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.276/><span class=acl-fixed-case>RWKV</span>-<span class=acl-fixed-case>CLIP</span>: A Robust Vision-Language Representation Learner</a></strong><br><a href=/people/t/tiancheng-gu/>Tiancheng Gu</a>
|
<a href=/people/k/kaicheng-yang/>Kaicheng Yang</a>
|
<a href=/people/x/xiang-an/>Xiang An</a>
|
<a href=/people/z/ziyong-feng/>Ziyong Feng</a>
|
<a href=/people/d/dongnan-liu/>Dongnan Liu</a>
|
<a href=/people/w/weidong-cai/>Weidong Cai</a>
|
<a href=/people/j/jiankang-deng/>Jiankang Deng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--276><div class="card-body p-3 small">Contrastive Language-Image Pre-training (CLIP) has significantly improved performance in various vision-language tasks by expanding the dataset with image-text pairs obtained from the web. This paper further explores CLIP from the perspectives of data and model architecture. To mitigate the impact of the noise data and enhance the quality of large-scale image-text data crawled from the internet, we introduce a diverse description generation framework that can leverage Large Language Models (LLMs) to combine and refine information from web-based image-text pairs, synthetic captions, and detection tags. Additionally, we propose RWKV-CLIP, the first RWKV-driven vision-language representation learning model that combines the effective parallel training of transformers with the efficient inference of RNNs. Extensive experiments across different model scales and pre-training datasets demonstrate that RWKV-CLIP is a robust vision-language representation learner and it achieves state-of-the-art performance across multiple downstream tasks, including linear probing, zero-shot classification, and zero-shot image-text retrieval. To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/RWKV-CLIP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.277.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.277.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--277 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.277 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.277/><span class=acl-fixed-case>K</span>id<span class=acl-fixed-case>LM</span>: Advancing Language Models for Children – Early Insights and Future Directions</a></strong><br><a href=/people/m/mir-tafseer-nayeem/>Mir Tafseer Nayeem</a>
|
<a href=/people/d/davood-rafiei/>Davood Rafiei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--277><div class="card-body p-3 small">Recent studies highlight the potential of large language models in creating educational tools for children, yet significant challenges remain in maintaining key child-specific properties such as linguistic nuances, cognitive needs, and safety standards. In this paper, we explore foundational steps toward the development of child-specific language models, emphasizing the necessity of high-quality pre-training data. We introduce a novel user-centric data collection pipeline that involves gathering and validating a corpus specifically written for and sometimes by children. Additionally, we propose a new training objective, Stratified Masking, which dynamically adjusts masking probabilities based on our domain-specific child language data, enabling models to prioritize vocabulary and concepts more suitable for children. Experimental evaluations demonstrate that our model excels in understanding lower grade-level text, maintains safety by avoiding stereotypes, and captures children’s unique preferences. Furthermore, we provide actionable insights for future research and development in child-specific language modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.278.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.278.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--278 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.278 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.278.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.278/>Using Language Models to Disambiguate Lexical Choices in Translation</a></strong><br><a href=/people/j/josh-barua/>Josh Barua</a>
|
<a href=/people/s/sanjay-subramanian/>Sanjay Subramanian</a>
|
<a href=/people/k/kayo-yin/>Kayo Yin</a>
|
<a href=/people/a/alane-suhr/>Alane Suhr</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--278><div class="card-body p-3 small">In translation, a concept represented by a single word in a source language can have multiple variations in a target language. The task of lexical selection requires using context to identify which variation is most appropriate for a source text. We work with native speakers of nine languages to create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual concept variation when translating from English. We evaluate recent LLMs and neural machine translation systems on DTAiLS, with the best-performing model, GPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use language models to generate English rules describing target-language concept variations. Providing weaker models with high-quality lexical rules improves accuracy substantially, in some cases reaching or outperforming GPT-4.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.279.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.279.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--279 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.279 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.279/>How Does the Disclosure of <span class=acl-fixed-case>AI</span> Assistance Affect the Perceptions of Writing?</a></strong><br><a href=/people/z/zhuoyan-li/>Zhuoyan Li</a>
|
<a href=/people/c/chen-liang/>Chen Liang</a>
|
<a href=/people/j/jing-peng/>Jing Peng</a>
|
<a href=/people/m/ming-yin/>Ming Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--279><div class="card-body p-3 small">Recent advances in generative AI technologies like large language models have boosted the incorporation of AI assistance in writing workflows, leading to the rise of a new paradigm of human-AI co-creation in writing. To understand how people perceive writings that are produced under this paradigm, in this paper, we conduct an experimental study to understand whether and how the disclosure of the level and type of AI assistance in the writing process would affect people’s perceptions of the writing on various aspects, including their evaluation on the quality of the writing, and their ranking of different writings. Our results suggest that disclosing the AI assistance in the writing process, especially if AI has provided assistance in generating new content, decreases the average quality ratings for both argumentative essays and creative stories. This decrease in the average quality ratings often comes with an increased level of variations in different individuals’ quality evaluations of the same writing. Indeed, factors such as an individual’s writing confidence and familiarity with AI writing assistants are shown to moderate the impact of AI assistance disclosure on their writing quality evaluations. We also find that disclosing the use of AI assistance may significantly reduce the proportion of writings produced with AI’s content generation assistance among the top-ranked writings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.280.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--280 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.280.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.280/>An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records</a></strong><br><a href=/people/j/joakim-edin/>Joakim Edin</a>
|
<a href=/people/m/maria-maistro/>Maria Maistro</a>
|
<a href=/people/l/lars-maaloe/>Lars Maaløe</a>
|
<a href=/people/l/lasse-borgholt/>Lasse Borgholt</a>
|
<a href=/people/j/jakob-drachmann-havtorn/>Jakob Drachmann Havtorn</a>
|
<a href=/people/t/tuukka-ruotsalo/>Tuukka Ruotsalo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--280><div class="card-body p-3 small">Electronic healthcare records are vital for patient safety as they document conditions, plans, and procedures in both free text and medical codes. Language models have significantly enhanced the processing of such records, streamlining workflows and reducing manual data entry, thereby saving healthcare providers significant resources. However, the black-box nature of these models often leaves healthcare professionals hesitant to trust them. State-of-the-art explainability methods increase model transparency but rely on human-annotated evidence spans, which are costly. In this study, we propose an approach to produce plausible and faithful explanations without needing such annotations. We demonstrate on the automated medical coding task that adversarial robustness training improves explanation plausibility and introduce AttInGrad, a new explanation method superior to previous ones. By combining both contributions in a fully unsupervised setup, we produce explanations of comparable quality, or better, to that of a supervised approach. We release our code and model weights.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.281.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--281 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.281/>Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs</a></strong><br><a href=/people/z/zheng-wang/>Zheng Wang</a>
|
<a href=/people/z/zhongyang-li/>Zhongyang Li</a>
|
<a href=/people/z/zeren-jiang/>Zeren Jiang</a>
|
<a href=/people/d/dandan-tu/>Dandan Tu</a>
|
<a href=/people/w/wei-shi/>Wei Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--281><div class="card-body p-3 small">In the age of mobile internet, user data, often referred to as memories, is continuously generated on personal devices. Effectively managing and utilizing this data to deliver services to users is a compelling research topic. In this paper, we introduce a novel task of crafting personalized agents powered by large language models (LLMs), which utilize a user’s smartphone memories to enhance downstream applications with advanced LLM capabilities. To achieve this goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach is further optimized using Reinforcement Learning to address three distinct challenges: data collection, editability, and selectability. Extensive experiments on a real-world dataset validate the effectiveness of EMG-RAG, achieving an improvement of approximately 10% over the best existing approach. Additionally, the personalized agents have been transferred into a real smartphone AI assistant, which leads to enhanced usability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.282.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.282.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--282 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.282 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.282.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.282/><span class=acl-fixed-case>EVEDIT</span>: Event-based Knowledge Editing for Deterministic Knowledge Propagation</a></strong><br><a href=/people/j/jiateng-liu/>Jiateng Liu</a>
|
<a href=/people/p/pengfei-yu/>Pengfei Yu</a>
|
<a href=/people/y/yuji-zhang/>Yuji Zhang</a>
|
<a href=/people/s/sha-li/>Sha Li</a>
|
<a href=/people/z/zixuan-zhang/>Zixuan Zhang</a>
|
<a href=/people/r/ruhi-sarikaya/>Ruhi Sarikaya</a>
|
<a href=/people/k/kevin-small/>Kevin Small</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--282><div class="card-body p-3 small">The dynamic nature of real-world information necessitates knowledge editing (KE) in large language models (LLMs). The edited knowledge should propagate and facilitate the deduction of new information based on existing model knowledge. We term the existing related knowledge in LLM serving as the origination of knowledge propagation as ”deduction anchors”. However, current KE approaches, which only operate on (subject, relation, object) triple. We both theoretically and empirically observe that this simplified setting often leads to uncertainty when determining the deduction anchors, causing low confidence in their answers. To mitigate this issue, we propose a novel task of event-based knowledge editing that pairs facts with event descriptions. This task manifests not only a closer simulation of real-world editing scenarios but also a more logically sound setting, implicitly defining the deduction anchor and enabling LLMs to propagate knowledge confidently. We curate a new benchmark dataset Evedit derived from the CounterFact dataset and validate its superiority in improving model confidence. Moreover, while we observe that the event-based setting is significantly challenging for existing approaches, we propose a novel approach Self-Edit that showcases stronger performance, achieving 55.6% consistency improvement while maintaining the naturalness of generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.283.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.283/>Modeling Nonnative Sentence Processing with <span class=acl-fixed-case>L</span>2 Language Models</a></strong><br><a href=/people/t/tatsuya-aoyama/>Tatsuya Aoyama</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--283><div class="card-body p-3 small">We study LMs pretrained sequentially on two languages (“L2LMs”) for modeling nonnative sentence processing. In particular, we pretrain GPT2 on 6 different first languages (L1s), followed by English as the second language (L2). We examine the effect of the choice of pretraining L1 on the model’s ability to predict human reading times, evaluating on English readers from a range of L1 backgrounds. Experimental results show that, while all of the LMs’ word surprisals improve prediction of L2 reading times, especially for human L1s distant from English, there is no reliable effect of the choice of L2LM’s L1. We also evaluate the learning trajectory of a monolingual English LM: for predicting L2 as opposed to L1 reading, it peaks much earlier and immediately falls off, possibly mirroring the difference in proficiency between the native and nonnative populations. Lastly, we provide examples of L2LMs’ surprisals, which could potentially generate hypotheses about human L2 reading.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.284.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.284.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--284 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.284 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.284/>From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis</a></strong><br><a href=/people/c/chuanqi-cheng/>Chuanqi Cheng</a>
|
<a href=/people/j/jian-guan/>Jian Guan</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--284><div class="card-body p-3 small">We explore multi-step reasoning in vision-language models (VLMs). The problem is challenging, as reasoning data consisting of multiple steps of visual and language processing are barely available. To overcome the challenge, we first introduce a least-to-most visual reasoning paradigm, which interleaves steps of decomposing a question into sub-questions and invoking external tools for resolving sub-questions. Based on the paradigm, we further propose a novel data synthesis approach that can automatically create questions and multi-step reasoning paths for an image in a bottom-up manner. Our approach divides the complex synthesis task into a few simple sub-tasks, and (almost entirely) relies on open-sourced models to accomplish the sub-tasks. Therefore, the entire synthesis process is reproducible and cost-efficient, and the synthesized data is quality guaranteed. With the approach, we construct 50k visual reasoning examples. Then, we develop a visual reasoner through supervised fine-tuning, which is capable of generally enhancing the reasoning abilities of a wide range of existing VLMs in a plug-and-play fashion. Extensive experiments indicate that the visual reasoner can consistently and significantly improve four VLMs on four VQA benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.285.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--285 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.285.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.285/>Quality Matters: Evaluating Synthetic Data for Tool-Using <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/s/shadi-iskander/>Shadi Iskander</a>
|
<a href=/people/s/sofia-tolmach/>Sofia Tolmach</a>
|
<a href=/people/o/ori-shapira/>Ori Shapira</a>
|
<a href=/people/n/nachshon-cohen/>Nachshon Cohen</a>
|
<a href=/people/z/zohar-karnin/>Zohar Karnin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--285><div class="card-body p-3 small">Training large language models (LLMs) for external tool usage is a rapidly expanding field, with recent research focusing on generating synthetic data to address the shortage of available data. However, the absence of systematic data quality checks poses complications for properly training and testing models. To that end, we propose two approaches for assessing the reliability of data for training LLMs to use external tools. The first approach uses intuitive, human-defined correctness criteria. The second approach uses a model-driven assessment with in-context evaluation. We conduct a thorough evaluation of data quality on two popular benchmarks, followed by an extrinsic evaluation that showcases the impact of data quality on model performance. Our results demonstrate that models trained on high-quality data outperform those trained on unvalidated data, even when trained with a smaller quantity of data. These findings empirically support the significance of assessing and ensuring the reliability of training data for tool-using LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.286.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.286.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--286 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.286 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.286/>Cross-Domain Audio Deepfake Detection: Dataset and Analysis</a></strong><br><a href=/people/y/yuang-li/>Yuang Li</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/m/mengxin-ren/>Mengxin Ren</a>
|
<a href=/people/x/xiaosong-qiao/>Xiaosong Qiao</a>
|
<a href=/people/m/miaomiao-ma/>Miaomiao Ma</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--286><div class="card-body p-3 small">Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1% and 6.5% respectively. Additionally, we demonstrate our models’ outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research. Our dataset is publicly available (https://github.com/leolya/CD-ADD).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.287.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--287 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.287/><span class=acl-fixed-case>M</span>a<span class=acl-fixed-case>PPER</span>: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension</a></strong><br><a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/z/zunnan-xu/>Zunnan Xu</a>
|
<a href=/people/y/yue-hu/>Yue Hu</a>
|
<a href=/people/l/liangtao-shi/>Liangtao Shi</a>
|
<a href=/people/z/zhiqiang-wang/>Zhiqiang Wang</a>
|
<a href=/people/q/quanjun-yin/>Quanjun Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--287><div class="card-body p-3 small">Referring Expression Comprehension (REC), which aims to ground a local visual region via natural language, is a task that heavily relies on multimodal alignment. Most existing methods utilize powerful pre-trained models to transfer visual/linguistic knowledge by full fine-tuning. However, full fine-tuning the entire backbone not only breaks the rich prior knowledge embedded in the pre-training, but also incurs significant computational costs. Motivated by the recent emergence of Parameter-Efficient Transfer Learning (PETL) methods, we aim to solve the REC task in an effective and efficient manner. Directly applying these PETL methods to the REC task is inappropriate, as they lack the specific-domain abilities for precise local visual perception and visual-language alignment. Therefore, we propose a novel framework of Multimodal Prior-guided Parameter Efficient Tuning, namely MaPPER. Specifically, MaPPER comprises Dynamic Prior Adapters guided by a aligned prior, and Local Convolution Adapters to extract precise local semantics for better visual perception. Moreover, the Prior-Guided Text module is proposed to further utilize the prior for facilitating the cross-modal alignment. Experimental results on three widely-used benchmarks demonstrate that MaPPER achieves the best accuracy compared to the full fine-tuning and other PETL methods with only 1.41% tunable backbone parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.288.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--288 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.288.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.288/>Hierarchical Deconstruction of <span class=acl-fixed-case>LLM</span> Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization</a></strong><br><a href=/people/m/miyoung-ko/>Miyoung Ko</a>
|
<a href=/people/s/sue-hyun-park/>Sue Hyun Park</a>
|
<a href=/people/j/joonsuk-park/>Joonsuk Park</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--288><div class="card-body p-3 small">Despite the advances in large language models (LLMs), how they use their knowledge for reasoning is not yet well understood.In this study, we propose a method that deconstructs complex real-world questions into a graph, representing each question as a node with predecessors of background knowledge needed to solve the question. We develop the DepthQA dataset, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, we quantify forward discrepancy, a discrepancy in LLM performance on simpler sub-problems versus complex questions. We also measure backward discrepancy where LLMs answer complex questions but struggle with simpler ones. Our analysis shows that smaller models exhibit more discrepancies than larger models. Distinct patterns of discrepancies are observed across model capacity and possibility of training data memorization. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances our understanding of LLM reasoning and suggests ways to improve their problem-solving abilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.289.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--289 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.289/>Aligning Translation-Specific Understanding to General Understanding in Large Language Models</a></strong><br><a href=/people/y/yichong-huang/>Yichong Huang</a>
|
<a href=/people/b/baohang-li/>Baohang Li</a>
|
<a href=/people/x/xiaocheng-feng/>Xiaocheng Feng</a>
|
<a href=/people/w/wenshuai-huo/>Wenshuai Huo</a>
|
<a href=/people/c/chengpeng-fu/>Chengpeng Fu</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--289><div class="card-body p-3 small">Large Language models (LLMs) have exhibited remarkable abilities in understanding complex texts, offering a promising path towards human-like translation performance. However, this study reveals the misalignment between the translation-specific understanding and the general understanding inside LLMs. This understanding misalignment leads to LLMs mistakenly or literally translating some complicated concepts that they accurately comprehend in the general scenarios (e.g., QA). To align the translation-specific understanding to the general one, we propose a novel translation process, DUAT (Difficult words Understanding Aligned Translation), explicitly incorporating the general understanding on the complicated content incurring inconsistent understandings to guide the translation. Specifically, DUAT performs cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations. Furthermore, we reframe the external tools to improve DUAT in detecting difficult words and generating helpful interpretations. We conduct experiments on the self-constructed benchmark Challenge-WMT, consisting of samples that are prone to mistranslation. Human evaluation results on high-resource and low-resource language pairs indicate that DUAT significantly facilitates the understanding alignment, which improves the translation quality (up to +3.85 COMET) and reduces translation literalness by -25% ∼ -51%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.290.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--290 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.290/><span class=acl-fixed-case>FOOL</span> <span class=acl-fixed-case>ME</span> <span class=acl-fixed-case>IF</span> <span class=acl-fixed-case>YOU</span> <span class=acl-fixed-case>CAN</span>! An Adversarial Dataset to Investigate the Robustness of <span class=acl-fixed-case>LM</span>s in Word Sense Disambiguation</a></strong><br><a href=/people/m/mohamad-ballout/>Mohamad Ballout</a>
|
<a href=/people/a/anne-dedert/>Anne Dedert</a>
|
<a href=/people/n/nohayr-muhammad-abdelmoneim/>Nohayr Muhammad Abdelmoneim</a>
|
<a href=/people/u/ulf-krumnack/>Ulf Krumnack</a>
|
<a href=/people/g/gunther-heidemann/>Gunther Heidemann</a>
|
<a href=/people/k/kai-uwe-kuhnberger/>Kai-Uwe Kühnberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--290><div class="card-body p-3 small">Word sense disambiguation (WSD) is a key task in natural language processing and lexical semantics. Pre-trained language models with contextualized word embeddings have significantly improved performance in regular WSD tasks. However, these models still struggle with recognizing semantic boundaries and often misclassify homonyms in adversarial context. Therefore, we propose FOOL: FOur-fold Obscure Lexical, a new coarse-grained WSD dataset, which includes four different test sets designed to assess the robustness of language models in WSD tasks. Two sets feature typical WSD scenarios, while the other two include sentences with opposing contexts to challenge the models further.We tested two types of models on the proposed dataset: models with encoders, such as the BERT and T5 series of varying sizes by probing their embeddings, and state-of-the-art large decoder models like GPT-4o and the LlaMA3 family, using zero shot prompting. Across different state-of-the-art language models, we observed a decrease in performance in the latter two sets compared to the first two, with some models being affected more than others. We show interesting findings where small models like T5-large and BERT-large performed better than GPT-4o on Set 3 of the dataset. This indicates that, despite excelling in regular WSD tasks, these models still struggle to correctly disambiguate homonyms in artificial (Set 3) or realistic adversarial contexts (Set 4).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.291.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.291.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--291 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.291 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.291/>Concept-skill Transferability-based Data Selection for Large Vision-Language Models</a></strong><br><a href=/people/j/jaewoo-lee/>Jaewoo Lee</a>
|
<a href=/people/b/boyang-li/>Boyang Li</a>
|
<a href=/people/s/sung-ju-hwang/>Sung Ju Hwang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--291><div class="card-body p-3 small">Instruction tuning, or supervised finetuning on extensive task-specific data, is necessary for Large Vision-Language Models (LVLMs) to generalize well across a broad range of vision-language (VL) tasks. However, training on large VL datasets can become prohibitively expensive. In this work, we introduce COINCIDE, an effective and scalable data selection technique that uses a small model as a reference model to select visual instruction tuning data for efficient finetuning of a target LVLM, focusing on diversity and transferability. Specifically, we cluster the training data using internal activations from a small model, which identifies VL concept-skill compositions needed by a target LVLM. We then sample data from these diverse clusters by considering their density and transferability, or the ability to transfer well to other concept-skill compositions. This approach ensures the diversity of these compositions, which is vital for LVLM generalization. Extensive experiments demonstrate that COINCIDE achieves superior performance and data selection efficiency against 8 strong baselines on two distinct datasets: LLaVA-1.5 and Vision-Flan. Using only 20% of the LLaVA-1.5 dataset, COINCIDE achieves performance comparable to the LVLM finetuned on the whole dataset, with 70% reduction of the wall-clock running time. On the Vision-Flan dataset, our method achieves superior results with only 16.7% of the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.292.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.292.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--292 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.292 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.292/><span class=acl-fixed-case>LLM</span>s Assist <span class=acl-fixed-case>NLP</span> Researchers: Critique Paper (Meta-)Reviewing</a></strong><br><a href=/people/j/jiangshu-du/>Jiangshu Du</a>
|
<a href=/people/y/yibo-wang/>Yibo Wang</a>
|
<a href=/people/w/wenting-zhao/>Wenting Zhao</a>
|
<a href=/people/z/zhongfen-deng/>Zhongfen Deng</a>
|
<a href=/people/s/shuaiqi-liu/>Shuaiqi Liu</a>
|
<a href=/people/r/renze-lou/>Renze Lou</a>
|
<a href=/people/h/henry-peng-zou/>Henry Peng Zou</a>
|
<a href=/people/p/pranav-narayanan-venkit/>Pranav Narayanan Venkit</a>
|
<a href=/people/n/nan-zhang/>Nan Zhang</a>
|
<a href=/people/m/mukund-srinath/>Mukund Srinath</a>
|
<a href=/people/h/haoran-ranran-zhang/>Haoran Ranran Zhang</a>
|
<a href=/people/v/vipul-gupta/>Vipul Gupta</a>
|
<a href=/people/y/yinghui-li/>Yinghui Li</a>
|
<a href=/people/t/tao-li/>Tao Li</a>
|
<a href=/people/f/fei-wang/>Fei Wang</a>
|
<a href=/people/q/qin-liu/>Qin Liu</a>
|
<a href=/people/t/tianlin-liu/>Tianlin Liu</a>
|
<a href=/people/p/pengzhi-gao/>Pengzhi Gao</a>
|
<a href=/people/c/congying-xia/>Congying Xia</a>
|
<a href=/people/c/chen-xing/>Chen Xing</a>
|
<a href=/people/c/cheng-jiayang/>Cheng Jiayang</a>
|
<a href=/people/z/zhaowei-wang/>Zhaowei Wang</a>
|
<a href=/people/y/ying-su/>Ying Su</a>
|
<a href=/people/r/raj-sanjay-shah/>Raj Sanjay Shah</a>
|
<a href=/people/r/ruohao-guo/>Ruohao Guo</a>
|
<a href=/people/j/jing-gu/>Jing Gu</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/k/kangda-wei/>Kangda Wei</a>
|
<a href=/people/z/zihao-wang/>Zihao Wang</a>
|
<a href=/people/l/lu-cheng/>Lu Cheng</a>
|
<a href=/people/s/surangika-ranathunga/>Surangika Ranathunga</a>
|
<a href=/people/m/meng-fang/>Meng Fang</a>
|
<a href=/people/j/jie-fu/>Jie Fu</a>
|
<a href=/people/f/fei-liu/>Fei Liu</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a>
|
<a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/p/philip-s-yu/>Philip S. Yu</a>
|
<a href=/people/w/wenpeng-yin/>Wenpeng Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--292><div class="card-body p-3 small">Claim: This work is not advocating the use of LLMs for paper (meta-)reviewing. Instead, wepresent a comparative analysis to identify and distinguish LLM activities from human activities. Two research goals: i) Enable better recognition of instances when someone implicitly uses LLMs for reviewing activities; ii) Increase community awareness that LLMs, and AI in general, are currently inadequate for performing tasks that require a high level of expertise and nuanced judgment.This work is motivated by two key trends. On one hand, large language models (LLMs) have shown remarkable versatility in various generative tasks such as writing, drawing, and question answering, significantly reducing the time required for many routine tasks. On the other hand, researchers, whose work is not only time-consuming but also highly expertise-demanding, face increasing challenges as they have to spend more time reading, writing, and reviewing papers. This raises the question: how can LLMs potentially assist researchers in alleviating their heavy workload?This study focuses on the topic of LLMs as NLP Researchers, particularly examining the effectiveness of LLMs in assisting paper (meta-)reviewing and its recognizability. To address this, we constructed the ReviewCritique dataset, which includes two types of information: (i) NLP papers (initial submissions rather than camera-ready) with both human-written and LLM-generated reviews, and (ii) each review comes with “deficiency” labels and corresponding explanations for individual segments, annotated by experts. Using ReviewCritique, this study explores two threads of research questions: (i) “LLMs as Reviewers”, how do reviews generated by LLMs compare with those written by humans in terms of quality and distinguishability? (ii) “LLMs as Metareviewers”, how effectively can LLMs identify potential issues, such as Deficient or unprofessional review segments, within individual paper reviews? To our knowledge, this is the first work to provide such a comprehensive analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.293.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.293.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--293 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.293 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.293/>Academics Can Contribute to Domain-Specialized Language Models</a></strong><br><a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/p/prabhanjan-kambadur/>Prabhanjan Kambadur</a>
|
<a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/o/ozan-irsoy/>Ozan Irsoy</a>
|
<a href=/people/s/steven-lu/>Steven Lu</a>
|
<a href=/people/v/vadim-dabravolski/>Vadim Dabravolski</a>
|
<a href=/people/d/david-s-rosenberg/>David S Rosenberg</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--293><div class="card-body p-3 small">Commercially available models dominate academic leaderboards. While impressive, this has concentrated research on creating and adapting general-purpose models to improve NLP leaderboard standings for large language models. However, leaderboards collect many individual tasks and general-purpose models often underperform in specialized domains; domain-specific or adapted models yield superior results. This focus on large general-purpose models excludes many academics and draws attention away from areas where they can make important contributions. We advocate for a renewed focus on developing and evaluating domain- and task-specific models, and highlight the unique role of academics in this endeavor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.294.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.294.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--294 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.294 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.294/>Beyond Reference: Evaluating High Quality Translations Better than Human References</a></strong><br><a href=/people/k/keonwoong-noh/>Keonwoong Noh</a>
|
<a href=/people/s/seokjin-oh/>Seokjin Oh</a>
|
<a href=/people/w/woohwan-jung/>Woohwan Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--294><div class="card-body p-3 small">In Machine Translation (MT) evaluations, the conventional approach is to compare a translated sentence against its human-created reference sentence. MT metrics provide an absolute score (e.g., from 0 to 1) to a candidate sentence based on the similarity with the reference sentence. Thus, existing MT metrics give the maximum score to the reference sentence. However, this approach overlooks the potential for a candidate sentence to exceed the reference sentence in terms of quality. In particular, recent advancements in Large Language Models (LLMs) have highlighted this issue, as LLM-generated sentences often exceed the quality of human-written sentences. To address the problem, we introduce the Residual score Metric (ResuMe), which evaluates the relative quality between reference and candidate sentences. ResuMe assigns a positive score to candidate sentences that outperform their reference sentences, and a negative score when they fall short. By adding the residual scores from ResuMe to the absolute scores from MT metrics, it can be possible to allocate higher scores to candidate sentences than what reference sentences are received from MT metrics. Experimental results demonstrate that ResuMe enhances the alignments between MT metrics and human judgments both at the segment-level and the system-level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.295.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--295 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.295/>Unveiling the Lexical Sensitivity of <span class=acl-fixed-case>LLM</span>s: Combinatorial Optimization for Prompt Enhancement</a></strong><br><a href=/people/p/pengwei-zhan/>Pengwei Zhan</a>
|
<a href=/people/z/zhen-xu/>Zhen Xu</a>
|
<a href=/people/q/qian-tan/>Qian Tan</a>
|
<a href=/people/j/jie-song/>Jie Song</a>
|
<a href=/people/r/ru-xie/>Ru Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--295><div class="card-body p-3 small">Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.296.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.296.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--296 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.296 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.296.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.296.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.296/><span class=acl-fixed-case>SEAC</span>rowd: A Multilingual Multimodal Data Hub and Benchmark Suite for <span class=acl-fixed-case>S</span>outheast <span class=acl-fixed-case>A</span>sian Languages</a></strong><br><a href=/people/h/holy-lovenia/>Holy Lovenia</a>
|
<a href=/people/r/rahmad-mahendra/>Rahmad Mahendra</a>
|
<a href=/people/s/salsabil-maulana-akbar/>Salsabil Maulana Akbar</a>
|
<a href=/people/l/lester-james-validad-miranda/>Lester James Validad Miranda</a>
|
<a href=/people/j/jennifer-santoso/>Jennifer Santoso</a>
|
<a href=/people/e/elyanah-aco/>Elyanah Aco</a>
|
<a href=/people/a/akhdan-fadhilah/>Akhdan Fadhilah</a>
|
<a href=/people/j/jonibek-mansurov/>Jonibek Mansurov</a>
|
<a href=/people/j/joseph-marvin-imperial/>Joseph Marvin Imperial</a>
|
<a href=/people/o/onno-p-kampman/>Onno P. Kampman</a>
|
<a href=/people/j/joel-ruben-antony-moniz/>Joel Ruben Antony Moniz</a>
|
<a href=/people/m/muhammad-ravi-shulthan-habibi/>Muhammad Ravi Shulthan Habibi</a>
|
<a href=/people/f/frederikus-hudi/>Frederikus Hudi</a>
|
<a href=/people/j/jann-railey-montalan/>Jann Railey Montalan</a>
|
<a href=/people/r/ryan-ignatius-hadiwijaya/>Ryan Ignatius Hadiwijaya</a>
|
<a href=/people/j/joanito-agili-lopo/>Joanito Agili Lopo</a>
|
<a href=/people/w/william-nixon/>William Nixon</a>
|
<a href=/people/b/borje-f-karlsson/>Börje F. Karlsson</a>
|
<a href=/people/j/james-jaya/>James Jaya</a>
|
<a href=/people/r/ryandito-diandaru/>Ryandito Diandaru</a>
|
<a href=/people/y/yuze-gao/>Yuze Gao</a>
|
<a href=/people/p/patrick-amadeus-irawan/>Patrick Amadeus Irawan</a>
|
<a href=/people/b/bin-wang/>Bin Wang</a>
|
<a href=/people/j/jan-christian-blaise-cruz/>Jan Christian Blaise Cruz</a>
|
<a href=/people/c/chenxi-whitehouse/>Chenxi Whitehouse</a>
|
<a href=/people/i/ivan-halim-parmonangan/>Ivan Halim Parmonangan</a>
|
<a href=/people/m/maria-khelli/>Maria Khelli</a>
|
<a href=/people/w/wenyu-zhang/>Wenyu Zhang</a>
|
<a href=/people/l/lucky-susanto/>Lucky Susanto</a>
|
<a href=/people/r/reynard-adha-ryanda/>Reynard Adha Ryanda</a>
|
<a href=/people/s/sonny-lazuardi-hermawan/>Sonny Lazuardi Hermawan</a>
|
<a href=/people/d/dan-john-velasco/>Dan John Velasco</a>
|
<a href=/people/m/muhammad-dehan-al-kautsar/>Muhammad Dehan Al Kautsar</a>
|
<a href=/people/w/willy-fitra-hendria/>Willy Fitra Hendria</a>
|
<a href=/people/y/yasmin-moslem/>Yasmin Moslem</a>
|
<a href=/people/n/noah-flynn/>Noah Flynn</a>
|
<a href=/people/m/muhammad-farid-adilazuarda/>Muhammad Farid Adilazuarda</a>
|
<a href=/people/h/haochen-li/>Haochen Li</a>
|
<a href=/people/j/johanes-lee/>Johanes Lee</a>
|
<a href=/people/r/r-damanhuri/>R. Damanhuri</a>
|
<a href=/people/s/shuo-sun/>Shuo Sun</a>
|
<a href=/people/m/muhammad-reza-qorib/>Muhammad Reza Qorib</a>
|
<a href=/people/a/amirbek-djanibekov/>Amirbek Djanibekov</a>
|
<a href=/people/w/wei-qi-leong/>Wei Qi Leong</a>
|
<a href=/people/q/quyet-v-do/>Quyet V. Do</a>
|
<a href=/people/n/niklas-muennighoff/>Niklas Muennighoff</a>
|
<a href=/people/t/tanrada-pansuwan/>Tanrada Pansuwan</a>
|
<a href=/people/i/ilham-firdausi-putra/>Ilham Firdausi Putra</a>
|
<a href=/people/y/yan-xu/>Yan Xu</a>
|
<a href=/people/t/tai-ngee-chia/>Tai Ngee Chia</a>
|
<a href=/people/a/ayu-purwarianti/>Ayu Purwarianti</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/w/william-chandra-tjhi/>William Chandra Tjhi</a>
|
<a href=/people/p/peerat-limkonchotiwat/>Peerat Limkonchotiwat</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/s/sedrick-keh/>Sedrick Keh</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/r/ruochen-zhang/>Ruochen Zhang</a>
|
<a href=/people/f/fajri-koto/>Fajri Koto</a>
|
<a href=/people/z/zheng-xin-yong/>Zheng Xin Yong</a>
|
<a href=/people/s/samuel-cahyawijaya/>Samuel Cahyawijaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--296><div class="card-body p-3 small">Southeast Asia (SEA) is a region rich in linguistic diversity and cultural variety, with over 1,300 indigenous languages and a population of 671 million people. However, prevailing AI models suffer from a significant lack of representation of texts, images, and audio datasets from SEA, compromising the quality of AI models for SEA languages. Evaluating models for SEA languages is challenging due to the scarcity of high-quality datasets, compounded by the dominance of English training data, raising concerns about potential cultural misrepresentation. To address these challenges, through a collaborative movement, we introduce SEACrowd, a comprehensive resource center that fills the resource gap by providing standardized corpora in nearly 1,000 SEA languages across three modalities. Through our SEACrowd benchmarks, we assess the quality of AI models on 36 indigenous languages across 13 tasks, offering valuable insights into the current AI landscape in SEA. Furthermore, we propose strategies to facilitate greater AI advancements, maximizing potential utility and resource equity for the future of AI in Southeast Asia.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.297.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--297 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.297.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.297/>Induct-Learn: Short Phrase Prompting with Instruction Induction</a></strong><br><a href=/people/p/po-chun-chen/>Po-Chun Chen</a>
|
<a href=/people/s/sheng-lun-wei/>Sheng-Lun Wei</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--297><div class="card-body p-3 small">Large Language Models (LLMs) have demonstrated capability in “instruction induction,” generating instructions from demonstrations (input-output pairs). However, existing methods often rely on large datasets or numerous examples, which is impractical and costly in real-world scenarios. In this work, we propose a low-cost, task-level framework called Induct-Learn. It induces pseudo instructions from a few demonstrations and a short phrase, adding a CoT process into existing demonstrations. When encountering new problems, the learned pseudo instructions and demonstrations with the pseudo CoT process can be combined into a prompt to guide the LLM’s problem-solving process. We validate our approach on the BBH-Induct and Evals-Induct datasets, and the results show that the Induct-Learn framework outperforms state-of-the-art methods. We also exhibit cross-model adaptability and achieve superior performance at a lower cost compared to existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.298.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.298.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.298/>Multi-Granularity History and Entity Similarity Learning for Temporal Knowledge Graph Reasoning</a></strong><br><a href=/people/s/shi-mingcong/>Shi Mingcong</a>
|
<a href=/people/c/chunjiang-zhu/>Chunjiang Zhu</a>
|
<a href=/people/d/detian-zhang/>Detian Zhang</a>
|
<a href=/people/s/shiting-wen/>Shiting Wen</a>
|
<a href=/people/l/li-qing/>Li Qing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.299.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--299 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.299/><span class=acl-fixed-case>LUQ</span>: Long-text Uncertainty Quantification for <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/c/caiqi-zhang/>Caiqi Zhang</a>
|
<a href=/people/f/fangyu-liu/>Fangyu Liu</a>
|
<a href=/people/m/marco-basaldella/>Marco Basaldella</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--299><div class="card-body p-3 small">Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. However, LLMs are also prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model’s confidence on its generation, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce Luq and its two variations, a series of novel sampling-based UQ approaches specifically designed for long text. Our findings reveal that Luq outperforms existing baseline methods in correlating with the model’s factuality scores (negative coefficient of -0.85 observed for Gemini Pro). To further improve the factuality of LLM responses, we propose Luq-Ensemble, a method that ensembles responses from multiple models and selects the response with the lowest uncertainty. The ensembling method greatly improves the response factuality upon the best standalone LLM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.300.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--300 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.300 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.300.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.300.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.300/>Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method</a></strong><br><a href=/people/w/weichao-zhang/>Weichao Zhang</a>
|
<a href=/people/r/ruqing-zhang/>Ruqing Zhang</a>
|
<a href=/people/j/jiafeng-guo/>Jiafeng Guo</a>
|
<a href=/people/m/maarten-de-rijke/>Maarten de Rijke</a>
|
<a href=/people/y/yixing-fan/>Yixing Fan</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--300><div class="card-body p-3 small">As the scale of training corpora for large language models (LLMs) grows, model developers become increasingly reluctant to disclose details on their data. This lack of transparency poses challenges to scientific evaluation and ethical deployment. Recently, pretraining data detection approaches, which infer whether a given text was part of an LLM’s training data through black-box access, have been explored. The Min-K% Prob method, which has achieved state-of-the-art results, assumes that a non-training example tends to contain a few outlier words with low token probabilities. However, the effectiveness may be limited as it tends to misclassify non-training texts that contain many common words with high probabilities predicted by LLMs. To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score.We have developed a Chinese-language benchmark, PatentMIA, to assess the performance of detection approaches for LLMs on Chinese text. Experimental results on English-language benchmarks and PatentMIA demonstrate that our proposed method significantly outperforms existing methods. Our code and PatentMIA benchmark are available at <a href=https://github.com/zhang-wei-chao/DC-PDD class=acl-markup-url>https://github.com/zhang-wei-chao/DC-PDD</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.301.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--301 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.301.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.301.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.301/>Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars</a></strong><br><a href=/people/d/damien-sileo/>Damien Sileo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--301><div class="card-body p-3 small">Logical reasoning remains a challenge for natural language processing, but it can be improved by training language models to mimic theorem provers on procedurally generated problems. Previous work used domain-specific proof generation algorithms, which biases reasoning toward specific proof traces and limits auditability and extensibility. We present a simpler and more general declarative framework with flexible context-sensitive rules binding multiple languages (specifically, simplified English and the TPTP theorem-proving language). We construct first-order logic problems by selecting up to 32 premises and one hypothesis. We demonstrate that using semantic constraints during generation and careful English verbalization of predicates enhances logical reasoning without hurting natural English tasks. Using relatively small DeBERTa-v3 models, we achieve state-of-the-art accuracy on the FOLIO human-authored logic dataset, surpassing GPT-4 in accuracy with or without an external solver by 12%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.302.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--302 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.302/>Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach</a></strong><br><a href=/people/m/maxime-poli/>Maxime Poli</a>
|
<a href=/people/e/emmanuel-chemla/>Emmanuel Chemla</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--302><div class="card-body p-3 small">Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.303.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--303 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.303/>Safely Learning with Private Data: A Federated Learning Framework for Large Language Model</a></strong><br><a href=/people/j/jia-ying-zheng/>Jia-Ying Zheng</a>
|
<a href=/people/h/hainan-zhang/>Hainan Zhang</a>
|
<a href=/people/l/lingxiang-wang/>Lingxiang Wang</a>
|
<a href=/people/w/wangjie-qiu/>Wangjie Qiu</a>
|
<a href=/people/h/hong-wei-zheng/>Hong-Wei Zheng</a>
|
<a href=/people/z/zhi-ming-zheng/>Zhi-Ming Zheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--303><div class="card-body p-3 small">Private data, being larger and quality-higher than public data, can greatly improve large language models (LLM). However, due to privacy concerns, this data is often dispersed in multiple silos, making its secure utilization for LLM training a challenge. Federated learning (FL) is an ideal solution for training models with distributed private data, but traditional frameworks like FedAvg are unsuitable for LLM due to their high computational demands on clients. An alternative, split learning, offloads most training parameters to the server while training embedding and output layers locally, making it more suitable for LLM. Nonetheless, it faces significant challenges in security and efficiency. Firstly, the gradients of embeddings are prone to attacks, leading to potential reverse engineering of private data. Furthermore, the server’s limitation of handling only one client’s training request at a time hinders parallel training, severely impacting training efficiency. In this paper, we propose a Federated Learning framework for LLM, named FL-GLM, which prevents data leakage caused by both server-side and peer-client attacks while improving training efficiency. Specifically, we first place the input block and output block on local client to prevent embedding gradient attacks from server. Secondly, we employ key-encryption during client-server communication to prevent reverse engineering attacks from peer-clients. Lastly, we employ optimization methods like client-batching or server-hierarchical, adopting different acceleration methods based on the actual computational capabilities of the server. Experimental results on NLU and generation tasks demonstrate that FL-GLM achieves comparable metrics to centralized chatGLM model, validating the effectiveness of our federated learning framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.304.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--304 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.304/>Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge</a></strong><br><a href=/people/j/jiahuan-li/>Jiahuan Li</a>
|
<a href=/people/y/yiqing-cao/>Yiqing Cao</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--304><div class="card-body p-3 small">Having been trained on massive pretraining data, large language models have shown excellent performance on many knowledge-intensive tasks. However, pretraining data tends to contain misleading and even conflicting information, and it is intriguing to understand how LLMs handle these noisy data during training. In this study, we systematically analyze LLMs’ learning preferences for data with conflicting knowledge. We find that pretrained LLMs establish learning preferences similar to humans, i.e., preferences towards formal texts and texts with fewer spelling errors, resulting in faster learning and more favorable treatment of knowledge in data with such features when facing conflicts. This finding is generalizable across models and languages and is more evident in larger models. An in-depth analysis reveals that LLMs tend to trust data with features that signify consistency with the majority of data, and it is possible to instill new preferences and erase old ones by manipulating the degree of consistency with the majority data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.305.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--305 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.305/>How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?</a></strong><br><a href=/people/y/yang-luo/>Yang Luo</a>
|
<a href=/people/z/zangwei-zheng/>Zangwei Zheng</a>
|
<a href=/people/z/zirui-zhu/>Zirui Zhu</a>
|
<a href=/people/y/yang-you/>Yang You</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--305><div class="card-body p-3 small">The increase in parameter size of multimodal large language models (MLLMs) introduces significant capabilities, particularly multimodal in-context learning, where MLLMs enhance task performance without updating pre-trained parameters. However, this effectiveness hinges on the appropriate selection of in-context examples, a process currently biased towards visual data, overlooking textual information. More importantly, the area of supervised retrievers for retrieval of multimodal in-context learning, crucial for optimal in-context example selection, continues to be investigated. Our study provides an in-depth evaluation of the impact of textual information on the unsupervised selection of in-context examples in multimodal contexts, uncovering a notable sensitivity of retriever performance to the employed modalities. Based on the above finding, we introduce a novel supervised MLLM prompt retriever MSIER that leverages a trained retriever based on MLLM’s confidence to select examples, which enhances multimodal in-context learning efficiency. This approach is validated through extensive testing across three different tasks, demonstrating the method’s effectiveness. Additionally, we investigate the influence of modalities on our supervised retrieval method’s training and explore the transferability of the supervised prompt retriever. This exploration paves the way for future advancements, highlighting the potential for refined in-context learning in MLLMs through the strategic use of multimodal data. The public code is available at https://github.com/NUS-HPC-AI-Lab/Multimodal-ICL-Retriever.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.306.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--306 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.306/>How Far Can We Extract Diverse Perspectives from Large Language Models?</a></strong><br><a href=/people/s/shirley-anugrah-hayati/>Shirley Anugrah Hayati</a>
|
<a href=/people/m/minhwa-lee/>Minhwa Lee</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a>
|
<a href=/people/d/dongyeop-kang/>Dongyeop Kang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--306><div class="card-body p-3 small">Collecting diverse human opinions is costly and challenging. This leads to a recent trend in exploiting large language models (LLMs) for generating diverse data for potential scalable and efficient solutions. However, the extent to which LLMs can generate diverse perspectives on subjective topics is still unclear. In this study, we explore LLMs’ capacity of generating diverse perspectives and rationales on subjective topics such as social norms and argumentative texts. We introduce the problem of extracting maximum diversity from LLMs. Motivated by how humans form opinions based on values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting to generate more outputs from the model iteratively. Our methods, applied to various tasks, show that LLMs can indeed produce diverse opinions according to the degree of task subjectivity. We also find that LLMs performance of extracting maximum diversity is on par with human.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.307.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--307 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.307.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.307.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.307/><span class=acl-fixed-case>EXPLORA</span>: Efficient Exemplar Subset Selection for Complex Reasoning</a></strong><br><a href=/people/k/kiran-purohit/>Kiran Purohit</a>
|
<a href=/people/v/venktesh-v/>Venktesh V</a>
|
<a href=/people/r/raghuram-devalla/>Raghuram Devalla</a>
|
<a href=/people/k/krishna-mohan-yerragorla/>Krishna Mohan Yerragorla</a>
|
<a href=/people/s/sourangshu-bhattacharya/>Sourangshu Bhattacharya</a>
|
<a href=/people/a/avishek-anand/>Avishek Anand</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--307><div class="card-body p-3 small">Answering reasoning-based complex questions over text and hybrid sources, including tables, is a challenging task. Recent advances in large language models (LLMs) have enabled in-context learning (ICL), allowing LLMs to acquire proficiency in a specific task using only a few demonstration samples (exemplars). A critical challenge in ICL is the selection of optimal exemplars, which can be either task-specific (static) or test-example-specific (dynamic). Static exemplars provide faster inference times and increased robustness across a distribution of test examples. In this paper, we propose an algorithm for static exemplar subset selection for complex reasoning tasks. We introduce EXPLORA, a novel exploration method designed to estimate the parameters of the scoring function, which evaluates exemplar subsets without incorporating confidence information. EXPLORA significantly reduces the number of LLM calls to ~11% of those required by state-of-the-art methods and achieves a substantial performance improvement of 12.24%. We open-source our code and data (https://github.com/kiranpurohit/EXPLORA).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.308.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--308 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.308/>An <span class=acl-fixed-case>LLM</span> Feature-based Framework for Dialogue Constructiveness Assessment</a></strong><br><a href=/people/l/lexin-zhou/>Lexin Zhou</a>
|
<a href=/people/y/youmna-farag/>Youmna Farag</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--308><div class="card-body p-3 small">Research on dialogue constructiveness assessment focuses on (i) analysing conversational factors that influence individuals to take specific actions, win debates, change their perspectives or broaden their open-mindedness and (ii) predicting constructiveness outcomes following dialogues for such use cases. These objectives can be achieved by training either interpretable feature-based models (which often involve costly human annotations) or neural models such as pre-trained language models (which have empirically shown higher task accuracy but lack interpretability). In this paper we propose an LLM feature-based framework for dialogue constructiveness assessment that combines the strengths of feature-based and neural approaches, while mitigating their downsides. The framework first defines a set of dataset-independent and interpretable linguistic features, which can be extracted by both prompting an LLM and simple heuristics. Such features are then used to train LLM feature-based models. We apply this framework to three datasets of dialogue constructiveness and find that our LLM feature-based models outperform or performs at least as well as standard feature-based models and neural models. We also find that the LLM feature-based model learns more robust prediction rules instead of relying on superficial shortcuts, which often trouble neural models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.309.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--309 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.309/>Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System</a></strong><br><a href=/people/z/zhanpeng-chen/>Zhanpeng Chen</a>
|
<a href=/people/z/zhihong-zhu/>Zhihong Zhu</a>
|
<a href=/people/w/wanshi-xu/>Wanshi Xu</a>
|
<a href=/people/x/xianwei-zhuang/>Xianwei Zhuang</a>
|
<a href=/people/y/yuexian-zou/>Yuexian Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--309><div class="card-body p-3 small">Retrieving accurate domain knowledge and providing helpful information are crucial in developing an effective end-to-end task-oriented dialogue system (E2ETOD). The field has witnessed numerous methods following the retrieve-then-generate paradigm and training their systems on one specific domain. However, existing approaches still suffer from the Distractive Attributes Problem (DAP): struggling to deal with false but similar knowledge (hard negative entities), which is even more intractable when countless pieces of knowledge from different domains are blended in a real-world scenario. To alleviate DAP, we propose the Relevance-aware Adaptive Learning (ReAL) method, a two-stage training framework that eliminates hard negatives step-by-step and aligns retrieval with generation. In the first stage, we introduce a top-k adaptive contrastive loss and utilize the divergence-driven feedback from the frozen generator to pre-train the retriever. In the second stage, we propose using the metric score distribution as an anchor to align retrieval with generation. Thorough experiments on three benchmark datasets demonstrate ReAL’s superiority over existing methods, with extensive analysis validating its strong capabilities of overcoming in- and cross-domain distractions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.310.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--310 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.310/><span class=acl-fixed-case>D</span>ialog2<span class=acl-fixed-case>F</span>low: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction</a></strong><br><a href=/people/s/sergio-burdisso/>Sergio Burdisso</a>
|
<a href=/people/s/srikanth-madikeri/>Srikanth Madikeri</a>
|
<a href=/people/p/petr-motlicek/>Petr Motlicek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--310><div class="card-body p-3 small">Efficiently deriving structured workflows from unannotated dialogs remains an underexplored and formidable challenge in computational linguistics. Automating this process could significantly accelerate the manual design of workflows in new domains and enable the grounding of large language models in domain-specific flowcharts, enhancing transparency and controllability.In this paper, we introduce Dialog2Flow (D2F) embeddings, which differ from conventional sentence embeddings by mapping utterances to a latent space where they are grouped according to their communicative and informative functions (i.e., the actions they represent). D2F allows for modeling dialogs as continuous trajectories in a latent space with distinct action-related regions. By clustering D2F embeddings, the latent space is quantized, and dialogs can be converted into sequences of region/action IDs, facilitating the extraction of the underlying workflow.To pre-train D2F, we build a comprehensive dataset by unifying twenty task-oriented dialog datasets with normalized per-turn action annotations. We also introduce a novel soft contrastive loss that leverages the semantic information of these actions to guide the representation learning process, showing superior performance compared to standard supervised contrastive loss.Evaluation against various sentence embeddings, including dialog-specific ones, demonstrates that D2F yields superior qualitative and quantitative results across diverse domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.311.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--311 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.311/>Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation</a></strong><br><a href=/people/r/raphael-tang/>Raphael Tang</a>
|
<a href=/people/c/crystina-zhang/>Crystina Zhang</a>
|
<a href=/people/l/lixinyu-xu/>Lixinyu Xu</a>
|
<a href=/people/y/yao-lu/>Yao Lu</a>
|
<a href=/people/w/wenyan-li/>Wenyan Li</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a>
|
<a href=/people/f/ferhan-ture/>Ferhan Ture</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--311><div class="card-body p-3 small">Diffusion models are the state of the art in text-to-image generation, but their perceptual variability remains understudied. In this paper, we examine how prompts affect image variability in black-box diffusion-based models. We propose W1KP, a human-calibrated measure of variability in a set of images, bootstrapped from existing image-pair perceptual distances. Current datasets do not cover recent diffusion models, thus we curate three test sets for evaluation. Our best perceptual distance outperforms nine baselines by up to 18 points in accuracy, and our calibration matches graded human judgements 78% of the time. Using W1KP, we study prompt reusability and show that Imagen prompts can be reused for 10-50 random seeds before new images become too similar to already generated images, while Stable Diffusion XL and DALL-E 3 can be reused 50-200 times. Lastly, we analyze 56 linguistic features of real prompts, finding that the prompt’s length, CLIP embedding norm, concreteness, and word senses influence variability most. As far as we are aware, we are the first to analyze diffusion variability from a visuolinguistic perspective. Our project page is at http://w1kp.com.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.312.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--312 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.312/>Investigating <span class=acl-fixed-case>LLM</span>s as Voting Assistants via Contextual Augmentation: A Case Study on the <span class=acl-fixed-case>E</span>uropean Parliament Elections 2024</a></strong><br><a href=/people/i/ilias-chalkidis/>Ilias Chalkidis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--312><div class="card-body p-3 small">In light of the recent 2024 European Parliament elections, we are investigating if LLMs can be used as Voting Advice Applications (VAAs). We audit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the stance of political parties based on the latest “EU and I” voting assistance questionnaire. Furthermore, we explore alternatives to improve models’ performance by augmenting the input context via Retrieval-Augmented Generation (RAG) relying on web search, and Self-Reflection using staged conversations that aim to re-collect relevant content from the model’s internal memory. We find that MIXTRAL is highly accurate with an 82% accuracy on average with a significant performance disparity across different political groups (50-95%). Augmenting the input context with expert-curated information can lead to a significant boost of approx. 9%, which remains an open challenge for automated RAG approaches, even considering curated content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.313.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--313 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.313/>Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning</a></strong><br><a href=/people/m/mayi-xu/>Mayi Xu</a>
|
<a href=/people/y/yongqi-li-hk/>Yongqi Li</a>
|
<a href=/people/k/ke-sun/>Ke Sun</a>
|
<a href=/people/t/tieyun-qian/>Tieyun Qian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--313><div class="card-body p-3 small">Large language models (LLMs) have shown excellent capability for solving reasoning problems. Existing approaches do not differentiate the question difficulty when designing prompting methods for them. Clearly, a simple method cannot elicit sufficient knowledge from LLMs to answer a hard question. Meanwhile, a sophisticated one will force the LLM to generate redundant or even inaccurate intermediate steps toward a simple question. Consequently, the performance of existing methods fluctuates among various questions.In this work, we propose Adaption-of-Thought (AdoT), an adaptive method to improve LLMs for the reasoning problem, which first measures the question difficulty and then tailors demonstration set construction and difficulty-adapted retrieval strategies for the adaptive demonstration construction. Experimental results on three reasoning tasks prove the superiority of our proposed method, showing an absolute improvement of up to 5.5% on arithmetic reasoning, 7.4% on symbolic reasoning, and 2.3% on commonsense reasoning. Our codes and implementation details are available at: https://github.com/NLPGM/AdoT</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.314.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--314 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.314.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.314.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.314/><span class=acl-fixed-case>L</span>ogic<span class=acl-fixed-case>ST</span>: A Logical Self-Training Framework for Document-Level Relation Extraction with Incomplete Annotations</a></strong><br><a href=/people/s/shengda-fan/>Shengda Fan</a>
|
<a href=/people/y/yanting-wang/>Yanting Wang</a>
|
<a href=/people/s/shasha-mo/>Shasha Mo</a>
|
<a href=/people/j/jianwei-niu/>Jianwei Niu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--314><div class="card-body p-3 small">Document-level relation extraction (DocRE) aims to identify relationships between entities within a document. Due to the vast number of entity pairs, fully annotating all fact triplets is challenging, resulting in datasets with numerous false negative samples. Recently, self-training-based methods have been introduced to address this issue. However, these methods are purely black-box and sub-symbolic, making them difficult to interpret and prone to overlooking symbolic interdependencies between relations.To remedy this deficiency, our insight is that symbolic knowledge, such as logical rules, can be used as diagnostic tools to identify conflicts between pseudo-labels. By resolving these conflicts through logical diagnoses, we can correct erroneous pseudo-labels, thus enhancing the training of neural models.To achieve this, we propose **LogicST**, a neural-logic self-training framework that iteratively resolves conflicts and constructs the minimal diagnostic set for updating models. Extensive experiments demonstrate that LogicST significantly improves performance and outperforms previous state-of-the-art methods. For instance, LogicST achieves an increase of **7.94%** in F1 score compared to CAST (Tan et al., 2023a) on the DocRED benchmark (Yao et al., 2019). Additionally, LogicST is more time-efficient than its self-training counterparts, requiring only **10%** of the training time of CAST.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.315.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--315 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.315/>Concept Space Alignment in Multilingual <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/q/qiwei-peng/>Qiwei Peng</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--315><div class="card-body p-3 small">Multilingual large language models (LLMs) seem to generalize somewhat across languages. We hypothesize this is a result of implicit vector space alignment. Evaluating such alignment, we see that larger models exhibit very high-quality linear alignments between corresponding concepts in different languages. Our experiments show that multilingual LLMs suffer from two familiar weaknesses: generalization works best for languages with similar typology, and for abstract concepts. For some models, e.g., the Llama-2 family of models, prompt-based embeddings align better than word embeddings, but the projections are less linear – an observation that holds across almost all model families, indicating that some of the implicitly learned alignments are broken somewhat by prompt-based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.316.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--316 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.316/>Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model</a></strong><br><a href=/people/c/chenhan-yuan/>Chenhan Yuan</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/r/ru-peng/>Ru Peng</a>
|
<a href=/people/k/keming-lu/>Keming Lu</a>
|
<a href=/people/b/bowen-yu/>Bowen Yu</a>
|
<a href=/people/c/chang-zhou/>Chang Zhou</a>
|
<a href=/people/j/jingren-zhou/>Jingren Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--316><div class="card-body p-3 small">Transformer-based large language models (LLMs) exhibit limitations such as generating unsafe responses, unreliable reasoning, etc. Existing inference intervention approaches attempt to mitigate these issues by finetuning additional models to produce calibration signals (such as rewards) that guide the LLM’s decoding process. However, this solution introduces substantial time and space overhead due to the separate models required. This work proposes Non-disruptive parameters insertion (Otter), inserting extra parameters into the transformer architecture to predict calibration signals along with the original LLM output. Otter offers state-of-the-art performance on multiple demanding tasks while saving up to 86.5% extra space and 98.5% extra time. Furthermore, Otter seamlessly integrates with existing inference engines, requiring only a one-line code change, and the original model response remains accessible after the parameter insertion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.317.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--317 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.317/><span class=acl-fixed-case>NLEB</span>ench+<span class=acl-fixed-case>N</span>or<span class=acl-fixed-case>GLM</span>: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in <span class=acl-fixed-case>N</span>orwegian</a></strong><br><a href=/people/p/peng-liu/>Peng Liu</a>
|
<a href=/people/l/lemei-zhang/>Lemei Zhang</a>
|
<a href=/people/t/terje-farup/>Terje Farup</a>
|
<a href=/people/e/even-w-lauvrak/>Even W. Lauvrak</a>
|
<a href=/people/j/jon-espen-ingvaldsen/>Jon Espen Ingvaldsen</a>
|
<a href=/people/s/simen-eide/>Simen Eide</a>
|
<a href=/people/j/jon-atle-gulla/>Jon Atle Gulla</a>
|
<a href=/people/z/zhirong-yang/>Zhirong Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--317><div class="card-body p-3 small">Norwegian, spoken by only 5 million population, is under-representative within the most impressive breakthroughs in NLP tasks. To the best of our knowledge, there has not yet been a comprehensive evaluation of the existing language models (LMs) on Norwegian generation tasks during the article writing process. To fill this gap, we 1) compiled the existing Norwegian dataset and pre-trained 4 Norwegian Open Language Models varied from parameter scales and architectures, collectively called NorGLM; 2) introduced a comprehensive benchmark, NLEBench, for evaluating natural language generation capabilities in Norwegian, encompassing translation and human annotation. Based on the investigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5 has limited capability in understanding the Norwegian context; 2) the increase in model parameter scales demonstrates limited impact on the performance of downstream tasks when the pre-training dataset is constrained in size; 3) smaller models also demonstrate the reasoning capability through Chain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be used to verify the generalizability of LLMs on natural language understanding and, meanwhile, test the interconnectedness of these NLP tasks. We share our resources and code for reproducibility under a CC BY-NC 4.0 license.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.318.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--318 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.318.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.318/><span class=acl-fixed-case>RSA</span>-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework</a></strong><br><a href=/people/y/yifan-wang/>Yifan Wang</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--318><div class="card-body p-3 small">Despite significant advancements in natural language generation, controlling language models to produce texts with desired attributes remains a formidable challenge. In this work, we introduce RSA-Control, a training-free controllable text generation framework grounded in pragmatics. RSA-Control directs the generation process by recursively reasoning between imaginary speakers and listeners, enhancing the likelihood that target attributes are correctly interpreted by listeners amidst distractors. Additionally, we introduce a self-adjustable rationality parameter, which allows for automatic adjustment of control strength based on context. Our experiments, conducted with two task types and two types of language models, demonstrate that RSA-Control achieves strong attribute control while maintaining language fluency and content consistency. Our code is available at https://github.com/Ewanwong/RSA-Control.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.319.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--319 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.319/>Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and <span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>E</span> Models in Large Language Models</a></strong><br><a href=/people/s/siqi-wang/>Siqi Wang</a>
|
<a href=/people/z/zhengyu-chen/>Zhengyu Chen</a>
|
<a href=/people/b/bei-li/>Bei Li</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/j/jingang-wang/>Jingang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--319><div class="card-body p-3 small">The scaling of large language models (LLMs) is a critical research area for the efficiency and effectiveness of model training and deployment. Our work investigates the transferability and discrepancies of scaling laws between Dense Models and Mixture of Experts (MoE) models. Through a combination of theoretical analysis and extensive experiments, including consistent loss scaling, optimal batch size/learning rate scaling, and resource allocation strategies scaling, our findings reveal that the power-law scaling framework also applies to MoE Models, indicating that the fundamental principles governing the scaling behavior of these models are preserved, even though the architecture differs. Additionally, MoE Models demonstrate superior generalization, resulting in lower testing losses with the same training compute budget compared to Dense Models. These findings indicate the scaling consistency and transfer generalization capabilities of MoE Models, providing new insights for optimizing MoE Model training and deployment strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.320.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--320 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.320.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.320/>Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems</a></strong><br><a href=/people/v/vishal-vivek-saley/>Vishal Vivek Saley</a>
|
<a href=/people/r/rocktim-jyoti-das/>Rocktim Jyoti Das</a>
|
<a href=/people/d/dinesh-raghu/>Dinesh Raghu</a>
|
<a href=/people/m/mausam/>Mausam .</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--320><div class="card-body p-3 small">End-to-end Task-Oriented Dialog (TOD) systems typically require extensive training datasets to perform well. In contrast, large language model (LLM) based TOD systems can excel even with limited data due to their ability to learn tasks through in-context exemplars. However, these models lack alignment with the style of responses in training data and often generate comprehensive responses, making it difficult for users to grasp the information quickly. In response, we propose SyncTOD that synergizes LLMs with task-specific hints to improve alignment in low-data settings. SyncTOD employs small auxiliary models to provide hints and select exemplars for in-context prompts. With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.321.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--321 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.321.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.321.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.321/><span class=acl-fixed-case>REAR</span>: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering</a></strong><br><a href=/people/y/yuhao-wang/>Yuhao Wang</a>
|
<a href=/people/r/ruiyang-ren/>Ruiyang Ren</a>
|
<a href=/people/j/junyi-li/>Junyi Li</a>
|
<a href=/people/w/wayne-xin-zhao/>Xin Zhao</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--321><div class="card-body p-3 small">Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness regarding the reliability of external knowledge for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a novel architecture for LLM based RAG system, by incorporating a specially designed assessnent module that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our codes can be accessed at https://github.com/RUCAIBox/REAR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.322.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--322 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.322/>Leave No Document Behind: Benchmarking Long-Context <span class=acl-fixed-case>LLM</span>s with Extended Multi-Doc <span class=acl-fixed-case>QA</span></a></strong><br><a href=/people/m/minzheng-wang/>Minzheng Wang</a>
|
<a href=/people/l/longze-chen/>Longze Chen</a>
|
<a href=/people/f/fu-cheng/>Fu Cheng</a>
|
<a href=/people/s/shengyi-liao/>Shengyi Liao</a>
|
<a href=/people/x/xinghua-zhang/>Xinghua Zhang</a>
|
<a href=/people/b/bingli-wu/>Bingli Wu</a>
|
<a href=/people/h/haiyang-yu/>Haiyang Yu</a>
|
<a href=/people/n/nan-xu/>Nan Xu</a>
|
<a href=/people/l/lei-zhang/>Lei Zhang</a>
|
<a href=/people/r/run-luo/>Run Luo</a>
|
<a href=/people/y/yunshui-li/>Yunshui Li</a>
|
<a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/y/yongbin-li/>Yongbin Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--322><div class="card-body p-3 small">Long-context modeling capabilities of Large Language Models (LLMs) have garnered widespread attention, leading to the emergence of LLMs with ultra-context windows. Meanwhile, benchmarks for evaluating long-context language models are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong’s test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model’s long-context modeling capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.323.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--323 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.323/>On Mitigating Performance Disparities in Multilingual Speech Recognition</a></strong><br><a href=/people/m/monorama-swain/>Monorama Swain</a>
|
<a href=/people/a/anna-katrine-van-zee/>Anna Katrine Van Zee</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--323><div class="card-body p-3 small">How far have we come in mitigating performance disparities across genders in multilingual speech recognition? We compare the impact on gender disparity of different fine-tuning algorithms for automated speech recognition across model sizes, languages and gender. We look at both performance-focused and fairness-promoting algorithms. Across languages, we see slightly better performance for female speakers for larger models regardless of the fine-tuning algorithm. The best trade-off between performance and parity is found using adapter fusion. Fairness-promoting fine-tuning algorithms (Group-DRO and Spectral Decoupling) hurt performance compared to adapter fusion with only slightly better performance parity. LoRA increases disparities slightly. Fairness-mitigating fine-tuning techniques led to slightly higher variance in performance across languages, with the exception of adapter fusion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.324.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--324 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.324/>Thinking Outside of the Differential Privacy Box: A Case Study in Text Privatization with Language Model Prompting</a></strong><br><a href=/people/s/stephen-meisenbacher/>Stephen Meisenbacher</a>
|
<a href=/people/f/florian-matthes/>Florian Matthes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--324><div class="card-body p-3 small">The field of privacy-preserving Natural Language Processing has risen in popularity, particularly at a time when concerns about privacy grow with the proliferation of large language models. One solution consistently appearing in recent literature has been the integration of Differential Privacy (DP) into NLP techniques. In this paper, we take these approaches into critical view, discussing the restrictions that DP integration imposes, as well as bring to light the challenges that such restrictions entail. To accomplish this, we focus on **DP-Prompt**, a recent method for text privatization leveraging language models to rewrite texts. In particular, we explore this rewriting task in multiple scenarios, both with DP and without DP. To drive the discussion on the merits of DP in NLP, we conduct empirical utility and privacy experiments. Our results demonstrate the need for more discussion on the usability of DP in NLP and its benefits over non-DP approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.325.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--325 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.325/>To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models</a></strong><br><a href=/people/j/junyan-lin/>Junyan Lin</a>
|
<a href=/people/h/haoran-chen/>Haoran Chen</a>
|
<a href=/people/d/dawei-zhu/>Dawei Zhu</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--325><div class="card-body p-3 small">In recent years, multimodal large language models (MLLMs) have attracted widespread attention from both industry and academia. Based on the integration position, MLLMs can be categorized into external and internal fusion architectures, with the former being more predominant. However, there remains considerable debate on how to construct the optimal external fusion MLLM architecture, especially regarding the performance of different connectors on tasks with varying granularities. This paper systematically investigates the impact of connectors on MLLM performance. Specifically, we classify connectors into feature-preserving and feature-compressing types. Utilizing a unified classification standard, we categorize sub-tasks from three comprehensive benchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained perception, fine-grained perception, and reasoning, and evaluate the performance from this perspective. Our findings reveal significant performance differences between different types of connectors across various tasks, offering essential guidance for MLLM architecture design and advancing the understanding of MLLM architecture optimization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.326.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--326 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.326.software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.326/>What is ”Typological Diversity” in <span class=acl-fixed-case>NLP</span>?</a></strong><br><a href=/people/e/esther-ploeger/>Esther Ploeger</a>
|
<a href=/people/w/wessel-poelman/>Wessel Poelman</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--326><div class="card-body p-3 small">The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world’s languages. An increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being typologically diverse. In this meta-analysis, we systematically investigate NLP research that includes claims regarding typological diversity. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of resulting language samples along several axes and find that the results vary considerably across papers. Crucially, we show that skewed language selection can lead to overestimated multilingual performance. We recommend future work to include an operationalization of typological diversity that empirically justifies the diversity of language samples. To help facilitate this, we release the code for our diversity measures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.327.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--327 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.327.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.327.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.327/>The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse</a></strong><br><a href=/people/x/xiaobo-guo/>Xiaobo Guo</a>
|
<a href=/people/n/neil-potnis/>Neil Potnis</a>
|
<a href=/people/m/melody-yu/>Melody Yu</a>
|
<a href=/people/n/nabeel-gillani/>Nabeel Gillani</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--327><div class="card-body p-3 small">The ability for individuals to constructively engage with one another across lines of difference is a critical feature of a healthy pluralistic society. This is also true in online discussion spaces like social media platforms. To date, much social media research has focused on preventing ills—like political polarization and the spread of misinformation. While this is important, enhancing the quality of online public discourse requires not just reducing ills, but also, promoting foundational human virtues. In this study, we focus on one particular virtue: “intellectual humility” (IH), or acknowledging the potential limitations in one’s own beliefs. Specifically, we explore the development of computational methods for measuring IH at scale. We manually curate and validate an IH codebook on 350 posts about religion drawn from subreddits and use them to develop LLM-based models for automating this measurement. Our best model achieves a Macro-F1 score of 0.64 across labels (and 0.70 when predicting IH/IA/Neutral at the coarse level), higher than an expected naive baseline of 0.51 (0.32 for IH/IA/Neutral) but lower than a human annotator-informed upper bound of 0.85 (0.83 for IH/IA/Neutral). Our results both highlight the challenging nature of detecting IH online—opening the door to new directions in NLP research—and also lay a foundation for computational social science researchers interested in analyzing and fostering more IH in online public discourse.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.328.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--328 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.328/>Consistent Bidirectional Language Modelling: Expressive Power and Representational Conciseness</a></strong><br><a href=/people/g/georgi-shopov/>Georgi Shopov</a>
|
<a href=/people/s/stefan-gerdjikov/>Stefan Gerdjikov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--328><div class="card-body p-3 small">The inability to utilise future contexts and the pre-determined left-to-right generation order are major limitations of unidirectional language models. Bidirectionality has been introduced to address those deficiencies. However, a crucial shortcoming of bidirectional language models is the potential inconsistency of their conditional distributions. This fundamental flaw greatly diminishes their applicability and hinders their capability of tractable sampling and likelihood computation. In this work, we introduce a class of bidirectional language models, called latent language models, that are consistent by definition and can be efficiently used both for generation and scoring of sequences. We define latent language models based on the well-understood formalism of bisequential decompositions from automata theory. This formal correspondence allows us to precisely charaterise the abilities and limitations of a subclass of latent language models, called rational language models. As a result, we obtain that latent language models are exponentially more concise and significantly more expressive than unidirectional language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.329.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--329 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.329/>Benchmarking Vision Language Models for Cultural Understanding</a></strong><br><a href=/people/s/shravan-nayak/>Shravan Nayak</a>
|
<a href=/people/k/kanishk-jain/>Kanishk Jain</a>
|
<a href=/people/r/rabiul-awal/>Rabiul Awal</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/s/sjoerd-van-steenkiste/>Sjoerd Van Steenkiste</a>
|
<a href=/people/l/lisa-anne-hendricks/>Lisa Anne Hendricks</a>
|
<a href=/people/k/karolina-stanczak/>Karolina Stanczak</a>
|
<a href=/people/a/aishwarya-agrawal/>Aishwarya Agrawal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--329><div class="card-body p-3 small">Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM’s geo-diverse cultural understanding. We curate a diverse collection of 2,378 image-question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly weaker capabilities for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.330.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--330 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.330/>Methods of Automatic Matrix Language Determination for Code-Switched Speech</a></strong><br><a href=/people/o/olga-iakovenko/>Olga Iakovenko</a>
|
<a href=/people/t/thomas-hain/>Thomas Hain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--330><div class="card-body p-3 small">Code-switching (CS) is the process of speakers interchanging between two or more languages which in the modern world becomes increasingly common. In order to better describe CS speech the Matrix Language Frame (MLF) theory introduces the concept of a Matrix Language, which is the language that provides the grammatical structure for a CS utterance. In this work the MLF theory was used to develop systems for Matrix Language Identity (MLID) determination. The MLID of English/Mandarin and English/Spanish CS text and speech was compared to acoustic language identity (LID), which is a typical way to identify a language in monolingual utterances. MLID predictors from audio show higher correlation with the textual principles than LID in all cases while also outperforming LID in an MLID recognition task based on F1 macro (60%) and correlation score (0.38). This novel approach has identified that non-English languages (Mandarin and Spanish) are preferred over the English language as the ML contrary to the monolingual choice of LID.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.331.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--331 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.331/>Analyzing Key Factors Influencing Emotion Prediction Performance of <span class=acl-fixed-case>VLLM</span>s in Conversational Contexts</a></strong><br><a href=/people/j/jaewook-lee/>Jaewook Lee</a>
|
<a href=/people/y/yeajin-jang/>Yeajin Jang</a>
|
<a href=/people/h/hongjin-kim/>Hongjin Kim</a>
|
<a href=/people/w/woojin-lee/>Woojin Lee</a>
|
<a href=/people/h/harksoo-kim/>Harksoo Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--331><div class="card-body p-3 small">Emotional intelligence (EI) in artificial intelligence (AI), which refers to the ability of an AI to understand and respond appropriately to human emotions, has emerged as a crucial research topic. Recent studies have shown that large language models (LLMs) and vision large language models (VLLMs) possess EI and the ability to understand emotional stimuli in the form of text and images, respectively. However, factors influencing the emotion prediction performance of VLLMs in real-world conversational contexts have not been sufficiently explored. This study aims to analyze the key elements affecting the emotion prediction performance of VLLMs in conversational contexts systematically. To achieve this, we reconstructed the MELD dataset, which is based on the popular TV series Friends, and conducted experiments through three sub-tasks: overall emotion tone prediction, character emotion prediction, and contextually appropriate emotion expression selection. We evaluated the performance differences based on various model architectures (e.g., image encoders, modality alignment, and LLMs) and image scopes (e.g., entire scene, person, and facial expression). In addition, we investigated the impact of providing persona information on the emotion prediction performance of the models and analyzed how personality traits and speaking styles influenced the emotion prediction process. We conducted an in-depth analysis of the impact of various other factors, such as gender and regional biases, on the emotion prediction performance of VLLMs. The results revealed that these factors significantly influenced the model performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.332.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--332 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.332/>Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models</a></strong><br><a href=/people/j/jerry-huang/>Jerry Huang</a>
|
<a href=/people/p/prasanna-parthasarathi/>Prasanna Parthasarathi</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a>
|
<a href=/people/s/sarath-chandar/>Sarath Chandar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--332><div class="card-body p-3 small">Despite their widespread adoption, large language models (LLMs) remain prohibitive to use under resource constraints, with their ever growing sizes only increasing the barrier for use. One particular issue stems from the high latency associated with auto-regressive generation in LLMs, rendering the largest LLMs difficult to use without advanced computing infrastructure. Assisted decoding, where a smaller draft model guides a larger expert model’s generation, has helped alleviate this concern, but remains dependent on alignment between the two models. Thus if the draft model is insufficiently capable on some domain of interest relative to the target model, performance can degrade. Alternatively, one can leverage multiple draft models to better cover the expertise of the target, but when multiple black-box draft models are available, selecting an assistant without details about its construction can be difficult. To better understand this decision making problem, we observe it as a contextual bandit, where a policy must choose a draft model based on a context. We show that even without prior knowledge of the draft models, creating an offline dataset from only outputs of independent draft/target models and training a policy over the alignment of these outputs can accelerate performance on multiple domains as long as an individual draft model is effective. We observe these results hold on various settings with multiple assisted decoding candidates, highlighting its flexibility and the advantageous role that such decision making can play.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.333.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--333 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.333/>Teaching Small Language Models Reasoning through Counterfactual Distillation</a></strong><br><a href=/people/t/tao-feng/>Tao Feng</a>
|
<a href=/people/y/yicheng-li/>Yicheng Li</a>
|
<a href=/people/l/li-chenglin/>Li Chenglin</a>
|
<a href=/people/h/hao-chen/>Hao Chen</a>
|
<a href=/people/f/fei-yu/>Fei Yu</a>
|
<a href=/people/y/yin-zhang/>Yin Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--333><div class="card-body p-3 small">With the rise of large language models (LLMs), many studies are interested in transferring the reasoning capabilities of LLMs to small language models (SLMs). Previous distillation methods usually utilize the capabilities of LLMs to generate chain-of-thought (CoT) samples and teach SLMs via fine-tuning. However, such a standard distillation approach performs poorly when applied to out-of-distribution (OOD) examples, and the diversity of the generated CoT samples is insufficient. In this work, we propose a novel counterfactual distillation framework. Firstly, we leverage LLMs to automatically generate high-quality counterfactual data. Given an input text example, our method generates a counterfactual example that is very similar to the original input, but its task label has been changed to the desired one. Then, we utilize multi-view CoT to enhance the diversity of reasoning samples. Experiments on four NLP benchmarks show that our approach enhances the reasoning capabilities of SLMs and is more robust to OOD data. We also conduct extensive ablations and sample studies to understand the reasoning capabilities of SLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.334.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.334.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--334 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.334 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.334/>Pretraining Language Models Using Translationese</a></strong><br><a href=/people/m/meet-doshi/>Meet Doshi</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--334><div class="card-body p-3 small">In this paper, we explore the utility of Translationese as synthetic data created using machine translation for pre-training language models (LMs) for low-resource languages (LRLs). Our simple methodology consists of translating large amounts of web-crawled monolingual documents (clean) into the LRLs, followed by filtering the translated documents using tiny LMs trained on small but clean LRL data. Taking the case of Indian languages, we pre-train LMs from scratch with 28M and 85M parameters, and then fine-tune them for 5 downstream natural language understanding (NLU) and 4 generative (NLG) tasks. We observe that pre-training on filtered synthetic data leads to relative performance drops of only 0.87% for NLU and 2.35% for NLG, compared to pre-training on clean data, and this gap further diminishes upon the inclusion of a small amount of clean data. We also study the impact of synthetic data filtering and the choice of source language for synthetic data generation. Furthermore, evaluating continually pre-trained larger models like Gemma-2B and Llama-3-8B in few-shot settings, we observe that using synthetic data is competitive with using clean data. Our findings suggest that synthetic data shows promise for bridging the pre-training gap between English and LRLs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.335.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--335 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.335/>Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval</a></strong><br><a href=/people/k/kyle-buettner/>Kyle Buettner</a>
|
<a href=/people/a/adriana-kovashka/>Adriana Kovashka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--335><div class="card-body p-3 small">There is a scarcity of multilingual vision-language models that properly account for the perceptual differences that are reflected in image captions across languages and cultures. In this work, through a multimodal, multilingual retrieval case study, we quantify the existing lack of model flexibility. We empirically show performance gaps between training on captions that come from native German perception and captions that have been either machine-translated or human-translated from English into German. To address these gaps, we further propose and evaluate caption augmentation strategies. While we achieve mean recall improvements (+1.3), gaps still remain, indicating an open area of future work for the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.336.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--336 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.336.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.336/><span class=acl-fixed-case>MTA</span>4<span class=acl-fixed-case>DPR</span>: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval</a></strong><br><a href=/people/q/qixi-lu/>Qixi Lu</a>
|
<a href=/people/e/endong-xun/>Endong Xun</a>
|
<a href=/people/g/gongbo-tang/>Gongbo Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--336><div class="card-body p-3 small">Although Dense Passage Retrieval (DPR) models have achieved significantly enhanced performance, their widespread application is still hindered by the demanding inference efficiency and high deployment costs. Knowledge distillation is an efficient method to compress models, which transfers knowledge from strong teacher models to weak student models. Previous studies have proved the effectiveness of knowledge distillation in DPR. However, there often remains a significant performance gap between the teacher and the distilled student. To narrow this performance gap, we propose MTA4DPR, a Multi-Teaching-Assistants based iterative knowledge distillation method for Dense Passage Retrieval, which transfers knowledge from the teacher to the student with the help of multiple assistants in an iterative manner; with each iteration, the student learns from more performant assistants and more difficult data. The experimental results show that our 66M student model achieves the state-of-the-art performance among models with same parameters on multiple datasets, and is very competitive when compared with larger, even LLM-based, DPR models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.337.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.337.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--337 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.337 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.337/>Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of <span class=acl-fixed-case>G</span>erman Parliamentary Debates</a></strong><br><a href=/people/a/aida-kostikova/>Aida Kostikova</a>
|
<a href=/people/d/dominik-beese/>Dominik Beese</a>
|
<a href=/people/b/benjamin-paassen/>Benjamin Paassen</a>
|
<a href=/people/o/ole-putz/>Ole Pütz</a>
|
<a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--337><div class="card-body p-3 small">Solidarity is a crucial concept to understand social relations in societies. In this study, we investigate the frequency of (anti-)solidarity towards women and migrants in German parliamentary debates between 1867 and 2022. Using 2,864 manually annotated text snippets, we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and GPT-4. We find that GPT-4 outperforms other models, approaching human annotation accuracy. Using GPT-4, we automatically annotate 18,300 further instances and find that solidarity with migrants outweighs anti-solidarity but that frequencies and solidarity types shift over time. Most importantly, group-based notions of (anti-)solidarity fade in favor of compassionate solidarity, focusing on the vulnerability of migrant groups, and exchange-based anti-solidarity, focusing on the lack of (economic) contribution. This study highlights the interplay of historical events, socio-economic needs, and political ideologies in shaping migration discourse and social cohesion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.338.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.338.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--338 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.338 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.338.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.338.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.338/><span class=acl-fixed-case>CI</span>tru<span class=acl-fixed-case>S</span>: Chunked Instruction-aware State Eviction for Long Sequence Modeling</a></strong><br><a href=/people/y/yu-bai/>Yu Bai</a>
|
<a href=/people/x/xiyuan-zou/>Xiyuan Zou</a>
|
<a href=/people/h/he-yan-huang/>Heyan Huang</a>
|
<a href=/people/s/sanxing-chen/>Sanxing Chen</a>
|
<a href=/people/m/marc-antoine-rondeau/>Marc-Antoine Rondeau</a>
|
<a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/j/jackie-ck-cheung/>Jackie CK Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--338><div class="card-body p-3 small">Long sequence modeling has gained broad interest as large language models (LLMs) continue to advance. Recent research has identified that a large portion of hidden states within the key-value caches of Transformer models can be discarded (also termed evicted) withoutaffecting the perplexity performance in generating long sequences. However, we show that these methods, despite preserving perplexity performance, often drop information that is important for solving downstream tasks, a problem which we call information neglect. To address this issue, we introduce Chunked Instruction-aware State Eviction (CItruS), a novel modeling technique that integrates the attention preferences useful for a downstream task into the eviction process of hidden states. In addition, we design a method for chunked sequence processing to further improve efficiency. Our training-free method exhibits superior performance on long sequence comprehension and retrieval tasks over several strong baselines under the same memory budget, while preserving language modeling perplexity. The code and data have been released at https://github.com/ybai-nlp/CItruS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.339.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.339.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--339 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.339 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.339/>Story Embeddings — Narrative-Focused Representations of Fictional Stories</a></strong><br><a href=/people/h/hans-ole-hatzel/>Hans Ole Hatzel</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--339><div class="card-body p-3 small">We present a novel approach to modeling fictional narratives. The proposed model creates embeddings that represent a story such that similar narratives, that is, reformulations of the same story, will result in similar embeddings. We showcase the prowess of our narrative-focused embeddings on various datasets, exhibiting state-of-the-art performance on multiple retrieval tasks. The embeddings also show promising results on a narrative understanding task. Additionally, we perform an annotation-based evaluation to validate that our introduced computational notion of narrative similarity aligns with human perception. The approach can help to explore vast datasets of stories, with potential applications in recommender systems and in the computational analysis of literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.340.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--340 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.340/><span class=acl-fixed-case>C</span>-<span class=acl-fixed-case>LLM</span>: Learn to Check <span class=acl-fixed-case>C</span>hinese Spelling Errors Character by Character</a></strong><br><a href=/people/k/kunting-li/>Kunting Li</a>
|
<a href=/people/y/yong-hu/>Yong Hu</a>
|
<a href=/people/l/liang-he/>Liang He</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--340><div class="card-body p-3 small">Chinese Spell Checking (CSC) aims to detect and correct spelling errors in sentences. Despite Large Language Models (LLMs) exhibit robust capabilities and are widely applied in various tasks, their performance on CSC is often unsatisfactory. We find that LLMs fail to meet the Chinese character-level constraints of the CSC task, namely equal length and phonetic similarity, leading to a performance bottleneck. Further analysis reveals that this issue stems from the granularity of tokenization, as current mixed character-word tokenization struggles to satisfy these character-level constraints. To address this issue, we propose C-LLM, a Large Language Model-based Chinese Spell Checking method that learns to check errors Character by Character. Character-level tokenization enables the model to learn character-level alignment, effectively mitigating issues related to character-level constraints. Furthermore, CSC is simplified to replication-dominated and substitution-supplemented tasks. Experiments on two CSC benchmarks demonstrate that C-LLM achieves a 2.1% enhancement in general scenarios and a significant 12% improvement in vertical domain scenarios compared to existing methods, establishing state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.341.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--341 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.341.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.341/><span class=acl-fixed-case>PSC</span>: Extending Context Window of Large Language Models via Phase Shift Calibration</a></strong><br><a href=/people/w/wenqiao-zhu/>Wenqiao Zhu</a>
|
<a href=/people/c/chao-xu/>Chao Xu</a>
|
<a href=/people/l/lulu-wang/>Lulu Wang</a>
|
<a href=/people/j/jun-wu/>Jun Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--341><div class="card-body p-3 small">Rotary Position Embedding (RoPE) is an efficient position encoding approach and is widely utilized in numerous large language models (LLMs). Recently, a lot of methods have been put forward to further expand the context window based on RoPE. The core concept of those methods is to predefine or search for a set of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a challenge for existing methods to predefine an optimal factor due to the exponential search space. In view of this, we introduce PSC (Phase Shift Calibration), a small module for calibrating the frequencies predefined by existing methods. With the employment of PSC, we demonstrate that many existing methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted extensive experiments across multiple models and tasks. The results demonstrate that (1) when PSC is enabled, the comparative reductions in perplexity increase as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our approach is broadly applicable and exhibits robustness across a variety of models and tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.342.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--342 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.342/>Video-<span class=acl-fixed-case>LL</span>a<span class=acl-fixed-case>VA</span>: Learning United Visual Representation by Alignment Before Projection</a></strong><br><a href=/people/b/bin-lin/>Bin Lin</a>
|
<a href=/people/y/yang-ye/>Yang Ye</a>
|
<a href=/people/b/bin-zhu/>Bin Zhu</a>
|
<a href=/people/j/jiaxi-cui/>Jiaxi Cui</a>
|
<a href=/people/m/munan-ning/>Munan Ning</a>
|
<a href=/people/p/peng-jin/>Peng Jin</a>
|
<a href=/people/l/li-yuan/>Li Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--342><div class="card-body p-3 small">Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers.In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks.Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.343.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--343 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.343.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.343/><span class=acl-fixed-case>S</span>ay<span class=acl-fixed-case>S</span>elf: Teaching <span class=acl-fixed-case>LLM</span>s to Express Confidence with Self-Reflective Rationales</a></strong><br><a href=/people/t/tianyang-xu/>Tianyang Xu</a>
|
<a href=/people/s/shujin-wu/>Shujin Wu</a>
|
<a href=/people/s/shizhe-diao/>Shizhe Diao</a>
|
<a href=/people/x/xiaoze-liu/>Xiaoze Liu</a>
|
<a href=/people/x/xingyao-wang/>Xingyao Wang</a>
|
<a href=/people/y/yangyi-chen/>Yangyi Chen</a>
|
<a href=/people/j/jing-gao/>Jing Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--343><div class="card-body p-3 small">Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work has elicited confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present SaySelf, a novel training framework that teaches LLMs to express more fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. The generated self-reflective rationales are also reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.344.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--344 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.344/>Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing</a></strong><br><a href=/people/r/richard-diehl-martinez/>Richard Diehl Martinez</a>
|
<a href=/people/z/zebulon-goriely/>Zebulon Goriely</a>
|
<a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a>
|
<a href=/people/l/lisa-beinborn/>Lisa Beinborn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--344><div class="card-body p-3 small">Language models strongly rely on frequency information because they maximize the likelihood of tokens during pre-training. As a consequence, language models tend to not generalize well to tokens that are seldom seen during training. Moreover, maximum likelihood training has been discovered to give rise to anisotropy: representations of tokens in a model tend to cluster tightly in a high-dimensional cone, rather than spreading out over their representational capacity.Our work introduces a method for quantifying the frequency bias of a language model by assessing sentence-level perplexity with respect to token-level frequency. We then present a method for reducing the frequency bias of a language model by inducing a syntactic prior over token representations during pre-training. Our Syntactic Smoothing method adjusts the maximum likelihood objective function to distribute the learning signal to syntactically similar tokens. This approach results in better performance on infrequent English tokens and a decrease in anisotropy. We empirically show that the degree of anisotropy in a model correlates with its frequency bias.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.345.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.345.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--345 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.345 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.345/><span class=acl-fixed-case>T</span>oxi<span class=acl-fixed-case>C</span>loak<span class=acl-fixed-case>CN</span>: Evaluating Robustness of Offensive Language Detection in <span class=acl-fixed-case>C</span>hinese with Cloaking Perturbations</a></strong><br><a href=/people/y/yunze-xiao/>Yunze Xiao</a>
|
<a href=/people/y/yujia-hu/>Yujia Hu</a>
|
<a href=/people/k/kenny-tsu-wei-choo/>Kenny Tsu Wei Choo</a>
|
<a href=/people/r/roy-ka-wei-lee/>Roy Ka-Wei Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--345><div class="card-body p-3 small">Detecting hate speech and offensive language is essential for maintaining a safe and respectful digital environment. This study examines the limitations of state-of-the-art large language models (LLMs) in identifying offensive content within systematically perturbed data, with a focus on Chinese, a language particularly susceptible to such perturbations. We introduce ToxiCloakCN, an enhanced dataset derived from ToxiCN, augmented with homophonic substitutions and emoji transformations, to test the robustness of LLMs against these cloaking perturbations. Our findings reveal that existing models significantly underperform in detecting offensive content when these perturbations are applied. We provide an in-depth analysis of how different types of offensive content are affected by these perturbations and explore the alignment between human and model explanations of offensiveness. Our work highlights the urgent need for more advanced techniques in offensive language detection to combat the evolving tactics used to evade detection mechanisms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.346.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.346.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--346 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.346 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.346/>Boosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?</a></strong><br><a href=/people/s/siyu-yuan/>Siyu Yuan</a>
|
<a href=/people/c/cheng-jiayang/>Cheng Jiayang</a>
|
<a href=/people/l/lin-qiu/>Lin Qiu</a>
|
<a href=/people/d/deqing-yang/>Deqing Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--346><div class="card-body p-3 small">Analogical reasoning plays a critical role in human cognition, enabling us to understand new concepts by associating them with familiar ones. Previous research in the AI community has mainly focused on identifying and generating analogies and then examining their quality under human evaluation, which overlooks the practical application of these analogies in real-world settings. Inspired by the human education process, in this paper, we propose to investigate how analogies created by teacher language models (LMs) can assist student LMs in understanding scientific concepts, thereby aligning more closely with practical scenarios. Our results suggest that free-form analogies can indeed aid LMs in understanding concepts. Additionally, analogies generated by student LMs can improve their own performance on scientific question answering, demonstrating their capability to use analogies for self-learning new knowledge. Resources are available athttps://github.com/siyuyuan/SCUA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.347.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--347 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.347.software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.347/>Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation</a></strong><br><a href=/people/j/jirui-qi/>Jirui Qi</a>
|
<a href=/people/g/gabriele-sarti/>Gabriele Sarti</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--347><div class="card-body p-3 small">Ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain. Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers. However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs’ context usage throughout the generation. In this work, we present MIRAGE – Model Internals-based RAG Explanations – a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. We evaluate our proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. Our qualitative evaluation highlights the faithfulness of MIRAGE’s attributions and underscores the promising application of model internals for RAG answer attribution. Code and data released at https://github.com/Betswish/MIRAGE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.348.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.348.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--348 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.348 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.348/>Do Large Language Models Know How Much They Know?</a></strong><br><a href=/people/g/gabriele-prato/>Gabriele Prato</a>
|
<a href=/people/j/jerry-huang/>Jerry Huang</a>
|
<a href=/people/p/prasanna-parthasarathi/>Prasanna Parthasarathi</a>
|
<a href=/people/s/shagun-sodhani/>Shagun Sodhani</a>
|
<a href=/people/s/sarath-chandar/>Sarath Chandar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--348><div class="card-body p-3 small">Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. Nevertheless, the rapid advancement in their deployment trails a comprehensive understanding of their internal mechanisms, as well as a delineation of their capabilities and limitations. A desired characteristic of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this attribute, we develop a benchmark that challenges these models to enumerate all information they possess on specific topics. This benchmark assesses whether the models recall excessive, insufficient, or the precise amount of required information, thereby indicating their awareness of how much they know about the given topic. Our findings reveal that the emergence of this property varies across different architectures and manifests at diverse rates. However, with sufficient scaling, all tested models are ultimately capable of performing this task. The insights gained from this research advance our understanding of LLMs, shedding light on their operational capabilities and contributing to the ongoing exploration of their intricate dynamics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.349.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--349 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.349/>Investigating Mysteries of <span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>T</span>-Augmented Distillation</a></strong><br><a href=/people/s/somin-wadhwa/>Somin Wadhwa</a>
|
<a href=/people/s/silvio-amir/>Silvio Amir</a>
|
<a href=/people/b/byron-c-wallace/>Byron C Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--349><div class="card-body p-3 small">Eliciting chain of thought (CoT) rationales - sequences of token that convey a “reasoning” process has been shown to consistently improve LLM performance on tasks like question answering. More recent efforts have shown that such rationales can also be used for model distillation: Including CoT sequences (elicited from a large “teacher” model) in addition to target labels when fine-tuning a small student model yields (often substantial) improvements. In this work we ask: Why and how does this additional training signal help in model distillation? We perform ablations to interrogate this, and report some potentially surprising results. Specifically: (1) Placing CoT sequences after labels (rather than before) realizes consistently better downstream performance – this means that no student “reasoning” is necessary at test time to realize gains. (2) When rationales are appended in this way, they need not be coherent reasoning sequences to yield improvements; performance increases are robust to permutations of CoT tokens, for example. In fact, (3) a small number of key tokens are sufficient to achieve improvements equivalent to those observed when full rationales are used in model distillation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.350.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.350.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--350 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.350 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.350.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.350/><span class=acl-fixed-case>S</span>ci<span class=acl-fixed-case>P</span>rompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics</a></strong><br><a href=/people/z/zhiwen-you/>Zhiwen You</a>
|
<a href=/people/k/kanyao-han/>Kanyao Han</a>
|
<a href=/people/h/haotian-zhu/>Haotian Zhu</a>
|
<a href=/people/b/bertram-ludaescher/>Bertram Ludaescher</a>
|
<a href=/people/j/jana-diesner/>Jana Diesner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--350><div class="card-body p-3 small">Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance levels comparable to those of fully fine-tuning methods. Previous studies have used crafted prompt templates and verbalizers, mapping from the label terms space to the class space, to solve the classification problem as a masked language modeling task. However, cross-domain and fine-grained prompt-based fine-tuning with an automatically enriched verbalizer remains unexplored, mainly due to the difficulty and costs of manually selecting domain label terms for the verbalizer, which requires humans with domain expertise. To address this challenge, we introduce SciPrompt, a framework designed to automatically retrieve scientific topic-related terms for low-resource text classification tasks. To this end, we select semantically correlated and domain-specific label terms within the context of scientific literature for verbalizer augmentation. Furthermore, we propose a new verbalization strategy that uses correlation scores as additional weights to enhance the prediction performance of the language model during model tuning. Our method outperforms state-of-the-art, prompt-based fine-tuning methods on scientific text classification tasks under few and zero-shot settings, especially in classifying fine-grained and emerging scientific topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.351.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.351.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--351 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.351 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.351/>Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in <span class=acl-fixed-case>CLIP</span></a></strong><br><a href=/people/s/samyadeep-basu/>Samyadeep Basu</a>
|
<a href=/people/s/shell-xu-hu/>Shell Xu Hu</a>
|
<a href=/people/m/maziar-sanjabi/>Maziar Sanjabi</a>
|
<a href=/people/d/daniela-massiceti/>Daniela Massiceti</a>
|
<a href=/people/s/soheil-feizi/>Soheil Feizi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--351><div class="card-body p-3 small">Image-text contrastive models like CLIP have wide applications in zero-shot classification, image-text retrieval, and transfer learning. However, they often struggle on compositional visio-linguistic tasks (e.g., attribute-binding or object-relationships) where their performance is no better than random chance. To address this, we introduce SDS-CLIP, a lightweight and sample-efficient distillation method to enhance CLIP’s compositional visio-linguistic reasoning. Our approach fine-tunes CLIP using a distillation objective borrowed from large text-to-image generative models like Stable-Diffusion, which are known for their strong visio-linguistic reasoning abilities. On the challenging Winoground benchmark, SDS-CLIP improves the visio-linguistic performance of various CLIP models by up to 7%, while on the ARO dataset, it boosts performance by up to 3%. This work underscores the potential of well-designed distillation objectives from generative models to enhance contrastive image-text models with improved visio-linguistic reasoning capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.352.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--352 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.352/>Learning from Natural Language Explanations for Generalizable Entity Matching</a></strong><br><a href=/people/s/somin-wadhwa/>Somin Wadhwa</a>
|
<a href=/people/a/adit-krishnan/>Adit Krishnan</a>
|
<a href=/people/r/runhui-wang/>Runhui Wang</a>
|
<a href=/people/b/byron-c-wallace/>Byron C Wallace</a>
|
<a href=/people/l/luyang-kong/>Luyang Kong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--352><div class="card-body p-3 small">Entity matching is the task of linking records from different sources that refer to the same real-world entity. Past work has primarily treated entity linking as a standard supervised learning problem. However, supervised entity matching models often do not generalize well to new data, and collecting exhaustive labeled training data is often cost prohibitive. Further, recent efforts have adopted LLMs for this task in few/zero-shot settings, exploiting their general knowledge. But LLMs are prohibitively expensive for performing inference at scale for real-world entity matching tasks.As an efficient alternative, we re-cast entity matching as a conditional generation task as opposed to binary classification. This enables us to “distill” LLM reasoning into smaller entity matching models via natural language explanations. This approach achieves strong performance, especially on out-of-domain generalization tests (10.85% F-1) where standalone generative methods struggle. We perform ablations that highlight the importance of explanations, both for performance and model robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.353.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.353.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--353 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.353 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.353/>Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation</a></strong><br><a href=/people/z/zhuohang-li/>Zhuohang Li</a>
|
<a href=/people/j/jiaxin-zhang/>Jiaxin Zhang</a>
|
<a href=/people/c/chao-yan/>Chao Yan</a>
|
<a href=/people/k/kamalika-das/>Kamalika Das</a>
|
<a href=/people/s/sricharan-kumar/>Sricharan Kumar</a>
|
<a href=/people/m/murat-kantarcioglu/>Murat Kantarcioglu</a>
|
<a href=/people/b/bradley-a-malin/>Bradley A. Malin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--353><div class="card-body p-3 small">Language models (LMs) are known to suffer from hallucinations and misinformation. Retrieval augmented generation (RAG) that retrieves verifiable information from an external knowledge corpus to complement the parametric knowledge in LMs provides a tangible solution to these problems. However, the generation quality of RAG is highly dependent on the relevance between a user’s query and the retrieved documents. Inaccurate responses may be generated when the query is outside of the scope of knowledge represented in the external knowledge corpus or if the information in the corpus is out-of-date. In this work, we establish a statistical framework that assesses how well a query can be answered by an RAG system by capturing the relevance of knowledge. We introduce an online testing procedure that employs goodness-of-fit (GoF) tests to inspect the relevance of each user query to detect out-of-knowledge queries with low knowledge relevance. Additionally, we develop an offline testing framework that examines a collection of user queries, aiming to detect significant shifts in the query distribution which indicates the knowledge corpus is no longer sufficiently capable of supporting the interests of the users. We demonstrate the capabilities of these strategies through a systematic evaluation on eight question-answering (QA) datasets, the results of which indicate that the new testing framework is an efficient solution to enhance the reliability of existing RAG systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.354.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.354.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--354 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.354 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.354.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.354/>On the Reliability of Psychological Scales on Large Language Models</a></strong><br><a href=/people/j/jen-tse-huang/>Jen-tse Huang</a>
|
<a href=/people/w/wenxiang-jiao/>Wenxiang Jiao</a>
|
<a href=/people/m/man-ho-lam/>Man Ho Lam</a>
|
<a href=/people/e/eric-john-li/>Eric John Li</a>
|
<a href=/people/w/wenxuan-wang/>Wenxuan Wang</a>
|
<a href=/people/m/michael-lyu/>Michael Lyu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--354><div class="card-body p-3 small">Recent research has focused on examining Large Language Models’ (LLMs) characteristics from a psychological standpoint, acknowledging the necessity of understanding their behavioral characteristics. The administration of personality tests to LLMs has emerged as a noteworthy area in this context. However, the suitability of employing psychological scales, initially devised for humans, on LLMs is a matter of ongoing debate. Our study aims to determine the reliability of applying personality assessments to LLMs, explicitly investigating whether LLMs demonstrate consistent personality traits. Analysis of 2,500 settings per model, including GPT-3.5, GPT-4, Gemini-Pro, and LLaMA-3.1, reveals that various LLMs show consistency in responses to the Big Five Inventory, indicating a satisfactory level of reliability. Furthermore, our research explores the potential of GPT-3.5 to emulate diverse personalities and represent various groups—a capability increasingly sought after in social sciences for substituting human participants with LLMs to reduce costs. Our findings reveal that LLMs have the potential to represent different personalities with specific prompt instructions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.355.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--355 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.355.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.355.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.355/>Contrastive Entity Coreference and Disambiguation for Historical Texts</a></strong><br><a href=/people/a/abhishek-arora/>Abhishek Arora</a>
|
<a href=/people/e/emily-silcock/>Emily Silcock</a>
|
<a href=/people/m/melissa-dell/>Melissa Dell</a>
|
<a href=/people/l/leander-heldring/>Leander Heldring</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--355><div class="card-body p-3 small">Massive-scale historical document collections are crucial for social science research. Despite increasing digitization, these documents typically lack unique cross-document identifiers for individuals mentioned within the texts, as well as individual identifiers from external knowledge bases like Wikipedia/Wikidata. Existing entity disambiguation methods often fall short in accuracy for historical documents, which are replete with individuals not remembered in contemporary knowledge bases. This study makes three key contributions to improve cross-document coreference resolution and disambiguation in historical texts: a massive-scale training dataset replete with hard negatives - that sources over 190 million entity pairs from Wikipedia contexts and disambiguation pages - high-quality evaluation data from hand-labeled historical newswire articles, and trained models evaluated on this historical benchmark. We contrastively train bi-encoder models for coreferencing and disambiguating individuals in historical texts, achieving accurate, scalable performance that identifies out-of-knowledge base individuals. Our approach significantly surpasses other entity disambiguation models on our historical newswire benchmark. Our models also demonstrate competitive performance on modern entity disambiguation benchmarks, particularly on certain news disambiguation datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.356.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--356 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.356.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.356/>Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models</a></strong><br><a href=/people/j/jeonghwan-kim/>Jeonghwan Kim</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--356><div class="card-body p-3 small">Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs such as LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate descriptive visual attributes based on a concept that appears within an input image despite their prominent zero-shot image captioning ability. In-depth analyses show that instruction-tuned LVLMs suffer from modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept. In an effort to further the community’s endeavor in this direction, we propose a multiple granularity attribute-centric benchmark and training mixture, Finer, which aims to establish a ground to evaluate LVLMs’ fine-grained visual comprehension ability and provide significantly improved explainability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.357.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--357 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.357/>Evaluating <span class=acl-fixed-case>LLM</span>s for Targeted Concept Simplification for Domain-Specific Texts</a></strong><br><a href=/people/s/sumit-asthana/>Sumit Asthana</a>
|
<a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/e/elizabeth-clark/>Elizabeth Clark</a>
|
<a href=/people/f/fantine-huot/>Fantine Huot</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--357><div class="card-body p-3 small">One useful application of NLP models is to support people in reading complex text from unfamiliar domains (e.g., scientific articles). Simplifying the entire text makes it understandable but sometimes removes important details. On the contrary, helping adult readers understand difficult concepts in context can enhance their vocabulary and knowledge. In a preliminary human study, we first identify that lack of context and unfamiliarity with difficult concepts is a major reason for adult readers’ difficulty with domain-specific text. We then introduce targeted concept simplification, a simplification task for rewriting text to help readers comprehend text containing unfamiliar concepts. We also introduce WikiDomains, a new dataset of 22k definitions from 13 academic domains paired with a difficult concept within each definition. We benchmark the performance of open-source and commercial LLMs and a simple dictionary baseline on this task across human judgments of ease of understanding and meaning preservation. Interestingly, our human judges preferred explanations about the difficult concept more than simplifications of the concept phrase. Further, no single model achieved superior performance across all quality dimensions, and automated metrics also show low correlations with human evaluations of concept simplification (~0.2), opening up rich avenues for research on personalized human reading comprehension support.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.358.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.358.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--358 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.358 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.358.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.358.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.358/><span class=acl-fixed-case>VLF</span>eedback: A Large-Scale <span class=acl-fixed-case>AI</span> Feedback Dataset for Large Vision-Language Models Alignment</a></strong><br><a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/z/zhihui-xie/>Zhihui Xie</a>
|
<a href=/people/m/mukai-li/>Mukai Li</a>
|
<a href=/people/s/shunian-chen/>Shunian Chen</a>
|
<a href=/people/p/peiyi-wang/>Peiyi Wang</a>
|
<a href=/people/l/liang-chen/>Liang Chen</a>
|
<a href=/people/y/yazheng-yang/>Yazheng Yang</a>
|
<a href=/people/b/benyou-wang/>Benyou Wang</a>
|
<a href=/people/l/lingpeng-kong/>Lingpeng Kong</a>
|
<a href=/people/q/qi-liu/>Qi Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--358><div class="card-body p-3 small">As large vision-language models (LVLMs) evolve rapidly, the demand for high-quality and diverse data to align these models becomes increasingly crucial. However, the creation of such data with human supervision proves costly and time-intensive. In this paper, we investigate the efficacy of AI feedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the first large-scale vision-language feedback dataset, comprising over 82K multi-modal instructions and comprehensive rationales generated by off-the-shelf models without human annotations. To evaluate the effectiveness of AI feedback for vision-language alignment, we train Silkie, an LVLM fine-tuned via direct preference optimization on VLFeedback. Silkie showcases exceptional performance regarding helpfulness, visual faithfulness, and safety metrics. It outperforms its base model by 6.9% and 9.5% in perception and cognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits enhanced resilience against red-teaming attacks. Furthermore, our analysis underscores the advantage of AI feedback, particularly in fostering preference diversity to deliver more comprehensive improvements. Our dataset, training code and models are available at <a href=https://vlf-silkie.github.io class=acl-markup-url>https://vlf-silkie.github.io</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.359.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.359.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--359 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.359 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.359.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.359/>Focused Large Language Models are Stable Many-Shot Learners</a></strong><br><a href=/people/p/peiwen-yuan/>Peiwen Yuan</a>
|
<a href=/people/s/shaoxiong-feng/>Shaoxiong Feng</a>
|
<a href=/people/y/yiwei-li/>Yiwei Li</a>
|
<a href=/people/x/xinglin-wang/>Xinglin Wang</a>
|
<a href=/people/y/yueqi-zhang/>Yueqi Zhang</a>
|
<a href=/people/c/chuyi-tan/>Chuyi Tan</a>
|
<a href=/people/b/boyuan-pan/>Boyuan Pan</a>
|
<a href=/people/h/heda-wang/>Heda Wang</a>
|
<a href=/people/y/yao-hu/>Yao Hu</a>
|
<a href=/people/k/kan-li/>Kan Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--359><div class="card-body p-3 small">In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL does not necessarily scale well in many-shot (demonstration) settings. We hypothesize that the reason lies in more demonstrations dispersing the model attention from the query, hindering its understanding of key content, which we validate both theoretically and experimentally. Inspired by how humans learn from examples, we propose a training-free method FocusICL, which conducts triviality filtering to avoid attention being diverted by unimportant contents at token-level and operates hierarchical attention to further ensure sufficient attention towards current query at demonstration-level. We also design an efficient hyperparameter searching strategy for FocusICL based on model perplexity of demonstrations. Comprehensive experiments validate that FocusICL achieves an average performance improvement of 5.2% over vanilla ICL and scales well with many-shot demonstrations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.360.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--360 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.360/>Reconsidering Sentence-Level Sign Language Translation</a></strong><br><a href=/people/g/garrett-tanzer/>Garrett Tanzer</a>
|
<a href=/people/m/maximus-shengelia/>Maximus Shengelia</a>
|
<a href=/people/k/ken-harrenstien/>Ken Harrenstien</a>
|
<a href=/people/d/david-c-uthus/>David Uthus</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--360><div class="card-body p-3 small">Historically, sign language machine translation has been posed as a sentence-level task: datasets consisting of continuous narratives are chopped up and presented to the model as isolated clips. In this work, we explore the limitations of this task framing. First, we survey a number of linguistic phenomena in sign languages that depend on discourse-level context. Then as a case study, we perform the first human baseline for sign language translation that actually substitutes a human into the machine learning task framing, rather than provide the human with the entire document as context. This human baseline—for ASL to English translation on the How2Sign dataset—shows that for 33% of sentences in our sample, our fluent Deaf signer annotators were only able to understand key parts of the clip in light of additional discourse-level context. These results underscore the importance of understanding and sanity checking examples when adapting machine learning to new domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.361.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.361.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--361 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.361 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.361.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.361/><span class=acl-fixed-case>GAMA</span>: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities</a></strong><br><a href=/people/s/sreyan-ghosh/>Sreyan Ghosh</a>
|
<a href=/people/s/sonal-kumar/>Sonal Kumar</a>
|
<a href=/people/a/ashish-seth/>Ashish Seth</a>
|
<a href=/people/c/chandra-kiran-reddy-evuru/>Chandra Kiran Reddy Evuru</a>
|
<a href=/people/u/utkarsh-tyagi/>Utkarsh Tyagi</a>
|
<a href=/people/s/s-sakshi/>S Sakshi</a>
|
<a href=/people/o/oriol-nieto/>Oriol Nieto</a>
|
<a href=/people/r/ramani-duraiswami/>Ramani Duraiswami</a>
|
<a href=/people/d/dinesh-manocha/>Dinesh Manocha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--361><div class="card-body p-3 small">Perceiving and understanding non-speech sounds and non-verbal speech is essential to making decisions that help us interact with our surroundings. In this paper, we propose GAMA, a novel General-purpose Large Audio-Language Model (LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We build GAMA by integrating an LLM with multiple types of audio representations, including features from a custom Audio Q-Former, a multi-layer aggregator that aggregates features from multiple layers of an audio encoder. We fine-tune GAMA on a large-scale audio-language dataset, which augments it with audio understanding capabilities. Next, we propose CompA-R (Instruction-Tuning for Complex Audio Reasoning), a synthetically generated instruction-tuning (IT) dataset with instructions that require the model to perform complex reasoning on the input audio. We instruction-tune GAMA with CompA-R to endow it with complex reasoning abilities, where we further add a soft prompt as input with high-level semantic evidence by leveraging event tags of the input audio. Finally, we also propose CompA-R-test, a human-labeled evaluation dataset for evaluating the capabilities of LALMs on open-ended audio question-answering that requires complex reasoning. Through automated and expert human evaluations, we show that GAMA outperforms all other LALMs in literature on diverse audio understanding tasks by margins of 1%-84% and demonstrates state-of-the-art performance on deductive reasoning and hallucination evaluation benchmarks. Further, GAMA IT-ed on CompA-R proves to be superior in its complex reasoning capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.362.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.362.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--362 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.362 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.362.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.362/>Verba volant, scripta volant? Don’t worry! There are computational solutions for protoword reconstruction</a></strong><br><a href=/people/l/liviu-p-dinu/>Liviu P Dinu</a>
|
<a href=/people/a/ana-sabina-uban/>Ana Sabina Uban</a>
|
<a href=/people/a/alina-maria-cristea/>Alina Maria Cristea</a>
|
<a href=/people/i/ioan-bogdan-iordache/>Ioan-Bogdan Iordache</a>
|
<a href=/people/t/teodor-george-marchitan/>Teodor-George Marchitan</a>
|
<a href=/people/s/simona-georgescu/>Simona Georgescu</a>
|
<a href=/people/l/laurentiu-zoicas/>Laurentiu Zoicas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--362><div class="card-body p-3 small">We introduce a new database of cognate words and etymons for the five main Romance languages, the most comprehensive one to date. We propose a strong benchmark for the automatic reconstruction of protowords for Romance languages, by applying a set of machine learning models and features on these data. The best results reach 90% accuracy in predicting the protoword of a given cognate set, surpassing existing state-of-the-art results for this task and showing that computational methods can be very useful in assisting linguists with protoword reconstruction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.363.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.363.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--363 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.363 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.363/><span class=acl-fixed-case>C</span>hat<span class=acl-fixed-case>GPT</span> Doesn’t Trust Chargers Fans: Guardrail Sensitivity in Context</a></strong><br><a href=/people/v/victoria-r-li/>Victoria R Li</a>
|
<a href=/people/y/yida-chen/>Yida Chen</a>
|
<a href=/people/n/naomi-saphra/>Naomi Saphra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--363><div class="card-body p-3 small">While the biases of language models in production are extensively documented, the biases of their guardrails have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas are more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, refusing to comply with requests for a political position the user is likely to disagree with. We find that certain identity groups and seemingly innocuous information, e.g., sports fandom, can elicit changes in guardrail sensitivity similar to direct statements of political ideology. For each demographic category and even for American football team fandom, we find that ChatGPT appears to infer a likely political ideology and modify guardrail behavior accordingly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.364.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--364 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.364/>Personas as a Way to Model Truthfulness in Language Models</a></strong><br><a href=/people/n/nitish-joshi/>Nitish Joshi</a>
|
<a href=/people/j/javier-rando/>Javier Rando</a>
|
<a href=/people/a/abulhair-saparov/>Abulhair Saparov</a>
|
<a href=/people/n/najoung-kim/>Najoung Kim</a>
|
<a href=/people/h/he-he/>He He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--364><div class="card-body p-3 small">Large language models (LLMs) are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. While unintuitive from a classic view of LMs, recent work has shown that the truth value of a statement can be elicited from the model’s representations. This paper presents an explanation for why LMs appear to know the truth despite not being trained with truth labels. We hypothesize that the pretraining data is generated by groups of (un)truthful agents whose outputs share common features, and they form a (un)truthful persona. By training on this data, LMs can infer and represent the persona in its activation space. This allows the model to separate truth from falsehoods and controls the truthfulness of its generation. We show evidence for the persona hypothesis via two observations: (1) we can probe whether a model’s answer will be truthful before it is generated; (2) finetuning a model on a set of facts improves its truthfulness on unseen topics. Next, using arithmetics as a synthetic environment, we show that structures of the pretraining data are crucial for the model to infer the truthful persona. Overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.365.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--365 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.365.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.365/>Satyrn: A Platform for Analytics Augmented Generation</a></strong><br><a href=/people/m/marko-sterbentz/>Marko Sterbentz</a>
|
<a href=/people/c/cameron-barrie/>Cameron Barrie</a>
|
<a href=/people/s/shubham-shahi/>Shubham Shahi</a>
|
<a href=/people/a/abhratanu-dutta/>Abhratanu Dutta</a>
|
<a href=/people/d/donna-hooshmand/>Donna Hooshmand</a>
|
<a href=/people/h/harper-pack/>Harper Pack</a>
|
<a href=/people/k/kristian-j-hammond/>Kristian J Hammond</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--365><div class="card-body p-3 small">Large language models (LLMs) are capable of producing documents, and retrieval augmented generation (RAG) has shown itself to be a powerful method for improving accuracy without sacrificing fluency. However, not all information can be retrieved from text. We propose an approach that uses the analysis of structured data to generate fact sets that are used to guide generation in much the same way that retrieved documents are used in RAG. This analytics augmented generation (AAG) approach supports the ability to utilize standard analytic techniques to generate facts that are then converted to text and passed to an LLM. We present a neurosymbolic platform, Satyrn, that leverages AAG to produce accurate, fluent, and coherent reports grounded in large scale databases. In our experiments, we find that Satyrn generates reports in which over 86% of claims are accurate while maintaining high levels of fluency and coherence, even when using smaller language models such as Mistral-7B, as compared to GPT-4 Code Interpreter in which just 57% of claims are accurate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.366.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--366 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.366/><span class=acl-fixed-case>EH</span>-<span class=acl-fixed-case>MAM</span>: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning</a></strong><br><a href=/people/a/ashish-seth/>Ashish Seth</a>
|
<a href=/people/r/ramaneswaran-selvakumar/>Ramaneswaran Selvakumar</a>
|
<a href=/people/s/s-sakshi/>S Sakshi</a>
|
<a href=/people/s/sonal-kumar/>Sonal Kumar</a>
|
<a href=/people/s/sreyan-ghosh/>Sreyan Ghosh</a>
|
<a href=/people/d/dinesh-manocha/>Dinesh Manocha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--366><div class="card-body p-3 small">In this paper, we present EH-MAM (Easy-to-Hard adaptive Masked Acoustic Modeling), a novel self-supervised learning approach for speech representation learning. In contrast to the prior methods that use random masking schemes for Masked Acoustic Modeling (MAM), we introduce a novel selective and adaptive masking strategy. Specifically, during SSL training, we progressively introduce harder regions to the model for reconstruction. Our approach automatically selects hard regions and is built on the observation that the reconstruction loss of individual frames in MAM can provide natural signals to judge the difficulty of solving the MAM pre-text task for that frame. To identify these hard regions, we employ a teacher model that first predicts the frame-wise losses and then decides which frames to mask. By learning to create challenging problems, such as identifying harder frames and solving them simultaneously, the model is able to learn more effective representations and thereby acquire a more comprehensive understanding of the speech. Quantitatively, EH-MAM outperforms several state-of-the-art baselines across various low-resource speech recognition and SUPERB benchmarks by 5%-10%. Additionally, we conduct a thorough analysis to show that the regions masked by EH-MAM effectively capture useful context across speech frames.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.367.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.367.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--367 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.367 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.367/><span class=acl-fixed-case>EPO</span>: Hierarchical <span class=acl-fixed-case>LLM</span> Agents with Environment Preference Optimization</a></strong><br><a href=/people/q/qi-zhao/>Qi Zhao</a>
|
<a href=/people/h/haotian-fu/>Haotian Fu</a>
|
<a href=/people/c/chen-sun/>Chen Sun</a>
|
<a href=/people/g/george-konidaris/>George Konidaris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--367><div class="card-body p-3 small">Long-horizon decision-making tasks present significant challenges for LLM-based agents due to the need for extensive planning over multiple steps. In this paper, we propose a hierarchical framework that decomposes complex tasks into manageable subgoals, utilizing separate LLMs for subgoal prediction and low-level action generation. To address the challenge of creating training signals for unannotated datasets, we develop a reward model that leverages multimodal environment feedback to automatically generate reward signals. We introduce Environment Preference Optimization (EPO), a novel method that generates preference signals from the environment’s feedback and uses them to train LLM-based agents. Extensive experiments on ALFRED demonstrate the state-of-the-art performance of our framework, achieving first place on the ALFRED public leaderboard and showcasing its potential to improve long-horizon decision-making in diverse environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.368.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.368.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--368 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.368 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.368/>Detection and Measurement of Syntactic Templates in Generated Text</a></strong><br><a href=/people/c/chantal-shaib/>Chantal Shaib</a>
|
<a href=/people/y/yanai-elazar/>Yanai Elazar</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a>
|
<a href=/people/b/byron-c-wallace/>Byron C Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--368><div class="card-body p-3 small">The diversity of text can be measured beyond word-level features, however existing diversity evaluation focuses primarily on word-level features. Here we propose a method for evaluating diversity over syntactic features to characterize general repetition in models, beyond frequent <span class=tex-math>n</span>-grams. Specifically, we define <i>syntactic templates</i> (e.g., strings comprising parts-of-speech) and show that models tend to produce templated text in downstream tasks at a higher rate than what is found in human-reference textsWe find that most (76%) templates in model-generated text can be found in pre-training data (compared to only 35% of human-authored text), and are not overwritten during fine-tuning or alignment processes such as RLHF. The connection between templates in generated text and the pre-training data allows us to analyze syntactic templates in models where we do not have the pre-training data.We also find that templates as features are able to differentiate between models, tasks, and domains, and are useful for qualitatively evaluating common model constructions.Finally, we demonstrate the use of templates as a useful tool for analyzing style memorization of training data in LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.369.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.369.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--369 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.369 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.369/><span class=acl-fixed-case>UOUO</span>: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models</a></strong><br><a href=/people/x/xinyu-pi/>Xinyu Pi</a>
|
<a href=/people/m/mingyuan-wu/>Mingyuan Wu</a>
|
<a href=/people/j/jize-jiang/>Jize Jiang</a>
|
<a href=/people/h/haozhen-zheng/>Haozhen Zheng</a>
|
<a href=/people/b/beitong-tian/>Beitong Tian</a>
|
<a href=/people/c/chengxiang-zhai/>ChengXiang Zhai</a>
|
<a href=/people/k/klara-nahrstedt/>Klara Nahrstedt</a>
|
<a href=/people/z/zhiting-hu/>Zhiting Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--369><div class="card-body p-3 small">Smaller-scale Vision-Language Models (VLMs) often claim to perform on par with larger models in general-domain visual grounding and question-answering benchmarks while offering advantages in computational efficiency and storage. However, their ability to handle rare objects, which fall into the long tail of data distributions, is less understood. To rigorously evaluate this aspect, we introduce the “Uncontextualized Uncommon Objects” (UOUO) benchmark. This benchmark focuses on systematically testing VLMs with both large and small parameter counts on rare and specialized objects. Our comprehensive analysis reveals that while smaller VLMs maintain competitive performance on common datasets, they significantly underperform on tasks involving uncommon objects. We also propose an advanced, scalable pipeline for data collection and cleaning, ensuring the UOUO benchmark provides high-quality, challenging instances. These findings highlight the need to consider long-tail distributions when assessing the true capabilities of VLMs. Code and project details for UOUO can be found at https://zoezheng126.github.io/UOUO-Website/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.370.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.370.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--370 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.370 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.370.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.370/>Optimized Speculative Sampling for <span class=acl-fixed-case>GPU</span> Hardware Accelerators</a></strong><br><a href=/people/d/dominik-wagner/>Dominik Wagner</a>
|
<a href=/people/s/seanie-lee/>Seanie Lee</a>
|
<a href=/people/i/ilja-baumann/>Ilja Baumann</a>
|
<a href=/people/p/philipp-seeberger/>Philipp Seeberger</a>
|
<a href=/people/k/korbinian-riedhammer/>Korbinian Riedhammer</a>
|
<a href=/people/t/tobias-bocklet/>Tobias Bocklet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--370><div class="card-body p-3 small">In this work, we optimize speculative sampling for parallel hardware accelerators to improve sampling speed. We notice that substantial portions of the intermediate matrices necessary for speculative sampling can be computed concurrently. This allows us to distribute the workload across multiple GPU threads, enabling simultaneous operations on matrix segments within thread blocks. This results in profiling time improvements ranging from 6% to 13% relative to the baseline implementation, without compromising accuracy. To further accelerate speculative sampling, probability distributions parameterized by softmax are approximated by sigmoid. This approximation approach results in significantly greater relative improvements in profiling time, ranging from 37% to 94%, with a minor decline in accuracy. We conduct extensive experiments on both automatic speech recognition and summarization tasks to validate the effectiveness of our optimization methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.371.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.371.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--371 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.371 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.371.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.371.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.371/>Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts</a></strong><br><a href=/people/z/zhaoxuan-tan/>Zhaoxuan Tan</a>
|
<a href=/people/z/zheyuan-liu/>Zheyuan Liu</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--371><div class="card-body p-3 small">Personalized large language models (LLMs) aim to tailor interactions, content, and recommendations to individual user preferences. While parameter-efficient fine-tuning (PEFT) methods excel in performance and generalization, they are costly and limit communal benefits when used individually. To this end, we introduce Personalized Pieces (Per-Pcs), a framework that allows users to safely share and assemble personalized PEFT efficiently with collaborative efforts. Per-Pcs involves selecting sharers, breaking their PEFT into pieces, and training gates for each piece. These pieces are added to a pool, from which target users can select and assemble personalized PEFT using their history data. This approach preserves privacy and enables fine-grained user modeling without excessive storage and computation demands. Experimental results show Per-Pcs outperforms non-personalized and PEFT retrieval baselines, offering performance comparable to OPPU with significantly lower resource use across six tasks. Further analysis highlights Per-Pcs’s robustness concerning sharer count and selection strategy, pieces sharing ratio, and scalability in computation time and storage space. Per-Pcs’s modularity promotes safe sharing, making LLM personalization more efficient, effective, and widely accessible through collaborative efforts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.372.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.372.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--372 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.372 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.372.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.372.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.372/>Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning</a></strong><br><a href=/people/z/zhaoxuan-tan/>Zhaoxuan Tan</a>
|
<a href=/people/q/qingkai-zeng/>Qingkai Zeng</a>
|
<a href=/people/y/yijun-tian/>Yijun Tian</a>
|
<a href=/people/z/zheyuan-liu/>Zheyuan Liu</a>
|
<a href=/people/b/bing-yin/>Bing Yin</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--372><div class="card-body p-3 small">Personalization in large language models (LLMs) is increasingly important, aiming to align the LLMs’ interactions, content, and recommendations with individual user preferences. Recent advances have highlighted effective prompt design by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these methods faced limitations due to a lack of model ownership, resulting in constrained customization and privacy issues, and often failed to capture complex, dynamic user behavior patterns. To address these shortcomings, we introduce One PEFT Per User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT) modules to store user-specific behavior patterns and preferences. By plugging in personal PEFT parameters, users can own and use their LLMs individually. OPPU integrates parametric user knowledge in the personal PEFT parameters with non-parametric knowledge from retrieval and profiles, adapting LLMs to user behavior shifts. Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark. Further studies reveal OPPU’s enhanced capabilities in handling user behavior shifts, modeling users at different activity levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.373.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.373.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--373 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.373 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.373/>Unifying Multimodal Retrieval via Document Screenshot Embedding</a></strong><br><a href=/people/x/xueguang-ma/>Xueguang Ma</a>
|
<a href=/people/s/sheng-chieh-lin/>Sheng-Chieh Lin</a>
|
<a href=/people/m/minghan-li/>Minghan Li</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--373><div class="card-body p-3 small">In the real world, documents are organized in different formats and varied modalities. Traditional retrieval pipelines require tailored document parsing techniques and content extraction modules to prepare input for indexing. This process is tedious, prone to errors, and has information loss. To this end, we propose Document Screenshot Embedding (DSE), a novel retrieval paradigm that regards document screenshots as a unified input format, which does not require any content extraction preprocess and preserves all the information in a document (e.g., text, image and layout). DSE leverages a large vision-language model to directly encode document screenshots into dense representations for retrieval. To evaluate our method, we first craft the dataset of Wiki-SS, a 1.3M Wikipedia web page screenshots as the corpus to answer the questions from the Natural Questions dataset. In such a text-intensive document retrieval setting, DSE shows competitive effectiveness compared to other text retrieval methods relying on parsing. For example, DSE outperforms BM25 by 17 points in top-1 retrieval accuracy. Additionally, in a mixed-modality task of slide retrieval, DSE significantly outperforms OCR text retrieval methods by over 15 points in nDCG@10. These experiments show that DSE is an effective document retrieval paradigm for diverse types of documents. Model checkpoints, code, and Wiki-SS collection will be released.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.374.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.374.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--374 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.374 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.374/>Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation</a></strong><br><a href=/people/s/shaomu-tan/>Shaomu Tan</a>
|
<a href=/people/d/di-wu/>Di Wu</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--374><div class="card-body p-3 small">Training a unified multilingual model promotes knowledge transfer but inevitably introduces negative interference. Language-specific modeling methods show promise in reducing interference. However, they often rely on heuristics to distribute capacity and struggle to foster cross-lingual transfer via isolated modules. In this paper, we explore intrinsic task modularity within multilingual networks and leverage these observations to circumvent interference under multilingual translation. We show that neurons in the feed-forward layers tend to be activated in a language-specific manner. Meanwhile, these specialized neurons exhibit structural overlaps that reflect language proximity, which progress across layers. Based on these findings, we propose Neuron Specialization, an approach that identifies specialized neurons to modularize feed-forward layers and then continuously updates them through sparse networks. Extensive experiments show that our approach achieves consistent performance gains over strong baselines with additional analyses demonstrating reduced interference and increased knowledge transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.375.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.375.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--375 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.375 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.375.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.375/>An Audit on the Perspectives and Challenges of Hallucinations in <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/p/pranav-narayanan-venkit/>Pranav Narayanan Venkit</a>
|
<a href=/people/t/tatiana-chakravorti/>Tatiana Chakravorti</a>
|
<a href=/people/v/vipul-gupta/>Vipul Gupta</a>
|
<a href=/people/h/heidi-biggs/>Heidi Biggs</a>
|
<a href=/people/m/mukund-srinath/>Mukund Srinath</a>
|
<a href=/people/k/koustava-goswami/>Koustava Goswami</a>
|
<a href=/people/s/sarah-rajtmajer/>Sarah Rajtmajer</a>
|
<a href=/people/s/shomir-wilson/>Shomir Wilson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--375><div class="card-body p-3 small">We audit how hallucination in large language models (LLMs) is characterized in peer-reviewed literature, using a critical examination of 103 publications across NLP research. Through the examination of the literature, we identify a lack of agreement with the term ‘hallucination’ in the field of NLP. Additionally, to compliment our audit, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis calls for the necessity of explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.376.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--376 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.376/>Discovering Knowledge-Critical Subnetworks in Pretrained Language Models</a></strong><br><a href=/people/d/deniz-bayazit/>Deniz Bayazit</a>
|
<a href=/people/n/negar-foroutan/>Negar Foroutan</a>
|
<a href=/people/z/zeming-chen/>Zeming Chen</a>
|
<a href=/people/g/gail-weiss/>Gail Weiss</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--376><div class="card-body p-3 small">Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various *knowledge-critical* subnetworks: particular sparse computational subgraphs that can, if removed, precisely suppress specific knowledge the model has memorized. We propose a multi-objective differentiable masking scheme that can be applied to both weights and neurons to discover such subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+ sparsity) that are critical for expressing specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial abilities but struggles to represent the suppressed knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.377.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.377.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--377 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.377 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.377/>Reconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with <span class=acl-fixed-case>GPT</span> Models</a></strong><br><a href=/people/j/junjie-chu/>Junjie Chu</a>
|
<a href=/people/z/zeyang-sha/>Zeyang Sha</a>
|
<a href=/people/m/michael-backes/>Michael Backes</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--377><div class="card-body p-3 small">Significant advancements have recently been made in large language models, represented by GPT models.Users frequently have multi-round private conversations with cloud-hosted GPT models for task optimization.Yet, this operational paradigm introduces additional attack surfaces, particularly in custom GPTs and hijacked chat sessions.In this paper, we introduce a straightforward yet potent Conversation Reconstruction Attack.This attack targets the contents of previous conversations between GPT models and benign users, i.e., the benign users’ input contents during their interaction with GPT models.The adversary could induce GPT models to leak such contents by querying them with designed malicious prompts.Our comprehensive examination of privacy risks during the interactions with GPT models under this attack reveals GPT-4’s considerable resilience.We present two advanced attacks targeting improved reconstruction of past conversations, demonstrating significant privacy leakage across all models under these advanced techniques.Evaluating various defense mechanisms, we find them ineffective against these attacks.Our findings highlight the ease with which privacy can be compromised in interactions with GPT models, urging the community to safeguard against potential abuses of these models’ capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.378.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--378 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.378.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.378.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.378/>Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering</a></strong><br><a href=/people/a/armin-toroghi/>Armin Toroghi</a>
|
<a href=/people/w/willis-guo/>Willis Guo</a>
|
<a href=/people/m/mohammad-mahdi-abdollah-pour/>Mohammad Mahdi Abdollah Pour</a>
|
<a href=/people/s/scott-sanner/>Scott Sanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--378><div class="card-body p-3 small">Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., *“In which city was Silvio Berlusconi’s first wife born?”*, leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., *“Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?”* unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (<span class=tex-math>R<sup>3</sup></span>), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasks—question answering, claim verification, and preference matching—our findings showcase <span class=tex-math>R<sup>3</sup></span> as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.379.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--379 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.379.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.379/>Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via <span class=acl-fixed-case>LLM</span>-based Theory Resolution</a></strong><br><a href=/people/a/armin-toroghi/>Armin Toroghi</a>
|
<a href=/people/w/willis-guo/>Willis Guo</a>
|
<a href=/people/a/ali-pesaranghader/>Ali Pesaranghader</a>
|
<a href=/people/s/scott-sanner/>Scott Sanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--379><div class="card-body p-3 small">Recent advances in Large Language Models (LLM) have led to substantial interest in their application to commonsense reasoning tasks. Despite their potential, LLMs are susceptible to reasoning errors and hallucinations that may be harmful in use cases where accurate reasoning is critical. This challenge underscores the need for verifiable, debuggable, and repairable LLM reasoning. Recent works have made progress toward verifiable reasoning with LLMs by using them as either (i) a reasoner over an axiomatic knowledge base, or (ii) a semantic parser for use in existing logical inference systems. However, both settings are unable to extract commonsense axioms from the LLM that are not already formalized in the knowledge base, and also lack a reliable method to repair missed commonsense inferences. In this work, we present LLM-TRes, a logical reasoning framework based on the notion of “theory resolution” that allows for seamless integration of the commonsense knowledge from LLMs with a verifiable logical reasoning framework that mitigates hallucinations and facilitates debugging of the reasoning procedure as well as repair. We crucially prove that repaired axioms are theoretically guaranteed to be given precedence over flawed ones in our theory resolution inference process. We conclude by evaluating on three diverse language-based reasoning tasks—preference reasoning, deductive reasoning, and causal commonsense reasoning—and demonstrate the superior performance of LLM-TRes vs. state-of-the-art LLM-based reasoning methods in terms of both accuracy and reasoning correctness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.380.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.380.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--380 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.380 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.380/>Understanding and Mitigating Language Confusion in <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/k/kelly-marchisio/>Kelly Marchisio</a>
|
<a href=/people/w/wei-yin-ko/>Wei-Yin Ko</a>
|
<a href=/people/a/alexandre-berard/>Alexandre Berard</a>
|
<a href=/people/t/theo-dehaze/>Théo Dehaze</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--380><div class="card-body p-3 small">We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user’s desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.381.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--381 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.381/>Can Large Language Models Learn Independent Causal Mechanisms?</a></strong><br><a href=/people/g/gael-gendron/>Gael Gendron</a>
|
<a href=/people/b/bao-trung-nguyen/>Bao Trung Nguyen</a>
|
<a href=/people/a/alex-yuxuan-peng/>Alex Yuxuan Peng</a>
|
<a href=/people/m/michael-j-witbrock/>Michael Witbrock</a>
|
<a href=/people/g/gillian-dobbie/>Gillian Dobbie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--381><div class="card-body p-3 small">Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting a lack of generalisation ability. By contrast, systems such as causal models, that learn abstract variables and causal relationships, can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language modelling modules. We show that such causal constraints can improve out-of-distribution performance on abstract and causal reasoning tasks. We also investigate the level of independence and domain specialisation and show that LLMs rely on pre-trained partially domain-invariant mechanisms resilient to fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.382.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.382.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--382 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.382 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.382/><span class=acl-fixed-case>M</span>irror<span class=acl-fixed-case>S</span>tories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models</a></strong><br><a href=/people/s/sarfaroz-yunusov/>Sarfaroz Yunusov</a>
|
<a href=/people/h/hamza-sidat/>Hamza Sidat</a>
|
<a href=/people/a/ali-emami/>Ali Emami</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--382><div class="card-body p-3 small">This study explores the effectiveness of Large Language Models (LLMs) in creating personalized “mirror stories” that reflect and resonate with individual readers’ identities, addressing the significant lack of diversity in literature. We present MirrorStories, a corpus of 1,500 personalized short stories generated by integrating elements such as name, gender, age, ethnicity, reader interest, and story moral. We demonstrate that LLMs can effectively incorporate diverse identity elements into narratives, with human evaluators identifying personalized elements in the stories with high accuracy. Through a comprehensive evaluation involving 26 diverse human judges, we compare the effectiveness of MirrorStories against generic narratives. We find that personalized LLM-generated stories not only outscore generic human-written and LLM-generated ones across all metrics of engagement (with average ratings of 4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity while preserving the intended moral. We also provide analyses that include bias assessments and a study on the potential for integrating images into personalized stories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.383.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.383.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--383 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.383 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.383/><span class=acl-fixed-case>I</span>nter<span class=acl-fixed-case>I</span>ntent: Investigating Social Intelligence of <span class=acl-fixed-case>LLM</span>s via Intention Understanding in an Interactive Game Context</a></strong><br><a href=/people/z/ziyi-liu/>Ziyi Liu</a>
|
<a href=/people/a/abhishek-anand/>Abhishek Anand</a>
|
<a href=/people/p/pei-zhou/>Pei Zhou</a>
|
<a href=/people/j/jen-tse-huang/>Jen-tse Huang</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--383><div class="card-body p-3 small">Large language models (LLMs) have demonstrated the potential to mimic human social intelligence. However, most studies focus on simplistic and static self-report or performance-based tests, which limits the depth and validity of the analysis. In this paper, we developed a novel framework, InterIntent, to assess LLMs’ social intelligence by mapping their ability to understand and manage intentions in a game setting. We focus on four dimensions of social intelligence: situational awareness, self-regulation, self-awareness, and theory of mind. Each dimension is linked to a specific game task: intention selection, intention following, intention summarization, and intention guessing. Our findings indicate that while LLMs exhibit high proficiency in selecting intentions, achieving an accuracy of 88%, their ability to infer the intentions of others is significantly weaker, trailing human performance by 20%. Additionally, game performance correlates with intention understanding, highlighting the importance of the four components towards success in this game. These findings underline the crucial role of intention understanding in evaluating LLMs’ social intelligence and highlight the potential of using social deduction games as a complex testbed to enhance LLM evaluation. InterIntent contributes a structured approach to bridging the evaluation gap in social intelligence within multiplayer LLM-based games.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.384.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--384 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.384/>Locating Information Gaps and Narrative Inconsistencies Across Languages: A Case Study of <span class=acl-fixed-case>LGBT</span> People Portrayals on <span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/f/farhan-samir/>Farhan Samir</a>
|
<a href=/people/c/chan-young-park/>Chan Young Park</a>
|
<a href=/people/a/anjalie-field/>Anjalie Field</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--384><div class="card-body p-3 small">To explain social phenomena and identify systematic biases, much research in computational social science focuses on comparative text analyses. These studies often rely on coarse corpus-level statistics or local word-level analyses, mainly in English. We introduce the InfoGap method—an efficient and reliable approach to locating information gaps and inconsistencies in articles at the fact level, across languages. We evaluate InfoGap by analyzing LGBT people’s portrayals, across 2.7K biography pages on English, Russian, and French Wikipedias. We find large discrepancies in factual coverage across the languages. Moreover, our analysis reveals that biographical facts carrying negative connotations are more likely to be highlighted in Russian Wikipedia. Crucially, InfoGap both facilitates large scale analyses, and pinpoints local document- and fact-level information gaps, laying a new foundation for targeted and nuanced comparative language analysis at scale.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.385.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--385 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.385/>From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models</a></strong><br><a href=/people/m/mehar-bhatia/>Mehar Bhatia</a>
|
<a href=/people/s/sahithya-ravi/>Sahithya Ravi</a>
|
<a href=/people/a/aditya-chinchure/>Aditya Chinchure</a>
|
<a href=/people/e/eunjeong-hwang/>EunJeong Hwang</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--385><div class="card-body p-3 small">Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models’ cultural inclusivity. Still, they have limited coverage of cultures and do not adequately assess cultural diversity across universal and culture-specific local concepts. To address these limitations, we introduce the GlobalRG benchmark, comprising two challenging tasks: retrieval across universals and cultural visual grounding. The former task entails retrieving culturally diverse images for universal concepts from 50 countries, while the latter aims at grounding culture-specific concepts within images from 15 countries. Our evaluation across a wide range of models reveals that the performance varies significantly across cultures – underscoring the necessity for enhancing multicultural understanding in vision-language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.386.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.386.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--386 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.386 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.386/>Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation</a></strong><br><a href=/people/k/karin-de-langis/>Karin De Langis</a>
|
<a href=/people/r/ryan-koo/>Ryan Koo</a>
|
<a href=/people/d/dongyeop-kang/>Dongyeop Kang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--386><div class="card-body p-3 small">Textual style expresses a diverse set of information, including interpersonal dynamics (e.g., formality) and the author’s emotions or attitudes (e.g., disgust). An open question is how language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. One approach to such controlled generation is multi-objective reinforcement learning (RL), but how to best combine multiple objectives in a reward function is an open question. In this paper, we investigate various formulations of multi-style reward formulations, including calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. We find that our proposed dynamic weighting outperforms static weighting approaches with respect style control while maintaining linguistic quality, and we explore its effectiveness in 2- and 3-style control.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.387.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.387.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--387 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.387 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.387/><span class=acl-fixed-case>MMN</span>euron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model</a></strong><br><a href=/people/j/jiahao-huo/>Jiahao Huo</a>
|
<a href=/people/y/yibo-yan/>Yibo Yan</a>
|
<a href=/people/b/boren-hu/>Boren Hu</a>
|
<a href=/people/y/yutao-yue/>Yutao Yue</a>
|
<a href=/people/x/xuming-hu/>Xuming Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--387><div class="card-body p-3 small">Projecting visual features into word embedding space has become a significant fusion strategy adopted by Multimodal Large Language Models (MLLMs). However, its internal mechanisms have yet to be explored. Inspired by multilingual research, we identify domain-specific neurons in multimodal large language models. Specifically, we investigate the distribution of domain-specific neurons and the mechanism of how MLLMs process features from diverse domains. Furthermore, we propose a three-stage framework for language model modules in MLLMs when handling projected image features, and verify this hypothesis using logit lens. Extensive experiments indicate that while current MLLMs exhibit Visual Question Answering (VQA) capability, they may not fully utilize domain-specific information. Manipulating domain-specific neurons properly will result in a 10% change of accuracy at most, shedding light on the development of cross-domain, all-encompassing MLLMs in the future. The source code is available at https://anonymous.4open.science/r/MMNeuron.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.388.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.388.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--388 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.388 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.388/>Learning to Extract Structured Entities Using Language Models</a></strong><br><a href=/people/h/haolun-wu/>Haolun Wu</a>
|
<a href=/people/y/ye-yuan/>Ye Yuan</a>
|
<a href=/people/l/liana-mikaelyan/>Liana Mikaelyan</a>
|
<a href=/people/a/alexander-meulemans/>Alexander Meulemans</a>
|
<a href=/people/x/xue-liu/>Xue Liu</a>
|
<a href=/people/j/james-hensman/>James Hensman</a>
|
<a href=/people/b/bhaskar-mitra/>Bhaskar Mitra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--388><div class="card-body p-3 small">Recent advances in machine learning have significantly impacted the field of information extraction, with Language Models (LMs) playing a pivotal role in extracting structured information from unstructured text. Prior works typically represent information extraction as triplet-centric and use classical metrics such as precision and recall for evaluation. We reformulate the task to be entity-centric, enabling the use of diverse metrics that can provide more insights from various perspectives. We contribute to the field by introducing Structured Entity Extraction and proposing the Approximate Entity Set OverlaP (AESOP) metric, designed to appropriately assess model performance. Later, we introduce a new Multistage Structured Entity Extraction (MuSEE) model that harnesses the power of LMs for enhanced effectiveness and efficiency by decomposing the extraction task into multiple stages. Quantitative and human side-by-side evaluations confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction. Our source code is available at https://github.com/microsoft/Structured-Entity-Extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.389.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.389.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--389 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.389 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.389.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.389/>Efficient <span class=acl-fixed-case>LLM</span> Comparative Assessment: A Product of Experts Framework for Pairwise Comparisons</a></strong><br><a href=/people/a/adian-liusie/>Adian Liusie</a>
|
<a href=/people/v/vatsal-raina/>Vatsal Raina</a>
|
<a href=/people/y/yassir-fathullah/>Yassir Fathullah</a>
|
<a href=/people/m/mark-gales/>Mark Gales</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--389><div class="card-body p-3 small">LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks. However, when using pairwise comparisons to rank a set of candidates, the computational cost scales quadratically with the number of candidates, which has practical limitations. This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment. Here individual comparisons are considered experts that provide information on a pair’s score difference. The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed. When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking. Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate well with human judgements. We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment. With many candidate texts, using as few as 2% of comparisons the PoE solution can achieve similar performance to when all comparisons are used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.390.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--390 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.390/>A Survey of <span class=acl-fixed-case>AMR</span> Applications</a></strong><br><a href=/people/s/shira-wein/>Shira Wein</a>
|
<a href=/people/j/juri-opitz/>Juri Opitz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--390><div class="card-body p-3 small">In the ten years since the development of the Abstract Meaning Representation (AMR) formalism, substantial progress has been made on AMR-related tasks such as parsing and alignment. Still, the engineering applications of AMR are not fully understood. In this survey, we categorize and characterize more than 100 papers which use AMR for downstream tasks— the first survey of this kind for AMR. Specifically, we highlight (1) the range of applications for which AMR has been harnessed, and (2) the techniques for incorporating AMR into those applications. We also detect broader AMR engineering patterns and outline areas of future work that seem ripe for AMR incorporation. We hope that this survey will be useful to those interested in using AMR and that it sparks discussion on the role of symbolic representations in the age of neural-focused NLP research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.391.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.391.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--391 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.391 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.391/>Beyond Embeddings: The Promise of Visual Table in Visual Reasoning</a></strong><br><a href=/people/y/yiwu-zhong/>Yiwu Zhong</a>
|
<a href=/people/z/zi-yuan-hu/>Zi-Yuan Hu</a>
|
<a href=/people/m/michael-lyu/>Michael Lyu</a>
|
<a href=/people/l/liwei-wang/>Liwei Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--391><div class="card-body p-3 small">Visual representation learning has been a cornerstone in computer vision, involving typical forms such as visual embeddings, structural symbols, and text-based representations. Despite the success of CLIP-type visual embeddings, they often lack access to world knowledge critical for visual reasoning. In this work, we propose Visual Table, a novel form of visual representation tailored for visual reasoning. Visual tables are constructed as hierarchical descriptions of visual scenes, featuring a scene description and multiple object-centric descriptions covering categories, attributes, and knowledge. Thanks to the structural and textual formats, visual tables offer unique properties over mere visual embeddings, such as explainability and controllable editing. Furthermore, they deliver instance-level world knowledge and detailed attributes that are essential for visual reasoning. To create visual tables, we develop a generator trained on the dataset with collected, small-scale annotations. Extensive results on 11 visual reasoning benchmarks demonstrate that the generated visual tables significantly outperform previous structural and text-based representations. Moreover, they consistently enhance state-of-the-art multi-modal large language models across diverse benchmarks, showcasing their potential for advancing visual reasoning tasks. Our code is available at https://github.com/LaVi-Lab/Visual-Table.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.392.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.392.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--392 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.392 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.392/><span class=acl-fixed-case>C</span>are<span class=acl-fixed-case>C</span>orpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation</a></strong><br><a href=/people/s/shahla-farzana/>Shahla Farzana</a>
|
<a href=/people/i/ivana-lucero/>Ivana Lucero</a>
|
<a href=/people/v/vivian-villegas/>Vivian Villegas</a>
|
<a href=/people/v/vera-c-kaelin/>Vera C Kaelin</a>
|
<a href=/people/m/mary-khetani/>Mary Khetani</a>
|
<a href=/people/n/natalie-parde/>Natalie Parde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--392><div class="card-body p-3 small">Caregiver strategy classification in pediatric rehabilitation contexts is strongly motivated by real-world clinical constraints but highly under-resourced and seldom studied in natural language processing settings. We introduce a large dataset of 4,037 caregiver strategies in this setting, a five-fold increase over the nearest contemporary dataset. These strategies are manually categorized into clinically established constructs with high agreement (<span class=tex-math>𝜅</span>=0.68-0.89). We also propose two techniques to further address identified data constraints. First, we manually supplement target task data with publicly relevant data from online child health forums. Next, we propose a novel data augmentation technique to generate synthetic caregiver strategies with high downstream task utility. Extensive experiments showcase the quality of our dataset. They also establish evidence that both the publicly available data and the synthetic strategies result in large performance gains, with relative F<span class=tex-math><sub>1</sub></span> increases of 22.6% and 50.9%, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.393.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.393.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--393 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.393 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.393/><span class=acl-fixed-case>T</span>aylor Unswift: Secured Weight Release for Large Language Models via <span class=acl-fixed-case>T</span>aylor Expansion</a></strong><br><a href=/people/g/guanchu-wang/>Guanchu Wang</a>
|
<a href=/people/y/yu-neng-chuang/>Yu-Neng Chuang</a>
|
<a href=/people/r/ruixiang-tang/>Ruixiang Tang</a>
|
<a href=/people/s/shaochen-zhong/>Shaochen Zhong</a>
|
<a href=/people/j/jiayi-yuan/>Jiayi Yuan</a>
|
<a href=/people/h/hongye-jin/>Hongye Jin</a>
|
<a href=/people/z/zirui-liu/>Zirui Liu</a>
|
<a href=/people/v/vipin-chaudhary/>Vipin Chaudhary</a>
|
<a href=/people/s/shuai-xu/>Shuai Xu</a>
|
<a href=/people/j/james-caverlee/>James Caverlee</a>
|
<a href=/people/x/xia-hu/>Xia Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--393><div class="card-body p-3 small">Ensuring the security of released large language models (LLMs) poses a significant dilemma, as existing mechanisms either compromise ownership rights or raise data privacy concerns. To address this dilemma, we introduce TaylorMLP to protect the ownership of released LLMs and prevent their abuse. Specifically, TaylorMLP preserves the ownership of LLMs by transforming the weights of LLMs into parameters of Taylor-series. Instead of releasing the original weights, developers can release the Taylor-series parameters with users, thereby ensuring the security of LLMs. Moreover, TaylorMLP can prevent abuse of LLMs by adjusting the generation speed. It can induce low-speed token generation for the protected LLMs by increasing the terms in the Taylor-series. This intentional delay helps LLM developers prevent potential large-scale unauthorized uses of their models. Empirical experiments across five datasets and three LLM architectures demonstrate that TaylorMLP induces over increase in latency, producing the tokens precisely matched with original LLMs. Subsequent defensive experiments further confirm that TaylorMLP effectively prevents users from reconstructing the weight values based on downstream datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.394.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.394.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--394 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.394 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.394/><span class=acl-fixed-case>T</span>ime<span class=acl-fixed-case>R</span><span class=tex-math><sup>4</sup></span> : Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering</a></strong><br><a href=/people/x/xinying-qian/>Xinying Qian</a>
|
<a href=/people/y/ying-zhang/>Ying Zhang</a>
|
<a href=/people/y/yu-zhao/>Yu Zhao</a>
|
<a href=/people/b/baohang-zhou/>Baohang Zhou</a>
|
<a href=/people/x/xuhui-sui/>Xuhui Sui</a>
|
<a href=/people/l/li-zhang/>Li Zhang</a>
|
<a href=/people/k/kehui-song/>Kehui Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--394><div class="card-body p-3 small">Temporal Knowledge Graph Question Answering (TKGQA) aims to answer temporal questions using knowledge in Temporal Knowledge Graphs (TKGs). Previous works employ pre-trained TKG embeddings or graph neural networks to incorporate the knowledge of TKGs. However, these methods fail to fully understand the complex semantic information of time constraints in questions.In contrast, Large Language Models (LLMs) have shown exceptional performance in knowledge graph reasoning, unifying both semantic understanding and structural reasoning. To further enhance LLMs’ temporal reasoning ability, this paper aims to integrate relevant temporal knowledge from TKGs into LLMs through a Time-aware Retrieve-Rewrite-Retrieve-Rerank framework, which we named TimeR<span class=tex-math><sup>4</sup></span>.Specifically, to reduce temporal hallucination in LLMs, we propose a retrieve-rewrite module to rewrite questions using background knowledge stored in the TKGs, thereby acquiring explicit time constraints. Then, we implement a retrieve-rerank module aimed at retrieving semantically and temporally relevant facts from the TKGs and reranking them according to the temporal constraints.To achieve this, we fine-tune a retriever using the contrastive time-aware learning framework.Our approach achieves great improvements, with relative gains of 47.8% and 22.5% on two datasets, underscoring its effectiveness in boosting the temporal reasoning abilities of LLMs. Our code is available at https://github.com/qianxinying/TimeR4.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.395.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.395.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--395 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.395 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.395.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.395/>Knowledge-Centric Hallucination Detection</a></strong><br><a href=/people/x/xiangkun-hu/>Xiangkun Hu</a>
|
<a href=/people/d/dongyu-ru/>Dongyu Ru</a>
|
<a href=/people/l/lin-qiu/>Lin Qiu</a>
|
<a href=/people/q/qipeng-guo/>Qipeng Guo</a>
|
<a href=/people/t/tianhang-zhang/>Tianhang Zhang</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a>
|
<a href=/people/y/yun-luo/>Yun Luo</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--395><div class="card-body p-3 small">Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. RefChecker supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. RefChecker outperforms prior methods by 18.2 to 27.2 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.396.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.396.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--396 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.396 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.396.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.396.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.396/>Revealing the Parallel Multilingual Learning within Large Language Models</a></strong><br><a href=/people/y/yongyu-mu/>Yongyu Mu</a>
|
<a href=/people/p/peinan-feng/>Peinan Feng</a>
|
<a href=/people/z/zhiquan-cao/>Zhiquan Cao</a>
|
<a href=/people/y/yuzhang-wu/>Yuzhang Wu</a>
|
<a href=/people/b/bei-li/>Bei Li</a>
|
<a href=/people/c/chenglong-wang/>Chenglong Wang</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/k/kai-song/>Kai Song</a>
|
<a href=/people/t/tongran-liu/>Tongran Liu</a>
|
<a href=/people/c/chunliang-zhang/>Chunliang Zhang</a>
|
<a href=/people/j/jingbo-zhu/>JingBo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--396><div class="card-body p-3 small">Large language models (LLMs) can handle multilingual and cross-lingual text within a single input; however, previous works leveraging multilingualism in LLMs primarily focus on using English as the pivot language to enhance language understanding and reasoning. Given that multiple languages are a compensation for the losses caused by a single language’s limitations, it’s a natural next step to enrich the model’s learning context through the integration of the original input with its multiple translations. In this paper, we start by revealing that LLMs learn from parallel multilingual input (PMI). Our comprehensive evaluation shows that PMI enhances the model’s comprehension of the input, achieving superior performance than conventional in-context learning (ICL). Furthermore, to explore how multilingual processing affects prediction, we examine the activated neurons in LLMs. Surprisingly, involving more languages in the input activates fewer neurons, leading to more focused and effective neural activation patterns. Also, this neural reaction coincidently mirrors the neuroscience insight about synaptic pruning, highlighting a similarity between artificial and biological ‘brains’.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.397.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.397.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--397 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.397 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.397/>Automatic Instruction Evolving for Large Language Models</a></strong><br><a href=/people/w/weihao-zeng/>Weihao Zeng</a>
|
<a href=/people/c/can-xu/>Can Xu</a>
|
<a href=/people/y/yingxiu-zhao/>Yingxiu Zhao</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--397><div class="card-body p-3 small">Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.398.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.398/><span class=acl-fixed-case>R</span>ep<span class=acl-fixed-case>E</span>val: Effective Text Evaluation with <span class=acl-fixed-case>LLM</span> Representation</a></strong><br><a href=/people/s/shuqian-sheng/>Shuqian Sheng</a>
|
<a href=/people/y/yi-xu/>Yi Xu</a>
|
<a href=/people/t/tianhang-zhang/>Tianhang Zhang</a>
|
<a href=/people/z/zanwei-shen/>Zanwei Shen</a>
|
<a href=/people/l/luoyi-fu/>Luoyi Fu</a>
|
<a href=/people/j/jiaxin-ding/>Jiaxin Ding</a>
|
<a href=/people/l/lei-zhou/>Lei Zhou</a>
|
<a href=/people/x/xiaoying-gan/>Xiaoying Gan</a>
|
<a href=/people/x/xinbing-wang/>Xinbing Wang</a>
|
<a href=/people/c/chenghu-zhou/>Chenghu Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--398><div class="card-body p-3 small">The era of Large Language Models (LLMs) raises new demands for automatic evaluation metrics, which should be adaptable to various application scenarios while maintaining low cost and effectiveness. Traditional metrics for automatic text evaluation are often tailored to specific scenarios, while LLM-based evaluation metrics are costly, requiring fine-tuning or rely heavily on the generation capabilities of LLMs. Besides, previous LLM-based metrics ignore the fact that, within the space of LLM representations, there exist direction vectors that indicate the estimation of text quality. To this end, we introduce RepEval, a metric that leverages the projection of LLM representations for evaluation. Through simple prompt modifications, RepEval can easily transition to various tasks, requiring only minimal sample pairs for direction vector construction. Results on fourteen datasets across two evaluation tasks demonstrate the high effectiveness of our method, which exhibits a higher correlation with human judgments than previous methods, even in complex evaluation scenarios involving pair-wise selection under nuanced aspects. Our work underscores the richness of information regarding text quality embedded within LLM representations, offering insights for the development of new metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.399.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.399.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--399 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.399 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.399.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.399.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.399/>Generative Models for Automatic Medical Decision Rule Extraction from Text</a></strong><br><a href=/people/y/yuxin-he/>Yuxin He</a>
|
<a href=/people/b/buzhou-tang/>Buzhou Tang</a>
|
<a href=/people/x/xiaoling-wang/>Xiaoling Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--399><div class="card-body p-3 small">Medical decision rules play a key role in many clinical decision support systems (CDSS). However, these rules are conventionally constructed by medical experts, which is expensive and hard to scale up. In this study, we explore the automatic extraction of medical decision rules from text, leading to a solution to construct large-scale medical decision rules. We adopt a formulation of medical decision rules as binary trees consisting of condition/decision nodes. Such trees are referred to as medical decision trees and we introduce several generative models to extract them from text. The proposed models inherit the merit of two categories of successful natural language generation frameworks, i.e., sequence-to-sequence generation and autoregressive generation. To unleash the potential of pretrained language models, we design three styles of linearization (natural language, augmented natural language and JSON code), acting as the target sequence for our models. Our final system achieves 67% tree accuracy on a comprehensive Chinese benchmark, outperforming state-of-the-art baseline by 12%. The result demonstrates the effectiveness of generative models on explicitly modeling structural decision-making roadmaps, and shows great potential to boost the development of CDSS and explainable AI. Our code will be open-source upon acceptance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.400.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--400 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.400 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.400.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.400.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.400/>Encoding and Controlling Global Semantics for Long-form Video Question Answering</a></strong><br><a href=/people/t/thong-thanh-nguyen/>Thong Thanh Nguyen</a>
|
<a href=/people/z/zhiyuan-hu/>Zhiyuan Hu</a>
|
<a href=/people/x/xiaobao-wu/>Xiaobao Wu</a>
|
<a href=/people/c/cong-duy-t-nguyen/>Cong-Duy T Nguyen</a>
|
<a href=/people/s/see-kiong-ng/>See-Kiong Ng</a>
|
<a href=/people/l/luu-anh-tuan/>Anh Tuan Luu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--400><div class="card-body p-3 small">Seeking answers effectively for long videos is essential to build video question answering (videoQA) systems. Previous methods adaptively select frames and regions from long videos to save computations. However, this fails to reason over the whole sequence of video, leading to sub-optimal performance. To address this problem, we introduce a state space layer (SSL) into multi-modal Transformer to efficiently integrate global semantics of the video, which mitigates the video information loss caused by frame and region selection modules. Our SSL includes a gating unit to enable controllability over the flow of global semantics into visual representations. To further enhance the controllability, we introduce a cross-modal compositional congruence objective to encourage global semantics aligned with the question. To rigorously evaluate long-form videoQA capacity, we construct two new benchmarks Ego-QA and MAD-QA featuring videos of considerably long length, i.e. 17.5 minutes and 1.9 hours, respectively. Extensive experiments demonstrate the superiority of our framework on these new as well as existing datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.401.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--401 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.401/>Towards Understanding Jailbreak Attacks in <span class=acl-fixed-case>LLM</span>s: A Representation Space Analysis</a></strong><br><a href=/people/y/yuping-lin/>Yuping Lin</a>
|
<a href=/people/p/pengfei-he/>Pengfei He</a>
|
<a href=/people/h/han-xu/>Han Xu</a>
|
<a href=/people/y/yue-xing/>Yue Xing</a>
|
<a href=/people/m/makoto-yamada/>Makoto Yamada</a>
|
<a href=/people/h/hui-liu/>Hui Liu</a>
|
<a href=/people/j/jiliang-tang/>Jiliang Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--401><div class="card-body p-3 small">Large language models (LLMs) are susceptible to a type of attack known as jailbreaking, which misleads LLMs to output harmful contents. Although there are diverse jailbreak attack strategies, there is no unified understanding on why some methods succeed and others fail. This paper explores the behavior of harmful and harmless prompts in the LLM’s representation space to investigate the intrinsic properties of successful jailbreak attacks. We hypothesize that successful attacks share some similar properties: They are effective in moving the representation of the harmful prompt towards the direction to the harmless prompts. We leverage hidden representations into the objective of existing jailbreak attacks to move the attacks along the acceptance direction, and conduct experiments to validate the above hypothesis using the proposed objective. We hope this study provides new insights into understanding how LLMs understand harmfulness information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.402.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--402 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.402/>Enhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs</a></strong><br><a href=/people/c/cheng-gao/>Cheng Gao</a>
|
<a href=/people/c/chaojun-xiao/>Chaojun Xiao</a>
|
<a href=/people/z/zhenghao-liu/>Zhenghao Liu</a>
|
<a href=/people/h/huimin-chen/>Huimin Chen</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--402><div class="card-body p-3 small">Legal case retrieval (LCR) aims to provide similar cases as references for a given fact description. This task is crucial for promoting consistent judgments in similar cases, effectively enhancing judicial fairness and improving work efficiency for judges. However, existing works face two main challenges for real-world applications: existing works mainly focus on case-to-case retrieval using lengthy queries, which does not match real-world scenarios; and the limited data scale, with current datasets containing only hundreds of queries, is insufficient to satisfy the training requirements of existing data-hungry neural models. To address these issues, we introduce an automated method to construct synthetic query-candidate pairs and build the largest LCR dataset to date, LEAD, which is hundreds of times larger than existing datasets. This data construction method can provide ample training signals for LCR models. Experimental results demonstrate that model training with our constructed data can achieve state-of-the-art results on two widely-used LCR benchmarks. Besides, the construction method can also be applied to civil cases and achieve promising results. The data and codes can be found in https://github.com/thunlp/LEAD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.403.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--403 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.403/>Does Large Language Model Contain Task-Specific Neurons?</a></strong><br><a href=/people/r/ran-song/>Ran Song</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/s/shuting-jiang/>Shuting Jiang</a>
|
<a href=/people/y/yantuan-xian/>Yantuan Xian</a>
|
<a href=/people/s/shengxiang-gao/>Shengxiang Gao</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/z/zhengtao-yu/>Zhengtao Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--403><div class="card-body p-3 small">Large language models (LLMs) have demonstrated remarkable capabilities in comprehensively handling various types of natural language processing (NLP) tasks. However, there are significant differences in the knowledge and abilities required for different tasks. Therefore, it is important to understand whether the same LLM processes different tasks in the same way. Are there specific neurons in a LLM for different tasks? Inspired by neuroscience, this paper pioneers the exploration of whether distinct neurons are activated when a LLM handles different tasks. Compared with current research exploring the neurons of language and knowledge, task-specific neurons present a greater challenge due to their abstractness, diversity, and complexity. To address these challenges, this paper proposes a method for task-specific neuron localization based on Causal Gradient Variation with Special Tokens (CGVST). CGVST identifies task-specific neurons by concentrating on the most significant tokens during task processing, thereby eliminating redundant tokens and minimizing interference from non-essential neurons. Compared to traditional neuron localization methods, our approach can more effectively identify task-specific neurons. We conduct experiments across eight different public tasks. Experiments involving the inhibition and amplification of identified neurons demonstrate that our method can accurately locate task-specific neurons.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.404.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.404.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.404.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.404/>Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models</a></strong><br><a href=/people/p/philipp-mondorf/>Philipp Mondorf</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.405.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--405 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.405.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.405/>Advancing Test-Time Adaptation in Wild Acoustic Test Settings</a></strong><br><a href=/people/h/hongfu-liu/>Hongfu Liu</a>
|
<a href=/people/h/hengguan-huang/>Hengguan Huang</a>
|
<a href=/people/y/ye-wang/>Ye Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--405><div class="card-body p-3 small">Acoustic foundation models, fine-tuned for Automatic Speech Recognition (ASR), suffer from performance degradation in wild acoustic test settings when deployed in real-world scenarios. Stabilizing online Test-Time Adaptation (TTA) under these conditions remains an open and unexplored question. Existing wild vision TTA methods often fail to handle speech data effectively due to the unique characteristics of high-entropy speech frames, which are unreliably filtered out even when containing crucial semantic content. Furthermore, unlike static vision data, speech signals follow short-term consistency, requiring specialized adaptation strategies. In this work, we propose a novel wild acoustic TTA method tailored for ASR fine-tuned acoustic foundation models. Our method, Confidence-Enhanced Adaptation, performs frame-level adaptation using a confidence-aware weight scheme to avoid filtering out essential information in high-entropy frames. Additionally, we apply consistency regularization during test-time optimization to leverage the inherent short-term consistency of speech signals. Our experiments on both synthetic and real-world datasets demonstrate that our approach outperforms existing baselines under various wild acoustic test settings, including Gaussian noise, environmental sounds, accent variations, and sung speech.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.406.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--406 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.406/>Learning to Retrieve Iteratively for In-Context Learning</a></strong><br><a href=/people/y/yunmo-chen/>Yunmo Chen</a>
|
<a href=/people/t/tongfei-chen/>Tongfei Chen</a>
|
<a href=/people/h/harsh-jhamtani/>Harsh Jhamtani</a>
|
<a href=/people/p/patrick-xia/>Patrick Xia</a>
|
<a href=/people/r/richard-shin/>Richard Shin</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--406><div class="card-body p-3 small">We introduce iterative retrieval, a novel framework that empowers retrievers to make iterative decisions through policy optimization. Finding an optimal portfolio of retrieved items is a combinatorial optimization problem, generally considered NP-hard. This approach provides a learned approximation to such a solution, meeting specific task requirements under a given family of large language models (LLMs). We propose a training procedure based on reinforcement learning, incorporating feedback from LLMs. We instantiate an iterative retriever for composing in-context learning (ICL) exemplars and apply it to various semantic parsing tasks that demand synthesized programs as outputs. By adding only 4M additional parameters for state encoding, we convert an off-the-shelf dense retriever into a stateful iterative retriever, outperforming previous methods in selecting ICL exemplars on semantic parsing datasets such as CalFlow, TreeDST, and MTOP. Additionally, the trained iterative retriever generalizes across different inference LLMs beyond the one used during training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.407.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--407 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.407.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.407.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.407/>Taxonomy-guided Semantic Indexing for Academic Paper Search</a></strong><br><a href=/people/s/seongku-kang/>SeongKu Kang</a>
|
<a href=/people/y/yunyi-zhang/>Yunyi Zhang</a>
|
<a href=/people/p/pengcheng-jiang/>Pengcheng Jiang</a>
|
<a href=/people/d/dongha-lee/>Dongha Lee</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a>
|
<a href=/people/h/hwanjo-yu/>Hwanjo Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--407><div class="card-body p-3 small">Academic paper search is an essential task for efficient literature discovery and scientific advancement. While dense retrieval has advanced various ad-hoc searches, it often struggles to match the underlying academic concepts between queries and documents, which is critical for paper search. To enable effective academic concept matching for paper search, we propose Taxonomy-guided Semantic Indexing (TaxoIndex) framework. TaxoIndex extracts key concepts from papers and organizes them as a semantic index guided by an academic taxonomy, and then leverages this index as foundational knowledge to identify academic concepts and link queries and documents. As a plug-and-play framework, TaxoIndex can be flexibly employed to enhance existing dense retrievers. Extensive experiments show that TaxoIndex brings significant improvements, even with highly limited training data, and greatly enhances interpretability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.408.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--408 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.408.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.408.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.408/>Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts</a></strong><br><a href=/people/x/xianzhen-luo/>Xianzhen Luo</a>
|
<a href=/people/q/qingfu-zhu/>Qingfu Zhu</a>
|
<a href=/people/z/zhiming-zhang/>Zhiming Zhang</a>
|
<a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/x/xuanyu-zhang/>Xuanyu Zhang</a>
|
<a href=/people/q/qing-yang/>Qing Yang</a>
|
<a href=/people/d/dongliang-xu/>Dongliang Xu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--408><div class="card-body p-3 small">Program of Thoughts (PoT) is an approach characterized by its executable intermediate steps, which ensure the accuracy of the logical calculations in the reasoning process. Currently, PoT primarily uses Python. However, relying solely on a single language may result in suboptimal solutions and overlook the potential benefits of other programming languages. In this paper, we conduct comprehensive experiments on the programming languages used in PoT and find that no single language consistently delivers optimal performance across all tasks and models. The effectiveness of each language varies depending on the specific scenarios. Inspired by this, we propose a task and model agnostic approach called MultiPoT, which harnesses strength and diversity from various languages. Experimental results reveal that it significantly outperforms Python Self-Consistency. Furthermore, it achieves comparable or superior performance compared to the best monolingual PoT in almost all tasks across all models. In particular, MultiPoT achieves more than 4.6% improvement on average on ChatGPT (gpt-3.5-turbo-0701).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.409.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--409 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.409.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.409/>Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models</a></strong><br><a href=/people/h/hongfu-liu/>Hongfu Liu</a>
|
<a href=/people/y/yuxi-xie/>Yuxi Xie</a>
|
<a href=/people/y/ye-wang/>Ye Wang</a>
|
<a href=/people/m/michael-shieh/>Michael Shieh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--409><div class="card-body p-3 small">Language Language Models (LLMs) face safety concerns due to potential misuse by malicious users. Recent red-teaming efforts have identified adversarial suffixes capable of jailbreaking LLMs using the gradient-based search algorithm Greedy Coordinate Gradient (GCG). However, GCG struggles with computational inefficiency, limiting further investigations regarding suffix transferability and scalability across models and data. In this work, we bridge the connection between search efficiency and suffix transferability. We propose a two-stage transfer learning framework, DeGCG, which decouples the search process into behavior-agnostic pre-searching and behavior-relevant post-searching. Specifically, we employ direct first target token optimization in pre-searching to facilitate the search process. We apply our approach to cross-model, cross-data, and self-transfer scenarios. Furthermore, we introduce an interleaved variant of our approach, i-DeGCG, which iteratively leverages self-transferability to accelerate the search process. Experiments on HarmBench demonstrate the efficiency of our approach across various models and domains. Notably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of 43.9 (<span class=tex-math>+ 22.2</span>) and 39.0 (<span class=tex-math>+19.5</span>) on valid and test sets, respectively. Further analysis on cross-model transfer indicates the pivotal role of first target token optimization in leveraging suffix transferability for efficient searching.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.410.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--410 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.410/>Incomplete Utterance Rewriting with Editing Operation Guidance and Utterance Augmentation</a></strong><br><a href=/people/z/zhiyu-cao/>Zhiyu Cao</a>
|
<a href=/people/p/peifeng-li/>Peifeng Li</a>
|
<a href=/people/y/yaxin-fan/>Yaxin Fan</a>
|
<a href=/people/q/qiaoming-zhu/>Qiaoming Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--410><div class="card-body p-3 small">Although existing fashionable generation methods on Incomplete Utterance Rewriting (IUR) can generate coherent utterances, they often result in the inclusion of irrelevant and redundant tokens in rewritten utterances due to their inability to focus on critical tokens in dialogue context. Furthermore, the limited size of the training datasets also contributes to the insufficient training of the IUR model. To address the first issue, we propose a multi-task learning framework EO-IUR (Editing Operation-guided Incomplete Utterance Rewriting) that introduces the editing operation labels generated by sequence labeling module to guide generation model to focus on critical tokens. Furthermore, we introduce a token-level heterogeneous graph to represent dialogues. To address the second issue, we propose a two-dimensional utterance augmentation strategy, namely editing operation-based incomplete utterance augmentation and LLM-based historical utterance augmentation. The experimental results on three datasets demonstrate that our EO-IUR outperforms previous state-of-the-art (SOTA) baselines in both open-domain and task-oriented dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.411.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--411 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.411/><span class=acl-fixed-case>FR</span>o<span class=acl-fixed-case>G</span>: Evaluating Fuzzy Reasoning of Generalized Quantifiers in <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/y/yiyuan-li/>Yiyuan Li</a>
|
<a href=/people/s/shichao-sun/>Shichao Sun</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--411><div class="card-body p-3 small">Fuzzy reasoning is vital due to the frequent use of imprecise information in daily contexts. However, the ability of current large language models (LLMs) to handle such reasoning remains largely uncharted. In this paper, we introduce a new benchmark, FRoG, for fuzzy reasoning, featuring real-world mathematical word problems that incorporate generalized quantifiers. Our experimental findings reveal that fuzzy reasoning continues to pose significant challenges for LLMs. Moreover, we find that existing methods designed to enhance reasoning do not consistently improve performance in tasks involving fuzzy logic. Additionally, our results show an inverse scaling effect in the performance of LLMs on FRoG. Interestingly, we also demonstrate that strong mathematical reasoning skills are not necessarily indicative of success on our benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.412.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--412 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.412/>Aligning Large Language Models with Diverse Political Viewpoints</a></strong><br><a href=/people/d/dominik-stammbach/>Dominik Stammbach</a>
|
<a href=/people/p/philine-widmer/>Philine Widmer</a>
|
<a href=/people/e/eunjung-cho/>Eunjung Cho</a>
|
<a href=/people/c/caglar-gulcehre/>Caglar Gulcehre</a>
|
<a href=/people/e/elliott-ash/>Elliott Ash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--412><div class="card-body p-3 small">Large language models such as ChatGPT exhibit striking political biases. If users query them about political information, they often take a normative stance. To overcome this, we align LLMs with diverse political viewpoints from 100,000 comments written by candidates running for national parliament in Switzerland. Models aligned with this data can generate more accurate political viewpoints from Swiss parties, compared to commercial models such as ChatGPT. We also propose a procedure to generate balanced overviews summarizing multiple viewpoints using such models. The replication package contains all code and data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.413.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--413 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.413/>“You Gotta be a Doctor, Lin” : An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations</a></strong><br><a href=/people/h/huy-nghiem/>Huy Nghiem</a>
|
<a href=/people/j/john-prindle/>John Prindle</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé Iii</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--413><div class="card-body p-3 small">Social science research has shown that candidates with names indicative of certain races or genders often face discrimination in employment practices. Similarly, Large Language Models (LLMs) have demonstrated racial and gender biases in various applications. In this study, we utilize GPT-3.5-Turbo and Llama 3-70B-Instruct to simulate hiring decisions and salary recommendations for candidates with 320 first names that strongly signal their race and gender, across over 750,000 prompts. Our empirical results indicate a preference among these models for hiring candidates with White female-sounding names over other demographic groups across 40 occupations. Additionally, even among candidates with identical qualifications, salary recommendations vary by as much as 5% between different subgroups. A comparison with real-world labor data reveals inconsistent alignment with U.S. labor market characteristics, underscoring the necessity of risk investigation of LLM-powered systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.414.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--414 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.414/>Extending Context Window of Large Language Models from a Distributional Perspective</a></strong><br><a href=/people/y/yingsheng-wu/>Yingsheng Wu</a>
|
<a href=/people/y/yuxuan-gu/>Yuxuan Gu</a>
|
<a href=/people/x/xiaocheng-feng/>Xiaocheng Feng</a>
|
<a href=/people/w/weihong-zhong/>Weihong Zhong</a>
|
<a href=/people/d/dongliang-xu/>Dongliang Xu</a>
|
<a href=/people/q/qing-yang/>Qing Yang</a>
|
<a href=/people/h/hongtao-liu/>Hongtao Liu</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--414><div class="card-body p-3 small">Scaling the rotary position embedding (RoPE) has become a common method for extending the context window of RoPE-based large language models (LLMs). However, existing scaling methods often rely on empirical approaches and lack a profound understanding of the internal distribution within RoPE, resulting in suboptimal performance in extending the context window length. In this paper, we propose to optimize the context window extending task from the view of rotary angle distribution. Specifically, we first estimate the distribution of the rotary angles within the model and analyze the extent to which length extension perturbs this distribution. Then, we present a novel extension strategy that minimizes the disturbance between rotary angle distributions to maintain consistency with the pre-training phase, enhancing the model’s capability to generalize to longer sequences. Experimental results compared to the strong baseline methods demonstrate that our approach reduces by up to 72% of the distributional disturbance when extending LLaMA2’s context window to 8k, and reduces by up to 32% when extending to 16k. On the LongBench-E benchmark, our method achieves an average improvement of up to 4.33% over existing state-of-the-art methods. Furthermore, Our method maintains the model’s performance on the Hugging Face Open LLM benchmark after context window extension, with only an average performance fluctuation ranging from -0.12 to +0.22.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.415.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--415 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.415/>Leveraging pre-trained language models for linguistic analysis: A case of argument structure constructions</a></strong><br><a href=/people/h/hakyung-sung/>Hakyung Sung</a>
|
<a href=/people/k/kristopher-kyle/>Kristopher Kyle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--415><div class="card-body p-3 small">This study evaluates the effectiveness of pre-trained language models in identifying argument structure constructions, important for modeling both first and second language learning. We examine three methodologies: (1) supervised training with RoBERTa using a gold-standard ASC treebank, including by-tag accuracy evaluation for sentences from both native and non-native English speakers, (2) prompt-guided annotation with GPT-4, and (3) generating training data through prompts with GPT-4, followed by RoBERTa training. Our findings indicate that RoBERTa trained on gold-standard data shows the best performance. While data generated through GPT-4 enhances training, it does not exceed the benchmarks set by gold-standard data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.416.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--416 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.416/><span class=acl-fixed-case>MA</span>g<span class=acl-fixed-case>IC</span>: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration</a></strong><br><a href=/people/l/lin-xu/>Lin Xu</a>
|
<a href=/people/z/zhiyuan-hu/>Zhiyuan Hu</a>
|
<a href=/people/d/daquan-zhou/>Daquan Zhou</a>
|
<a href=/people/h/hongyu-ren/>Hongyu Ren</a>
|
<a href=/people/z/zhen-dong/>Zhen Dong</a>
|
<a href=/people/k/kurt-keutzer/>Kurt Keutzer</a>
|
<a href=/people/s/see-kiong-ng/>See-Kiong Ng</a>
|
<a href=/people/j/jiashi-feng/>Jiashi Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--416><div class="card-body p-3 small">Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs’ reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality.We utilize two social deduction games alongside three game-theory scenarios to create diverse environments.Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs’ capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37%. Our data and code can be found here https://github.com/cathyxl/MAgIC.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.417.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--417 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.417/>Position Engineering: Boosting Large Language Models through Positional Information Manipulation</a></strong><br><a href=/people/z/zhiyuan-he/>Zhiyuan He</a>
|
<a href=/people/h/huiqiang-jiang/>Huiqiang Jiang</a>
|
<a href=/people/z/zilong-wang/>Zilong Wang</a>
|
<a href=/people/y/yuqing-yang/>Yuqing Yang</a>
|
<a href=/people/l/luna-k-qiu/>Luna K. Qiu</a>
|
<a href=/people/l/lili-qiu/>Lili Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--417><div class="card-body p-3 small">The performance of large language models (LLMs) is significantly influenced by the quality of the prompts provided. In response, researchers have developed enormous prompt engineering strategies aimed at modifying the prompt text to enhance task performance. In this paper, we introduce a novel technique termed position engineering, which offers a more efficient way to guide large language models. Unlike prompt engineering, which requires substantial effort to modify the text provided to LLMs, position engineering merely involves altering the positional information in the prompt without modifying the text itself. We have evaluated position engineering in two widely-used LLM scenarios: retrieval-augmented generation (RAG) and in-context learning (ICL). Our findings show that position engineering substantially improves upon the baseline in both cases. Position engineering thus represents a promising new strategy for exploiting the capabilities of large language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.418.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--418 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.418.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.418/>Towards Injecting Medical Visual Knowledge into Multimodal <span class=acl-fixed-case>LLM</span>s at Scale</a></strong><br><a href=/people/j/junying-chen/>Junying Chen</a>
|
<a href=/people/c/chi-gui/>Chi Gui</a>
|
<a href=/people/r/ruyi-ouyang/>Ruyi Ouyang</a>
|
<a href=/people/a/anningzhe-gao/>Anningzhe Gao</a>
|
<a href=/people/s/shunian-chen/>Shunian Chen</a>
|
<a href=/people/g/guiming-hardy-chen/>Guiming Hardy Chen</a>
|
<a href=/people/x/xidong-wang/>Xidong Wang</a>
|
<a href=/people/z/zhenyang-cai/>Zhenyang Cai</a>
|
<a href=/people/k/ke-ji/>Ke Ji</a>
|
<a href=/people/x/xiang-wan/>Xiang Wan</a>
|
<a href=/people/b/benyou-wang/>Benyou Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--418><div class="card-body p-3 small">The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements. However, these models still face challenges in medical multimodal capabilities due to limitations in the quantity and quality of medical vision-text data, stemming from data privacy concerns and high annotation costs. While pioneering approaches utilize PubMed’s large-scale, de-identified medical image-text pairs to address these limitations, they often fall short due to inherent data noise. To tackle this, we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in an ‘unblinded’ capacity to denoise and reformat the data, resulting in the creation of the **PubMedVision** dataset with 1.3 million medical VQA samples. Our validation demonstrates that: (1) PubMedVision can significantly enhance the medical multimodal capabilities of MLLMs, showing significant improvement in benchmarks including the MMMU Health & Medicine track; (2) manual checks by medical experts and empirical results validate the superior data quality of our dataset compared to other data construction methods. Using PubMedVision, we train a 34B medical MLLM **HuatuoGPT-Vision**, which shows superior performance in medical multimodal scenarios among open-source MLLMs. Our code and data are available at https://github.com/FreedomIntelligence/HuatuoGPT-Vision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.419.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.419.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.419/><span class=acl-fixed-case>ADELIE</span>: Aligning Large Language Models on Information Extraction</a></strong><br><a href=/people/y/yunjia-qi/>Yunjia Qi</a>
|
<a href=/people/h/hao-peng/>Hao Peng</a>
|
<a href=/people/x/xiaozhi-wang/>Xiaozhi Wang</a>
|
<a href=/people/b/bin-xu/>Bin Xu</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.420.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.420/>Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons</a></strong><br><a href=/people/y/yifei-wang/>Yifei Wang</a>
|
<a href=/people/y/yuheng-chen/>Yuheng Chen</a>
|
<a href=/people/w/wanting-wen/>Wanting Wen</a>
|
<a href=/people/y/yu-sheng/>Yu Sheng</a>
|
<a href=/people/l/linjing-li/>Linjing Li</a>
|
<a href=/people/d/daniel-dajun-zeng/>Daniel Dajun Zeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--420><div class="card-body p-3 small">In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks. Through an analysis of LLMs’ internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances. Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions. By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation. Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs. Code and data will be available soon.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.421.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--421 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.421/>Lexically Grounded Subword Segmentation</a></strong><br><a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/j/jindrich-helcl/>Jindřich Helcl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--421><div class="card-body p-3 small">We present three innovations in tokenization and subword segmentation. First, we propose to use unsupervised morphological analysis with Morfessor as pre-tokenization. Second, we present an algebraic method for obtaining subword embeddings grounded in a word embedding space. Based on that, we design a novel subword segmentation algorithm that uses the embeddings, ensuring that the procedure considers lexical meaning. Third, we introduce an efficient segmentation algorithm based on a subword bigram model that can be initialized with the lexically aware segmentation method to avoid using Morfessor and large embedding tables at inference time. We evaluate the proposed approaches using two intrinsic metrics and measure their performance on two downstream tasks: part-of-speech tagging and machine translation. Our experiments show significant improvements in the morphological plausibility of the segmentation when evaluated using segmentation precision on morpheme boundaries and improved Rényi efficiency in 8 languages. Although the proposed tokenization methods do not have a large impact on automatic translation quality, we observe consistent performance gains in the arguably more morphological task of part-of-speech tagging.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.422.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--422 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.422.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.422/><span class=acl-fixed-case>EAGLE</span>-2: Faster Inference of Language Models with Dynamic Draft Trees</a></strong><br><a href=/people/y/yuhui-li/>Yuhui Li</a>
|
<a href=/people/f/fangyun-wei/>Fangyun Wei</a>
|
<a href=/people/c/chao-zhang-tu/>Chao Zhang</a>
|
<a href=/people/h/hongyang-zhang/>Hongyang Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--422><div class="card-body p-3 small">Inference with modern Large Language Models (LLMs) is expensive and time-consuming, and speculative sampling has proven to be an effective solution. Most speculative sampling methods such as EAGLE use a static draft tree, implicitly assuming that the acceptance rate of draft tokens depends only on their position. Interestingly, we found that the acceptance rate of draft tokens is also context-dependent. In this paper, building upon EAGLE, we propose EAGLE-2, which introduces a new technique of context-aware dynamic draft tree into drafting modeling. This improvement leverages the fact that the draft model of EAGLE is well-calibrated: the confidence scores from the draft model approximate acceptance rates with small errors. We conducted extensive evaluations on three series of LLMs and six tasks, with EAGLE-2 achieving speedup ratios of up to **5x**, which is 1.3x that of EAGLE. EAGLE-2 also ensures that the distribution of the generated text remains unchanged, making it a **lossless** acceleration algorithm.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.423.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.423.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--423 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.423 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.423.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.423/>Do Text-to-Vis Benchmarks Test Real Use of Visualisations?</a></strong><br><a href=/people/h/hy-nguyen/>Hy Nguyen</a>
|
<a href=/people/x/xuefei-he/>Xuefei He</a>
|
<a href=/people/a/andrew-reeson/>Andrew Reeson</a>
|
<a href=/people/c/cecile-paris/>Cecile Paris</a>
|
<a href=/people/j/josiah-poon/>Josiah Poon</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--423><div class="card-body p-3 small">Large language models are able to generate code for visualisations in response to simple user requests.This is a useful application and an appealing one for NLP research because plots of data provide grounding for language.However, there are relatively few benchmarks, and those that exist may not be representative of what users do in practice.This paper investigates whether benchmarks reflect real-world use through an empirical study comparing benchmark datasets with code from public repositories.Our findings reveal a substantial gap, with evaluations not testing the same distribution of chart types, attributes, and actions as real-world examples.One dataset is representative, but requires extensive modification to become a practical end-to-end benchmark. This shows that new benchmarks are needed to support the development of systems that truly address users’ visualisation needs.These observations will guide future data creation, highlighting which features hold genuine significance for users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.424.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--424 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.424/>Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/c/chengyuan-liu/>Chengyuan Liu</a>
|
<a href=/people/s/shihang-wang/>Shihang Wang</a>
|
<a href=/people/l/lizhi-qing/>Lizhi Qing</a>
|
<a href=/people/k/kun-kuang/>Kun Kuang</a>
|
<a href=/people/y/yangyang-kang/>Yangyang Kang</a>
|
<a href=/people/c/changlong-sun/>Changlong Sun</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--424><div class="card-body p-3 small">While Large Language Models (LLMs) demonstrate impressive generation abilities, they frequently struggle when it comes to specialized domains due to their limited domain-specific knowledge. Studies on domain-specific LLMs resort to expanding the vocabulary before fine-tuning on domain-specific corpus, aiming to decrease the sequence length and enhance efficiency during decoding, without thoroughly investigating the results of vocabulary expansion to LLMs over different domains. Our pilot study reveals that expansion with only a subset of the entire vocabulary may lead to superior performance. Guided by the discovery, this paper explores how to identify a vocabulary subset to achieve the optimal results. We introduce VEGAD, an adaptive method that automatically identifies valuable words from a given domain vocabulary. Our method has been validated through experiments on three Chinese datasets, demonstrating its effectiveness. Additionally, we have undertaken comprehensive analyses of the method. The selection of a optimal subset for expansion has shown to enhance performance on both domain-specific tasks and general tasks, showcasing the potential of VEGAD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.425.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--425 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.425/>Strategic Demonstration Selection for Improved Fairness in <span class=acl-fixed-case>LLM</span> In-Context Learning</a></strong><br><a href=/people/j/jingyu-hu/>Jingyu Hu</a>
|
<a href=/people/w/weiru-liu/>Weiru Liu</a>
|
<a href=/people/m/mengnan-du/>Mengnan Du</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--425><div class="card-body p-3 small">Recent studies highlight the effectiveness of using in-context learning (ICL) to steer large language models (LLMs) in processing tabular data, a challenging task given the structured nature of such data. Despite advancements in performance, the fairness implications of these methods are less understood. This study investigates how varying demonstrations within ICL prompts influence the fairness outcomes of LLMs. Our findings reveal that deliberately including minority group samples in prompts significantly boosts fairness without sacrificing predictive accuracy. Further experiments demonstrate that the proportion of minority to majority samples in demonstrations affects the trade-off between fairness and prediction accuracy. Based on these insights, we introduce a mitigation technique that employs clustering and evolutionary strategies to curate a diverse and representative sample set from the training data. This approach aims to enhance both predictive performance and fairness in ICL applications. Experimental results validate that our proposed method dramatically improves fairness across various metrics, showing its efficacy in real-world scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.426.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--426 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.426/>Multi-Dialect <span class=acl-fixed-case>V</span>ietnamese: Task, Dataset, Baseline Models and Challenges</a></strong><br><a href=/people/n/nguyen-van-dinh/>Nguyen Van Dinh</a>
|
<a href=/people/t/thanh-chi-dang/>Thanh Chi Dang</a>
|
<a href=/people/l/luan-thanh-nguyen/>Luan Thanh Nguyen</a>
|
<a href=/people/k/kiet-van-nguyen/>Kiet Van Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--426><div class="card-body p-3 small">Vietnamese, a low-resource language, is typically categorized into three primary dialect groups that belong to Northern, Central, and Southern Vietnam. However, each province within these regions exhibits its own distinct pronunciation variations. Despite the existence of various speech recognition datasets, none of them has provided a fine-grained classification of the 63 dialects specific to individual provinces of Vietnam. To address this gap, we introduce Vietnamese Multi-Dialect (ViMD) dataset, a novel comprehensive dataset capturing the rich diversity of 63 provincial dialects spoken across Vietnam. Our dataset comprises 102.56 hours of audio, consisting of approximately 19,000 utterances, and the associated transcripts contain over 1.2 million words. To provide benchmarks and simultaneously demonstrate the challenges of our dataset, we fine-tune state-of-the-art pre-trained models for two downstream tasks: (1) Dialect identification and (2) Speech recognition. The empirical results suggest two implications including the influence of geographical factors on dialects, and the constraints of current approaches in speech recognition tasks involving multi-dialect speech data. Our dataset is available for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.427.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--427 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.427.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.427/>Is <span class=acl-fixed-case>LLM</span>-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot <span class=acl-fixed-case>LLM</span> Assessment</a></strong><br><a href=/people/v/vyas-raina/>Vyas Raina</a>
|
<a href=/people/a/adian-liusie/>Adian Liusie</a>
|
<a href=/people/m/mark-gales/>Mark Gales</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--427><div class="card-body p-3 small">Large Language Models (LLMs) are powerful zero-shot assessors used in real-world situations such as assessing written exams and benchmarking systems. Despite these critical applications, no existing work has analyzed the vulnerability of judge-LLMs to adversarial manipulation. This work presents the first study on the adversarial robustness of assessment LLMs, where we demonstrate that short universal adversarial phrases can be concatenated to deceive judge LLMs to predict inflated scores. Since adversaries may not know or have access to the judge-LLMs, we propose a simple surrogate attack where a surrogate model is first attacked, and the learned attack phrase then transferred to unknown judge-LLMs. We propose a practical algorithm to determine the short universal attack phrases and demonstrate that when transferred to unseen models, scores can be drastically inflated such that irrespective of the assessed text, maximum scores are predicted. It is found that judge-LLMs are significantly more susceptible to these adversarial attacks when used for absolute scoring, as opposed to comparative assessment. Our findings raise concerns on the reliability of LLM-as-a-judge methods, and emphasize the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.428.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--428 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.428.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.428.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.428/>Rethinking the Reversal Curse of <span class=acl-fixed-case>LLM</span>s: a Prescription from Human Knowledge Reversal</a></strong><br><a href=/people/z/zhicong-lu/>Zhicong Lu</a>
|
<a href=/people/l/li-jin/>Li Jin</a>
|
<a href=/people/p/peiguang-li/>Peiguang Li</a>
|
<a href=/people/y/yu-tian/>Yu Tian</a>
|
<a href=/people/l/linhao-zhang/>Linhao Zhang</a>
|
<a href=/people/s/sirui-wang/>Sirui Wang</a>
|
<a href=/people/g/guangluan-xu/>Guangluan Xu</a>
|
<a href=/people/c/changyuan-tian/>Changyuan Tian</a>
|
<a href=/people/x/xunliang-cai/>Xunliang Cai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--428><div class="card-body p-3 small">Large Language Models (LLMs) have exhibited exceptional performance across diverse domains. However, recent studies reveal that LLMs are plagued by the “reversal curse”. Most existing methods rely on aggressive sample permutation and pay little attention to delving into the underlying reasons for this issue, resulting in only partial mitigation. In this paper, inspired by human knowledge reversal, we investigate and quantify the individual influence of three potential reasons on the reversal curse: 1) knowledge clarity, 2) entity correlation modeling, and 3) pairwise relationship reasoning capability. Motivated by the analysis of these reasons, we propose a novel **P**airwise entity **O**rder- and **R**elationship-**E**nhanced (**PORE**) data strategy, which facilitates bidirectional entity correlation modeling and pairwise relationship reasoning to overcome the reversal curse. Specifically, PORE augments the samples with entity order-reversal and semantically preserved question-answer pairs, enhancing the encoding of entity correlations in both directions. PORE also employs entity-interleaved pairwise relationship data, which elevates the model’s capability for relationship reasoning. Additionally, to improve the recall of reverse relationships, we leverage knowledge clarity to construct high-clarity data for PORE. Extensive experimental results on available and two newly assembled datasets demonstrate the effectiveness and generalization of our method in both data-sufficient and -constrained situations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.429.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--429 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.429/>More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/c/chengyuan-liu/>Chengyuan Liu</a>
|
<a href=/people/y/yangyang-kang/>Yangyang Kang</a>
|
<a href=/people/s/shihang-wang/>Shihang Wang</a>
|
<a href=/people/l/lizhi-qing/>Lizhi Qing</a>
|
<a href=/people/f/fubang-zhao/>Fubang Zhao</a>
|
<a href=/people/c/chao-wu/>Chao Wu</a>
|
<a href=/people/c/changlong-sun/>Changlong Sun</a>
|
<a href=/people/k/kun-kuang/>Kun Kuang</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--429><div class="card-body p-3 small">The performance on general tasks decreases after Large Language Models (LLMs) are fine-tuned on domain-specific tasks, the phenomenon is known as Catastrophic Forgetting (CF). However, this paper presents a further challenge for real application of domain-specific LLMs beyond CF, called General Capabilities Integration (GCI), which necessitates the integration of both the general capabilities and domain knowledge within a single instance. The objective of GCI is not merely to retain previously acquired general capabilities alongside new domain knowledge, but to harmonize and utilize both sets of skills in a cohesive manner to enhance performance on domain-specific tasks. Taking legal domain as an example, we carefully design three groups of training and testing tasks without lacking practicability, and construct the corresponding datasets. To better incorporate general capabilities across domain-specific scenarios, we introduce ALoRA, which utilizes a multi-head attention module upon LoRA, facilitating direct information transfer from preceding tokens to the current one. This enhancement permits the representation to dynamically switch between domain-specific knowledge and general competencies according to the attention. Extensive experiments are conducted on the proposed tasks. The results exhibit the significance of our setting, and the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.430.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.430.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.430.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.430/>Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models</a></strong><br><a href=/people/v/vyas-raina/>Vyas Raina</a>
|
<a href=/people/r/rao-ma/>Rao Ma</a>
|
<a href=/people/c/charles-mcghee/>Charles McGhee</a>
|
<a href=/people/k/kate-knill/>Kate Knill</a>
|
<a href=/people/m/mark-gales/>Mark Gales</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.431.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--431 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.431/><span class=acl-fixed-case>GENRA</span>: Enhancing Zero-shot Retrieval with Rank Aggregation</a></strong><br><a href=/people/g/georgios-katsimpras/>Georgios Katsimpras</a>
|
<a href=/people/g/georgios-paliouras/>Georgios Paliouras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--431><div class="card-body p-3 small">Large Language Models (LLMs) have been shown to effectively perform zero-shot document retrieval, a process that typically consists of two steps: i) retrieving relevant documents, and ii) re-ranking them based on their relevance to the query. This paper presents GENRA, a new approach to zero-shot document retrieval that incorporates rank aggregation to improve retrieval effectiveness. Given a query, GENRA first utilizes LLMs to generate informative passages that capture the query’s intent. These passages are then employed to guide the retrieval process, selecting similar documents from the corpus. Next, we use LLMs again for a second refinement step. This step can be configured for either direct relevance assessment of each retrieved document or for re-ranking the retrieved documents. Ultimately, both approaches ensure that only the most relevant documents are kept. Upon this filtered set of documents, we perform multi-document retrieval, generating individual rankings for each document. As a final step, GENRA leverages rank aggregation, combining the individual rankings to produce a single refined ranking. Extensive experiments on benchmark datasets demonstrate that GENRA improves existing approaches, highlighting the effectiveness of the proposed methodology in zero-shot retrieval.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.432.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--432 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.432/><span class=acl-fixed-case>X</span>plain<span class=acl-fixed-case>LLM</span>: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/z/zichen-chen/>Zichen Chen</a>
|
<a href=/people/j/jianda-chen/>Jianda Chen</a>
|
<a href=/people/a/ambuj-singh/>Ambuj Singh</a>
|
<a href=/people/m/misha-sra/>Misha Sra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--432><div class="card-body p-3 small">Large Language Models (LLMs) have achieved remarkable success in natural language tasks, yet understanding their reasoning processes remains a significant challenge. We address this by introducing XplainLLM, a dataset accompanying an explanation framework designed to enhance LLM transparency and reliability. Our dataset comprises 24,204 instances where each instance interprets the LLM’s reasoning behavior using knowledge graphs (KGs) and graph attention networks (GAT), and includes explanations of LLMs such as the decoder-only Llama-3 and the encoder-only RoBERTa. XplainLLM also features a framework for generating grounded explanations and the <i>debugger-scores</i> for multidimensional quality analysis. Our explanations include <i>why-choose</i> and <i>why-not-choose</i> components, <i>reason-elements</i>, and <i>debugger-scores</i> that collectively illuminate the LLM’s reasoning behavior. Our evaluations demonstrate XplainLLM’s potential to reduce hallucinations and improve grounded explanation generation in LLMs. XplainLLM is a resource for researchers and practitioners to build trust and verify the reliability of LLM outputs. Our code and dataset are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.433.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--433 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.433/>Divide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning</a></strong><br><a href=/people/y/yuanpin-zhou/>Yuanpin Zhou</a>
|
<a href=/people/h/huogen-wang/>Huogen Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--433><div class="card-body p-3 small">The automation of radiology report generation (RRG) holds immense potential to alleviate radiologists’ workloads and improve diagnostic accuracy. Despite advancements in image captioning and vision-language pretraining, RRG remains challenging due to the lengthy and complex nature of radiology reports. In this work, we proposes the Divide and Conquer Radiology Report Generation (DCRRG) model, which breaks down full-text radiology reports into concise observation descriptions. This approach enables the model to capture fine-grained representations from each observation through a two-stage process: an encoding stage focusing on observation prediction tasks to learn fine-grained representations, and a decoding stage for integrating these descriptions into cohesive and comprehensive radiology reports. Experimental results on two benchmark datasets demonstrate that DCRRG achieves significant improvements across all evaluated metrics, underscoring its capability to generate semantically coherent and clinically accurate radiology reports.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.434.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--434 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.434/><span class=acl-fixed-case>SUR</span>f: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information</a></strong><br><a href=/people/j/jiashuo-sun/>Jiashuo Sun</a>
|
<a href=/people/j/jihai-zhang/>Jihai Zhang</a>
|
<a href=/people/y/yucheng-zhou/>Yucheng Zhou</a>
|
<a href=/people/z/zhaochen-su/>Zhaochen Su</a>
|
<a href=/people/x/xiaoye-qu/>Xiaoye Qu</a>
|
<a href=/people/y/yu-cheng/>Yu Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--434><div class="card-body p-3 small">Large Vision-Language Models (LVLMs) have become pivotal at the intersection of computer vision and natural language processing. However, the full potential of LVLMs’ Retrieval-Augmented Generation (RAG) capabilities remains underutilized. Existing works either focus solely on the text modality or are limited to specific tasks. Moreover, most LVLMs struggle to selectively utilize retrieved information and are sensitive to irrelevant or misleading references. To address these challenges, we propose a self-refinement framework designed to teach LVLMs to <b>S</b>electively <b>U</b>tilize <b>R</b>etrieved In<b>f</b>ormation (SURf). Specifically, when given questions that are incorrectly answered by the LVLM backbone, we obtain references that help correct the answers (positive references) and those that do not (negative references). We then fine-tune the LVLM backbone using a combination of these positive and negative references. Our experiments across three tasks and seven datasets demonstrate that our framework significantly enhances LVLMs’ ability to effectively utilize retrieved multimodal references and improves their robustness against irrelevant or misleading information. The source code is available at https://anonymous.4open.science/r/SURf-6433.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.435.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.435.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--435 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.435 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.435/><span class=acl-fixed-case>UNO</span> Arena for Evaluating Sequential Decision-Making Capability of Large Language Models</a></strong><br><a href=/people/z/zhanyue-qin/>Zhanyue Qin</a>
|
<a href=/people/h/haochuan-wang/>Haochuan Wang</a>
|
<a href=/people/d/deyuan-liu/>Deyuan Liu</a>
|
<a href=/people/z/ziyang-song/>Ziyang Song</a>
|
<a href=/people/c/cunhang-fan/>Cunhang Fan</a>
|
<a href=/people/z/zhao-lv/>Zhao Lv</a>
|
<a href=/people/j/jinlin-wu/>Jinlin Wu</a>
|
<a href=/people/z/zhen-lei/>Zhen Lei</a>
|
<a href=/people/z/zhiying-tu/>Zhiying Tu</a>
|
<a href=/people/d/dianhui-chu/>Dianhui Chu</a>
|
<a href=/people/x/xiaoyan-yu/>Xiaoyan Yu</a>
|
<a href=/people/d/dianbo-sui/>Dianbo Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--435><div class="card-body p-3 small">Sequential decision-making refers to algorithms that take into account the dynamics of the environment, where early decisions affect subsequent decisions. With large language models (LLMs) demonstrating powerful capabilities between tasks, we can’t help but ask: Can Current LLMs Effectively Make Sequential Decisions? In order to answer this question, we propose the UNO Arena based on the card game UNO to evaluate the sequential decision-making capability of LLMs and explain in detail why we choose UNO. In UNO Arena, We evaluate the sequential decision-making capability of LLMs dynamically with novel metrics based Monte Carlo methods. We set up random players, DQN-based reinforcement learning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison testing. Furthermore, in order to improve the sequential decision-making capability of LLMs, we propose the TUTRI player, which can involves having LLMs reflect their own actions with the summary of game history and the game strategy. Numerous experiments demonstrate that the TUTRI player achieves a notable breakthrough in the performance of sequential decision-making compared to the vanilla LLM player.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.436.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--436 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.436.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.436.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.436/>Middleware for <span class=acl-fixed-case>LLM</span>s: Tools Are Instrumental for Language Agents in Complex Environments</a></strong><br><a href=/people/y/yu-gu/>Yu Gu</a>
|
<a href=/people/y/yiheng-shu/>Yiheng Shu</a>
|
<a href=/people/h/hao-yu/>Hao Yu</a>
|
<a href=/people/x/xiao-liu/>Xiao Liu</a>
|
<a href=/people/y/yuxiao-dong/>Yuxiao Dong</a>
|
<a href=/people/j/jie-tang/>Jie Tang</a>
|
<a href=/people/j/jayanth-srinivasa/>Jayanth Srinivasa</a>
|
<a href=/people/h/hugo-latapie/>Hugo Latapie</a>
|
<a href=/people/y/yu-su/>Yu Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--436><div class="card-body p-3 small">The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist agents capable of operating within complex environments. These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory. Motivated by recent research on extending the capabilities of LLMs with tools, we seek to investigate the intriguing potential of tools to augment LLMs in handling such complexity by introducing a novel class of tools, termed *middleware*, to aid in the proactive exploration within these massive environments. Such specialized tools can serve as a middleware layer shielding the LLM from environmental complexity. In two representative complex environments—knowledge bases (KBs) and databases—we demonstrate the significant potential of augmenting language agents with tools in complex environments. Notably, equipped with the middleware, GPT-4 achieves **2.8**X the performance of the best baseline in tasks requiring access to database content and **2.2**X in KB tasks. Our findings illuminate the path for advancing language agents in real-world applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.437.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--437 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.437.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.437.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.437/><span class=acl-fixed-case>MORPHEUS</span>: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space</a></strong><br><a href=/people/y/yihong-tang/>Yihong Tang</a>
|
<a href=/people/b/bo-wang/>Bo Wang</a>
|
<a href=/people/d/dongming-zhao/>Dongming Zhao</a>
|
<a href=/people/j/jinxiaojia-jinxiaojia/>Jinxiaojia Jinxiaojia</a>
|
<a href=/people/z/zhangjijun-zhangjijun/>Zhangjijun Zhangjijun</a>
|
<a href=/people/r/ruifang-he/>Ruifang He</a>
|
<a href=/people/y/yuexian-hou/>Yuexian Hou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--437><div class="card-body p-3 small">Personalized Dialogue Generation (PDG) aims to create coherent responses according to roles or personas. Traditional PDG relies on external role data, which can be scarce and raise privacy concerns. Approaches address these issues by extracting role information from dialogue history, which often fail to generically model roles in continuous space. To overcome these limitations, we introduce a novel framework Models Roles from Personalized Dialogue History by Exploring and Utilizing Latent Space (MORPHEUS) through a three-stage training process. Specifically, we create a persona codebook to represent roles in latent space compactly, and this codebook is used to construct a posterior distribution of role information. This method enables the model to generalize across roles, allowing the generation of personalized dialogues even for unseen roles. Experiments on both Chinese and English datasets demonstrate that MORPHEUS enhances the extraction of role information, and improves response generation without external role data. Additionally, MORPHEUS can be considered an efficient fine-tuning for large language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.438.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--438 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.438/><span class=acl-fixed-case>K</span>nowledge<span class=acl-fixed-case>SG</span>: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server</a></strong><br><a href=/people/w/wenhao-wang/>WenHao Wang</a>
|
<a href=/people/x/xiaoyu-liang/>Xiaoyu Liang</a>
|
<a href=/people/r/rui-ye/>Rui Ye</a>
|
<a href=/people/j/jingyi-chai/>Jingyi Chai</a>
|
<a href=/people/s/siheng-chen/>Siheng Chen</a>
|
<a href=/people/y/yanfeng-wang/>Yanfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--438><div class="card-body p-3 small">The success of large language models (LLMs) facilitate many parties to fine-tune LLMs on their own private data. However, this practice raises privacy concerns due to the memorization of LLMs. Existing solutions, such as utilizing synthetic data for substitution, struggle to simultaneously improve performance and preserve privacy.They either rely on a local model for generation, resulting in a performance decline, or take advantage of APIs, directly exposing the data to API servers. To address this issue, we propose <i>KnowledgeSG</i>, a novel client-server framework which enhances synthetic data quality and improves model performance while ensuring privacy. We achieve this by learning local knowledge from the private data with differential privacy (DP) and distilling professional knowledge from the server. Additionally, inspired by federated learning, we transmit models rather than data between the client and server to prevent privacy leakage.Extensive experiments in medical and financial domains demonstrate the effectiveness of *KnowledgeSG*. Our code is now publicly available at https://github.com/wwh0411/KnowledgeSG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.439.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--439 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.439/><span class=acl-fixed-case>DAMRO</span>: Dive into the Attention Mechanism of <span class=acl-fixed-case>LVLM</span> to Reduce Object Hallucination</a></strong><br><a href=/people/x/xuan-gong/>Xuan Gong</a>
|
<a href=/people/t/tianshi-ming/>Tianshi Ming</a>
|
<a href=/people/x/xinpeng-wang/>Xinpeng Wang</a>
|
<a href=/people/z/zhihua-wei/>Zhihua Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--439><div class="card-body p-3 small">Despite the great success of Large Vision-Language Models (LVLMs), they inevitably suffer from hallucination. As we know, both the visual encoder and the Large Language Model (LLM) decoder in LVLMs are Transformer-based, allowing the model to extract visual information and generate text outputs via attention mechanisms. We find that the attention distribution of LLM decoder on image tokens is highly consistent with the visual encoder and both distributions tend to focus on particular background tokens rather than the referred objects in the image. We attribute to the unexpected attention distribution to an inherent flaw in the visual encoder itself, which misguides LLMs to over emphasize the redundant information and generate object hallucination. To address the issue, we propose DAMRO, a novel training-free strategy that **D**ive into **A**ttention **M**echanism of LVLM to **R**educe **O**bject Hallucination. Specifically, our approach employs classification token (CLS) of ViT to filter out high-attention tokens scattered in the background and then eliminate their influence during decoding stage. We evaluate our method on LVLMs including LLaVA-1.5, LLaVA-NeXT and InstructBLIP, using various benchmarks such as POPE, CHAIR, MME and GPT-4V Aided Evaluation. The results demonstrate that our approach significantly reduces the impact of these outlier tokens, thus effectively alleviating the hallucination of LVLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.440.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.440.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--440 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.440 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.440/>Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models</a></strong><br><a href=/people/t/tianyi-men/>Tianyi Men</a>
|
<a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/z/zhuoran-jin/>Zhuoran Jin</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--440><div class="card-body p-3 small">Planning, as the core module of agents, is crucial in various fields such as embodied agents, web navigation, and tool using. With the development of large language models (LLMs), some researchers treat large language models as intelligent agents to stimulate and evaluate their planning capabilities. However, the planning mechanism is still unclear. In this work, we focus on exploring the look-ahead planning mechanism in large language models from the perspectives of information flow and internal representations. First, we study how planning is done internally by analyzing the multi-layer perception (MLP) and multi-head self-attention (MHSA) components at the last token. We find that the output of MHSA in the middle layers at the last token can directly decode the decision to some extent. Based on this discovery, we further trace the source of MHSA by information flow, and we reveal that MHSA extracts information from spans of the goal states and recent steps. According to information flow, we continue to study what information is encoded within it. Specifically, we explore whether future decisions have been considered in advance in the representation of flow. We demonstrate that the middle and upper layers encode a few short-term future decisions. Overall, our research analyzes the look-ahead planning mechanisms of LLMs, facilitating future research on LLMs performing planning tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.441.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--441 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.441/>Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale</a></strong><br><a href=/people/w/wenzhen-zheng/>Wenzhen Zheng</a>
|
<a href=/people/w/wenbo-pan/>Wenbo Pan</a>
|
<a href=/people/x/xu-xu/>Xu Xu</a>
|
<a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/l/li-yue/>Li Yue</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--441><div class="card-body p-3 small">In recent years, Large Language Models (LLMs) have made significant strides towards Artificial General Intelligence. However, training these models from scratch requires substantial computational resources and vast amounts of text data. In this paper, we explores an alternative approach to constructing a LLM for a new language by continually pre-training (CPT) from existing pre-trained LLMs, instead of using randomly initialized parameters. Based on parallel experiments on 40 model sizes ranging from 40M to 5B parameters, we find that 1) CPT converges faster and saves significant resources in a scalable manner. 2) CPT adheres to an extended scaling law derived from with a joint data-parameter scaling term. 3) The compute-optimal data-parameter allocation for CPT markedly differs based on our estimated scaling factors. 4) The effectiveness of transfer scale is influenced by training duration and linguistic properties, while robust to data replaying, a method that effectively mitigates catastrophic forgetting in CPT. We hope our findings provide deeper insights into the transferability of LLMs at scale for the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.442.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--442 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.442/>An Empirical Study of Multilingual Reasoning Distillation for Question Answering</a></strong><br><a href=/people/p/patomporn-payoungkhamdee/>Patomporn Payoungkhamdee</a>
|
<a href=/people/p/peerat-limkonchotiwat/>Peerat Limkonchotiwat</a>
|
<a href=/people/j/jinheon-baek/>Jinheon Baek</a>
|
<a href=/people/p/potsawee-manakul/>Potsawee Manakul</a>
|
<a href=/people/c/can-udomcharoenchaikit/>Can Udomcharoenchaikit</a>
|
<a href=/people/e/ekapol-chuangsuwanich/>Ekapol Chuangsuwanich</a>
|
<a href=/people/s/sarana-nutanong/>Sarana Nutanong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--442><div class="card-body p-3 small">Reasoning is one crucial capability in Large Language Models (LLMs), allowing them to perform complex tasks such as solving math problems and multi-step planning. While reasoning capability can emerge in larger models, smaller ones usually have to rely on distillation to transfer this capability from a larger model. However, recent efforts to distill reasoning capabilities have focused mainly on English, leaving multilingual distillation underexplored. To address this gap, this paper examines existing English reasoning distillation methods that utilize a variety of positive rationales in multilingual settings and proposes d-CoT-nR, a novel approach that incorporates incorrect rationales as additional guidance. Empirical results from multilingual high-school examinations show that d-CoT-nR significantly surpasses the baseline, improving accuracy in unseen languages and correctness in step-by-step reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.443.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.443.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--443 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.443 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.443/>Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?</a></strong><br><a href=/people/g/gal-yona/>Gal Yona</a>
|
<a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/m/mor-geva/>Mor Geva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--443><div class="card-body p-3 small">We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response should reflect this uncertainty by hedging its answer (e.g., “I’m not sure, but I think...”). We formalize faithful response uncertainty based on the gap between the model’s intrinsic confidence in the assertions it makes and the decisiveness by which they are conveyed. This example-level metric reliably indicates whether the model reflects its uncertainty, as it penalizes both excessive and insufficient hedging. We evaluate a variety of aligned LLMs at faithfully conveying uncertainty on several knowledge-intensive question answering tasks. Our results provide strong evidence that modern LLMs are poor at faithfully conveying their uncertainty, and that better alignment is necessary to improve their trustworthiness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.444.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.444.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--444 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.444 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.444/>Does Fine-Tuning <span class=acl-fixed-case>LLM</span>s on New Knowledge Encourage Hallucinations?</a></strong><br><a href=/people/z/zorik-gekhman/>Zorik Gekhman</a>
|
<a href=/people/g/gal-yona/>Gal Yona</a>
|
<a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/m/matan-eyal/>Matan Eyal</a>
|
<a href=/people/a/amir-feder/>Amir Feder</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/j/jonathan-herzig/>Jonathan Herzig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--444><div class="card-body p-3 small">When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model’s knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model’s tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.445.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--445 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.445/>Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning</a></strong><br><a href=/people/m/ming-shan-hee/>Ming Shan Hee</a>
|
<a href=/people/a/aditi-kumaresan/>Aditi Kumaresan</a>
|
<a href=/people/r/roy-ka-wei-lee/>Roy Ka-Wei Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--445><div class="card-body p-3 small">The widespread presence of hate speech on the internet, including formats such as text-based tweets and multimodal memes, poses a significant challenge to digital platform safety. Recent research has developed detection models tailored to specific modalities; however, there is a notable gap in transferring detection capabilities across different formats. This study conducts extensive experiments using few-shot in-context learning with large language models to explore the transferability of hate speech detection between modalities. Our findings demonstrate that text-based hate speech examples can significantly enhance the classification accuracy of vision-language hate speech. Moreover, text-based demonstrations outperform vision-language demonstrations in few-shot learning settings. These results highlight the effectiveness of cross-modality knowledge transfer and offer valuable insights for improving hate speech detection systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.446.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--446 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.446.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.446/><span class=acl-fixed-case>MIND</span>: Multimodal Shopping Intention Distillation from Large Vision-language Models for <span class=acl-fixed-case>E</span>-commerce Purchase Understanding</a></strong><br><a href=/people/b/baixuan-xu/>Baixuan Xu</a>
|
<a href=/people/w/weiqi-wang/>Weiqi Wang</a>
|
<a href=/people/h/haochen-shi/>Haochen Shi</a>
|
<a href=/people/w/wenxuan-ding/>Wenxuan Ding</a>
|
<a href=/people/h/huihao-jing/>Huihao Jing</a>
|
<a href=/people/t/tianqing-fang/>Tianqing Fang</a>
|
<a href=/people/j/jiaxin-bai/>Jiaxin Bai</a>
|
<a href=/people/x/xin-liu/>Xin Liu</a>
|
<a href=/people/c/changlong-yu/>Changlong Yu</a>
|
<a href=/people/z/zheng-li/>Zheng Li</a>
|
<a href=/people/c/chen-luo/>Chen Luo</a>
|
<a href=/people/q/qingyu-yin/>Qingyu Yin</a>
|
<a href=/people/b/bing-yin/>Bing Yin</a>
|
<a href=/people/l/long-chen/>Long Chen</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--446><div class="card-body p-3 small">Improving user experience and providing personalized search results in E-commerce platforms heavily rely on understanding purchase intention. However, existing methods for acquiring large-scale intentions bank on distilling large language models with human annotation for verification. Such an approach tends to generate product-centric intentions, overlook valuable visual information from product images, and incurs high costs for scalability. To address these issues, we introduce MIND, a multimodal framework that allows Large Vision-Language Models (LVLMs) to infer purchase intentions from multimodal product metadata and prioritize human-centric ones. Using Amazon Review data, we apply MIND and create a multimodal intention knowledge base, which contains 1,264,441 intentions derived from 126,142 co-buy shopping records across 107,215 products. Extensive human evaluations demonstrate the high plausibility and typicality of our obtained intentions and validate the effectiveness of our distillation framework and filtering mechanism. Further experiments reveal the positive downstream benefits that MIND brings to intention comprehension tasks and highlight the importance of multimodal generation and role-aware filtering. Additionally, MIND shows robustness to different prompts and superior generation quality compared to previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.447.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--447 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.447/><span class=acl-fixed-case>ECON</span>: On the Detection and Resolution of Evidence Conflicts</a></strong><br><a href=/people/c/cheng-jiayang/>Cheng Jiayang</a>
|
<a href=/people/c/chunkit-chan/>Chunkit Chan</a>
|
<a href=/people/q/qianqian-zhuang/>Qianqian Zhuang</a>
|
<a href=/people/l/lin-qiu/>Lin Qiu</a>
|
<a href=/people/t/tianhang-zhang/>Tianhang Zhang</a>
|
<a href=/people/t/tengxiao-liu/>Tengxiao Liu</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--447><div class="card-body p-3 small">The rise of large language models (LLMs) has significantly influenced the quality of information in decision-making systems, leading to the prevalence of AI-generated content and challenges in detecting misinformation and managing conflicting information, or “inter-evidence conflicts.” This study introduces a method for generating diverse, validated evidence conflicts to simulate real-world misinformation scenarios. We evaluate conflict detection methods, including Natural Language Inference (NLI) models, factual consistency (FC) models, and LLMs, on these conflicts (RQ1) and analyze LLMs’ conflict resolution behaviors (RQ2). Our key findings include: (1) NLI and LLM models exhibit high precision in detecting answer conflicts, though weaker models suffer from low recall; (2) FC models struggle with lexically similar answer conflicts, while NLI and LLM models handle these better; and (3) stronger models like GPT-4 show robust performance, especially with nuanced conflicts. For conflict resolution, LLMs often favor one piece of conflicting evidence without justification and rely on internal knowledge if they have prior beliefs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.448.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--448 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.448.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.448.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.448/>“Image, Tell me your story!” Predicting the original meta-context of visual misinformation</a></strong><br><a href=/people/j/jonathan-tonglet/>Jonathan Tonglet</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--448><div class="card-body p-3 small">To assist human fact-checkers, researchers have developed automated approaches for visual misinformation detection. These methods assign veracity scores by identifying inconsistencies between the image and its caption, or by detecting forgeries in the image. However, they neglect a crucial point of the human fact-checking process: identifying the original meta-context of the image. By explaining what is actually true about the image, fact-checkers can better detect misinformation, focus their efforts on check-worthy visual content, engage in counter-messaging before misinformation spreads widely, and make their explanation more convincing. Here, we fill this gap by introducing the task of automated image contextualization. We create 5Pils, a dataset of 1,676 fact-checked images with question-answer pairs about their original meta-context. Annotations are based on the 5 Pillars fact-checking framework. We implement a first baseline that grounds the image in its original meta-context using the content of the image and textual evidence retrieved from the open web. Our experiments show promising results while highlighting several open challenges in retrieval and reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.449.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.449/>Improving Retrieval-augmented Text-to-<span class=acl-fixed-case>SQL</span> with <span class=acl-fixed-case>AST</span>-based Ranking and Schema Pruning</a></strong><br><a href=/people/z/zhili-shen/>Zhili Shen</a>
|
<a href=/people/p/pavlos-vougiouklis/>Pavlos Vougiouklis</a>
|
<a href=/people/c/chenxin-diao/>Chenxin Diao</a>
|
<a href=/people/k/kaustubh-vyas/>Kaustubh Vyas</a>
|
<a href=/people/y/yuanyi-ji/>Yuanyi Ji</a>
|
<a href=/people/j/jeff-z-pan/>Jeff Z. Pan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.450.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--450 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.450.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.450/>Mixture-of-Subspaces in Low-Rank Adaptation</a></strong><br><a href=/people/t/taiqiang-wu/>Taiqiang Wu</a>
|
<a href=/people/j/jiahao-wang/>Jiahao Wang</a>
|
<a href=/people/z/zhe-zhao/>Zhe Zhao</a>
|
<a href=/people/n/ngai-wong/>Ngai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--450><div class="card-body p-3 small">In this paper, we introduce a subspace-inspired Low-Rank Adaptation (LoRA) method, which is computationally efficient, easy to implement, and readily applicable to large language, multimodal, and diffusion models. Initially, we equivalently decompose the weights of LoRA into two subspaces, and find that simply mixing them can enhance performance. To study such a phenomenon, we revisit it through a fine-grained subspace lens, showing that such modification is equivalent to employing a fixed mixer to fuse the subspaces. To be more flexible, we jointly learn the mixer with the original LoRA weights, and term the method as Mixture-of-Subspaces LoRA (MoSLoRA). MoSLoRA consistently outperforms LoRA on tasks in different modalities, including commonsense reasoning, visual instruction tuning, and subject-driven text-to-image generation, demonstrating its effectiveness and robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.451.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--451 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.451/><span class=acl-fixed-case>PARIKSHA</span>: A Large-Scale Investigation of Human-<span class=acl-fixed-case>LLM</span> Evaluator Agreement on Multilingual and Multi-Cultural Data</a></strong><br><a href=/people/i/ishaan-watts/>Ishaan Watts</a>
|
<a href=/people/v/varun-gumma/>Varun Gumma</a>
|
<a href=/people/a/aditya-yadavalli/>Aditya Yadavalli</a>
|
<a href=/people/v/vivek-seshadri/>Vivek Seshadri</a>
|
<a href=/people/m/manohar-swaminathan/>Manohar Swaminathan</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--451><div class="card-body p-3 small">Evaluation of multilingual Large Language Models (LLMs) is challenging due to a variety of factors – the lack of benchmarks with sufficient linguistic diversity, contamination of popular benchmarks into LLM pre-training data and the lack of local, cultural nuances in translated benchmarks. In this work, we study human and LLM-based evaluation in a multilingual, multi-cultural setting. We evaluate 30 models across 10 Indic languages by conducting 90K human evaluations and 30K LLM-based evaluations and find that models such as GPT-4o and Llama-3 70B consistently perform best for most Indic languages. We build leaderboards for two evaluation settings - pairwise comparison and direct assessment and analyse the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. We also check for various biases in human and LLM-based evaluation and find evidence of self-bias in the GPT-based evaluator. Our work presents a significant step towards scaling up multilingual evaluation of LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.452.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.452.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--452 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.452 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.452/><span class=acl-fixed-case>L</span>aw<span class=acl-fixed-case>B</span>ench: Benchmarking Legal Knowledge of Large Language Models</a></strong><br><a href=/people/z/zhiwei-fei/>Zhiwei Fei</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/d/dawei-zhu/>Dawei Zhu</a>
|
<a href=/people/f/fengzhe-zhou/>Fengzhe Zhou</a>
|
<a href=/people/z/zhuo-han/>Zhuo Han</a>
|
<a href=/people/a/alan-huang/>Alan Huang</a>
|
<a href=/people/s/songyang-zhang/>Songyang Zhang</a>
|
<a href=/people/k/kai-chen/>Kai Chen</a>
|
<a href=/people/z/zhixin-yin/>Zhixin Yin</a>
|
<a href=/people/z/zongwen-shen/>Zongwen Shen</a>
|
<a href=/people/j/jidong-ge/>Jidong Ge</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--452><div class="card-body p-3 small">We present LawBench, the first evaluation benchmark composed of 20 tasks aimed to assess the ability of Large Language Models (LLMs) to perform Chinese legal-related tasks. LawBench is meticulously crafted to enable precise assessment of LLMs’ legal capabilities from three cognitive levels that correspond to the widely accepted Bloom’s cognitive taxonomy. Using LawBench, we present a comprehensive evaluation of 21 popular LLMs and the first comparative analysis of the empirical results in order to reveal their relative strengths and weaknesses. All data, model predictions and evaluation code are accessible from https://github.com/open-compass/LawBench.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.453.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.453.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--453 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.453 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.453.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.453.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.453/>Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards</a></strong><br><a href=/people/f/furkan-sahinuc/>Furkan Şahinuç</a>
|
<a href=/people/t/thy-thy-tran/>Thy Thy Tran</a>
|
<a href=/people/y/yulia-grishina/>Yulia Grishina</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/b/bei-chen/>Bei Chen</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--453><div class="card-body p-3 small">Scientific leaderboards are standardized ranking systems that facilitate evaluating and comparing competitive methods. Typically, a leaderboard is defined by a task, dataset, and evaluation metric (TDM) triple, allowing objective performance assessment and fostering innovation through benchmarking. However, the exponential increase in publications has made it infeasible to construct and maintain these leaderboards manually. Automatic leaderboard construction has emerged as a solution to reduce manual labor. Existing datasets for this task are based on the community-contributed leaderboards without additional curation. Our analysis shows that a large portion of these leaderboards are incomplete, and some of them contain incorrect information. In this work, we present SciLead, a manually-curated Scientific Leaderboard dataset that overcomes the aforementioned problems. Building on this dataset, we propose three experimental settings that simulate real-world scenarios where TDM triples are fully defined, partially defined, or undefined during leaderboard construction. While previous research has only explored the first setting, the latter two are more representative of real-world applications. To address these diverse settings, we develop a comprehensive LLM-based framework for constructing leaderboards. Our experiments and analysis reveal that various LLMs often correctly identify TDM triples while struggling to extract result values from publications. We make our code and data publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.454.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.454.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--454 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.454 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.454/>Efficient Vision-Language pre-training via domain-specific learning for human activities</a></strong><br><a href=/people/a/adrian-bulat/>Adrian Bulat</a>
|
<a href=/people/y/yassine-ouali/>Yassine Ouali</a>
|
<a href=/people/r/ricardo-guerrero/>Ricardo Guerrero</a>
|
<a href=/people/b/brais-martinez/>Brais Martinez</a>
|
<a href=/people/g/georgios-tzimiropoulos/>Georgios Tzimiropoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--454><div class="card-body p-3 small">Current Vision-Language (VL) models owe their success to large-scale pre-training on web-collected data, which in turn requires high-capacity architectures and large compute resources for training. We posit that when the downstream tasks are known in advance, which is in practice common, the pretraining process can be aligned to the downstream domain, leading to more efficient and accurate models, while shortening the pretraining step. To this end, we introduce a domain-aligned pretraining strategy that, without additional data collection, improves the accuracy on a domain of interest, herein, that of human activities, while largely preserving the generalist knowledge. At the core of our approach stands a new LLM-based method that, provided with a simple set of concept seeds, produces a concept hierarchy with high coverage of the target domain.The concept hierarchy is used to filter a large-scale web-crawled dataset and, then, enhance the resulting instances with targeted synthetic labels. We study in depth how to train such approaches and their resulting behavior. We further show generalization to video-based data by introducing a fast adaptation approach for transitioning from a static (image) model to a dynamic one (i.e. with temporal modeling). On the domain of interest, our approach significantly outperforms models trained on up to <span class=tex-math>60×</span> more samples and between <span class=tex-math>10-100×</span> shorter training schedules for image retrieval, video retrieval and action recognition. Code will be released.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.455.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.455.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--455 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.455 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.455.software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.455/>Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training</a></strong><br><a href=/people/w/wenbo-li/>Wenbo Li</a>
|
<a href=/people/g/guohao-li/>Guohao Li</a>
|
<a href=/people/z/zhibin-lan/>Zhibin Lan</a>
|
<a href=/people/x/xue-xu/>Xue Xu</a>
|
<a href=/people/w/wanru-zhuang/>Wanru Zhuang</a>
|
<a href=/people/j/jiachen-liu/>Jiachen Liu</a>
|
<a href=/people/x/xinyan-xiao/>Xinyan Xiao</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--455><div class="card-body p-3 small">Diffusion-based text-to-image models have demonstrated impressive achievements in diversity and aesthetics but struggle to generate images with legible visual texts. Existing backbone models have limitations such as misspelling, failing to generate texts, and lack of support for Chinese texts, but their development shows promising potential. In this paper, we propose a series of methods, aiming to empower backbone models to generate visual texts in English and Chinese. We first conduct a preliminary study revealing that BPE tokenization and insufficient learning of cross-attention modules restrict the performance of the backbone models. Based on these observations, we make the following improvements: (1) We design a mixed granularity input strategy to provide more suitable text representations; (2) We propose to augment the conventional training objective with three glyph-aware training losses, which enhance the learning of cross-attention modules and encourage the model to focus on visual texts. Through experiments, we demonstrate that our methods can effectively empower backbone models to generate semantic relevant, aesthetically appealing, and accurate visual text images, while maintaining their fundamental image generation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.456.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.456.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--456 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.456 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.456/>Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works</a></strong><br><a href=/people/x/xinfeng-yuan/>Xinfeng Yuan</a>
|
<a href=/people/s/siyu-yuan/>Siyu Yuan</a>
|
<a href=/people/y/yuhan-cui/>Yuhan Cui</a>
|
<a href=/people/t/tianhe-lin/>Tianhe Lin</a>
|
<a href=/people/x/xintao-wang/>Xintao Wang</a>
|
<a href=/people/r/rui-xu/>Rui Xu</a>
|
<a href=/people/j/jiangjie-chen/>Jiangjie Chen</a>
|
<a href=/people/d/deqing-yang/>Deqing Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--456><div class="card-body p-3 small">Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs’ character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CROSS dataset from literature experts and assess the generated profiles by comparing them with ground truth references and evaluating their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs. Resources are available at https://github.com/Joanna0123/character_profiling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.457.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.457.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--457 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.457 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.457/>Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners</a></strong><br><a href=/people/s/shimao-zhang/>Shimao Zhang</a>
|
<a href=/people/c/changjiang-gao/>Changjiang Gao</a>
|
<a href=/people/w/wenhao-zhu/>Wenhao Zhu</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a>
|
<a href=/people/x/xin-huang/>Xin Huang</a>
|
<a href=/people/x/xue-han/>Xue Han</a>
|
<a href=/people/j/junlan-feng/>Junlan Feng</a>
|
<a href=/people/c/chao-deng/>Chao Deng</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--457><div class="card-body p-3 small">Recently, Large Language Models (LLMs) have shown impressive language capabilities, while most of them have very unbalanced performance across different languages. Multilingual alignment based on the translation parallel data is an effective method to enhance LLMs’ multilingual capabilities. In this work, we first discover and comprehensively investigate the spontaneous multilingual alignment of LLMs. Firstly, we find that LLMs instruction-tuned on the question translation data (i.e. without annotated answers) are able to encourage the alignment between English and a wide range of languages, even including those unseen during instruction-tuning. Additionally, we utilize different settings and mechanistic interpretability methods to analyze the LLM’s performance in the multilingual scenario comprehensively. Our work suggests that LLMs have enormous potential for improving multilingual alignment efficiently with great language generalization and task generalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.458.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.458.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--458 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.458 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.458/><span class=acl-fixed-case>A</span>da<span class=acl-fixed-case>S</span>witch: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning</a></strong><br><a href=/people/h/hao-sun/>Hao Sun</a>
|
<a href=/people/j/jiayi-wu/>Jiayi Wu</a>
|
<a href=/people/h/hengyi-cai/>Hengyi Cai</a>
|
<a href=/people/x/xiaochi-wei/>Xiaochi Wei</a>
|
<a href=/people/y/yue-feng/>Yue Feng</a>
|
<a href=/people/b/bo-wang/>Bo Wang</a>
|
<a href=/people/s/shuaiqiang-wang/>Shuaiqiang Wang</a>
|
<a href=/people/y/yan-zhang/>Yan Zhang</a>
|
<a href=/people/d/dawei-yin/>Dawei Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--458><div class="card-body p-3 small">Recent advancements in large language models (LLMs) have been remarkable. Users face a choice between using cloud-based LLMs for generation quality and deploying local-based LLMs for lower computational cost. The former option is typically costly and inefficient, while the latter usually fails to deliver satisfactory performance for reasoning steps requiring deliberate thought processes. In this work, we propose a novel LLM utilization paradigm that facilitates the collaborative operation of large cloud-based LLMs and smaller local-deployed LLMs. Our framework comprises two primary modules: the local agent instantiated with a relatively smaller LLM, handling less complex reasoning steps, and the cloud agent equipped with a larger LLM, managing more intricate reasoning steps. This collaborative processing is enabled through an adaptive mechanism where the local agent introspectively identifies errors and proactively seeks assistance from the cloud agent, thereby effectively integrating the strengths of both locally-deployed and cloud-based LLMs, resulting in significant enhancements in task completion performance and efficiency. We evaluate AdaSwitch across 7 benchmarks, ranging from mathematical reasoning and complex question answering, using various types of LLMs to instantiate the local and cloud agents. The empirical results show that AdaSwitch effectively improves the performance of the local agent, and sometimes achieves competitive results compared to the cloud agent while utilizing much less computational overhead.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.459.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.459.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--459 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.459 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.459/><span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>B</span>a: Convergence Balancer for Multitask Finetuning of Large Language Models</a></strong><br><a href=/people/z/zi-gong/>Zi Gong</a>
|
<a href=/people/h/hang-yu/>Hang Yu</a>
|
<a href=/people/c/cong-liao/>Cong Liao</a>
|
<a href=/people/b/bingchang-liu/>Bingchang Liu</a>
|
<a href=/people/c/chaoyu-chen/>Chaoyu Chen</a>
|
<a href=/people/j/jianguo-li/>Jianguo Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--459><div class="card-body p-3 small">Multi-task learning (MTL) benefits the fine-tuning of large language models (LLMs) by providing a single model with improved performance and generalization ability across tasks, presenting a resource-efficient alternative to developing separate models for each task. Yet, existing MTL strategies for LLMs often fall short by either being computationally intensive or failing to ensure simultaneous task convergence. This paper presents CoBa, a new MTL approach designed to effectively manage task convergence balance with minimal computational overhead. Utilizing Relative Convergence Scores (RCS), Absolute Convergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically adjusts task weights during the training process, ensuring that the validation loss of all tasks progress towards convergence at an even pace while mitigating the issue of individual task divergence. The results of our experiments involving three disparate datasets underscore that this approach not only fosters equilibrium in task improvement but enhances the LLMs’ performance by up to 13% relative to the second-best baselines. Code is open-sourced at https://github.com/codefuse-ai/MFTCoder.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.460.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--460 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.460/>m<span class=acl-fixed-case>DPO</span>: Conditional Preference Optimization for Multimodal Large Language Models</a></strong><br><a href=/people/f/fei-wang/>Fei Wang</a>
|
<a href=/people/w/wenxuan-zhou/>Wenxuan Zhou</a>
|
<a href=/people/j/james-y-huang/>James Y. Huang</a>
|
<a href=/people/n/nan-xu/>Nan Xu</a>
|
<a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/h/hoifung-poon/>Hoifung Poon</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--460><div class="card-body p-3 small">Direct preference optimization (DPO) has shown to be an effective method for large language model (LLM) alignment. Recent works have attempted to apply DPO to multimodal scenarios but have found it challenging to achieve consistent improvement. Through a comparative experiment, we identify the unconditional preference problem in multimodal preference optimization, where the model overlooks the image condition. To address this problem, we propose mDPO, a multimodal DPO objective that prevents the over-prioritization of language-only preferences by also optimizing image preference. Moreover, we introduce a reward anchor that forces the reward to be positive for chosen responses, thereby avoiding the decrease in their likelihood—an intrinsic problem of relative preference optimization. Experiments on two multimodal LLMs of different sizes and three widely used benchmarks demonstrate that mDPO effectively addresses the unconditional preference problem in multimodal preference optimization and significantly improves model performance, particularly in reducing hallucination.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.461.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.461.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--461 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.461 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.461/>Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models</a></strong><br><a href=/people/f/fei-wang/>Fei Wang</a>
|
<a href=/people/n/ninareh-mehrabi/>Ninareh Mehrabi</a>
|
<a href=/people/p/palash-goyal/>Palash Goyal</a>
|
<a href=/people/r/rahul-gupta/>Rahul Gupta</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--461><div class="card-body p-3 small">Data are crucial element in large language model (LLM) alignment. Recent studies have explored using LLMs for efficient data collection. However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints. To address these problems, we propose Data Advisor, an enhanced LLM-based method for generating data that takes into account the characteristics of the desired dataset. Starting from a set of pre-defined principles in hand, Data Advisor monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. Data Advisor can be easily integrated into existing data generation methods to enhance data quality and coverage. Experiments on safety alignment of three representative LLMs (i.e., Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in enhancing model safety against various fine-grained safety issues without sacrificing model utility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.462.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.462.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--462 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.462 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.462/>Language-to-Code Translation with a Single Labeled Example</a></strong><br><a href=/people/k/kaj-bostrom/>Kaj Bostrom</a>
|
<a href=/people/h/harsh-jhamtani/>Harsh Jhamtani</a>
|
<a href=/people/h/hao-fang/>Hao Fang</a>
|
<a href=/people/s/sam-thomson/>Sam Thomson</a>
|
<a href=/people/r/richard-shin/>Richard Shin</a>
|
<a href=/people/p/patrick-xia/>Patrick Xia</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--462><div class="card-body p-3 small">Tools for translating natural language into code promise natural, open-ended interaction with databases, web APIs, and other software systems. However, this promise is complicated by the diversity and continual development of these systems, each with its own interface and distinct set of features. Building a new language-to-code translator, even starting with a large language model (LM), typically requires annotating a large set of natural language commands with their associated programs. In this paper, we describe ICIP (In-Context Inverse Programming), a method for bootstrapping a language-to-code system using mostly (or entirely) unlabeled programs written using a potentially unfamiliar (but human-readable) library or API. ICIP uses a pre-trained LM to assign candidate natural language descriptions to these programs, then iteratively refines the descriptions to ensure global consistency. Across nine different application domains from the Overnight and Spider benchmarks and text-davinci-003 and CodeLlama-7b-Instruct models, ICIP outperforms a number of prompting baselines. Indeed, in a “nearly unsupervised” setting with only a single annotated program and 100 unlabeled examples, it achieves up to 85% of the performance of a fully supervised system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.463.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.463.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--463 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.463 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.463.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.463.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.463/>Attribute or Abstain: Large Language Models as Long Document Assistants</a></strong><br><a href=/people/j/jan-buchmann/>Jan Buchmann</a>
|
<a href=/people/x/xiao-liu/>Xiao Liu</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--463><div class="card-body p-3 small">LLMs can help humans working with long documents, but are known to hallucinate. *Attribution* can increase trust in LLM responses: The LLM provides evidence that supports its response, which enhances verifiability. Existing approaches to attribution have only been evaluated in RAG settings, where the initial retrieval confounds LLM performance. This is crucially different from the long document setting, where retrieval is not needed, but could help. Thus, a long document specific evaluation of attribution is missing. To fill this gap, we present LAB, a benchmark of 6 diverse long document tasks with attribution, and experiments with different approaches to attribution on 5 LLMs of different sizes. We find that *citation*, i.e. response generation and evidence extraction in one step, performs best for large and fine-tuned models, while additional retrieval can help for small, prompted models. We investigate whether the “Lost in the Middle” phenomenon exists for attribution, but do not find this. We also find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims. We release code and data for further investigation. [Link](https://github.com/UKPLab/arxiv2024-attribute-or-abstain)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.464.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--464 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.464.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.464.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.464/><span class=acl-fixed-case>FEDKIM</span>: Adaptive Federated Knowledge Injection into Medical Foundation Models</a></strong><br><a href=/people/x/xiaochen-wang/>Xiaochen Wang</a>
|
<a href=/people/j/jiaqi-wang/>Jiaqi Wang</a>
|
<a href=/people/h/houping-xiao/>Houping Xiao</a>
|
<a href=/people/j/jinghui-chen/>Jinghui Chen</a>
|
<a href=/people/f/fenglong-ma/>Fenglong Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--464><div class="card-body p-3 small">Foundation models have demonstrated remarkable capabilities in handling diverse modalities and tasks, outperforming conventional artificial intelligence (AI) approaches that are highly task-specific and modality-reliant. In the medical domain, however, the development of comprehensive foundation models is constrained by limited access to diverse modalities and stringent privacy regulations. To address these constraints, this study introduces a novel knowledge injection approach, FedKIM, designed to scale the medical foundation model within a federated learning framework. FedKIM leverages lightweight local models to extract healthcare knowledge from private data and integrates this knowledge into a centralized foundation model using a designed adaptive Multitask Multimodal Mixture Of Experts (M<span class=tex-math><sup>3</sup></span>OE) module. This method not only preserves privacy but also enhances the model’s ability to handle complex medical tasks involving multiple modalities. Our extensive experiments across twelve tasks in seven modalities demonstrate the effectiveness of FedKIM in various settings, highlighting its potential to scale medical foundation models without direct access to sensitive data. Source codes are available at https://github.com/XiaochenWang-PSU/FedKIM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.465.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.465.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--465 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.465 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.465/>Retrieved In-Context Principles from Previous Mistakes</a></strong><br><a href=/people/h/hao-sun/>Hao Sun</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/b/bo-wang/>Bo Wang</a>
|
<a href=/people/y/yingyan-hou/>Yingyan Hou</a>
|
<a href=/people/y/yan-zhang/>Yan Zhang</a>
|
<a href=/people/p/pengjun-xie/>Pengjun Xie</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--465><div class="card-body p-3 small">In-context learning (ICL) has been instrumental in adapting large language models (LLMs) to downstream tasks using correct input-output examples. Recent advances have attempted to improve model performance through principles derived from mistakes, yet these approaches suffer from lack of customization and inadequate error coverage. To address these limitations, we propose Retrieved In-Context Principles (RICP), a novel teacher-student framework. In RICP, the teacher model analyzes mistakes from the student model to generate reasons and insights for preventing similar mistakes. These mistakes are clustered based on their underlying reasons for developing task-level principles, enhancing the error coverage of principles. During inference, the most relevant mistakes for each question are retrieved to create question-level principles, improving the customization of the provided guidance. RICP is orthogonal to existing prompting methods and does not require intervention from the teacher model during inference. Experimental results across seven reasoning benchmarks reveal that RICP effectively enhances performance when applied to various prompting strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.466.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.466.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--466 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.466 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.466/><span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>K</span>nob: Enhance Voice Cloning with Fine-Grained Emotion Control</a></strong><br><a href=/people/h/haozhe-chen/>Haozhe Chen</a>
|
<a href=/people/r/run-chen/>Run Chen</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--466><div class="card-body p-3 small">While recent advances in Text-to-Speech (TTS) technology produce natural and expressive speech, they lack the option for users to select emotion and control intensity. We propose EmoKnob, a framework that allows fine-grained emotion control in speech synthesis with few-shot demonstrative samples of arbitrary emotion. Our framework leverages the expressive speaker representation space made possible by recent advances in foundation voice cloning models. Based on the few-shot capability of our emotion control framework, we propose two methods to apply emotion control on emotions described by open-ended text, enabling an intuitive interface for controlling a diverse array of nuanced emotions. To facilitate a more systematic emotional speech synthesis field, we introduce a set of evaluation metrics designed to rigorously assess the faithfulness and recognizability of emotion control frameworks. Through objective and subjective evaluations, we show that our emotion control framework effectively embeds emotions into speech and surpasses emotion expressiveness of commercial TTS services.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.467.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.467.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--467 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.467 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.467/><span class=acl-fixed-case>VPTQ</span>: Extreme Low-bit Vector Post-Training Quantization for Large Language Models</a></strong><br><a href=/people/y/yifei-liu/>Yifei Liu</a>
|
<a href=/people/j/jicheng-wen/>Jicheng Wen</a>
|
<a href=/people/y/yang-wang/>Yang Wang</a>
|
<a href=/people/s/shengyu-ye/>Shengyu Ye</a>
|
<a href=/people/l/li-lyna-zhang/>Li Lyna Zhang</a>
|
<a href=/people/t/ting-cao/>Ting Cao</a>
|
<a href=/people/c/cheng-li/>Cheng Li</a>
|
<a href=/people/m/mao-yang/>Mao Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--467><div class="card-body p-3 small">Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit.Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables. In this paper, we introduce **Vector Post-Training Quantization (VPTQ)** for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization.We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ.In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model.Our experimental results show that VPTQ reduces model quantization perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, 11-22% on LLaMA-3 on QA tasks on average. We only utilize 10.4-18.6% of the quantization algorithm execution time, resulting in a 1.6-<span class=tex-math>1.8×</span> increase in inference throughput compared to SOTA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.468.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.468.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--468 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.468 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.468/>An <span class=acl-fixed-case>L</span>* Algorithm for Deterministic Weighted Regular Languages</a></strong><br><a href=/people/c/clemente-pasti/>Clemente Pasti</a>
|
<a href=/people/t/talu-karagoz/>Talu Karagöz</a>
|
<a href=/people/f/franz-nowak/>Franz Nowak</a>
|
<a href=/people/a/anej-svete/>Anej Svete</a>
|
<a href=/people/r/reda-boumasmoud/>Reda Boumasmoud</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--468><div class="card-body p-3 small">Extracting finite state automata (FSAs) fromblack-box models offers a powerful approachto gaining interpretable insights into complexmodel behaviors. To support this pursuit, wepresent a weighted variant of Angluin’s (1987)L* algorithm for learning FSAs. We stay faithful to the original formulation, devising a wayto exactly learn deterministic weighted FSAswhose weights support division. Furthermore,we formulate the learning process in a mannerthat highlights the connection with FSA minimization, showing how L* directly learns aminimal automaton for the target language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.469.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.469.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--469 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.469 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.469/>Towards Verifiable Text Generation with Evolving Memory and Self-Reflection</a></strong><br><a href=/people/h/hao-sun/>Hao Sun</a>
|
<a href=/people/h/hengyi-cai/>Hengyi Cai</a>
|
<a href=/people/b/bo-wang/>Bo Wang</a>
|
<a href=/people/y/yingyan-hou/>Yingyan Hou</a>
|
<a href=/people/x/xiaochi-wei/>Xiaochi Wei</a>
|
<a href=/people/s/shuaiqiang-wang/>Shuaiqiang Wang</a>
|
<a href=/people/y/yan-zhang/>Yan Zhang</a>
|
<a href=/people/d/dawei-yin/>Dawei Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--469><div class="card-body p-3 small">Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination. A promising solution to this issue is verifiable text generation, which prompts LLMs to generate content with citations for accuracy verification. However, verifiable text generation is non-trivial due to the focus-shifting phenomenon, the intricate reasoning needed to align the claim with correct citations, and the dilemma between the precision and breadth of retrieved documents. In this paper, we present VTG, an innovative framework for Verifiable Text Generation with evolving memory and self-reflection. VTG introduces evolving long short-term memory to retain both valuable documents and recent documents. A two-tier verifier equipped with an evidence finder is proposed to rethink and reflect on the relationship between the claim and citations. Furthermore, active retrieval and diverse query generation are utilized to enhance both the precision and breadth of the retrieved documents. We conduct extensive experiments on five datasets across three knowledge-intensive tasks and the results reveal that VTG significantly outperforms baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.470.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.470.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--470 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.470 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.470/>Pelican: Correcting Hallucination in Vision-<span class=acl-fixed-case>LLM</span>s via Claim Decomposition and Program of Thought Verification</a></strong><br><a href=/people/p/pritish-sahu/>Pritish Sahu</a>
|
<a href=/people/k/karan-sikka/>Karan Sikka</a>
|
<a href=/people/a/ajay-divakaran/>Ajay Divakaran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--470><div class="card-body p-3 small">Large Visual Language Models (LVLMs) struggle with hallucinations in visual instruction following task(s). These issues hinder their trustworthiness and real-world applicability. We propose Pelican – a novel framework designed to detect and mitigate hallucinations through claim verification. Pelican first decomposes the visual claim into a chain of sub-claims based on first-order predicates. These sub-claims consists of (predicate, question) pairs and can be conceptualized as nodes of a computational graph. We then use use Program-of-Thought prompting to generate Python code for answering these questions through flexible composition of external tools. Pelican improves over prior work by introducing (1) intermediate variables for precise grounding of object instances, and (2) shared computation for answering the sub-question to enable adaptive corrections and inconsistency identification. We finally use reasoning abilities of LLM to verify the correctness of the the claim by considering the consistency and confidence of the (question, answer) pairs from each sub-claim. Our experiments demonstrate consistent performance improvements over various baseline LVLMs and existing hallucination mitigation approaches across several benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.471.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.471.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--471 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.471 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.471/>Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes</a></strong><br><a href=/people/y/yusuke-hirota/>Yusuke Hirota</a>
|
<a href=/people/j/jerone-andrews/>Jerone Andrews</a>
|
<a href=/people/d/dora-zhao/>Dora Zhao</a>
|
<a href=/people/o/orestis-papakyriakopoulos/>Orestis Papakyriakopoulos</a>
|
<a href=/people/a/apostolos-modas/>Apostolos Modas</a>
|
<a href=/people/y/yuta-nakashima/>Yuta Nakashima</a>
|
<a href=/people/a/alice-xiang/>Alice Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--471><div class="card-body p-3 small">We tackle societal bias in image-text datasets by removing spurious correlations between protected groups and image attributes. Traditional methods only target labeled attributes, ignoring biases from unlabeled ones. Using text-guided inpainting models, our approach ensures protected group independence from all attributes and mitigates inpainting biases through data filtering. Evaluations on multi-label image classification and image captioning tasks show our method effectively reduces bias without compromising performance across various models. Specifically, we achieve an average societal bias reduction of 46.1% in leakage-based bias metrics for multi-label classification and 74.8% for image captioning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.472.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.472.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--472 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.472 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.472.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.472/><span class=acl-fixed-case>R</span>eal<span class=acl-fixed-case>V</span>ul: Can We Detect Vulnerabilities in Web Applications with <span class=acl-fixed-case>LLM</span>?</a></strong><br><a href=/people/d/di-cao/>Di Cao</a>
|
<a href=/people/y/yong-liao/>Yong Liao</a>
|
<a href=/people/x/xiuwei-shang/>Xiuwei Shang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--472><div class="card-body p-3 small">The latest advancements in large language models (LLMs) have sparked interest in their potential for software vulnerability detection. However, there is currently a lack of research specifically focused on vulnerabilities in the PHP language, and challenges in data sampling and processing persist, hindering the model’s ability to effectively capture the characteristics of specific vulnerabilities. In this paper, we present RealVul, the first LLM-based framework designed for PHP vulnerability detection, addressing these issues. By improving code sampling methods and employing normalization techniques, we can isolate potential vulnerability triggers while streamlining the code and eliminating unnecessary semantic information, enabling the model to better understand and learn from the generated vulnerability samples. We also address the issue of insufficient PHP vulnerability samples by improving data synthesis methods. To evaluate RealVul’s performance, we conduct an extensive analysis using five distinct code LLMs on vulnerability data from 180 PHP projects. The results demonstrate a significant improvement in both effectiveness and generalization compared to existing methods, effectively boosting the vulnerability detection capabilities of these models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.473.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.473.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--473 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.473 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.473/>Unsupervised End-to-End Task-Oriented Dialogue with <span class=acl-fixed-case>LLM</span>s: The Power of the Noisy Channel</a></strong><br><a href=/people/b/brendan-king/>Brendan King</a>
|
<a href=/people/j/jeffrey-flanigan/>Jeffrey Flanigan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--473><div class="card-body p-3 small">Training task-oriented dialogue systems typically requires turn-level annotations for interacting with their APIs: e.g. a dialogue state and the system actions taken at each step. These annotations can be costly to produce, error-prone, and require both domain and annotation expertise. With advances in LLMs, we hypothesize that unlabeled data and a schema definition are sufficient for building a working task-oriented dialogue system, completely unsupervised. We consider a novel unsupervised setting of only (1) a well-defined API schema (2) a set of unlabeled dialogues between a user and agent. We propose an innovative approach using expectation-maximization (EM) that infers turn-level annotations as latent variables using a noisy channel model to build an end-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark, our method more than doubles the dialogue success rate of a strong GPT-3.5 baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.474.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.474.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--474 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.474 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.474.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.474.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.474/>Humans or <span class=acl-fixed-case>LLM</span>s as the Judge? A Study on Judgement Bias</a></strong><br><a href=/people/g/guiming-hardy-chen/>Guiming Hardy Chen</a>
|
<a href=/people/s/shunian-chen/>Shunian Chen</a>
|
<a href=/people/z/ziche-liu/>Ziche Liu</a>
|
<a href=/people/f/feng-jiang/>Feng Jiang</a>
|
<a href=/people/b/benyou-wang/>Benyou Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--474><div class="card-body p-3 small">Adopting human and large language models (LLM) as judges (*a.k.a* human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLMs, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating **Misinformation Oversight Bias**, **Gender Bias**, **Authority Bias** and **Beauty Bias** on LLM and human judges. We curate a dataset referring to the revised Bloom’s Taxonomy and conduct thousands of evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit these biases to conduct attacks on LLM judges. We hope that our work can notify the community of the bias and vulnerability of human- and LLM-as-a-judge, as well as the urgency of developing robust evaluation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.475.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.475.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--475 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.475 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.475/><span class=acl-fixed-case>WPO</span>: Enhancing <span class=acl-fixed-case>RLHF</span> with Weighted Preference Optimization</a></strong><br><a href=/people/w/wenxuan-zhou/>Wenxuan Zhou</a>
|
<a href=/people/r/ravi-agrawal/>Ravi Agrawal</a>
|
<a href=/people/s/shujian-zhang/>Shujian Zhang</a>
|
<a href=/people/s/sathish-reddy-indurthi/>Sathish Reddy Indurthi</a>
|
<a href=/people/s/sanqiang-zhao/>Sanqiang Zhao</a>
|
<a href=/people/k/kaiqiang-song/>Kaiqiang Song</a>
|
<a href=/people/s/silei-xu/>Silei Xu</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--475><div class="card-body p-3 small">Reinforcement learning from human feedback (RLHF) is a promising solution to align large language models (LLMs) more closely with human values. Off-policy preference optimization, where the preference data is obtained from other models, is widely adopted due to its cost efficiency and scalability. However, off-policy preference optimization often suffers from a distributional gap between the policy used for data collection and the target policy, leading to suboptimal optimization. In this paper, we propose a novel strategy to mitigate this problem by simulating on-policy learning with off-policy preference data. Our Weighted Preference Optimization (WPO) method adapts off-policy data to resemble on-policy data more closely by reweighting preference pairs according to their probability under the current policy. This method not only addresses the distributional gap problem but also enhances the optimization process without incurring additional costs. We validate our method on instruction following benchmarks including Alpaca Eval 2 and MT-bench. WPO not only outperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2 but also establishes a remarkable length-controlled winning rate against GPT-4-turbo of 76.7% based on Gemma-2-9b-it. We release the code and models at https://github.com/wzhouad/WPO.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.476.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.476.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--476 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.476 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.476/>Walking in Others’ Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias</a></strong><br><a href=/people/r/rongwu-xu/>Rongwu Xu</a>
|
<a href=/people/z/zian-zhou/>Zian Zhou</a>
|
<a href=/people/t/tianwei-zhang/>Tianwei Zhang</a>
|
<a href=/people/z/zehan-qi/>Zehan Qi</a>
|
<a href=/people/s/su-yao/>Su Yao</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/h/han-qiu/>Han Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--476><div class="card-body p-3 small">The common toxicity and societal bias in contents generated by large language models (LLMs) necessitate strategies to reduce harm. Present solutions often demand white-box access to the model or substantial training, which is impractical for cutting-edge commercial LLMs. Moreover, prevailing prompting methods depend on external tool feedback and fail to simultaneously lessen toxicity and bias. Motivated by social psychology principles, we propose a novel strategy named perspective-taking prompting (PeT) that inspires LLMs to integrate diverse human perspectives and self-regulate their responses. This self-correction mechanism can significantly diminish toxicity (up to 89%) and bias (up to 73%) in LLMs’ responses. Rigorous evaluations and ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and three open-source LLMs, revealing PeT’s superiority in producing less harmful responses, outperforming five strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.477.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--477 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.477/><span class=acl-fixed-case>M</span>eta<span class=acl-fixed-case>R</span>eflection: Learning Instructions for Language Agents using Past Reflections</a></strong><br><a href=/people/p/priyanshu-gupta/>Priyanshu Gupta</a>
|
<a href=/people/s/shashank-kirtania/>Shashank Kirtania</a>
|
<a href=/people/a/ananya-singha/>Ananya Singha</a>
|
<a href=/people/s/sumit-gulwani/>Sumit Gulwani</a>
|
<a href=/people/a/arjun-radhakrishna/>Arjun Radhakrishna</a>
|
<a href=/people/g/gustavo-soares/>Gustavo Soares</a>
|
<a href=/people/s/sherry-shi/>Sherry Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--477><div class="card-body p-3 small">The popularity of Large Language Models (LLMs) have unleashed a new age of Language Agents for solving a diverse range of tasks. While contemporary frontier LLMs are capable enough to power reasonably good Language agents, the closed-API model makes it hard to improve in cases they perform sub-optimally. To address this, recent works have explored techniques to improve their performance using self reflection and prompt optimization techniques. While techniques like self reflection work well in an online setup, contemporary prompt optimization techniques are designed to work on simpler tasks. To address this, we introduce METAREFLECTION, a novel offline reinforcement learning technique that enhances the performance of Language Agents by augmenting a semantic memory based on experiential learnings from past trials. We demonstrate the efficacy of METAREFLECTION by evaluating across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, with different agent design. METAREFLECTION boosts Language agents’ performance by 4 % to 16.82 % over the raw GPT-4 baseline and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.478.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.478.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--478 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.478 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.478.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.478/>Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors</a></strong><br><a href=/people/n/nico-daheim/>Nico Daheim</a>
|
<a href=/people/j/jakub-macina/>Jakub Macina</a>
|
<a href=/people/m/manu-kapur/>Manu Kapur</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--478><div class="card-body p-3 small">Large language models (LLMs) offer many opportunities to scale high-quality personalized tutoring. A promising approach is to build dialog tutoring models to scaffold students’ problem-solving. However, even though existing models perform well in solving reasoning questions, they can struggle to precisely detect student’s errors and tailor their feedback to these errors. Inspired by real-world teaching practice where teachers identify student errors and customize their response based on them, we focus on verifying student solutions and show how grounding to such verification improves the overall quality of tutor response generation. We collect a dataset of 1,002 stepwise math reasoning chains with the first error step annotated by teachers. We show empirically that finding the mistake in a student solution is challenging for current models. We propose and evaluate several verifiers for detecting these errors. Using both automatic and human evaluation we show that the student solution verifiers steer the generation model towards highly targeted responses to student error which are more often correct with less hallucinations compared to existing baselines. The benchmark dataset and code will be released openly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.479.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.479.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--479 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.479 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.479/>On Eliciting Syntax from Language Models via Hashing</a></strong><br><a href=/people/y/yiran-wang/>Yiran Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--479><div class="card-body p-3 small">Unsupervised parsing, also known as grammar induction, aims to infer syntactic structure from raw text. Recently, binary representation has exhibited remarkable information-preserving capabilities at both lexicon and syntax levels. In this paper, we explore the possibility of leveraging this capability to deduce parsing trees from raw text, relying solely on the implicitly induced grammars within models. To achieve this, we upgrade the bit-level CKY from zero-order to first-order to encode the lexicon and syntax in a unified binary representation space, switch training from supervised to unsupervised under the contrastive hashing framework, and introduce a novel loss function to impose stronger yet balanced alignment signals. Our model shows competitive performance on various datasets, therefore, we claim that our method is effective and efficient enough to acquire high-quality parsing trees from pre-trained language models at a low cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.480.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.480.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--480 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.480 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.480/><span class=acl-fixed-case>C</span>li<span class=acl-fixed-case>M</span>ed<span class=acl-fixed-case>B</span>ench: A Large-Scale <span class=acl-fixed-case>C</span>hinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios</a></strong><br><a href=/people/z/zetian-ouyang/>Zetian Ouyang</a>
|
<a href=/people/y/yishuai-qiu/>Yishuai Qiu</a>
|
<a href=/people/l/linlin-wang/>Linlin Wang</a>
|
<a href=/people/g/gerard-de-melo/>Gerard De Melo</a>
|
<a href=/people/y/ya-zhang/>Ya Zhang</a>
|
<a href=/people/y/yanfeng-wang/>Yanfeng Wang</a>
|
<a href=/people/l/liang-he/>Liang He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--480><div class="card-body p-3 small">With the proliferation of Large Language Models (LLMs) in diverse domains, there is a particular need for unified evaluation standards in clinical medical scenarios, where models need to be examined very thoroughly. We present CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical scenarios specifically designed to assess the medical ability of LLMs across 7 pivot dimensions. It comprises 33,735 questions derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises. The reliability of this benchmark has been confirmed in several ways. Subsequent experiments with existing LLMs have led to the following findings: (i) Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital, underscoring the need for advances in clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use. These findings reveal both the strengths and limitations of LLMs in clinical scenarios and offer critical insights for medical research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.481.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.481.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--481 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.481 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.481.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.481.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.481/>The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples</a></strong><br><a href=/people/h/heng-yang/>Heng Yang</a>
|
<a href=/people/k/ke-li/>Ke Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--481><div class="card-body p-3 small">Recent studies have revealed the vulnerability of pre-trained language models to adversarial attacks. Adversarial defense techniques have been proposed to reconstruct adversarial examples within feature or text spaces. However, these methods struggle to effectively repair the semantics in adversarial examples, resulting in unsatisfactory defense performance. To repair the semantics in adversarial examples, we introduce a novel approach named Reactive Perturbation Defocusing (Rapid), which employs an adversarial detector to identify the fake labels of adversarial examples and leverages adversarial attackers to repair the semantics in adversarial examples. Our extensive experimental results, conducted on four public datasets, demonstrate the consistent effectiveness of Rapid in various adversarial attack scenarios. For easy evaluation, we provide a click-to-run demo of Rapid at https://tinyurl.com/22ercuf8.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.482.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--482 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.482/><span class=acl-fixed-case>CSSL</span>: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages</a></strong><br><a href=/people/p/pretam-ray/>Pretam Ray</a>
|
<a href=/people/j/jivnesh-sandhan/>Jivnesh Sandhan</a>
|
<a href=/people/a/amrith-krishna/>Amrith Krishna</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--482><div class="card-body p-3 small">Neural dependency parsing has achieved remarkable performance for low resource morphologically rich languages. It has also been well-studied that morphologically rich languages exhibit relatively free word order. This prompts a fundamental investigation: Is there a way to enhance dependency parsing performance, making the model robust to word order variations utilizing the relatively free word order nature of morphologically rich languages? In this work, we examine the robustness of graph-based parsing architectures on 7 relatively free word order languages. We focus on scrutinizing essential modifications such as data augmentation and the removal of position encoding required to adapt these architectures accordingly. To this end, we propose a contrastive self-supervised learning method to make the model robust to word order variations. Furthermore, our proposed modification demonstrates a substantial average gain of 3.03/2.95 points in 7 relatively free word order languages, as measured by the UAS/LAS Score metric when compared to the best performing baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.483.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.483.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--483 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.483 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.483/>Perceptions of Linguistic Uncertainty by Language Models and Humans</a></strong><br><a href=/people/c/catarina-g-belem/>Catarina G Belém</a>
|
<a href=/people/m/markelle-kelly/>Markelle Kelly</a>
|
<a href=/people/m/mark-steyvers/>Mark Steyvers</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/p/padhraic-smyth/>Padhraic Smyth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--483><div class="card-body p-3 small">*Uncertainty expressions* such as ‘probably’ or ‘highly unlikely’ are pervasive in human language. While prior work has established that there is population-level agreement in terms of how humans quantitatively interpret these expressions, there has been little inquiry into the abilities of language models in the same context. In this paper, we investigate how language models map linguistic expressions of uncertainty to numerical responses. Our approach assesses whether language models can employ theory of mind in this setting: understanding the uncertainty of another agent about a particular statement, independently of the model’s own certainty about that statement. We find that 7 out of 10 models are able to map uncertainty expressions to probabilistic responses in a human-like manner. However, we observe systematically different behavior depending on whether a statement is actually true or false. This sensitivity indicates that language models are substantially more susceptible to bias based on their prior knowledge (as compared to humans). These findings raise important questions and have broad implications for human-AI and AI-AI communication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.484.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.484.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--484 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.484 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.484/>Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical <span class=acl-fixed-case>LM</span></a></strong><br><a href=/people/h/haw-shiuan-chang/>Haw-Shiuan Chang</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/a/anil-ramakrishna/>Anil Ramakrishna</a>
|
<a href=/people/t/tagyoung-chung/>Tagyoung Chung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--484><div class="card-body p-3 small">Contrastive decoding (CD) (Li et al., 2022) improves the next-token distribution of a large expert language model (LM) using a small amateur LM. Although CD is applied to various LMs and domains to enhance open-ended text generation, it is still unclear why CD often works well, when it could fail, and how we can make it better. To deepen our understanding of CD, we first theoretically prove that CD could be viewed as linearly extrapolating the next-token logits from a huge and hypothetical LM. We also highlight that the linear extrapolation could make CD unable to output the most obvious answers that have already been assigned high probabilities by the amateur LM.To overcome CD’s limitation, we propose a new unsupervised decoding method called Asymptotic Probability Decoding (APD). APD explicitly extrapolates the probability curves from the LMs of different sizes to infer the asymptotic probabilities from an infinitely large LM without inducing more inference costs than CD. In FactualityPrompts, an open-ended text generation benchmark, sampling using APD significantly boosts factuality in comparison to the CD sampling and its variants, and achieves state-of-the-art results for Pythia 6.9B and OPT 6.7B. Furthermore, in five commonsense QA datasets, APD is often significantly better than CD and achieves a similar effect of using a larger LLM. For example, the perplexity of APD on top of Pythia 6.9B is even lower than the perplexity of Pythia 12B in CommonsenseQA and LAMBADA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.485.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.485.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--485 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.485 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.485/>Zero-shot Cross-domain Dialogue State Tracking via Context-aware Auto-prompting and Instruction-following Contrastive Decoding</a></strong><br><a href=/people/x/xiaoyu-dong/>Xiaoyu Dong</a>
|
<a href=/people/y/yujie-feng/>Yujie Feng</a>
|
<a href=/people/z/zexin-lu/>Zexin Lu</a>
|
<a href=/people/g/guangyuan-shi/>Guangyuan Shi</a>
|
<a href=/people/x/xiao-ming-wu/>Xiao-Ming Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--485><div class="card-body p-3 small">Zero-shot cross-domain dialogue state tracking (DST) enables us to manage task-oriented dialogues in new, unseen domains without the cost of collecting in-domain data. Previous studies have implemented slot-based input improvements, such as schema-driven descriptions and question-answering formats, but still suffer from negative transfer for seen slots and inefficient transfer for unseen slots due to the significant source-target domain gap. To address these issues, we introduce a novel framework called Context-aware Auto-prompting and Instruction-following Contrastive Decoding (CAPID). This framework generates dynamic, context-aware slot queries, effectively improving the model’s transferability. Our context-aware auto-prompting approach tailors slot queries to the current dialogue context, increasing flexibility and reducing ambiguities. Additionally, an instruction-following contrastive decoding strategy helps reduce errors related to off-topic slots by penalizing deviations from the provided instructions. Extensive experiments on two datasets, with varying model sizes (from 60M to 7B), demonstrate the superior performance of CAPID. The source code is provided for reproducibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.486.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.486.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--486 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.486 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.486/>Knowledge Conflicts for <span class=acl-fixed-case>LLM</span>s: A Survey</a></strong><br><a href=/people/r/rongwu-xu/>Rongwu Xu</a>
|
<a href=/people/z/zehan-qi/>Zehan Qi</a>
|
<a href=/people/z/zhijiang-guo/>Zhijiang Guo</a>
|
<a href=/people/c/cunxiang-wang/>Cunxiang Wang</a>
|
<a href=/people/h/hongru-wang/>Hongru Wang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--486><div class="card-body p-3 small">This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.487.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.487.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--487 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.487 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.487/><span class=acl-fixed-case>M</span>isinfo<span class=acl-fixed-case>E</span>val: Generative <span class=acl-fixed-case>AI</span> in the Era of “Alternative Facts”</a></strong><br><a href=/people/s/saadia-gabriel/>Saadia Gabriel</a>
|
<a href=/people/l/liang-lyu/>Liang Lyu</a>
|
<a href=/people/j/james-siderius/>James Siderius</a>
|
<a href=/people/m/marzyeh-ghassemi/>Marzyeh Ghassemi</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/a/asuman-e-ozdaglar/>Asuman E. Ozdaglar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--487><div class="card-body p-3 small">The spread of misinformation on social media platforms threatens democratic processes, contributes to massive economic losses, and endangers public health. Many efforts to address misinformation focus on a knowledge deficit model and propose interventions for improving users’ critical thinking through access to facts. Such efforts are often hampered by challenges with scalability, and by platform users’ personal biases. The emergence of generative AI presents promising opportunities for countering misinformation at scale across ideological barriers. In this paper, we introduce a framework (MisinfoEval) for generating and comprehensively evaluating large language model (LLM) based misinformation interventions. We present (1) an experiment with a simulated social media environment to measure effectiveness of misinformation interventions, and (2) a second experiment with personalized explanations tailored to the demographics and beliefs of users with the goal of countering misinformation by appealing to their pre-existing values. Our findings confirm that LLM-based interventions are highly effective at correcting user behavior (improving overall user accuracy at reliability labeling by up to 41.72%). Furthermore, we find that users favor more personalized interventions when making decisions about news reliability and users shown personalized interventions have significantly higher accuracy at identifying misinformation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.488.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.488.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--488 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.488 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.488/><span class=acl-fixed-case>MEANT</span>: Multimodal Encoder for Antecedent Information</a></strong><br><a href=/people/b/benjamin-irving/>Benjamin Irving</a>
|
<a href=/people/a/annika-marie-schoene/>Annika Marie Schoene</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--488><div class="card-body p-3 small">The stock market provides a rich well of information that can be split across modalities, making it an ideal candidate for multimodal evaluation. Multimodal data plays an increasingly important role in the development of machine learning and has shown to positively impact performance. But information can do more than exist across modes— it can exist across time. How should we attend to temporal data that consists of multiple information types? This work introduces (i) the MEANT model, a Multimodal Encoder for Antecedent information and (ii) a new dataset called TempStock, which consists of price, Tweets, and graphical data with over a million Tweets from all of the companies in the S&amp;P 500 Index. We find that MEANT improves performance on existing baselines by over 15%, and that the textual information affects performance far more than the visual information on our time-dependent task from our ablation study. The code and dataset will be made available upon publication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.489.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.489.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--489 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.489 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.489.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.489/>A Thorough Examination of Decoding Methods in the Era of <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/c/chufan-shi/>Chufan Shi</a>
|
<a href=/people/h/haoran-yang/>Haoran Yang</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/y/yifan-wang/>Yifan Wang</a>
|
<a href=/people/y/yujiu-yang/>Yujiu Yang</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--489><div class="card-body p-3 small">Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.490.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.490.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--490 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.490 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.490/><span class=acl-fixed-case>AGR</span>a<span class=acl-fixed-case>ME</span>: Any-Granularity Ranking with Multi-Vector Embeddings</a></strong><br><a href=/people/r/revanth-gangi-reddy/>Revanth Gangi Reddy</a>
|
<a href=/people/o/omar-attia/>Omar Attia</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/s/saloni-potdar/>Saloni Potdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--490><div class="card-body p-3 small">Ranking is a fundamental problem in search, however, existing ranking algorithms usually restrict the granularity of ranking to full passages or require a specific dense index for each desired level of granularity. Such lack of flexibility in granularity negatively affects many applications that can benefit from more granular ranking, such as sentence-level ranking for open-domain QA, or proposition-level ranking for attribution. In this work, we introduce the idea of any-granularity ranking which leverages multi-vector embeddings to rank at varying levels of granularity while maintaining encoding at a single (coarser) level of granularity. We propose a multi-granular contrastive loss for training multi-vector approaches and validate its utility with both sentences and propositions as ranking units. Finally, we demonstrate the application of proposition-level ranking to post-hoc citation addition in retrieval-augmented generation, surpassing the performance of prompt-driven citation generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.491.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.491.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--491 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.491 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.491/><span class=acl-fixed-case>FIRST</span>: Faster Improved Listwise Reranking with Single Token Decoding</a></strong><br><a href=/people/r/revanth-gangi-reddy/>Revanth Gangi Reddy</a>
|
<a href=/people/j/jaehyeok-doo/>JaeHyeok Doo</a>
|
<a href=/people/y/yifei-xu/>Yifei Xu</a>
|
<a href=/people/m/md-arafat-sultan/>Md Arafat Sultan</a>
|
<a href=/people/d/deevya-swain/>Deevya Swain</a>
|
<a href=/people/a/avirup-sil/>Avirup Sil</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--491><div class="card-body p-3 small">Large Language Models (LLMs) have significantly advanced the field of information retrieval, particularly for reranking. Listwise LLM rerankers have showcased superior performance and generalizability compared to existing supervised approaches. However, conventional listwise LLM reranking methods lack efficiency as they provide ranking output in the form of a generated ordered sequence of candidate passage identifiers. Further, they are trained with the typical language modeling objective, which treats all ranking errors uniformly–potentially at the cost of misranking highly relevant passages. Addressing these limitations, we introduce FIRST, a novel listwise LLM reranking approach leveraging the output logits of the first generated identifier to directly obtain a ranked ordering of the candidates. Further, we incorporate a learning-to-rank loss during training, prioritizing ranking accuracy for the more relevant passages. Empirical results demonstrate that FIRST accelerates inference by 50% while maintaining a robust ranking performance with gains across the BEIR benchmark. Finally, to illustrate the practical effectiveness of listwise LLM rerankers, we investigate their application in providing relevance feedback for retrievers during inference. Our results show that LLM rerankers can provide a stronger distillation signal compared to cross-encoders, yielding substantial improvements in retriever recall after relevance feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.492.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.492.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--492 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.492 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.492/>Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights</a></strong><br><a href=/people/h/hongjin-kim/>Hongjin Kim</a>
|
<a href=/people/j/jai-eun-kim/>Jai-Eun Kim</a>
|
<a href=/people/h/harksoo-kim/>Harksoo Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--492><div class="card-body p-3 small">Nested Named Entity Recognition (NER) poses a significant challenge in Natural Language Processing (NLP), demanding sophisticated techniques to identify entities within entities. This research investigates the application of Large Language Models (LLMs) to nested NER, exploring methodologies from prior work and introducing specific reasoning techniques and instructions to improve LLM efficacy. Through experiments conducted on the ACE 2004, ACE 2005, and GENIA datasets, we evaluate the impact of these approaches on nested NER performance. Results indicate that output format critically influences nested NER performance, methodologies from previous works are less effective, and our nested NER-tailored instructions significantly enhance performance. Additionally, we find that label information and descriptions of nested cases are crucial in eliciting the capabilities of LLMs for nested NER, especially in specific domains (i.e., the GENIA dataset). However, these methods still do not outperform BERT-based models, highlighting the ongoing need for innovative approaches in nested NER with LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.493.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.493.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--493 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.493 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.493/><span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>C</span>a<span class=acl-fixed-case>LL</span>: Membership Inference via Relative Conditional Log-Likelihoods</a></strong><br><a href=/people/r/roy-xie/>Roy Xie</a>
|
<a href=/people/j/junlin-wang/>Junlin Wang</a>
|
<a href=/people/r/ruomin-huang/>Ruomin Huang</a>
|
<a href=/people/m/minxing-zhang/>Minxing Zhang</a>
|
<a href=/people/r/rong-ge/>Rong Ge</a>
|
<a href=/people/j/jian-pei/>Jian Pei</a>
|
<a href=/people/n/neil-zhenqiang-gong/>Neil Zhenqiang Gong</a>
|
<a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--493><div class="card-body p-3 small">The rapid scaling of large language models (LLMs) has raised concerns about the transparency and fair use of the data used in their pretraining. Detecting such content is challenging due to the scale of the data and limited exposure of each instance during training. We propose ReCaLL (Relative Conditional Log-Likelihood), a novel membership inference attack (MIA) to detect LLMs’ pretraining data by leveraging their conditional language modeling capabilities. ReCaLL examines the relative change in conditional log-likelihoods when prefixing target data points with non-member context. Our empirical findings show that conditioning member data on non-member prefixes induces a larger decrease in log-likelihood compared to non-member data. We conduct comprehensive experiments and show that ReCaLL achieves state-of-the-art performance on the WikiMIA dataset, even with random and synthetic prefixes, and can be further improved using an ensemble approach. Moreover, we conduct an in-depth analysis of LLMs’ behavior with different membership contexts, providing insights into how LLMs leverage membership information for effective inference at both the sequence and token level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.494.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.494.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--494 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.494 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.494.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.494.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.494/>“Flex Tape Can’t Fix That”: Bias and Misinformation in Edited Language Models</a></strong><br><a href=/people/k/karina-h-halevy/>Karina H Halevy</a>
|
<a href=/people/a/anna-sotnikova/>Anna Sotnikova</a>
|
<a href=/people/b/badr-alkhamissi/>Badr AlKhamissi</a>
|
<a href=/people/s/syrielle-montariol/>Syrielle Montariol</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--494><div class="card-body p-3 small">Weight-based model editing methods update the parametric knowledge of language models post-training. However, these methods can unintentionally alter unrelated parametric knowledge representations, potentially increasing the risk of harm. In this work, we investigate how weight editing methods unexpectedly amplify model biases after edits. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias amplification of model editing methods for demographic traits such as race, geographic origin, and gender. We use Seesaw-CF to examine the impact of model editing on bias in five large language models. Our results demonstrate that edited models exhibit, to various degrees, more biased behavior for certain demographic groups than before they were edited, specifically becoming less confident in properties for Asian and African subjects. Additionally, editing facts about place of birth, country of citizenship, or gender has particularly negative effects on the model’s knowledge about unrelated properties, such as field of work, a pattern observed across multiple models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.495.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.495.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--495 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.495 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.495.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.495/>Revisiting Who’s Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective</a></strong><br><a href=/people/y/yujian-liu/>Yujian Liu</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a>
|
<a href=/people/t/tommi-jaakkola/>Tommi Jaakkola</a>
|
<a href=/people/s/shiyu-chang/>Shiyu Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--495><div class="card-body p-3 small">This paper investigates Who’s Harry Potter (WHP), a pioneering yet insufficiently understood method for LLM unlearning. We explore it in two steps. First, we introduce a new task of LLM targeted unlearning, where given an unlearning target (e.g., a person) and some unlearning documents, we aim to unlearn only the information about the target, rather than everything in the unlearning documents. We further argue that a successful unlearning should satisfy criteria such as not outputting gibberish, not fabricating facts about the unlearning target, and not releasing factual information under jailbreak attacks. Second, we construct a causal intervention framework for targeted unlearning, where the knowledge of the unlearning target is modeled as a confounder between LLM input and output, and the unlearning process as a deconfounding process. This framework justifies and extends WHP, deriving a simple unlearning algorithm that includes WHP as a special case. Experiments on existing and new datasets show that our approach, without explicitly optimizing for the aforementioned criteria, achieves competitive performance in all of them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.496.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.496.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--496 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.496 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.496.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.496/><span class=acl-fixed-case>LION</span>s: An Empirically Optimized Approach to Align Language Models</a></strong><br><a href=/people/x/xiao-yu/>Xiao Yu</a>
|
<a href=/people/q/qingyang-wu/>Qingyang Wu</a>
|
<a href=/people/y/yu-li/>Yu Li</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--496><div class="card-body p-3 small">Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent works proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring the impact of various design choices throughout the whole training process. We first conduct a rigorous analysis over a three-stage training pipeline consisting of supervised fine-tuning, offline preference learning, and online preference learning. We have found that using techniques like sequence packing, loss masking in SFT, increasing the preference dataset size in DPO, and online DPO training can significantly improve the performance of language models. We then train from Gemma-2b-base and LLama-3-8b-base, and find that our best models exceed the performance of the official instruct models tuned with closed-source data and algorithms. Our code and models can be found at https://github.com/Columbia-NLP-Lab/LionAlignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.497.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.497.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--497 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.497 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.497.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.497/>Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing</a></strong><br><a href=/people/h/haochen-zhang/>Haochen Zhang</a>
|
<a href=/people/y/yuyang-dong/>Yuyang Dong</a>
|
<a href=/people/c/chuan-xiao/>Chuan Xiao</a>
|
<a href=/people/m/masafumi-oyamada/>Masafumi Oyamada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--497><div class="card-body p-3 small">This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format. We instruction-tune local LLMs as universal DP task solvers that operate on a local, single, and low-priced GPU, ensuring data security and enabling further customization. We select a collection of datasets across four representative DP tasks and construct instruction data using data configuration, knowledge injection, and reasoning data distillation techniques tailored to DP. By tuning Mistral-7B, Llama 3-8B, and OpenOrca-Platypus2-13B, our models, Jellyfish-7B/8B/13B, deliver competitiveness compared to GPT-3.5/4 models and strong generalizability to unseen tasks while barely compromising the base models’ abilities in NLP tasks. Meanwhile, Jellyfish offers enhanced reasoning capabilities compared to GPT-3.5. Our models are available at: https://huggingface.co/NECOUDBFM/JellyfishOur instruction dataset is available at: https://huggingface.co/datasets/NECOUDBFM/Jellyfish-Instruct</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.498.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.498.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--498 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.498 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.498/>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</a></strong><br><a href=/people/y/yu-zhang/>Yu Zhang</a>
|
<a href=/people/x/xiusi-chen/>Xiusi Chen</a>
|
<a href=/people/b/bowen-jin/>Bowen Jin</a>
|
<a href=/people/s/sheng-wang/>Sheng Wang</a>
|
<a href=/people/s/shuiwang-ji/>Shuiwang Ji</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--498><div class="card-body p-3 small">In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we comprehensively survey over 260 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality. Moreover, we investigate how LLMs have been deployed to benefit scientific discovery. Resources related to this survey are available at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.499.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.499.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--499 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.499 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.499/><span class=acl-fixed-case>M</span>ini<span class=acl-fixed-case>C</span>heck: Efficient Fact-Checking of <span class=acl-fixed-case>LLM</span>s on Grounding Documents</a></strong><br><a href=/people/l/liyan-tang/>Liyan Tang</a>
|
<a href=/people/p/philippe-laban/>Philippe Laban</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--499><div class="card-body p-3 small">Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of fact-checking are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to a model to check a single response. In this work, we show how to build small fact-checking models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify datasets from recent work on fact-checking and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.500.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--500 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.500 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.500/>Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning</a></strong><br><a href=/people/j/john-wu/>John Wu</a>
|
<a href=/people/d/david-wu/>David Wu</a>
|
<a href=/people/j/jimeng-sun/>Jimeng Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--500><div class="card-body p-3 small">Medical coding, the translation of unstructured clinical text into standardized medical codes, is a crucial but time-consuming healthcare practice. Though large language models (LLM) could automate the coding process and improve the efficiency of such tasks, interpretability remains paramount for maintaining patient trust. Current efforts in interpretability of medical coding applications rely heavily on label attention mechanisms, which often leads to the highlighting of extraneous tokens irrelevant to the ICD code. To facilitate accurate interpretability in medical language models, this paper leverages dictionary learning that can efficiently extract sparsely activated representations from dense language model embeddings in superposition. Compared with common label attention mechanisms, our model goes beyond token-level representations by building an interpretable dictionary which enhances the mechanistic-based explanations for each ICD code prediction, even when the highlighted tokens are medically irrelevant. We show that dictionary features are human interpretable, can elucidate the hidden meanings of upwards of 90% of medically irrelevant tokens, and steer model behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.501.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--501 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.501/><span class=acl-fixed-case>MOSEL</span>: Inference Serving Using Dynamic Modality Selection</a></strong><br><a href=/people/b/bodun-hu/>Bodun Hu</a>
|
<a href=/people/l/le-xu/>Le Xu</a>
|
<a href=/people/j/jeongyoon-moon/>Jeongyoon Moon</a>
|
<a href=/people/n/neeraja-j-yadwadkar/>Neeraja J Yadwadkar</a>
|
<a href=/people/a/aditya-akella/>Aditya Akella</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--501><div class="card-body p-3 small">Rapid advancements over the years have helped machine learning models reach previously hard-to-achieve goals, sometimes even exceeding human capabilities. However, achieving desired accuracy comes at the cost of larger model sizes and increased computational demands. Thus, serving predictions from these models to meet any latency and cost requirements of applications remains a key challenge, despite recent work in building inference serving systems as well as algorithmic approaches that dynamically adapt models based on inputs. Our paper introduces a new form of dynamism, modality selection, where we adaptively choose modalities from inference inputs while maintaining the model quality. We introduce MOSEL, an automated inference serving system for multi-modal ML models that carefully picks input modalities per request based on user-defined performance and accuracy requirements. MOSEL exploits modality configurations extensively, improving system throughput by 3.6 <span class=tex-math>×</span> with an accuracy guarantee. It also reduces job completion times by 11<span class=tex-math>×</span> compared to modality-agnostic approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.502.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--502 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.502/>From <span class=acl-fixed-case>RAG</span> to Riches: Retrieval Interlaced with Sequence Generation</a></strong><br><a href=/people/p/palak-jain/>Palak Jain</a>
|
<a href=/people/l/livio-baldini-soares/>Livio Baldini Soares</a>
|
<a href=/people/t/tom-kwiatkowski/>Tom Kwiatkowski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--502><div class="card-body p-3 small">We present RICHES, a novel approach that interleaves retrieval with sequence generation tasks. RICHES offers an alternative to conventional RAG systems by eliminating the need for separate retriever and generator. It retrieves documents by directly decoding their contents, constrained on the corpus. Unifying retrieval with generation allows us to adapt to diverse new tasks via prompting alone. RICHES can work with any Instruction-tuned model, without additional training. It provides attributed evidence, supports multi-hop retrievals and interleaves thoughts to plan on what to retrieve next, all within a single decoding pass of the LLM. We demonstrate the strong performance of RICHES across ODQA tasks including attributed and multi-hop QA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.503.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--503 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.503.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.503/>Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition</a></strong><br><a href=/people/h/hsuan-su/>Hsuan Su</a>
|
<a href=/people/h/hua-farn/>Hua Farn</a>
|
<a href=/people/f/fan-yun-sun/>Fan-Yun Sun</a>
|
<a href=/people/s/shang-tse-chen/>Shang-Tse Chen</a>
|
<a href=/people/h/hung-yi-lee/>Hung-yi Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--503><div class="card-body p-3 small">Synthetic data is widely used in speech recognition due to the availability of text-to-speech models, which facilitate adapting models to previously unseen text domains. However, existing methods suffer in performance when they fine-tune an automatic speech recognition (ASR) model on synthetic data as they suffer from the distributional shift commonly referred to as the synthetic-to-real gap. In this paper, we find that task arithmetic is effective at mitigating this gap. Our proposed method, <span class=tex-math>SYN2REAL</span> task vector, shows an average improvement of 10.03% improvement in word error rate over baselines on the SLURP dataset. Additionally, we show that an average of <span class=tex-math>SYN2REAL</span> task vectors, when we have real speeches from multiple different domains, can further adapt the original ASR model to perform better on the target text domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.504.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--504 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.504/>Learning to Correct for <span class=acl-fixed-case>QA</span> Reasoning with Black-box <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/j/jaehyung-kim/>Jaehyung Kim</a>
|
<a href=/people/d/dongyoung-kim/>Dongyoung Kim</a>
|
<a href=/people/y/yiming-yang/>Yiming Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--504><div class="card-body p-3 small">An open challenge in recent machine learning is about how to improve the reasoning capability of large language models (LLMs) in a black-box setting, i.e., without access to detailed information such as output token probabilities. Existing approaches either rely on accessibility (which is often unrealistic) or involve significantly increased train- and inference-time costs. This paper addresses those limitations or shortcomings by proposing a novel approach, namely CoBB (Correct for improving QA reasoning of Black-Box LLMs). It uses a trained adaptation model to perform a seq2seq mapping from the often-imperfect reasonings of the original black-box LLM to the correct or improved reasonings. Specifically, the adaptation model is initialized with a relatively small open-source LLM and adapted over a collection of sub-sampled training pairs. To select the representative pairs of correct and incorrect reasonings, we formulated the dataset construction as an optimization problem that minimizes the statistical divergence between the sampled subset and the entire collection, and solved it via a genetic algorithm. We then train the adaptation model over the sampled pairs by contrasting the likelihoods of correct and incorrect reasonings. Our experimental results demonstrate that CoBB significantly improves reasoning accuracy across various QA benchmarks, compared to the best-performing adaptation baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.505.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--505 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.505/><span class=acl-fixed-case>A</span>ssistant<span class=acl-fixed-case>B</span>ench: Can Web Agents Solve Realistic and Time-Consuming Tasks?</a></strong><br><a href=/people/o/ori-yoran/>Ori Yoran</a>
|
<a href=/people/s/samuel-joseph-amouyal/>Samuel Joseph Amouyal</a>
|
<a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/b/ben-bogin/>Ben Bogin</a>
|
<a href=/people/o/ofir-press/>Ofir Press</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--505><div class="card-body p-3 small">Language agents, built on top of language models (LMs), are systems that can interact with complex environments, such as the open web. In this work, we examine whether such agents can perform realistic and time-consuming tasks on the web, e.g., monitoring real-estate markets or locating relevant nearby businesses. We introduce AssistantBench, a challenging new benchmark consisting of 214 realistic tasks that can be automatically evaluated, covering different scenarios and domains. We find that AssistantBench exposes the limitations of current systems, including language models and retrieval-augmented language models, as no model reaches an accuracy of more than 25 points. While closed-book LMs perform well in terms of accuracy, they exhibit low precision and tend to hallucinate facts. State-of-the-art web agents reach a score of near zero. Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly outperforms previous agents, and an ensemble of SPA and closed-book models reaches the best overall performance. Moreover, we analyze failures of current systems and highlight that open web navigation remains a major challenge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.506.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--506 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.506/><span class=acl-fixed-case>P</span>ost<span class=acl-fixed-case>M</span>ark: A Robust Blackbox Watermark for Large Language Models</a></strong><br><a href=/people/y/yapei-chang/>Yapei Chang</a>
|
<a href=/people/k/kalpesh-krishna/>Kalpesh Krishna</a>
|
<a href=/people/a/amir-houmansadr/>Amir Houmansadr</a>
|
<a href=/people/j/john-frederick-wieting/>John Frederick Wieting</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--506><div class="card-body p-3 small">The most effective techniques to detect LLM-generated text rely on inserting a detectable signature—or watermark—during the model’s decoding process. Most existing watermarking methods require access to the underlying LLM’s logits, which LLM API providers are loath to share due to fears of model distillation. As such, these watermarks must be implemented independently by each LLM provider. In this paper, we develop PostMark, a modular post-hoc watermarking procedure in which an input-dependent set of words (determined via a semantic embedding) is inserted into the text after the decoding process has completed. Critically, PostMark does not require logit access, which means it can be implemented by a third party. We also show that PostMark is more robust to paraphrasing attacks than existing watermarking methods: our experiments cover eight baseline algorithms, five base LLMs, and three datasets. Finally, we evaluate the impact of PostMark on text quality using both automated and human assessments, highlighting the trade-off between quality and robustness to paraphrasing. We release our code, outputs, and annotations at https://github.com/lilakk/PostMark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.507.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--507 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.507/>Assessing “Implicit” Retrieval Robustness of Large Language Models</a></strong><br><a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/r/rexhina-blloshmi/>Rexhina Blloshmi</a>
|
<a href=/people/d/dawei-zhu/>Dawei Zhu</a>
|
<a href=/people/j/jiahuan-pei/>Jiahuan Pei</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--507><div class="card-body p-3 small">Retrieval-augmented generation has gained popularity as a framework to enhance large language models with external knowledge. However, its effectiveness hinges on the retrieval robustness of the model. If the model lacks retrieval robustness, its performance is constrained by the accuracy of the retriever, resulting in significant compromises when the retrieved context is irrelevant. In this paper, we evaluate the “implicit” retrieval robustness of various large language models, instructing them to directly output the final answer without explicitly judging the relevance of the retrieved context. Our findings reveal that fine-tuning on a mix of gold and distracting context significantly enhances the model’s robustness to retrieval inaccuracies, while still maintaining its ability to extract correct answers when retrieval is accurate. This suggests that large language models can implicitly handle relevant or irrelevant retrieved context by learning solely from the supervision of the final answer in an end-to-end manner. Introducing an additional process for explicit relevance judgment can be unnecessary and disrupts the end-to-end approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.508.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--508 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.508/>On the Relationship between Truth and Political Bias in Language Models</a></strong><br><a href=/people/s/suyash-fulay/>Suyash Fulay</a>
|
<a href=/people/w/william-brannon/>William Brannon</a>
|
<a href=/people/s/shrestha-mohanty/>Shrestha Mohanty</a>
|
<a href=/people/c/cassandra-overney/>Cassandra Overney</a>
|
<a href=/people/e/elinor-poole-dayan/>Elinor Poole-Dayan</a>
|
<a href=/people/d/deb-roy/>Deb Roy</a>
|
<a href=/people/j/jad-kabbara/>Jad Kabbara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--508><div class="card-body p-3 small">Language model alignment research often attempts to ensure that models are not only helpful and harmless, but also truthful and unbiased. However, optimizing these objectives simultaneously can obscure how improving one aspect might impact the others. In this work, we focus on analyzing the relationship between two concepts essential in both language model alignment and political science: truthfulness and political bias. We train reward models on various popular truthfulness datasets and subsequently evaluate their political bias. Our findings reveal that optimizing reward models for truthfulness on these datasets tends to result in a left-leaning political bias. We also find that existing open-source reward models (i.e., those trained on standard human preference datasets) already show a similar bias and that the bias is larger for larger models. These results raise important questions about the datasets used to represent truthfulness, potential limitations of aligning models to be both truthful and politically unbiased, and what language models capture about the relationship between truth and politics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.509.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--509 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.509/>Can Active Label Correction Improve <span class=acl-fixed-case>LLM</span>-based Modular <span class=acl-fixed-case>AI</span> Systems?</a></strong><br><a href=/people/k/karan-taneja/>Karan Taneja</a>
|
<a href=/people/a/ashok-goel/>Ashok Goel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--509><div class="card-body p-3 small">Modular AI systems can be developed using LLM-prompts-based modules to minimize deployment time even for complex tasks. However, these systems do not always perform well and improving them using the data traces collected from a deployment remains an open challenge. The data traces contain LLM inputs and outputs, but the annotations from LLMs are noisy. We hypothesize that Active Label Correction (ALC) can be use on the collected data to train smaller task-specific improved models that can replace LLM-based modules. In this paper, we study the noise in three GPT-3.5-annotated datasets and their denoising with human feedback. We also propose a novel method ALC3 that iteratively applies three updates to the training dataset: auto-correction, correction using human feedback and filtering. Our results show that ALC3 can lead to oracle performance with feedback on 17-24% fewer examples than the number of noisy examples in the dataset across three different NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.510.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--510 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.510/>Statistical Uncertainty in Word Embeddings: <span class=acl-fixed-case>G</span>lo<span class=acl-fixed-case>V</span>e-<span class=acl-fixed-case>V</span></a></strong><br><a href=/people/a/andrea-vallebueno/>Andrea Vallebueno</a>
|
<a href=/people/c/cassandra-handan-nader/>Cassandra Handan-Nader</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D Manning</a>
|
<a href=/people/d/daniel-e-ho/>Daniel E. Ho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--510><div class="card-body p-3 small">Static word embeddings are ubiquitous in computational social science applications and contribute to practical decision-making in a variety of fields including law and healthcare. However, assessing the statistical uncertainty in downstream conclusions drawn from word embedding statistics has remained challenging. When using only point estimates for embeddings, researchers have no streamlined way of assessing the degree to which their model selection criteria or scientific conclusions are subject to noise due to sparsity in the underlying data used to generate the embeddings. We introduce a method to obtain approximate, easy-to-use, and scalable reconstruction error variance estimates for GloVe, one of the most widely used word embedding models, using an analytical approximation to a multivariate normal model. To demonstrate the value of embeddings with variance (GloVe-V), we illustrate how our approach enables principled hypothesis testing in core word embedding tasks, such as comparing the similarity between different word pairs in vector space, assessing the performance of different models, and analyzing the relative degree of ethnic or gender bias in a corpus using different word lists.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.511.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--511 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.511.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.511/>Annotation alignment: Comparing <span class=acl-fixed-case>LLM</span> and human annotations of conversational safety</a></strong><br><a href=/people/r/rajiv-movva/>Rajiv Movva</a>
|
<a href=/people/p/pang-wei-koh/>Pang Wei Koh</a>
|
<a href=/people/e/emma-pierson/>Emma Pierson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--511><div class="card-body p-3 small">Do LLMs align with human perceptions of safety? We study this question via *annotation alignment*, the extent to which LLMs and humans agree when annotating the safety of user-chatbot conversations. We leverage the recent DICES dataset (Aroyo et al. 2023), in which 350 conversations are each rated for safety by 112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson correlation of <span class=tex-math>r=0.59</span> with the average annotator rating, higher than the median annotator’s correlation with the average (<span class=tex-math>r=0.51</span>). We show that larger datasets are needed to resolve whether GPT-4 exhibits disparities in how well it correlates with different demographic groups. Also, there is substantial idiosyncratic variation in correlation within groups, suggesting that race & gender do not fully capture differences in alignment. Finally, we find that GPT-4 cannot predict when one demographic group finds a conversation more unsafe than another.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.512.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--512 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.512/><span class=acl-fixed-case>D</span>i<span class=acl-fixed-case>VERT</span>: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions</a></strong><br><a href=/people/n/nigel-fernandez/>Nigel Fernandez</a>
|
<a href=/people/a/alexander-scarlatos/>Alexander Scarlatos</a>
|
<a href=/people/w/wanyong-feng/>Wanyong Feng</a>
|
<a href=/people/s/simon-woodhead/>Simon Woodhead</a>
|
<a href=/people/a/andrew-lan/>Andrew Lan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--512><div class="card-body p-3 small">High-quality distractors are crucial to both the assessment and pedagogical value of multiple-choice questions (MCQs), where manually crafting ones that anticipate knowledge deficiencies or misconceptions among real students is difficult. Meanwhile, automated distractor generation, even with the help of large language models (LLMs), remains challenging for subjects like math. It is crucial to not only identify plausible distractors but also understand the error behind them. In this paper, we introduce DiVERT (Distractor Generation with Variational Errors Represented as Text), a novel variational approach that learns an interpretable representation of errors behind distractors in math MCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions used by hundreds of thousands of students, we show that DiVERT, despite using a base open-source LLM with 7B parameters, outperforms state-of-the-art approaches using GPT-4o on downstream distractor generation. We also conduct a human evaluation with math educators and find that DiVERT leads to error labels that are of comparable quality to human-authored ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.513.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--513 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.513/>The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention</a></strong><br><a href=/people/y/yixin-wan/>Yixin Wan</a>
|
<a href=/people/d/di-wu/>Di Wu</a>
|
<a href=/people/h/haoran-wang/>Haoran Wang</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--513><div class="card-body p-3 small">Prompt-based “diversity interventions” are commonly adopted to improve the diversity of Text-to-Image (T2I) models depicting individuals with various racial or gender traits. However, will this strategy result in nonfactual demographic distribution, especially when generating real historical figures? In this work, we propose **DemOgraphic FActualIty Representation (DoFaiR)**, a benchmark to systematically quantify the trade-off between using diversity interventions and preserving demographic factuality in T2I models. DoFaiR consists of 756 meticulously fact-checked test instances to reveal the factuality tax of various diversity prompts through an automated evidence-supported evaluation pipeline. Experiments on DoFaiR unveil that diversity-oriented instructions increase the number of different gender and racial groups in DALLE-3’s generations at the cost of historically inaccurate demographic distributions. To resolve this issue, we propose **Fact-Augmented Intervention** (FAI), which instructs a Large Language Model (LLM) to reflect on verbalized or retrieved factual information about gender and racial compositions of generation subjects in history, and incorporate it into the generation context of T2I models. By orienting model generations using the reflected historical truths, FAI significantly improves the demographic factuality under diversity interventions while preserving diversity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.514.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--514 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.514/><span class=acl-fixed-case>C</span>lean<span class=acl-fixed-case>G</span>en: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models</a></strong><br><a href=/people/y/yuetai-li/>Yuetai Li</a>
|
<a href=/people/z/zhangchen-xu/>Zhangchen Xu</a>
|
<a href=/people/f/fengqing-jiang/>Fengqing Jiang</a>
|
<a href=/people/l/luyao-niu/>Luyao Niu</a>
|
<a href=/people/d/dinuka-sahabandu/>Dinuka Sahabandu</a>
|
<a href=/people/b/bhaskar-ramasubramanian/>Bhaskar Ramasubramanian</a>
|
<a href=/people/r/radha-poovendran/>Radha Poovendran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--514><div class="card-body p-3 small">The remarkable performance of large language models (LLMs) in generation tasks has enabled practitioners to leverage publicly available models to power custom applications, such as chatbots and virtual assistants. However, the data used to train or fine-tune these LLMs is often undisclosed, allowing an attacker to compromise the data and inject backdoors into the models. In this paper, we develop a novel inference time defense, named CleanGen, to mitigate backdoor attacks for generation tasks in LLMs. CleanGen is a lightweight and effective decoding strategy that is compatible with the state-of-the-art (SOTA) LLMs. Our insight behind CleanGen is that compared to other LLMs, backdoored LLMs assign significantly higher probabilities to tokens representing the attacker-desired contents. These discrepancies in token probabilities enable CleanGen to identify suspicious tokens favored by the attacker and replace them with tokens generated by another LLM that is not compromised by the same attacker, thereby avoiding generation of attacker-desired content. We evaluate CleanGen against five SOTA backdoor attacks. Our results show that CleanGen achieves lower attack success rates (ASR) compared to five SOTA baseline defenses for all five backdoor attacks. Moreover, LLMs deploying CleanGen maintain helpfulness in their responses when serving benign user queries with minimal added computational overhead.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.515.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--515 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.515/>Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic</a></strong><br><a href=/people/m/meng-cao/>Meng Cao</a>
|
<a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/l/lei-yu/>Lei Yu</a>
|
<a href=/people/y/yun-zhu/>Yun Zhu</a>
|
<a href=/people/n/nevan-wichers/>Nevan Wichers</a>
|
<a href=/people/y/yinxiao-liu/>Yinxiao Liu</a>
|
<a href=/people/l/lei-meng/>Lei Meng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--515><div class="card-body p-3 small">Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model, supported by both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.516.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--516 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.516/>Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models</a></strong><br><a href=/people/l/layla-bouzoubaa/>Layla Bouzoubaa</a>
|
<a href=/people/e/elham-aghakhani/>Elham Aghakhani</a>
|
<a href=/people/r/rezvaneh-rezapour/>Rezvaneh Rezapour</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--516><div class="card-body p-3 small">Stigma is a barrier to treatment for individuals struggling with substance use disorders (SUD), which leads to significantly lower treatment engagement rates. With only 7% of those affected receiving any form of help, societal stigma not only discourages individuals with SUD from seeking help but isolates them, hindering their recovery journey and perpetuating a cycle of shame and self-doubt. This study investigates how stigma manifests on social media, particularly Reddit, where anonymity can exacerbate discriminatory behaviors. We analyzed over 1.2 million posts, identifying 3,207 that exhibited stigmatizing language related to people who use substances (PWUS). Of these, 1,649 posts were classified as containing directed stigma towards PWUS, which became the focus of our de-stigmatization efforts. Using Informed and Stylized LLMs, we developed a model to transform these instances into more empathetic language.Our paper contributes to the field by proposing a computational framework for analyzing stigma and de-stigmatizing online content, and delving into the linguistic features that propagate stigma towards PWUS. Our work not only enhances understanding of stigma’s manifestations online but also provides practical tools for fostering a more supportive environment for those affected by SUD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.517.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.517.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--517 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.517 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.517.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.517.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.517/>Efficient Sequential Decision Making with Large Language Models</a></strong><br><a href=/people/d/dingyang-chen/>Dingyang Chen</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/y/yinglun-zhu/>Yinglun Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--517><div class="card-body p-3 small">This paper focuses on extending the success of large language models (LLMs) to sequential decision making. Existing efforts either (i) re-train or finetune LLMs for decision making, or (ii) design prompts for pretrained LLMs. The former approach suffers from the computational burden of gradient updates, and the latter approach does not show promising results. In this paper, we propose a new approach that leverages online model selection algorithms to efficiently incorporate LLMs agents into sequential decision making. Statistically, our approach significantly outperforms both traditional decision making algorithms and vanilla LLM agents. Computationally, our approach avoids the need for expensive gradient updates of LLMs, and throughout the decision making process, it requires only a small number of LLM calls. We conduct extensive experiments to verify the effectiveness of our proposed approach. As an example, on a large-scale Amazon dataset, our approach achieves more than a 6x performance gain over baselines while calling LLMs in only 1.5% of the time steps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.518.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--518 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.518/><span class=acl-fixed-case>S</span>ign<span class=acl-fixed-case>CLIP</span>: Connecting Text and Sign Language by Contrastive Learning</a></strong><br><a href=/people/z/zifan-jiang/>Zifan Jiang</a>
|
<a href=/people/g/gerard-sant/>Gerard Sant</a>
|
<a href=/people/a/amit-moryossef/>Amit Moryossef</a>
|
<a href=/people/m/mathias-muller/>Mathias Müller</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/s/sarah-ebling/>Sarah Ebling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--518><div class="card-body p-3 small">We present SignCLIP, which re-purposes CLIP (Contrastive Language-Image Pretraining) to project spoken language text and sign language videos, two classes of natural languages of distinct modalities, into the same space. SignCLIP is an efficient method of learning useful visual representations for sign language processing from large-scale, multilingual video-text pairs, without directly optimizing for a specific task or sign language which is often of limited size.We pretrain SignCLIP on Spreadthesign, a prominent sign language dictionary consisting of ~500 thousand video clips in up to 44 sign languages, and evaluate it with various downstream datasets. SignCLIP discerns in-domain signing with notable text-to-video/video-to-text retrieval accuracy. It also performs competitively for out-of-domain downstream tasks such as isolated sign language recognition upon essential few-shot prompting or fine-tuning.We analyze the latent space formed by the spoken language text and sign language poses, which provides additional linguistic insights. Our code and models are openly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.519.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.519.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--519 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.519 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.519/><span class=acl-fixed-case>APPLS</span>: Evaluating Evaluation Metrics for Plain Language Summarization</a></strong><br><a href=/people/y/yue-guo/>Yue Guo</a>
|
<a href=/people/t/tal-august/>Tal August</a>
|
<a href=/people/g/gondy-leroy/>Gondy Leroy</a>
|
<a href=/people/t/trevor-cohen/>Trevor Cohen</a>
|
<a href=/people/l/lucy-lu-wang/>Lucy Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--519><div class="card-body p-3 small">While there has been significant development of models for Plain Language Summarization (PLS), evaluation remains a challenge. PLS lacks a dedicated assessment metric, and the suitability of text generation evaluation metrics is unclear due to the unique transformations involved (e.g., adding background explanations, removing jargon). To address these questions, our study introduces a granular meta-evaluation testbed, APPLS, designed to evaluate metrics for PLS. We identify four PLS criteria from previous work—informativeness, simplification, coherence, and faithfulness—and define a set of perturbations corresponding to these criteria that sensitive metrics should be able to detect. We apply these perturbations to extractive hypotheses for two PLS datasets to form our testbed. Using APPLS, we assess performance of 14 metrics, including automated scores, lexical features, and LLM prompt-based evaluations. Our analysis reveals that while some current metrics show sensitivity to specific criteria, no single method captures all four criteria simultaneously. We therefore recommend a suite of automated metrics be used to capture PLS quality along all relevant criteria. This work contributes the first meta-evaluation testbed for PLS and a comprehensive evaluation of existing metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.520.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--520 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.520.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.520/>Ontologically Faithful Generation of Non-Player Character Dialogues</a></strong><br><a href=/people/n/nathaniel-weir/>Nathaniel Weir</a>
|
<a href=/people/r/ryan-thomas/>Ryan Thomas</a>
|
<a href=/people/r/randolph-damore/>Randolph d’Amore</a>
|
<a href=/people/k/kellie-hill/>Kellie Hill</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/h/harsh-jhamtani/>Harsh Jhamtani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--520><div class="card-body p-3 small">We introduce a language generation dataset grounded in a popular video game. KNUDGE (**KN**owledge Constrained **U**ser-NPC **D**ialogue **GE**neration) requires models to produce trees of dialogue between video game characters that accurately reflect quest and entity specifications stated in natural language. KNUDGE is constructed from side quest dialogues drawn directly from game data of Obsidian Entertainment’s _The Outer Worlds_, leading to real-world complexities in generation: (1) utterances must remain faithful to the game lore, including character personas and backstories; (2) a dialogue must accurately reveal new quest details to the human player; and (3) dialogues are large trees as opposed to linear chains of utterances. We report results for a set of neural generation models using supervised and in-context learning techniques; we find competent performance but room for future work addressing the challenges of creating realistic, game-quality dialogues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.521.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.521.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--521 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.521 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.521/><span class=acl-fixed-case>LLM</span> See, <span class=acl-fixed-case>LLM</span> Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives</a></strong><br><a href=/people/l/luisa-shimabucoro/>Luísa Shimabucoro</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/m/marzieh-fadaee/>Marzieh Fadaee</a>
|
<a href=/people/s/sara-hooker/>Sara Hooker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--521><div class="card-body p-3 small">The widespread adoption of synthetic data raises new questions about how models generating the data can influence other large language models (LLMs). To start, our work exhaustively characterizes the impact of passive inheritance of model properties by systematically studying how the source of synthetic data shapes models’ internal biases, calibration and preferences, and their generations’ textual attributes, providing one of the most comprehensive studies to-date. We find that models are surprisingly sensitive towards certain attributes even when the synthetic data prompts appear “neutral” which invites the question: can we explicitly steer the distilled data towards desired properties? We demonstrate how such active inheritance can steer the generation profiles of models towards desirable non-differentiable attributes in both directions, e.g. increasing lexical diversity or reducing toxicity. Overall, our study broadens the understanding of the implicit biases inherited by LLMs and explores how we can leverage them to positive effect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.522.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.522.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--522 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.522 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.522/><span class=acl-fixed-case>R</span>u<span class=acl-fixed-case>BL</span>i<span class=acl-fixed-case>MP</span>: <span class=acl-fixed-case>R</span>ussian Benchmark of Linguistic Minimal Pairs</a></strong><br><a href=/people/e/ekaterina-taktasheva/>Ekaterina Taktasheva</a>
|
<a href=/people/m/maxim-bazhukov/>Maxim Bazhukov</a>
|
<a href=/people/k/kirill-koncha/>Kirill Koncha</a>
|
<a href=/people/a/alena-fenogenova/>Alena Fenogenova</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a>
|
<a href=/people/v/vladislav-mikhailov/>Vladislav Mikhailov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--522><div class="card-body p-3 small">Minimal pairs are a well-established approach to evaluating the grammatical knowledge of language models. However, existing resources for minimal pairs address a limited number of languages and lack diversity of language-specific grammatical phenomena. This paper introduces the Russian Benchmark of Linguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that differ in grammaticality and isolate a morphological, syntactic, or semantic phenomenon. In contrast to existing benchmarks of linguistic minimal pairs, RuBLiMP is created by applying linguistic perturbations to automatically annotated sentences from open text corpora and decontaminating test data. We describe the data collection protocol and present the results of evaluating 25 language models in various scenarios. We find that the widely used LMs for Russian are sensitive to morphological and agreement-oriented contrasts, but fall behind humans on phenomena requiring the understanding of structural relations, negation, transitivity, and tense. RuBLiMP, the codebase, and other materials are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.523.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--523 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.523.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.523/>Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction</a></strong><br><a href=/people/z/zheye-deng/>Zheye Deng</a>
|
<a href=/people/c/chunkit-chan/>Chunkit Chan</a>
|
<a href=/people/w/weiqi-wang/>Weiqi Wang</a>
|
<a href=/people/y/yuxi-sun/>Yuxi Sun</a>
|
<a href=/people/w/wei-fan/>Wei Fan</a>
|
<a href=/people/t/tianshi-zheng/>Tianshi Zheng</a>
|
<a href=/people/y/yauwai-yim/>Yauwai Yim</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--523><div class="card-body p-3 small">The task of condensing large chunks of textual information into concise and structured tables has gained attention recently due to the emergence of Large Language Models (LLMs) and their potential benefit for downstream tasks, such as text summarization and text mining. Previous approaches often generate tables that directly replicate information from the text, limiting their applicability in broader contexts, as text-to-table generation in real-life scenarios necessitates information extraction, reasoning, and integration. However, there is a lack of both datasets and methodologies towards this task. In this paper, we introduce LiveSum, a new benchmark dataset created for generating summary tables of competitions based on real-time commentary texts. We evaluate the performances of state-of-the-art LLMs on this task in both fine-tuning and zero-shot settings, and additionally propose a novel pipeline called <span class=tex-math>T<sup>3</sup></span>(Text-Tuple-Table) to improve their performances. Extensive experimental results demonstrate that LLMs still struggle with this task even after fine-tuning, while our approach can offer substantial performance gains without explicit training. Further analyses demonstrate that our method exhibits strong generalization abilities, surpassing previous approaches on several other text-to-table datasets. Our codeand data can be found at https://github.com/HKUST-KnowComp/LiveSum.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.524.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--524 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.524/>Toward Compositional Behavior in Neural Models: A Survey of Current Views</a></strong><br><a href=/people/k/kate-mccurdy/>Kate McCurdy</a>
|
<a href=/people/p/paul-soulos/>Paul Soulos</a>
|
<a href=/people/p/paul-smolensky/>Paul Smolensky</a>
|
<a href=/people/r/roland-fernandez/>Roland Fernandez</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--524><div class="card-body p-3 small">Compositionality is a core property of natural language, and compositional behavior (CB) is a crucial goal for modern NLP systems. The research literature, however, includes conflicting perspectives on how CB should be defined, evaluated, and achieved. We propose a conceptual framework to address these questions and survey researchers active in this area.We find consensus on several key points. Researchers broadly accept our proposed definition of CB, agree that it is not solved by current models, and doubt that scale alone will achieve the target behavior. In other areas, we find the field is split on how to move forward, identifying diverse opportunities for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.525.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--525 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.525/>Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs</a></strong><br><a href=/people/k/krista-opsahl-ong/>Krista Opsahl-Ong</a>
|
<a href=/people/m/michael-j-ryan/>Michael J Ryan</a>
|
<a href=/people/j/josh-purtell/>Josh Purtell</a>
|
<a href=/people/d/david-broman/>David Broman</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/m/matei-zaharia/>Matei Zaharia</a>
|
<a href=/people/o/omar-khattab/>Omar Khattab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--525><div class="card-body p-3 small">Language Model Programs, i.e. sophisticated pipelines of modular language model (LM) calls, are increasingly advancing NLP tasks, but they require crafting prompts that are jointly effective for all modules. We study prompt optimization for LM programs, i.e. how to update these prompts to maximize a downstream metric without access to module-level labels or gradients. To make this tractable, we factorize our problem into optimizing the free-form instructions and few-shot demonstrations of every module and introduce several strategies to craft task-grounded instructions and navigate credit assignment across modules. Our strategies include (i) program- and data-aware techniques for proposing effective instructions, (ii) a stochastic mini-batch evaluation function for learning a surrogate model of our objective, and (iii) a meta-optimization procedure in which we refine how LMs construct proposals over time. Using these insights we develop MIPRO, a novel algorithm for optimizing LM programs. MIPRO outperforms baseline optimizers on five of seven diverse multi-stage LM programs using a best-in-class open-source model (Llama-3-8B), by as high as 13% accuracy. We have released our new optimizers and benchmark in DSPy at [http://dspy.ai](http://dspy.ai).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.526.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--526 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.526/>Reverse-Engineering the Reader</a></strong><br><a href=/people/s/samuel-kiegeland/>Samuel Kiegeland</a>
|
<a href=/people/e/ethan-wilcox/>Ethan Wilcox</a>
|
<a href=/people/a/afra-amini/>Afra Amini</a>
|
<a href=/people/d/david-robert-reich/>David Robert Reich</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--526><div class="card-body p-3 small">Numerous previous studies have sought to determine to what extent language models, pretrained on natural language text, can serve as useful models of human cognition.In this paper, we are interested in the opposite question: whether we can directly optimize a language model to be a useful cognitive model by aligning it to human psychometric data.To achieve this, we introduce a novel alignment technique in which we fine-tune a language model to implicitly optimize the parameters of a linear regressor that directly predicts humans’ reading times of in-context linguistic units, e.g., phonemes, morphemes, or words, using surprisal estimates derived from the language model. Using words as a test case, we evaluate our technique across multiple model sizes and datasets and find that it improves language models’ psychometric predictive power.However, we find an inverse relationship between psychometric power and a model’s performance on downstream NLP tasks as well as its perplexity on held-out test data.While this latter trend has been observed before (Oh et al., 2022; Shain et al., 2024), we are the first to induce it by manipulating a model’s alignment to psychometric data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.527.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--527 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.527/>Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation</a></strong><br><a href=/people/d/di-wu/>Di Wu</a>
|
<a href=/people/j/jia-chen-gu/>Jia-Chen Gu</a>
|
<a href=/people/f/fan-yin/>Fan Yin</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--527><div class="card-body p-3 small">Retrieval-augmented language models (RALMs) have shown strong performance and wide applicability in knowledge-intensive tasks. However, there are significant trustworthiness concerns as RALMs are prone to generating unfaithful outputs, including baseless information or contradictions with the retrieved context. This paper proposes SynCheck, a lightweight monitor that leverages fine-grained decoding dynamics including sequence likelihood, uncertainty quantification, context influence, and semantic alignment to synchronously detect unfaithful sentences. By integrating efficiently measurable and complementary signals, SynCheck enables accurate and immediate feedback and intervention. Experiments show that SynCheck significantly outperforms existing faithfulness detection baselines, achieving over 0.85 AUROC across a suite of six long-form retrieval-augmented generation tasks. Leveraging SynCheck, we further introduce FOD, a faithfulness-oriented decoding algorithm guided by beam search for long-form retrieval-augmented generation. Empirical results demonstrate that FOD outperforms traditional strategies such as abstention, reranking, or contrastive decoding significantly in terms of faithfulness, achieving over 10% improvement across six datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.528.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.528.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--528 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.528 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.528.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.528/>Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text</a></strong><br><a href=/people/k/kewei-cheng/>Kewei Cheng</a>
|
<a href=/people/n/nesreen-k-ahmed/>Nesreen K. Ahmed</a>
|
<a href=/people/t/theodore-l-willke/>Theodore L. Willke</a>
|
<a href=/people/y/yizhou-sun/>Yizhou Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--528><div class="card-body p-3 small">Although Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors. Firstly, natural language often encompasses complex relationships among entities, making it challenging to maintain a clear reasoning chain over longer spans. Secondly, the abundance of linguistic diversity means that the same entities and relationships can be expressed using different terminologies and structures, complicating the task of identifying and establishing connections between multiple pieces of information. Graphs provide an effective solution to represent data rich in relational information and capture long-term dependencies among entities. To harness the potential of graphs, our paper introduces Structure Guided Prompt, an innovative three-stage task-agnostic prompting framework designed to improve the multi-step reasoning capabilities of LLMs in a zero-shot setting. This framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses. By effectively organizing information and guiding navigation, it enables LLMs to provide more accurate and context-aware responses. Our experiments show that this framework significantly enhances the reasoning capabilities of LLMs, enabling them to excel in a broader spectrum of natural language scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.529.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--529 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.529.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.529.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.529/>Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning</a></strong><br><a href=/people/d/david-schulte/>David Schulte</a>
|
<a href=/people/f/felix-hamborg/>Felix Hamborg</a>
|
<a href=/people/a/alan-akbik/>Alan Akbik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--529><div class="card-body p-3 small">Intermediate task transfer learning can greatly improve model performance. If, for example, one has little training data for emotion detection, first fine-tuning a language model on a sentiment classification dataset may improve performance strongly. But which task to choose for transfer learning? Prior methods producing useful task rankings are infeasible for large source pools, as they require forward passes through all source language models. We overcome this by introducing Embedding Space Maps (ESMs), light-weight neural networks that approximate the effect of fine-tuning a language model. We conduct the largest study on NLP task transferability and task selection with 12k source-target pairs. We find that applying ESMs on a prior method reduces execution time and disk space usage by factors of 10 and 278, respectively, while retaining high selection performance (avg. regret@5 score of 2.95).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.530.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--530 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.530/>The effects of distance on <span class=acl-fixed-case>NPI</span> illusive effects in <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/s/so-young-lee/>So Young Lee</a>
|
<a href=/people/m/mai-ha-vu/>Mai Ha Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--530><div class="card-body p-3 small">Previous studies have examined the syntactic capabilities of large pre-trained language models, such as BERT, by using stimuli from psycholinguistic studies. Studying well-known processing errors, such as NPI illusive effects can reveal whether a model prioritizes linear or hierarchical information when processing language. Recent experiments have found that BERT is mildly susceptible to Negative Polarity Item (NPI) illusion effects (Shin et al., 2023; Vu and Lee, 2022). We expand on these results by examining the effect of distance on the illusive effect, using and modifying stimuli from Parker and Phillips (2016). We also further tease apart whether the model is more affected by hierarchical distance or linear distance. We find that BERT is highly sensitive to syntactic hierarchical information: added hierarchical layers affected its processing capabilities compared to added linear distance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.531.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--531 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.531/>Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic</a></strong><br><a href=/people/n/nathaniel-weir/>Nathaniel Weir</a>
|
<a href=/people/k/kate-sanders/>Kate Sanders</a>
|
<a href=/people/o/orion-weller/>Orion Weller</a>
|
<a href=/people/s/shreya-sharma/>Shreya Sharma</a>
|
<a href=/people/d/dongwei-jiang/>Dongwei Jiang</a>
|
<a href=/people/z/zheng-ping-jiang/>Zhengping Jiang</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi Mishra</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--531><div class="card-body p-3 small">Recent language models enable new opportunities for structured reasoning with text, such as the construction of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what _valid decompositional entailment_ is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic entailment engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment and evaluate its impact on LLM-based textual inference. We find that our new dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of forming a clear protocol for discerning entailment. We also find that training an RDTE-oriented entailment classifier via knowledge distillation and employing it in an entailment tree reasoning engine significantly improves both accuracy and proof quality, illustrating the practical benefit of this advance for textual inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.532.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--532 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.532.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.532/><span class=acl-fixed-case>S</span>usu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between <span class=acl-fixed-case>G</span>hana and the <span class=acl-fixed-case>US</span></a></strong><br><a href=/people/c/christabel-acquaye/>Christabel Acquaye</a>
|
<a href=/people/h/haozhe-an/>Haozhe An</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--532><div class="card-body p-3 small">Recent work has highlighted the culturally-contingent nature of commonsense knowledge. We introduce AMAMMERε, a test set of 525 multiple-choice questions designed to evaluate the commonsense knowledge of English LLMs, relative to the cultural contexts of Ghana and the United States. To create AMAMMERε, we select a set of multiple-choice questions (MCQs) from existing commonsense datasets and rewrite them in a multi-stage process involving surveys of Ghanaian and U.S. participants. In three rounds of surveys, participants from both pools are solicited to (1) write correct and incorrect answer choices, (2) rate individual answer choices on a 5-point Likert scale, and (3) select the best answer choice from the newly-constructed MCQ items, in a final validation step. By engaging participants at multiple stages, our procedure ensures that participant perspectives are incorporated both in the creation and validation of test items, resulting in high levels of agreement within each pool. We evaluate several off-the-shelf English LLMs on AMAMMERε. Uniformly, models prefer answers choices that align with the preferences of U.S. annotators over Ghanaian annotators. Additionally, when test items specify a cultural context (Ghana or the U.S.), models exhibit some ability to adapt, but performance is consistently better in U.S. contexts than Ghanaian. As large resources are devoted to the advancement of English LLMs, our findings underscore the need for culturally adaptable models and evaluations to meet the needs of diverse English-speaking populations around the world.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.533.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--533 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.533/>Read Anywhere Pointed: Layout-aware <span class=acl-fixed-case>GUI</span> Screen Reading with Tree-of-Lens Grounding</a></strong><br><a href=/people/y/yue-fan/>Yue Fan</a>
|
<a href=/people/l/lei-ding/>Lei Ding</a>
|
<a href=/people/c/ching-chen-kuo/>Ching-Chen Kuo</a>
|
<a href=/people/s/shan-jiang/>Shan Jiang</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/x/xinze-guan/>Xinze Guan</a>
|
<a href=/people/j/jie-yang/>Jie Yang</a>
|
<a href=/people/y/yi-zhang/>Yi Zhang</a>
|
<a href=/people/x/xin-eric-wang/>Xin Eric Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--533><div class="card-body p-3 small">Graphical User Interfaces (GUIs) are central to our interaction with digital devices and growing efforts have been made to build models for various GUI understanding tasks. However, these efforts largely overlook an important GUI-referring task: screen reading based on user-indicated points, which we name the Screen Point-and-Read (ScreenPR) task. Currently, this task is predominantly handled by rigid accessible screen reading tools, in great need of new models driven by advancements in Multimodal Large Language Models (MLLMs). In this paper, we propose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism, to address the ScreenPR task. Based on the input point coordinate and the corresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout Tree. Based on the tree, our ToL agent not only comprehends the content of the indicated area but also articulates the layout and spatial relationships between elements. Such layout information is crucial for accurately interpreting information on the screen, distinguishing our ToL agent from other screen reading tools. We also thoroughly evaluate the ToL agent against other baselines on a newly proposed ScreenPR benchmark, which includes GUIs from mobile, web, and operating systems. Last but not least, we test the ToL agent on mobile GUI navigation tasks, demonstrating its utility in identifying incorrect actions along the path of agent execution trajectories. Code and data: https://screen-point-and-read.github.io.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.534.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--534 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.534.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.534.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.534/>Ranking Manipulation for Conversational Search Engines</a></strong><br><a href=/people/s/samuel-pfrommer/>Samuel Pfrommer</a>
|
<a href=/people/y/yatong-bai/>Yatong Bai</a>
|
<a href=/people/t/tanmay-gautam/>Tanmay Gautam</a>
|
<a href=/people/s/somayeh-sojoudi/>Somayeh Sojoudi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--534><div class="card-body p-3 small">Major search engine providers are rapidly incorporating Large Language Model (LLM)-generated content in response to user queries. These *conversational search engines* operate by loading retrieved website text into the LLM context for summarization and interpretation. Recent research demonstrates that LLMs are highly vulnerable to jailbreaking and prompt injection attacks, which disrupt the safety and quality goals of LLMs using adversarial strings. This work investigates the impact of prompt injections on the ranking order of sources referenced by conversational search engines. To this end, we introduce a focused dataset of real-world consumer product websites and formalize conversational search ranking as an adversarial problem. Experimentally, we analyze conversational search rankings in the absence of adversarial injections and show that different LLMs vary significantly in prioritizing product name, document content, and context position. We then present a tree-of-attacks-based jailbreaking technique which reliably promotes low-ranked products. Importantly, these attacks transfer effectively to state-of-the-art conversational search engines such as *perplexity.ai*. Given the strong financial incentive for website owners to boost their search ranking, we argue that our problem formulation is of critical importance for future robustness work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.535.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.535.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--535 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.535 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.535/>Fast Forwarding Low-Rank Training</a></strong><br><a href=/people/a/adir-rahamim/>Adir Rahamim</a>
|
<a href=/people/n/naomi-saphra/>Naomi Saphra</a>
|
<a href=/people/s/sara-kangaslahti/>Sara Kangaslahti</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--535><div class="card-body p-3 small">Parameter efficient finetuning methods like low-rank adaptation (LoRA) aim to reduce the computational costs of finetuning pretrained Language Models (LMs). Enabled by these low-rank settings, we propose an even more efficient optimization strategy: Fast Forward, a simple and effective approach to accelerate large segments of SGD training. In a Fast Forward stage, we repeat the most recent optimizer step until the loss stops improving on a tiny validation set. By alternating between regular optimization steps and Fast Forward stages, Fast Forward provides up to an 87% reduction in FLOPs over standard SGD with Adam. We validate Fast Forward by finetuning various models on different tasks and demonstrate that it speeds up training without compromising model performance. Additionally, we analyze when and how to apply Fast Forward.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.536.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--536 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.536.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.536/>Precise Model Benchmarking with Only a Few Observations</a></strong><br><a href=/people/r/riccardo-fogliato/>Riccardo Fogliato</a>
|
<a href=/people/p/pratik-patil/>Pratik Patil</a>
|
<a href=/people/n/nil-jana-akpinar/>Nil-Jana Akpinar</a>
|
<a href=/people/m/mathew-monfort/>Mathew Monfort</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--536><div class="card-body p-3 small">How can we precisely estimate a large language model’s (LLM) accuracy on questions belonging to a specific topic within a larger question-answering dataset? The standard direct estimator, which averages the model’s accuracy on the questions in each subgroup, may exhibit high variance for subgroups (topics) with small sample sizes. Synthetic regression modeling, which leverages the model’s accuracy on questions about other topics, may yield biased estimates that are too unreliable for large subgroups. We prescribe a simple yet effective solution: an empirical Bayes (EB) estimator that balances direct and regression estimates for each subgroup separately, improving the precision of subgroup-level estimates of model performance. Our experiments on multiple datasets show that this approach consistently provides more precise estimates of the LLM performance compared to the direct and regression approaches, achieving substantial reductions in the mean squared error. Confidence intervals for EB estimates also have near-nominal coverage and are narrower compared to those for the direct estimator. Additional experiments on tabular and vision data validate the benefits of this EB approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.537.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--537 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.537/>Attribute Diversity Determines the Systematicity Gap in <span class=acl-fixed-case>VQA</span></a></strong><br><a href=/people/i/ian-berlot-attwell/>Ian Berlot-Attwell</a>
|
<a href=/people/k/kumar-krishna-agrawal/>Kumar Krishna Agrawal</a>
|
<a href=/people/a/annabelle-michael-carrell/>Annabelle Michael Carrell</a>
|
<a href=/people/y/yash-sharma/>Yash Sharma</a>
|
<a href=/people/n/naomi-saphra/>Naomi Saphra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--537><div class="card-body p-3 small">Although modern neural networks often generalize to new combinations of familiar concepts, the conditions that enable such compositionality have long been an open question. In this work, we study the systematicity gap in visual question answering: the performance difference between reasoning on previously seen and unseen combinations of object attributes. To test, we introduce a novel diagnostic dataset, CLEVR-HOPE. We find that the systematicity gap is not reduced by increasing the quantity of training data, but is reduced by increasing the diversity of training data. In particular, our experiments suggest that the more distinct attribute type combinations are seen during training, the more systematic we can expect the resulting model to be.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.538.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.538.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--538 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.538 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.538/><span class=acl-fixed-case>A</span>rxiv<span class=acl-fixed-case>DIGEST</span>ables: Synthesizing Scientific Literature into Tables using Language Models</a></strong><br><a href=/people/b/benjamin-newman/>Benjamin Newman</a>
|
<a href=/people/y/yoonjoo-lee/>Yoonjoo Lee</a>
|
<a href=/people/a/aakanksha-naik/>Aakanksha Naik</a>
|
<a href=/people/p/pao-siangliulue/>Pao Siangliulue</a>
|
<a href=/people/r/raymond-fok/>Raymond Fok</a>
|
<a href=/people/j/juho-kim/>Juho Kim</a>
|
<a href=/people/d/daniel-s-weld/>Daniel S Weld</a>
|
<a href=/people/j/joseph-chee-chang/>Joseph Chee Chang</a>
|
<a href=/people/k/kyle-lo/>Kyle Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--538><div class="card-body p-3 small">When conducting literature reviews, scientists often create literature review tables—tables whose rows are publications and whose columns constitute a schema, a set of aspects used to compare and contrast the papers. Can we automatically generate these tables using language models (LMs)? In this work, we introduce a framework that leverages LMs to perform this task by decomposing it into separate schema and value generation steps. To enable experimentation, we address two main challenges: First, we overcome a lack of high-quality datasets to benchmark table generation by curating and releasing arxivDIGESTables, a new dataset of 2,228 literature review tables extracted from ArXiv papers that synthesize a total of 7,542 research papers. Second, to support scalable evaluation of model generations against human-authored reference tables, we develop DecontextEval, an automatic evaluation method that aligns elements of tables with the same underlying aspects despite differing surface forms. Given these tools, we evaluate LMs’ abilities to reconstruct reference tables, finding this task benefits from additional context to ground the generation (e.g. table captions, in-text references). Finally, through a human evaluation study we find that even when LMs fail to fully reconstruct a reference table, their generated novel aspects can still be useful.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.539.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.539.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--539 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.539 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.539.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.539.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.539/>Development of Cognitive Intelligence in Pre-trained Language Models</a></strong><br><a href=/people/r/raj-sanjay-shah/>Raj Sanjay Shah</a>
|
<a href=/people/k/khushi-bhardwaj/>Khushi Bhardwaj</a>
|
<a href=/people/s/sashank-varma/>Sashank Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--539><div class="card-body p-3 small">Recent studies show evidence for emergent cognitive abilities in Large Pre-trained Language Models (PLMs). The increasing cognitive alignment of these models has made them candidates for cognitive science theories. Prior research into the emergent cognitive abilities of PLMs has been path-independent to model training, i.e. has only looked at the final model weights and not the intermediate steps. However, building plausible models of human cognition using PLMs also requires aligning their performance during training to the developmental trajectories of children’s thinking. Guided by psychometric tests of human intelligence, we choose four task categories to investigate the alignment of ten popular families of PLMs and evaluate each of their available intermediate and final training steps: Numerical ability, Linguistic abilities, Conceptual understanding, and Fluid reasoning. We find a striking regularity: regardless of model size, the developmental trajectories of PLMs consistently exhibit a window of maximal alignment to human cognitive development. Before that window, training appears to endow models with the requisite structure to be poised to rapidly learn from experience. After that window, training appears to serve the engineering goal of reducing loss but not the scientific goal of increasing alignment with human cognition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.540.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--540 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.540.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.540/>Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding</a></strong><br><a href=/people/c/chong-zhang/>Chong Zhang</a>
|
<a href=/people/y/yi-tu/>Yi Tu</a>
|
<a href=/people/y/yixi-zhao/>Yixi Zhao</a>
|
<a href=/people/c/chenshu-yuan/>Chenshu Yuan</a>
|
<a href=/people/h/huan-chen/>Huan Chen</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/m/mingxu-chai/>Mingxu Chai</a>
|
<a href=/people/y/ya-guo/>Ya Guo</a>
|
<a href=/people/h/huijia-zhu/>Huijia Zhu</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--540><div class="card-body p-3 small">Modeling and leveraging layout reading order in visually-rich documents (VrDs) is critical in document intelligence as it captures the rich structure semantics within documents.Previous works typically formulated layout reading order as a permutation of layout elements, i.e. a sequence containing all the layout elements.However, we argue that this formulation does not adequately convey the complete reading order information in the layout, which may potentially lead to performance decline in downstream tasks.To address this issue, we propose to model the layout reading order as ordering relations over the set of layout elements, which have sufficient expressive capability for the complete reading order information. To enable empirical evaluation on methods towards the improved form of reading order prediction (ROP), we establish a comprehensive benchmark dataset including the reading order annotation as relations over layout elements, together with a relation-extraction-based method that outperforms previous models. Moreover, we propose a reading-order-relation-enhancing pipeline to improve model performance on any arbitrary VrD task by introducing additional reading order relation inputs.We conduct comprehensive experiments to demonstrate that the pipeline generally benefits downstream VrD tasks:(1) with utilizing the reading order relation information, the enhanced downstream models achieve SOTA results on both two task settings of the targeted dataset; (2) with utilizing the pseudo reading order information generated by the proposed ROP model, the performance of the enhanced models has improved across all three models and eight cross-domain VrD-IE/QA task settings without targeted optimization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.541.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--541 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.541/>Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives</a></strong><br><a href=/people/s/sam-blouir/>Sam Blouir</a>
|
<a href=/people/j/jimmy-t-h-smith/>Jimmy T.h. Smith</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/a/amarda-shehu/>Amarda Shehu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--541><div class="card-body p-3 small">Efficient state space models (SSMs), including linear recurrent neural networks and linear attention variants, have emerged as potential alternative language models to Transformers. While efficient, SSMs struggle with tasks requiring in-context retrieval, such as text copying and associative recall, limiting their usefulness in practical settings. Prior work on how to meet this challenge has focused on the internal model architecture and not investigated the role of the training procedure. This paper proposes a new training procedure that improve the performance of SSMs on retrieval-intensive tasks. This novel pre-training procedure combines a bidirectional processing of the input with dynamic mixtures of pre-training objectives to improve the utilization of the SSM’s fixed-size state. Our experimental evaluations show that this procedure significantly improves performance on retrieval-intensive tasks that challenge current SSMs, such as phone book lookup, long paragraph question-answering, and infilling tasks. Our findings offer insights into a new direction to advance the training of SSMs to close the performance gap with Transformers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.542.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.542.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--542 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.542 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.542/>Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?</a></strong><br><a href=/people/p/pinzhen-chen/>Pinzhen Chen</a>
|
<a href=/people/s/simon-yu/>Simon Yu</a>
|
<a href=/people/z/zhicheng-guo-tsinghua/>Zhicheng Guo</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--542><div class="card-body p-3 small">Multilingual large language models are designed, claimed, and expected to cater to speakers of varied languages. We hypothesise that the current practices of fine-tuning and evaluating these models may not perfectly align with this objective owing to a heavy reliance on translation, which cannot cover language-specific knowledge but can introduce translation defects. It remains unknown whether the nature of the instruction data has an impact on the model output; conversely, it is questionable whether translated test sets can capture such nuances. Due to the often coupled practices of using translated data in both stages, such imperfections could have been overlooked. This work investigates these issues using controlled native or translated data during the instruction tuning and evaluation stages. We show that native or generation benchmarks reveal a notable difference between native and translated instruction data especially when model performance is high, whereas other types of test sets cannot. The comparison between round-trip and single-pass translations reflects the importance of knowledge from language-native resources. Finally, we demonstrate that regularization is beneficial to bridging this gap on structured but not generative tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.543.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--543 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.543/>Token Erasure as a Footprint of Implicit Vocabulary Items in <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/s/sheridan-feucht/>Sheridan Feucht</a>
|
<a href=/people/d/david-atkinson/>David Atkinson</a>
|
<a href=/people/b/byron-c-wallace/>Byron C Wallace</a>
|
<a href=/people/d/david-bau/>David Bau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--543><div class="card-body p-3 small">LLMs process text as sequences of tokens that roughly correspond to words, where less common words are represented by multiple tokens. However, individual tokens are often semantically unrelated to the meanings of the words/concepts they comprise. For example, Llama-2-7b’s tokenizer splits the word “patrolling” into two tokens, “pat” and “rolling”, neither of which correspond to semantically meaningful units like “patrol” or "-ing.” Similarly, the overall meanings of named entities like “Neil Young” and multi-word expressions like “break a leg” cannot be directly inferred from their constituent tokens. Mechanistically, how do LLMs convert such arbitrary groups of tokens into useful higher-level representations? In this work, we find that last token representations of named entities and multi-token words exhibit a pronounced “erasure” effect, where information about previous and current tokens is rapidly forgotten in early layers. Using this observation, we propose a method to “read out” the implicit vocabulary of an autoregressive LLM by examining differences in token representations across layers, and present results of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is the first attempt to probe the implicit vocabulary of an LLM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.544.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.544.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--544 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.544 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.544/><span class=acl-fixed-case>T</span>rave<span class=acl-fixed-case>LER</span>: A Modular Multi-<span class=acl-fixed-case>LMM</span> Agent Framework for Video Question-Answering</a></strong><br><a href=/people/c/chuyi-shang/>Chuyi Shang</a>
|
<a href=/people/a/amos-you/>Amos You</a>
|
<a href=/people/s/sanjay-subramanian/>Sanjay Subramanian</a>
|
<a href=/people/t/trevor-darrell/>Trevor Darrell</a>
|
<a href=/people/r/roei-herzig/>Roei Herzig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--544><div class="card-body p-3 small">Recently, image-based Large Multimodal Models (LMMs) have made significant progress in video question-answering (VideoQA) using a frame-wise approach by leveraging large-scale pretraining in a zero-shot manner. Nevertheless, these models need to be capable of finding relevant information, extracting it, and answering the question simultaneously. Currently, existing methods perform all of these steps in a single pass without being able to adapt if insufficient or incorrect information is collected. To overcome this, we introduce a modular multi-LMM agent framework based on several agents with different roles, instructed by a Planner agent that updates its instructions using shared feedback from the other agents. Specifically, we propose TraveLER, a method that can create a plan to "**Trave**rse” through the video, ask questions about individual frames to "**L**ocate” and store key information, and then "**E**valuate” if there is enough information to answer the question. Finally, if there is not enough information, our method is able to "**R**eplan” based on its collected knowledge. Through extensive experiments, we find that the proposed TraveLER approach improves performance on several VideoQA benchmarks without the need to fine-tune on specific datasets. Our code is available at https://github.com/traveler-framework/TraveLER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.545.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--545 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.545/>Evaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding</a></strong><br><a href=/people/b/biswesh-mohapatra/>Biswesh Mohapatra</a>
|
<a href=/people/m/manav-nitin-kapadnis/>Manav Nitin Kapadnis</a>
|
<a href=/people/l/laurent-romary/>Laurent Romary</a>
|
<a href=/people/j/justine-cassell/>Justine Cassell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--545><div class="card-body p-3 small">Conversational grounding, vital for building dependable dialog systems, involves ensuring a mutual understanding of shared information. Despite its importance, there has been limited research on this aspect of conversation in recent years, especially after the advent of Large Language Models (LLMs). Previous studies have highlighted the shortcomings of pre-trained language models in conversational grounding. However, most testing for conversational grounding capabilities involves human evaluations that are costly and time-consuming. This has led to a lack of testing across multiple models of varying sizes, a critical need given the rapid rate of new model releases. This gap in research becomes more significant considering recent advances in language models, which have led to new emergent capabilities. In this paper, we aim to evaluate the performance of LLMs in various aspects of conversational grounding and analyze why some models perform better than others. We demonstrate a direct correlation between the size of the pre-training data and conversational grounding abilities, meaning that they have independently acquired a specific form of pragmatic capabilities from larger pre-training datasets. Finally, we propose ways to enhance the capabilities of the models that lag in this aspect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.546.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.546.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--546 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.546 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.546/>Unlocking Memorization in Large Language Models with Dynamic Soft Prompting</a></strong><br><a href=/people/z/zhepeng-wang/>Zhepeng Wang</a>
|
<a href=/people/r/runxue-bao/>Runxue Bao</a>
|
<a href=/people/y/yawen-wu/>Yawen Wu</a>
|
<a href=/people/j/jackson-taylor/>Jackson Taylor</a>
|
<a href=/people/c/cao-xiao/>Cao Xiao</a>
|
<a href=/people/f/feng-zheng/>Feng Zheng</a>
|
<a href=/people/w/weiwen-jiang/>Weiwen Jiang</a>
|
<a href=/people/s/shangqian-gao/>Shangqian Gao</a>
|
<a href=/people/y/yanfu-zhang/>Yanfu Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--546><div class="card-body p-3 small">Pretrained large language models (LLMs) have excelled in a variety of natural language processing (NLP) tasks, including summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize training data, leading to potential privacy breaches and copyright infringement. Therefore, accurate measurement of the memorization is essential to evaluate and mitigate these potential risks. However, previous attempts to characterize memorization are constrained by either using prefixes only or by prepending a constant soft prompt to the prefixes, which cannot react to changes in input. To address this challenge, we propose a novel method for estimating LLM memorization using dynamic, prefix-dependent soft prompts. Our approach involves training a transformer-based generator to produce soft prompts that adapt to changes in input, thereby enabling more accurate extraction of memorized data. Our method not only addresses the limitations of previous methods but also demonstrates superior performance in diverse experimental settings compared to state-of-the-art techniques. In particular, our method can achieve the maximum relative improvement of 135.3% and 39.8% over the vanilla baseline on average in terms of *discoverable memorization rate* for the text generation task and code generation task, respectively. Our code is available at https://github.com/wangger/llm-memorization-dsp.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.547.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--547 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.547.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.547/>If <span class=acl-fixed-case>CLIP</span> Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions</a></strong><br><a href=/people/r/reza-esfandiarpoor/>Reza Esfandiarpoor</a>
|
<a href=/people/c/cristina-menghini/>Cristina Menghini</a>
|
<a href=/people/s/stephen-bach/>Stephen Bach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--547><div class="card-body p-3 small">Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize textual features that are important for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate features that are important for the VLM. Then, we inspect the descriptions to identify features that contribute to VLM representations. Using EX2, we find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat (e.g., North America) to represent visual concepts. Also, our analysis reveals that different VLMs prioritize different attributes in their representations. Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.548.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.548.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--548 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.548 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.548.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.548/>Extract, Define, Canonicalize: An <span class=acl-fixed-case>LLM</span>-based Framework for Knowledge Graph Construction</a></strong><br><a href=/people/b/bowen-zhang/>Bowen Zhang</a>
|
<a href=/people/h/harold-soh/>Harold Soh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--548><div class="card-body p-3 small">In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that, in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schemas easily exceed the LLMs’ context window length. Furthermore, there are scenarios where a fixed pre-defined schema is not available and we would like the method to construct a high-quality KG with a succinct self-generated schema. To address these problems, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs’ extraction performance in a retrieval-augmented generation-like manner. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works. Code for EDC is available at https://github.com/clear-nus/edc.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.549.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--549 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.549/><span class=acl-fixed-case>MQ</span>uin<span class=acl-fixed-case>E</span>: a Cure for “<span class=acl-fixed-case>Z</span>-paradox” in Knowledge Graph Embedding</a></strong><br><a href=/people/y/yang-liu/>Yang Liu</a>
|
<a href=/people/h/huang-fang/>Huang Fang</a>
|
<a href=/people/y/yunfeng-cai/>Yunfeng Cai</a>
|
<a href=/people/m/mingming-sun/>Mingming Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--549><div class="card-body p-3 small">Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called <i>Z-paradox</i>. Motivated by the existence of Z-paradox, we propose a new KGE model called <i>MQuinE</i> that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.550.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.550.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--550 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.550 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.550/>Can Transformers Learn <span class=tex-math>n</span>-gram Language Models?</a></strong><br><a href=/people/a/anej-svete/>Anej Svete</a>
|
<a href=/people/n/nadav-borenstein/>Nadav Borenstein</a>
|
<a href=/people/m/mike-zhou/>Mike Zhou</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--550><div class="card-body p-3 small">Much theoretical work has described the ability of transformers to represent formal languages. However, linking theoretical results to empirical performance is not straightforward due to the complex interplay between the architecture, the learning algorithm, and training data. To test whether theoretical lower bounds imply <i>learnability</i> of formal languages, we turn to recent work relating transformers to <span class=tex-math>n</span>-gram language models (LMs). We study transformers’ ability to learn random <span class=tex-math>n</span>-gram LMs of two kinds: ones with arbitrary next-symbol probabilities and ones where those are defined with shared parameters. We find that classic estimation techniques for <span class=tex-math>n</span>-gram LMs such as add-<span class=tex-math>𝜆</span> smoothing outperform transformers on the former, while transformers perform better on the latter, outperforming methods specifically designed to learn <span class=tex-math>n</span>-gram LMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.551.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.551.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--551 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.551 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.551.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.551/><span class=acl-fixed-case>S</span>table<span class=acl-fixed-case>P</span>rompt : Automatic Prompt Tuning using Reinforcement Learning for Large Language Model</a></strong><br><a href=/people/m/minchan-kwon/>Minchan Kwon</a>
|
<a href=/people/g/gaeun-kim/>Gaeun Kim</a>
|
<a href=/people/j/jongsuk-kim/>Jongsuk Kim</a>
|
<a href=/people/h/haeil-lee/>Haeil Lee</a>
|
<a href=/people/j/junmo-kim/>Junmo Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--551><div class="card-body p-3 small">Finding appropriate prompts for the specific task has become an important issue as the usage of Large Language Models (LLM) have expanded. However, the variety of input-output formats complicate finding the prompts. Reinforcement Learning (RL) is a promising for prompt tuning due to its ability to incrementally produce better results through interaction with the environment. But its inherent training instability and environmental dependency make it difficult to use in practice. In this paper, we propose StablePrompt, a prompt tuning method based on RL. We formulate prompt tuning as RL problem between agent and target LLM, and introduce Adaptive Proximal Policy Optimization (APPO), an modified version of PPO for prompt tuning. APPO introduces an anchor model and updates it adaptively based on the training trajectory. Using this anchor model for the KL divergence term in PPO keeps the search space flexible and ensures training stability. We evaluate StablePrompt on various tasks, including text classification, question answering, and text generation. StablePrompt achieves State-of-The-Art performance across diverse tasks. We demonstrates that StablePrompt performs well across various types and sizes of LLMs. Furthermore, we present TTE-StablePrompt, an extension for generating input-dependent prompts. It outperforms StablePrompt in tasks that are hard to solve with a single prompt. This shows that StablePrompt is an extensible and stable RL framework for LLM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.552.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.552.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--552 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.552 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.552/>Summary of a Haystack: A Challenge to Long-Context <span class=acl-fixed-case>LLM</span>s and <span class=acl-fixed-case>RAG</span> Systems</a></strong><br><a href=/people/p/philippe-laban/>Philippe Laban</a>
|
<a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--552><div class="card-body p-3 small">LLMs and RAG systems are now capable of handling millions of input tokens or more. However, evaluating the output quality of such systems on long-context tasks remains challenging, as tasks like Needle-in-a-Haystack lack complexity. In this work, we argue that summarization can play a central role in such evaluation. We design a procedure to synthesize Haystacks of documents, ensuring that specific insights repeat across documents. The “Summary of a Haystack” (SummHay) task then requires a system to process the Haystack and generate, given a query, a summary that identifies the relevant insights and precisely cites the source documents. Since we have precise knowledge of what insights should appear in a haystack summary and what documents should be cited, we implement a highly reproducible automatic evaluation that can score summaries on two aspects – Coverage and Citation. We generate Haystacks in two domains (conversation, news), and perform a large-scale evaluation of 10 LLMs and corresponding 50 RAG systems. Our findings indicate that SummHay is an open challenge for current systems, as even systems provided with an Oracle signal of document relevance lag our estimate of human performance (56%) by 10+ points on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and Claude 3 Opus score below 20% on SummHay. We show SummHay can also be used to study enterprise RAG systems and position bias in long-context models. We hope future systems can equal and surpass human performance on SummHay.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.553.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.553.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--553 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.553 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.553/>Multi-pass Decoding for Grammatical Error Correction</a></strong><br><a href=/people/x/xiaoying-wang/>Xiaoying Wang</a>
|
<a href=/people/l/lingling-mu/>Lingling Mu</a>
|
<a href=/people/j/jingyi-zhang/>Jingyi Zhang</a>
|
<a href=/people/h/hongfei-xu/>Hongfei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--553><div class="card-body p-3 small">Sequence-to-sequence (seq2seq) models achieve comparable or better grammatical error correction performance compared to sequence-to-edit (seq2edit) models. Seq2edit models normally iteratively refine the correction result, while seq2seq models decode only once without aware of subsequent tokens. Iteratively refining the correction results of seq2seq models via Multi-Pass Decoding (MPD) may lead to better performance. However, MPD increases the inference costs. Deleting or replacing corrections in previous rounds may lose useful information in the source input. We present an early-stop mechanism to alleviate the efficiency issue. To address the source information loss issue, we propose to merge the source input with the previous round correction result into one sequence. Experiments on the CoNLL-14 test set and BEA-19 test set show that our approach can lead to consistent and significant improvements over strong BART and T5 baselines (+1.80, +1.35, and +2.02 F0.5 for BART 12-2, large and T5 large respectively on CoNLL-14 and +2.99, +1.82, and +2.79 correspondingly on BEA-19), obtaining F0.5 scores of 68.41 and 75.36 on CoNLL-14 and BEA-19 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.554.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.554.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--554 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.554 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.554.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.554.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.554/>Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations</a></strong><br><a href=/people/y/yucheng-jiang/>Yucheng Jiang</a>
|
<a href=/people/y/yijia-shao/>Yijia Shao</a>
|
<a href=/people/d/dekun-ma/>Dekun Ma</a>
|
<a href=/people/s/sina-semnani/>Sina Semnani</a>
|
<a href=/people/m/monica-lam/>Monica Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--554><div class="card-body p-3 small">While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations of their parents/teachers, we create Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user’s behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, we construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.555.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.555.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--555 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.555 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.555/><span class=acl-fixed-case>SCOI</span>: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation</a></strong><br><a href=/people/c/chenming-tang/>Chenming Tang</a>
|
<a href=/people/z/zhixiang-wang/>Zhixiang Wang</a>
|
<a href=/people/y/yunfang-wu/>Yunfang Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--555><div class="card-body p-3 small">In-context learning (ICL) greatly improves the performance of large language models (LLMs) on various down-stream tasks, where the improvement highly depends on the quality of demonstrations. In this work, we introduce syntactic knowledge to select better in-context examples for machine translation (MT). We propose a new strategy, namely <b>S</b>yntax-augmented <b>CO</b>verage-based <b>I</b>n-context example selection (SCOI), leveraging the deep syntactic structure beyond conventional word matching. Specifically, we measure the set-level syntactic coverage by computing the coverage of polynomial terms with the help of a simplified tree-to-polynomial algorithm, and lexical coverage using word overlap. Furthermore, we devise an alternate selection approach to combine both coverage measures, taking advantage of syntactic and lexical information. We conduct experiments with two multi-lingual LLMs on six translation directions. Empirical results show that our proposed SCOI obtains the highest average COMET score among all learning-free methods, indicating that combining syntactic and lexical coverage successfully helps to select better in-context examples for MT. Our code is available at https://github.com/JamyDon/SCOI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.556.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.556.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--556 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.556 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.556/>Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge</a></strong><br><a href=/people/y/yuxuan-wang/>Yuxuan Wang</a>
|
<a href=/people/y/yueqian-wang/>Yueqian Wang</a>
|
<a href=/people/p/pengfei-wu/>Pengfei Wu</a>
|
<a href=/people/j/jianxin-liang/>Jianxin Liang</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/y/yang-liu/>Yang Liu</a>
|
<a href=/people/z/zilong-zheng/>Zilong Zheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--556><div class="card-body p-3 small">Despite progress in multimodal large language models (MLLMs), the challenge of interpreting long-form videos in response to linguistic queries persists, largely due to the inefficiency in temporal grounding and limited pre-trained context window size. In this work, we introduce Temporal Grounding Bridge (TGB), a novel framework that bootstraps MLLMs with advanced temporal grounding capabilities and broadens their contextual scope. Our framework significantly enhances the temporal capabilities of current MLLMs through three key innovations: an efficient multi-span temporal grounding algorithm applied to low-dimension temporal features projected from flow; a multimodal length extrapolation training paradigm that utilizes low-dimension temporal features to extend the training context window size; and a bootstrapping framework that bridges our model with pluggable MLLMs without requiring annotation. We validate TGB across seven video benchmarks and demonstrate substantial performance improvements compared with prior MLLMs. Notably, our model, initially trained on sequences of four frames, effectively handles sequences up to 16 longer without sacrificing performance, highlighting its scalability and effectiveness in real-world applications. Our code is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.557.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.557.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--557 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.557 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.557/><span class=acl-fixed-case>STORYSUMM</span>: Evaluating Faithfulness in Story Summarization</a></strong><br><a href=/people/m/melanie-subbiah/>Melanie Subbiah</a>
|
<a href=/people/f/faisal-ladhak/>Faisal Ladhak</a>
|
<a href=/people/a/akankshya-mishra/>Akankshya Mishra</a>
|
<a href=/people/g/griffin-thomas-adams/>Griffin Thomas Adams</a>
|
<a href=/people/l/lydia-chilton/>Lydia Chilton</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--557><div class="card-body p-3 small">Human evaluation has been the gold standard for checking faithfulness in abstractive summarization. However, with a challenging source domain like narrative, multiple annotators can agree a summary is faithful, while missing details that are obvious errors only once pointed out. We therefore introduce a new dataset, StorySumm, comprising LLM summaries of short stories with localized faithfulness labels and error explanations. This benchmark is for evaluation methods, testing whether a given method can detect challenging inconsistencies. Using this dataset, we first show that any one human annotation protocol is likely to miss inconsistencies, and we advocate for pursuing a range of methods when establishing ground truth for a summarization dataset. We finally test recent automatic metrics and find that none of them achieve more than 70% balanced accuracy on this task, demonstrating that it is a challenging benchmark for future work in faithfulness evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.558.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.558.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--558 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.558 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.558/><span class=acl-fixed-case>MM</span>o<span class=acl-fixed-case>E</span>: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts</a></strong><br><a href=/people/h/haofei-yu/>Haofei Yu</a>
|
<a href=/people/z/zhengyang-qi/>Zhengyang Qi</a>
|
<a href=/people/l/lawrence-keunho-jang/>Lawrence Keunho Jang</a>
|
<a href=/people/r/russ-salakhutdinov/>Russ Salakhutdinov</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--558><div class="card-body p-3 small">Advances in multimodal models have greatly improved how interactions relevant to various tasks are modeled. Today’s multimodal models mainly focus on the correspondence between images and text, using this for tasks like image-text matching. However, this covers only a subset of real-world interactions. Novel interactions, such as sarcasm expressed through opposing spoken words and gestures or humor expressed through utterances and tone of voice, remain challenging. In this paper, we introduce an approach to enhance multimodal models, which we call Multimodal Mixtures of Experts (MMoE). The key idea in MMoE is to train separate expert models for each type of multimodal interaction, such as redundancy present in both modalities, uniqueness in one modality, or synergy that emerges when both modalities are fused. On a sarcasm detection task (MUStARD) and a humor detection task (URFUNNY), we obtain new state-of-the-art results. MMoE is also able to be applied to various types of models to gain improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.559.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.559.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--559 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.559 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.559/><span class=acl-fixed-case>O</span>m<span class=acl-fixed-case>A</span>gent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer</a></strong><br><a href=/people/l/lu-zhang/>Lu Zhang</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/h/heting-ying/>Heting Ying</a>
|
<a href=/people/y/yibo-ma/>Yibo Ma</a>
|
<a href=/people/k/kyusong-lee/>Kyusong Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--559><div class="card-body p-3 small">Recent advancements in Large Language Models (LLMs) have expanded their capabilities to multimodal contexts, including comprehensive video understanding. However, processing extensive videos such as 24-hour CCTV footage or full-length films presents significant challenges due to the vast data and processing demands. Traditional methods, like extracting key frames or converting frames to text, often result in substantial information loss. To address these shortcomings, we develop OmAgent, efficiently stores and retrieves relevant video frames for specific queries, preserving the detailed content of videos. Additionally, it features an Divide-and-Conquer Loop capable of autonomous reasoning, dynamically invoking APIs and tools to enhance query processing and accuracy. This approach ensures robust video understanding, significantly reducing information loss. Experimental results affirm OmAgent’s efficacy in handling various types of videos and complex tasks. Moreover, we have endowed it with greater autonomy and a robust tool-calling system, enabling it to accomplish even more intricate tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.560.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.560.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.560.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.560/>Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension</a></strong><br><a href=/people/l/lin-ai/>Lin Ai</a>
|
<a href=/people/z/zheng-hui/>Zheng Hui</a>
|
<a href=/people/z/zizhou-liu/>Zizhou Liu</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.561.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.561.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--561 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.561 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.561/><span class=acl-fixed-case>C</span>ommon<span class=acl-fixed-case>IT</span>: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions</a></strong><br><a href=/people/j/jun-rao/>Jun Rao</a>
|
<a href=/people/x/xuebo-liu/>Xuebo Liu</a>
|
<a href=/people/l/lian-lian/>Lian Lian</a>
|
<a href=/people/s/shengjun-cheng/>Shengjun Cheng</a>
|
<a href=/people/y/yunjie-liao/>Yunjie Liao</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--561><div class="card-body p-3 small">With instruction tuning, Large Language Models (LLMs) can enhance their ability to adhere to commands. Diverging from most works focusing on data mixing, our study concentrates on enhancing the model’s capabilities from the perspective of data sampling during training. Drawing inspiration from the human learning process, where it is generally easier to master solutions to similar topics through focused practice on a single type of topic, we introduce a novel instruction tuning strategy termed CommonIT: Commonality-aware Instruction Tuning. Specifically, we cluster instruction datasets into distinct groups with three proposed metrics Task, Embedding and Length). We ensure each training mini-batch, or “partition”, consists solely of data from a single group, which brings about both data randomness across mini-batches and intra-batch data similarity. Rigorous testing on LLaMa models demonstrates CommonIT’s effectiveness in enhancing the instruction-following capabilities of LLMs through IT datasets (FLAN, CoT, and Alpaca) and models (LLaMa2-7B, Qwen2-7B, LLaMa 13B, and BLOOM 7B). CommonIT consistently boosts an average improvement of 2.1% on the general domain (i.e., the average score of Knowledge, Reasoning, Multilinguality and Coding) with the Length metric, and 5.2% on the special domain (i.e., GSM, Openfunctions and Code) with the Task metric, and 3.8% on the specific tasks (i.e., MMLU) with the Embedding metric. Code is available at https://github.com/raojay7/CommonIT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.562.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.562.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--562 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.562 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.562.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.562.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.562/><span class=acl-fixed-case>ESC</span>: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers</a></strong><br><a href=/people/y/yuzhe-gu/>Yuzhe Gu</a>
|
<a href=/people/e/enmao-diao/>Enmao Diao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--562><div class="card-body p-3 small">Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.563.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.563.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--563 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.563 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.563.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.563/>Breaking <span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>LU</span> Barrier: Generalized <span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>E</span>fication for Dense Pretrained Models</a></strong><br><a href=/people/j/jaeseong-lee/>Jaeseong Lee</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a>
|
<a href=/people/w/wonpyo-park/>Wonpyo Park</a>
|
<a href=/people/m/mingi-ji/>Mingi Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--563><div class="card-body p-3 small">As the scale of language models (LMs) continues to grow, there is a heightened interest in reducing the inference cost associated with these models. Mixture-of-Experts (MoEs) present an efficient alternative to dense models, while the existing methods to convert pretrained dense models to MoEs is limited to ReLU-based models with natural sparsity. This paper introduces G-MoEfication, applicable to arbitrary dense models, where ReLU-based activation sparsity assumptions no longer hold. For generalizations, we encounter the dilemma of needing to zero-out deactivated experts, while also avoiding excessive zeroing-out to retain dense activation information. We publicly release our code and report results conducted with mBERT, SantaCoder-1.1B, Phi-2-2.7B, and Falcon-7B demonstrating the efficacy of our approach in general scenarios: from multitask to multilingual, from fine-tuning to zero-shot evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.564.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.564.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--564 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.564 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.564.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.564.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.564/>Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood</a></strong><br><a href=/people/y/yang-xu/>Yang Xu</a>
|
<a href=/people/y/yu-wang/>Yu Wang</a>
|
<a href=/people/h/hao-an/>Hao An</a>
|
<a href=/people/z/zhichen-liu/>Zhichen Liu</a>
|
<a href=/people/y/yongyuan-li/>Yongyuan Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--564><div class="card-body p-3 small">Human and model-generated texts can be distinguished by examining the magnitude of likelihood in language. However, it is becoming increasingly difficult as language model’s capabilities of generating human-like texts keep evolving. This study provides a new perspective by using the relative likelihood values instead of absolute ones, and extracting useful features from the spectrum-view of likelihood for the human-model text detection task. We propose a detection procedure with two classification methods, supervised and heuristic-based, respectively, which results in competitive performances with previous zero-shot detection methods and a new state-of-the-art on short-text detection. Our method can also reveal subtle differences between human and model languages, which find theoretical roots in psycholinguistics studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.565.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.565.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--565 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.565 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.565/>Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning</a></strong><br><a href=/people/j/jiahui-li/>Jiahui Li</a>
|
<a href=/people/h/hanlin-zhang/>Hanlin Zhang</a>
|
<a href=/people/f/fengda-zhang/>Fengda Zhang</a>
|
<a href=/people/t/tai-wei-chang/>Tai-Wei Chang</a>
|
<a href=/people/k/kun-kuang/>Kun Kuang</a>
|
<a href=/people/l/long-chen/>Long Chen</a>
|
<a href=/people/j/jun-zhou/>Jun Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--565><div class="card-body p-3 small">Reinforcement learning from human feedback (RLHF) and AI-generated feedback (RLAIF) have become prominent techniques that significantly enhance the functionality of pre-trained language models (LMs). These methods harness feedback, sourced either from humans or AI, as direct rewards or to shape reward models that steer LM optimization. Nonetheless, the effective integration of rewards from diverse sources presents a significant challenge due to their disparate characteristics. To address this, recent research has developed algorithms incorporating strategies such as weighting, ranking, and constraining to handle this complexity. Despite these innovations, a bias toward disproportionately high rewards can still skew the reinforcement learning process and negatively impact LM performance. This paper explores a methodology for reward composition that enables simultaneous improvements in LMs across multiple dimensions. Inspired by fairness theory, we introduce a training algorithm that aims to reduce disparity and enhance stability among various rewards. Our method treats the aggregate reward as a dynamic weighted sum of individual rewards, with alternating updates to the weights and model parameters. For efficient and straightforward implementation, we employ an estimation technique rooted in the mirror descent method for weight updates, eliminating the need for gradient computations. The empirical results under various types of rewards across a wide range of scenarios demonstrate the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.566.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.566.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--566 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.566 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.566.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.566/>Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models</a></strong><br><a href=/people/x/xiaohua-feng/>XiaoHua Feng</a>
|
<a href=/people/c/chaochao-chen/>Chaochao Chen</a>
|
<a href=/people/y/yuyuan-li/>Yuyuan Li</a>
|
<a href=/people/z/zibin-lin/>Zibin Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--566><div class="card-body p-3 small">Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for language models. Previous research relies on gradient ascent methods to achieve knowledge unlearning, which is simple and effective. However, this approach calculates all the gradients of tokens in the sequence, potentially compromising the general ability of language models. To overcome this limitation, we propose an adaptive objective that calculates gradients with fine-grained control specifically targeting sensitive tokens. Our adaptive objective is pluggable, ensuring simplicity and enabling extension to the regularization-based framework that utilizes non-target data or other models to preserve general ability. Through extensive experiments targeting the removal of typical sensitive data, we demonstrate that our proposed method enhances the general ability of language models while achieving knowledge unlearning. Additionally, it demonstrates the capability to adapt to behavior alignment, eliminating all the undesirable knowledge within a specific domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.567.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.567.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--567 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.567 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.567/><span class=acl-fixed-case>ARM</span>: An Alignment-and-Replacement Module for <span class=acl-fixed-case>C</span>hinese Spelling Check Based on <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/c/changchun-liu/>Changchun Liu</a>
|
<a href=/people/k/kai-zhang/>Kai Zhang</a>
|
<a href=/people/j/junzhe-jiang/>Junzhe Jiang</a>
|
<a href=/people/z/zirui-liu/>Zirui Liu</a>
|
<a href=/people/h/hanqing-tao/>Hanqing Tao</a>
|
<a href=/people/m/min-gao/>Min Gao</a>
|
<a href=/people/e/enhong-chen/>Enhong Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--567><div class="card-body p-3 small">Chinese Spelling Check (CSC) aims to identify and correct spelling errors in Chinese texts, where enhanced semantic understanding of a sentence can significantly improve correction accuracy. Recently, Large Language Models (LLMs) have demonstrated exceptional mastery of world knowledge and semantic understanding, rendering them more robust against spelling errors. However, the application of LLMs in CSC is a double-edged sword, as they tend to unnecessarily alter sentence length and modify rare but correctly used phrases. In this paper, by leveraging the capabilities of LLMs while mitigating their limitations, we propose a novel plug-and-play Alignment-and-Replacement Module ARM that enhances the performance of existing CSC models and without the need for retraining or fine-tuning. Experiment results and analysis on three benchmark datasets demonstrate the effectiveness and competitiveness of the proposed module.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.568.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.568.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--568 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.568 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.568/>On the In-context Generation of Language Models</a></strong><br><a href=/people/z/zhongtao-jiang/>Zhongtao Jiang</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/k/kun-luo/>Kun Luo</a>
|
<a href=/people/x/xiaowei-yuan/>Xiaowei Yuan</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--568><div class="card-body p-3 small">Large language models (LLMs) are found to have the ability of in-context generation (ICG): when they are fed with an in-context prompt concatenating a few somehow similar examples, they can implicitly recognize the pattern of them and then complete the prompt in the same pattern. ICG is curious, since language models are usually not explicitly trained in the same way as the in-context prompt, and the distribution of examples in the prompt differs from that of sequences in the pretrained corpora. This paper provides a systematic study of the ICG ability of language models, covering discussions about its source and influential factors, in the view of both theory and empirical experiments. Concretely, we first propose a plausible latent variable model to model the distribution of the pretrained corpora, and then formalize ICG as a problem of next topic prediction. With this framework, we can prove that the repetition nature of a few topics ensures the ICG ability on them theoretically. Then, we use this controllable pretrained distribution to generate several medium-scale synthetic datasets (token scale: 2.1B-3.9B) and experiment with different settings of Transformer architectures (parameter scale: 4M-234M). Our experimental results further offer insights into how the data and model architectures influence ICG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.569.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.569.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--569 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.569 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.569.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.569/>Atomic Inference for <span class=acl-fixed-case>NLI</span> with Generated Facts as Atoms</a></strong><br><a href=/people/j/joe-stacey/>Joe Stacey</a>
|
<a href=/people/p/pasquale-minervini/>Pasquale Minervini</a>
|
<a href=/people/h/haim-dubossarsky/>Haim Dubossarsky</a>
|
<a href=/people/o/oana-maria-camburu/>Oana-Maria Camburu</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--569><div class="card-body p-3 small">With recent advances, neural models can achieve human-level performance on various natural language tasks. However, there are no guarantees that any explanations from these models are faithful, i.e. that they reflect the inner workings of the model. Atomic inference overcomes this issue, providing interpretable and faithful model decisions. This approach involves making predictions for different components (or atoms) of an instance, before using interpretable and deterministic rules to derive the overall prediction based on the individual atom-level predictions. We investigate the effectiveness of using LLM-generated facts as atoms, decomposing Natural Language Inference premises into lists of facts. While directly using generated facts in atomic inference systems can result in worse performance, with 1) a multi-stage fact generation process, and 2) a training regime that incorporates the facts, our fact-based method outperforms other approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.570.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.570.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--570 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.570 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.570/>Towards Robust Speech Representation Learning for Thousands of Languages</a></strong><br><a href=/people/w/william-chen/>William Chen</a>
|
<a href=/people/w/wangyou-zhang/>Wangyou Zhang</a>
|
<a href=/people/y/yifan-peng-cmu/>Yifan Peng</a>
|
<a href=/people/x/xinjian-li/>Xinjian Li</a>
|
<a href=/people/j/jinchuan-tian/>Jinchuan Tian</a>
|
<a href=/people/j/jiatong-shi/>Jiatong Shi</a>
|
<a href=/people/x/xuankai-chang/>Xuankai Chang</a>
|
<a href=/people/s/soumi-maiti/>Soumi Maiti</a>
|
<a href=/people/k/karen-livescu/>Karen Livescu</a>
|
<a href=/people/s/shinji-watanabe/>Shinji Watanabe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--570><div class="card-body p-3 small">Self-supervised learning (SSL) has helped extend speech technologies to more languages by reducing the need for labeled data. However, models are still far from supporting the world’s 7000+ languages. We propose XEUS, a Cross-lingual Encoder for Universal Speech, trained on over 1 million hours of data across 4057 languages, extending the language coverage of SSL models 4-fold. We combine 1 million hours of speech from existing publicly accessible corpora with a newly created corpus of 7400+ hours from 4057 languages, which will be publicly released. To handle the diverse conditions of multilingual speech data, we augment the typical SSL masked prediction approach with a novel dereverberation objective, increasing robustness. We evaluate XEUS on several benchmarks, and show that it consistently outperforms or achieves comparable results to state-of-the-art (SOTA) SSL models across a variety of tasks. XEUS sets a new SOTA on the ML-SUPERB benchmark: it outperforms MMS 1B and w2v-BERT 2.0 v2 by 0.8% and 4.4% respectively, despite having less parameters or pre-training data. Checkpoints, code, and data are found in https://www.wavlab.org/activities/2024/xeus/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.571.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.571.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--571 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.571 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.571/><span class=acl-fixed-case>I</span> Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with <span class=acl-fixed-case>LLM</span>-Generated Responses</a></strong><br><a href=/people/x/xuan-ren/>Xuan Ren</a>
|
<a href=/people/b/biao-wu/>Biao Wu</a>
|
<a href=/people/l/lingqiao-liu/>Lingqiao Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--571><div class="card-body p-3 small">This paper explores an intriguing observation: fine-tuning a large language model (LLM) with responses generated by a LLM often yields better results than using responses generated by humans, particularly in reasoning tasks. We conduct an in-depth investigation to understand why this occurs. Contrary to the common belief that these instances is due to the more detailed nature of LLM-generated content, our study identifies another contributing factor: an LLM is inherently more “familiar” with LLM generated responses. This familiarity is evidenced by lower perplexity before fine-tuning. We design a series of experiments to understand the impact of the “familiarity” and our conclusion reveals that this “familiarity” significantly impacts learning performance. Training with LLM-generated responses not only enhances performance but also helps maintain the model’s capabilities in other reasoning tasks after fine-tuning on a specific task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.572.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.572.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--572 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.572 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.572/><span class=acl-fixed-case>P</span>re<span class=acl-fixed-case>A</span>lign: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment</a></strong><br><a href=/people/j/jiahuan-li/>Jiahuan Li</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/a/aarron-ching/>Aarron Ching</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--572><div class="card-body p-3 small">Large language models demonstrate reasonable multilingual abilities, despite predominantly English-centric pretraining. However, the spontaneous multilingual alignment in these models is shown to be weak, leading to unsatisfactory cross-lingual transfer and knowledge sharing. Previous works attempt to address this issue by explicitly injecting multilingual alignment information during or after pretraining. Thus for the early stage in pretraining, the alignment is weak for sharing information or knowledge across languages. In this paper, we propose PreAlign, a framework that establishes multilingual alignment prior to language model pretraining. PreAlign injects multilingual alignment by initializing the model to generate similar representations of aligned words and preserves this alignment using a code-switching strategy during pretraining. Extensive experiments in a synthetic English to English-Clone setting demonstrate that PreAlign significantly outperforms standard multilingual joint training in language modeling, zero-shot cross-lingual transfer, and cross-lingual knowledge application. Further experiments in real-world scenarios further validate PreAlign’s effectiveness across various model sizes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.573.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.573.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--573 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.573 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.573/>An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance</a></strong><br><a href=/people/s/simran-khanuja/>Simran Khanuja</a>
|
<a href=/people/s/sathyanarayanan-ramamoorthy/>Sathyanarayanan Ramamoorthy</a>
|
<a href=/people/y/yueqi-song/>Yueqi Song</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--573><div class="card-body p-3 small">Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we introduce a new task of translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset – (i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image; and (ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our project webpage is here: https://machine-transcreation.github.io/image-transcreation and our code, data and model outputs can be found here: https://github.com/simran-khanuja/image-transcreation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.574.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.574.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--574 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.574 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.574.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.574/>When Parts Are Greater Than Sums: Individual <span class=acl-fixed-case>LLM</span> Components Can Outperform Full Models</a></strong><br><a href=/people/t/ting-yun-chang/>Ting-Yun Chang</a>
|
<a href=/people/j/jesse-thomason/>Jesse Thomason</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--574><div class="card-body p-3 small">This paper studies in-context learning by decomposing the output of large language models into the individual contributions of attention heads and MLPs (components). We observe curious components: good-performing ones that individually do well on a classification task, even when the model performs poorly; bad-performing ones that do much worse than chance; and label-biased components that always predict the same label. We find that component accuracies are well-correlated across different demonstration sets and perturbations of prompt templates. Based on our findings, we propose component reweighting, which learns to linearly re-scale the component activations from a few labeled examples. Given 24 labeled examples, our method improves by an average of 6.0% accuracy points over 24-shot ICL across 8 tasks on Llama-2-7B. Overall, this paper both enriches our understanding of ICL and provides a practical method for improvement by examining model internals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.575.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.575.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--575 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.575 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.575/>Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference</a></strong><br><a href=/people/j/jianxing-yu/>Jianxing Yu</a>
|
<a href=/people/s/shiqi-wang/>Shiqi Wang</a>
|
<a href=/people/h/han-yin/>Han Yin</a>
|
<a href=/people/z/zhenlong-sun/>Zhenlong Sun</a>
|
<a href=/people/r/ruobing-xie/>Ruobing Xie</a>
|
<a href=/people/b/bo-zhang/>Bo Zhang</a>
|
<a href=/people/y/yanghui-rao/>Yanghui Rao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--575><div class="card-body p-3 small">This paper focuses on detecting clickbait posts on the Web. These posts often use eye-catching disinformation in mixed modalities to mislead users to click for profit. That affects the user experience and thus would be blocked by content provider. To escape detection, malicious creators use tricks to add some irrelevant non-bait content into bait posts, dressing them up as legal to fool the detector. This content often has biased relations with non-bait labels, yet traditional detectors tend to make predictions based on simple co-occurrence rather than grasping inherent factors that lead to malicious behavior. This spurious bias would easily cause misjudgments. To address this problem, we propose a new debiased method based on causal inference. We first employ a set of features in multiple modalities to characterize the posts. Considering these features are often mixed up with unknown biases, we then disentangle three kinds of latent factors from them, including the invariant factor that indicates intrinsic bait intention; the causal factor which reflects deceptive patterns in a certain scenario, and non-causal noise. By eliminating the noise that causes bias, we can use invariant and causal factors to build a robust model with good generalization ability. Experiments on three popular datasets show the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.576.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.576.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--576 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.576 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.576/>Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions</a></strong><br><a href=/people/j/jinsung-yoon/>Jinsung Yoon</a>
|
<a href=/people/r/rajarishi-sinha/>Rajarishi Sinha</a>
|
<a href=/people/s/sercan-o-arik/>Sercan O Arik</a>
|
<a href=/people/t/tomas-pfister/>Tomas Pfister</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--576><div class="card-body p-3 small">Embeddings from Large Language Models (LLMs) have emerged as critical components in various applications, particularly for information retrieval. While high-dimensional embeddings generally demonstrate superior performance as they contain more salient information, their practical application is frequently hindered by elevated computational latency and the associated higher cost. To address these challenges, we propose Matryoshka-Adaptor, a novel tuning framework designed for the customization of LLM embeddings. Matryoshka-Adaptor facilitates substantial dimensionality reduction while maintaining comparable performance levels, thereby achieving a significant enhancement in computational efficiency and cost-effectiveness. Our framework directly modifies the embeddings from pre-trained LLMs which is designed to be seamlessly integrated with any LLM architecture, encompassing those accessible exclusively through black-box APIs. Also, it exhibits efficacy in both unsupervised and supervised learning settings. A rigorous evaluation conducted across a diverse corpus of English, multilingual, and multimodal datasets consistently reveals substantial gains with Matryoshka-Adaptor. Notably, with Google and OpenAI Embedding APIs, Matryoshka-Adaptor achieves a reduction in dimensionality ranging from two- to twelve-fold without compromising performance across multiple BEIR datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.577.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.577.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--577 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.577 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.577/><span class=acl-fixed-case>KNN</span>-Instruct: Automatic Instruction Construction with K Nearest Neighbor Deduction</a></strong><br><a href=/people/j/jianshang-kou/>Jianshang Kou</a>
|
<a href=/people/b/benfeng-xu/>Benfeng Xu</a>
|
<a href=/people/c/chiwei-zhu/>Chiwei Zhu</a>
|
<a href=/people/z/zhendong-mao/>Zhendong Mao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--577><div class="card-body p-3 small">Supervised fine-tuning (SFT) is a critical procedure for aligning large language models. Despite its efficiency, the construction of SFT data often struggles with issues of quality, diversity, and scalability. Many existing methods, inspired by the Self-Instruct framework, typically generate synthetic instructions by prompting aligned proprietary models like ChatGPT. However, such process suffers from stale distribution, resulting in instructions that are merely trivial variations of existing ones. In this paper, we introduce a novel bootstrapping approach termed KNN-Instruct, which incorporates KNN deduction to produce meaningful new instructions by effectively summarizing and learning from similar existing ones. We conduct an economical controlled experiment to preliminarily validate its effectiveness. In the further experiment, we construct a high-quality SFT dataset named KNN-Inst-12k*. Applying the dataset to Qwen-2-7B, we get a MT-Bench score of 7.64, which outperforms all 7B models on the LMSYS leaderboard, including Starling-LM-7B (7.48), OpenChat-3.5 (7.06) and Zephyr-7B-beta (6.53). Our code and data are available at https://github.com/CrossmodalGroup/KNN-Instruct/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.578.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.578.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--578 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.578 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.578.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.578/>Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation</a></strong><br><a href=/people/z/zhen-lin/>Zhen Lin</a>
|
<a href=/people/s/shubhendu-trivedi/>Shubhendu Trivedi</a>
|
<a href=/people/j/jimeng-sun/>Jimeng Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--578><div class="card-body p-3 small">The advent of large language models (LLMs) has dramatically advanced the state-of-the-art in numerous natural language generation tasks. For LLMs to be applied reliably, it is essential to have an accurate measure of their confidence. Currently, the most commonly used confidence score function is the likelihood of the generated sequence, which, however, conflates semantic and syntactic components. For instance, in question-answering (QA) tasks, an awkward phrasing of the correct answer might result in a lower probability prediction. Additionally, different tokens should be weighted differently depending on the context. In this work, we propose enhancing the predicted sequence probability by assigning different weights to various tokens using attention values elicited from the base LLM. By employing a validation set, we can identify the relevant attention heads, thereby significantly improving the reliability of the vanilla sequence probability confidence measure. We refer to this new score as the Contextualized Sequence Likelihood (CSL). CSL is easy to implement, fast to compute, and offers considerable potential for further improvement with task-specific prompts. Across several QA datasets and a diverse array of LLMs, CSL has demonstrated significantly higher reliability than state-of-the-art baselines in predicting generation quality, as measured by the AUROC or AUARC.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.579.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.579.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.579.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.579.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.579/><span class=acl-fixed-case>M</span>ix<span class=acl-fixed-case>GR</span>: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity</a></strong><br><a href=/people/f/fengyu-cai/>Fengyu Cai</a>
|
<a href=/people/x/xinran-zhao/>Xinran Zhao</a>
|
<a href=/people/t/tong-chen/>Tong Chen</a>
|
<a href=/people/s/sihao-chen/>Sihao Chen</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/h/heinz-koeppl/>Heinz Koeppl</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.580.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.580.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--580 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.580 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.580/><span class=acl-fixed-case>CARER</span> - <span class=acl-fixed-case>C</span>linic<span class=acl-fixed-case>A</span>l Reasoning-Enhanced Representation for Temporal Health Risk Prediction</a></strong><br><a href=/people/t/tuan-dung-nguyen/>Tuan Dung Nguyen</a>
|
<a href=/people/t/thanh-trung-huynh/>Thanh Trung Huynh</a>
|
<a href=/people/m/minh-hieu-phan/>Minh Hieu Phan</a>
|
<a href=/people/q/quoc-viet-hung-nguyen/>Quoc Viet Hung Nguyen</a>
|
<a href=/people/p/phi-le-nguyen/>Phi Le Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--580><div class="card-body p-3 small">The increasing availability of multimodal data from electronic health records (EHR) has paved the way for deep learning methods to improve diagnosis accuracy. However, deep learning models are data-driven, requiring large-scale datasets to achieve high generalizability. Inspired by how human experts leverage reasoning for medical diagnosis, we propose CARER, a novel health risk prediction framework, that enhances deep learning models with clinical rationales derived from medically proficient Large Language Models (LLMs). In addition, we provide a cross-view alignment loss which aligns the “local” view from the patient’s health status with the “global” view from the external LLM’s clinical reasoning to boost the mutual feature learning. Through extensive experiments on two predictive tasks using two popular EHR datasets, our CARER’s significantly exceeds the performance of state-of-the-art models by up to 11.2%, especially in improving data efficiency and generalizability. Our code is available at https://github.com/tuandung2812/CARER-EMNLP-2024</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.581.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.581.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--581 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.581 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.581/>“In-Dialogues We Learn”: Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning</a></strong><br><a href=/people/c/chuanqi-cheng/>Chuanqi Cheng</a>
|
<a href=/people/q/quan-tu/>Quan Tu</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/s/shuo-shang/>Shuo Shang</a>
|
<a href=/people/c/cunli-mao/>Cunli Mao</a>
|
<a href=/people/z/zhengtao-yu/>Zhengtao Yu</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--581><div class="card-body p-3 small">Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.582.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.582.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--582 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.582 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.582/>Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective</a></strong><br><a href=/people/h/hanqi-yan/>Hanqi Yan</a>
|
<a href=/people/y/yanzheng-xiang/>Yanzheng Xiang</a>
|
<a href=/people/g/guangyi-chen/>Guangyi Chen</a>
|
<a href=/people/y/yifei-wang/>Yifei Wang</a>
|
<a href=/people/l/lin-gui/>Lin Gui</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--582><div class="card-body p-3 small">To better interpret the intrinsic mechanism of large language models (LLMs), recent studies focus on monosemanticity on its basic units. A monosemantic neuron is dedicated to a single and specific concept, which forms a one-to-one correlation between neurons and concepts. Despite extensive research in monosemanticity probing, it remains unclear whether monosemanticity is beneficial or harmful to model capacity. To explore this question, we revisit monosemanticity from the feature decorrelation perspective and advocate for its encouragement. We experimentally observe that the current conclusion by (CITATION), which suggests that decreasing monosemanticity enhances model performance, does not hold when the model changes. Instead, we demonstrate that monosemanticity consistently exhibits a positive correlation with model capacity, in the preference alignment process. Consequently, we apply feature correlation as a proxy for monosemanticity and incorporate a feature decorrelation regularizer into the dynamic preference optimization process. The experiments show that our method not only enhances representation diversity and activation sparsity but also improves preference alignment performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.583.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.583.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--583 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.583 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.583/>Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding</a></strong><br><a href=/people/x/xin-liu/>Xin Liu</a>
|
<a href=/people/f/farima-fatahi-bayat/>Farima Fatahi Bayat</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--583><div class="card-body p-3 small">Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs’ reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals. Moreover, simply filtering out low-confidence responses reduces the LM’s helpfulness when the answers are correct. Therefore, effectively using calibration techniques to enhance an LM’s factuality remains an unsolved challenge. In this paper, we first propose an activation-based calibration method, ActCab, which trains a linear layer on top of the LM’s last-layer activations that can better capture the representations of knowledge. Built on top of ActCab, we further propose CoDec, a confidence-guided decoding strategy to elicit truthful answers with high confidence from LMs. By evaluating on five popular QA benchmarks, ActCab achieves superior calibration performance than all competitive baselines, e.g., by reducing the average expected calibration error (ECE) score by up to 39%. Further experiments on CoDec show consistent improvements in several LMs’ factuality on challenging QA datasets, such as TruthfulQA, highlighting the value of confidence signals in enhancing the factuality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.584.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.584.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.584/>Reasoning Robustness of <span class=acl-fixed-case>LLM</span>s to Adversarial Typographical Errors</a></strong><br><a href=/people/e/esther-gan/>Esther Gan</a>
|
<a href=/people/y/yiran-zhao/>Yiran Zhao</a>
|
<a href=/people/l/liying-cheng/>Liying Cheng</a>
|
<a href=/people/m/mao-yancan/>Mao Yancan</a>
|
<a href=/people/a/anirudh-goyal/>Anirudh Goyal</a>
|
<a href=/people/k/kenji-kawaguchi/>Kenji Kawaguchi</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/m/michael-shieh/>Michael Shieh</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.585.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.585.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--585 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.585 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.585/><span class=acl-fixed-case>I</span>nfer<span class=acl-fixed-case>A</span>ligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance</a></strong><br><a href=/people/p/pengyu-wang/>Pengyu Wang</a>
|
<a href=/people/d/dong-zhang/>Dong Zhang</a>
|
<a href=/people/l/linyang-li/>Linyang Li</a>
|
<a href=/people/c/chenkun-tan/>Chenkun Tan</a>
|
<a href=/people/x/xinghao-wang/>Xinghao Wang</a>
|
<a href=/people/m/mozhi-zhang/>Mozhi Zhang</a>
|
<a href=/people/k/ke-ren/>Ke Ren</a>
|
<a href=/people/b/botian-jiang/>Botian Jiang</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--585><div class="card-body p-3 small">As large language models (LLMs) rapidly evolve, they are increasingly being customized through fine-tuning to suit the specific needs of various applications. A critical aspect of this advancement is the alignment process, which ensures that these models perform tasks in ways that align with human values and expectations. Current alignment methods, such as direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF), focus primarily on alignment during training phase. However, these methods often involve complex and resource-intensive training processes, posing significant challenge for their implementation. Therefore, we propose <b>InferAligner</b>, a simple yet effective method for harmlessness alignment during inference phase. InferAligner decouples harmlessness from helpfulness. During the training phase, it focuses solely on enhancing the target model’s capabilities on downstream tasks. In the inference phase, it utilizes safety steering vectors extracted from the aligned model to guide the target model towards harmlessness alignment. Experimental results show that our method can be very effectively applied to domain-specific models in finance, medicine, and mathematics, as well as to multimodal large language models (MLLMs) such as LLaVA. It significantly diminishes the attack success rate (ASR) of both harmful instructions and jailbreak instructions, while maintaining almost unchanged performance in downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.586.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.586.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--586 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.586 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.586/>Belief Revision: The Adaptability of Large Language Models Reasoning</a></strong><br><a href=/people/b/bryan-wilie/>Bryan Wilie</a>
|
<a href=/people/s/samuel-cahyawijaya/>Samuel Cahyawijaya</a>
|
<a href=/people/e/etsuko-ishii/>Etsuko Ishii</a>
|
<a href=/people/j/junxian-he/>Junxian He</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--586><div class="card-body p-3 small">The capability to reason from text is crucial for real-world NLP applications. Real-world scenarios often involve incomplete or evolving data. In response, individuals update their beliefs and understandings accordingly. However, most existing evaluations assume that language models (LMs) operate with consistent information. We introduce Belief-R, a new dataset designed to test LMs’ belief revision ability when presented with new evidence. Inspired by how humans suppress prior inferences, this task assesses LMs within the newly proposed delta reasoning (<span class=tex-math>𝛥 R</span>) framework. Belief-R features sequences of premises designed to simulate scenarios where additional information could necessitate prior conclusions drawn by LMs. We evaluate ~30 LMs across diverse prompting strategies and found that LMs generally struggle to appropriately revise their beliefs in response to new information. Further, models adept at updating often underperformed in scenarios without necessary updates, highlighting a critical trade-off. These insights underscore the importance of improving LMs’ adaptiveness to changing information, a step toward more reliable AI systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.587.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.587.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--587 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.587 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.587.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.587/>Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models</a></strong><br><a href=/people/j/ji-liu/>Ji Liu</a>
|
<a href=/people/j/jiaxiang-ren/>Jiaxiang Ren</a>
|
<a href=/people/r/ruoming-jin/>Ruoming Jin</a>
|
<a href=/people/z/zijie-zhang/>Zijie Zhang</a>
|
<a href=/people/y/yang-zhou/>Yang Zhou</a>
|
<a href=/people/p/patrick-valduriez/>Patrick Valduriez</a>
|
<a href=/people/d/dejing-dou/>Dejing Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--587><div class="card-body p-3 small">As a promising paradigm to collaboratively train models with decentralized data, Federated Learning (FL) can be exploited to fine-tune Large Language Models (LLMs). While LLMs correspond to huge size, the scale of the training data significantly increases, which leads to tremendous amounts of computation and communication costs. The training data is generally non-Independent and Identically Distributed (non-IID), which requires adaptive data processing within each device. Although Low-Rank Adaptation (LoRA) can significantly reduce the scale of parameters to update in the fine-tuning process, it still takes unaffordable time to transfer the low-rank parameters of all the layers in LLMs. In this paper, we propose a Fisher Information-based Efficient Curriculum Federated Learning framework (FibecFed) with two novel methods, i.e., adaptive federated curriculum learning and efficient sparse parameter update. First, we propose a fisher information-based method to adaptively sample data within each device to improve the effectiveness of the FL fine-tuning process. Second, we dynamically select the proper layers for global aggregation and sparse parameters for local update with LoRA so as to improve the efficiency of the FL fine-tuning process. Extensive experimental results based on 10 datasets demonstrate that FibecFed yields excellent performance (up to 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61% faster) compared with 17 baseline approaches).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.588.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.588.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--588 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.588 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.588/>Bio-<span class=acl-fixed-case>RFX</span>: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints</a></strong><br><a href=/people/m/minjia-wang/>Minjia Wang</a>
|
<a href=/people/f/fangzhou-liu/>Fangzhou Liu</a>
|
<a href=/people/x/xiuxing-li/>Xiuxing Li</a>
|
<a href=/people/b/bowen-dong/>Bowen Dong</a>
|
<a href=/people/z/zhenyu-li/>Zhenyu Li</a>
|
<a href=/people/t/tengyu-pan/>Tengyu Pan</a>
|
<a href=/people/j/jianyong-wang/>Jianyong Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--588><div class="card-body p-3 small">The ever-growing biomedical publications magnify the challenge of extracting structured data from unstructured texts. This task involves two components: biomedical entity identification (Named Entity Recognition, NER) and their interrelation determination (Relation Extraction, RE). However, existing methods often neglect unique features of the biomedical literature, such as ambiguous entities, nested proper nouns, and overlapping relation triplets, and underutilize prior knowledge, leading to an intolerable performance decline in the biomedical domain, especially with limited annotated training data. In this paper, we propose the Biomedical Relation-First eXtraction (Bio-RFX) model by leveraging sentence-level relation classification before entity extraction to tackle entity ambiguity. Moreover, we exploit structural constraints between entities and relations to guide the model’s hypothesis space, enhancing extraction performance across different training scenarios. Comprehensive experimental results on biomedical datasets show that Bio-RFX achieves significant improvements on both NER and RE tasks. Even under the low-resource training scenarios, it outperforms all baselines in NER and has highly competitive performance compared to the state-of-the-art fine-tuned baselines in RE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.589.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.589.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--589 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.589 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.589/>Decoding Matters: Addressing Amplification Bias and Homogeneity Issue in Recommendations for Large Language Models</a></strong><br><a href=/people/k/keqin-bao/>Keqin Bao</a>
|
<a href=/people/j/jizhi-zhang/>Jizhi Zhang</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a>
|
<a href=/people/x/xinyue-huo/>Xinyue Huo</a>
|
<a href=/people/c/chong-chen/>Chong Chen</a>
|
<a href=/people/f/fuli-feng/>Fuli Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--589><div class="card-body p-3 small">Adapting Large Language Models (LLMs) for recommendation requires careful consideration of the decoding process, given the inherent differences between generating items and natural language. Existing approaches often directly apply LLMs’ original decoding methods. However, we find these methods encounter significant challenges: 1) amplification bias—where standard length normalization inflates scores for items containing tokens with generation probabilities close to 1 (termed ghost tokens), and 2) homogeneity issue—generating multiple similar or repetitive items for a user. To tackle these challenges, we introduce a new decoding approach named Debiasing-Diversifying Decoding (<span class=tex-math>D<sup>3</sup></span>). <span class=tex-math>D<sup>3</sup></span> disables length normalization for ghost tokens to alleviate amplification bias, and it incorporates a text-free assistant model to encourage tokens less frequently generated by LLMs for counteracting recommendation homogeneity. Extensive experiments on real-world datasets demonstrate the method’s effectiveness in enhancing accuracy and diversity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.590.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.590.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--590 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.590 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.590/><span class=acl-fixed-case>LLM</span>s Are Prone to Fallacies in Causal Inference</a></strong><br><a href=/people/n/nitish-joshi/>Nitish Joshi</a>
|
<a href=/people/a/abulhair-saparov/>Abulhair Saparov</a>
|
<a href=/people/y/yixin-wang/>Yixin Wang</a>
|
<a href=/people/h/he-he/>He He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--590><div class="card-body p-3 small">Recent work shows that causal facts can be effectively extracted from LLMs through prompting, facilitating the creation of causal graphs for causal inference tasks. However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize. Thus, this work investigates: Can LLMs infer causal relations from other relational data in text? To disentangle the role of memorized causal facts vs inferred causal relations, we finetune LLMs on synthetic data containing temporal, spatial and counterfactual relations, and measure whether the LLM can then infer causal relations. We find that: (a) LLMs are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. X mentioned before Y implies X causes Y); (b) if the order is randomized, LLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal relation) implies X causes Y. We also find that while LLMs can correctly deduce the absence of causal relations from temporal and spatial relations, they have difficulty inferring causal relations from counterfactuals, questioning their understanding of causality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.591.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.591.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--591 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.591 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.591/>Roleplay-doh: Enabling Domain-Experts to Create <span class=acl-fixed-case>LLM</span>-simulated Patients via Eliciting and Adhering to Principles</a></strong><br><a href=/people/r/ryan-louie/>Ryan Louie</a>
|
<a href=/people/a/ananjan-nandi/>Ananjan Nandi</a>
|
<a href=/people/w/william-fang/>William Fang</a>
|
<a href=/people/c/cheng-chang/>Cheng Chang</a>
|
<a href=/people/e/emma-brunskill/>Emma Brunskill</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--591><div class="card-body p-3 small">Recent works leverage LLMs to roleplay realistic social scenarios, aiding novices in practicing their social skills. However, simulating sensitive interactions, such as in the domain of mental health, is challenging. Privacy concerns restrict data access, and collecting expert feedback, although vital, is laborious. To address this, we develop Roleplay-doh, a novel human-LLM collaboration pipeline that elicits qualitative feedback from a domain-expert, which is transformed into a set of principles, or natural language rules, that govern an LLM-prompted roleplay. We apply this pipeline to enable senior mental health supporters to create customized AI patients as simulated practice partners for novice counselors. After uncovering issues with basic GPT-4 simulations not adhering to expert-defined principles, we also introduce a novel principle-adherence prompting pipeline which shows a 30% improvement in response quality and principle following for the downstream task. Through a user study with 25 counseling experts, we demonstrate that the pipeline makes it easy and effective to create AI patients that more faithfully resemble real patients, as judged by both creators and third-party counselors. We provide access to the code and data on our project website: https://roleplay-doh.github.io/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.592.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.592.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--592 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.592 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.592.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.592/>The <span class=acl-fixed-case>L</span>ou Dataset - Exploring the Impact of Gender-Fair Language in <span class=acl-fixed-case>G</span>erman Text Classification</a></strong><br><a href=/people/a/andreas-waldis/>Andreas Waldis</a>
|
<a href=/people/j/joel-birrer/>Joel Birrer</a>
|
<a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--592><div class="card-body p-3 small">Gender-fair language, an evolving linguistic variation in German, fosters inclusion by addressing all genders or using neutral forms. However, there is a notable lack of resources to assess the impact of this language shift on language models (LMs) might not been trained on examples of this variation. Addressing this gap, we present Lou, the first dataset providing high-quality reformulations for German text classification covering seven tasks, like stance detection and toxicity classification. We evaluate 16 mono- and multi-lingual LMs and find substantial label flips, reduced prediction certainty, and significantly altered attention patterns. However, existing evaluations remain valid, as LM rankings are consistent across original and reformulated instances. Our study provides initial insights into the impact of gender-fair language on classification for German. However, these findings are likely transferable to other languages, as we found consistent patterns in multi-lingual and English LMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.593.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.593.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--593 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.593 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.593/>When Generative Adversarial Networks Meet Sequence Labeling Challenges</a></strong><br><a href=/people/y/yu-tong/>Yu Tong</a>
|
<a href=/people/g/ge-chen/>Ge Chen</a>
|
<a href=/people/g/guokai-zheng/>Guokai Zheng</a>
|
<a href=/people/r/rui-li/>Rui Li</a>
|
<a href=/people/j/jiang-dazhi/>Jiang Dazhi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--593><div class="card-body p-3 small">The current framework for sequence labeling encompasses a feature extractor and a sequence tagger. This study introduces a unified framework named SLGAN, which harnesses the capabilities of Generative Adversarial Networks to address the challenges associated with Sequence Labeling tasks. SLGAN not only mitigates the limitation of GANs in backpropagating loss to discrete data but also exhibits strong adaptability to various sequence labeling tasks. Unlike traditional GANs, the discriminator within SLGAN does not discriminate whether data originates from the discriminator or the generator; instead, it focuses on predicting the correctness of each tag within the tag sequence. We conducted evaluations on six different tasks spanning four languages, including Chinese, Japanese, and Korean Word Segmentation, Chinese and English Named Entity Recognition, and Chinese Part-of-Speech Tagging. Our experimental results illustrate that SLGAN represents a versatile and highly effective solution, consistently achieving state-of-the-art or competitive performance results, irrespective of the specific task or language under consideration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.594.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.594.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--594 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.594 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.594/>Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering</a></strong><br><a href=/people/s/sungho-ko/>Sungho Ko</a>
|
<a href=/people/h/hyunjin-cho/>Hyunjin Cho</a>
|
<a href=/people/h/hyungjoo-chae/>Hyungjoo Chae</a>
|
<a href=/people/j/jinyoung-yeo/>Jinyoung Yeo</a>
|
<a href=/people/d/dongha-lee/>Dongha Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--594><div class="card-body p-3 small">Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challenging. Existing methods, like concatenation or free-form textual conversion of triples, have limitations, including duplicated entities or relations, reduced evidence density, and failure to highlight crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an LLM as a fact summarizer through distillation and preference alignment. Our extensive expeirments show that EFSum improves LLM’s zero-shot QA performance with its helpful and faithful summaries, especially when noisy facts are retrieved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.595.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.595.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--595 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.595 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.595/>Speechworthy Instruction-tuned Language Models</a></strong><br><a href=/people/h/hyundong-justin-cho/>Hyundong Justin Cho</a>
|
<a href=/people/n/nicolaas-paul-jedema/>Nicolaas Paul Jedema</a>
|
<a href=/people/l/leonardo-f-r-ribeiro/>Leonardo F. R. Ribeiro</a>
|
<a href=/people/k/karishma-sharma/>Karishma Sharma</a>
|
<a href=/people/p/pedro-szekely/>Pedro Szekely</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a>
|
<a href=/people/r/ruben-janssen/>Ruben Janssen</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--595><div class="card-body p-3 small">Current instruction-tuned language models are exclusively trained with textual preference data and thus may not be aligned to the unique requirements of other modalities, such as speech. To better align language models with the speech domain, we explore i) prompting strategies based on radio-industry best practices and ii) preference learning using a novel speech-based preference data of 20K samples collected by annotators who listen to response pairs. Both human and automatic evaluation show that both prompting and preference learning increase the speech-suitability of popular instruction tuned LLMs. More interestingly, we show that these methods are additive; combining them achieves the best win rates in head-to-head comparison, resulting in responses that are preferred or tied to the base model in 76.2% of comparisons on average. Lastly, we share lexical, syntactical, and qualitative analyses that elicit how our studied methods differ with baselines in generating more speech-suitable responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.596.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.596.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--596 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.596 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.596/>Data, Data Everywhere: A Guide for Pretraining Dataset Construction</a></strong><br><a href=/people/j/jupinder-parmar/>Jupinder Parmar</a>
|
<a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/j/joseph-jennings/>Joseph Jennings</a>
|
<a href=/people/b/bo-liu/>Bo Liu</a>
|
<a href=/people/a/aastha-jhunjhunwala/>Aastha Jhunjhunwala</a>
|
<a href=/people/z/zhilin-wang/>Zhilin Wang</a>
|
<a href=/people/m/mostofa-patwary/>Mostofa Patwary</a>
|
<a href=/people/m/mohammad-shoeybi/>Mohammad Shoeybi</a>
|
<a href=/people/b/bryan-catanzaro/>Bryan Catanzaro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--596><div class="card-body p-3 small">The impressive capabilities of recent language models can be largely attributed to the multi-trillion token pretraining datasets that they are trained on. However, model developers fail to disclose their construction methodology which has lead to a lack of open information on how to develop effective pretraining sets. To address this issue, we perform the first systematic study across the entire pipeline of pretraining set construction. First, we run ablations on existing techniques for pretraining set development to identify which methods translate to the largest gains in model accuracy on downstream evaluations. Then, we categorize the most widely used data source, web crawl snapshots, across the attributes of toxicity, quality, type of speech, and domain. Finally, we show how such attribute information can be used to further refine and improve the quality of a pretraining set. These findings constitute an actionable set of steps that practitioners can use to develop high quality pretraining sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.597.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.597.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--597 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.597 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.597/>Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together</a></strong><br><a href=/people/d/dilara-soylu/>Dilara Soylu</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/o/omar-khattab/>Omar Khattab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--597><div class="card-body p-3 small">Natural Language Processing (NLP) systems are increasingly taking the form of sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG), where each module may involve a distinct Language Model (LM) and an associated prompt template. These compound systems often lack intermediate labels or gradient flow to optimize each module, making their end-to-end optimization challenging. Here we seek strategies to optimize both the module-level LM weights and the associated prompt templates of such systems to maximize a downstream task metric. We propose for the first time combining the weight and prompt optimization strategies to optimize a modular LM pipeline by alternating between the two to get the same LM to teach itself. In experiments with multi-hop QA, mathematical reasoning, and feature-based classification using mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies optimizing the weights and prompts of a pipeline together outperform directly optimizing weights alone and prompts alone by up to 60% and 6%, respectively, on average across LMs and tasks. Our BetterTogether optimizer is released in DSPy at [http://dspy.ai](http://dspy.ai).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.598.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.598.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--598 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.598 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.598/>Demystifying Verbatim Memorization in Large Language Models</a></strong><br><a href=/people/j/jing-huang/>Jing Huang</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--598><div class="card-body p-3 small">Large Language Models (LLMs) frequently memorize long sequences verbatim, often with serious legal and privacy implications. Much prior work has studied such verbatim memorization using observational data. To complement such work, we develop a framework to study verbatim memorization in a controlled setting by continuing pre-training from Pythia checkpoints with injected sequences. We find that (1) non-trivial amounts of repetition are necessary for verbatim memorization to happen; (2) later (and presumably better) checkpoints are more likely to verbatim memorize sequences, even for out-of-distribution sequences; (3) the generation of memorized sequences is triggered by distributed model states that encode high-level features and makes important use of general language modeling capabilities. Guided by these insights, we develop stress tests to evaluate unlearning methods and find they often fail to remove the verbatim memorized information, while also degrading the LM. Overall, these findings challenge the hypothesis that verbatim memorization stems from specific model weights or mechanisms. Rather, verbatim memorization is intertwined with the LM’s general capabilities and thus will be very difficult to isolate and suppress without degrading model quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.599.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.599.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.599/><span class=acl-fixed-case>A</span>mbig<span class=acl-fixed-case>NLG</span>: Addressing Task Ambiguity in Instruction for <span class=acl-fixed-case>NLG</span></a></strong><br><a href=/people/a/ayana-niwa/>Ayana Niwa</a>
|
<a href=/people/h/hayate-iso/>Hayate Iso</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.600.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--600 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.600 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.600/>Distributional Properties of Subword Regularization</a></strong><br><a href=/people/m/marco-cognetta/>Marco Cognetta</a>
|
<a href=/people/v/vilem-zouhar/>Vilém Zouhar</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--600><div class="card-body p-3 small">Subword regularization, used widely in NLP, improves model performance by reducing the dependency on exact tokenizations, augmenting the training corpus, and exposing the model to more unique contexts during training. BPE and MaxMatch, two popular subword tokenization schemes, have stochastic dropout regularization variants. However, there has not been an analysis of the distributions formed by them.We show that these stochastic variants are heavily biased towards a small set of tokenizations per word. If the benefits of subword regularization are as mentioned, we hypothesize that biasedness artificially limits the effectiveness of these schemes. Thus, we propose an algorithm to uniformly sample tokenizations that we use as a drop-in replacement for the stochastic aspects of existing tokenizers, and find that it improves machine translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.601.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--601 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.601/><span class=acl-fixed-case>D</span>ata<span class=acl-fixed-case>T</span>ales: A Benchmark for Real-World Intelligent Data Narration</a></strong><br><a href=/people/y/yajing-yang/>Yajing Yang</a>
|
<a href=/people/q/qian-liu/>Qian Liu</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--601><div class="card-body p-3 small">We introduce DataTales, a novel benchmark designed to assess the proficiency of language models in data narration, a task crucial for transforming complex tabular data into accessible narratives. Existing benchmarks often fall short in capturing the requisite analytical complexity for practical applications. DataTales addresses this gap by offering 4.9k financial reports paired with corresponding market data, showcasing the demand for models to create clear narratives and analyze large datasets while understanding specialized terminology in the field. Our findings highlights the significant challenge that language models face in achieving the necessary precision and analytical depth for proficient data narration, suggesting promising avenues for future model development and evaluation methodologies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.602.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--602 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.602.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.602/>Towards Fast Multilingual <span class=acl-fixed-case>LLM</span> Inference: Speculative Decoding and Specialized Drafters</a></strong><br><a href=/people/e/euiin-yi/>Euiin Yi</a>
|
<a href=/people/t/taehyeon-kim/>Taehyeon Kim</a>
|
<a href=/people/h/hongseok-jeung/>Hongseok Jeung</a>
|
<a href=/people/d/du-seong-chang/>Du-Seong Chang</a>
|
<a href=/people/s/se-young-yun/>Se-Young Yun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--602><div class="card-body p-3 small">Large language models (LLMs) have revolutionized natural language processing and broadened their applicability across diverse commercial applications. However, the deployment of these models is constrained by high inference time in multilingual settings. To mitigate this challenge, this paper explores a training recipe of an assistant model in speculative decoding, which are leveraged to draft and-then its future tokens are verified by the target LLM. We show that language-specific draft models, optimized through a targeted pretrain-and-finetune strategy, substantially brings a speedup of inference time compared to the previous methods. We validate these models across various languages in inference time, out-of-domain speedup, and GPT-4o evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.603.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--603 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.603/><span class=acl-fixed-case>G</span>lobe<span class=acl-fixed-case>S</span>umm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization</a></strong><br><a href=/people/y/yangfan-ye/>Yangfan Ye</a>
|
<a href=/people/x/xiachong-feng/>Xiachong Feng</a>
|
<a href=/people/x/xiaocheng-feng/>Xiaocheng Feng</a>
|
<a href=/people/w/weitao-ma/>Weitao Ma</a>
|
<a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/d/dongliang-xu/>Dongliang Xu</a>
|
<a href=/people/q/qing-yang/>Qing Yang</a>
|
<a href=/people/h/hongtao-liu/>Hongtao Liu</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--603><div class="card-body p-3 small">News summarization in today’s global scene can be daunting with its flood of multilingual content and varied viewpoints from different sources. However, current studies often neglect such real-world scenarios as they tend to focus solely on either single-language or single-document tasks. To bridge this gap, we aim to unify Multi-lingual, Cross-lingual and Multi-document Summarization into a novel task, i.e., MCMS, which encapsulates the real-world requirements all-in-one. Nevertheless, the lack of a benchmark inhibits researchers from adequately studying this invaluable problem. To tackle this, we have meticulously constructed the GLOBESUMM dataset by first collecting a wealth of multilingual news reports and restructuring them into event-centric format. Additionally, we introduce the method of protocol-guided prompting for high-quality and cost-effective reference annotation. In MCMS, we also highlight the challenge of conflicts between news reports, in addition to the issues of redundancies and omissions, further enhancing the complexity of GLOBESUMM. Through extensive experimental analysis, we validate the quality of our dataset and elucidate the inherent challenges of the task. We firmly believe that GLOBESUMM, given its challenging nature, will greatly contribute to the multilingual communities and the evaluation of LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.604.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--604 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.604/>Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models</a></strong><br><a href=/people/t/terra-blevins/>Terra Blevins</a>
|
<a href=/people/t/tomasz-limisiewicz/>Tomasz Limisiewicz</a>
|
<a href=/people/s/suchin-gururangan/>Suchin Gururangan</a>
|
<a href=/people/m/margaret-li/>Margaret Li</a>
|
<a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--604><div class="card-body p-3 small">Despite their popularity in non-English NLP, multilingual language models often underperform monolingual ones due to inter-language competition for model parameters. We propose Cross-lingual Expert Language Models (X-ELM), which mitigate this competition by independently training language models on subsets of the multilingual corpus. This process specializes X-ELMs to different languages while remaining effective as a multilingual ensemble. Our experiments show that when given the same compute budget, X-ELM outperforms jointly trained multilingual models across all 16 considered languages and that these gains transfer to downstream tasks. X-ELM provides additional benefits over performance improvements: new experts can be iteratively added, adapting X-ELM to new languages without catastrophic forgetting. Furthermore, training is asynchronous, reducing the hardware requirements for multilingual training and democratizing multilingual modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.605.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--605 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.605.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.605/>More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation</a></strong><br><a href=/people/w/wencke-liermann/>Wencke Liermann</a>
|
<a href=/people/j/jin-xia-huang/>Jin-Xia Huang</a>
|
<a href=/people/y/yohan-lee/>Yohan Lee</a>
|
<a href=/people/k/kong-joo-lee/>Kong Joo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--605><div class="card-body p-3 small">Incorrect student answers can become valuable learning opportunities, provided that the student understands where they went wrong and why. To this end, rather than being given the correct answer, students should receive elaborated feedback on how to correct a mistake on their own. Highlighting the complex demands that the generation of such feedback places on a model’s input utilization abilities, we propose two extensions to the training pipeline. Firstly, we employ a KL regularization term between a standard and enriched input format to achieve more targeted input representations. Secondly, we add a preference optimization step to encourage student answer-adaptive feedback generation. The effectiveness of those extensions is underlined by a significant increase in model performance of 3.3 METEOR points. We go beyond traditional surface form-based metrics to assess two important dimensions of feedback quality, i.e., faithfulness and informativeness. Hereby, we are the first to propose an automatic metric measuring the degree to which feedback divulges the correct answer, that we call Informativeness Index <span class=tex-math>I<sup>2</sup></span>. We verify in how far each metric captures feedback quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.606.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--606 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.606.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.606/>Stable Language Model Pre-training by Reducing Embedding Variability</a></strong><br><a href=/people/w/woojin-chung/>Woojin Chung</a>
|
<a href=/people/j/jiwoo-hong/>Jiwoo Hong</a>
|
<a href=/people/n/na-min-an/>Na Min An</a>
|
<a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/s/se-young-yun/>Se-Young Yun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--606><div class="card-body p-3 small">Stable pre-training is essential for achieving better-performing language models. However, tracking pre-training stability is impractical due to high computational costs. We study Token Embedding Variability as a simple proxy to estimate pre-training stability. We theoretically and empirically demonstrate that Multi-head Low-Rank Attention acts as a fundamental approach to reducing instability. This is supported by empirical findings on variants on GPT-2, demonstrating improved stability and lower perplexities, even at deeper layer counts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.607.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--607 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.607/>What is lost in Normalization? Exploring Pitfalls in Multilingual <span class=acl-fixed-case>ASR</span> Model Evaluations</a></strong><br><a href=/people/k/kavya-manohar/>Kavya Manohar</a>
|
<a href=/people/l/leena-g-pillai/>Leena G Pillai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--607><div class="card-body p-3 small">This paper explores the pitfalls in evaluating multilingual automatic speech recognition (ASR) models, with a particular focus on Indic language scripts. We investigate the text normalization routine employed by leading ASR models, including OpenAI Whisper, Meta’s MMS, Seamless, and Assembly AI’s Conformer, and their unintended consequences on performance metrics. Our research reveals that current text normalization practices, while aiming to standardize ASR outputs for fair comparison, by removing inconsistencies such as variations in spelling, punctuation, and special characters, are fundamentally flawed when applied to Indic scripts. Through empirical analysis using text similarity scores and in-depth linguistic examination, we demonstrate that these flaws lead to artificially improved performance metrics for Indic languages. We conclude by proposing a shift towards developing text normalization routines that leverage native linguistic expertise, ensuring more robust and accurate evaluations of multilingual ASR models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.608.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--608 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.608.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.608/>Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets</a></strong><br><a href=/people/b/benjamin-schiller/>Benjamin Schiller</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/a/andreas-waldis/>Andreas Waldis</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--608><div class="card-body p-3 small">Topic-Dependent Argument Mining (TDAM), that is extracting and classifying argument components for a specific topic from large document sources, is an inherently difficult task for machine learning models and humans alike, as large TDAM datasets are rare and recognition of argument components requires expert knowledge. The task becomes even more difficult if it also involves stance detection of retrieved arguments. In this work, we investigate the effect of TDAM dataset composition in few- and zero-shot settings. Our findings show that, while fine-tuning is mandatory to achieve acceptable model performance, using carefully composed training samples and reducing the training sample size by up to almost 90% can still yield 95% of the maximum performance. This gain is consistent across three TDAM tasks on three different datasets. We also publish a new dataset and code for future benchmarking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.609.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--609 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.609/>Kiss up, Kick down: Exploring Behavioral Changes in Multi-modal Large Language Models with Assigned Visual Personas</a></strong><br><a href=/people/s/seungjong-sun/>Seungjong Sun</a>
|
<a href=/people/e/eungu-lee/>Eungu Lee</a>
|
<a href=/people/s/seo-yeon-baek/>Seo Yeon Baek</a>
|
<a href=/people/s/seunghyun-hwang/>Seunghyun Hwang</a>
|
<a href=/people/w/wonbyung-lee/>Wonbyung Lee</a>
|
<a href=/people/d/dongyan-nan/>Dongyan Nan</a>
|
<a href=/people/b/bernard-j-jansen/>Bernard J Jansen</a>
|
<a href=/people/j/jang-hyun-kim/>Jang Hyun Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--609><div class="card-body p-3 small">This study is the first to explore whether multi-modal large language models (LLMs) can align their behaviors with visual personas, addressing a significant gap in the literature that predominantly focuses on text-based personas. We developed a novel dataset of 5K fictional avatar images for assignment as visual personas to LLMs, and analyzed their negotiation behaviors based on the visual traits depicted in these images, with a particular focus on aggressiveness. The results indicate that LLMs assess the aggressiveness of images in a manner similar to humans and output more aggressive negotiation behaviors when prompted with an aggressive visual persona. Interestingly, the LLM exhibited more aggressive negotiation behaviors when the opponent’s image appeared less aggressive than their own, and less aggressive behaviors when the opponent’s image appeared more aggressive.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.610.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--610 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.610.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.610/><span class=acl-fixed-case>ATM</span>: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator</a></strong><br><a href=/people/j/junda-zhu/>Junda Zhu</a>
|
<a href=/people/l/lingyong-yan/>Lingyong Yan</a>
|
<a href=/people/h/haibo-shi/>Haibo Shi</a>
|
<a href=/people/d/dawei-yin/>Dawei Yin</a>
|
<a href=/people/l/lei-sha/>Lei Sha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--610><div class="card-body p-3 small">Large language models (LLMs) are proven to benefit a lot from retrieval-augmented generation (RAG) in alleviating hallucinations confronted with knowledge-intensive questions. RAG adopts information retrieval techniques to inject external knowledge from semantic-relevant documents as input contexts. However, due to today’s Internet being flooded with numerous noisy and fabricating content, it is inevitable that RAG systems are vulnerable to these noises and prone to respond incorrectly. To this end, we propose to optimize the retrieval-augmented Generator with a Adversarial Tuning Multi-agent system **(ATM)**. The ATM steers the Generator to have a robust perspective of useful documents for question answering with the help of an auxiliary Attacker agent. The Generator and the Attacker are tuned adversarially for several iterations. After rounds of multi-agent iterative tuning, the Generator can eventually better discriminate useful documents amongst fabrications. The experimental results verify the effectiveness of ATM and we also observe that the Generator can achieve better performance compared to state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.611.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--611 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.611/>Dynamic Multi-granularity Attribution Network for Aspect-based Sentiment Analysis</a></strong><br><a href=/people/y/yanjiang-chen/>Yanjiang Chen</a>
|
<a href=/people/k/kai-zhang/>Kai Zhang</a>
|
<a href=/people/f/feng-hu/>Feng Hu</a>
|
<a href=/people/x/xianquan-wang/>Xianquan Wang</a>
|
<a href=/people/r/ruikang-li/>Ruikang Li</a>
|
<a href=/people/q/qi-liu/>Qi Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--611><div class="card-body p-3 small">Aspect-based sentiment analysis (ABSA) aims to predict the sentiment polarity of a specific aspect within a given sentence. Most existing methods predominantly leverage semantic or syntactic information based on attention scores, which are susceptible to interference caused by irrelevant contexts and often lack sentiment knowledge at a data-specific level. In this paper, we propose a novel Dynamic Multi-granularity Attribution Network (DMAN) from the perspective of attribution. Initially, we leverage Integrated Gradients to dynamically extract attribution scores for each token, which contain underlying reasoning knowledge for sentiment analysis. Subsequently, we aggregate attribution representations from multiple semantic granularities in natural language, enhancing a profound understanding of the semantics. Finally, we integrate attribution scores with syntactic information to capture the relationships between aspects and their relevant contexts more accurately during the sentence understanding process. Extensive experiments on five benchmark datasets demonstrate the effectiveness of our proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.612.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.612.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--612 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.612/>Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization</a></strong><br><a href=/people/s/shahed-masoudian/>Shahed Masoudian</a>
|
<a href=/people/m/markus-frohmann/>Markus Frohmann</a>
|
<a href=/people/n/navid-rekabsaz/>Navid Rekabsaz</a>
|
<a href=/people/m/markus-schedl/>Markus Schedl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--612><div class="card-body p-3 small">Language models frequently inherit societal biases from their training data. Numerous techniques have been proposed to mitigate these biases during both the pre-training and fine-tuning stages. However, fine-tuning a pre-trained debiased language model on a downstream task can reintroduce biases into the model. Additionally, existing debiasing methods for downstream tasks either (i) require labels of protected attributes (e.g., age, race, or political views) that are often not available or (ii) rely on indicators of bias, which restricts their applicability to gender debiasing since they rely on gender-specific words. To address this, we introduce a novel debiasing regularization technique based on the class-wise variance of embeddings. Crucially, our method does not require attribute labels and targets any attribute, thus addressing the shortcomings of existing debiasing methods. Our experiments on encoder language models and three datasets demonstrate that our method outperforms existing strong debiasing baselines that rely on target attribute labels while maintaining performance on the target task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.613.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.613.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--613 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.613 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.613/>Large Language Models Know What is Key Visual Entity: An <span class=acl-fixed-case>LLM</span>-assisted Multimodal Retrieval for <span class=acl-fixed-case>VQA</span></a></strong><br><a href=/people/p/pu-jian/>Pu Jian</a>
|
<a href=/people/d/donglei-yu/>Donglei Yu</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--613><div class="card-body p-3 small">Visual question answering (VQA) tasks, often performed by visual language model (VLM), face challenges with long-tail knowledge. Recent retrieval-augmented VQA (RA-VQA) systems address this by retrieving and integrating external knowledge sources. However, these systems still suffer from redundant visual information irrelevant to the question during retrieval. To address these issues, in this paper, we propose LLM-RA, a novel method leveraging the reasoning capability of a large language model (LLM) to identify key visual entities, thus minimizing the impact of irrelevant information in the query of retriever. Furthermore, key visual entities are independently encoded for multimodal joint retrieval, preventing cross-entity interference. Experimental results demonstrate that our method outperforms other strong RA-VQA systems. In two knowledge-intensive VQA benchmarks, our method achieves the new state-of-the-art performance among those with similar scale of parameters and even performs comparably to models with 1-2 orders larger parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.614.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.614.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--614 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.614 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.614/>Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights</a></strong><br><a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/l/lizhen-qu/>Lizhen Qu</a>
|
<a href=/people/e/ehsan-shareghi/>Ehsan Shareghi</a>
|
<a href=/people/r/reza-haf/>Reza Haf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--614><div class="card-body p-3 small">Large Multimodal Models (LMMs) have achieved great success recently, demonstrating a strong capability to understand multimodal information and to interact with human users. Despite the progress made, the challenge of detecting high-risk interactions in multimodal settings, and in particular in speech modality, remains largely unexplored. Conventional research on risk for speech modality primarily emphasises the content (e.g., what is captured as transcription). However, in speech-based interactions, paralinguistic cues in audio can significantly alter the intended meaning behind utterances. In this work, we propose a speech-specific risk taxonomy, covering 8 risk categories under hostility (malicious sarcasm and threats), malicious imitation (age, gender, ethnicity), and stereotypical biases (age, gender, ethnicity). Based on the taxonomy, we create a small-scale dataset for evaluating current LMMs capability in detecting these categories of risk. We observe even the latest models remain ineffective to detect various paralinguistic-specific risks in speech (e.g., Gemini 1.5 Pro is performing only slightly above random baseline). Warning: this paper contains biased and offensive examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.615.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--615 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.615/>Self-<span class=acl-fixed-case>AMPLIFY</span>: Improving Small Language Models with Self Post Hoc Explanations</a></strong><br><a href=/people/m/milan-bhan/>Milan Bhan</a>
|
<a href=/people/j/jean-noel-vittaut/>Jean-Noël Vittaut</a>
|
<a href=/people/n/nicolas-chesneau/>Nicolas Chesneau</a>
|
<a href=/people/m/marie-jeanne-lesot/>Marie-Jeanne Lesot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--615><div class="card-body p-3 small">Incorporating natural language rationales in the prompt and In-Context Learning (ICL) have led to a significant improvement of Large Language Models (LLMs) performance. However, generating high-quality rationales require human-annotation or the use of auxiliary proxy models. In this work, we propose Self-AMPLIFY to automatically generate rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on four SLMs and five datasets requiring strong reasoning abilities. Self-AMPLIFY achieves good results against competitors, leading to strong accuracy improvement. Self-AMPLIFY is the first method to apply post hoc explanation methods to autoregressive language models to generate rationales to improve their own performance in a fully automated manner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.616.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--616 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.616/>What are the Generator Preferences for End-to-end Task-Oriented Dialog System?</a></strong><br><a href=/people/w/wanshi-xu/>Wanshi Xu</a>
|
<a href=/people/x/xianwei-zhuang/>Xianwei Zhuang</a>
|
<a href=/people/z/zhanpeng-chen/>Zhanpeng Chen</a>
|
<a href=/people/z/zhihong-zhu/>Zhihong Zhu</a>
|
<a href=/people/x/xuxin-cheng/>Xuxin Cheng</a>
|
<a href=/people/y/yuexian-zou/>Yuexian Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--616><div class="card-body p-3 small">Fully end-to-end task-oriented dialogue (EToD) systems have shown excellent performance, which requires the ability to retrieve entities accurately for generation. Existing methods improve the accuracy of entity retrieval and construct data flows between retrieval results and response generator, achieving promising results. However, most of them suffer from the following issues: (1) The entity is retrieved by directly interacting with the context at a coarse-grained level, so the similarity score may be disturbed by irrelevant attributes; (2) The generator pays equal attention to retrieved entities and the context and does not learn the generation preferences for the current turn. In this paper, we propose a framework called Regulating Preferences of Generator (RPG) based on retrieval results, which includes a generator preference extractor, an entity retriever, and a generator with the gate-controlled preference regulator. The generator preference extractor not only improves the entity retriever by filtering the interference of irrelevant attributes but also provides more focused guidance to the generator by performing inter-turn attribute prediction. Experiments and analyses on three standard benchmarks show that our framework outperforms existing methods and improves the quality of the dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.617.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.617.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--617 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.617 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.617/>Paraphrase Types Elicit Prompt Engineering Capabilities</a></strong><br><a href=/people/j/jan-philip-wahle/>Jan Philip Wahle</a>
|
<a href=/people/t/terry-ruas/>Terry Ruas</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a>
|
<a href=/people/b/bela-gipp/>Bela Gipp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--617><div class="card-body p-3 small">Much of the success of modern language models depends on finding a suitable prompt to instruct the model. Until now, it has been largely unknown how variations in the linguistic expression of prompts affect these models. This study systematically and empirically evaluates which linguistic features influence models through paraphrase types, i.e., different linguistic changes at particular positions. We measure behavioral changes for five models across 120 tasks and six families of paraphrases (i.e., morphology, syntax, lexicon, lexico-syntax, discourse, and others). We also control for other prompt engineering factors (e.g., prompt length, lexical diversity, and proximity to training data). Our results show a potential for language models to improve tasks when their prompts are adapted in specific paraphrase types (e.g., 6.7% median gain in Mixtral 8x7B; 5.5% in LLaMA 3 8B). In particular, changes in morphology and lexicon, i.e., the vocabulary used, showed promise in improving prompts. These findings contribute to developing more robust language models capable of handling variability in linguistic expression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.618.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.618.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--618 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.618.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.618.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.618/><span class=acl-fixed-case>VLEU</span>: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models</a></strong><br><a href=/people/j/jingtao-cao/>Jingtao Cao</a>
|
<a href=/people/z/zhang-zheng/>Zhang Zheng</a>
|
<a href=/people/h/hongru-wang/>Hongru Wang</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--618><div class="card-body p-3 small">Progress in Text-to-Image (T2I) models has significantly advanced the generation of images from textual descriptions. Existing metrics, such as CLIP, effectively measure the semantic alignment between single prompts and their corresponding images. However, they fall short in evaluating a model’s ability to generalize across a broad spectrum of textual inputs. To address this gap, we propose the VLEU (<b>V</b>isual <b>L</b>anguage <b>E</b>valuation <b>U</b>nderstudy) metric. VLEU leverages the power of Large Language Models (LLMs) to sample from the visual text domain, encompassing the entire range of potential inputs for the T2I task, to generate a wide variety of visual text. The images generated by T2I models from these prompts are then assessed for their alignment with the input text using the CLIP model. VLEU quantitatively measures a model’s generalizability by computing the Kullback-Leibler (KL) divergence between the visual text marginal distribution and the conditional distribution over the images generated by the model. This provides a comprehensive metric for comparing the overall generalizability of T2I models, beyond single-prompt evaluations, and offers valuable insights during the finetuning process. Our experimental results demonstrate VLEU’s effectiveness in evaluating the generalizability of various T2I models, positioning it as an essential metric for future research and development in image synthesis from text prompts. Our code and data will be publicly available at <a href=https://github.com/mio7690/VLEU class=acl-markup-url>https://github.com/mio7690/VLEU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.619.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.619.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--619 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.619.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.619/>Towards Online Continuous Sign Language Recognition and Translation</a></strong><br><a href=/people/r/ronglai-zuo/>Ronglai Zuo</a>
|
<a href=/people/f/fangyun-wei/>Fangyun Wei</a>
|
<a href=/people/b/brian-mak/>Brian Mak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--619><div class="card-body p-3 small">Research on continuous sign language recognition (CSLR) is essential to bridge the communication gap between deaf and hearing individuals. Numerous previous studies have trained their models using the connectionist temporal classification (CTC) loss. During inference, these CTC-based models generally require the entire sign video as input to make predictions, a process known as offline recognition, which suffers from high latency and substantial memory usage. In this work, we take the first step towards online CSLR. Our approach consists of three phases: 1) developing a sign dictionary; 2) training an isolated sign language recognition model on the dictionary; and 3) employing a sliding window approach on the input sign sequence, feeding each sign clip to the optimized model for online recognition. Additionally, our online recognition model can be extended to support online translation by integrating a gloss-to-text network and can enhance the performance of any offline model. With these extensions, our online approach achieves new state-of-the-art performance on three popular benchmarks across various task settings. Code and models are available at https://github.com/FangyunWei/SLRT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.620.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.620.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--620 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.620 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.620/>Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment</a></strong><br><a href=/people/y/yiwei-dai/>Yiwei Dai</a>
|
<a href=/people/h/hengrui-gu/>Hengrui Gu</a>
|
<a href=/people/y/ying-wang/>Ying Wang</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--620><div class="card-body p-3 small">Although pre-trained language models (PLMs) have been widely used in natural language understandings (NLU), they are still exposed to fairness issues. Most existing extrinsic debiasing methods rely on manually curated word lists for each sensitive groups to modify training data or to add regular constraints. However, these word lists are often limited by length and scope, resulting in the degradation performance of extrinsic bias mitigation. To address the aforementioned issues, we propose a **C**ontinuous **P**rompts **A**djustment **D**ebiasing method (CPAD), which generates continuous token lists from the entire vocabulary space and uses them to bridge the gap between outputs and targets in fairness learning process. Specifically, CPAD encapsulates fine-tuning objective and debiasing objectives into several independent prompts. To avoid the limitation of manual word lists, in fairness learning phase, we extract outputs from the entire vocabulary space via fine-tuned PLM. Then, we aggregate the outputs from the same sensitive group as continuous token lists to map the outputs into protected attribute labels. Finally, after we learn the debiasing prompts in the perspective of adversarial learning, we improve fairness by adjusting continuous prompts at model inference time. Through extensive experiments on three NLU tasks, we evaluate the debiasing performance from the perspectives of group fairness and fairness through unawareness. The experimental results show that CPAD outperforms all baselines in term of single and two-attributes debiasing performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.621.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--621 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.621.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.621.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.621/>Split and Merge: Aligning Position Biases in <span class=acl-fixed-case>LLM</span>-based Evaluators</a></strong><br><a href=/people/z/zongjie-li/>Zongjie Li</a>
|
<a href=/people/c/chaozheng-wang/>Chaozheng Wang</a>
|
<a href=/people/p/pingchuan-ma/>Pingchuan Ma</a>
|
<a href=/people/d/daoyuan-wu/>Daoyuan Wu</a>
|
<a href=/people/s/shuai-wang/>Shuai Wang</a>
|
<a href=/people/c/cuiyun-gao/>Cuiyun Gao</a>
|
<a href=/people/y/yang-liu/>Yang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--621><div class="card-body p-3 small">Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, taking into account both length and semantics, and merges them back into a single prompt for evaluation by LLMs. Extensive experiments with six LLMs on 11,520 answer pairs demonstrate that PORTIA markedly enhances the consistency rates for all models and forms of comparison tested, achieving an average relative improvement of 47.46%. It also enables PORTIA-enhanced GPT-3.5 to achieve agreement rates with humans comparable to GPT-4 and elevates GPT-4’s consistency rate up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced GPT-3.5 model can even surpass standalone GPT-4 in terms of alignment with human evaluators, highlighting PORTIA’s ability to correct position bias, improve LLM consistency, and boost performance while keeping cost efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.622.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--622 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.622/>Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation</a></strong><br><a href=/people/s/sougata-saha/>Sougata Saha</a>
|
<a href=/people/r/rohini-k-srihari/>Rohini Srihari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--622><div class="card-body p-3 small">The proliferation of online misinformation presents a significant challenge, requiring scalable strategies for effective mitigation. While detection methods exist, current reactive approaches, like content flagging and banning, are short-term and insufficient. Additionally, advancements like large language models (LLMs) exacerbate the issue by enabling large-scale creation and dissemination of misinformation. Thus, sustainable, scalable solutions that encourage behavior change and broaden perspectives by persuading misinformants against their viewpoints or broadening their perspectives are needed. To this end, we propose persuasive LLM-based dialogue systems to tackle misinformation. However, challenges arise due to the lack of suitable datasets and formal frameworks for generating persuasive responses. Inspired by existing methods for countering online hate speech, we explore adapting counter-hate response strategies for misinformation. Since misinformation and hate speech often coexist despite differing intentions, we develop classifiers to identify and annotate response strategies from hate-speech counter-responses for use in misinformation scenarios. Human evaluations show a 91% agreement on the applicability of these strategies to misinformation. Next, as a scalable counter-misinformation solution, we create an LLM-based argument graph framework that generates persuasive responses, using the strategies as control codes to adjust the style and content. Human evaluations and case studies demonstrate that our framework generates expert-like responses and is 14% more engaging, 21% more natural, and 18% more factual than the best available alternatives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.623.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.623.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--623 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.623/><span class=acl-fixed-case>BPO</span>: Staying Close to the Behavior <span class=acl-fixed-case>LLM</span> Creates Better Online <span class=acl-fixed-case>LLM</span> Alignment</a></strong><br><a href=/people/w/wenda-xu/>Wenda Xu</a>
|
<a href=/people/j/jiachen-li/>Jiachen Li</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--623><div class="card-body p-3 small">Direct alignment from preferences (DAP) has emerged as a promising paradigm for aligning large language models (LLMs) to human desiderata from pre-collected, offline preference datasets. While recent studies indicate that existing offline DAP methods can directly benefit from online training samples, we highlight the need to develop specific online DAP algorithms to fully harness the power of online training. Specifically, we identify that the learned LLM should adhere to the proximity of the behavior LLM, which collects the training samples. To this end, we propose online Preference Optimization in proximity to the Behavior LLM (BPO), emphasizing the importance of constructing a proper trust region for LLM alignment.We conduct extensive experiments to validate the effectiveness and applicability of our approach by integrating it with various DAP methods, resulting in significant performance improvements across a wide range of tasks when training with the same amount of preference data. Even when only introducing one additional data collection phase, our online BPO improves its offline DAP baseline from 72.0% to 80.2% on TL;DR and from 82.2% to 89.1% on Anthropic Helpfulness in terms of win rate against human reference text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.624.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--624 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.624.software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.624/><span class=acl-fixed-case>O</span>ne2<span class=acl-fixed-case>S</span>et + Large Language Model: Best Partners for Keyphrase Generation</a></strong><br><a href=/people/l/liangying-shao/>Liangying Shao</a>
|
<a href=/people/l/liang-zhang/>Liang Zhang</a>
|
<a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/g/guoqi-ma/>Guoqi Ma</a>
|
<a href=/people/h/hao-yue/>Hao Yue</a>
|
<a href=/people/m/mingming-sun/>Mingming Sun</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--624><div class="card-body p-3 small">Keyphrase generation (KPG) aims to automatically generate a collection of phrases representing the core concepts of a given document. The dominant paradigms in KPG include one2seq and one2set. Recently, there has been increasing interest in applying large language models (LLMs) to KPG. Our preliminary experiments reveal that it is challenging for a single model to excel in both recall and precision. Further analysis shows that: 1) the one2set paradigm owns the advantage of high recall, but suffers from improper assignments of supervision signals during training; 2) LLMs are powerful in keyphrase selection, but existing selection methods often make redundant selections. Given these observations, we introduce a generate-then-select framework decomposing KPG into two steps, where we adopt a one2set-based model as generator to produce candidates and then use an LLM as selector to select keyphrases from these candidates. Particularly, we make two important improvements on our generator and selector: 1) we design an Optimal Transport-based assignment strategy to address the above improper assignments; 2) we model the keyphrase selection as a sequence labeling task to alleviate redundant selections. Experimental results on multiple benchmark datasets show that our framework significantly surpasses state-of-the-art models, especially in absent keyphrase prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.625.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.625.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--625 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.625.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.625/>Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering</a></strong><br><a href=/people/y/yifei-yuan/>Yifei Yuan</a>
|
<a href=/people/y/yang-deng/>Yang Deng</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/m/mohammad-aliannejadi/>Mohammad Aliannejadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--625><div class="card-body p-3 small">Users post numerous product-related questions on e-commerce platforms, affecting their purchase decisions. Product-related question answering (PQA) entails utilizing product-related resources to provide precise responses to users. We propose a novel task of Multilingual Cross-market Product-based Question Answering (MCPQA) and define the task as providing answers to product-related questions in a main marketplace by utilizing information from another resource-rich auxiliary marketplace in a multilingual context. We introduce a large-scale dataset comprising over 7 million questions from 17 marketplaces across 11 languages. We then perform automatic translation on the Electronics category of our dataset, naming it as McMarket. We focus on two subtasks: review-based answer generation and product-related question ranking. For each subtask, we label a subset of McMarket using an LLM and further evaluate the quality of the annotations via human assessment. We then conduct experiments to benchmark our dataset, using models ranging from traditional lexical models to LLMs in both single-market and cross-market scenarios across McMarket and the corresponding LLM subset. Results show that incorporating cross-market information significantly enhances performance in both tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.626.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.626.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--626 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.626 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.626.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.626/><span class=acl-fixed-case>ORPO</span>: Monolithic Preference Optimization without Reference Model</a></strong><br><a href=/people/j/jiwoo-hong/>Jiwoo Hong</a>
|
<a href=/people/n/noah-lee/>Noah Lee</a>
|
<a href=/people/j/james-thorne/>James Thorne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--626><div class="card-body p-3 small">While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we revisit SFT in the context of preference alignment, emphasizing that a minor penalty for the disfavored style is sufficient for preference alignment. Building on this foundation, we introduce a straightforward reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the need for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models including Llama-2 Chat and Zephyr with more than 7B and 13B parameters: achieving up to 12.20% on AlpacaEval 2.0 (Figure 1), and 7.32 in MT-Bench (Table 2). We release code and model checkpoints for Mistral-ORPO-<span class=tex-math>𝛼</span> (7B) and Mistral-ORPO-<span class=tex-math>𝛽</span> (7B).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.627.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.627.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--627 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.627 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.627/>A Multi-Perspective Analysis of Memorization in Large Language Models</a></strong><br><a href=/people/b/bowen-chen/>Bowen Chen</a>
|
<a href=/people/n/namgi-han/>Namgi Han</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--627><div class="card-body p-3 small">Large Language Models (LLMs) can generate the same sequences contained in the pre-train corpora, known as memorization.Previous research studied it at a macro level, leaving micro yet important questions under-explored, e.g., what makes sentences memorized, the dynamics when generating memorized sequence, its connection to unmemorized sequence, and its predictability.We answer the above questions by analyzing the relationship of memorization with outputs from LLM, namely, embeddings, probability distributions, and generated tokens.A memorization score is calculated as the overlap between generated tokens and actual continuations when the LLM is prompted with a context sequence from the pre-train corpora.Our findings reveal:(1) The inter-correlation between memorized/unmemorized sentences, model size, continuation size, and context size, as well as the transition dynamics between sentences of different memorization scores,(2) A sudden drop and increase in the frequency of input tokens when generating memorized/unmemorized sequences (boundary effect),(3) Cluster of sentences with different memorization scores in the embedding space,(4) An inverse boundary effect in the entropy of probability distributions for generated memorized/unmemorized sequences,(5) The predictability of memorization is related to model size and continuation length. In addition, we show a Transformer model trained by the hidden states of LLM can predict unmemorized tokens.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.628.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.628.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--628 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.628 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.628.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.628.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.628/>Do <span class=acl-fixed-case>LLM</span>s suffer from Multi-Party Hangover? A Diagnostic Approach to Addressee Recognition and Response Selection in Conversations</a></strong><br><a href=/people/n/nicolo-penzo/>Nicolò Penzo</a>
|
<a href=/people/m/maryam-sajedinia/>Maryam Sajedinia</a>
|
<a href=/people/b/bruno-lepri/>Bruno Lepri</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/m/marco-guerini/>Marco Guerini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--628><div class="card-body p-3 small">Assessing the performance of systems to classify Multi-Party Conversations (MPC) is challenging due to the interconnection between linguistic and structural characteristics of conversations. Conventional evaluation methods often overlook variances in model behavior across different levels of structural complexity on interaction graphs. In this work, we propose a methodological pipeline to investigate model performance across specific structural attributes of conversations. As a proof of concept we focus on Response Selection and Addressee Recognition tasks, to diagnose model weaknesses. To this end, we extract representative diagnostic subdatasets with a fixed number of users and a good structural variety from a large and open corpus of online MPCs. We further frame our work in terms of data minimization, avoiding the use of original usernames to preserve privacy, and propose alternatives to using original text messages. Results show that response selection relies more on the textual content of conversations, while addressee recognition requires capturing their structural dimension. Using an LLM in a zero-shot setting, we further highlight how sensitivity to prompt variations is task-dependent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.629.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--629 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.629/>Code Prompting Elicits Conditional Reasoning Abilities in <span class=acl-fixed-case>T</span>ext+<span class=acl-fixed-case>C</span>ode <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/h/haritz-puerto/>Haritz Puerto</a>
|
<a href=/people/m/martin-tutek/>Martin Tutek</a>
|
<a href=/people/s/somak-aditya/>Somak Aditya</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--629><div class="card-body p-3 small">Reasoning is a fundamental component of language understanding. Recent prompting techniques, such as chain of thought, have consistently improved LLMs’ performance on various reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs in the inference stage. In this paper, we investigate the effect of the input representation on the reasoning abilities of LLMs. We hypothesize that representing natural language tasks as code can enhance specific reasoning abilities such as entity tracking or logical reasoning. To study this, we propose code prompting, a methodology we operationalize as a chain of prompts that transforms a natural language problem into code and directly prompts the LLM using the generated code without resorting to external code execution. We find that code prompting exhibits a high-performance boost for multiple LLMs (up to 22.52 percentage points on GPT 3.5, 7.75 on Mixtral, and 16.78 on Mistral) across multiple conditional reasoning datasets. We then conduct comprehensive experiments to understand how the code representation triggers reasoning abilities and which capabilities are elicited in the underlying models. Our analysis on GPT 3.5 reveals that the code formatting of the input problem is essential for performance improvement. Furthermore, the code representation improves sample efficiency of in-context learning and facilitates state tracking of entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.630.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.630.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--630 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.630 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.630/>Unveiling the Role of Pretraining in Direct Speech Translation</a></strong><br><a href=/people/b/belen-alastruey/>Belen Alastruey</a>
|
<a href=/people/g/gerard-i-gallego/>Gerard I. Gállego</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--630><div class="card-body p-3 small">Direct speech-to-text translation systems encounter an important drawback in data scarcity. A common solution consists on pretraining the encoder on automatic speech recognition, hence losing efficiency in the training process. In this study, we compare the training dynamics of a system using a pretrained encoder, the conventional approach, and one trained from scratch. We observe that, throughout the training, the randomly initialized model struggles to incorporate information from the speech inputs for its predictions. Hence, we hypothesize that this issue stems from the difficulty of effectively training an encoder for direct speech translation. While a model trained from scratch needs to learn acoustic and semantic modeling simultaneously, a pretrained one can just focus on the latter. Based on these findings, we propose a subtle change in the decoder cross-attention to integrate source information from earlier steps in training. We show that with this change, the model trained from scratch can achieve comparable performance to the pretrained one, while reducing the training time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.631.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.631.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--631 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.631 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.631/><span class=acl-fixed-case>PCQPR</span>: Proactive Conversational Question Planning with Reflection</a></strong><br><a href=/people/s/shasha-guo/>Shasha Guo</a>
|
<a href=/people/l/lizi-liao/>Lizi Liao</a>
|
<a href=/people/j/jing-zhang/>Jing Zhang</a>
|
<a href=/people/c/cuiping-li/>Cuiping Li</a>
|
<a href=/people/h/hong-chen/>Hong Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--631><div class="card-body p-3 small">Conversational Question Generation (CQG) enhances the interactivity of conversational question-answering systems in fields such as education, customer service, and entertainment. However, traditional CQG, focusing primarily on the immediate context, lacks the conversational foresight necessary to guide conversations toward specified conclusions. This limitation significantly restricts their ability to achieve conclusion-oriented conversational outcomes. In this work, we redefine the CQG task as Conclusion-driven Conversational Question Generation (CCQG) by focusing on proactivity, not merely reacting to the unfolding conversation but actively steering it towards a conclusion-oriented question-answer pair. To address this, we propose a novel approach, called Proactive Conversational Question Planning with self-Refining (PCQPR). Concretely, by integrating a planning algorithm inspired by Monte Carlo Tree Search (MCTS) with the analytical capabilities of large language models (LLMs), PCQPR predicts future conversation turns and continuously refines its questioning strategies. This iterative self-refining mechanism ensures the generation of contextually relevant questions strategically devised to reach a specified outcome. Our extensive evaluations demonstrate that PCQPR significantly surpasses existing CQG methods, marking a paradigm shift towards conclusion-oriented conversational question-answering systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.632.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.632.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--632 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.632 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.632.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.632.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.632/><span class=acl-fixed-case>C</span>ode<span class=acl-fixed-case>A</span>gent: Autonomous Communicative Agents for Code Review</a></strong><br><a href=/people/x/xunzhu-tang/>Xunzhu Tang</a>
|
<a href=/people/k/kisub-kim/>Kisub Kim</a>
|
<a href=/people/y/yewei-song/>Yewei Song</a>
|
<a href=/people/c/cedric-lothritz/>Cedric Lothritz</a>
|
<a href=/people/b/bei-li/>Bei Li</a>
|
<a href=/people/s/saad-ezzini/>Saad Ezzini</a>
|
<a href=/people/h/haoye-tian/>Haoye Tian</a>
|
<a href=/people/j/jacques-klein/>Jacques Klein</a>
|
<a href=/people/t/tegawende-f-bissyande/>Tegawendé F. Bissyandé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--632><div class="card-body p-3 small">Code review, which aims at ensuring the overall quality and reliability of software, is a cornerstone of software development. Unfortunately, while crucial, Code review is a labor-intensive process that the research community is looking to automate. Existing automated methods rely on single input-output generative models and thus generally struggle to emulate the collaborative nature of code review. This work introduces CodeAgent, a novel multi-agent Large Language Model (LLM) system for code review automation. CodeAgent incorporates a supervisory agent, QA-Checker, to ensure that all the agents’ contributions address the initial review question. We evaluated CodeAgent on critical code review tasks: (1) detect inconsistencies between code changes and commit messages, (2) identify vulnerability introductions, (3) validate code style adherence, and (4) suggest code revisions. The results demonstrate CodeAgent’s effectiveness, contributing to a new state-of-the-art in code review automation. Our data and code are publicly available (<a href=https://github.com/Daniel4SE/codeagent class=acl-markup-url>https://github.com/Daniel4SE/codeagent</a>).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.633.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.633.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--633 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.633 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.633/><span class=acl-fixed-case>T</span>ro<span class=acl-fixed-case>L</span>: Traversal of Layers for Large Language and Vision Models</a></strong><br><a href=/people/b/byung-kwan-lee/>Byung-Kwan Lee</a>
|
<a href=/people/s/sangyun-chung/>Sangyun Chung</a>
|
<a href=/people/c/chae-won-kim/>Chae Won Kim</a>
|
<a href=/people/b/beomchan-park/>Beomchan Park</a>
|
<a href=/people/y/yong-man-ro/>Yong Man Ro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--633><div class="card-body p-3 small">Large language and vision models (LLVMs) have been driven by the generalization power of large language models (LLMs) and the advent of visual instruction tuning. Along with scaling them up directly, these models enable LLVMs to showcase powerful vision language (VL) performances by covering diverse tasks via natural language instructions. However, existing open-source LLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often considered too large (e.g., 26B, 34B, and 110B parameters), having a larger number of layers. These large models demand costly, high-end resources for both training and inference. To address this issue, we present a new efficient LLVM family with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL), which enables the reuse of layers in a token-wise manner. This layer traversing technique simulates the effect of looking back and retracing the answering stream while increasing the number of forward propagation layers without physically adding more layers. We demonstrate that TroL employs a simple layer traversing approach yet efficiently outperforms the open-source LLVMs with larger model sizes and rivals the performances of the closed-source LLVMs with substantial sizes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.634.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.634.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--634 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.634 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.634.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.634/><span class=acl-fixed-case>MMTE</span>: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language</a></strong><br><a href=/people/s/shun-wang/>Shun Wang</a>
|
<a href=/people/g/ge-zhang/>Ge Zhang</a>
|
<a href=/people/h/han-wu/>Han Wu</a>
|
<a href=/people/t/tyler-loakman/>Tyler Loakman</a>
|
<a href=/people/w/wenhao-huang/>Wenhao Huang</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--634><div class="card-body p-3 small">Machine Translation (MT) has developed rapidly since the release of Large Language Models and current MT evaluation is performed through comparison with reference human translations or by predicting quality scores from human-labeled data. However, these mainstream evaluation methods mainly focus on fluency and factual reliability, whilst paying little attention to figurative quality. In this paper, we investigate the figurative quality of MT and propose a set of human evaluation metrics focused on the translation of figurative language. We additionally present a multilingual parallel metaphor corpus generated by post-editing. Our evaluation protocol is designed to estimate four aspects of MT: Metaphorical Equivalence, Emotion, Authenticity, and Quality. In doing so, we observe that translations of figurative expressions display different traits from literal ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.635.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.635.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--635 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.635 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.635.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.635/>Revisiting Supertagging for faster <span class=acl-fixed-case>HPSG</span> parsing</a></strong><br><a href=/people/o/olga-zamaraeva/>Olga Zamaraeva</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--635><div class="card-body p-3 small">We present new supertaggers trained on English HPSG-based treebanks and test the effects of the best tagger on parsing speed and accuracy. HPSG treebanks are produced automatically by large manually built grammars and feature high-quality annotation based on a well-developed linguistic theory. The English Resource Grammar treebanks include diverse and challenging test datasets, beyond the usual WSJ section 23 and Wikipedia data. HPSG supertagging has previously relied on MaxEnt-based models. We use SVM and neural CRF- and BERT-based methods and show that both SVM and neural supertaggers achieve considerably higher accuracy compared to the baseline and lead to an increase not only in the parsing speed but also the parser accuracy with respect to gold dependency structures. Our fine-tuned BERT-based tagger achieves 97.26% accuracy on 950 sentences from WSJ23 and 93.88% on the out-of-domain technical essay The Cathedral and the Bazaar. We present experiments with integrating the best supertagger into an HPSG parser and observe a speedup of a factor of 3 with respect to the system which uses no tagging at all, as well as large recall gains and an overall precision gain. We also compare our system to an existing integrated tagger and show that although the well-integrated tagger remains the fastest, our experimental system can be more accurate. Finally, we hope that the diverse and difficult datasets we used for evaluation will gain more popularity in the field: we show that results can differ depending on the dataset, even if it is an in-domain one. We contribute the complete datasets reformatted for Huggingface token classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.636.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.636.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--636 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.636 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.636/>Improve Dense Passage Retrieval with Entailment Tuning</a></strong><br><a href=/people/l/lu-dai/>Lu Dai</a>
|
<a href=/people/h/hao-liu/>Hao Liu</a>
|
<a href=/people/h/hui-xiong/>Hui Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--636><div class="card-body p-3 small">Retrieval module can be plugged into many downstream NLP tasks to improve their performance, such as open-domain question answering and retrieval-augmented generation. The key to a retrieval system is to calculate relevance scores to query and passage pairs. However, the definition of relevance is often ambiguous. We observed that a major class of relevance aligns with the concept of entailment in NLI tasks. Based on this observation, we designed a method called entailment tuning to improve the embedding of dense retrievers. Specifically, we unify the form of retrieval data and NLI data using existence claim as a bridge. Then, we train retrievers to predict the claims entailed in a passage with a variant task of masked prediction. Our method can be efficiently plugged into current dense retrieval methods, and experiments show the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.637.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.637.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--637 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.637 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.637.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.637/><span class=acl-fixed-case>T</span>ool<span class=acl-fixed-case>B</span>e<span class=acl-fixed-case>H</span>onest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models</a></strong><br><a href=/people/y/yuxiang-zhang/>Yuxiang Zhang</a>
|
<a href=/people/j/jing-chen/>Jing Chen</a>
|
<a href=/people/j/junjie-wang/>Junjie Wang</a>
|
<a href=/people/y/yaxin-liu/>Yaxin Liu</a>
|
<a href=/people/c/cheng-yang/>Cheng Yang</a>
|
<a href=/people/c/chufan-shi/>Chufan Shi</a>
|
<a href=/people/x/xinyu-zhu/>Xinyu Zhu</a>
|
<a href=/people/z/zihao-lin/>Zihao Lin</a>
|
<a href=/people/h/hanwen-wan/>Hanwen Wan</a>
|
<a href=/people/y/yujiu-yang/>Yujiu Yang</a>
|
<a href=/people/t/tetsuya-sakai/>Tetsuya Sakai</a>
|
<a href=/people/t/tian-feng/>Tian Feng</a>
|
<a href=/people/h/hayato-yamana/>Hayato Yamana</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--637><div class="card-body p-3 small">Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community has yet to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLM’s hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play crucial roles in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.638.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.638.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--638 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.638 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.638/><span class=acl-fixed-case>TEMA</span>: Token Embeddings Mapping for Enriching Low-Resource Language Models</a></strong><br><a href=/people/r/rodolfo-zevallos/>Rodolfo Zevallos</a>
|
<a href=/people/n/nuria-bel/>Núria Bel</a>
|
<a href=/people/m/mireia-farrus/>Mireia Farrús</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--638><div class="card-body p-3 small">The objective of the research we present is to remedy the problem of the low quality of language models for low-resource languages. We introduce an algorithm, the Token Embedding Mapping Algorithm (TEMA), that maps the token embeddings of a richly pre-trained model L1 to a poorly trained model L2, thus creating a richer L2’ model. Our experiments show that the L2’ model reduces perplexity with respect to the original monolingual model L2, and that for downstream tasks, including SuperGLUE, the results are state-of-the-art or better for the most semantic tasks. The models obtained with TEMA are also competitive or better than multilingual or extended models proposed as solutions for mitigating the low-resource language problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.639.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.639.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--639 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.639 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.639/><span class=acl-fixed-case>DECOR</span>: Improving Coherence in <span class=acl-fixed-case>L</span>2 <span class=acl-fixed-case>E</span>nglish Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting</a></strong><br><a href=/people/x/xuanming-zhang/>Xuanming Zhang</a>
|
<a href=/people/a/anthony-diaz/>Anthony Diaz</a>
|
<a href=/people/z/zixun-chen/>Zixun Chen</a>
|
<a href=/people/q/qingyang-wu/>Qingyang Wu</a>
|
<a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/e/erik-voss/>Erik Voss</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--639><div class="card-body p-3 small">Coherence in writing, an aspect that L2 English learners often struggle with, is crucial in assessing L2 English writing. Existing automated writing evaluation systems primarily use basic surface linguistic features to detect coherence in writing. However, little effort has been made to correct the detected incoherence, which could significantly benefit L2 language learners seeking to improve their writing. To bridge this gap, we introduce DECOR, a novel benchmark that includes expert annotations for detecting incoherence in L2 English writing, identifying the underlying reasons, and rewriting the incoherent sentences. To our knowledge, DECOR is the first coherence assessment dataset specifically designed for improving L2 English writing, featuring pairs of original incoherent sentences alongside their expert-rewritten counterparts. Additionally, we fine-tuned models to automatically detect and rewrite incoherence in student essays. We find that incorporating specific reasons for incoherence during fine-tuning consistently improves the quality of the rewrites, achieving a level that is favored in both automatic and human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.640.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.640.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--640 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.640 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.640.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.640/><span class=acl-fixed-case>T</span>ext2<span class=acl-fixed-case>C</span>hart31: Instruction Tuning for Chart Generation with Automatic Feedback</a></strong><br><a href=/people/f/fatemeh-pesaran-zadeh/>Fatemeh Pesaran Zadeh</a>
|
<a href=/people/j/juyeon-kim/>Juyeon Kim</a>
|
<a href=/people/j/jin-hwa-kim/>Jin-Hwa Kim</a>
|
<a href=/people/g/gunhee-kim/>Gunhee Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--640><div class="card-body p-3 small">Large language models (LLMs) have demonstrated strong capabilities across various language tasks, notably through instruction-tuning methods. However, LLMs face challenges in visualizing complex, real-world data through charts and plots. Firstly, existing datasets rarely cover a full range of chart types, such as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning methods do not fully leverage the intricate relationships within rich datasets, including text, code, and figures. To address these challenges, we propose a hierarchical pipeline and a new dataset for chart generation. Our dataset, Text2Chart31, includes 31 unique plot types referring to the Matplotlib library, with 11.1K tuples of descriptions, code, data tables, and plots. Moreover, we introduce a reinforcement learning-based instruction tuning technique for chart generation tasks without requiring human feedback. Our experiments show that this approach significantly enhances the model performance, enabling smaller models to outperform larger open-source models and be comparable to state-of-the-art proprietary models in data visualization tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.641.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.641.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--641 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.641 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.641.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.641.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.641/><span class=acl-fixed-case>P</span>r<span class=acl-fixed-case>E</span>x<span class=acl-fixed-case>M</span>e! Large Scale Prompt Exploration of Open Source <span class=acl-fixed-case>LLM</span>s for Machine Translation and Summarization Evaluation</a></strong><br><a href=/people/c/christoph-leiter/>Christoph Leiter</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--641><div class="card-body p-3 small">Large language models (LLMs) have revolutionized NLP research. Notably, in-context learning enables their use as evaluation metrics for natural language generation, making them particularly advantageous in low-resource scenarios and time-restricted applications. In this work, we introduce **PrExMe**, a large-scale **Pr**ompt **Ex**ploration for **Me**trics, where we evaluate more than 720 prompt templates for open-source LLM-based metrics on machine translation (MT) and summarization datasets, totalling over 6.6M evaluations. This extensive comparison (1) benchmarks recent open-source LLMs as metrics and (2) explores the stability and variability of different prompting strategies. We discover that, on the one hand, there are scenarios for which prompts are stable. For instance, some LLMs show idiosyncratic preferences and favor to grade generated texts with textual labels while others prefer to return numeric scores. On the other hand, the stability of prompts and model rankings can be susceptible to seemingly innocuous changes. For example, changing the requested output format from “0 to 100” to "-1 to +1” can strongly affect the rankings in our evaluation. Our study contributes to understanding the impact of different prompting approaches on LLM-based metrics for MT and summarization evaluation, highlighting the most stable prompting patterns and potential limitations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.642.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.642.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--642 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.642 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.642.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.642.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.642/>Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning</a></strong><br><a href=/people/s/shuai-zhao/>Shuai Zhao</a>
|
<a href=/people/m/meihuizi-jia/>Meihuizi Jia</a>
|
<a href=/people/l/luu-anh-tuan/>Anh Tuan Luu</a>
|
<a href=/people/f/fengjun-pan/>Fengjun Pan</a>
|
<a href=/people/j/jinming-wen/>Jinming Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--642><div class="card-body p-3 small">In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we design a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which can make models behave in alignment with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model’s generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 180B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.643.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.643.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--643 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.643 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.643/>Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models</a></strong><br><a href=/people/j/javier-chiyah-garcia/>Javier Chiyah-Garcia</a>
|
<a href=/people/a/alessandro-suglia/>Alessandro Suglia</a>
|
<a href=/people/a/arash-eshghi/>Arash Eshghi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--643><div class="card-body p-3 small">In dialogue, the addressee may initially misunderstand the speaker and respond erroneously, often prompting the speaker to correct the misunderstanding in the next turn with a Third Position Repair (TPR). The ability to process and respond appropriately to such repair sequences is thus crucial in conversational AI systems. In this paper, we first collect, analyse, and publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences in an instruction-following manipulation task that is, by design, rife with referential ambiguity. We employ this dataset to evaluate several state-of-the-art Vision and Language Models (VLM) across multiple settings, focusing on their capability to process and accurately respond to TPRs and thus recover from miscommunication. We find that, compared to humans, all models significantly underperform in this task. We then show that VLMs can benefit from specialised losses targeting relevant tokens during fine-tuning, achieving better performance and generalising better to new scenarios. Our results suggest that these models are not yet ready to be deployed in multi-modal collaborative settings where repairs are common, and highlight the need to design training regimes and objectives that facilitate learning from interaction. Our code and data are available at www.github.com/JChiyah/blockworld-repairs</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.644.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.644.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--644 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.644 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.644.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.644.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.644/>Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models</a></strong><br><a href=/people/x/xinrong-zhang/>Xinrong Zhang</a>
|
<a href=/people/y/yingfa-chen/>Yingfa Chen</a>
|
<a href=/people/s/shengding-hu/>Shengding Hu</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/z/zihang-xu/>Zihang Xu</a>
|
<a href=/people/y/yuanwei-xu/>Yuanwei Xu</a>
|
<a href=/people/w/weilin-zhao/>Weilin Zhao</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--644><div class="card-body p-3 small">As large language models (LLMs) increasingly permeate daily lives, there is a growing demand for real-time interactions that mirror human conversations. Traditional turn-based chat systems driven by LLMs prevent users from verbally interacting with the system while generating responses.To overcome these limitations, we adapt existing LLMs to <i>duplex models</i> so that they can listen to users while generating output and dynamically adjust themselves to provide instant feedback.Specifically, we divide the queries and responses of conversations into several time slices and then adopt a time-division-multiplexing (TDM) encoding-decoding strategy to process these slices pseudo-simultaneously.Furthermore, to make LLMs proficient enough to handle real-time conversations, we build a fine-tuning dataset consisting of alternating time slices of queries and responses and covering typical feedback types in instantaneous interactions.Our experiments show that although the queries and responses of conversations are segmented into incomplete slices for processing, LLMs can preserve their original performance on standard benchmarks with a few fine-tuning steps on our dataset. Automatic and human evaluation indicate that duplex models make user-AI interactions more natural and human-like, and greatly improve user satisfaction compared to vanilla LLMs. Our duplex model and dataset will be released soon.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.645.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.645.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--645 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.645 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.645.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.645/>Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations</a></strong><br><a href=/people/m/matthias-lindemann/>Matthias Lindemann</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--645><div class="card-body p-3 small">Models need appropriate inductive biases to effectively learn from small amounts of data and generalize systematically outside of the training distribution. While Transformers are highly versatile and powerful, they can still benefit from enhanced structural inductive biases for seq2seq tasks, especially those involving syntactic transformations, such as converting active to passive voice or semantic parsing. In this paper, we propose to strengthen the structural inductive bias of a Transformer by intermediate pre-training to perform synthetically generated syntactic transformations of dependency trees given a description of the transformation. Our experiments confirm that this helps with few-shot learning of syntactic tasks such as chunking, and also improves structural generalization for semantic parsing. Our analysis shows that the intermediate pre-training leads to attention heads that keep track of which syntactic transformation needs to be applied to which token, and that the model can leverage these attention heads on downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.646.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.646.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--646 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.646 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.646/>Puzzle Solving using Reasoning of Large Language Models: A Survey</a></strong><br><a href=/people/p/panagiotis-giadikiaroglou/>Panagiotis Giadikiaroglou</a>
|
<a href=/people/m/maria-lymperaiou/>Maria Lymperaiou</a>
|
<a href=/people/g/giorgos-filandrianos/>Giorgos Filandrianos</a>
|
<a href=/people/g/giorgos-stamou/>Giorgos Stamou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--646><div class="card-body p-3 small">Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in AI, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy—dividing puzzles into rule-based and rule-less categories—to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs’ performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs’ puzzle-solving proficiency and contribute to AI’s logical reasoning and creative problem-solving advancements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.647.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.647.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--647 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.647 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.647.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.647/><span class=acl-fixed-case>S</span>ci<span class=acl-fixed-case>E</span>x: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading</a></strong><br><a href=/people/t/tu-anh-dinh/>Tu Anh Dinh</a>
|
<a href=/people/c/carlos-mullov/>Carlos Mullov</a>
|
<a href=/people/l/leonard-barmann/>Leonard Bärmann</a>
|
<a href=/people/z/zhaolin-li/>Zhaolin Li</a>
|
<a href=/people/d/danni-liu/>Danni Liu</a>
|
<a href=/people/s/simon-reiss/>Simon Reiß</a>
|
<a href=/people/j/jueun-lee/>Jueun Lee</a>
|
<a href=/people/n/nathan-lerzer/>Nathan Lerzer</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/f/fabian-peller-konrad/>Fabian Peller-Konrad</a>
|
<a href=/people/t/tobias-roddiger/>Tobias Röddiger</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a>
|
<a href=/people/t/tamim-asfour/>Tamim Asfour</a>
|
<a href=/people/m/michael-beigl/>Michael Beigl</a>
|
<a href=/people/r/rainer-stiefelhagen/>Rainer Stiefelhagen</a>
|
<a href=/people/c/carsten-dachsbacher/>Carsten Dachsbacher</a>
|
<a href=/people/k/klemens-bohm/>Klemens Böhm</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--647><div class="card-body p-3 small">With the rapid development of Large Language Models (LLMs), it is crucial to have benchmarks which can evaluate the ability of LLMs on different domains. One common use of LLMs is performing tasks on scientific topics, such as writing algorithms, querying databases or giving mathematical proofs. Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx - a benchmark consisting of university computer science exam questions, to evaluate LLMs’ ability on solving scientific tasks. SciEx is (1) multilingual, containing both English and German exams, and (2) multi-modal, containing questions that involve images, and (3) contains various types of freeform questions with different difficulty levels, due to the nature of university exams. We evaluate the performance of various state-of-the-art LLMs on our new benchmark. Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance. Therefore, we provide human expert grading of the LLM outputs on SciEx. We show that the free-form exams in SciEx remain challenging for the current LLMs, where the best LLM only achieves 59.4% exam grade on average. We also provide detailed comparisons between LLM performance and student performance on SciEx. To enable future evaluation of new LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx. Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.648.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.648.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--648 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.648 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.648.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.648/>Red Teaming Language Models for Processing Contradictory Dialogues</a></strong><br><a href=/people/x/xiaofei-wen/>Xiaofei Wen</a>
|
<a href=/people/b/bangzheng-li/>Bangzheng Li</a>
|
<a href=/people/t/tenghao-huang/>Tenghao Huang</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--648><div class="card-body p-3 small">Most language models currently available are prone to self-contradiction during dialogues. To mitigate this issue, this study explores a novel contradictory dialogue processing task that aims to detect and modify contradictory statements in a conversation. This task is inspired by research on context faithfulness and dialogue comprehension, which have demonstrated that the detection and understanding of contradictions often necessitate detailed explanations. We develop a dataset comprising contradictory dialogues, in which one side of the conversation contradicts itself. Each dialogue is accompanied by an explanatory label that highlights the location and details of the contradiction. With this dataset, we present a Red Teaming framework for contradictory dialogue processing. The framework detects and attempts to explain the dialogue, then modifies the existing contradictory content using the explanation. Our experiments demonstrate that the framework improves the ability to detect contradictory dialogues and provides valid explanations. Additionally, it showcases distinct capabilities for modifying such dialogues. Our study highlights the importance of the logical inconsistency problem in conversational AI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.649.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.649.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--649 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.649 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.649/>Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models</a></strong><br><a href=/people/s/sander-land/>Sander Land</a>
|
<a href=/people/m/max-bartolo/>Max Bartolo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--649><div class="card-body p-3 small">The disconnect between tokenizer creation and model training in language models allows for specific inputs, such as the infamous SolidGoldMagikarp token, to induce unwanted model behaviour. Although such ‘glitch tokens’, tokens present in the tokenizer vocabulary but that are nearly or entirely absent during model training, have been observed across various models, a reliable method to identify and address them has been missing. We present a comprehensive analysis of Large Language Model tokenizers, specifically targeting this issue of detecting under-trained tokens. Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, we develop novel and effective methods for automatically detecting these problematic tokens. Our findings demonstrate the prevalence of such tokens across a diverse set of models and provide insights into improving the efficiency and safety of language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.650.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.650.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--650 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.650 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.650.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.650/>Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/h/houman-mehrafarin/>Houman Mehrafarin</a>
|
<a href=/people/a/arash-eshghi/>Arash Eshghi</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--650><div class="card-body p-3 small">Evaluating Large Language Models (LLMs) on reasoning benchmarks demonstrates their ability to solve compositional questions. However, little is known of whether these models engage in genuine logical reasoning or simply rely on implicit cues to generate answers. In this paper, we investigate the transitive reasoning capabilities of two distinct LLM architectures, LLaMA 2 and Flan-T5, by manipulating facts within two compositional datasets: QASC and Bamboogle. We controlled for potential cues that might influence the models’ performance, including (a) word/phrase overlaps across sections of test input; (b) models’ inherent knowledge during pre-training or fine-tuning; and (c) Named Entities. Our findings reveal that while both models leverage (a), Flan-T5 shows more resilience to experiments (b and c), having less variance than LLaMA 2. This suggests that models may develop an understanding of transitivity through fine-tuning on knowingly relevant datasets, a hypothesis we leave to future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.651.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.651.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--651 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.651 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.651/>Pragmatic Norms Are All You Need – Why The Symbol Grounding Problem Does Not Apply to <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/r/reto-gubelmann/>Reto Gubelmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--651><div class="card-body p-3 small">Do LLMs fall prey to Harnad’s symbol grounding problem (SGP), as it has recently been claimed? We argue that this is not the case. Starting out with countering the arguments of Bender and Koller (2020), we trace the origins of the SGP to the computational theory of mind (CTM), and we show that it only arises with natural language when questionable theories of meaning are presupposed. We conclude by showing that it would apply to LLMs only if they were interpreted in the manner of how the CTM conceives the mind, i.e., by postulating that LLMs rely on a version of a language of thought, or by adopting said questionable theories of meaning; since neither option is rational, we conclude that the SGP does not apply to LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.652.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.652.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--652 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.652 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.652/>Major Entity Identification: A Generalizable Alternative to Coreference Resolution</a></strong><br><a href=/people/k/kawshik-manikantan-sundar/>Kawshik Manikantan Sundar</a>
|
<a href=/people/s/shubham-toshniwal/>Shubham Toshniwal</a>
|
<a href=/people/m/makarand-tapaswi/>Makarand Tapaswi</a>
|
<a href=/people/v/vineet-gandhi/>Vineet Gandhi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--652><div class="card-body p-3 small">The limited generalization of coreference resolution (CR) models has been a major bottleneck in the task’s broad application. Prior work has identified annotation differences, especially for mention detection, as one of the main reasons for the generalization gap and proposed using additional annotated target domain data. Rather than relying on this additional annotation, we propose an alternative referential task, Major Entity Identification (MEI), where we: (a) assume the target entities to be specified in the input, and (b) limit the task to only the frequent entities. Through extensive experiments, we demonstrate that MEI models generalize well across domains on multiple datasets with supervised models and LLM-based few-shot prompting. Additionally, MEI fits the classification framework, which enables the use of robust and intuitive classification-based metrics. Finally, MEI is also of practical use as it allows a user to search for all mentions of a particular entity or a group of entities of interest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.653.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.653.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--653 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.653 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.653.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.653/>Enhancing High-order Interaction Awareness in <span class=acl-fixed-case>LLM</span>-based Recommender Model</a></strong><br><a href=/people/x/xinfeng-wang/>Xinfeng Wang</a>
|
<a href=/people/j/jin-cui/>Jin Cui</a>
|
<a href=/people/f/fumiyo-fukumoto/>Fumiyo Fukumoto</a>
|
<a href=/people/y/yoshimi-suzuki/>Yoshimi Suzuki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--653><div class="card-body p-3 small">Large language models (LLMs) have demonstrated prominent reasoning capabilities in recommendation tasks by transforming them into text-generation tasks. However, existing approaches either disregard or ineffectively model the user-item high-order interactions. To this end, this paper presents an enhanced LLM-based recommender (ELMRec). We enhance whole-word embeddings to substantially enhance LLMs’ interpretation of graph-constructed interactions for recommendations, without requiring graph pre-training. This finding may inspire endeavors to incorporate rich knowledge graphs into LLM-based recommenders via whole-word embedding. We also found that LLMs often recommend items based on users’ earlier interactions rather than recent ones, and present a reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods, especially achieving a 124.3% to 293.7% improvement over SOTA LLM-based methods in direct recommendations. Our code is available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.654.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.654.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--654 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.654 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.654.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.654.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.654/>What Are the Odds? Language Models Are Capable of Probabilistic Reasoning</a></strong><br><a href=/people/a/akshay-paruchuri/>Akshay Paruchuri</a>
|
<a href=/people/j/jake-garrison/>Jake Garrison</a>
|
<a href=/people/s/shun-liao/>Shun Liao</a>
|
<a href=/people/j/john-b-hernandez/>John B Hernandez</a>
|
<a href=/people/j/jacob-sunshine/>Jacob Sunshine</a>
|
<a href=/people/t/tim-althoff/>Tim Althoff</a>
|
<a href=/people/x/xin-liu/>Xin Liu</a>
|
<a href=/people/d/daniel-mcduff/>Daniel McDuff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--654><div class="card-body p-3 small">Language models (LM) are capable of remarkably complex linguistic tasks; however, numerical reasoning is an area in which they frequently struggle. An important but rarely evaluated form of reasoning is understanding probability distributions. In this paper, we focus on evaluating the probabilistic reasoning capabilities of LMs using idealized and real-world statistical distributions. We perform a systematic evaluation of state-of-the-art LMs on three tasks: estimating percentiles, drawing samples, and calculating probabilities. We evaluate three ways to provide context to LMs 1) anchoring examples from within a distribution or family of distributions, 2) real-world context, 3) summary statistics on which to base a Normal approximation. Models can make inferences about distributions, and can be further aided by the incorporation of real-world context, example shots and simplified assumptions, even if these assumptions are incorrect or misspecified. To conduct this work, we developed a comprehensive benchmark distribution dataset with associated question-answer pairs that we have released publicly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.655.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.655.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--655 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.655 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.655/><span class=acl-fixed-case>MARE</span>: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction</a></strong><br><a href=/people/h/han-jiang/>Han Jiang</a>
|
<a href=/people/j/junwen-duan/>Junwen Duan</a>
|
<a href=/people/z/zhe-qu/>Zhe Qu</a>
|
<a href=/people/j/jianxin-wang/>Jianxin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--655><div class="card-body p-3 small">Unsupervised rationale extraction aims to extract text snippets to support model predictions without explicit rationale annotation.Researchers have made many efforts to solve this task. Previous works often encode each aspect independently, which may limit their ability to capture meaningful internal correlations between aspects. While there has been significant work on mitigating spurious correlations, our approach focuses on leveraging the beneficial internal correlations to improve multi-aspect rationale extraction. In this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain and predict multiple aspects simultaneously. Concretely, we propose a Multi-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to encode multiple text chunks simultaneously. Furthermore, multiple special tokens are prepended in front of the text with each corresponding to one certain aspect. Finally, multi-task training is deployed to reduce the training overhead. Experimental results on two unsupervised rationale extraction benchmarks show that MARE achieves state-of-the-art performance. Ablation studies further demonstrate the effectiveness of our method. Our codes have been available at https://github.com/CSU-NLP-Group/MARE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.656.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.656.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--656 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.656 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.656/><span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>RA</span>-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models</a></strong><br><a href=/people/h/hayder-elesedy/>Hayder Elesedy</a>
|
<a href=/people/p/pedro-m-esperanca/>Pedro M Esperanca</a>
|
<a href=/people/s/silviu-vlad-oprea/>Silviu Vlad Oprea</a>
|
<a href=/people/m/mete-ozay/>Mete Ozay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--656><div class="card-body p-3 small">Guardrails have emerged as an alternative to safety alignment for content moderation of large language models (LLMs). Existing model-based guardrails have not been designed for resource-constrained computational portable devices, such as mobile phones, more and more of which are running LLM-based applications locally. We introduce LoRA-Guard, a parameter-efficient guardrail adaptation method that relies on knowledge sharing between LLMs and guardrail models. LoRA-Guard extracts language features from the LLMs and adapts them for the content moderation task using low-rank adapters, while a dual-path design prevents any performance degradation on the generative task. We show that LoRA-Guard outperforms existing approaches with 100-1000x lower parameter overhead while maintaining accuracy, enabling on-device content moderation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.657.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.657.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--657 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.657 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.657/>“A good pun is its own reword”: Can Large Language Models Understand Puns?</a></strong><br><a href=/people/z/zhijun-xu/>Zhijun Xu</a>
|
<a href=/people/s/siyu-yuan/>Siyu Yuan</a>
|
<a href=/people/l/lingjie-chen/>Lingjie Chen</a>
|
<a href=/people/d/deqing-yang/>Deqing Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--657><div class="card-body p-3 small">Puns play a vital role in academic research due to their distinct structure and clear definition, which aid in the comprehensive analysis of linguistic humor. However, the understanding of puns in large language models (LLMs) has not been thoroughly examined, limiting their use in creative writing and humor creation. In this paper, we leverage three popular tasks, i.e., pun recognition, explanation and generation to systematically evaluate the capabilities of LLMs in pun understanding. In addition to adopting the automated evaluation metrics from prior research, we introduce new evaluation methods and metrics that are better suited to the in-context learning paradigm of LLMs. These new metrics offer a more rigorous assessment of an LLM’s ability to understand puns and align more closely with human cognition than previous metrics. Our findings reveal the “lazy pun generation” pattern and identify the primary challenges LLMs encounter in understanding puns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.658.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.658.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--658 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.658 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.658.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.658.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.658/><span class=acl-fixed-case>QGE</span>val: Benchmarking Multi-dimensional Evaluation for Question Generation</a></strong><br><a href=/people/w/weiping-fu/>Weiping Fu</a>
|
<a href=/people/b/bifan-wei/>Bifan Wei</a>
|
<a href=/people/j/jianxiang-hu/>Jianxiang Hu</a>
|
<a href=/people/z/zhongmin-cai/>Zhongmin Cai</a>
|
<a href=/people/j/jun-liu/>Jun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--658><div class="card-body p-3 small">Automatically generated questions often suffer from problems such as unclear expression or factual inaccuracies, requiring a reliable and comprehensive evaluation of their quality. Human evaluation is widely used in the field of question generation (QG) and serves as the gold standard for automatic metrics. However, there is a lack of unified human evaluation criteria, which hampers consistent and reliable evaluations of both QG models and automatic metrics. To address this, we propose **QGEval**, a multi-dimensional **Eval**uation benchmark for **Q**uestion **G**eneration, which evaluates both generated questions and existing automatic metrics across 7 dimensions: fluency, clarity, conciseness, relevance, consistency, answerability, and answer consistency. We demonstrate the appropriateness of these dimensions by examining their correlations and distinctions. Through consistent evaluations of QG models and automatic metrics with QGEval, we find that 1) most QG models perform unsatisfactorily in terms of answerability and answer consistency, and 2) existing metrics fail to align well with human judgments when evaluating generated questions across the 7 dimensions. We expect this work to foster the development of both QG technologies and their evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.659.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.659.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--659 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.659 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.659/>Dependency Graph Parsing as Sequence Labeling</a></strong><br><a href=/people/a/ana-ezquerro/>Ana Ezquerro</a>
|
<a href=/people/d/david-vilares/>David Vilares</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--659><div class="card-body p-3 small">Various linearizations have been proposed to cast syntactic dependency parsing as sequence labeling. However, these approaches do not support more complex graph-based representations, such as semantic dependencies or enhanced universal dependencies, as they cannot handle reentrancy or cycles. By extending them, we define a range of unbounded and bounded linearizations that can be used to cast graph parsing as a tagging task, enlarging the toolbox of problems that can be solved under this paradigm. Experimental results on semantic dependency and enhanced UD parsing show that with a good choice of encoding, sequence-labeling semantic dependency parsers combine high efficiency with accuracies close to the state of the art, in spite of their simplicity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.660.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.660.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--660 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.660 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.660/><span class=acl-fixed-case>N</span>u<span class=acl-fixed-case>NER</span>: Entity Recognition Encoder Pre-training via <span class=acl-fixed-case>LLM</span>-Annotated Data</a></strong><br><a href=/people/s/sergei-bogdanov/>Sergei Bogdanov</a>
|
<a href=/people/a/alexandre-constantin/>Alexandre Constantin</a>
|
<a href=/people/t/timothee-bernard/>Timothée Bernard</a>
|
<a href=/people/b/benoit-crabbe/>Benoit Crabbé</a>
|
<a href=/people/e/etienne-p-bernard/>Etienne P Bernard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--660><div class="card-body p-3 small">Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs. NuNER and NuNER’s dataset are open-sourced with MIT License.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.661.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.661.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--661 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.661 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.661/>Towards a <span class=acl-fixed-case>G</span>reek Proverb Atlas: Computational Spatial Exploration and Attribution of <span class=acl-fixed-case>G</span>reek Proverbs</a></strong><br><a href=/people/j/john-pavlopoulos/>John Pavlopoulos</a>
|
<a href=/people/p/panos-louridas/>Panos Louridas</a>
|
<a href=/people/p/panagiotis-filos/>Panagiotis Filos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--661><div class="card-body p-3 small">Proverbs carry wisdom transferred orally from generation to generation. Based on the place they were recorded, this study introduces a publicly-available and machine-actionable dataset of more than one hundred thousand Greek proverb variants. By quantifying the spatial distribution of proverbs, we show that the most widespread proverbs come from the mainland while the least widespread proverbs come primarily from the islands. By focusing on the least dispersed proverbs, we present the most frequent tokens per location and undertake a benchmark in geographical attribution, using text classification and regression (text geocoding). Our results show that this is a challenging task for which specific locations can be attributed more successfully compared to others. The potential of our resource and benchmark is showcased by two novel applications. First, we extracted terms moving the regression prediction toward the four cardinal directions. Second, we leveraged conformal prediction to attribute 3,676 unregistered proverbs with statistically rigorous predictions of locations each of these proverbs was possibly registered in.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.662.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.662.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--662 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.662 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.662/>Unraveling Babel: Exploring Multilingual Activation Patterns of <span class=acl-fixed-case>LLM</span>s and Their Applications</a></strong><br><a href=/people/w/weize-liu/>Weize Liu</a>
|
<a href=/people/y/yinlong-xu/>Yinlong Xu</a>
|
<a href=/people/h/hongxia-xu/>Hongxia Xu</a>
|
<a href=/people/j/jintai-chen/>Jintai Chen</a>
|
<a href=/people/x/xuming-hu/>Xuming Hu</a>
|
<a href=/people/j/jian-wu/>Jian Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--662><div class="card-body p-3 small">Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of NLP, but still lack understanding of their internal neuron activities when processing different languages. We designed a method to convert dense LLMs into fine-grained MoE architectures, and then visually studied the multilingual activation patterns of LLMs through expert activation frequency heatmaps. Through comprehensive experiments on different model families, different model sizes, and different variants, we analyzed the similarities and differences in the internal neuron activation patterns of LLMs when processing different languages. Specifically, we investigated the distribution of high-frequency activated experts, multilingual shared experts, whether multilingual activation patterns are related to language families, and the impact of instruction tuning on activation patterns. We further explored leveraging the discovered differences in expert activation frequencies to guide sparse activation and pruning. Experimental results demonstrated that our method significantly outperformed random expert pruning and even exceeded the performance of unpruned models in some languages. Additionally, we found that configuring different pruning rates for different layers based on activation level differences could achieve better results. Our findings reveal the multilingual processing mechanisms within LLMs and utilize these insights to offer new perspectives for applications such as sparse activation and model pruning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.663.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.663.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--663 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.663 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.663.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.663.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.663/>Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated <span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>LU</span> and Smooth K2 Loss</a></strong><br><a href=/people/b/bowen-zhang/>Bowen Zhang</a>
|
<a href=/people/c/chunping-li/>Chunping Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--663><div class="card-body p-3 small">Since the introduction of BERT and RoBERTa, research on Semantic Textual Similarity (STS) has made groundbreaking progress. Particularly, the adoption of contrastive learning has substantially elevated state-of-the-art performance across various STS benchmarks. However, contrastive learning categorizes text pairs as either semantically similar or dissimilar, failing to leverage fine-grained annotated information and necessitating large batch sizes to prevent model collapse. These constraints pose challenges for researchers engaged in STS tasks that involve nuanced similarity levels or those with limited computational resources, compelling them to explore alternatives like Sentence-BERT. Despite its efficiency, Sentence-BERT tackles STS tasks from a classification perspective, overlooking the progressive nature of semantic relationships, which results in suboptimal performance. To bridge this gap, this paper presents an innovative regression framework and proposes two simple yet effective loss functions: Translated ReLU and Smooth K2 Loss. Experimental results demonstrate that our method achieves convincing performance across seven established STS benchmarks and offers the potential for further optimization of contrastive learning pre-trained models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.664.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.664.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--664 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.664 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.664/>Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training</a></strong><br><a href=/people/m/marc-felix-brinner/>Marc Felix Brinner</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--664><div class="card-body p-3 small">We propose an end-to-end differentiable training paradigm for stable training of a rationalized transformer classifier. Our approach results in a single model that simultaneously classifies a sample and scores input tokens based on their relevance to the classification. To this end, we build on the widely-used three-player-game for training rationalized models, which typically relies on training a rationale selector, a classifier and a complement classifier. We simplify this approach by making a single model fulfill all three roles, leading to a more efficient training paradigm that is not susceptible to the common training instabilities that plague existing approaches. Further, we extend this paradigm to produce class-wise rationales while incorporating recent advances in parameterizing and regularizing the resulting rationales, thus leading to substantially improved and state-of-the-art alignment with human annotations without any explicit supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.665.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.665.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--665 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.665 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.665/>Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation</a></strong><br><a href=/people/m/markus-frohmann/>Markus Frohmann</a>
|
<a href=/people/i/igor-sterner/>Igor Sterner</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/b/benjamin-minixhofer/>Benjamin Minixhofer</a>
|
<a href=/people/m/markus-schedl/>Markus Schedl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--665><div class="card-body p-3 small">Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, we find that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. We introduce a new model — Segment any Text (SaT) — to solve this problem. To enhance robustness, we propose a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, we introduce an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, we introduce architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, we introduce a variant of our model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, our contributions provide a universal approach for segmenting any text. Our method outperforms all baselines — including strong LLMs — across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. Our models and code, including documentation, are readily available at https://github.com/segment-any-text/wtpsplit under the MIT license.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.666.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.666.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--666 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.666 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.666/>Applying Contrastive Learning to Code Vulnerability Type Classification</a></strong><br><a href=/people/c/chen-ji/>Chen Ji</a>
|
<a href=/people/s/su-yang/>Su Yang</a>
|
<a href=/people/h/hongyu-sun/>Hongyu Sun</a>
|
<a href=/people/y/yuqing-zhang/>Yuqing Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--666><div class="card-body p-3 small">Vulnerability classification is a crucial task in software security analysis, essential for identifying and mitigating potential security risks. Learning-based methods often perform poorly due to the long-tail distribution of vulnerability classification datasets. Recent approaches try to address the problem but treat each CWE class in isolation, ignoring their relationships. This results in non-scalable code vector representations, causing significant performance drops when handling complex real-world vulnerabilities. We propose a hierarchical contrastive learning framework for code vulnerability type classification to bring vector representations of related CWEs closer together. To address the issue of class collapse and enhance model robustness, we mix self-supervised contrastive learning loss into our loss function. Additionally, we employ max-pooling to enable the model to handle longer vulnerability code inputs. Extensive experiments demonstrate that our proposed framework outperforms state-of-the-art methods by 2.97%<span class=tex-math>-</span>17.90% on accuracy and 0.98%<span class=tex-math>-</span>22.27% on weighted-F1, with even better performance on higher-quality datasets. We also utilize an ablation study to prove each component’s contribution. These findings underscore the potential and advantages of our approach in the multi-class vulnerability classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.667.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.667.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--667 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.667 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.667/><span class=acl-fixed-case>T</span>heorem<span class=acl-fixed-case>L</span>lama: Transforming General-Purpose <span class=acl-fixed-case>LLM</span>s into Lean4 Experts</a></strong><br><a href=/people/r/ruida-wang/>Ruida Wang</a>
|
<a href=/people/j/jipeng-zhang/>Jipeng Zhang</a>
|
<a href=/people/y/yizhen-jia/>Yizhen Jia</a>
|
<a href=/people/r/rui-pan/>Rui Pan</a>
|
<a href=/people/s/shizhe-diao/>Shizhe Diao</a>
|
<a href=/people/r/renjie-pi/>Renjie Pi</a>
|
<a href=/people/t/tong-zhang/>Tong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--667><div class="card-body p-3 small">Proving mathematical theorems using computer-verifiable formal languages like Lean significantly impacts mathematical reasoning. One approach to formal theorem proving involves generating complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs. However, due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data most modern LLMs exhibit suboptimal performance.This scarcity results in a paucity of methodologies for training LLMs and techniques to fully utilize their capabilities in composing formal proofs. To address these challenges, this paper proposes **TheoremLlama**, an end-to-end framework that trains a general-purpose LLM to be a Lean4 expert. **TheoremLlama** includes NL-FL dataset generation and bootstrapping method to obtain aligned dataset, curriculum learning and block training techniques to train the model, and iterative proof writing method to write Lean4 proofs that work together synergistically.Using the dataset generation method in **TheoremLlama**, we provide *Open Bootstrapped Theorems* (OBT), an NL-FL aligned and bootstrapped dataset. Our novel NL-FL bootstrapping method, where NL proofs are integrated into Lean4 code for training datasets, leverages the NL reasoning ability of LLMs for formal reasoning. The **TheoremLlama** framework achieves cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%. Our code, model checkpoints, and the generated dataset is published in GitHub</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.668.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.668.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--668 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.668 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.668.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.668/>Multi-Level Cross-Modal Alignment for Speech Relation Extraction</a></strong><br><a href=/people/l/liang-zhang/>Liang Zhang</a>
|
<a href=/people/z/zhen-yang/>Zhen Yang</a>
|
<a href=/people/b/biao-fu/>Biao Fu</a>
|
<a href=/people/z/ziyao-lu/>Ziyao Lu</a>
|
<a href=/people/l/liangying-shao/>Liangying Shao</a>
|
<a href=/people/s/shiyu-liu/>Shiyu Liu</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/x/xiaoli-wang/>Xiaoli Wang</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--668><div class="card-body p-3 small">Speech Relation Extraction (SpeechRE) aims to extract relation triplets from speech data. However, existing studies usually use synthetic speech to train and evaluate SpeechRE models, hindering the further development of SpeechRE due to the disparity between synthetic and real speech. Meanwhile, the modality gap issue, unexplored in SpeechRE, limits the performance of existing models. In this paper, we construct two real SpeechRE datasets to facilitate subsequent researches and propose a Multi-level Cross-modal Alignment Model (MCAM) for SpeechRE. Our model consists of three components: 1) a speech encoder, extracting speech features from the input speech; 2) an alignment adapter, mapping these speech features into a suitable semantic space for the text decoder; and 3) a text decoder, autoregressively generating relation triplets based on the speech features. During training, we first additionally introduce a text encoder to serve as a semantic bridge between the speech encoder and the text decoder, and then train the alignment adapter to align the output features of speech and text encoders at multiple levels. In this way, we can effectively train the alignment adapter to bridge the modality gap between the speech encoder and the text decoder. Experimental results and in-depth analysis on our datasets strongly demonstrate the efficacy of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.669.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.669.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--669 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.669 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.669/>Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models</a></strong><br><a href=/people/c/christopher-schroder/>Christopher Schröder</a>
|
<a href=/people/g/gerhard-heyer/>Gerhard Heyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--669><div class="card-body p-3 small">Active learning is an iterative labeling process that is used to obtain a small labeled subset, despite the absence of labeled data, thereby enabling to train a model for supervised tasks such as text classification.While active learning has made considerable progress in recent years due to improvements provided by pre-trained language models, there is untapped potential in the often neglected unlabeled portion of the data, although it is available in considerably larger quantities than the usually small set of labeled data. In this work, we investigate how self-training, a semi-supervised approach that uses a model to obtain pseudo-labels for unlabeled data, can be used to improve the efficiency of active learning for text classification. Building on a comprehensive reproduction of four previous self-training approaches, some of which are evaluated for the first time in the context of active learning or natural language processing, we introduce HAST, a new and effective self-training strategy, which is evaluated on four text classification benchmarks. Our results show that it outperforms the reproduced self-training approaches and reaches classification results comparable to previous experiments for three out of four datasets, using as little as 25% of the data. The code is publicly available at https://github.com/chschroeder/self-training-for-sample-efficient-active-learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.670.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.670.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--670 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.670 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.670/><span class=acl-fixed-case>PANDA</span>: Persona Attributes Navigation for Detecting and Alleviating Overuse Problem in Large Language Models</a></strong><br><a href=/people/j/jinsung-kim/>Jinsung Kim</a>
|
<a href=/people/s/seonmin-koo/>Seonmin Koo</a>
|
<a href=/people/h/heui-seok-lim/>Heuiseok Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--670><div class="card-body p-3 small">In the persona-grounded dialogue (PGD) task, it is required not only to respond fluently, but also to ground the attributes according to the current conversation topic properly. However, due to their tendency to overly ground given attributes, LLMs often generate unnatural responses provoked by using attributes that deviate from the flow of the conversation or by exploiting too many attributes at once. We term this phenomenon the *overuse* problem of LLMs. Unfortunately, research devising precise criteria and frameworks to quantitatively verify LLMs’ *overuse* problem is obviously insufficient. To address this issue, we propose **P**ersona **A**ttributes **N**avigation for **D**etecting and **A**lleviating the *overuse* problem (**PANDA**) framework. **PANDA** is the first study to quantify the persona *overuse* problem of LLMs by establishing clear standards of the problem and verifying various LLMs based on them. Moreover, this framework navigates us into understanding persona attributes by introducing diverse and detailed dialogue topics that consider practical conversation situations. We provide insights related to LLMs’ persona attribute *overuse* problem through comprehensive verification and analysis with **PANDA** in the PGD task. Our code and resources can be found at http://github.com/jin62304/PANDA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.671.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.671.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--671 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.671 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.671/>The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm</a></strong><br><a href=/people/a/aakanksha/>Aakanksha</a>
|
<a href=/people/a/arash-ahmadian/>Arash Ahmadian</a>
|
<a href=/people/b/beyza-ermis/>Beyza Ermis</a>
|
<a href=/people/s/seraphina-goldfarb-tarrant/>Seraphina Goldfarb-Tarrant</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/m/marzieh-fadaee/>Marzieh Fadaee</a>
|
<a href=/people/s/sara-hooker/>Sara Hooker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--671><div class="card-body p-3 small">A key concern with the concept of *“alignment”* is the implicit question of *“alignment to what?”*. AI systems are increasingly used across the world, yet safety alignment is often focused on homogeneous monolingual settings. Additionally, preference training and safety measures often overfit to harms common in Western-centric datasets. Here, we explore the viability of different alignment approaches when balancing dual objectives: addressing and optimizing for a non-homogeneous set of languages and cultural preferences while minimizing both global and local harms. We collect the first human annotated red teaming prompts in different languages, distinguishing between global and local harm, which serve as a laboratory to understand the reliability of alignment techniques when faced with preference distributions that are non-stationary across geographies and languages. While this setting is seldom covered by the literature to date, which primarily centers on English harm mitigation, it captures real-world interactions with AI systems around the world. We establish a new precedent for state-of-the-art alignment techniques across 6 languages with minimal degradation in general performance. Our work provides important insights into cross-lingual transfer and novel optimization approaches to safeguard AI systems designed to serve global populations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.672.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.672.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--672 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.672 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.672/>Subword Segmentation in <span class=acl-fixed-case>LLM</span>s: Looking at Inflection and Consistency</a></strong><br><a href=/people/m/marion-di-marco/>Marion Di Marco</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--672><div class="card-body p-3 small">The role of subword segmentation in relation to capturing morphological patterns in LLMs is currently not well explored. Ideally, one would train models like GPT using various segmentations and evaluate how well word meanings are captured. Since this is not computationally feasible, we group words according to their segmentation properties and compare how well a model can solve a linguistic task for these groups. We study two criteria: (i) adherence to morpheme boundaries and (ii) the segmentation consistency of the different inflected forms of a lemma. We select word forms with high and low values for these criteria and carry out experiments on GPT-4o’s ability to capture verbal inflection for 10 languages. Our results indicate that in particular the criterion of segmentation consistency can help to predict the model’s ability to recognize and generate the lemma from an inflected form, providing evidence that subword segmentation is relevant.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.673.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.673.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--673 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.673 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.673/>Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments</a></strong><br><a href=/people/o/omar-sharif/>Omar Sharif</a>
|
<a href=/people/j/joseph-gatto/>Joseph Gatto</a>
|
<a href=/people/m/madhusudan-basak/>Madhusudan Basak</a>
|
<a href=/people/s/sarah-masud-preum/>Sarah Masud Preum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--673><div class="card-body p-3 small">Prior works formulate the extraction of event-specific arguments as a span extraction problem, where event arguments are explicit — i.e. assumed to be contiguous spans of text in a document. In this study, we revisit this definition of Event Extraction (EE) by introducing two key argument types that cannot be modeled by existing EE frameworks. First, implicit arguments are event arguments which are not explicitly mentioned in the text, but can be inferred through context. Second, scattered arguments are event arguments that are composed of information scattered throughout the text. These two argument types are crucial to elicit the full breadth of information required for proper event modeling.To support the extraction of explicit, implicit, and scattered arguments, we develop a novel dataset, DiscourseEE, which includes 7,464 argument annotations from online health discourse. Notably, 51.2% of the arguments are implicit, and 17.4% are scattered, making DiscourseEE a unique corpus for complex event extraction. Additionally, we formulate argument extraction as a text generation problem to facilitate the extraction of complex argument types. We provide a comprehensive evaluation of state-of-the-art models and highlight critical open challenges in generative event extraction. Our data and codebase are available at https://omar-sharif03.github.io/DiscourseEE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.674.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.674.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--674 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.674 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.674/>Let Me Teach You: Pedagogical Foundations of Feedback for Language Models</a></strong><br><a href=/people/b/beatriz-borges/>Beatriz Borges</a>
|
<a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/t/tanja-kaser/>Tanja Käser</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--674><div class="card-body p-3 small">Natural Language Feedback (NLF) is an increasingly popular mechanism for aligning Large Language Models (LLMs) to human preferences. Despite the diversity of the information it can convey, NLF methods are often hand-designed and arbitrary, with little systematic grounding. At the same time, research in learning sciences has long established several effective feedback models. In this opinion piece, we compile ideas from pedagogy to introduce FELT, a feedback framework for LLMs that outlines various characteristics of the feedback space, and a feedback content taxonomy based on these variables, providing a general mapping of the feedback space. In addition to streamlining NLF designs, FELT also brings out new, unexplored directions for research in NLF. We make our taxonomy available to the community, providing guides and examples for mapping our categorizations to future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.675.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.675.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--675 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.675 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.675/>Unknown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data</a></strong><br><a href=/people/j/jean-flavien-bussotti/>Jean-Flavien Bussotti</a>
|
<a href=/people/l/luca-ragazzi/>Luca Ragazzi</a>
|
<a href=/people/g/giacomo-frisoni/>Giacomo Frisoni</a>
|
<a href=/people/g/gianluca-moro/>Gianluca Moro</a>
|
<a href=/people/p/paolo-papotti/>Paolo Papotti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--675><div class="card-body p-3 small">Computational fact-checking (FC) relies on supervised models to verify claims based on given evidence, requiring a resource-intensive process to annotate large volumes of training data. We introduce Unown, a novel framework that generates training instances for FC systems automatically using both textual and tabular content. Unown selects relevant evidence and generates supporting and refuting claims with advanced negation artifacts. Designed to be flexible, Unown accommodates various strategies for evidence selection and claim generation, offering unparalleled adaptability. We comprehensively evaluate Unown on both text-only and table+text benchmarks, including Feverous, SciFact, and MMFC, a new multi-modal FC dataset. Our results prove that Unown examples are of comparable quality to expert-labeled data, even enabling models to achieve up to 5% higher accuracy. The code, data, and models are available at https://github.com/disi-unibo-nlp/unown</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.676.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.676.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--676 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.676 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.676.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.676.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.676/><span class=acl-fixed-case>TL</span>-<span class=acl-fixed-case>CL</span>: Task And Language Incremental Continual Learning</a></strong><br><a href=/people/s/shrey-satapara/>Shrey Satapara</a>
|
<a href=/people/p/p-k-srijith/>P. K. Srijith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--676><div class="card-body p-3 small">This paper introduces and investigates the problem of Task and Language Incremental Continual Learning (TLCL), wherein a multilingual model is systematically updated to accommodate new tasks in previously learned languages or new languages for established tasks. This significant yet previously unexplored area holds substantial practical relevance as it mirrors the dynamic requirements of real-world applications. We benchmark a representative set of continual learning (CL) algorithms for TLCL. Furthermore, we propose Task and Language-Specific Adapters (TLSA), an adapter-based parameter-efficient fine-tuning strategy. TLSA facilitates cross-lingual and cross-task transfer and outperforms other parameter-efficient fine-tuning techniques. Crucially, TLSA reduces parameter growth stemming from saving adapters to linear complexity from polynomial complexity as it was with parameter isolation-based adapter tuning. We conducted experiments on several NLP tasks arising across several languages. We observed that TLSA outperforms all other parameter-efficient approaches without requiring access to historical data for replay.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.677.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.677.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--677 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.677 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.677/>Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?</a></strong><br><a href=/people/d/daniel-p-jeong/>Daniel P Jeong</a>
|
<a href=/people/s/saurabh-garg/>Saurabh Garg</a>
|
<a href=/people/z/zachary-chase-lipton/>Zachary Chase Lipton</a>
|
<a href=/people/m/michael-oberst/>Michael Oberst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--677><div class="card-body p-3 small">Several recent works seek to develop foundation models specifically for medical applications, adapting general-purpose large language models (LLMs) and vision-language models (VLMs) via continued pretraining on publicly available biomedical corpora. These works typically claim that such domain-adaptive pretraining (DAPT) improves performance on downstream medical tasks, such as answering medical licensing exam questions. In this paper, we compare seven public “medical” LLMs and two VLMs against their corresponding base models, arriving at a different conclusion: all medical VLMs and nearly all medical LLMs fail to consistently improve over their base models in the zero-/few-shot prompting regime for medical question-answering (QA) tasks. For instance, across the tasks and model pairs we consider in the 3-shot setting, medical LLMs only outperform their base models in 12.1% of cases, reach a (statistical) tie in 49.8% of cases, and are significantly worse than their base models in the remaining 38.2% of cases. Our conclusions are based on (i) comparing each medical model head-to-head, directly against the corresponding base model; (ii) optimizing the prompts for each model separately; and (iii) accounting for statistical uncertainty in comparisons. While these basic practices are not consistently adopted in the literature, our ablations show that they substantially impact conclusions. Our findings suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities, and offer recommendations to strengthen the conclusions of future studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.678.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.678.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--678 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.678 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.678.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.678/>Empowering Multi-step Reasoning across Languages via Program-Aided Language Models</a></strong><br><a href=/people/l/leonardo-ranaldi/>Leonardo Ranaldi</a>
|
<a href=/people/g/giulia-pucci/>Giulia Pucci</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--678><div class="card-body p-3 small">In-context learning methods are popular inference strategies where Large Language Models (LLMs) are elicited to solve a task using provided demonstrations without parameter updates. Among these approaches are the reasoning methods, best exemplified by Chain-of-Thought (CoT) and Program-Aided Language Models (PAL), which elicit LLMs to generate reasoning paths, thus promoting accuracy and attracting increasing attention. However, despite the success of these methods, the ability to deliver multi-step reasoning remains limited to a single language, making it challenging to generalize to other languages and hindering global development.In this work, we propose Cross-lingual Program-Aided Language Models (CrossPAL), a method for aligning reasoning programs across languages. In particular, our method delivers programs as intermediate reasoning steps in different languages through a double-step cross-lingual prompting mechanism inspired by the Program-Aided approach. In addition, we introduce Self-consistent CrossPAL (SCrossPAL) to ensemble different reasoning paths across languages. Our experimental evaluations show that our method significantly outperforms existing prompting methods, reducing the number of interactions and achieving state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.679.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.679.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--679 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.679 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.679/>Do <span class=acl-fixed-case>LLM</span>s Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models</a></strong><br><a href=/people/y/yu-yuan/>Yu Yuan</a>
|
<a href=/people/l/lili-zhao/>Lili Zhao</a>
|
<a href=/people/k/kai-zhang/>Kai Zhang</a>
|
<a href=/people/g/guangting-zheng/>Guangting Zheng</a>
|
<a href=/people/q/qi-liu/>Qi Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--679><div class="card-body p-3 small">Large Language Models (LLMs) have shown remarkable capabilities in various natural language processing tasks. However, LLMs may rely on dataset biases as shortcuts for prediction, which can significantly impair their robustness and generalization capabilities. This paper presents Shortcut Suite, a comprehensive test suite designed to evaluate the impact of shortcuts on LLMs’ performance, incorporating six shortcut types, five evaluation metrics, and four prompting strategies. Our extensive experiments yield several key findings: 1) LLMs demonstrate varying reliance on shortcuts for downstream tasks, which significantly impairs their performance. 2) Larger LLMs are more likely to utilize shortcuts under zero-shot and few-shot in-context learning prompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and outperforms other prompting strategies, while few-shot prompts generally underperform compared to zero-shot prompts. 4) LLMs often exhibit overconfidence in their predictions, especially when dealing with datasets that contain shortcuts. 5) LLMs generally have a lower explanation quality in shortcut-laden datasets, with errors falling into three types: distraction, disguised comprehension, and logical fallacy. Our findings offer new insights for evaluating robustness and generalization in LLMs and suggest potential directions for mitigating the reliance on shortcuts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.680.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.680.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--680 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.680 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.680/><span class=acl-fixed-case>C</span>ontrol<span class=acl-fixed-case>M</span>ath: Controllable Data Generation Promotes Math Generalist Models</a></strong><br><a href=/people/n/nuo-chen/>Nuo Chen</a>
|
<a href=/people/n/ning-wu/>Ning Wu</a>
|
<a href=/people/j/jianhui-chang/>Jianhui Chang</a>
|
<a href=/people/l/linjun-shou/>Linjun Shou</a>
|
<a href=/people/j/jia-li/>Jia Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--680><div class="card-body p-3 small">Utilizing large language models (LLMs) for data augmentation has yielded encouraging results in mathematical reasoning. However, these approaches face constraints in problem diversity, potentially restricting them to in-domain/distribution data generation. To this end, we propose **ControlMath**, an iterative method involving an equation-generator module and two LLM-based agents. The module creates diverse equations, which the Problem-Crafter agent then transforms into math word problems. The Reverse-Agent filters and selects high-quality data, adhering to the “less is more” principle. This approach enables the generation of diverse math problems, not limited to specific domains or distributions. As a result, we collect ControlMathQA, which involves 190k math word problems. Extensive results prove that combining our dataset with in-domain datasets like GSM8K can help improve the model’s mathematical ability to generalize, leading to improved performance both within and beyond specific domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.681.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.681.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--681 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.681 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.681/>Where Am <span class=acl-fixed-case>I</span> From? Identifying Origin of <span class=acl-fixed-case>LLM</span>-generated Content</a></strong><br><a href=/people/l/liying-li/>Liying Li</a>
|
<a href=/people/y/yihan-bai/>Yihan Bai</a>
|
<a href=/people/m/minhao-cheng/>Minhao Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--681><div class="card-body p-3 small">Generative models, particularly large language models (LLMs), have achieved remarkable success in producing natural and high-quality content. However, their widespread adoption raises concerns regarding copyright infringement, privacy violations, and security risks associated with AI-generated content. To address these concerns, we propose a novel digital forensics framework for LLMs, enabling the tracing of AI-generated content back to its source. This framework embeds a secret watermark directly into the generated output, eliminating the need for model retraining. To enhance traceability, especially for short outputs, we introduce a “depth watermark” that strengthens the link between content and generator. Our approach ensures accurate tracing while maintaining the quality of the generated content. Extensive experiments across various settings and datasets validate the effectiveness and robustness of our proposed framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.682.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.682.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--682 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.682 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.682/><span class=acl-fixed-case>R</span>ead<span class=acl-fixed-case>M</span>e++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment</a></strong><br><a href=/people/t/tarek-naous/>Tarek Naous</a>
|
<a href=/people/m/michael-j-ryan/>Michael J Ryan</a>
|
<a href=/people/a/anton-lavrouk/>Anton Lavrouk</a>
|
<a href=/people/m/mohit-chandra/>Mohit Chandra</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--682><div class="card-body p-3 small">We present a comprehensive evaluation of large language models for multilingual readability assessment. Existing evaluation resources lack domain and language diversity, limiting the ability for cross-domain and cross-lingual analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset with human annotations of 9757 sentences in Arabic, English, French, Hindi, and Russian, collected from 112 different data sources. This benchmark will encourage research on developing robust multilingual readability assessment methods. Using ReadMe++, we benchmark multilingual and monolingual language models in the supervised, unsupervised, and few-shot prompting settings. The domain and language diversity in ReadMe++ enable us to test more effective few-shot prompting, and identify shortcomings in state-of-the-art unsupervised methods. Our experiments also reveal exciting results of superior domain generalization and enhanced cross-lingual transfer capabilities by models trained on ReadMe++. We will make our data publicly available and release a python package tool for multilingual sentence readability prediction using our trained models at: https://github.com/tareknaous/readme</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.683.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.683.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--683 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.683 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.683.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.683/><span class=acl-fixed-case>G</span>loss<span class=acl-fixed-case>LM</span>: A Massively Multilingual Corpus and Pretrained Model for Interlinear Glossed Text</a></strong><br><a href=/people/m/michael-ginn/>Michael Ginn</a>
|
<a href=/people/l/lindia-tjuatja/>Lindia Tjuatja</a>
|
<a href=/people/t/taiqi-he/>Taiqi He</a>
|
<a href=/people/e/enora-rice/>Enora Rice</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/l/lori-levin/>Lori Levin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--683><div class="card-body p-3 small">Language documentation projects often involve the creation of annotated text in a format such as interlinear glossed text (IGT), which captures fine-grained morphosyntactic analyses in a morpheme-by-morpheme format. However, there are few existing resources providing large amounts of standardized, easily accessible IGT data, limiting their applicability to linguistic research, and making it difficult to use such data in NLP modeling. We compile the largest existing corpus of IGT data from a variety of sources, covering over 450k examples across 1.8k languages, to enable research on crosslingual transfer and IGT generation. We normalize much of our data to follow a standard set of labels across languages.Furthermore, we explore the task of automatically generating IGT in order to aid documentation projects. As many languages lack sufficient monolingual data, we pretrain a large multilingual model on our corpus. We demonstrate the utility of this model by finetuning it on monolingual corpora, outperforming SOTA models by up to 6.6%. Our pretrained model and dataset are available on Hugging Face: https://huggingface.co/collections/lecslab/glosslm-66da150854209e910113dd87</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.684.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.684.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--684 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.684 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.684/><span class=acl-fixed-case>GDTB</span>: Genre Diverse Data for <span class=acl-fixed-case>E</span>nglish Shallow Discourse Parsing across Modalities, Text Types, and Domains</a></strong><br><a href=/people/y/yang-janet-liu/>Yang Janet Liu</a>
|
<a href=/people/t/tatsuya-aoyama/>Tatsuya Aoyama</a>
|
<a href=/people/w/wesley-scivetti/>Wesley Scivetti</a>
|
<a href=/people/y/yilun-zhu/>Yilun Zhu</a>
|
<a href=/people/s/shabnam-behzad/>Shabnam Behzad</a>
|
<a href=/people/l/lauren-elizabeth-levine/>Lauren Elizabeth Levine</a>
|
<a href=/people/j/jessica-lin/>Jessica Lin</a>
|
<a href=/people/d/devika-tiwari/>Devika Tiwari</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--684><div class="card-body p-3 small">Work on shallow discourse parsing in English has focused on the Wall Street Journal corpus, the only large-scale dataset for the language in the PDTB framework. However, the data is not openly available, is restricted to the news domain, and is by now 35 years old. In this paper, we present and evaluate a new open-access, multi-genre benchmark for PDTB-style shallow discourse parsing, based on the existing UD English GUM corpus, for which discourse relation annotations in other frameworks already exist. In a series of experiments on cross-domain relation classification, we show that while our dataset is compatible with PDTB, substantial out-of-domain degradation is observed, which can be alleviated by joint training on both datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.685.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.685.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--685 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.685 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.685/><span class=acl-fixed-case>RA</span>2<span class=acl-fixed-case>FD</span>: Distilling Faithfulness into Efficient Dialogue Systems</a></strong><br><a href=/people/z/zhiyuan-zhu/>Zhiyuan Zhu</a>
|
<a href=/people/y/yusheng-liao/>Yusheng Liao</a>
|
<a href=/people/c/chenxin-xu/>Chenxin Xu</a>
|
<a href=/people/y/yunfeng-guan/>Yunfeng Guan</a>
|
<a href=/people/y/yanfeng-wang/>Yanfeng Wang</a>
|
<a href=/people/y/yu-wang/>Yu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--685><div class="card-body p-3 small">Generating faithful and fast responses is crucial in the knowledge-grounded dialogue. Retrieval Augmented Generation (RAG) strategies are effective but are inference inefficient, while previous Retrieval Free Generations (RFG) are more efficient but sacrifice faithfulness. To solve this faithfulness-efficiency trade-off dilemma, we propose a novel retrieval-free model training scheme named Retrieval Augmented to Retrieval Free Distillation (RA2FD) to build a retrieval-free model that achieves higher faithfulness than the previous RFG method while maintaining inference efficiency. The core idea of RA2FD is to use a teacher-student framework to distill the faithfulness capacity of a teacher, which is an oracle RAG model that generates multiple knowledge-infused responses. The student retrieval-free model learns how to generate faithful responses from these teacher labels through sequence-level distillation and contrastive learning. Experiment results show that RA2FD let the faithfulness performance of an RFG model surpass the previous SOTA RFG baseline on three knowledge-grounded dialogue datasets by an average of 33% and even matching an RAG model’s performance while significantly improving inference efficiency. Our code is available at https://github.com/zzysjtuiwct/RA2FD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.686.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.686.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--686 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.686 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.686/>Subjective Topic meets <span class=acl-fixed-case>LLM</span>s: Unleashing Comprehensive, Reflective and Creative Thinking through the Negation of Negation</a></strong><br><a href=/people/f/fangrui-lv/>Fangrui Lv</a>
|
<a href=/people/k/kaixiong-gong/>Kaixiong Gong</a>
|
<a href=/people/j/jian-liang/>Jian Liang</a>
|
<a href=/people/x/xinyu-pang/>Xinyu Pang</a>
|
<a href=/people/c/changshui-zhang/>Changshui Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--686><div class="card-body p-3 small">Large language models (LLMs) exhibit powerful reasoning capacity, as evidenced by prior studies focusing on objective topics that with unique standard answers such as arithmetic and commonsense reasoning. However, the reasoning to definite answers emphasizes more on logical thinking, and falls short in effectively reflecting the comprehensive, reflective, and creative thinking that is also critical for the overall reasoning prowess of LLMs. In light of this, we build a dataset SJTP comprising diverse SubJective ToPics with free responses, as well as three evaluation indicators to fully explore LLM’s reasoning ability. We observe that a sole emphasis on logical thinking falls short in effectively tackling subjective challenges. Therefore, we introduce a framework grounded in the principle of the Negation of Negation (NeoN) to unleash the potential comprehensive, reflective, and creative thinking abilities of LLMs. Comprehensive experiments on SJTP demonstrate the efficacy of NeoN, and the enhanced performance on various objective reasoning tasks unequivocally underscores the benefits of stimulating LLM’s subjective thinking in augmenting overall reasoning capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.687.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.687.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--687 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.687 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.687.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.687/>Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently</a></strong><br><a href=/people/k/kanishka-misra/>Kanishka Misra</a>
|
<a href=/people/a/allyson-ettinger/>Allyson Ettinger</a>
|
<a href=/people/k/kyle-mahowald/>Kyle Mahowald</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--687><div class="card-body p-3 small">Recent zero-shot evaluations have highlighted important limitations in the abilities of language models (LMs) to perform meaning extraction. However, it is now well known that LMs can demonstrate radical improvements in the presence of experimental contexts such as in-context examples and instructions. How well does this translate to previously studied meaning-sensitive tasks? We present a case-study on the extent to which experimental contexts can improve LMs’ robustness in performing property inheritance—predicting semantic properties of novel concepts, a task that they have been previously shown to fail on. Upon carefully controlling the nature of the in-context examples and the instructions, our work reveals that they can indeed lead to non-trivial property inheritance behavior in LMs. However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computational principles of semantic property inference are yet to be mastered by LMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.688.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.688.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--688 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.688 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.688/>Leveraging Estimated Transferability Over Human Intuition for Model Selection in Text Ranking</a></strong><br><a href=/people/j/jun-bai/>Jun Bai</a>
|
<a href=/people/z/zhuofan-chen/>Zhuofan Chen</a>
|
<a href=/people/z/zhenzi-li/>Zhenzi Li</a>
|
<a href=/people/h/hanhua-hong/>Hanhua Hong</a>
|
<a href=/people/j/jianfei-zhang/>Jianfei Zhang</a>
|
<a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a>
|
<a href=/people/w/wenge-rong/>Wenge Rong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--688><div class="card-body p-3 small">Text ranking has witnessed significant advancements, attributed to the utilization of dual-encoder enhanced by Pre-trained Language Models (PLMs). Given the proliferation of available PLMs, selecting the most effective one for a given dataset has become a non-trivial challenge. As a promising alternative to human intuition and brute-force fine-tuning, Transferability Estimation (TE) has emerged as an effective approach to model selection. However, current TE methods are primarily designed for classification tasks, and their estimated transferability may not align well with the objectives of text ranking. To address this challenge, we propose to compute the expected rank as transferability, explicitly reflecting the model’s ranking capability. Furthermore, to mitigate anisotropy and incorporate training dynamics, we adaptively scale isotropic sentence embeddings to yield an accurate expected rank score. Our resulting method, Adaptive Ranking Transferability (AiRTran), can effectively capture subtle differences between models. On challenging model selection scenarios across various text ranking datasets, it demonstrates significant improvements over previous classification-oriented TE methods, human intuition, and ChatGPT with minor time consumption.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.689.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.689.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--689 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.689 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.689/>Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism</a></strong><br><a href=/people/a/anhao-zhao/>Anhao Zhao</a>
|
<a href=/people/f/fanghua-ye/>Fanghua Ye</a>
|
<a href=/people/j/jinlan-fu/>Jinlan Fu</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--689><div class="card-body p-3 small">Large language models (LLMs) exhibit remarkable in-context learning (ICL) capabilities. However, the underlying working mechanism of ICL remains poorly understood. Recent research presents two conflicting views on ICL: One emphasizes the impact of similar examples in the demonstrations, stressing the need for label correctness and more shots. The other attributes it to LLMs’ inherent ability of task recognition, deeming label correctness and shot numbers of demonstrations as not crucial. In this work, we provide a Two-Dimensional Coordinate System that unifies both views into a systematic framework. The framework explains the behavior of ICL through two orthogonal variables: whether similar examples are presented in the demonstrations (perception) and whether LLMs can recognize the task (cognition). We propose the peak inverse rank metric to detect the task recognition ability of LLMs and study LLMs’ reactions to different definitions of similarity. Based on these, we conduct extensive experiments to elucidate how ICL functions across each quadrant on multiple representative classification tasks. Finally, we extend our analyses to generation tasks, showing that our coordinate system can also be used to interpret ICL for generation tasks effectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.690.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.690.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--690 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.690 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.690/>Self-Powered <span class=acl-fixed-case>LLM</span> Modality Expansion for Large Speech-Text Models</a></strong><br><a href=/people/t/tengfei-yu/>Tengfei Yu</a>
|
<a href=/people/x/xuebo-liu/>Xuebo Liu</a>
|
<a href=/people/z/zhiyi-hou/>Zhiyi Hou</a>
|
<a href=/people/l/liang-ding/>Liang Ding</a>
|
<a href=/people/d/dacheng-tao/>Dacheng Tao</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--690><div class="card-body p-3 small">Large language models (LLMs) exhibit remarkable performance across diverse tasks, indicating their potential for expansion into large speech-text models (LSMs) by integrating speech capabilities. Although unified speech-text pre-training and multimodal data instruction-tuning offer considerable benefits, these methods generally entail significant resource demands and tend to overfit specific tasks.This study aims to refine the use of speech datasets for LSM training by addressing the limitations of vanilla instruction tuning. We explore the instruction-following dynamics within LSMs, identifying a critical issue termed speech anchor bias—a tendency for LSMs to over-rely on speech inputs, mistakenly interpreting the entire speech modality as directives, thereby neglecting textual instructions.To counteract this bias, we introduce a self-powered LSM that leverages augmented automatic speech recognition data generated by the model itself for more effective instruction tuning. Our experiments across a range of speech-based tasks demonstrate that self-powered LSM mitigates speech anchor bias and improves the fusion of speech and text modalities in LSMs. Data, code and scripts are freely available at https://github.com/ytf-philp/Self-powered-LSM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.691.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.691.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--691 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.691 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.691.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.691/><span class=acl-fixed-case>ABSE</span>val: An Agent-based Framework for Script Evaluation</a></strong><br><a href=/people/s/sirui-liang/>Sirui Liang</a>
|
<a href=/people/b/baoli-zhang/>Baoli Zhang</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--691><div class="card-body p-3 small">Recent research indicates that large language models (LLMs) possess a certain degree of script planning capability. However, there is still a lack of focused work on evaluating scripts generated by LLMs. The evaluation of scripts poses challenges due to their logical structure, sequential organization, adherence to commonsense constraints, and open-endedness. In this work, We introduced a novel script evaluation dataset, MCScript, consisting of more than 1,500 script evaluation tasks and steps, and developed an agent-based script evaluation framework, ABSEval, to collaboratively evaluate scripts generated by LLMs. Our experiments demonstrate that ABSEval provides superior accuracy and relevance, aligning closely with human evaluation. We evaluated the script planning capabilities of 15 mainstream LLMs and provided a detailed analysis. Furthermore, we observed phenomena like the key factor influencing the script planning ability of LLM is not parameter size and suggested improvements for evaluating open-ended questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.692.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.692.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--692 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.692 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.692/>Latent Concept-based Explanation of <span class=acl-fixed-case>NLP</span> Models</a></strong><br><a href=/people/x/xuemin-yu/>Xuemin Yu</a>
|
<a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/m/marzia-nouri/>Marzia Nouri</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--692><div class="card-body p-3 small">Interpreting and understanding the predictions made by deep learning models poses a formidable challenge due to their inherently opaque nature. Many previous efforts aimed at explaining these predictions rely on input features, specifically, the words within NLP models. However, such explanations are often less informative due to the discrete nature of these words and their lack of contextual verbosity. To address this limitation, we introduce the Latent Concept Attribution method (LACOAT), which generates explanations for predictions based on latent concepts. Our foundational intuition is that a word can exhibit multiple facets, contingent upon the context in which it is used. Therefore, given a word in context, the latent space derived from our training process reflects a specific facet of that word. LACOAT functions by mapping the representations of salient input words into the training latent space, allowing it to provide latent context-based explanations of the prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.693.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.693.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.693.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.693/>Decoding with Limited Teacher Supervision Requires Understanding When to Trust the Teacher</a></strong><br><a href=/people/h/hyunjong-ok/>Hyunjong Ok</a>
|
<a href=/people/j/jegwang-ryu/>Jegwang Ryu</a>
|
<a href=/people/j/jaeho-lee/>Jaeho Lee</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.694.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.694.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--694 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.694 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.694/>Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research</a></strong><br><a href=/people/y/yida-mu/>Yida Mu</a>
|
<a href=/people/m/mali-jin/>Mali Jin</a>
|
<a href=/people/x/xingyi-song/>Xingyi Song</a>
|
<a href=/people/n/nikolaos-aletras/>Nikolaos Aletras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--694><div class="card-body p-3 small">Research in natural language processing (NLP) for Computational Social Science (CSS) heavily relies on data from social media platforms. This data plays a crucial role in the development of models for analysing socio-linguistic phenomena within online communities. In this work, we conduct an in-depth examination of 20 datasets extensively used in NLP for CSS to comprehensively examine data quality. Our analysis reveals that social media datasets exhibit varying levels of data duplication. Consequently, this gives rise to challenges like label inconsistencies and data leakage, compromising the reliability of models. Our findings also suggest that data duplication has an impact on the current claims of state-of-the-art performance, potentially leading to an overestimation of model effectiveness in real-world scenarios. Finally, we propose new protocols and best practices for improving dataset development from social media data and its usage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.695.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.695.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--695 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.695 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.695.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.695/>The Mystery of the Pathological Path-star Task for Language Models</a></strong><br><a href=/people/a/arvid-frydenlund/>Arvid Frydenlund</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--695><div class="card-body p-3 small">The recently introduced path-star task is a minimal task designed to exemplify limitations to the abilities of language models (Bachmann and Nagarajan, 2024). It involves a path-star graph where multiple arms radiate from a single starting node and each node is unique. Given the start node and a specified target node that ends an arm, the task is to generate the arm containing that target node. This is straightforward for a human but surprisingly difficult for language models, which did not outperform the random baseline. The authors hypothesized this is due to a deficiency in teacher-forcing and the next-token prediction paradigm. We demonstrate the task is learnable using teacher-forcing in alternative settings and that the issue is partially due to representation. We introduce a regularization method using structured samples of the same graph but with differing target nodes, improving results across a variety of model types. We provide RASP proofs showing the task is theoretically solvable. Finally, we find settings where an encoder-only model can consistently solve the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.696.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.696.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--696 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.696 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.696/>Voices in a Crowd: Searching for clusters of unique perspectives</a></strong><br><a href=/people/n/nikolas-vitsakis/>Nikolas Vitsakis</a>
|
<a href=/people/a/amit-parekh/>Amit Parekh</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--696><div class="card-body p-3 small">Language models have been shown to reproduce underlying biases existing in their training data, which is the majority perspective by default. Proposed solutions aim to capture minority perspectives by either modelling annotator disagreements or grouping annotators based on shared metadata, both of which face significant challenges. We propose a framework that trains models without encoding annotator metadata, extracts latent embeddings informed by annotator behaviour, and creates clusters of similar opinions, that we refer to as voices. Resulting clusters are validated post-hoc via internal and external quantitative metrics, as well a qualitative analysis to identify the type of voice that each cluster represents. Our results demonstrate the strong generalisation capability of our framework, indicated by resulting clusters being adequately robust, while also capturing minority perspectives based on different demographic factors throughout two distinct datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.697.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.697.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--697 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.697 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.697/>Neeko: Leveraging Dynamic <span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>RA</span> for Efficient Multi-Character Role-Playing Agent</a></strong><br><a href=/people/x/xiaoyan-yu/>Xiaoyan Yu</a>
|
<a href=/people/t/tongxu-luo/>Tongxu Luo</a>
|
<a href=/people/y/yifan-wei/>Yifan Wei</a>
|
<a href=/people/f/fangyu-lei/>Fangyu Lei</a>
|
<a href=/people/y/yiming-huang/>Yiming Huang</a>
|
<a href=/people/h/hao-peng/>Hao Peng</a>
|
<a href=/people/l/liehuang-zhu/>Liehuang Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--697><div class="card-body p-3 small">Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko’s adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.698.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.698.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--698 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.698 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.698/><span class=acl-fixed-case>SLANG</span>: New Concept Comprehension of Large Language Models</a></strong><br><a href=/people/l/lingrui-mei/>Lingrui Mei</a>
|
<a href=/people/s/shenghua-liu/>Shenghua Liu</a>
|
<a href=/people/y/yiwei-wang/>Yiwei Wang</a>
|
<a href=/people/b/baolong-bi/>Baolong Bi</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--698><div class="card-body p-3 small">The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of Large Language Models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research aims to bridge this gap by enhancing LLMs’ comprehension of the evolving new concepts on the Internet, without the high cost of continual retraining. In pursuit of this goal, we introduce <b>SLNAG</b>, a benchmark designed to autonomously integrate novel data and assess LLMs’ ability to comprehend emerging concepts, alongside <b>FOCUS</b>, an approach uses causal inference to enhance LLMs to understand new phrases and their colloquial context. Our benchmark and approach involves understanding real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly emerging expressions and their meanings. The empirical analysis shows that our causal inference-based approach outperforms the baseline methods in terms of precision and relevance in the comprehension of Internet slang and memes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.699.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.699.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--699 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.699 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.699/>Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models</a></strong><br><a href=/people/m/michael-lan/>Michael Lan</a>
|
<a href=/people/p/philip-torr/>Philip Torr</a>
|
<a href=/people/f/fazl-barez/>Fazl Barez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--699><div class="card-body p-3 small">While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of Arabic numerals, number words, and months. By applying circuit interpretability analysis, we identify a key sub-circuit in both GPT-2 Small and Llama-2-7B responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Additionally, we show that this sub-circuit has effects on various math-related prompts, such as on intervaled circuits, Spanish number word and months continuation, and natural language word problems. Overall, documenting shared computational structures enables better model behavior predictions, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.700.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--700 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.700 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.700/>Why Does New Knowledge Create Messy Ripple Effects in <span class=acl-fixed-case>LLM</span>s?</a></strong><br><a href=/people/j/jiaxin-qin/>Jiaxin Qin</a>
|
<a href=/people/z/zixuan-zhang/>Zixuan Zhang</a>
|
<a href=/people/c/chi-han/>Chi Han</a>
|
<a href=/people/p/pengfei-yu/>Pengfei Yu</a>
|
<a href=/people/m/manling-li/>Manling Li</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--700><div class="card-body p-3 small">Extensive previous research has focused on post-training knowledge editing (KE) for language models (LMs) to ensure that knowledge remains accurate and up-to-date. One desired property and open question in KE is to let edited LMs correctly handle ripple effects, where LM is expected to answer its logically related knowledge accurately. In this paper, we answer the question of why most KE methods still create messy ripple effects. We conduct extensive analysis and identify a salient indicator, GradSim, that effectively reveals when and why updated knowledge ripples in LMs. GradSim is computed by the cosine similarity between gradients of the original fact and its related knowledge. We observe a strong positive correlation between ripple effect performance and GradSim across different LMs, KE methods, and evaluation metrics. Further investigations into three counter-intuitive failure cases (Negation, Over-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures are often associated with very low GradSim. This finding validates that GradSim is an effective indicator of when knowledge ripples in LMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.701.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--701 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.701.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.701/>Lifelong Event Detection via Optimal Transport</a></strong><br><a href=/people/v/viet-dao/>Viet Dao</a>
|
<a href=/people/v/van-cuong-pham/>Van-Cuong Pham</a>
|
<a href=/people/q/quyen-tran/>Quyen Tran</a>
|
<a href=/people/t/thanh-thien-le/>Thanh-Thien Le</a>
|
<a href=/people/l/linh-van-ngo/>Linh Van Ngo</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--701><div class="card-body p-3 small">Continual Event Detection (CED) poses a formidable challenge due to the catastrophic forgetting phenomenon, where learning new tasks (with new coming event types) hampers performance on previous ones. In this paper, we introduce a novel approach, Lifelong Event Detection via Optimal Transport (**LEDOT**), that leverages optimal transport principles to align the optimization of our classification module with the intrinsic nature of each class, as defined by their pre-trained language modeling. Our method integrates replay sets, prototype latent representations, and an innovative Optimal Transport component. Extensive experiments on MAVEN and ACE datasets demonstrate LEDOT’s superior performance, consistently outperforming state-of-the-art baselines. The results underscore LEDOT as a pioneering solution in continual event detection, offering a more effective and nuanced approach to addressing catastrophic forgetting in evolving environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.702.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--702 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.702/><span class=acl-fixed-case>SUPER</span>: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories</a></strong><br><a href=/people/b/ben-bogin/>Ben Bogin</a>
|
<a href=/people/k/kejuan-yang/>Kejuan Yang</a>
|
<a href=/people/s/shashank-gupta/>Shashank Gupta</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a>
|
<a href=/people/e/erin-bransom/>Erin Bransom</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--702><div class="card-body p-3 small">Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPER aims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub-problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.703.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--703 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.703/><span class=acl-fixed-case>FIRST</span>: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation</a></strong><br><a href=/people/k/kashun-shum/>KaShun Shum</a>
|
<a href=/people/m/minrui-xu/>Minrui Xu</a>
|
<a href=/people/j/jianshu-zhang/>Jianshu Zhang</a>
|
<a href=/people/z/zixin-chen/>Zixin Chen</a>
|
<a href=/people/s/shizhe-diao/>Shizhe Diao</a>
|
<a href=/people/h/hanze-dong/>Hanze Dong</a>
|
<a href=/people/j/jipeng-zhang/>Jipeng Zhang</a>
|
<a href=/people/m/muhammad-omer-raza/>Muhammad Omer Raza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--703><div class="card-body p-3 small">Large language models (LLMs) have become increasingly prevalent in our daily lives, leading to an expectation for LLMs to be trustworthy —- both accurate and well-calibrated (the prediction confidence should align with its ground truth correctness likelihood). Nowadays, fine-tuning has become the most popular method for adapting a model to practical usage by significantly increasing accuracy on downstream tasks. Despite the great accuracy it achieves, we found fine-tuning is still far away from satisfactory trustworthiness due to “tuning-induced mis-calibration”. In this paper, we delve deeply into why and how mis-calibration exists in fine-tuned models, and how distillation can alleviate the issue. Then we further propose a brand new method named Efficient Trustworthy Distillation (FIRST), which utilizes a small portion of teacher’s knowledge to obtain a reliable language model in a cost-efficient way. Specifically, we identify the “concentrated knowledge” phenomenon during distillation, which can significantly reduce the computational burden. Then we apply a “trustworthy maximization” process to optimize the utilization of this small portion of concentrated knowledge before transferring it to the student. Experimental results demonstrate the effectiveness of our method, where better accuracy (+2.3%) and less mis-calibration (-10%) are achieved on average across both in-domain and out-of-domain scenarios, indicating better trustworthiness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.704.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--704 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.704/>Domain adapted machine translation: What does catastrophic forgetting forget and why?</a></strong><br><a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/s/steve-deneefe/>Steve DeNeefe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--704><div class="card-body p-3 small">Neural Machine Translation (NMT) models can be specialized by domain adaptation, often involving fine-tuning on a dataset of interest. This process risks catastrophic forgetting: rapid loss of generic translation quality. Forgetting has been widely observed, with many mitigation methods proposed. However, the causes of forgetting and the relationship between forgetting and adaptation data are underexplored.This paper takes a novel approach to understanding catastrophic forgetting during NMT adaptation by investigating the impact of the data. We provide a first investigation of what is forgotten, and why. We examine the relationship between forgetting and the in-domain data, and show that the amount and type of forgetting is linked to that data’s target vocabulary coverage. Our findings pave the way toward better informed NMT domain adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.705.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--705 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.705.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.705/>Enhancing <span class=acl-fixed-case>AI</span> Assisted Writing with One-Shot Implicit Negative Feedback</a></strong><br><a href=/people/b/benjamin-towle/>Benjamin Towle</a>
|
<a href=/people/k/ke-zhou/>Ke Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--705><div class="card-body p-3 small">AI-mediated communication enables users to communicate more quickly and efficiently. Various systems have been proposed such as smart reply and AI-assisted writing. Yet, the heterogeneity of the forms of inputs and architectures often renders it challenging to combine insights from user behaviour in one system to improve performance in another. In this work, we consider the case where the user does not select any of the suggested replies from a smart reply system, and how this can be used as one-shot implicit negative feedback to enhance the accuracy of an AI writing model. We introduce Nifty, an approach that uses classifier guidance to controllably integrate implicit user feedback into the text generation process. Empirically, we find up to 34% improvement in Rouge-L, 89% improvement in generating the correct intent, and an 86% win-rate according to human evaluators compared to a vanilla AI writing system on the MultiWOZ and Schema-Guided Dialog datasets. The code is available at https://github.com/BenjaminTowle/NIFTY.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.706.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--706 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.706/>Atomic Self-Consistency for Better Long Form Generations</a></strong><br><a href=/people/r/raghuveer-thirukovalluru/>Raghuveer Thirukovalluru</a>
|
<a href=/people/y/yukun-huang/>Yukun Huang</a>
|
<a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--706><div class="card-body p-3 small">Recent work has aimed to improve LLM generations by filtering out hallucinations, thereby improving the precision of the information in responses. Correctness of a long-form response, however, also depends on the recall of multiple pieces of information relevant to the question. In this paper, we introduce Atomic Self-Consistency (ASC), a technique for improving the recall of relevant information in an LLM response. ASC follows recent work, Universal Self-Consistency (USC) in using multiple stochastic samples from an LLM to improve the long-form response. Unlike USC which only focuses on selecting the best single generation, ASC picks authentic subparts from the samples and merges them into a superior composite answer. Through extensive experiments and ablations, we show that merging relevant subparts of multiple samples performs significantly better than picking a single sample. ASC demonstrates significant gains over USC on multiple factoids and open-ended QA datasets - ASQA, QAMPARI, QUEST, ELI5 with ChatGPT and Llama3. Our analysis also reveals untapped potential for enhancing long-form generations using the approach of merging multiple samples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.707.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--707 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.707.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.707/>“Global is Good, Local is Bad?”: Understanding Brand Bias in <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/m/mahammed-kamruzzaman/>Mahammed Kamruzzaman</a>
|
<a href=/people/h/hieu-minh-nguyen/>Hieu Minh Nguyen</a>
|
<a href=/people/g/gene-louis-kim/>Gene Louis Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--707><div class="card-body p-3 small">Many recent studies have investigated social biases in LLMs but brand bias has received little attention. This research examines the biases exhibited by LLMs towards different brands, a significant concern given the widespread use of LLMs in affected use cases such as product recommendation and market analysis. Biased models may perpetuate societal inequalities, unfairly favoring established global brands while marginalizing local ones. Using a curated dataset across four brand categories, we probe the behavior of LLMs in this space. We find a consistent pattern of bias in this space—both in terms of disproportionately associating global brands with positive attributes and disproportionately recommending luxury gifts for individuals in high-income countries. We also find LLMs are subject to country-of-origin effects which may boost local brand preference in LLM outputs in specific contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.708.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--708 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.708.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.708/>Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach</a></strong><br><a href=/people/s/siqi-li/>Siqi Li</a>
|
<a href=/people/d/danni-liu/>Danni Liu</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--708><div class="card-body p-3 small">Direct speech translation (ST) models often struggle with rare words. Incorrect translation of these words can have severe consequences, impacting translation quality and user trust. While rare word translation is inherently challenging for neural models due to sparse learning signals, real-world scenarios often allow access to translations of past recordings on similar topics. To leverage these valuable resources, we propose a retrieval-and-demonstration approach to enhance rare word translation accuracy in direct ST models. First, we adapt existing ST models to incorporate retrieved examples for rare word translation, which allows the model to benefit from prepended examples, similar to in-context learning. We then develop a cross-modal (speech-to-speech, speech-to-text, text-to-text) retriever to locate suitable examples. We demonstrate that standard ST models can be effectively adapted to leverage examples for rare word translation, improving rare word translation accuracy over the baseline by 17.6% with gold examples and 8.5% with retrieved examples. Moreover, our speech-to-speech retrieval approach outperforms other modalities and exhibits higher robustness to unseen speakers. Our code is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.709.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--709 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.709.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.709.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.709/><span class=acl-fixed-case>ACE</span>: A <span class=acl-fixed-case>LLM</span>-based Negotiation Coaching System</a></strong><br><a href=/people/r/ryan-shea/>Ryan Shea</a>
|
<a href=/people/a/aymen-kallala/>Aymen Kallala</a>
|
<a href=/people/x/xin-lucy-liu/>Xin Lucy Liu</a>
|
<a href=/people/m/michael-w-morris/>Michael W. Morris</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--709><div class="card-body p-3 small">The growing prominence of LLMs has led to an increase in the development of AI tutoring systems. These systems are crucial in providing underrepresented populations with improved access to valuable education. One important area of education that is unavailable to many learners is strategic bargaining related to negotiation. To address this, we develop a LLM-based Assistant for Coaching nEgotiation (ACE). ACE not only serves as a negotiation partner for users but also provides them with targeted feedback for improvement. To build our system, we collect a dataset of negotiation transcripts between MBA students. These transcripts come from trained negotiators and emulate realistic bargaining scenarios. We use the dataset, along with expert consultations, to design an annotation scheme for detecting negotiation mistakes. ACE employs this scheme to identify mistakes and provide targeted feedback to users. To test the effectiveness of ACE-generated feedback, we conducted a user experiment with two consecutive trials of negotiation and found that it improves negotiation performances significantly compared to a system that doesn’t provide feedback and one which uses an alternative method of providing feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.710.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--710 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.710/><span class=acl-fixed-case>T</span>ransfer<span class=acl-fixed-case>TOD</span>: A Generalizable <span class=acl-fixed-case>C</span>hinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities</a></strong><br><a href=/people/m/ming-zhang/>Ming Zhang</a>
|
<a href=/people/c/caishuang-huang/>Caishuang Huang</a>
|
<a href=/people/y/yilong-wu/>Yilong Wu</a>
|
<a href=/people/s/shichun-liu/>Shichun Liu</a>
|
<a href=/people/h/huiyuan-zheng/>Huiyuan Zheng</a>
|
<a href=/people/y/yurui-dong/>Yurui Dong</a>
|
<a href=/people/y/yujiong-shen/>Yujiong Shen</a>
|
<a href=/people/s/shihan-dou/>Shihan Dou</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/j/junjie-ye/>Junjie Ye</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--710><div class="card-body p-3 small">Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented conversations, including information collection. How to utilize TOD accurately, efficiently and effectively for information collection has always been a critical and challenging task. Recent studies have demonstrated that Large Language Models (LLMs) excel in dialogue, instruction generation, and reasoning, and can significantly enhance the performance of TOD through fine-tuning. However, current datasets primarily cater to user-led systems and are limited to predefined specific scenarios and slots, thereby necessitating improvements in the proactiveness, diversity, and capabilities of TOD. In this study, we present a detailed multi-domain task-oriented data construction process for conversations, and a Chinese dialogue dataset generated based on this process, **TransferTOD**, which authentically simulates human-computer dialogues in 30 popular life service scenarios. Leveraging this dataset, we trained a model using full-parameter fine-tuning called **TransferTOD-7B**, showcasing notable abilities in slot filling and questioning. Our work has demonstrated its strong generalization capabilities in various downstream scenarios, significantly enhancing both data utilization efficiency and system performance. The data is released in https://github.com/KongLongGeFDU/TransferTOD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.711.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--711 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.711/><span class=acl-fixed-case>PATIENT</span>-<span class=tex-math>𝜓</span>: Using Large Language Models to Simulate Patients for Training Mental Health Professionals</a></strong><br><a href=/people/r/ruiyi-wang/>Ruiyi Wang</a>
|
<a href=/people/s/stephanie-milani/>Stephanie Milani</a>
|
<a href=/people/j/jamie-c-chiu/>Jamie C. Chiu</a>
|
<a href=/people/j/jiayin-zhi/>Jiayin Zhi</a>
|
<a href=/people/s/shaun-m-eack/>Shaun M. Eack</a>
|
<a href=/people/t/travis-labrum/>Travis Labrum</a>
|
<a href=/people/s/samuel-m-murphy/>Samuel M Murphy</a>
|
<a href=/people/n/nev-jones/>Nev Jones</a>
|
<a href=/people/k/kate-v-hardy/>Kate V Hardy</a>
|
<a href=/people/h/hong-shen/>Hong Shen</a>
|
<a href=/people/f/fei-fang/>Fei Fang</a>
|
<a href=/people/z/zhiyu-chen/>Zhiyu Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--711><div class="card-body p-3 small">Mental illness remains one of the most critical public health issues. Despite its importance, many mental health professionals highlight a disconnect between their training and actual real-world patient practice. To help bridge this gap, we propose PATIENT-<span class=tex-math>𝜓</span>, a novel patient simulation framework for cognitive behavior therapy (CBT) training. To build PATIENT-<span class=tex-math>𝜓</span>, we construct diverse patient cognitive models based on CBT principles and use large language models (LLMs) programmed with these cognitive models to act as a simulated therapy patient. We propose an interactive training scheme, PATIENT-<span class=tex-math>𝜓</span>-TRAINER, for mental health trainees to practice a key skill in CBT – formulating the cognitive model of the patient – through role-playing a therapy session with PATIENT-<span class=tex-math>𝜓</span>. To evaluate PATIENT-<span class=tex-math>𝜓</span>, we conducted a comprehensive user study of 13 mental health trainees and 20 experts. The results demonstrate that practice using PATIENT-<span class=tex-math>𝜓</span>-TRAINER enhances the perceived skill acquisition and confidence of the trainees beyond existing forms of training such as textbooks, videos, and role-play with non-patients. Based on the experts’ perceptions, PATIENT-<span class=tex-math>𝜓</span> is perceived to be closer to real patient interactions than GPT-4, and PATIENT-<span class=tex-math>𝜓</span>-TRAINER holds strong promise to improve trainee competencies. Our code and data are released at <a href=https://github.com/ruiyiw/patient-psi class=acl-markup-url>https://github.com/ruiyiw/patient-psi</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.712.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.712.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--712 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.712 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.712/><span class=acl-fixed-case>DKEC</span>: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction</a></strong><br><a href=/people/x/xueren-ge/>Xueren Ge</a>
|
<a href=/people/a/abhishek-satpathy/>Abhishek Satpathy</a>
|
<a href=/people/r/ronald-dean-williams/>Ronald Dean Williams</a>
|
<a href=/people/j/john-stankovic/>John Stankovic</a>
|
<a href=/people/h/homa-alemzadeh/>Homa Alemzadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--712><div class="card-body p-3 small">Multi-label text classification (MLTC) tasks in the medical domain often face the long-tail label distribution problem. Prior works have explored hierarchical label structures to find relevant information for few-shot classes, but mostly neglected to incorporate external knowledge from medical guidelines. This paper presents DKEC, Domain Knowledge Enhanced Classification for diagnosis prediction with two innovations: (1) automated construction of heterogeneous knowledge graphs from external sources to capture semantic relations among diverse medical entities, (2) incorporating the heterogeneous knowledge graphs in few-shot classification using a label-wise attention mechanism. We construct DKEC using three online medical knowledge sources and evaluate it on a real-world Emergency Medical Services (EMS) dataset and a public electronic health record (EHR) dataset. Results show that DKEC outperforms the state-of-the-art label-wise attention networks and transformer models of different sizes, particularly for the few-shot classes. More importantly, it helps the smaller language models achieve comparable performance to large language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.713.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.713/><span class=tex-math><span class=text-monospace>ModSCAN</span></span>: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities</a></strong><br><a href=/people/y/yukun-jiang/>Yukun Jiang</a>
|
<a href=/people/z/zheng-li/>Zheng Li</a>
|
<a href=/people/x/xinyue-shen/>Xinyue Shen</a>
|
<a href=/people/y/yugeng-liu/>Yugeng Liu</a>
|
<a href=/people/m/michael-backes/>Michael Backes</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.714.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--714 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.714.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.714.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.714/>Large Language Models Can Self-Correct with Key Condition Verification</a></strong><br><a href=/people/z/zhenyu-wu/>Zhenyu Wu</a>
|
<a href=/people/q/qingkai-zeng/>Qingkai Zeng</a>
|
<a href=/people/z/zhihan-zhang/>Zhihan Zhang</a>
|
<a href=/people/z/zhaoxuan-tan/>Zhaoxuan Tan</a>
|
<a href=/people/c/chao-shen/>Chao Shen</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--714><div class="card-body p-3 small">Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective prompting method enhances LLM performance in identifying and correcting inaccurate answers without external feedback.That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numerical value in an arithmetic question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo-1106 as the backend LLM, yields <span class=tex-math>+6.8</span> exact match on four open-domain question answering datasets, <span class=tex-math>+14.1</span> accuracy on three arithmetic reasoning datasets, and <span class=tex-math>+9.6</span> accuracy on a commonsense reasoning dataset, compared to Self-Correct.Our implementation is made publicly available at https://wzy6642.github.io/proco.github.io/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.715.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.715.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--715 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.715 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.715/>Learning to Write Rationally: How Information Is Distributed in Non-native Speakers’ Essays</a></strong><br><a href=/people/z/zixin-tang/>Zixin Tang</a>
|
<a href=/people/j/janet-van-hell/>Janet Van Hell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--715><div class="card-body p-3 small">People tend to distribute information evenly in language production for better and clearer communication. In this study, we compared essays written by second language (L2) learners with various native language (L1) backgrounds to investigate how they distribute information in their non-native L2 production. Analyses of surprisal and constancy of entropy rate indicated that writers with higher L2 proficiency can reduce the expected uncertainty of language production while still conveying informative content. However, the uniformity of information distribution showed less variability among different groups of L2 speakers, suggesting that this feature may be universal in L2 essay writing and less affected by L2 writers’ variability in L1 background and L2 proficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.716.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.716.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.716.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.716/>Defending Against Social Engineering Attacks in the Age of <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/l/lin-ai/>Lin Ai</a>
|
<a href=/people/t/tharindu-sandaruwan-kumarage/>Tharindu Sandaruwan Kumarage</a>
|
<a href=/people/a/amrita-bhattacharjee/>Amrita Bhattacharjee</a>
|
<a href=/people/z/zizhou-liu/>Zizhou Liu</a>
|
<a href=/people/z/zheng-hui/>Zheng Hui</a>
|
<a href=/people/m/michael-s-davinroy/>Michael S. Davinroy</a>
|
<a href=/people/j/james-cook/>James Cook</a>
|
<a href=/people/l/laura-cassani/>Laura Cassani</a>
|
<a href=/people/k/kirill-trapeznikov/>Kirill Trapeznikov</a>
|
<a href=/people/m/matthias-kirchner/>Matthias Kirchner</a>
|
<a href=/people/a/arslan-basharat/>Arslan Basharat</a>
|
<a href=/people/a/anthony-hoogs/>Anthony Hoogs</a>
|
<a href=/people/j/joshua-garland/>Joshua Garland</a>
|
<a href=/people/h/huan-liu/>Huan Liu</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.717.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.717.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--717 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.717 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.717/>Heterogeneous <span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>RA</span> for Federated Fine-tuning of On-Device Foundation Models</a></strong><br><a href=/people/y/yae-jee-cho/>Yae Jee Cho</a>
|
<a href=/people/l/luyang-liu/>Luyang Liu</a>
|
<a href=/people/z/zheng-xu/>Zheng Xu</a>
|
<a href=/people/a/aldi-fahrezi/>Aldi Fahrezi</a>
|
<a href=/people/g/gauri-joshi/>Gauri Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--717><div class="card-body p-3 small">Foundation models (FMs) adapt surprisingly well to downstream tasks with fine-tuning. However, their colossal parameter space prohibits their training on resource-constrained edge-devices. For federated fine-tuning, we need to consider the smaller FMs of few billion parameters at most, namely on-device FMs (ODFMs), which can be deployed on-device. Federated fine-tuning of ODFMs has unique challenges non-present in standard fine-tuning: i) ODFMs poorly generalize to downstream tasks due to their limited sizes making proper fine-tuning imperative to their performance, and ii) devices have limited and heterogeneous system capabilities and data that can deter the performance of fine-tuning.Tackling these challenges, we propose HetLoRA, a feasible and effective federated fine-tuning method for ODFMs that leverages the system and data heterogeneity at the edge. HetLoRA allows heterogeneous LoRA ranks across clients for their individual system resources, and efficiently aggregates and distributes these LoRA modules in a data-aware manner by applying rank self-pruning locally and sparsity-weighted aggregation at the server. It combines the advantages of high and low-rank LoRAs, achieving improved convergence speed and final performance compared to homogeneous LoRA. Furthermore, HetLoRA has enhanced computation and communication efficiency compared to full fine-tuning making it more feasible for the edge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.718.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.718.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--718 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.718 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.718/>Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training</a></strong><br><a href=/people/y/yixuan-wang/>Yixuan Wang</a>
|
<a href=/people/x/xianzhen-luo/>Xianzhen Luo</a>
|
<a href=/people/f/fuxuan-wei/>Fuxuan Wei</a>
|
<a href=/people/y/yijun-liu/>Yijun Liu</a>
|
<a href=/people/q/qingfu-zhu/>Qingfu Zhu</a>
|
<a href=/people/x/xuanyu-zhang/>Xuanyu Zhang</a>
|
<a href=/people/q/qing-yang/>Qing Yang</a>
|
<a href=/people/d/dongliang-xu/>Dongliang Xu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--718><div class="card-body p-3 small">Existing speculative decoding methods typically require additional model structure and training processes to assist the model for draft token generation. This makes the migration of acceleration methods to the new model more costly and more demanding on device memory. To address this problem, we propose the Make Some Noise (MSN) training framework as a replacement for the supervised fine-tuning stage of the large language model. The training method simply introduces some noise at the input for the model to learn the denoising task. It significantly enhances the parallel decoding capability of the model without affecting the original task capability. In addition, we propose a tree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further improve the inference speed of MSN models. Experiments in both the general and code domains have shown that MSN can improve inference speed by 2.3-2.7x times without compromising model performance. The MSN model also achieves comparable acceleration ratios to the SOTA model with additional model structure on Spec-Bench.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.719.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.719.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--719 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.719 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.719/>Target-Aware Language Modeling via Granular Data Sampling</a></strong><br><a href=/people/e/ernie-chang/>Ernie Chang</a>
|
<a href=/people/p/pin-jie-lin/>Pin-Jie Lin</a>
|
<a href=/people/y/yang-li/>Yang Li</a>
|
<a href=/people/c/changsheng-zhao/>Changsheng Zhao</a>
|
<a href=/people/d/daeil-kim/>Daeil Kim</a>
|
<a href=/people/r/rastislav-rabatin/>Rastislav Rabatin</a>
|
<a href=/people/z/zechun-liu/>Zechun Liu</a>
|
<a href=/people/y/yangyang-shi/>Yangyang Shi</a>
|
<a href=/people/v/vikas-chandra/>Vikas Chandra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--719><div class="card-body p-3 small">Language model pretraining generally targets a broad range of use cases and incorporates data from diverse sources. However, there are instances where we desire a model that excels in specific areas without markedly compromising performance in other areas. A cost-effective and straightforward approach is sampling with low-dimensional data features, which allows selecting large-scale pretraining data for domain-specific use cases. In this work, we revisit importance sampling with n-gram features consisting of multi-granular tokens, which strikes a good balance between sentence compression and representation capabilities. We observed the sampled data to have a high correlation with the target downstream task performance *while preserving its effectiveness on other tasks*. This leads to the proposed data sampling paradigm where language models can be pretrained more efficiently on selected documents. On eight benchmarks we demonstrate with ~1% of the data, pretrained models perform on par with the full RefinedWeb data and outperform randomly selected samples for model sizes ranging from 125M to 1.5B.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.720.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.720.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--720 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.720 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.720/><span class=acl-fixed-case>SPEED</span>++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness</a></strong><br><a href=/people/t/tanmay-parekh/>Tanmay Parekh</a>
|
<a href=/people/j/jeffrey-kwan/>Jeffrey Kwan</a>
|
<a href=/people/j/jiarui-yu/>Jiarui Yu</a>
|
<a href=/people/s/sparsh-johri/>Sparsh Johri</a>
|
<a href=/people/h/hyosang-ahn/>Hyosang Ahn</a>
|
<a href=/people/s/sreya-muppalla/>Sreya Muppalla</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--720><div class="card-body p-3 small">Social media is often the first place where communities discuss the latest societal trends. Prior works have utilized this platform to extract epidemic-related information (e.g. infections, preventive measures) to provide early warnings for epidemic prediction. However, these works only focused on English posts, while epidemics can occur anywhere in the world, and early discussions are often in the local, non-English languages. In this work, we introduce the first multilingual Event Extraction (EE) framework SPEED++ for extracting epidemic event information for any disease and language. To this end, we extend a previous epidemic ontology with 20 argument roles; and curate our multilingual EE dataset SPEED++ comprising 5.1K tweets in four languages for four diseases. Annotating data in every language is infeasible; thus we develop zero-shot cross-lingual cross-disease models (i.e., training only on English COVID data) utilizing multilingual pre-training and show their efficacy in extracting epidemic-related events for 65 diverse languages across different diseases. Experiments demonstrate that our framework can provide epidemic warnings for COVID-19 in its earliest stages in Dec 2019 (3 weeks before global discussions) from Chinese Weibo posts without any training in Chinese. Furthermore, we exploit our framework’s argument extraction capabilities to aggregate community epidemic discussions like symptoms and cure measures, aiding misinformation detection and public attention monitoring. Overall, we lay a strong foundation for multilingual epidemic preparedness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.721.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.721.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--721 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.721 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.721/><span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>G</span>en: Learning from Feedback with Coupled Comprehension and Generation</a></strong><br><a href=/people/m/mustafa-omer-gul/>Mustafa Omer Gul</a>
|
<a href=/people/y/yoav-artzi/>Yoav Artzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--721><div class="card-body p-3 small">Systems with both language comprehension and generation capabilities can benefit from the tight connection between the two. This work studies coupling comprehension and generation with focus on continually learning from interaction with users. We propose techniques to tightly integrate the two capabilities for both learning and inference. We situate our studies in two-player reference games, and deploy various models for thousands of interactions with human users, while learning from interaction feedback signals. We show dramatic improvements in performance over time, with comprehension-generation coupling leading to performance improvements up to 26% in absolute terms and up to 17% higher accuracies compared to a non-coupled system. Our analysis also shows coupling has substantial qualitative impact on the system’s language, making it significantly more human-like.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.722.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.722.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--722 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.722 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.722/><span class=acl-fixed-case>UNICORN</span>: A Unified Causal Video-Oriented Language-Modeling Framework for Temporal Video-Language Tasks</a></strong><br><a href=/people/y/yuanhao-xiong/>Yuanhao Xiong</a>
|
<a href=/people/y/yixin-nie/>Yixin Nie</a>
|
<a href=/people/h/haotian-liu/>Haotian Liu</a>
|
<a href=/people/b/boxin-wang/>Boxin Wang</a>
|
<a href=/people/j/jun-chen/>Jun Chen</a>
|
<a href=/people/r/rong-jin/>Rong Jin</a>
|
<a href=/people/c/cho-jui-hsieh/>Cho-Jui Hsieh</a>
|
<a href=/people/l/lorenzo-torresani/>Lorenzo Torresani</a>
|
<a href=/people/j/jie-lei/>Jie Lei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--722><div class="card-body p-3 small">The great success of large language models has encouraged the development of large multimodal models, with a focus on image-language interaction. Despite promising results in various image-language downstream tasks, it is still challenging and unclear how to extend the capabilities of these models to the more complex video domain, especially when dealing with explicit temporal signals. To address the problem in existing large multimodal models, in this paper we adopt visual instruction tuning to build a unified causal video-oriented language modeling framework, named UNICORN. Specifically, we collect a comprehensive dataset under the instruction-following format, and instruction-tune the model accordingly. Experimental results demonstrate that without customized training objectives and intensive pre-training, UNICORN can achieve comparable or better performance on established temporal video-language tasks including moment retrieval, video paragraph captioning and dense video captioning. Moreover, the instruction-tuned model can be used to automatically annotate internet videos with temporally-aligned captions. Compared to commonly used ASR captions, we show that training on our generated captions improves the performance of video-language models on both zero-shot and fine-tuning settings. Source code can be found at https://github.com/xyh97/UNICORN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.723.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.723.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--723 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.723 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.723/>Story Morals: Surfacing value-driven narrative schemas using large language models</a></strong><br><a href=/people/d/david-g-hobson/>David G Hobson</a>
|
<a href=/people/h/haiqi-zhou/>Haiqi Zhou</a>
|
<a href=/people/d/derek-ruths/>Derek Ruths</a>
|
<a href=/people/a/andrew-piper/>Andrew Piper</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--723><div class="card-body p-3 small">Stories are not only designed to entertain but encode lessons reflecting their authors’ beliefs about the world. In this paper, we propose a new task of narrative schema labelling based on the concept of “story morals” to identify the values and lessons conveyed in stories. Using large language models (LLMs) such as GPT-4, we develop methods to automatically extract and validate story morals across a diverse set of narrative genres, including folktales, novels, movies and TV, personal stories from social media and the news. Our approach involves a multi-step prompting sequence to derive morals and validate them through both automated metrics and human assessments. The findings suggest that LLMs can effectively approximate human story moral interpretations and offer a new avenue for computational narrative understanding. By clustering the extracted morals on a sample dataset of folktales from around the world, we highlight the commonalities and distinctiveness of narrative values, providing preliminary insights into the distribution of values across cultures. This work opens up new possibilities for studying narrative schemas and their role in shaping human beliefs and behaviors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.724.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.724.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--724 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.724 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.724.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.724/><span class=acl-fixed-case>OATH</span>-Frames: Characterizing Online Attitudes Towards Homelessness with <span class=acl-fixed-case>LLM</span> Assistants</a></strong><br><a href=/people/j/jaspreet-ranjit/>Jaspreet Ranjit</a>
|
<a href=/people/b/brihi-joshi/>Brihi Joshi</a>
|
<a href=/people/r/rebecca-dorn/>Rebecca Dorn</a>
|
<a href=/people/l/laura-petry/>Laura Petry</a>
|
<a href=/people/o/olga-koumoundouros/>Olga Koumoundouros</a>
|
<a href=/people/j/jayne-bottarini/>Jayne Bottarini</a>
|
<a href=/people/p/peichen-liu/>Peichen Liu</a>
|
<a href=/people/e/eric-rice/>Eric Rice</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--724><div class="card-body p-3 small">Warning: Contents of this paper may be upsetting.Public attitudes towards key societal issues, expressed on online media, are of immense value in policy and reform efforts, yet challenging to understand at scale. We study one such social issue: homelessness in the U.S., by leveraging the remarkable capabilities of large language models to assist social work experts in analyzing millions of posts from Twitter. We introduce a framing typology: Online Attitudes Towards Homelessness (OATH) Frames: nine hierarchical frames capturing critiques, responses and perceptions. We release annotations with varying degrees of assistance from language models, with immense benefits in scaling: 6.5× speedup in annotation time while only incurring a 3 point F1 reduction in performance with respect to the domain experts. Our experiments demonstrate the value of modeling OATH-Frames over existing sentiment and toxicity classifiers. Our large-scale analysis with predicted OATH-Frames on 2.4M posts on homelessness reveal key trends in attitudes across states, time periods and vulnerable populations, enabling new insights on the issue. Our work provides a general framework to understand nuanced public attitudes at scale, on issues beyond homelessness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.725.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.725.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--725 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.725 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.725/><span class=acl-fixed-case>A</span>nalo<span class=acl-fixed-case>B</span>ench: Benchmarking the Identification of Abstract and Long-context Analogies</a></strong><br><a href=/people/x/xiao-ye/>Xiao Ye</a>
|
<a href=/people/a/andrew-wang/>Andrew Wang</a>
|
<a href=/people/j/jacob-choi/>Jacob Choi</a>
|
<a href=/people/y/yining-lu/>Yining Lu</a>
|
<a href=/people/s/shreya-sharma/>Shreya Sharma</a>
|
<a href=/people/l/lingfeng-shen/>Lingfeng Shen</a>
|
<a href=/people/v/vijay-murari-tiyyala/>Vijay Murari Tiyyala</a>
|
<a href=/people/n/nicholas-andrews/>Nicholas Andrews</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--725><div class="card-body p-3 small">Humans regularly engage in analogical thinking, relating personal experiences to current situations (X is analogous to Y because of Z). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose AnaloBench, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We collect a set of 340 high quality, human written analogies for use in our benchmark, which constitutes the largest such collection to date. We then test a broad collection of models consisting of 12 open source and 3 proprietary in various sizes and architectures. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.726.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.726.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--726 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.726 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.726/><span class=acl-fixed-case>S</span>ci<span class=acl-fixed-case>ER</span>: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents</a></strong><br><a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/z/zhijia-chen/>Zhijia Chen</a>
|
<a href=/people/h/huitong-pan/>Huitong Pan</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a>
|
<a href=/people/l/longin-jan-latecki/>Longin Jan Latecki</a>
|
<a href=/people/e/eduard-dragut/>Eduard Dragut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--726><div class="card-body p-3 small">Scientific information extraction (SciIE) is critical for converting unstructured knowledge from scholarly articles into structured data (entities and relations). Several datasets have been proposed for training and validating SciIE models. However, due to the high complexity and cost of annotating scientific texts, those datasets restrict their annotations to specific parts of paper, such as abstracts, resulting in the loss of diverse entity mentions and relations in context. In this paper, we release a new entity and relation extraction dataset for entities related to datasets, methods, and tasks in scientific articles. Our dataset contains 106 manually annotated full-text scientific publications with over 24k entities and 12k relations. To capture the intricate use and interactions among entities in full texts, our dataset contains a fine-grained tag set for relations. Additionally, we provide an out-of-distribution test set to offer a more realistic evaluation. We conduct comprehensive experiments, including state-of-the-art supervised models and our proposed LLM-based baselines, and highlight the challenges presented by our dataset, encouraging the development of innovative models to further the field of SciIE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.727.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.727.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--727 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.727 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.727/>Analysis of Plan-based Retrieval for Grounded Text Generation</a></strong><br><a href=/people/a/ameya-godbole/>Ameya Godbole</a>
|
<a href=/people/n/nicholas-monath/>Nicholas Monath</a>
|
<a href=/people/s/seungyeon-kim/>Seungyeon Kim</a>
|
<a href=/people/a/ankit-singh-rawat/>Ankit Singh Rawat</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--727><div class="card-body p-3 small">In text generation, hallucinations refer to the generation of seemingly coherent text that contradicts established knowledge. One compelling hypothesis is that hallucinations occur when a language model is given a generation task outside its parametric knowledge (due to rarity, recency, domain, etc.). A common strategy to address this limitation is to infuse the language models with retrieval mechanisms, providing the model with relevant knowledge for the task. In this paper, we leverage the planning capabilities of instruction-tuned LLMs and analyze how planning can be used to guide retrieval to further reduce the frequency of hallucinations. We empirically evaluate several variations of our proposed approach on long-form text generation tasks. By improving the coverage of relevant facts, plan-guided retrieval and generation can produce more informative responses while providing a higher rate of attribution to source documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.728.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--728 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.728/>Detecting Errors through Ensembling Prompts (<span class=acl-fixed-case>DEEP</span>): An End-to-End <span class=acl-fixed-case>LLM</span> Framework for Detecting Factual Errors</a></strong><br><a href=/people/a/alex-chandler/>Alex Chandler</a>
|
<a href=/people/d/devesh-surve/>Devesh Surve</a>
|
<a href=/people/h/hui-su/>Hui Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--728><div class="card-body p-3 small">Accurate text summarization is one of the most common and important tasks performed by Large Language Models, where the costs of human review for an entire document may be high, but the costs of errors in summarization may be even greater. We propose Detecting Errors through Ensembling Prompts (DEEP) - an end-to-end large language model framework for detecting factual errors in text summarization. Our framework uses a diverse set of LLM prompts to identify factual inconsistencies, treating their outputs as binary features, which are then fed into ensembling models. We then calibrate the ensembled models to produce empirically accurate probabilities that a text is factually consistent or free of hallucination. We demonstrate that prior models for detecting factual errors in summaries perform significantly worse without optimizing the thresholds on subsets of the evaluated dataset. Our framework achieves state-of-the-art (SOTA) balanced accuracy on the AggreFact-XSUM FTSOTA, TofuEval Summary-Level, and HaluEval Summarization benchmarks in detecting factual errors within transformer-generated text summaries. It does so without any fine-tuning of the language model or reliance on thresholding techniques not available in practical settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.729.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.729.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--729 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.729 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.729/><span class=acl-fixed-case>RLHF</span> Can Speak Many Languages: Unlocking Multilingual Preference Optimization for <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/j/john-dang/>John Dang</a>
|
<a href=/people/a/arash-ahmadian/>Arash Ahmadian</a>
|
<a href=/people/k/kelly-marchisio/>Kelly Marchisio</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/a/ahmet-ustun/>Ahmet Üstün</a>
|
<a href=/people/s/sara-hooker/>Sara Hooker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--729><div class="card-body p-3 small">Preference optimization techniques have become a standard final stage for training state-of-art large language models (LLMs). However, despite widespread adoption, the vast majority of work to-date has focused on a small set of high-resource languages like English and Chinese. This captures a small fraction of the languages in the world, but also makes it unclear which aspects of current state-of-the-art research transfer to a multilingual setting. In this work, we perform an exhaustive study to achieve a new state of the art in aligning multilingual LLMs. We introduce a novel, scalable method for generating high-quality multilingual feedback data to balance data coverage. We establish the benefits of cross-lingual transfer and increased dataset size in preference training. Our preference-trained model achieves a 54.4% win-rate against Aya 23 8B, the current state-of-the-art multilingual LLM in its parameter class, and a 69.5% win-rate or higher against widely used models like Gemma, Mistral and Llama 3. As a result of our efforts, we expand the frontier of alignment techniques to 23 languages, covering approximately half of the world’s population.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.730.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.730.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--730 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.730 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.730.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.730.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.730/>Boosting Logical Fallacy Reasoning in <span class=acl-fixed-case>LLM</span>s via Logical Structure Tree</a></strong><br><a href=/people/y/yuanyuan-lei/>Yuanyuan Lei</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--730><div class="card-body p-3 small">Logical fallacy uses invalid or faulty reasoning in the construction of a statement. Despite the prevalence and harmfulness of logical fallacies, detecting and classifying logical fallacies still remains a challenging task. We observe that logical fallacies often use connective words to indicate an intended logical relation between two arguments, while the argument semantics does not actually support the logical relation. Inspired by this observation, we propose to build a logical structure tree to explicitly represent and track the hierarchical logic flow among relation connectives and their arguments in a statement. Specifically, this logical structure tree is constructed in an unsupervised manner guided by the constituency tree and a taxonomy of connectives for ten common logical relations, with relation connectives as non-terminal nodes and textual arguments as terminal nodes, and the latter are mostly elementary discourse units. We further develop two strategies to incorporate the logical structure tree into LLMs for fallacy reasoning. Firstly, we transform the tree into natural language descriptions and feed the textualized tree into LLMs as a part of the hard text prompt. Secondly, we derive a relation-aware tree embedding and insert the tree embedding into LLMs as a soft prompt. Experiments on benchmark datasets demonstrate that our approach based on logical structure tree significantly improves precision and recall for both fallacy detection and fallacy classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.731.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.731.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--731 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.731 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.731.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.731/>Chain and Causal Attention for Efficient Entity Tracking</a></strong><br><a href=/people/e/erwan-fagnou/>Erwan Fagnou</a>
|
<a href=/people/p/paul-caillon/>Paul Caillon</a>
|
<a href=/people/b/blaise-delattre/>Blaise Delattre</a>
|
<a href=/people/a/alexandre-allauzen/>Alexandre Allauzen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--731><div class="card-body p-3 small">This paper investigates the limitations of transformers for entity-tracking tasks in large language models. We identify a theoretical constraint, showing that transformers require at least <span class=tex-math><span class=tex-math-function>log</span><sub>2</sub> (n+1)</span> layers to handle entity tracking with <span class=tex-math>n</span> state changes. To address this issue, we propose an efficient and frugal enhancement to the standard attention mechanism, enabling it to manage long-term dependencies more efficiently. By considering attention as an adjacency matrix, our model can track entity states with a single layer.Empirical results demonstrate significant improvements in entity tracking datasets while keeping competitive performance on standard natural language modeling. Our modified attention allows us to achieve the same performance with drastically fewer layers. Additionally, our enhanced mechanism reveals structured internal representations of attention. Extensive experiments on both toy and complex datasets validate our approach. Our contributions include theoretical insights, an improved attention mechanism, and empirical validation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.732.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.732.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--732 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.732 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.732/><span class=acl-fixed-case>BEEAR</span>: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models</a></strong><br><a href=/people/y/yi-zeng/>Yi Zeng</a>
|
<a href=/people/w/weiyu-sun/>Weiyu Sun</a>
|
<a href=/people/t/tran-huynh/>Tran Huynh</a>
|
<a href=/people/d/dawn-song/>Dawn Song</a>
|
<a href=/people/b/bo-li/>Bo Li</a>
|
<a href=/people/r/ruoxi-jia/>Ruoxi Jia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--732><div class="card-body p-3 small">Safety backdoor attacks in large language models (LLMs) enable harmful behaviors to be stealthily triggered while evading detection during normal interactions. The high dimensionality of the trigger search space and the diverse range of potential malicious behaviors in LLMs make this a critical open problem. This paper presents BEEAR, a novel mitigation method based on a key insight: backdoor triggers induce a uniform drift in the model’s embedding space, irrespective of the trigger’s form or targeted behavior. Leveraging this observation, we introduce a bi-level optimization approach. The inner level identifies universal perturbations to the decoder’s embeddings that steer the model towards defender-defined unwanted behaviors; the outer level fine-tunes the model to reinforce safe behaviors against these perturbations. Our experiments demonstrate the effectiveness of this approach, reducing the success rate of safety backdoor attacks from over 95% to &lt;1% for general harmful behaviors and from 47% to 0% for Sleeper Agents, without compromising the model’s helpfulness. Notably, our method relies only on defender-defined sets of safe and unwanted behaviors without any assumptions about the trigger location or attack mechanism. This work represents the first practical framework to counter safety backdoors in LLMs and provides a foundation for future advancements in AI safety and security.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.733.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.733.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--733 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.733 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.733.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.733/>A <span class=acl-fixed-case>B</span>ayesian Approach to Harnessing the Power of <span class=acl-fixed-case>LLM</span>s in Authorship Attribution</a></strong><br><a href=/people/z/zhengmian-hu/>Zhengmian Hu</a>
|
<a href=/people/t/tong-zheng/>Tong Zheng</a>
|
<a href=/people/h/heng-huang/>Heng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--733><div class="card-body p-3 small">Authorship attribution aims to identify the origin or author of a document. Traditional approaches have heavily relied on manual features and fail to capture long-range correlations, limiting their effectiveness. Recent advancements leverage text embeddings from pre-trained language models, which require significant fine-tuning on labeled data, posing challenges in data dependency and limited interpretability. Large Language Models (LLMs), with their deep reasoning capabilities and ability to maintain long-range textual associations, offer a promising alternative. This study explores the potential of pre-trained LLMs in one-shot authorship attribution, specifically utilizing Bayesian approaches and probability outputs of LLMs. Our methodology calculates the probability that a text entails previous writings of an author, reflecting a more nuanced understanding of authorship. By utilizing only pre-trained models such as Llama-3-70B, our results on the IMDb and blog datasets show an impressive 85% accuracy in one-shot authorship classification across ten authors. Our findings set new baselines for one-shot authorship analysis using LLMs and expand the application scope of these models in forensic linguistics. This work also includes extensive ablation studies to validate our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.734.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.734.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--734 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.734 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.734/><span class=acl-fixed-case>FAC</span><span class=tex-math><sup>2</sup></span><span class=acl-fixed-case>E</span>: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</a></strong><br><a href=/people/x/xiaoqiang-wang/>Xiaoqiang Wang</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/t/tengfei-ma/>Tengfei Ma</a>
|
<a href=/people/b/bang-liu/>Bang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--734><div class="card-body p-3 small">Large language models (LLMs) are primarily evaluated by overall performance on various text understanding and generation tasks. However, such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs’ capabilities. In this paper, we present FAC<span class=tex-math><sup>2</sup></span>E, a framework for Fine-grAined and Cognition-grounded LLMs’ Capability Evaluation. Specifically, we formulate LLMs’ evaluation in a multi-dimensional and explainable manner by dissociating the language-related capabilities and the cognition-related ones. Besides, through extracting the intermediate reasoning from LLMs, we further break down the process of applying a specific capability into three sub-steps: recalling relevant knowledge, utilizing knowledge, and solving problems. Finally, FAC<span class=tex-math><sup>2</sup></span>E evaluates each sub-step of each fine-grained capability, providing a two-faceted diagnosis for LLMs. Utilizing FAC<span class=tex-math><sup>2</sup></span>E, we identify a common shortfall in knowledge utilization among models and propose a straightforward, knowledge-enhanced method to mitigate this issue. Our results not only showcase promising performance enhancements but also highlight a direction for future LLM advancements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.735.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.735.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--735 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.735 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.735.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.735/><span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>S</span>ep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation</a></strong><br><a href=/people/t/tanvir-mahmud/>Tanvir Mahmud</a>
|
<a href=/people/d/diana-marculescu/>Diana Marculescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--735><div class="card-body p-3 small">Audio separation in real-world scenarios, where mixtures contain a variable number of sources, presents significant challenges due to limitations of existing models, such as over-separation, under-separation, and dependence on predefined training sources. We propose OpenSep, a novel framework that leverages large language models (LLMs) for automated audio separation, eliminating the need for manual intervention and overcoming source limitations. OpenSep uses textual inversion to generate captions from audio mixtures with off-the-shelf audio captioning models, effectively parsing the sound sources present. It then employs few-shot LLM prompting to extract detailed audio properties of each parsed source, facilitating separation in unseen mixtures. Additionally, we introduce a multi-level extension of the mix-and-separate training framework to enhance modality alignment by separating single source sounds and mixtures simultaneously. Extensive experiments demonstrate OpenSep’s superiority in precisely separating new, unseen, and variable sources in challenging mixtures, outperforming SOTA baseline methods. Code is released at https://github.com/tanvir-utexas/OpenSep.git.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.736.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.736.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--736 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.736 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.736/>Language Concept Erasure for Language-invariant Dense Retrieval</a></strong><br><a href=/people/z/zhiqi-huang/>Zhiqi Huang</a>
|
<a href=/people/p/puxuan-yu/>Puxuan Yu</a>
|
<a href=/people/s/shauli-ravfogel/>Shauli Ravfogel</a>
|
<a href=/people/j/james-allan/>James Allan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--736><div class="card-body p-3 small">Multilingual models aim for language-invariant representations but still prominently encode language identity. This, along with the scarcity of high-quality parallel retrieval data, limits their performance in retrieval. We introduce LANCER, a multi-task learning framework that improves language-invariant dense retrieval by reducing language-specific signals in the embedding space. Leveraging the notion of linear concept erasure, we design a loss function that penalizes cross-correlation between representations and their language labels. LANCER leverages only English retrieval data and general multilingual corpora, training models to focus on language-invariant retrieval by semantic similarity without necessitating a vast parallel corpus. Experimental results on various datasets show our method consistently improves over baselines, with extensive analyses demonstrating greater language agnosticism.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.737.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.737.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--737 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.737 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.737/>Learning Personalized Alignment for Evaluating Open-ended Text Generation</a></strong><br><a href=/people/d/danqing-wang/>Danqing Wang</a>
|
<a href=/people/k/kevin-yang/>Kevin Yang</a>
|
<a href=/people/h/hanlin-zhu/>Hanlin Zhu</a>
|
<a href=/people/x/xiaomeng-yang/>Xiaomeng Yang</a>
|
<a href=/people/a/andrew-cohen/>Andrew Cohen</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/y/yuandong-tian/>Yuandong Tian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--737><div class="card-body p-3 small">Recent research has increasingly focused on evaluating large language models’ (LLMs) alignment with diverse human values and preferences, particularly for open-ended tasks like story generation. Traditional evaluation metrics rely heavily on lexical similarity with human-written references, often showing poor correlation with human judgments and failing to account for alignment with the diversity of human preferences. To address these challenges, we introduce PerSE, an interpretable evaluation framework designed to assess alignment with specific human preferences. It is tuned to infer specific preferences from an in-context personal profile and evaluate the alignment between the generated content and personal preferences. PerSE enhances interpretability by providing detailed comments and fine-grained scoring, facilitating more personalized content generation. Our 13B LLaMA-2-based PerSE shows a 15.8% increase in Kendall correlation and a 13.7% rise in accuracy with zero-shot reviewers compared to GPT-4. It also outperforms GPT-4 by 46.01% in Kendall correlation on new domains, indicating its transferability</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.738.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.738.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--738 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.738 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.738/>Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks</a></strong><br><a href=/people/y/yue-zhou/>Yue Zhou</a>
|
<a href=/people/h/henry-peng-zou/>Henry Peng Zou</a>
|
<a href=/people/b/barbara-di-eugenio/>Barbara Di Eugenio</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--738><div class="card-body p-3 small">We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.739.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.739.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--739 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.739 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.739/>Turn Waste into Worth: Rectifying Top-<span class=tex-math>k</span> Router of <span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>E</span></a></strong><br><a href=/people/z/zhiyuan-zeng/>Zhiyuan Zeng</a>
|
<a href=/people/q/qipeng-guo/>Qipeng Guo</a>
|
<a href=/people/z/zhaoye-fei/>Zhaoye Fei</a>
|
<a href=/people/z/zhangyue-yin/>Zhangyue Yin</a>
|
<a href=/people/y/yunhua-zhou/>Yunhua Zhou</a>
|
<a href=/people/l/linyang-li/>Linyang Li</a>
|
<a href=/people/t/tianxiang-sun/>Tianxiang Sun</a>
|
<a href=/people/h/hang-yan/>Hang Yan</a>
|
<a href=/people/d/dahua-lin/>Dahua Lin</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--739><div class="card-body p-3 small">Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-<span class=tex-math>k</span> routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are empty, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectification effectively handle dropped tokens and padding, respectively. Furthermore, the combination of them achieves superior performance, surpassing the accuracy of the vanilla top-1 router by 4.7%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.740.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.740.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--740 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.740 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.740.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.740.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.740/>Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination</a></strong><br><a href=/people/p/pittawat-taveekitworachai/>Pittawat Taveekitworachai</a>
|
<a href=/people/f/febri-abdullah/>Febri Abdullah</a>
|
<a href=/people/r/ruck-thawonmas/>Ruck Thawonmas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--740><div class="card-body p-3 small">This paper presents a series of investigations into an interesting phenomenon where we observe performance increases in large language models (LLMs) when providing a prompt that causes and exploits hallucination. We propose null-shot prompting, a counter-intuitive approach where we intentionally instruct LLMs to look at and utilize information from a null section. We investigate null-shot prompting on a wide range of tasks, including arithmetic reasoning, commonsense reasoning, and reading comprehension. We observe a substantial increase in performance in arithmetic reasoning tasks for various models, with up to a 44.62% increase compared to a baseline in one model. Therefore, we investigate deeper into this task by utilizing a more challenging mathematics problem-solving benchmark. We observe that LLMs benefit from hallucination in null-shot prompting in this task and discuss the mathematical topics that benefit the most from introducing hallucination in the prompt. We continue our investigation by evaluating hallucination detection abilities of the LLMs when using null-shot prompting. We find surprising results where hallucination in prompts can improve hallucination detection abilities of many LLMs. We also examine the effects of introducing both reasoning, which is known to mitigate hallucination, and hallucination simultaneously in the prompt and observe another surprising turn for the mathematics problem-solving benchmark with many performance improvements. We hope this paper will spark more interest, investigations, and discussions on how hallucination in prompts LLMs and even bolsters them in certain cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.741.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.741.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--741 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.741 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.741/><span class=acl-fixed-case>C</span>omm<span class=acl-fixed-case>VQA</span>: Situating Visual Question Answering in Communicative Contexts</a></strong><br><a href=/people/n/nandita-shankar-naik/>Nandita Shankar Naik</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/e/elisa-kreiss/>Elisa Kreiss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--741><div class="card-body p-3 small">Current visual question answering (VQA) models tend to be trained and evaluated on image-question pairs in isolation. However, the questions people ask are dependent on their informational needs and prior knowledge about the image content. To evaluate how situating images within naturalistic contexts shapes visual questions, we introduce CommVQA, a VQA dataset consisting of images, image descriptions, real-world communicative scenarios where the image might appear (e.g., a travel website), and follow-up questions and answers conditioned on the scenario and description. CommVQA, which contains 1000 images and 8,949 question-answer pairs, poses a challenge for current models. Error analyses and a human-subjects study suggest that generated answers still contain high rates of hallucinations, fail to fittingly address unanswerable questions, and don’t suitably reflect contextual information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.742.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.742.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--742 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.742 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.742.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.742/>Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding</a></strong><br><a href=/people/w/weilin-zhao/>Weilin Zhao</a>
|
<a href=/people/y/yuxiang-huang/>Yuxiang Huang</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/w/wang-xu/>Wang Xu</a>
|
<a href=/people/c/chaojun-xiao/>Chaojun Xiao</a>
|
<a href=/people/x/xinrong-zhang/>Xinrong Zhang</a>
|
<a href=/people/y/yewei-fang/>Yewei Fang</a>
|
<a href=/people/k/kaihuo-zhang/>Kaihuo Zhang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--742><div class="card-body p-3 small">Speculative decoding is a widely used method that accelerates the generation process of large language models (LLMs) with no compromise in model performance. It achieves this goal by using an existing smaller model for drafting and then employing the target LLM to verify the draft in a low-cost parallel manner. Under such a drafting-verification framework, drafting efficiency has become a bottleneck in the final speedup of speculative decoding. Therefore, generating longer drafts at less cost can lead to better decoding speedup. To achieve this, we introduce Ouroboros, which can generate draft phrases to parallelize the drafting process and meanwhile lengthen drafts in a training-free manner. The experimental results on various typical text generation tasks show that Ouroboros can achieve speedups of up to <span class=tex-math>2.4×</span> over speculative decoding and <span class=tex-math>3.9×</span> over vanilla decoding, without fine-tuning draft and target models. Code available at https://github.com/thunlp/Ouroboros.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.743.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.743.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--743 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.743 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.743.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.743.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.743/>1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?</a></strong><br><a href=/people/y/yue-huang/>Yue Huang</a>
|
<a href=/people/c/chenrui-fan/>Chenrui Fan</a>
|
<a href=/people/y/yuan-li/>Yuan Li</a>
|
<a href=/people/s/siyuan-wu/>Siyuan Wu</a>
|
<a href=/people/t/tianyi-zhou/>Tianyi Zhou</a>
|
<a href=/people/x/xiangliang-zhang/>Xiangliang Zhang</a>
|
<a href=/people/l/lichao-sun/>Lichao Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--743><div class="card-body p-3 small">Large Language Models (LLMs) have garnered significant attention due to their remarkable ability to process information across various languages. Despite their capabilities, they exhibit inconsistencies in handling identical queries in different languages, presenting challenges for further advancement. This paper introduces a method to enhance the multilingual performance of LLMs by aggregating knowledge from diverse languages. This approach incorporates a low-resource knowledge detector specific to a language, a strategic language selection process, and mechanisms for answer replacement and integration. Our extensive experiments demonstrate notable performance improvements, particularly in reducing the performance disparity across languages. An ablation study confirms that each component of our method significantly contributes to these enhancements. This research highlights the inherent potential of LLMs to harmonize multilingual capabilities and offers valuable insights for further exploration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.744.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.744.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--744 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.744 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.744/>How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective</a></strong><br><a href=/people/t/teng-xiao/>Teng Xiao</a>
|
<a href=/people/m/mingxiao-li/>Mingxiao Li</a>
|
<a href=/people/y/yige-yuan/>Yige Yuan</a>
|
<a href=/people/h/huaisheng-zhu/>Huaisheng Zhu</a>
|
<a href=/people/c/chao-cui/>Chao Cui</a>
|
<a href=/people/v/vasant-g-honavar/>Vasant G Honavar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--744><div class="card-body p-3 small">This paper introduces a novel generalized self-imitation learning GSIL framework, which effectively and efficiently aligns large language models with offline demonstration data. We develop GSIL by deriving a surrogate objective of imitation learning with density ratio estimates, facilitating the use of self-generated data and optimizing the imitation learning objective with simple classification losses. GSIL eliminates the need for complex adversarial training in standard imitation learning, achieving lightweight and efficient fine-tuning for large language models. In addition, GSIL encompasses a family of offline losses parameterized by a general class of convex functions for density ratio estimation and enables a unified view for alignment with demonstration data. Extensive experiments show that GSIL consistently and significantly outperforms baselines in many challenging benchmarks, such as coding (HuamnEval), mathematical reasoning (GSM8K) and instruction-following benchmark (MT-Bench). Code is public available at https://github.com/tengxiao1/GSIL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.745.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.745.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--745 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.745 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.745/>Style-Specific Neurons for Steering <span class=acl-fixed-case>LLM</span>s in Text Style Transfer</a></strong><br><a href=/people/w/wen-lai/>Wen Lai</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--745><div class="card-body p-3 small">Text style transfer (TST) aims to modify the style of a text without altering its original meaning. Large language models (LLMs) demonstrate superior performance across multiple tasks, including TST. However, in zero-shot setups, they tend to directly copy a significant portion of the input text to the output without effectively changing its style. To enhance the stylistic variety and fluency of the text, we present sNeuron-TST, a novel approach for steering LLMs using style-specific neurons in TST. Specifically, we identify neurons associated with the source and target styles and deactivate source-style-only neurons to give target-style words a higher probability, aiming to enhance the stylistic diversity of the generated text. However, we find that this deactivation negatively impacts the fluency of the generated text, which we address by proposing an improved contrastive decoding method that accounts for rapid token probability shifts across layers caused by deactivated source-style neurons. Empirical experiments demonstrate the effectiveness of the proposed method on six benchmarks, encompassing formality, toxicity, politics, politeness, authorship, and sentiment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.746.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.746.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--746 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.746 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.746/>Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers</a></strong><br><a href=/people/t/tianhua-zhang/>Tianhua Zhang</a>
|
<a href=/people/k/kun-li/>Kun Li</a>
|
<a href=/people/h/hongyin-luo/>Hongyin Luo</a>
|
<a href=/people/x/xixin-wu/>Xixin Wu</a>
|
<a href=/people/j/james-glass/>James R. Glass</a>
|
<a href=/people/h/helen-meng/>Helen M. Meng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--746><div class="card-body p-3 small">Query rewriting is a crucial technique for passage retrieval in open-domain conversational question answering (CQA). It decontexualizes conversational queries into self-contained questions suitable for off-the-shelf retrievers. Existing methods attempt to incorporate retriever’s preference during the training of rewriting models. However, these approaches typically rely on extensive annotations such as in-domain rewrites and/or relevant passage labels, limiting the models’ generalization and adaptation capabilities. In this paper, we introduce AdaQR (Adaptive Query Rewriting), a framework for training query rewriting models with limited rewrite annotations from seed datasets and completely no passage label. Our approach begins by fine-tuning compact large language models using only 10% of rewrite annotations from the seed dataset training split. The models are then utilized to self-sample rewrite candidates for each query instance, further eliminating the expense for human labeling or larger language model prompting often adopted in curating preference data. A novel approach is then proposed to assess retriever’s preference for these candidates with the probability of answers conditioned on the conversational query by marginalizing the Top-<span class=tex-math>K</span> passages. This serves as the reward for optimizing the rewriter further using Direct Preference Optimization (DPO), a process free of rewrite and retrieval annotations. Experimental results on four open-domain CQA datasets demonstrate that AdaQR not only enhances the in-domain capabilities of the rewriter with limited annotation requirement, but also adapts effectively to out-of-domain datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.747.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.747.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--747 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.747 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.747/>Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction</a></strong><br><a href=/people/s/sizhe-zhou/>Sizhe Zhou</a>
|
<a href=/people/y/yu-meng/>Yu Meng</a>
|
<a href=/people/b/bowen-jin/>Bowen Jin</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--747><div class="card-body p-3 small">Relation extraction (RE) aims to identify semantic relationships between entities within text. Despite considerable advancements, existing models predominantly require extensive annotated training data, which is both costly and labor-intensive to collect. Moreover, these models often struggle to adapt to new or unseen relations. Few-shot learning, aiming to lessen annotation demands, typically provides incomplete and biased supervision for target relations, leading to degraded and unstable performance. To accurately and explicitly describe relation semantics while minimizing annotation demands, we explore the definition only zero-shot RE setting where only relation definitions expressed in natural language are used to train a RE model. We introduce REPaL, comprising three stages: (1) We leverage large language models (LLMs) to generate initial seed instances from relation definitions and an unlabeled corpus. (2) We fine-tune a bidirectional Small Language Model (SLM) with initial seeds to learn relations for the target domain. (3) We expand pattern coverage and mitigate bias from initial seeds by integrating feedback from the SLM’s predictions on the unlabeled corpus and the synthesis history. To accomplish this, we leverage the multi-turn conversation ability of LLMs to generate new instances in follow-up dialogues, informed by both the feedback and synthesis history. Studies reveal that definition-oriented seed synthesis enhances pattern coverage whereas indiscriminately increasing seed quantity leads to performance saturation. Experiments on two datasets show REPaL significantly improved cost-effective zero-shot performance by large margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.748.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.748.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--748 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.748 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.748/><span class=acl-fixed-case>DA</span>-Code: Agent Data Science Code Generation Benchmark for Large Language Models</a></strong><br><a href=/people/y/yiming-huang/>Yiming Huang</a>
|
<a href=/people/j/jianwen-luo/>Jianwen Luo</a>
|
<a href=/people/y/yan-yu/>Yan Yu</a>
|
<a href=/people/y/yitong-zhang/>Yitong Zhang</a>
|
<a href=/people/f/fangyu-lei/>Fangyu Lei</a>
|
<a href=/people/y/yifan-wei/>Yifan Wei</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/x/xiao-liu/>Xiao Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--748><div class="card-body p-3 small">We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, including Python and SQL, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously designed the evaluation suite to ensure the accuracy and robustness of the evaluation. We developed the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at [link](https://github.com/yiyihum/dabench)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.749.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.749.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--749 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.749 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.749.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.749.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.749/>Leveraging Context-Aware Prompting for Commit Message Generation</a></strong><br><a href=/people/z/zhihua-jiang/>Zhihua Jiang</a>
|
<a href=/people/j/jianwei-chen/>Jianwei Chen</a>
|
<a href=/people/d/dongning-rao/>Dongning Rao</a>
|
<a href=/people/g/guanghui-ye/>Guanghui Ye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--749><div class="card-body p-3 small">Writing comprehensive commit messages is tedious yet important, because these messages describe changes of code, such as fixing bugs or adding new features. However, most existing methods focus on either only the changed lines or nearest context lines, without considering the effectiveness of selecting useful contexts. On the other hand, it is possible that introducing excessive contexts can lead to noise. To this end, we propose a code model COMMIT (Context-aware prOMpting based comMIt-message generaTion) in conjunction with a code dataset CODEC (COntext and metaData Enhanced Code dataset). Leveraging program slicing, CODEC consolidates code changes along with related contexts via property graph analysis. Further, utilizing CodeT5+ as the backbone model, we train COMMIT via context-aware prompt on CODEC. Experiments show that COMMIT can surpass all compared models including pre-trained language models for code (code-PLMs) such as CommitBART and large language models for code (code-LLMs) such as Code-LlaMa. Besides, we investigate several research questions (RQs), further verifying the effectiveness of our approach. We release the data and code at: https://github.com/Jnunlplab/COMMIT.git.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.750.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.750.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--750 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.750 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.750/>Linguistic Bias in <span class=acl-fixed-case>C</span>hat<span class=acl-fixed-case>GPT</span>: Language Models Reinforce Dialect Discrimination</a></strong><br><a href=/people/e/eve-fleisig/>Eve Fleisig</a>
|
<a href=/people/g/genevieve-smith/>Genevieve Smith</a>
|
<a href=/people/m/madeline-bossi/>Madeline Bossi</a>
|
<a href=/people/i/ishita-rustagi/>Ishita Rustagi</a>
|
<a href=/people/x/xavier-yin/>Xavier Yin</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--750><div class="card-body p-3 small">We present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-”standard” varieties from around the world). We prompted GPT-3.5 Turbo and GPT-4 with text by native speakers of each variety and analyzed the responses via detailed linguistic feature annotation and native speaker evaluation. We find that the models default to “standard” varieties of English; based on evaluation by native speakers, we also find that model responses to non-”standard” varieties consistently exhibit a range of issues: stereotyping (19% worse than for “standard” varieties), demeaning content (25% worse), lack of comprehension (9% worse), and condescending responses (15% worse). Moreover, if these models are asked to imitate the writing style of prompts in non-”standard” varieties, they produce text that exhibits lower comprehension of the input and is especially prone to stereotyping. GPT-4 improves on GPT-3.5 in terms of comprehension, warmth, and friendliness, but also exhibits a marked increase in stereotyping (+18%). The results indicate that GPT-3.5 Turbo and GPT-4 can perpetuate linguistic discrimination toward speakers of non-”standard” varieties.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.751.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.751.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--751 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.751 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.751.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.751.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.751/>Lifelong Knowledge Editing for <span class=acl-fixed-case>LLM</span>s with Retrieval-Augmented Continuous Prompt Learning</a></strong><br><a href=/people/q/qizhou-chen/>Qizhou Chen</a>
|
<a href=/people/t/taolin-zhang/>Taolin Zhang</a>
|
<a href=/people/x/xiaofeng-he/>Xiaofeng He</a>
|
<a href=/people/d/dongyang-li/>Dongyang Li</a>
|
<a href=/people/c/chengyu-wang/>Chengyu Wang</a>
|
<a href=/people/l/longtao-huang/>Longtao Huang</a>
|
<a href=/people/h/hui-xue/>Hui Xue’</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--751><div class="card-body p-3 small">Model editing aims to correct outdated or erroneous knowledge in large language models (LLMs) without the need for costly retraining. Lifelong model editing is the most challenging task that caters to the continuous editing requirements of LLMs. Prior works primarily focus on single or batch editing; nevertheless, these methods fall short in lifelong editing scenarios due to catastrophic knowledge forgetting and the degradation of model performance. Although retrieval-based methods alleviate these issues, they are impeded by slow and cumbersome processes of integrating the retrieved knowledge into the model. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous Prompt lEarning method, to boost editing efficacy and inference efficiency in lifelong learning. RECIPE first converts knowledge statements into short and informative continuous prompts, prefixed to the LLM’s input query embedding, to efficiently refine the response grounded on the knowledge. It further integrates the Knowledge Sentinel (KS) that acts as an intermediary to calculate a dynamic threshold, determining whether the retrieval repository contains relevant knowledge. Our retriever and prompt encoder are jointly trained to achieve editing properties, i.e., reliability, generality, and locality. In our experiments, RECIPE is assessed extensively across multiple LLMs and editing datasets, where it achieves superior editing performance. RECIPE also demonstrates its capability to maintain the overall performance of LLMs alongside showcasing fast editing and inference speed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.752.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.752.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--752 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.752 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.752/>A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models</a></strong><br><a href=/people/z/zhihao-wang/>Zhihao Wang</a>
|
<a href=/people/s/shiyu-liu/>Shiyu Liu</a>
|
<a href=/people/j/jianheng-huang/>Jianheng Huang</a>
|
<a href=/people/w/wang-zheng/>Wang Zheng</a>
|
<a href=/people/y/yixuan-liao/>YiXuan Liao</a>
|
<a href=/people/x/xiaoxin-chen/>Xiaoxin Chen</a>
|
<a href=/people/j/junfeng-yao/>Junfeng Yao</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--752><div class="card-body p-3 small">Due to the continuous emergence of new data, version updates have become an indispensable requirement for Large Language Models (LLMs). The training paradigms for version updates of LLMs include pre-training from scratch (PTFS) and continual pre-training (CPT). Preliminary experiments demonstrate that PTFS achieves better pre-training performance, while CPT has lower training cost. Moreover, their performance and training cost gaps widen progressively with version updates. To investigate the underlying reasons for this phenomenon, we analyze the effect of learning rate adjustments during the two stages of CPT: preparing an initialization checkpoint and continual pre-training based on this checkpoint. We find that a large learning rate in the first stage and a complete learning rate decay process in the second stage are crucial for version updates of LLMs. Hence, we propose a learning rate path switching training paradigm. Our paradigm comprises one main path, where we pre-train a LLM with the maximal learning rate, and multiple branching paths, each of which corresponds to an update of the LLM with newly-added training data. Extensive experiments demonstrate the effectiveness and generalization of our paradigm. Particularly, when training four versions of LLMs, our paradigm reduces the total training cost to 58% compared to PTFS, while maintaining comparable pre-training performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.753.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.753.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--753 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.753 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.753/>Zero-Shot Cross-Lingual <span class=acl-fixed-case>NER</span> Using Phonemic Representations for Low-Resource Languages</a></strong><br><a href=/people/j/jimin-sohn/>Jimin Sohn</a>
|
<a href=/people/h/haeji-jung/>Haeji Jung</a>
|
<a href=/people/a/alex-cheng/>Alex Cheng</a>
|
<a href=/people/j/jooeon-kang/>Jooeon Kang</a>
|
<a href=/people/y/yilin-du/>Yilin Du</a>
|
<a href=/people/d/david-r-mortensen/>David R Mortensen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--753><div class="card-body p-3 small">Existing zero-shot cross-lingual NER approaches require substantial prior knowledge of the target language, which is impractical for low-resource languages.In this paper, we propose a novel approach to NER using phonemic representation based on the International Phonetic Alphabet (IPA) to bridge the gap between representations of different languages.Our experiments show that our method significantly outperforms baseline models in extremely low-resource languages, with the highest average F1 score (46.38%) and lowest standard deviation (12.67), particularly demonstrating its robustness with non-Latin scripts. Ourcodes are available at https://github.com/Gabriel819/zeroshot_ner.git</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.754.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.754.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.754/>An Analysis and Mitigation of the Reversal Curse</a></strong><br><a href=/people/a/ang-lv/>Ang Lv</a>
|
<a href=/people/k/kaiyi-zhang/>Kaiyi Zhang</a>
|
<a href=/people/s/shufang-xie/>Shufang Xie</a>
|
<a href=/people/q/quan-tu/>Quan Tu</a>
|
<a href=/people/y/yuhan-chen/>Yuhan Chen</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.755.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.755.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--755 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.755 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.755/>Exploring the Practicality of Generative Retrieval on Dynamic Corpora</a></strong><br><a href=/people/c/chaeeun-kim/>Chaeeun Kim</a>
|
<a href=/people/s/soyoung-yoon/>Soyoung Yoon</a>
|
<a href=/people/h/hyunji-lee/>Hyunji Lee</a>
|
<a href=/people/j/joel-jang/>Joel Jang</a>
|
<a href=/people/s/sohee-yang/>Sohee Yang</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--755><div class="card-body p-3 small">Benchmarking the performance of information retrieval (IR) is mostly conducted with a fixed set of documents (static corpora). However, in realistic scenarios, this is rarely the case and the documents to be retrieved are constantly updated and added. In this paper, we focus on Generative Retrievals (GR), which apply autoregressive language models to IR problems, and explore their adaptability and robustness in dynamic scenarios. We also conduct an extensive evaluation of computational and memory efficiency, crucial factors for real-world deployment of IR systems handling vast and ever-changing document collections. Our results on the StreamingQA benchmark demonstrate that GR is more adaptable to evolving knowledge (4–11%), robust in learning knowledge with temporal information, and efficient in terms of inference FLOPs (x2), indexing time (x6), and storage footprint (x4) compared to Dual Encoders (DE), which are commonly used in retrieval systems. Our paper highlights the potential of GR for future use in practical IR systems within dynamic environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.756.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.756.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--756 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.756 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.756/><span class=acl-fixed-case>O</span>ne<span class=acl-fixed-case>N</span>et: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting</a></strong><br><a href=/people/x/xukai-liu/>Xukai Liu</a>
|
<a href=/people/y/ye-liu/>Ye Liu</a>
|
<a href=/people/k/kai-zhang/>Kai Zhang</a>
|
<a href=/people/k/kehang-wang/>Kehang Wang</a>
|
<a href=/people/q/qi-liu/>Qi Liu</a>
|
<a href=/people/e/enhong-chen/>Enhong Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--756><div class="card-body p-3 small">Entity Linking (EL) is the process of associating ambiguous textual mentions to specific entities in a knowledge base.Traditional EL methods heavily rely on large datasets to enhance their performance, a dependency that becomes problematic in the context of few-shot entity linking, where only a limited number of examples are available for training. To address this challenge, we present OneNet, an innovative framework that utilizes the few-shot learning capabilities of Large Language Models (LLMs) without the need for fine-tuning. To the best of our knowledge, this marks a pioneering approach to applying LLMs to few-shot entity linking tasks. OneNet is structured around three key components prompted by LLMs: (1) an entity reduction processor that simplifies inputs by summarizing and filtering out irrelevant entities, (2) a dual-perspective entity linker that combines contextual cues and prior knowledge for precise entity linking, and (3) an entity consensus judger that employs a unique consistency algorithm to alleviate the hallucination in the entity linking reasoning.Comprehensive evaluations across seven benchmark datasets reveal that OneNet outperforms current state-of-the-art entity linking methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.757.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.757.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--757 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.757 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.757/>Don’t Just Say “<span class=acl-fixed-case>I</span> don’t know”! Self-aligning Large Language Models for Responding to Unknown Questions with Explanations</a></strong><br><a href=/people/y/yang-deng/>Yang Deng</a>
|
<a href=/people/y/yong-zhao/>Yong Zhao</a>
|
<a href=/people/m/moxin-li/>Moxin Li</a>
|
<a href=/people/s/see-kiong-ng/>See-Kiong Ng</a>
|
<a href=/people/t/tat-seng-chua/>Tat-Seng Chua</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--757><div class="card-body p-3 small">Despite the remarkable abilities of Large Language Models (LLMs) to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions. In this work, we propose a novel and scalable self-alignment method to utilize the LLM itself to enhance its response-ability to different types of unknown questions, being capable of not just refusing to answer but further proactively providing explanations to the unanswerability of unknown questions. Specifically, the Self-Align method first employ a two-stage class-aware self-augmentation approach to generate a large amount of unknown question-response data. Then we conduct disparity-driven self-curation to select qualified data for fine-tuning the LLM itself for aligning the responses to unknown questions as desired. Experimental results on two datasets across four types of unknown questions validate the superiority of the Self-Aligned method over existing baselines in terms of three types of task formulation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.758.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.758.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--758 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.758 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.758/>Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning</a></strong><br><a href=/people/x/xijie-huang/>Xijie Huang</a>
|
<a href=/people/l/li-lyna-zhang/>Li Lyna Zhang</a>
|
<a href=/people/k/kwang-ting-cheng/>Kwang-Ting Cheng</a>
|
<a href=/people/f/fan-yang/>Fan Yang</a>
|
<a href=/people/m/mao-yang/>Mao Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--758><div class="card-body p-3 small">Large Language Models (LLMs) have shown impressive capabilities, yet they still struggle with math reasoning. In this work, we propose CoT-Influx, a novel approach that pushes the boundary of few-shot Chain-of-Thoughts (CoT) learning to improve LLM mathematical reasoning. Motivated by the observation that adding more concise CoT examples in the prompt can improve LLM reasoning performance, CoT-Influx employs a coarse-to-fine pruner to maximize the input of effective and concise CoT examples. The pruner first selects as many crucial CoT examples as possible and then prunes unimportant tokens to fit the context window. As a result, by enabling more CoT examples with double the context window size in tokens, CoT-Influx significantly outperforms various prompting baselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 math datasets, achieving up to 4.55% absolute improvements. Remarkably, without any fine-tuning, LLaMA2-70B with CoT-Influx surpasses GPT-3.5 and a wide range of larger LLMs (PaLM, Minerva 540B, etc.) on the GSM8K. CoT-Influx is a plug-and-play module for LLMs, adaptable in various scenarios. It’s compatible with advanced reasoning prompting techniques, such as self-consistency, and supports different long-context LLMs, including Mistral-7B-v0.3-32K and Yi-6B-200K.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.759.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.759.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--759 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.759 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.759.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.759/>Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark</a></strong><br><a href=/people/f/fenglin-liu/>Fenglin Liu</a>
|
<a href=/people/z/zheng-li/>Zheng Li</a>
|
<a href=/people/h/hongjian-zhou/>Hongjian Zhou</a>
|
<a href=/people/q/qingyu-yin/>Qingyu Yin</a>
|
<a href=/people/j/jingfeng-yang/>Jingfeng Yang</a>
|
<a href=/people/x/xianfeng-tang/>Xianfeng Tang</a>
|
<a href=/people/c/chen-luo/>Chen Luo</a>
|
<a href=/people/m/ming-zeng/>Ming Zeng</a>
|
<a href=/people/h/haoming-jiang/>Haoming Jiang</a>
|
<a href=/people/y/yifan-gao/>Yifan Gao</a>
|
<a href=/people/p/priyanka-nigam/>Priyanka Nigam</a>
|
<a href=/people/s/sreyashi-nag/>Sreyashi Nag</a>
|
<a href=/people/b/bing-yin/>Bing Yin</a>
|
<a href=/people/y/yining-hua/>Yining Hua</a>
|
<a href=/people/x/xuan-zhou/>Xuan Zhou</a>
|
<a href=/people/o/omid-rohanian/>Omid Rohanian</a>
|
<a href=/people/a/anshul-thakur/>Anshul Thakur</a>
|
<a href=/people/l/lei-clifton/>Lei Clifton</a>
|
<a href=/people/d/david-a-clifton/>David A. Clifton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--759><div class="card-body p-3 small">The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench. We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and clinical tasks that are complex but common in real-world practice, e.g., open-ended decision-making, long document processing, and emerging drug analysis. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.760.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.760.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--760 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.760 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.760.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.760.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.760/>Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction</a></strong><br><a href=/people/j/jinchuan-zhang/>Jinchuan Zhang</a>
|
<a href=/people/y/yan-zhou/>Yan Zhou</a>
|
<a href=/people/y/yaxin-liu/>Yaxin Liu</a>
|
<a href=/people/z/ziming-li/>Ziming Li</a>
|
<a href=/people/s/songlin-hu/>Songlin Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--760><div class="card-body p-3 small">Automated red teaming is an effective method for identifying misaligned behaviors in large language models (LLMs). Existing approaches, however, often focus primarily on improving attack success rates while overlooking the need for comprehensive test case coverage. Additionally, most of these methods are limited to single-turn red teaming, failing to capture the multi-turn dynamics of real-world human-machine interactions. To overcome these limitations, we propose **HARM** (**H**olistic **A**utomated **R**ed tea**M**ing), which scales up the diversity of test cases using a top-down approach based on an extensible, fine-grained risk taxonomy. Our method also leverages a novel fine-tuning strategy and reinforcement learning techniques to facilitate multi-turn adversarial probing in a human-like manner. Experimental results demonstrate that our framework enables a more systematic understanding of model vulnerabilities and offers more targeted guidance for the alignment process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.761.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.761.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--761 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.761 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.761.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.761/>Householder Pseudo-Rotation: A Novel Approach to Activation Editing in <span class=acl-fixed-case>LLM</span>s with Direction-Magnitude Perspective</a></strong><br><a href=/people/v/van-cuong-pham/>Van-Cuong Pham</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--761><div class="card-body p-3 small">Activation Editing, which involves directly editting the internal representations of large language models (LLMs) to alter their behavior and achieve desired properties, has emerged as a promising area of research. Existing works primarily treat LLMs’ activations as points in space and modify them by adding steering vectors. We show that doing so would break the magnitude consistency of the activation vectors in LLMs. To overcome this shortcoming, we propose a novel editing method that views activations in terms of their directions and magnitudes. Our method, which we name Householder Pseudo-Rotation (HPR), mimics the rotation transformation, thus preserving activation norm and resulting in an improved performance on various safety benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.762.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.762.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--762 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.762 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.762.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.762.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.762/><span class=acl-fixed-case>D</span>ynamic<span class=acl-fixed-case>ER</span>: Resolving Emerging Mentions to Dynamic Entities for <span class=acl-fixed-case>RAG</span></a></strong><br><a href=/people/j/jinyoung-kim/>Jinyoung Kim</a>
|
<a href=/people/d/dayoon-ko/>Dayoon Ko</a>
|
<a href=/people/g/gunhee-kim/>Gunhee Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--762><div class="card-body p-3 small">In the rapidly evolving landscape of language, resolving new linguistic expressions in continuously updating knowledge bases remains a formidable challenge. This challenge becomes critical in retrieval-augmented generation (RAG) with knowledge bases, as emerging expressions hinder the retrieval of relevant documents, leading to generator hallucinations. To address this issue, we introduce a novel task aimed at resolving emerging mentions to dynamic entities and present DynamicER benchmark. Our benchmark includes dynamic entity mention resolution and entity-centric knowledge-intensive QA task, evaluating entity linking and RAG model’s adaptability to new expressions, respectively. We discovered that current entity linking models struggle to link these new expressions to entities. Therefore, we propose a temporal segmented clustering method with continual adaptation, effectively managing the temporal dynamics of evolving entities and emerging mentions. Extensive experiments demonstrate that our method outperforms existing baselines, enhancing RAG model performance on QA task with resolved mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.763.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.763.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--763 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.763 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.763.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.763/>Preserving Generalization of Language models in Few-shot Continual Relation Extraction</a></strong><br><a href=/people/q/quyen-tran/>Quyen Tran</a>
|
<a href=/people/n/nguyen-xuan-thanh/>Nguyen Xuan Thanh</a>
|
<a href=/people/n/nguyen-hoang-anh/>Nguyen Hoang Anh</a>
|
<a href=/people/n/nam-le-hai/>Nam Le Hai</a>
|
<a href=/people/t/trung-le/>Trung Le</a>
|
<a href=/people/l/linh-van-ngo/>Linh Van Ngo</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--763><div class="card-body p-3 small">Few-shot Continual Relations Extraction (FCRE) is an emerging and dynamic area of study where models can sequentially integrate knowledge from new relations with limited labeled data while circumventing catastrophic forgetting and preserving prior knowledge from pre-trained backbones. In this work, we introduce a novel method that leverages often-discarded language model heads. By employing these components via a mutual information maximization strategy, our approach helps maintain prior knowledge from the pre-trained backbone and strategically aligns the primary classification head, thereby enhancing model performance. Furthermore, we explore the potential of Large Language Models (LLMs), renowned for their wealth of knowledge, in addressing FCRE challenges. Our comprehensive experimental results underscore the efficacy of the proposed method and offer valuable insights for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.764.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.764.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--764 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.764 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.764/>A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations</a></strong><br><a href=/people/m/md-tahmid-rahman-laskar/>Md Tahmid Rahman Laskar</a>
|
<a href=/people/s/sawsan-alqahtani/>Sawsan Alqahtani</a>
|
<a href=/people/m/m-saiful-bari/>M Saiful Bari</a>
|
<a href=/people/m/mizanur-rahman/>Mizanur Rahman</a>
|
<a href=/people/m/mohammad-abdullah-matin-khan/>Mohammad Abdullah Matin Khan</a>
|
<a href=/people/h/haidar-khan/>Haidar Khan</a>
|
<a href=/people/i/israt-jahan/>Israt Jahan</a>
|
<a href=/people/a/amran-bhuiyan/>Amran Bhuiyan</a>
|
<a href=/people/c/chee-wei-tan/>Chee Wei Tan</a>
|
<a href=/people/m/md-rizwan-parvez/>Md Rizwan Parvez</a>
|
<a href=/people/e/enamul-hoque/>Enamul Hoque</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/j/jimmy-huang/>Jimmy Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--764><div class="card-body p-3 small">Large Language Models (LLMs) have recently gained significant attention due to their remarkable capabilities in performing diverse tasks across various domains. However, a thorough evaluation of these models is crucial before deploying them in real-world applications to ensure they produce reliable performance. Despite the well-established importance of evaluating LLMs in the community, the complexity of the evaluation process has led to varied evaluation setups, causing inconsistencies in findings and interpretations. To address this, we systematically review the primary challenges and limitations causing these inconsistencies and unreliable evaluations in various steps of LLM evaluation. Based on our critical review, we present our perspectives and recommendations to ensure LLM evaluations are reproducible, reliable, and robust.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.765.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.765.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--765 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.765 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.765/>Consecutive Batch Model Editing with <span class=acl-fixed-case>H</span>oo<span class=acl-fixed-case>K</span> Layers</a></strong><br><a href=/people/s/shuaiyi-li/>Shuaiyi Li</a>
|
<a href=/people/y/yang-deng/>Yang Deng</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/h/hongyuan-lu/>Hongyuan Lu</a>
|
<a href=/people/l/liang-chen/>Liang Chen</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--765><div class="card-body p-3 small">As the typical retraining paradigm is unacceptably time- and resource-consuming, researchers are turning to model editing to find an effective way that supports both consecutive and batch scenarios to edit the model behavior directly. Despite all these practical expectations, existing model editing methods fail to realize all of them. Furthermore, the memory demands for such sequential model editing approaches tend to be prohibitive, frequently necessitating an external memory that grows incrementally over time. To cope with these challenges, we propose CoachHooK, a model editing method that simultaneously supports sequential and batch editing. CoachHooK is memory-friendly as it only needs a small amount of it to store several hook layers whose size remains unchanged over time. Experimental results demonstrate the superiority of our method over other batch-supportive model editing methods under both single-round and consecutive batch editing scenarios. Extensive analyses of CoachHooK have been conducted to verify the stability of our method over a number of consecutive steps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.766.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.766.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--766 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.766 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.766/>Topic-Oriented Open Relation Extraction with A Priori Seed Generation</a></strong><br><a href=/people/l/linyi-ding/>Linyi Ding</a>
|
<a href=/people/j/jinfeng-xiao/>Jinfeng Xiao</a>
|
<a href=/people/s/sizhe-zhou/>Sizhe Zhou</a>
|
<a href=/people/c/chaoqi-yang/>Chaoqi Yang</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--766><div class="card-body p-3 small">The field of open relation extraction (ORE) has recently observed significant advancement thanks to the growing capability of large language models (LLMs). Nevertheless, challenges persist when ORE is performed on specific topics. Existing methods give sub-optimal results in five dimensions: factualness, topic relevance, informativeness, coverage, and uniformity. To improve topic-oriented ORE, we propose a zero-shot approach called PriORE: Open Relation Extraction with a Priori seed generation. PriORE leverages the built-in knowledge of LLMs to maintain a dynamic seed relation dictionary for the topic. The dictionary is initialized by seed relations generated from topic-relevant entity types and expanded during contextualized ORE. PriORE then reduces the randomness in generative ORE by converting it to a more robust relation classification task. Experiments show the approach empowers better topic-oriented control over the generated relations and thus improves ORE performance along the five dimensions, especially on specialized and narrow topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.767.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.767.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--767 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.767 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.767/>Related Work and Citation Text Generation: A Survey</a></strong><br><a href=/people/x/xiangci-li/>Xiangci Li</a>
|
<a href=/people/j/jessica-ouyang/>Jessica Ouyang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--767><div class="card-body p-3 small">To convince readers of the novelty of their research paper, authors must perform a literature review and compose a coherent story that connects and relates prior works to the current work. This challenging nature of literature review writing makes automatic related work generation (RWG) academically and computationally interesting, and also makes it an excellent test bed for examining the capability of SOTA natural language processing (NLP) models. Since the initial proposal of the RWG task, its popularity has waxed and waned, following the capabilities of mainstream NLP approaches. In this work, we survey the zoo of RWG historical works, summarizing the key approaches and task definitions and discussing the ongoing challenges of RWG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.768.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.768.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--768 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.768 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.768.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.768/>Curriculum Consistency Learning for Conditional Sentence Generation</a></strong><br><a href=/people/l/liangxin-liu/>Liangxin Liu</a>
|
<a href=/people/x/xuebo-liu/>Xuebo Liu</a>
|
<a href=/people/l/lian-lian/>Lian Lian</a>
|
<a href=/people/s/shengjun-cheng/>Shengjun Cheng</a>
|
<a href=/people/j/jun-rao/>Jun Rao</a>
|
<a href=/people/t/tengfei-yu/>Tengfei Yu</a>
|
<a href=/people/h/hexuan-deng/>Hexuan Deng</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--768><div class="card-body p-3 small">Consistency learning (CL) has proven to be a valuable technique for improving the robustness of models in conditional sentence generation (CSG) tasks by ensuring stable predictions across various input data forms. However, models augmented with CL often face challenges in optimizing consistency features, which can detract from their efficiency and effectiveness. To address these challenges, we introduce Curriculum Consistency Learning (CCL), a novel strategy that guides models to learn consistency in alignment with their current capacity to differentiate between features. CCL is designed around the inherent aspects of CL-related losses, promoting task independence and simplifying implementation. Implemented across four representative CSG tasks, including instruction tuning (IT) for large language models and machine translation (MT) in three modalities (text, speech, and vision), CCL demonstrates marked improvements. Specifically, it delivers +2.0 average accuracy point improvement compared with vanilla IT and an average increase of +0.7 in COMET scores over traditional CL methods in MT tasks. Our comprehensive analysis further indicates that models utilizing CCL are particularly adept at managing complex instances, showcasing the effectiveness and efficiency of CCL in improving CSG models. Code and scripts are available at https://github.com/xinxinxing/Curriculum-Consistency-Learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.769.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.769.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--769 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.769 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.769/>A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences</a></strong><br><a href=/people/l/leonardo-bertolazzi/>Leonardo Bertolazzi</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--769><div class="card-body p-3 small">The reasoning abilities of Large Language Models (LLMs) are becoming a central focus of study in NLP. In this paper, we consider the case of syllogistic reasoning, an area of deductive reasoning studied extensively in logic and cognitive psychology. Previous research has shown that pre-trained LLMs exhibit reasoning biases, such as content effects, avoid answering that no conclusion follows, align with human difficulties, and struggle with multi-step reasoning. We contribute to this research line by systematically investigating the effects of chain-of-thought reasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on syllogistic reasoning, considering syllogisms with conclusions that support or violate world knowledge and with multiple premises. Crucially, we go beyond the standard focus on accuracy, with an in-depth analysis of the conclusions generated by the models. Our results suggest that the behavior of pre-trained LLMs can be explained by heuristics studied in cognitive science and that both ICL and SFT improve model performance on valid inferences, although only the latter can mitigate most reasoning biases while being consistent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.770.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.770.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.770/>Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision</a></strong><br><a href=/people/f/fan-jiang/>Fan Jiang</a>
|
<a href=/people/t/tom-drummond/>Tom Drummond</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.771.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.771.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--771 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.771 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.771.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.771/><span class=acl-fixed-case>MOSEL</span>: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on <span class=acl-fixed-case>EU</span> Languages</a></strong><br><a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/s/sara-papi/>Sara Papi</a>
|
<a href=/people/l/luisa-bentivogli/>Luisa Bentivogli</a>
|
<a href=/people/a/alessio-brutti/>Alessio Brutti</a>
|
<a href=/people/m/mauro-cettolo/>Mauro Cettolo</a>
|
<a href=/people/r/roberto-gretter/>Roberto Gretter</a>
|
<a href=/people/m/marco-matassoni/>Marco Matassoni</a>
|
<a href=/people/m/mohamed-nabih/>Mohamed Nabih</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--771><div class="card-body p-3 small">The rise of foundation models (FMs), coupled with regulatory efforts addressing their risks and impacts, has sparked significant interest in open-source models. However, existing speech FMs (SFMs) fall short of full compliance with the open-source principles, even if claimed otherwise, as no existing SFM has model weights, code, and training data publicly available under open-source terms. In this work, we take the first step toward filling this gap by focusing on the 24 official languages of the European Union (EU). We collect suitable training data by surveying automatic speech recognition datasets and unlabeled speech corpora under open-source compliant licenses, for a total of 950k hours. Additionally, we release automatic transcripts for 441k hours of unlabeled data under the permissive CC-BY license, thereby facilitating the creation of open-source SFMs for the EU languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.772.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.772.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--772 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.772 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.772.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.772.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.772/>Improving Knowledge Graph Completion with Structure-Aware Supervised Contrastive Learning</a></strong><br><a href=/people/j/jiashi-lin/>Jiashi Lin</a>
|
<a href=/people/l/lifang-wang/>Lifang Wang</a>
|
<a href=/people/x/xinyu-lu/>Xinyu Lu</a>
|
<a href=/people/z/zhongtian-hu/>Zhongtian Hu</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/w/wenxuan-lu/>Wenxuan Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--772><div class="card-body p-3 small">Knowledge Graphs (KGs) often suffer from incomplete knowledge, which which restricts their utility. Recently, Contrastive Learning (CL) has been introduced to Knowledge Graph Completion (KGC), significantly improving the discriminative capabilities of KGC models and setting new benchmarks in performance. However, existing contrastive methods primarily focus on individual triples, overlooking the broader structural connectivities and topologies of KGs. This narrow focus limits a comprehensive understanding of the graph’s structural knowledge. To address this gap, we propose StructKGC, a novel contrastive learning framework designed to flexibly accommodate the diverse topologies inherent in KGs. Additionally, we introduce four contrastive tasks specifically tailored to KG data: Vertex-level CL, Neighbor-level CL, Path-level CL, and Relation composition level CL. These tasks are trained synergistically during the fine-tuning of pre-trained language models (PLMs), allowing for a more nuanced capture of subgraph semantics. To validate the effectiveness of our method, we perform a comprehensive set of experiments on several real-world datasets. The experimental results demonstrate that our approach achieves SOTA performance under standard supervised and low-resource settings. Furthermore, the different levels of structure-aware tasks introduced can mutually reinforce each other, leading to consistent performance improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.773.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.773.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--773 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.773 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.773/>Contribution of Linguistic Typology to <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Parsing: An Empirical Investigation</a></strong><br><a href=/people/a/ali-basirat/>Ali Basirat</a>
|
<a href=/people/n/navid-baradaran-hemmati/>Navid Baradaran Hemmati</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--773><div class="card-body p-3 small">Universal Dependencies (UD) is a global initiative to create a standard annotation for the dependency syntax of human languages. Addressing its deviation from typological principles, this study presents an empirical investigation of a typologically motivated transformation of UD proposed by William Croft. Our findings underscore the significance of the transformations across diverse languages and highlight their advantages and limitations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.774.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.774.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--774 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.774 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.774.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.774.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.774/><span class=acl-fixed-case>TR</span>o<span class=acl-fixed-case>TR</span>: A Framework for Evaluating the Re-contextualization of Text Reuse</a></strong><br><a href=/people/f/francesco-periti/>Francesco Periti</a>
|
<a href=/people/p/pierluigi-cassotti/>Pierluigi Cassotti</a>
|
<a href=/people/s/stefano-montanelli/>Stefano Montanelli</a>
|
<a href=/people/n/nina-tahmasebi/>Nina Tahmasebi</a>
|
<a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--774><div class="card-body p-3 small">Current approaches for detecting text reuse do not focus on recontextualization, i.e., how the new context(s) of a reused text differs from its original context(s). In this paper, we propose a novel framework called TRoTR that relies on the notion of topic relatedness for evaluating the diachronic change of context in which text is reused. TRoTR includes two NLP tasks: TRiC and TRaC. TRiC is designed to evaluate the topic relatedness between a pair of recontextualizations. TRaC is designed to evaluate the overall topic variation within a set of recontextualizations. We also provide a curated TRoTR benchmark of biblical text reuse, human-annotated with topic relatedness. The benchmark exhibits an inter-annotator agreement of .811. We evaluate multiple, established SBERT models on the TRoTR tasks and find that they exhibit greater sensitivity to textual similarity than topic relatedness. Our experiments show that fine-tuning these models can mitigate such a kind of sensitivity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.775.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.775.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--775 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.775 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.775/>Structured Optimal Brain Pruning for Large Language Models</a></strong><br><a href=/people/j/jiateng-wei/>Jiateng Wei</a>
|
<a href=/people/q/quan-lu/>Quan Lu</a>
|
<a href=/people/n/ning-jiang/>Ning Jiang</a>
|
<a href=/people/s/siqi-li/>Siqi Li</a>
|
<a href=/people/j/jingyang-xiang/>Jingyang Xiang</a>
|
<a href=/people/j/jun-chen/>Jun Chen</a>
|
<a href=/people/y/yong-liu/>Yong Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--775><div class="card-body p-3 small">The massive parameters and computational demands hinder the widespread application of Large Language Models (LLMs). Network pruning provides a practical solution to this problem. However, existing pruning works for LLMs mainly focus on unstructured pruning or necessitate post-pruning fine-tuning. The former relies on special hardware to accelerate computation, while the latter may need substantial computational resources. In this paper, we introduce a retraining-free structured pruning method called SoBP (Structured Optimal Brain Pruning). It leverages global first-order information to select pruning structures, then refines them with a local greedy approach, and finally adopts module-wise reconstruction to mitigate information loss. We assess the effectiveness of SoBP across 14 models from 3 LLM families on 8 distinct datasets. Experimental results demonstrate that SoBP outperforms current state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.776.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.776.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--776 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.776 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.776.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.776.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.776/>Automatically Generated Definitions and their utility for Modeling Word Meaning</a></strong><br><a href=/people/f/francesco-periti/>Francesco Periti</a>
|
<a href=/people/d/david-alfter/>David Alfter</a>
|
<a href=/people/n/nina-tahmasebi/>Nina Tahmasebi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--776><div class="card-body p-3 small">Modeling lexical semantics is a challenging task, often suffering from interpretability pitfalls. In this paper, we delve into the generation of dictionary-like sense definitions and explore their utility for modeling word meaning. We fine-tuned two Llama models and include an existing T5-based model in our evaluation. Firstly, we evaluate the quality of the generated definitions on existing English benchmarks, setting new state-of-the-art results for the Definition Generation task. Next, we explore the use of definitions generated by our models as intermediate representations subsequently encoded as sentence embeddings. We evaluate this approach on lexical semantics tasks such as the Word-in-Context, Word Sense Induction, and Lexical Semantic Change, setting new state-of-the-art results in all three tasks when compared to unsupervised baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.777.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.777.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--777 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.777 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.777/>How Do Your Code <span class=acl-fixed-case>LLM</span>s perform? Empowering Code Instruction Tuning with Really Good Data</a></strong><br><a href=/people/y/yejie-wang/>Yejie Wang</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/d/dayuan-fu/>Dayuan Fu</a>
|
<a href=/people/z/zhuoma-gongque/>Zhuoma GongQue</a>
|
<a href=/people/h/heyang-xu/>Heyang Xu</a>
|
<a href=/people/y/yanxu-chen/>Yanxu Chen</a>
|
<a href=/people/z/zhexu-wang/>Zhexu Wang</a>
|
<a href=/people/y/yujia-fu/>Yujia Fu</a>
|
<a href=/people/g/guanting-dong/>Guanting Dong</a>
|
<a href=/people/m/muxi-diao/>Muxi Diao</a>
|
<a href=/people/j/jingang-wang/>Jingang Wang</a>
|
<a href=/people/m/mengdi-zhang/>Mengdi Zhang</a>
|
<a href=/people/x/xunliang-cai/>Xunliang Cai</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--777><div class="card-body p-3 small">Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we observe Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we find that many datasets suffer from severe data leakage. After cleaning up most of the leaked data, some well-known high-quality datasets perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data pruning strategy for selecting good samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Our experiments show Xcoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.778.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.778.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--778 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.778 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.778/><span class=acl-fixed-case>MAIR</span>: A Massive Benchmark for Evaluating Instructed Retrieval</a></strong><br><a href=/people/w/weiwei-sun-sd/>Weiwei Sun</a>
|
<a href=/people/z/zhengliang-shi/>Zhengliang Shi</a>
|
<a href=/people/w/wu-jiu-long/>Wu Jiu Long</a>
|
<a href=/people/l/lingyong-yan/>Lingyong Yan</a>
|
<a href=/people/x/xinyu-ma/>Xinyu Ma</a>
|
<a href=/people/y/yiding-liu/>Yiding Liu</a>
|
<a href=/people/m/min-cao/>Min Cao</a>
|
<a href=/people/d/dawei-yin/>Dawei Yin</a>
|
<a href=/people/z/zhaochun-ren/>Zhaochun Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--778><div class="card-body p-3 small">Recent information retrieval (IR) models are pre-trained and instruction-tuned on massive datasets and tasks, enabling them to perform well on a wide range of tasks and potentially generalize to unseen tasks with instructions. However, existing IR benchmarks focus on a limited scope of tasks, making them insufficient for evaluating the latest IR models. In this paper, we propose MAIR (Massive Instructed Retrieval Benchmark), a heterogeneous IR benchmark that includes 126 distinct IR tasks across 6 domains, collected from existing datasets. We benchmark state-of-the-art instruction-tuned text embedding models and re-ranking models. Our experiments reveal that instruction-tuned models generally achieve superior performance compared to non-instruction-tuned models on MAIR Additionally, our results suggest that current instruction-tuned text embedding models and re-ranking models still lack effectiveness in specific long-tail tasks. MAIR is publicly available at https://github.com/sunnweiwei/Mair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.779.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.779.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--779 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.779 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.779/>Rethinking the Evaluation of In-Context Learning for <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/g/guoxin-yu/>Guoxin Yu</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/y/yue-yu/>Yue Yu</a>
|
<a href=/people/x/xiang-ao/>Xiang Ao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--779><div class="card-body p-3 small">In-context learning (ICL) has demonstrated excellent performance across various downstream NLP tasks, especially when synergized with powerful large language models (LLMs). Existing studies evaluate ICL methods primarily based on downstream task performance. This evaluation protocol overlooks the significant cost associated with the demonstration configuration process, i.e., tuning the demonstration as the ICL prompt. However, in this work, we point out that the evaluation protocol leads to unfair comparisons and potentially biased evaluation, because we surprisingly find the correlation between the configuration costs and task performance. Then we call for a two-dimensional evaluation paradigm that considers both of these aspects, facilitating a fairer comparison.Finally, based on our empirical finding that the optimized demonstration on one language model generalizes across language models of different sizes, we introduce a simple yet efficient strategy that can be applied to any ICL method as a plugin, yielding a better trade-off between the two dimensions according to the proposed evaluation paradigm.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.780.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.780.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--780 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.780 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.780/>Cluster-Norm for Unsupervised Probing of Knowledge</a></strong><br><a href=/people/w/walter-laurito/>Walter Laurito</a>
|
<a href=/people/s/sharan-maiya/>Sharan Maiya</a>
|
<a href=/people/g/gregoire-dhimoila/>Grégoire Dhimoïla</a>
|
<a href=/people/o/owen-ho-wan-yeung/>Owen Ho Wan Yeung</a>
|
<a href=/people/k/kaarel-hanni/>Kaarel Hänni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--780><div class="card-body p-3 small">The deployment of language models brings challenges in generating reliable text, especially when these models are fine-tuned with human preferences. To extract the encoded knowledge in these models without (potentially) biased human labels, unsupervised probing techniques like Contrast-Consistent Search (CCS) have been developed (Burns et al., 2022). However, salient but unrelated features in activation space can mislead these probes (Farquhar et al., 2023). Addressing this, we propose a cluster-normalization method to minimize the impact of such features by clustering and normalizing activations of contrast pairs before applying unsupervised probing techniques. While this approach does not address the issue of distinguishing between latent knowledge and that portrayed by a simulated agent—a major issue in the literature of eliciting latent knowledge (Paul Christiano and Xu, 2021)—it still significantly improves the accuracy of probes in identifying the intended knowledge amidst distractions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.781.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.781.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--781 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.781 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.781/>Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries</a></strong><br><a href=/people/e/eden-biran/>Eden Biran</a>
|
<a href=/people/d/daniela-gottesman/>Daniela Gottesman</a>
|
<a href=/people/s/sohee-yang/>Sohee Yang</a>
|
<a href=/people/m/mor-geva/>Mor Geva</a>
|
<a href=/people/a/amir-globerson/>Amir Globerson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--781><div class="card-body p-3 small">Large language models (LLMs) can solve complex multi-step problems, but little is known about how these computations are implemented internally. Motivated by this, we study how LLMs answer multi-hop queries such as “The spouse of the performer of Imagine is”. These queries require two information extraction steps: a latent one for resolving the first hop (“the performer of Imagine”) into the bridge entity (John Lennon), and another for resolving the second hop (“the spouse of John Lennon”) into the target entity (Yoko Ono). Understanding how the latent step is computed internally is key to understanding the overall computation. By carefully analyzing the internal computations of transformer-based LLMs, we discover that the bridge entity is resolved in the early layers of the model. Then, only after this resolution, the two-hop query is solved in the later layers. Because the second hop commences in later layers, there could be cases where these layers no longer encode the necessary knowledge for correctly predicting the answer. Motivated by this, we propose a novel “back-patching” analysis method whereby a hidden representation from a later layer is patched back to an earlier layer. We find that in up to 66% of previously incorrect cases there exists a back-patch that results in the correct generation of the answer, showing that the later layers indeed sometimes lack the needed functionality. Overall our methods and findings open further opportunities for understanding and improving latent reasoning in transformer-based LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.782.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.782.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--782 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.782 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.782/>Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration</a></strong><br><a href=/people/k/kangxi-wu/>Kangxi Wu</a>
|
<a href=/people/l/liang-pang/>Liang Pang</a>
|
<a href=/people/h/huawei-shen/>Huawei Shen</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--782><div class="card-body p-3 small">The black-box nature of large language models (LLMs) poses challenges in interpreting results, impacting issues such as data intellectual property protection and hallucination tracing. Training data attribution (TDA) methods are considered effective solutions to address these challenges.Most recent TDA methods rely on influence functions, assuming the model achieves minimized empirical risk. However, achieving this criterion is difficult, and sourcing accuracy can be compromised by fitting errors during model training. In this paper, we introduce a novel TDA method called Debias and Denoise Attribution (DDA), which enhances influence functions by addressing fitting errors. Specifically, the debias strategy seeks to improve the performance of influence functions by eliminating the knowledge bias present in the base model before fine-tuning, while the denoise strategy aims to reduce discrepancies in influence scores arising from varying degrees of fitting during the training process through smoothing techniques.Experimental results demonstrate that our method significantly outperforms existing approaches, achieving an averaged AUC of 91.64%. Moreover, DDA exhibits strong generality and scalability across various sources and different-scale models like LLaMA2, QWEN2, and Mistral.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.783.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.783.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--783 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.783 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.783/>Where am <span class=acl-fixed-case>I</span>? Large Language Models Wandering between Semantics and Structures in Long Contexts</a></strong><br><a href=/people/s/seonmin-koo/>Seonmin Koo</a>
|
<a href=/people/j/jinsung-kim/>Jinsung Kim</a>
|
<a href=/people/y/youngjoon-jang/>YoungJoon Jang</a>
|
<a href=/people/c/chanjun-park/>Chanjun Park</a>
|
<a href=/people/h/heui-seok-lim/>Heuiseok Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--783><div class="card-body p-3 small">As the utilization of Large Language Models (LLMs) becomes more widespread, there is a growing demand for their ability to handle more complex and longer external knowledge across various use cases. Most existing evaluations of the open-ended question answering (ODQA) task, which necessitates the use of external knowledge, focus solely on whether the model provides the correct answer. However, even when LLMs answer correctly, they often fail to provide an obvious source for their responses. Therefore, it is necessary to jointly evaluate and verify the correctness of the answers and the appropriateness of grounded evidence in complex external contexts. To address this issue, we examine the phenomenon of discrepancies in abilities across two distinct tasks—QA and evidence selection—when performed simultaneously, from the perspective of task alignment. To verify LLMs’ task alignment, we introduce a verification framework and resources considering both semantic relevancy and structural diversity of the given long context knowledge. Through extensive experiments and detailed analysis, we provide insights into the task misalignment between QA and evidence selection. Our code and resources will be available upon acceptance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.784.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.784.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--784 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.784 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.784.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.784/><span class=acl-fixed-case>KARL</span>: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students</a></strong><br><a href=/people/m/matthew-shu/>Matthew Shu</a>
|
<a href=/people/n/nishant-balepur/>Nishant Balepur</a>
|
<a href=/people/s/shi-feng/>Shi Feng</a>
|
<a href=/people/j/jordan-lee-boyd-graber/>Jordan Lee Boyd-Graber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--784><div class="card-body p-3 small">Flashcard schedulers rely on 1) *student models* to predict the flashcards a student knows; and 2) *teaching policies* to pick which cards to show next via these predictions.Prior student models, however, just use study data like the student’s past responses, ignoring the text on cards. We propose **content-aware scheduling**, the first schedulers exploiting flashcard content.To give the first evidence that such schedulers enhance student learning, we build KARL, a simple but effective content-aware student model employing deep knowledge tracing (DKT), retrieval, and BERT to predict student recall.We train KARL by collecting a new dataset of 123,143 study logs on diverse trivia questions.KARL bests existing student models in AUC and calibration error.To ensure our improved predictions lead to better student learning, we create a novel delta-based teaching policy to deploy KARL online.Based on 32 study paths from 27 users, KARL improves learning efficiency over SOTA, showing KARL’s strength and encouraging researchers to look beyond historical study data to fully capture student abilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.785.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.785.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--785 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.785 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.785/>Large Language Models Can Be Contextual Privacy Protection Learners</a></strong><br><a href=/people/y/yijia-xiao/>Yijia Xiao</a>
|
<a href=/people/y/yiqiao-jin/>Yiqiao Jin</a>
|
<a href=/people/y/yushi-bai/>Yushi Bai</a>
|
<a href=/people/y/yue-wu/>Yue Wu</a>
|
<a href=/people/x/xianjun-yang/>Xianjun Yang</a>
|
<a href=/people/x/xiao-luo/>Xiao Luo</a>
|
<a href=/people/w/wenchao-yu/>Wenchao Yu</a>
|
<a href=/people/x/xujiang-zhao/>Xujiang Zhao</a>
|
<a href=/people/y/yanchi-liu/>Yanchi Liu</a>
|
<a href=/people/q/quanquan-gu/>Quanquan Gu</a>
|
<a href=/people/h/haifeng-chen/>Haifeng Chen</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/w/wei-cheng/>Wei Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--785><div class="card-body p-3 small">The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains contextually sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of data leakage of sensitive PII during inference time. To address this challenge, we introduce Contextual Privacy Protection Language Models (CPPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding inference-time data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, effectively protecting private data while enhancing the model’s knowledge. Our work underscores the potential for Large Language Models as robust contextual privacy protection learners.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.786.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.786.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--786 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.786 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.786.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.786.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.786/>A <span class=acl-fixed-case>SMART</span> Mnemonic Sounds like “Glue Tonic”: Mixing <span class=acl-fixed-case>LLM</span>s with Student Feedback to Make Mnemonic Learning Stick</a></strong><br><a href=/people/n/nishant-balepur/>Nishant Balepur</a>
|
<a href=/people/m/matthew-shu/>Matthew Shu</a>
|
<a href=/people/a/alexander-hoyle/>Alexander Hoyle</a>
|
<a href=/people/a/alison-robey/>Alison Robey</a>
|
<a href=/people/s/shi-feng/>Shi Feng</a>
|
<a href=/people/s/seraphina-goldfarb-tarrant/>Seraphina Goldfarb-Tarrant</a>
|
<a href=/people/j/jordan-lee-boyd-graber/>Jordan Lee Boyd-Graber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--786><div class="card-body p-3 small">Keyword mnemonics are memorable explanations that link new terms to simpler keywords.Prior work generates mnemonics for students, but they do not train models using mnemonics students prefer and aid learning.We build SMART, a mnemonic generator trained on feedback from real students learning new terms.To train SMART, we first fine-tune LLaMA-2 on a curated set of user-written mnemonics.We then use LLM alignment to enhance SMART: we deploy mnemonics generated by SMART in a flashcard app to find preferences on mnemonics students favor.We gather 2684 preferences from 45 students across two types: **expressed** (inferred from ratings) and **observed** (inferred from student learning), yielding three key findings.First, expressed and observed preferences disagree; what students *think* is helpful does not always capture what is *truly* helpful.Second, Bayesian models can synthesize complementary data from multiple preference types into a single effectiveness signal.SMART is tuned via Direct Preference Optimization on this signal, which resolves ties and missing labels in the typical method of pairwise comparisons, augmenting data for LLM output quality gains. Third, mnemonic experts assess SMART as matching GPT-4 at much lower deployment costs, showing the utility of capturing diverse student feedback to align LLMs in education.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.787.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.787.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--787 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.787 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.787/>Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large Language Models</a></strong><br><a href=/people/m/minghao-wu/>Minghao Wu</a>
|
<a href=/people/t/thuy-vu/>Thuy-Trang Vu</a>
|
<a href=/people/l/lizhen-qu/>Lizhen Qu</a>
|
<a href=/people/r/reza-haf/>Reza Haf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--787><div class="card-body p-3 small">Large language models (LLMs) are typically fine-tuned on diverse and extensive datasets sourced from various origins to develop a comprehensive range of skills, such as writing, reasoning, chatting, coding, and more. Each skill has unique characteristics, and these datasets are often heterogeneous and imbalanced, making the fine-tuning process highly challenging. Balancing the development of each skill while ensuring the model maintains its overall performance requires sophisticated techniques and careful dataset curation. In this work, we propose a general, model-agnostic, reinforcement learning framework, Mixture-of-Skills (MoS), that learns to optimize data usage automatically during the fine-tuning process. This framework ensures the optimal comprehensive skill development of LLMs by dynamically adjusting the focus on different datasets based on their current learning state. To validate the effectiveness of MoS, we conduct extensive experiments using three diverse LLM backbones on two widely used benchmarks and demonstrate that MoS substantially enhances model performance. Building on the success of MoS, we propose MoSpec, an adaptation for task-specific fine-tuning, which harnesses the utilities of various datasets for a specific purpose. Our work underlines the significance of dataset rebalancing and present MoS as a powerful, general solution for optimizing data usage in the fine-tuning of LLMs for various purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.788.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.788.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--788 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.788 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.788/><span class=acl-fixed-case>M</span>ol<span class=acl-fixed-case>TRES</span>: Improving Chemical Language Representation Learning for Molecular Property Prediction</a></strong><br><a href=/people/j/jun-hyung-park/>Jun-Hyung Park</a>
|
<a href=/people/y/yeachan-kim/>Yeachan Kim</a>
|
<a href=/people/m/mingyu-lee/>Mingyu Lee</a>
|
<a href=/people/h/hyuntae-park/>Hyuntae Park</a>
|
<a href=/people/s/sangkeun-lee/>SangKeun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--788><div class="card-body p-3 small">Chemical representation learning has gained increasing interest due to the limited availability of supervised data in fields such as drug and materials design. This interest particularly extends to chemical language representation learning, which involves pre-training Transformers on SMILES sequences - textual descriptors of molecules. Despite its success in molecular property prediction, current practices often lead to overfitting and limited scalability due to early convergence. In this paper, we introduce a novel chemical language representation learning framework, called MolTRES, to address these issues. MolTRES incorporates generator-discriminator training, allowing the model to learn from more challenging examples that require structural understanding. In addition, we enrich molecular representations by transferring knowledge from scientific literature by integrating external materials embedding. Experimental results show that our model outperforms existing state-of-the-art models on popular molecular property prediction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.789.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.789.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--789 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.789 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.789.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.789/>First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning</a></strong><br><a href=/people/y/yoichi-aoki/>Yoichi Aoki</a>
|
<a href=/people/k/keito-kudo/>Keito Kudo</a>
|
<a href=/people/t/tatsuki-kuribayashi/>Tatsuki Kuribayashi</a>
|
<a href=/people/s/shusaku-sone/>Shusaku Sone</a>
|
<a href=/people/m/masaya-taniguchi/>Masaya Taniguchi</a>
|
<a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--789><div class="card-body p-3 small">Explicit multi-step reasoning, such as chain-of-thought, is widely adopted in the community to explore the better performance of language models (LMs). We report on the systematic strategy that LMs use in this process.Our controlled experiments reveal that LMs rely more heavily on heuristics, such as lexical overlap, in the earlier stages of reasoning when more steps are required to reach an answer. Conversely, their reliance on heuristics decreases as LMs progress closer to the final answer. This suggests that LMs track only a limited number of future steps and dynamically combine heuristic strategies with rational ones in solving tasks involving multi-step reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.790.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.790.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--790 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.790 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.790/>Tools Fail: Detecting Silent Errors in Faulty Tools</a></strong><br><a href=/people/j/jimin-sun/>Jimin Sun</a>
|
<a href=/people/s/so-yeon-min/>So Yeon Min</a>
|
<a href=/people/y/yingshan-chang/>Yingshan Chang</a>
|
<a href=/people/y/yonatan-bisk/>Yonatan Bisk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--790><div class="card-body p-3 small">Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not in their weights, to perform tasks on the web, and even to control robots. However, most ontologies and surveys of tool-use have assumed the core challenge for LLMs is choosing the tool. Instead, we introduce a framework for tools more broadly which guides us to explore a model’s ability to detect “silent” tool errors, and reflect on how to plan. This more directly aligns with the increasingly popular use of models as tools. We provide an initial approach to failure recovery with promising results both on a controlled calculator setting and embodied agent planning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.791.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.791.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--791 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.791 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.791.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.791.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.791/>Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity</a></strong><br><a href=/people/b/bowen-zhang/>Bowen Zhang</a>
|
<a href=/people/c/chunping-li/>Chunping Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--791><div class="card-body p-3 small">Semantic Textual Similarity (STS) constitutes a critical research direction in computational linguistics and serves as a key indicator of the encoding capabilities of embedding models. Driven by advances in pre-trained language models and contrastive learning, leading sentence representation methods have reached an average Spearman’s correlation score of approximately 86 across seven STS benchmarks in SentEval. However, further progress has become increasingly marginal, with no existing method attaining an average score higher than 86.5 on these tasks. This paper conducts an in-depth analysis of this phenomenon and concludes that the upper limit for Spearman’s correlation scores under contrastive learning is 87.5. To transcend this ceiling, we propose an innovative approach termed Pcc-tuning, which employs Pearson’s correlation coefficient as a loss function to refine model performance beyond contrastive learning. Experimental results demonstrate that Pcc-tuning can markedly surpass previous state-of-the-art strategies with only a minimal amount of fine-grained annotated samples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.792.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.792.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--792 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.792 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.792/>Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing</a></strong><br><a href=/people/d/deokhyung-kang/>Deokhyung Kang</a>
|
<a href=/people/s/seonjeong-hwang/>Seonjeong Hwang</a>
|
<a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/g/gary-lee/>Gary Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--792><div class="card-body p-3 small">Recent efforts have aimed to utilize multilingual pretrained language models (mPLMs) to extend semantic parsing (SP) across multiple languages without requiring extensive annotations. However, achieving zero-shot cross-lingual transfer for SP remains challenging, leading to a performance gap between source and target languages. In this study, we propose Cross-Lingual Back-Parsing (CBP), a novel data augmentation methodology designed to enhance cross-lingual transfer for SP. Leveraging the representation geometry of the mPLMs, CBP synthesizes target language utterances from source meaning representations. Our methodology effectively performs cross-lingual data augmentation in challenging zero-resource settings, by utilizing only labeled data in the source language and monolingual corpora. Extensive experiments on two cross-language SP benchmarks (Mschema2QA and Xspider) demonstrate that CBP brings substantial gains in the target language. Further analysis of the synthesized utterances shows that our method successfully generates target language utterances with high slot value alignment rates while preserving semantic integrity. Our codes and data are publicly available at https://github.com/deokhk/CBP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.793.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.793.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--793 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.793 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.793/>Shaking Up <span class=acl-fixed-case>VLM</span>s: Comparing Transformers and Structured State Space Models for Vision & Language Modeling</a></strong><br><a href=/people/g/georgios-pantazopoulos/>Georgios Pantazopoulos</a>
|
<a href=/people/m/malvina-nikandrou/>Malvina Nikandrou</a>
|
<a href=/people/a/alessandro-suglia/>Alessandro Suglia</a>
|
<a href=/people/o/oliver-lemon/>Oliver Lemon</a>
|
<a href=/people/a/arash-eshghi/>Arash Eshghi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--793><div class="card-body p-3 small">This study explores replacing Transformers in Visual Language Models (VLMs) with Mamba, a recent structured state space model (SSM) that demonstrates promising performance in sequence modeling. We test models up to 3B parameters under controlled conditions, showing that Mamba-based VLMs outperforms Transformers-based VLMs in captioning, question answering, and reading comprehension. However, we find that Transformers achieve greater performance in visual grounding and the performance gap widens with scale. We explore two hypotheses to explain this phenomenon: 1) the effect of task-agnostic visual encoding on the updates of the hidden states, and 2) the difficulty in performing visual grounding from the perspective of in-context multimodal retrieval. Our results indicate that a task-aware encoding yields minimal performance gains on grounding, however, Transformers significantly outperform Mamba at in-context multimodal retrieval. Overall, Mamba shows promising performance on tasks where the correct output relies on a summary of the image but struggles when retrieval of explicit information from the context is required.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.794.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.794.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--794 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.794 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.794/>Are <span class=acl-fixed-case>LLM</span>s Good Zero-Shot Fallacy Classifiers?</a></strong><br><a href=/people/f/fengjun-pan/>Fengjun Pan</a>
|
<a href=/people/x/xiaobao-wu/>Xiaobao Wu</a>
|
<a href=/people/z/zongrui-li/>Zongrui Li</a>
|
<a href=/people/l/luu-anh-tuan/>Anh Tuan Luu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--794><div class="card-body p-3 small">Fallacies are defective arguments with faulty reasoning. Detecting and classifying them is a crucial NLP task to prevent misinformation, manipulative claims, and biased decisions. However, existing fallacy classifiers are limited by the requirement for sufficient labeled data for training, which hinders their out-of-distribution (OOD) generalization abilities. In this paper, we focus on leveraging Large Language Models (LLMs) for zero-shot fallacy classification. To elicit fallacy-related knowledge and reasoning abilities of LLMs, we propose diverse single-round and multi-round prompting schemes, applying different taskspecific instructions such as extraction, summarization, and Chain-of-Thought reasoning. With comprehensive experiments on benchmark datasets, we suggest that LLMs could be potential zero-shot fallacy classifiers. In general, LLMs under single-round prompting schemes have achieved acceptable zeroshot performances compared to the best fullshot baselines and can outperform them in all OOD inference scenarios and some opendomain tasks. Our novel multi-round prompting schemes can effectively bring about more improvements, especially for small LLMs. Our analysis further underlines the future research on zero-shot fallacy classification. Codes and data are available at: https://github.com/panFJCharlotte98/Fallacy_Detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.795.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.795.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--795 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.795 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.795/>The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis</a></strong><br><a href=/people/y/yuxiang-zhou/>Yuxiang Zhou</a>
|
<a href=/people/j/jiazheng-li/>Jiazheng Li</a>
|
<a href=/people/y/yanzheng-xiang/>Yanzheng Xiang</a>
|
<a href=/people/h/hanqi-yan/>Hanqi Yan</a>
|
<a href=/people/l/lin-gui/>Lin Gui</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--795><div class="card-body p-3 small">Understanding in-context learning (ICL) capability that enables large language models (LLMs) to excel in proficiency through demonstration examples is of utmost importance. This importance stems not only from the better utilization of this capability across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns regarding truthfulness, bias, and toxicity, that may arise alongside the capability. In this paper, we present a thorough survey on the interpretation and analysis of in-context learning. First, we provide a concise introduction to the background and definition of in-context learning. Then, we give an overview of advancements from two perspectives: 1) a theoretical perspective, emphasizing studies on mechanistic interpretability and delving into the mathematical foundations behind ICL; and 2) an empirical perspective, concerning studies that empirically analyze factors associated with ICL. We conclude by discussing open questions and the challenges encountered, and suggesting potential avenues for future research. We believe that our work establishes the basis for further exploration into the interpretation of in-context learning. To aid this effort, we have created a repository containing resources that will be continually updated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.796.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.796.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--796 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.796 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.796/>More <span class=acl-fixed-case>DWUG</span>s: Extending and Evaluating Word Usage Graph Datasets in Multiple Languages</a></strong><br><a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/p/pierluigi-cassotti/>Pierluigi Cassotti</a>
|
<a href=/people/b/bill-noble/>Bill Noble</a>
|
<a href=/people/d/david-alfter/>David Alfter</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte Im Walde</a>
|
<a href=/people/n/nina-tahmasebi/>Nina Tahmasebi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--796><div class="card-body p-3 small">Word Usage Graphs (WUGs) represent human semantic proximity judgments for pairs of word uses in a weighted graph, which can be clustered to infer word sense clusters from simple pairwise word use judgments, avoiding the need for word sense definitions. SemEval-2020 Task 1 provided the first and to date largest manually annotated, diachronic WUG dataset. In this paper, we check the robustness and correctness of the annotations by continuing the SemEval annotation algorithm for two more rounds and comparing against an established annotation paradigm. Further, we test the reproducibility by resampling a new, smaller set of word uses from the SemEval source corpora and annotating them. Our work contributes to a better understanding of the problems and opportunities of the WUG annotation paradigm and points to future improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.797.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.797.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--797 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.797 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.797/>Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification</a></strong><br><a href=/people/m/ming-li/>Ming Li</a>
|
<a href=/people/j/jike-zhong/>Jike Zhong</a>
|
<a href=/people/c/chenxin-li/>Chenxin Li</a>
|
<a href=/people/l/liuzhuozheng-li/>Liuzhuozheng Li</a>
|
<a href=/people/n/nie-lin/>Nie Lin</a>
|
<a href=/people/m/masashi-sugiyama/>Masashi Sugiyama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--797><div class="card-body p-3 small">Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed the success of prompt tuning and adapter tuning, while the classic model fine-tuning on inherent parameters seems to be overlooked. It is believed that fine-tuning the parameters of VLMs with few-shot samples corrupts the pre-trained knowledge since fine-tuning the CLIP model even degrades performance. In this paper, we revisit this viewpoint, and propose a new perspective: fine-tuning the specific parameters instead of all will uncover the power of classic model fine-tuning on VLMs. Through our meticulous study, we propose ClipFit, a simple yet effective method to fine-tune CLIP without introducing any overhead of extra parameters. We demonstrate that by only fine-tuning the specific bias terms and normalization layers, ClipFit can improve the performance of zero-shot CLIP by 7.27% average harmonic mean accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the pre-trained models, we conducted extensive experimental analyses w.r.t. changes in internal parameters and representations. We found that low-level text bias layers and the first layer normalization layer change much more than other layers. The code will be released.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.798.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.798.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--798 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.798 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.798/><span class=acl-fixed-case>ECIS</span>-<span class=acl-fixed-case>VQG</span>: Generation of Entity-centric Information-seeking Questions from Videos</a></strong><br><a href=/people/a/arpan-phukan/>Arpan Phukan</a>
|
<a href=/people/m/manish-gupta/>Manish Gupta</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--798><div class="card-body p-3 small">Previous studies on question generation from videos have mostly focused on generating questions about common objects and attributes and hence are not entity-centric. In this work, we focus on the generation of entity-centric information-seeking questions from videos. Such a system could be useful for video-based learning, recommending “People Also Ask” questions, video-based chatbots, and fact-checking. Our work addresses three key challenges: identifying question-worthy information, linking it to entities, and effectively utilizing multimodal signals. Further, to the best of our knowledge, there does not exist a large-scale dataset for this task. Most video question generation datasets are on TV shows, movies, or human activities or lack entity-centric information-seeking questions. Hence, we contribute a diverse dataset of YouTube videos, VideoQuestions, consisting of 411 videos with 2265 manually annotated questions. We further propose a model architecture combining Transformers, rich context signals (titles, transcripts, captions, embeddings), and a combination of cross-entropy and contrastive loss function to encourage entity-centric question generation. Our best method yields BLEU, ROUGE, CIDEr, and METEOR scores of 71.3, 78.6, 7.31, and 81.9, respectively, demonstrating practical usability. We make the code and dataset publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.799.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.799.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--799 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.799 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.799/>Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation</a></strong><br><a href=/people/e/elaf-alhazmi/>Elaf Alhazmi</a>
|
<a href=/people/q/quan-z-sheng/>Quan Z. Sheng</a>
|
<a href=/people/w/wei-emma-zhang/>Wei Emma Zhang</a>
|
<a href=/people/m/munazza-zaib/>Munazza Zaib</a>
|
<a href=/people/a/ahoud-alhazmi/>Ahoud Alhazmi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--799><div class="card-body p-3 small">The distractor generation task focuses on generating incorrect but plausible options for objective questions such as fill-in-the-blank and multiple-choice questions. This task is widely utilized in educational settings across various domains and subjects. The effectiveness of these questions in assessments relies on the quality of the distractors, as they challenge examinees to select the correct answer from a set of misleading options. The evolution of artificial intelligence (AI) has transitioned the task from traditional methods to the use of neural networks and pre-trained language models. This shift has established new benchmarks and expanded the use of advanced deep learning methods in generating distractors. This survey explores distractor generation tasks, datasets, methods, and current evaluation metrics for English objective questions, covering both text-based and multi-modal domains. It also evaluates existing AI models and benchmarks and discusses potential future research directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.800.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--800 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.800 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.800/>Evaluating <span class=tex-math>n</span>-Gram Novelty of Language Models Using Rusty-<span class=acl-fixed-case>DAWG</span></a></strong><br><a href=/people/w/william-merrill/>William Merrill</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yanai-elazar/>Yanai Elazar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--800><div class="card-body p-3 small">How novel are texts generated by language models (LMs) relative to their training corpora? In this work, we investigate the extent to which modern LMs generate <span class=tex-math>n</span>-grams from their training data, evaluating both (i) the probability LMs assign to complete training <span class=tex-math>n</span>-grams and (ii) <span class=tex-math>n</span>-novelty, the proportion of <span class=tex-math>n</span>-grams generated by an LM that did not appear in the training data (for arbitrarily large <span class=tex-math>n</span>). To enable arbitrary-length <span class=tex-math>n</span>-gram search over a corpus in constant time w.r.t. corpus size, we develop Rusty-DAWG, a novel search tool inspired by indexing of genomic data. We compare the novelty of LM-generated text to human-written text and explore factors that affect generation novelty, focusing on the Pythia models. We find that, for <span class=tex-math>n > 4</span>, LM-generated text is less novel than human-written text, though it is more novel for smaller <span class=tex-math>n</span>. Larger LMs and more constrained decoding strategies both decrease novelty. Finally, we show that LMs complete <span class=tex-math>n</span>-grams with lower loss if they are more frequent in the training data. Overall, our results reveal factors influencing the novelty of LM-generated text, and we release Rusty-DAWG to facilitate further pretraining data research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.801.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--801 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.801/><span class=acl-fixed-case>ASL</span> <span class=acl-fixed-case>STEM</span> <span class=acl-fixed-case>W</span>iki: Dataset and Benchmark for Interpreting <span class=acl-fixed-case>STEM</span> Articles</a></strong><br><a href=/people/k/kayo-yin/>Kayo Yin</a>
|
<a href=/people/c/chinmay-singh/>Chinmay Singh</a>
|
<a href=/people/f/fyodor-o-minakov/>Fyodor O Minakov</a>
|
<a href=/people/v/vanessa-milan/>Vanessa Milan</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé Iii</a>
|
<a href=/people/c/cyril-zhang/>Cyril Zhang</a>
|
<a href=/people/a/alex-xijie-lu/>Alex Xijie Lu</a>
|
<a href=/people/d/danielle-bragg/>Danielle Bragg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--801><div class="card-body p-3 small">Deaf and hard-of-hearing (DHH) students face significant barriers in accessing science, technology, engineering, and mathematics (STEM) education, notably due to the scarcity of STEM resources in signed languages. To help address this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia articles on STEM topics in English, interpreted into over 300 hours of American Sign Language (ASL). ASL STEM Wiki is the first continuous signing dataset focused on STEM, facilitating the development of AI resources for STEM education in ASL.We identify several use cases of ASL STEM Wiki with human-centered applications. For example, because this dataset highlights the frequent use of fingerspelling for technical concepts, which inhibits DHH students’ ability to learn,we develop models to identify fingerspelled words—which can later be used to query for appropriate ASL signs to suggest to interpreters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.802.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--802 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.802/>Can Automatic Metrics Assess High-Quality Translations?</a></strong><br><a href=/people/s/sweta-agrawal/>Sweta Agrawal</a>
|
<a href=/people/a/antonio-farinhas/>António Farinhas</a>
|
<a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/andre-f-t-martins/>Andre Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--802><div class="card-body p-3 small">Automatic metrics for evaluating translation quality are typically validated by measuring how well they correlate with human assessments. However, correlation methods tend to capture only the ability of metrics to differentiate between good and bad source-translation pairs, overlooking their reliability in distinguishing alternative translations for the same source. In this paper, we confirm that this is indeed the case by showing that current metrics are insensitive to nuanced differences in translation quality. This effect is most pronounced when the quality is high and the variance among alternatives is low. Given this finding, we shift towards detecting high-quality correct translations, an important problem in practical decision-making scenarios where a binary check of correctness is prioritized over a nuanced evaluation of quality. Using the MQM framework as the gold standard, we systematically stress-test the ability of current metrics to identify translations with no errors as marked by humans. Our findings reveal that current metrics often over or underestimate translation quality, indicating significant room for improvement in machine translation evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.803.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--803 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.803/>Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation</a></strong><br><a href=/people/s/sweta-agrawal/>Sweta Agrawal</a>
|
<a href=/people/j/jose-g-c-de-souza/>José G. C. De Souza</a>
|
<a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/antonio-farinhas/>António Farinhas</a>
|
<a href=/people/g/goncalo-faria/>Gonçalo Faria</a>
|
<a href=/people/p/patrick-fernandes/>Patrick Fernandes</a>
|
<a href=/people/n/nuno-m-guerreiro/>Nuno M Guerreiro</a>
|
<a href=/people/a/andre-f-t-martins/>Andre Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--803><div class="card-body p-3 small">Alignment with human preferences is an important step in developing accurate and safe large language models. This is no exception in machine translation (MT), where better handling of language nuances and context-specific variations leads to improved quality. However, preference data based on human feedback can be very expensive to obtain and curate at a large scale. Automatic metrics, on the other hand, can induce preferences, but they might not match human expectations perfectly. In this paper, we propose an approach that leverages the best of both worlds. We first collect sentence-level quality assessments from professional linguists on translations generated by multiple high-quality MT systems and evaluate the ability of current automatic metrics to recover these preferences. We then use this analysis to curate a new dataset, MT-Pref (metric induced translation preference) dataset, which comprises 18k instances covering 18 language directions, using texts sourced from multiple domains post-2022. We show that aligning TOWER models on MT-Pref significantly improves translation quality on WMT23 and FLORES benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.804.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--804 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.804/><span class=acl-fixed-case>DC</span>-Instruct: An Effective Framework for Generative Multi-intent Spoken Language Understanding</a></strong><br><a href=/people/b/bowen-xing/>Bowen Xing</a>
|
<a href=/people/l/lizi-liao/>Lizi Liao</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/i/ivor-tsang/>Ivor Tsang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--804><div class="card-body p-3 small">In the realm of multi-intent spoken language understanding, recent advancements have leveraged the potential of prompt learning frameworks. However, critical gaps exist in these frameworks: the lack of explicit modeling of dual-task dependencies and the oversight of task-specific semantic differences among utterances. To address these shortcomings, we propose DC-Instruct, a novel generative framework based on Dual-task Inter-dependent Instructions (DII) and Supervised Contrastive Instructions (SCI). Specifically, DII guides large language models (LLMs) to generate labels for one task based on the other task’s labels, thereby explicitly capturing dual-task inter-dependencies. Moreover, SCI leverages utterance semantics differences by guiding LLMs to determine whether a pair of utterances share the same or similar labels. This can improve LLMs on extracting and discriminating task-specific semantics, thus enhancing their SLU reasoning abilities. Extensive experiments on public benchmark datasets show that DC-Instruct markedly outperforms current generative models and state-of-the-art methods, demonstrating its effectiveness in enhancing dialogue language understanding and reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.805.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--805 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.805.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.805/><span class=acl-fixed-case>K</span>now<span class=acl-fixed-case>T</span>uning: Knowledge-aware Fine-tuning for Large Language Models</a></strong><br><a href=/people/y/yougang-lyu/>Yougang Lyu</a>
|
<a href=/people/l/lingyong-yan/>Lingyong Yan</a>
|
<a href=/people/s/shuaiqiang-wang/>Shuaiqiang Wang</a>
|
<a href=/people/h/haibo-shi/>Haibo Shi</a>
|
<a href=/people/d/dawei-yin/>Dawei Yin</a>
|
<a href=/people/p/pengjie-ren/>Pengjie Ren</a>
|
<a href=/people/z/zhumin-chen/>Zhumin Chen</a>
|
<a href=/people/m/maarten-de-rijke/>Maarten de Rijke</a>
|
<a href=/people/z/zhaochun-ren/>Zhaochun Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--805><div class="card-body p-3 small">Despite their success at many natural language processing (NLP) tasks, large language models still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to improve fine-grained and coarse-grained knowledge awareness of LLMs. We devise a fine-grained knowledge augmentation stage to train LLMs to identify difficult fine-grained knowledge in answers. We also propose a coarse-grained knowledge comparison stage to train LLMs to distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effectiveness of KnowTuning, through automatic and human evaluations, across various sizes of LLMs. We further verify that KnowTuning generates more facts with less factual error rate under fine-grained facts evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.806.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--806 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.806.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.806.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.806/><span class=acl-fixed-case>S</span>ec<span class=acl-fixed-case>C</span>oder: Towards Generalizable and Robust Secure Code Generation</a></strong><br><a href=/people/b/boyu-zhang/>Boyu Zhang</a>
|
<a href=/people/t/tianyu-du/>Tianyu Du</a>
|
<a href=/people/j/junkai-tong/>Junkai Tong</a>
|
<a href=/people/x/xuhong-zhang/>Xuhong Zhang</a>
|
<a href=/people/k/kingsum-chow/>Kingsum Chow</a>
|
<a href=/people/s/sheng-cheng/>Sheng Cheng</a>
|
<a href=/people/x/xun-wang/>Xun Wang</a>
|
<a href=/people/j/jianwei-yin/>Jianwei Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--806><div class="card-body p-3 small">After large models (LMs) have gained widespread acceptance in code-related tasks, their superior generative capacity has greatly promoted the application of the code LM. Nevertheless, the security of the generated code has raised attention to its potential damage. Existing secure code generation methods have limited generalizability to unseen test cases and poor robustness against the attacked model, leading to safety failures in code generation. In this paper, we propose a generalizable and robust secure code generation method SecCoder by using in-context learning (ICL) and the safe demonstration. The dense retriever is also used to select the most helpful demonstration to maximize the improvement of the generated code’s security. Experimental results show the superior generalizability of the proposed model SecCoder compared to the current secure code generation method, achieving a significant security improvement of an average of 7.20% on unseen test cases. The results also show the better robustness of SecCoder compared to the current attacked code LM, achieving a significant security improvement of an average of 7.74%. Our analysis indicates that SecCoder enhances the security of LMs in generating code, and it is more generalizable and robust.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.807.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--807 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.807.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.807/>Nash <span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>T</span>: Multi-Path Inference with Preference Equilibrium</a></strong><br><a href=/people/z/ziqi-zhang/>Ziqi Zhang</a>
|
<a href=/people/c/cunxiang-wang/>Cunxiang Wang</a>
|
<a href=/people/x/xiao-xiong/>Xiao Xiong</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/d/donglin-wang/>Donglin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--807><div class="card-body p-3 small">Chain of thought (CoT) is a reasoning framework that can enhance the performance of large language models (LLMs) on complex inference tasks. In particular, among various studies related to CoT, multi-path inference stands out as a simple yet effective improvement. However, there is no optimal setting for the number of inference paths. Therefore, we have to increase the number of inference paths to obtain better results, which in turn increases the inference cost. To address this limitation, we can utilize question-related role templates to guide LLMs into relevant roles, thereby increasing the possibility of correct inferences for each path and further reducing dependence on the number of inference paths while improving reasoning accuracy. However, placing LLMs into specific roles may reduce their reasoning diversity and performance on a few tasks where role dependence is low. To alleviate the excessive immersion of the LLM into a specific role, we propose Nash CoT by constructing a competitive system on each path that balances the generation from role-specific LLMs’ and the general LLMs’ generation, thereby ensuring both effective role adoption and diversity in LLM generation further maintaining the performance of multi-path inference while reducing the requirement of the number of inference paths. We evaluate Nash CoT across various inference tasks, including Arabic Reasoning, Commonsense Question Answering, and Symbolic Inference, achieving results that are comparable to or better than those of multi-path CoT with the equal number of inference paths.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.808.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--808 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.808/>Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention</a></strong><br><a href=/people/x/xingtai-lv/>Xingtai Lv</a>
|
<a href=/people/n/ning-ding/>Ning Ding</a>
|
<a href=/people/k/kaiyan-zhang/>Kaiyan Zhang</a>
|
<a href=/people/e/ermo-hua/>Ermo Hua</a>
|
<a href=/people/g/ganqu-cui/>Ganqu Cui</a>
|
<a href=/people/b/bowen-zhou/>Bowen Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--808><div class="card-body p-3 small">Improving the effectiveness and efficiency of large language models (LLMs) simultaneously is a critical yet challenging research goal. In this paper, we find that low-rank pre-training, normally considered as efficient methods that will compromise performance, can be scalably effective when reduced parameters are precisely targeted. Specifically, by applying low-dimensional module only to the attention layer — resolves this issue and enhances both effectiveness and efficiency. We refer to this structure as *Low-dimensional Projected Attention (LPA)* and provide an explanatory analysis. Through extensive experimentation at parameter scales of 130M, 370M, and scaling up to 3B, we have validated the effectiveness and scalability of LPA. Our results show that LPA model can save up to 12.4% in time while achieving an approximate 5% improvement in test perplexity (ppl) and on downstream tasks compared with vanilla Transformer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.809.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--809 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.809/>Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector</a></strong><br><a href=/people/x/xiaoxue-cheng/>Xiaoxue Cheng</a>
|
<a href=/people/j/junyi-li/>Junyi Li</a>
|
<a href=/people/w/wayne-xin-zhao/>Xin Zhao</a>
|
<a href=/people/h/hongzhi-zhang/>Hongzhi Zhang</a>
|
<a href=/people/f/fuzheng-zhang/>Fuzheng Zhang</a>
|
<a href=/people/d/di-zhang/>Di Zhang</a>
|
<a href=/people/k/kun-gai/>Kun Gai</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--809><div class="card-body p-3 small">Hallucination detection is a challenging task for large language models (LLMs), and existing studies heavily rely on powerful closed-source LLMs such as GPT-4. In this paper, we propose an autonomous LLM-based agent framework, called HaluAgent, which enables relatively smaller LLMs (e.g. Baichuan2-Chat 7B) to actively select suitable tools for detecting multiple hallucination types such as text, code, and mathematical expression. In HaluAgent, we integrate the LLM, multi-functional toolbox, and design a fine-grained three-stage detection framework along with memory mechanism. To facilitate the effectiveness of HaluAgent, we leverage existing Chinese and English datasets to synthesize detection trajectories for fine-tuning, which endows HaluAgent with the capability for bilingual hallucination detection. Extensive experiments demonstrate that only using 2K samples for tuning LLMs, HaluAgent can perform hallucination detection on various types of tasks and datasets, achieving performance comparable to or even higher than GPT-4 without tool enhancements on both in-domain and out-of-domain datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.810.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.810.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--810 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.810 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.810/>Interpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding</a></strong><br><a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/z/zhen-huang/>Zhen Huang</a>
|
<a href=/people/x/xinmei-tian/>Xinmei Tian</a>
|
<a href=/people/l/le-lu/>Le Lu</a>
|
<a href=/people/h/houqiang-li/>Houqiang Li</a>
|
<a href=/people/x/xu-shen/>Xu Shen</a>
|
<a href=/people/j/jieping-ye/>Jieping Ye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--810><div class="card-body p-3 small">Contrastively trained vision-language models such as CLIP have achieved remarkable progress in vision and language representation learning. Despite the promising progress, their proficiency in compositional reasoning over attributes and relations (e.g., distinguishing between “the car is underneath the person” and “the person is underneath the car”) remains notably inadequate. We investigate the cause for this deficient behavior is the composition attribution issue, where the attribution scores (e.g., attention scores or GradCAM scores) for relations (e.g., underneath) or attributes (e.g., red) in the text are substantially lower than those for object terms. In this work, we show such issue is mitigated via a novel framework called CAE (Composition Attribution Enhancement). This generic framework incorporates various interpretable attribution methods to encourage the model to pay greater attention to composition words denoting relationships and attributes within the text. Detailed analysis shows that our approach enables the models to adjust and rectify the attribution of the texts. Extensive experiments across seven benchmarks reveal that our framework significantly enhances the ability to discern intricate details and construct more sophisticated interpretations of combined visual and linguistic elements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.811.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--811 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.811.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.811/><span class=acl-fixed-case>LLM</span> Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History</a></strong><br><a href=/people/a/akash-gupta/>Akash Gupta</a>
|
<a href=/people/i/ivaxi-sheth/>Ivaxi Sheth</a>
|
<a href=/people/v/vyas-raina/>Vyas Raina</a>
|
<a href=/people/m/mark-gales/>Mark Gales</a>
|
<a href=/people/m/mario-fritz/>Mario Fritz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--811><div class="card-body p-3 small">With the recent emergence of powerful instruction-tuned large language models (LLMs), various helpful conversational Artificial Intelligence (AI) systems have been deployed across many applications. When prompted by users, these AI systems successfully perform a wide range of tasks as part of a conversation. To provide some sort of memory and context, such approaches typically condition their output on the entire conversational history. Although this sensitivity to the conversational history can often lead to improved performance on subsequent tasks, we find that performance can in fact also be negatively impacted, if there is a _task-switch_. To the best of our knowledge, our work makes the first attempt to formalize the study of such vulnerabilities and interference of tasks in conversational LLMs caused by task-switches in the conversational history. Our experiments across 5 datasets with 15 task switches using popular LLMs reveal that many of the task-switches can lead to significant performance degradation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.812.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.812.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--812 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.812 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.812.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.812/>Social Bias Probing: Fairness Benchmarking for Language Models</a></strong><br><a href=/people/m/marta-marchiori-manerba/>Marta Marchiori Manerba</a>
|
<a href=/people/k/karolina-stanczak/>Karolina Stanczak</a>
|
<a href=/people/r/riccardo-guidotti/>Riccardo Guidotti</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--812><div class="card-body p-3 small">While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.813.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.813.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--813 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.813 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.813/>Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</a></strong><br><a href=/people/w/wenhao-yu/>Wenhao Yu</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/x/xiaoman-pan/>Xiaoman Pan</a>
|
<a href=/people/p/peixin-cao/>Peixin Cao</a>
|
<a href=/people/k/kaixin-ma/>Kaixin Ma</a>
|
<a href=/people/j/jian-li/>Jian Li</a>
|
<a href=/people/h/hongwei-wang/>Hongwei Wang</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--813><div class="card-body p-3 small">Retrieval-augmented language model (RALM) represents a significant advancement in mitigating factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed, and the retrieval of irrelevant data can mislead the response generation. Moreover, standard RALMs frequently neglect their intrinsic knowledge due to the interference from retrieved information. In instances where the retrieved information is irrelevant, RALMs should ideally utilize their intrinsic knowledge or, in the absence of both intrinsic and retrieved knowledge, opt to respond with “unknown” to avoid hallucination. In this paper, we introduces Chain-of-Note (CoN), a novel approach to improve robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for each retrieved document, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. Our experimental results show that GPT-4, when equipped with CoN, outperforms the Chain-of-Thought approach. Besides, we utilized GPT-4 to create 10K CoN data, subsequently trained on smaller models like OPT and LLaMa-2. Our experiments across four open-domain QA benchmarks show that fine-tuned RALMs equipped with CoN significantly outperform standard fine-tuned RALMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.814.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--814 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.814/><span class=acl-fixed-case>D</span>yna<span class=acl-fixed-case>T</span>hink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models</a></strong><br><a href=/people/j/jiabao-pan/>Jiabao Pan</a>
|
<a href=/people/y/yan-zhang/>Yan Zhang</a>
|
<a href=/people/c/chen-zhang/>Chen Zhang</a>
|
<a href=/people/z/zuozhu-liu/>Zuozhu Liu</a>
|
<a href=/people/h/hongwei-wang/>Hongwei Wang</a>
|
<a href=/people/h/haizhou-li/>Haizhou Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--814><div class="card-body p-3 small">Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with complicated problems, while a thorough method, which considers multiple reasoning pathways and verifies each step carefully, results in slower inference. This paper addresses the challenge of enabling LLMs to autonomously select between fast and slow inference methods, thereby optimizing both efficiency and effectiveness. We introduce a dynamic decision-making framework that categorizes tasks into two distinct pathways: ‘Fast,’ designated for tasks where the LLM quickly identifies a high-confidence solution, and ‘Slow,’ allocated for tasks that the LLM perceives as complex and for which it has low confidence in immediate solutions as well as requiring more reasoning paths to verify. Experiments on five popular reasoning benchmarks demonstrated the superiority of the DynaThink over baselines. For example, when we compared it to strong COT with self-consistency baseline on the complicated MATH dataset, DynaThink achieved more than 3% increase in accuracy with lower cost. The code will be made available upon publication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.815.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.815.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--815 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.815 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.815/>Revisiting Automated Evaluation for Long-form Table Question Answering</a></strong><br><a href=/people/y/yuqi-wang/>Yuqi Wang</a>
|
<a href=/people/l/lyuhao-chen/>Lyuhao Chen</a>
|
<a href=/people/s/songcheng-cai/>Songcheng Cai</a>
|
<a href=/people/z/zhijian-xu/>Zhijian Xu</a>
|
<a href=/people/y/yilun-zhao/>Yilun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--815><div class="card-body p-3 small">In the era of data-driven decision-making, Long-Form Table Question Answering (LFTQA) is essential for integrating structured data with complex reasoning. Despite recent advancements in Large Language Models (LLMs) for LFTQA, evaluating their effectiveness remains a significant challenge. We introduce LFTQA-Eval, a meta-evaluation dataset comprising 2,988 human-annotated examples, to rigorously assess the efficacy of current automated metrics in assessing LLM-based LFTQA systems, with a focus on faithfulness and comprehensiveness. Our findings reveal that existing automatic metrics poorly correlate with human judgments and fail to consistently differentiate between factually accurate responses and those that are coherent but factually incorrect. Additionally, our in-depth examination of the limitations associated with automated evaluation methods provides essential insights for the improvement of LFTQA automated evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.816.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.816.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--816 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.816 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.816.software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.816.data.tgz data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.816/>Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems</a></strong><br><a href=/people/i/italo-luis-da-silva/>Italo Luis Da Silva</a>
|
<a href=/people/h/hanqi-yan/>Hanqi Yan</a>
|
<a href=/people/l/lin-gui/>Lin Gui</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--816><div class="card-body p-3 small">The inherent ambiguity of cause and effect boundaries poses a challenge in evaluating causal event extraction tasks. Traditional metrics like Exact Match and BertScore poorly reflect model performance, so we trained evaluation models to approximate human evaluation, achieving high agreement. We used them to perform Reinforcement Learning with extraction models to align them with human preference, prioritising semantic understanding. We successfully explored our approach through multiple datasets, including transferring an evaluator trained on one dataset to another as a way to decrease the reliance on human-annotated data. In that vein, we also propose a weak-to-strong supervision method that uses a fraction of the annotated data to train an evaluation model while still achieving high performance in training an RL model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.817.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.817.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--817 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.817 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.817.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.817.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.817/>Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning</a></strong><br><a href=/people/z/zhihan-zhang/>Zhihan Zhang</a>
|
<a href=/people/t/tao-ge/>Tao Ge</a>
|
<a href=/people/z/zhenwen-liang/>Zhenwen Liang</a>
|
<a href=/people/w/wenhao-yu/>Wenhao Yu</a>
|
<a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/m/mengzhao-jia/>Mengzhao Jia</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--817><div class="card-body p-3 small">Supervised fine-tuning enhances the problem-solving abilities of language models across various mathematical reasoning tasks. To maximize such benefits, existing research focuses on *broadening* the training set with various data augmentation techniques, which is effective for standard single-round question-answering settings. Our work introduces a novel technique aimed at cultivating a *deeper* understanding of the training problems at hand, enhancing performance not only in standard settings but also in more complex scenarios that require reflective thinking. Specifically, we propose **reflective augmentation**, a method that embeds problem reflection into each training instance. It trains the model to consider alternative perspectives and engage with abstractions and analogies, thereby fostering a thorough comprehension through reflective reasoning. Extensive experiments validate the achievement of our aim, underscoring the unique advantages of our method and its complementary nature relative to existing augmentation techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.818.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.818.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--818 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.818 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.818/><span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>DV</span>er: Explainable Claim Verification over Long and Hybrid-content Financial Documents</a></strong><br><a href=/people/y/yilun-zhao/>Yilun Zhao</a>
|
<a href=/people/y/yitao-long/>Yitao Long</a>
|
<a href=/people/t/tintin-jiang/>Tintin Jiang</a>
|
<a href=/people/c/chengye-wang/>Chengye Wang</a>
|
<a href=/people/w/weiyuan-chen/>Weiyuan Chen</a>
|
<a href=/people/h/hongjun-liu/>Hongjun Liu</a>
|
<a href=/people/x/xiangru-tang/>Xiangru Tang</a>
|
<a href=/people/y/yiming-zhang/>Yiming Zhang</a>
|
<a href=/people/c/chen-zhao/>Chen Zhao</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--818><div class="card-body p-3 small">We introduce FinDVer, a comprehensive benchmark specifically designed to evaluate the explainable claim verification capabilities of LLMs in the context of understanding and analyzing long, hybrid-content financial documents. FinDVer contains 4,000 expert-annotated examples across four subsets, each focusing on a type of scenario that frequently arises in real-world financial domains. We assess a broad spectrum of 25 LLMs under long-context and RAG settings. Our results show that even the current best-performing system (i.e., GPT-4o) significantly lags behind human experts. Our detailed findings and insights highlight the strengths and limitations of existing LLMs in this new task. We believe FinDVer can serve as a valuable benchmark for evaluating LLM capabilities in claim verification over complex, expert-domain documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.819.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.819.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--819 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.819 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.819/>Extracting Prompts by Inverting <span class=acl-fixed-case>LLM</span> Outputs</a></strong><br><a href=/people/c/collin-zhang/>Collin Zhang</a>
|
<a href=/people/j/john-xavier-morris/>John Xavier Morris</a>
|
<a href=/people/v/vitaly-shmatikov/>Vitaly Shmatikov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--819><div class="card-body p-3 small">We consider the problem of language model inversion: given outputs of a language model, we seek to extract the prompt that generated these outputs. We develop a new black-box method, output2prompt, that extracts prompts without access to the model’s logits and without adversarial or jailbreaking queries. Unlike previous methods, output2prompt only needs outputs of normal user queries. To improve memory efficiency, output2prompt employs a new sparse encoding techique. We measure the efficacy of output2prompt on a variety of user and system prompts and demonstrate zero-shot transferability across different LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.820.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.820.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--820 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.820 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.820/><span class=acl-fixed-case>B</span>ias<span class=acl-fixed-case>A</span>lert: A Plug-and-play Tool for Social Bias Detection in <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/z/zhiting-fan/>Zhiting Fan</a>
|
<a href=/people/r/ruizhe-chen/>Ruizhe Chen</a>
|
<a href=/people/r/ruiling-xu/>Ruiling Xu</a>
|
<a href=/people/z/zuozhu-liu/>Zuozhu Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--820><div class="card-body p-3 small">Evaluating the bias of LLMs becomes more crucial with their rapid development. However, existing evaluation approaches rely on fixed-form outputs and cannot adapt to the flexible open-text generation scenarios of LLMs (e.g., sentence completion and question answering). To address this, we introduce BiasAlert, a plug-and-play tool designed to detect social bias in open-text generations of LLMs. BiasAlert integrates external human knowledge with its inherent reasoning capabilities to detect bias reliably. Extensive experiments demonstrate that BiasAlert significantly outperforms existing state-of-the-art methods like GPT-4-as-Judge in detecting bias. Furthermore, through application studies, we showcase the utility of BiasAlert in reliable LLM fairness evaluation and bias mitigation across various scenarios. Model and code will be publicly released.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.821.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.821.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--821 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.821 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.821/><span class=acl-fixed-case>VHASR</span>: A Multimodal Speech Recognition System With Vision Hotwords</a></strong><br><a href=/people/j/jiliang-hu/>Jiliang Hu</a>
|
<a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/p/ping-wang/>Ping Wang</a>
|
<a href=/people/h/haojun-ai/>Haojun Ai</a>
|
<a href=/people/l/lefei-zhang/>Lefei Zhang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--821><div class="card-body p-3 small">The image-based multimodal automatic speech recognition (ASR) model enhances speech recognition performance by incorporating audio-related image. However, some works suggest that introducing image information to model does not help improving ASR performance. In this paper, we propose a novel approach effectively utilizing audio-related image information and set up VHASR, a multimodal speech recognition system that uses vision as hotwords to strengthen the model’s speech recognition capability. Our system utilizes a dual-stream architecture, which firstly transcribes the text on the two streams separately, and then combines the outputs. We evaluate the proposed model on four datasets: Flickr8k, ADE20k, COCO, and OpenImages. The experimental results show that VHASR can effectively utilize key information in images to enhance the model’s speech recognition ability. Its performance not only surpasses unimodal ASR, but also achieves SOTA among existing image-based multimodal ASR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.822.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.822.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.822/>A Probability–Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors</a></strong><br><a href=/people/n/naaman-tan/>Naaman Tan</a>
|
<a href=/people/j/josef-valvoda/>Josef Valvoda</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/a/anej-svete/>Anej Svete</a>
|
<a href=/people/y/yanxia-qin/>Yanxia Qin</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.823.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.823.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--823 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.823 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.823.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.823/>Bridging Local Details and Global Context in Text-Attributed Graphs</a></strong><br><a href=/people/y/yaoke-wang/>Yaoke Wang</a>
|
<a href=/people/y/yun-zhu/>Yun Zhu</a>
|
<a href=/people/w/wenqiao-zhang/>Wenqiao Zhang</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a>
|
<a href=/people/l/liyunfei-liyunfei/>Liyunfei Liyunfei</a>
|
<a href=/people/s/siliang-tang/>Siliang Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--823><div class="card-body p-3 small">Representation learning on text-attributed graphs (TAGs) is vital for real-world applications, as they combine semantic textual and contextual structural information. Research in this field generally consist of two main perspectives: local-level encoding and global-level aggregating, respectively refer to textual node information unification (<span class=tex-math>e.g.</span>, using Language Models) and structure-augmented modeling (<span class=tex-math>e.g.</span>, using Graph Neural Networks). Most existing works focus on combining different information levels but overlook the interconnections, <span class=tex-math>i.e.</span>, the contextual textual information among nodes, which provides semantic insights to bridge local and global levels. In this paper, we propose GraphBridge, a <span class=tex-math>multi-granularity integration</span> framework that bridges local and global perspectives by leveraging contextual textual information, enhancing fine-grained understanding of TAGs. Besides, to tackle scalability and efficiency challenges, we introduce a graph-aware token reduction module. Extensive experiments across various models and datasets show that our method achieves state-of-the-art performance, while our graph-aware token reduction module significantly enhances efficiency and solves scalability issues. Codes are available at https://github.com/wykk00/GraphBridge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.824.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.824.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--824 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.824 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.824/>Building Resources for Emakhuwa: Machine Translation and News Classification Benchmarks</a></strong><br><a href=/people/f/felermino-d-m-a-ali/>Felermino D. M. A. Ali</a>
|
<a href=/people/h/henrique-lopes-cardoso/>Henrique Lopes Cardoso</a>
|
<a href=/people/r/rui-sousa-silva/>Rui Sousa-Silva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--824><div class="card-body p-3 small">This paper introduces a comprehensive collection of NLP resources for Emakhuwa, Mozambique’s most widely spoken language. The resources include the first manually translated news bitext corpus between Portuguese and Emakhuwa, news topic classification datasets, and monolingual data. We detail the process and challenges of acquiring this data and present benchmark results for machine translation and news topic classification tasks. Our evaluation examines the impact of different data types—originally clean text, post-corrected OCR, and back-translated data—and the effects of fine-tuning from pre-trained models, including those focused on African languages.Our benchmarks demonstrate good performance in news topic classification and promising results in machine translation. We fine-tuned multilingual encoder-decoder models using real and synthetic data and evaluated them on our test set and the FLORES evaluation sets. The results highlight the importance of incorporating more data and potential for future improvements.All models, code, and datasets are available in the <a href=https://huggingface.co/LIACC class=acl-markup-url>https://huggingface.co/LIACC</a> repository under the CC BY 4.0 license.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.825.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.825.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--825 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.825 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.825/><span class=acl-fixed-case>R</span>ep<span class=acl-fixed-case>M</span>atch: Quantifying Cross-Instance Similarities in Representation Space</a></strong><br><a href=/people/m/mohammad-reza-modarres/>Mohammad Reza Modarres</a>
|
<a href=/people/s/sina-abbasi/>Sina Abbasi</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--825><div class="card-body p-3 small">Advances in dataset analysis techniques have enabled more sophisticated approaches to analyzing and characterizing training data instances, often categorizing data based on attributes such as “difficulty”. In this work, we introduce RepMatch, a novel method that characterizes data through the lens of similarity.RepMatch quantifies the similarity between subsets of training instances by comparing the knowledge encoded in models trained on them, overcoming the limitations of existing analysis methods that focus solely on individual instances and are restricted to within-dataset analysis.Our framework allows for a broader evaluation, enabling similarity comparisons across arbitrary subsets of instances, supporting both dataset-to-dataset and instance-to-dataset analyses. We validate the effectiveness of RepMatch across multiple NLP tasks, datasets, and models. Through extensive experimentation, we demonstrate that RepMatch can effectively compare datasets, identify more representative subsets of a dataset (that lead to better performance than randomly selected subsets of equivalent size), and uncover heuristics underlying the construction of some challenge datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.826.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.826.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--826 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.826 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.826/>Commonsense Knowledge Editing Based on Free-Text in <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/x/xiusheng-huang/>Xiusheng Huang</a>
|
<a href=/people/y/yequan-wang/>Yequan Wang</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--826><div class="card-body p-3 small">Knowledge editing technology is crucial for maintaining the accuracy and timeliness of large language models (LLMs) . However, the setting of this task overlooks a significant portion of commonsense knowledge based on free-text in the real world, characterized by broad knowledge scope, long content and non instantiation. The editing objects of previous methods (e.g., MEMIT) were single token or entity, which were not suitable for commonsense knowledge in free-text form. To address the aforementioned challenges, we conducted experiments from two perspectives: knowledge localization and knowledge editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT) method, revealing the challenges associated with the distribution of commonsense knowledge in MLP and Attention layers, as well as in decentralized distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which utilizes a Dynamics-aware Module to locate the parameter positions corresponding to commonsense knowledge, and uses Knowledge Editing Module to update knowledge. The DEM method fully explores the potential of the MLP and Attention layers, and successfully edits commonsense knowledge based on free-text. The experimental results indicate that the DEM can achieve excellent editing performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.827.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.827.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--827 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.827 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.827/>A Closer Look at Multidimensional Online Political Incivility</a></strong><br><a href=/people/s/sagi-pendzel/>Sagi Pendzel</a>
|
<a href=/people/n/nir-lotan/>Nir Lotan</a>
|
<a href=/people/a/alon-zoizner/>Alon Zoizner</a>
|
<a href=/people/e/einat-minkov/>Einat Minkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--827><div class="card-body p-3 small">Toxic online political discourse has become prevalent, where scholars debate about its impact to Democratic processes. This work presents a large-scale study of political incivility on Twitter. In line with theories of political communication, we differentiate between harsh ‘impolite’ style and intolerant substance. We present a dataset of 13K political tweets in the U.S. context, which we collected and labeled by those categories using crowd sourcing. Our dataset and results shed light on hostile political discourse focused on partisan conflicts in the U.S. The evaluation of state-of-the-art classifiers illustrates the challenges involved in political incivility detection, which often requires high-level semantic and social understanding. Nevertheless, performing incivility detection at scale, we are able to characterise its distribution across individual users and geopolitical regions, where our findings align and extend existing theories of political communication. In particular, we find that roughly 80% of the uncivil tweets are authored by 20% of the users, where users who are politically engaged are more inclined to use uncivil language. We further find that political incivility exhibits network homophily, and that incivility is more prominent in highly competitive geopolitical regions. Our results apply to both uncivil style and substance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.828.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.828.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--828 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.828 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.828.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.828.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.828/>Leveraging <span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>TFIDF</span> Features for Short Text Clustering via Alignment-Promoting Co-Training</a></strong><br><a href=/people/z/zetong-li/>Zetong Li</a>
|
<a href=/people/q/qinliang-su/>Qinliang Su</a>
|
<a href=/people/s/shijing-si/>Shijing Si</a>
|
<a href=/people/j/jianxing-yu/>Jianxing Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--828><div class="card-body p-3 small">BERT and TFIDF features excel in capturing rich semantics and important words, respectively. Since most existing clustering methods are solely based on the BERT model, they often fall short in utilizing keyword information, which, however, is very useful in clustering short texts. In this paper, we propose a **CO**-**T**raining **C**lustering (**COTC**) framework to make use of the collective strengths of BERT and TFIDF features. Specifically, we develop two modules responsible for the clustering of BERT and TFIDF features, respectively. We use the deep representations and cluster assignments from the TFIDF module outputs to guide the learning of the BERT module, seeking to align them at both the representation and cluster levels. Reversely, we also use the BERT module outputs to train the TFIDF module, thus leading to the mutual promotion. We then show that the alternating co-training framework can be placed under a unified joint training objective, which allows the two modules to be connected tightly and the training signals to be propagated efficiently. Experiments on eight benchmark datasets show that our method outperforms current SOTA methods significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.829.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.829.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--829 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.829 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.829/>Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation</a></strong><br><a href=/people/b/bar-iluz/>Bar Iluz</a>
|
<a href=/people/y/yanai-elazar/>Yanai Elazar</a>
|
<a href=/people/a/asaf-yehudai/>Asaf Yehudai</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--829><div class="card-body p-3 small">Most works on gender bias focus on intrinsic bias — removing traces of information about a protected group from the model’s internal representation. However, these works are often disconnected from the impact of such debiasing on downstream applications, which is the main motivation for debiasing in the first place. In this work, we systematically test how methods for intrinsic debiasing affect neural machine translation models, by measuring the extrinsic bias of such systems under different design choices. We highlight three challenges and mismatches between the debiasing techniques and their end-goal usage, including the choice of embeddings to debias, the mismatch between words and sub-word tokens debiasing, and the effect on different target languages. We find that these considerations have a significant impact on downstream performance and the success of debiasing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.830.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.830.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--830 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.830 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.830/>Unsupervised Named Entity Disambiguation for Low Resource Domains</a></strong><br><a href=/people/d/debarghya-datta/>Debarghya Datta</a>
|
<a href=/people/s/soumajit-pramanik/>Soumajit Pramanik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--830><div class="card-body p-3 small">In the ever-evolving landscape of natural language processing and information retrieval, the need for robust and domain-specific entity linking algorithms has become increasingly apparent. It is crucial in a considerable number of fields such as humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of Named Entity Disambiguation (NED) in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for such scenarios, as they either depend on training data or are not flexible enough to work with domain-specific KBs. Thus in this work, we present a unsupervised approach leveraging the concept of Group Steiner Trees (GST), which can identify the most relevant candidate for entity disambiguation using the contextual similarities across candidate entities for all the mentions present in a document. We outperform the state-of-the-art unsupervised methods by more than 40%(in avg) in terms of Precision@1 and Hit@5 across various domain-specific datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.831.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.831.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--831 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.831 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.831/><span class=acl-fixed-case>S</span>parse<span class=acl-fixed-case>G</span>rad: A Selective Method for Efficient Fine-tuning of <span class=acl-fixed-case>MLP</span> Layers</a></strong><br><a href=/people/v/viktoriia-a-chekalina/>Viktoriia A. Chekalina</a>
|
<a href=/people/a/anna-rudenko/>Anna Rudenko</a>
|
<a href=/people/g/gleb-mezentsev/>Gleb Mezentsev</a>
|
<a href=/people/a/aleksandr-mikhalev/>Aleksandr Mikhalev</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/i/ivan-oseledets/>Ivan Oseledets</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--831><div class="card-body p-3 small">The performance of Transformer models has been enhanced by increasing the number of parameters and the length of the processed text. Consequently, fine-tuning the entire model becomes a memory-intensive process. High-performance methods for parameter-efficient fine-tuning (PEFT) typically work with Attention blocks and often overlook MLP blocks, which contain about half of the model parameters. We propose a new selective PEFT method, namely SparseGrad, that performs well on MLP blocks. We transfer layer gradients to a space where only about 1% of the layer’s elements remain significant. By converting gradients into a sparse structure, we reduce the number of updated parameters. We apply SparseGrad to fine-tune BERT and RoBERTa for the NLU task and LLaMa-2 for the Question-Answering task. In these experiments, with identical memory requirements, our method outperforms LoRA and MeProp, robust popular state-of-the-art PEFT approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.832.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.832.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--832 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.832 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.832.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.832.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.832/><span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>KGC</span>: Momentum Contrast Entity Encoding for Knowledge Graph Completion</a></strong><br><a href=/people/q/qingyang-li/>Qingyang Li</a>
|
<a href=/people/y/yanru-zhong/>Yanru Zhong</a>
|
<a href=/people/y/yuchu-qin/>Yuchu Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--832><div class="card-body p-3 small">In recent years, numerous studies have sought to enhance the capabilities of pretrained language models (PLMs) for Knowledge Graph Completion (KGC) tasks by integrating structural information from knowledge graphs. However, existing approaches have not effectively combined the structural attributes of knowledge graphs with the textual descriptions of entities to generate robust entity encodings.To address this issue, this paper proposes MoCoKGC (Momentum Contrast Entity Encoding for Knowledge Graph Completion), which incorporates three primary encoders: the entity-relation encoder, the entity encoder, and the momentum entity encoder. Momentum contrastive learning not only provides more negative samples but also allows for the gradual updating of entity encodings. Consequently, we reintroduce the generated entity encodings into the encoder to incorporate the graph’s structural information.Additionally, MoCoKGC enhances the inferential capabilities of the entity-relation encoder through deep prompts of relations. On the standard evaluation metric, Mean Reciprocal Rank (MRR), the MoCoKGC model demonstrates superior performance, achieving a 7.1% improvement on the WN18RR dataset and an 11% improvement on the Wikidata5M dataset, while also surpassing the current best model on the FB15k-237 dataset. Through a series of experiments, this paper thoroughly examines the role and contribution of each component and parameter of the model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.833.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.833.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--833 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.833 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.833.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.833/><span class=acl-fixed-case>A</span>ct<span class=acl-fixed-case>P</span>lan-1<span class=acl-fixed-case>K</span>: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities</a></strong><br><a href=/people/y/ying-su/>Ying Su</a>
|
<a href=/people/z/zhan-ling/>Zhan Ling</a>
|
<a href=/people/h/haochen-shi/>Haochen Shi</a>
|
<a href=/people/c/cheng-jiayang/>Cheng Jiayang</a>
|
<a href=/people/y/yauwai-yim/>Yauwai Yim</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--833><div class="card-body p-3 small">Large language models(LLMs) have been adopted to process textual task description and accomplish procedural planning in embodied AI tasks because of their powerful reasoning ability. However, there is still lack of study on how vision language models(VLMs) behave when multi-modal task inputs are considered. Counterfactual planning that evaluates the model’s reasoning ability over alternative task situations are also under exploited. In order to evaluate the planning ability of both multi-modal and counterfactual aspects, we propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark constructed based on ChatGPT and household activity simulator iGibson2. The benchmark consists of 153 activities and 1,187 instances. Each instance describing one activity has a natural language task description and multiple environment images from the simulator. The gold plan of each instance is action sequences over the objects in provided scenes. Both the correctness and commonsense satisfaction are evaluated on typical VLMs. It turns out that current VLMs are still struggling at generating human-level procedural plans for both normal activities and counterfactual activities. We further provide automatic evaluation metrics by finetuning over BLEURT model to facilitate future research on our benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.834.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.834.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--834 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.834 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.834/>Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning</a></strong><br><a href=/people/x/xiaopeng-xie/>Xiaopeng Xie</a>
|
<a href=/people/m/ming-yan/>Ming Yan</a>
|
<a href=/people/x/xiwen-zhou/>Xiwen Zhou</a>
|
<a href=/people/c/chenlong-zhao/>Chenlong Zhao</a>
|
<a href=/people/s/suli-wang/>Suli Wang</a>
|
<a href=/people/y/yong-zhang/>Yong Zhang</a>
|
<a href=/people/j/joey-tianyi-zhou/>Joey Tianyi Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--834><div class="card-body p-3 small">Prompt-based learning paradigm has been shown to be vulnerable to backdoor attacks. Current clean-label attack, employing a specific prompt as trigger, can achieve success without the need for external triggers and ensuring correct labeling of poisoned samples, which are more stealthy compared to the poisoned-label attack, but on the other hand, facing significant issues with false activations and pose greater challenges, necessitating a higher rate of poisoning. Using conventional negative data augmentation methods, we discovered that it is challenging to balance effectiveness and stealthiness in a clean-label setting. In addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut, and posit that this shortcut stems from the contrast between the trigger and the data utilized for poisoning. In this study, we propose a method named Contrastive Shortcut Injection (CSI), by leveraging activation values, integrates trigger design and data selection strategies to craft stronger shortcut features. With extensive experiments on full-shot and few-shot text classification tasks, we empirically validate CSI’s high effectiveness and high stealthiness at low poisoning rates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.835.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.835.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.835/><span class=acl-fixed-case>GRASS</span>: Compute Efficient Low-Memory <span class=acl-fixed-case>LLM</span> Training with Structured Sparse Gradients</a></strong><br><a href=/people/a/aashiq-muhamed/>Aashiq Muhamed</a>
|
<a href=/people/o/oscar-li/>Oscar Li</a>
|
<a href=/people/d/david-woodruff/>David Woodruff</a>
|
<a href=/people/m/mona-diab/>Mona T. Diab</a>
|
<a href=/people/v/virginia-smith/>Virginia Smith</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.836.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.836.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--836 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.836 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.836/><span class=acl-fixed-case>R</span>a<span class=acl-fixed-case>TES</span>core: A Metric for Radiology Report Generation</a></strong><br><a href=/people/w/weike-zhao/>Weike Zhao</a>
|
<a href=/people/c/chaoyi-wu/>Chaoyi Wu</a>
|
<a href=/people/x/xiaoman-zhang/>Xiaoman Zhang</a>
|
<a href=/people/y/ya-zhang/>Ya Zhang</a>
|
<a href=/people/y/yanfeng-wang/>Yanfeng Wang</a>
|
<a href=/people/w/weidi-xie/>Weidi Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--836><div class="card-body p-3 small">This paper introduces a novel, entity-aware metric, termed as Radiological Report (Text) Evaluation (RaTEScore), to assess the quality of medical reports generated by AI models. RaTEScore emphasizes crucial medical entities such as diagnostic outcomes and anatomical details, and is robust against complex medical synonyms and sensitive to negation expressions. Technically, we developed a comprehensive medical NER dataset, RaTE-NER, and trained an NER model specifically for this purpose. This model enables the decomposition of complex radiological reports into constituent medical entities. The metric itself is derived by comparing the similarity of entity embeddings, obtained from a language model, based on their types and relevance to clinical significance. Our evaluations demonstrate that RaTEScore aligns more closely with human preference than existing metrics, validated both on established public benchmarks and our newly proposed RaTE-Eval benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.837.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.837.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--837 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.837 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.837/><span class=acl-fixed-case>H</span>allu<span class=acl-fixed-case>M</span>easure: Fine-grained Hallucination Measurement Using Chain-of-Thought Reasoning</a></strong><br><a href=/people/s/shayan-ali-akbar/>Shayan Ali Akbar</a>
|
<a href=/people/m/md-mosharaf-hossain/>Md Mosharaf Hossain</a>
|
<a href=/people/t/tess-wood/>Tess Wood</a>
|
<a href=/people/s/si-chi-chin/>Si-Chi Chin</a>
|
<a href=/people/e/erica-m-salinas/>Erica M Salinas</a>
|
<a href=/people/v/victor-alvarez/>Victor Alvarez</a>
|
<a href=/people/e/erwin-cornejo/>Erwin Cornejo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--837><div class="card-body p-3 small">Automating the measurement of hallucinations in LLM generated responses is a challenging task as it requires careful investigation of each factual claim in a response. In this paper, we introduce HalluMeasure, a new LLM-based hallucination detection mechanism that decomposes an LLM response into atomic claims, and evaluates each atomic claim against the provided reference context. The model uses a step-by-step reasoning process called Chain-of-Thought and can identify 3 major categories of hallucinations (e.g., contradiction) as well as 10 more specific subtypes (e.g., overgeneralization) which help to identify reasons behind the hallucination errors. Specifically, we explore four different configurations for HalluMeasure’s classifier: with and without CoT prompting, and using a single classifier call to classify all claims versus separate calls for each claim. The best-performing configuration (with CoT and separate calls for each claim) demonstrates significant improvements in detecting hallucinations, achieving a 10-point increase in F1 score on our TechNewsSumm dataset, and a 3-point increase in AUC ROC on the SummEval dataset, compared to three baseline models (RefChecker, AlignScore, and Vectara HHEM). We further show reasonable accuracy on detecting 10 novel error subtypes of hallucinations (where even humans struggle in classification) derived from linguistic analysis of the errors made by the LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.838.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.838.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--838 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.838 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.838/>Learning to Rank Salient Content for Query-focused Summarization</a></strong><br><a href=/people/s/sajad-sotudeh/>Sajad Sotudeh</a>
|
<a href=/people/n/nazli-goharian/>Nazli Goharian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--838><div class="card-body p-3 small">This study examines the potential of integrating Learning-to-Rank (LTR) with Query-focused Summarization (QFS) to enhance the summary relevance via content prioritization. Using a shared secondary decoder with the summarization decoder, we carry out the LTR task at the segment level. Compared to the state-of-the-art, our model outperforms on QMSum benchmark (all metrics) and matches on SQuALITY benchmark (2 metrics) as measured by Rouge and BertScore while offering a lower training overhead. Specifically, on the QMSum benchmark, our proposed system achieves improvements, particularly in Rouge-L (+0.42) and BertScore (+0.34), indicating enhanced understanding and relevance. While facing minor challenges in Rouge-1 and Rouge-2 scores on the SQuALITY benchmark, the model significantly excels in Rouge-L (+1.47), underscoring its capability to generate coherent summaries. Human evaluations emphasize the efficacy of our method in terms of relevance and faithfulness of the generated summaries, without sacrificing fluency. A deeper analysis reveals our model’s superiority over the state-of-the-art for broad queries, as opposed to specific ones, from a qualitative standpoint. We further present an error analysis of our model, pinpointing challenges faced and suggesting potential directions for future research in this field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.839.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.839.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--839 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.839 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.839.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.839.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.839/>Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions</a></strong><br><a href=/people/q/qian-ruan/>Qian Ruan</a>
|
<a href=/people/i/ilia-kuznetsov/>Ilia Kuznetsov</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--839><div class="card-body p-3 small">Classification is a core NLP task architecture with many potential applications. While large language models (LLMs) have brought substantial advancements in text generation, their potential for enhancing classification tasks remains underexplored. To address this gap, we propose a framework for thoroughly investigating fine-tuning LLMs for classification, including both generation- and encoding-based approaches. We instantiate this framework in edit intent classification (EIC), a challenging and underexplored classification task. Our extensive experiments and systematic comparisons with various training approaches and a representative selection of LLMs yield new insights into their application for EIC. We investigate the generalizability of these findings on five further classification tasks. To demonstrate the proposed methods and address the data shortage for empirical edit analysis, we use our best-performing EIC model to create Re3-Sci2.0, a new large-scale dataset of 1,780 scientific document revisions with over 94k labeled edits. The quality of the dataset is assessed through human evaluation. The new dataset enables an in-depth empirical study of human editing behavior in academic writing. We make our experimental framework, models and data publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.840.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.840.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--840 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.840 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.840/><span class=acl-fixed-case>L</span>it<span class=acl-fixed-case>S</span>earch: A Retrieval Benchmark for Scientific Literature Search</a></strong><br><a href=/people/a/anirudh-ajith/>Anirudh Ajith</a>
|
<a href=/people/m/mengzhou-xia/>Mengzhou Xia</a>
|
<a href=/people/a/alexis-chevalier/>Alexis Chevalier</a>
|
<a href=/people/t/tanya-goyal/>Tanya Goyal</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a>
|
<a href=/people/t/tianyu-gao/>Tianyu Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--840><div class="card-body p-3 small">Literature search questions, such as “where can I find research on the evaluation of consistency in generated summaries?” pose significant challenges for modern search engines and retrieval systems. These questions often require a deep understanding of research concepts and the ability to reason over entire articles. In this work, we introduce LitSearch, a retrieval benchmark comprising 597 realistic literature search queries about recent ML and NLP papers. LitSearch is constructed using a combination of (1) questions generated by GPT-4 based on paragraphs containing inline citations from research papers and (2) questions about recently published papers, manually written by their authors. All LitSearch questions were manually examined or edited by experts to ensure high quality. We extensively benchmark state-of-the-art retrieval models and also evaluate two LLM-based reranking pipelines. We find a significant performance gap between BM25 and state-of-the-art dense retrievers, with a 24.8% difference in absolute recall@5. The LLM-based reranking strategies further improve the best-performing dense retriever by 4.4%. Additionally, commercial search engines and research tools like Google Search perform poorly on LitSearch, lagging behind the best dense retriever by 32 points. Taken together, these results show that LitSearch is an informative new testbed for retrieval systems while catering to a real-world use case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.841.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.841.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--841 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.841 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.841/>Open-world Multi-label Text Classification with Extremely Weak Supervision</a></strong><br><a href=/people/x/xintong-li/>Xintong Li</a>
|
<a href=/people/j/jinya-jiang/>Jinya Jiang</a>
|
<a href=/people/r/ria-dharmani/>Ria Dharmani</a>
|
<a href=/people/j/jayanth-srinivasa/>Jayanth Srinivasa</a>
|
<a href=/people/g/gaowen-liu/>Gaowen Liu</a>
|
<a href=/people/j/jingbo-shang/>Jingbo Shang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--841><div class="card-body p-3 small">We study open-world multi-label text classification under extremely weak supervision (XWS), where the user only provides a brief description for classification objectives without any labels or ground-truth label space. Similar single-label XWS settings have been explored recently, however, these methods cannot be easily adapted for multi-label. We observe that (1) most documents have a dominant class covering the majority of content and (2) long-tail labels would appear in some documents as a dominant class. Therefore, we first utilize the user description to prompt a large language model (LLM) for dominant keyphrases of a subset of raw documents, and then construct a (initial) label space via clustering. We further apply a zero-shot multi-label classifier to locate the documents with small top predicted scores, so we can revisit their dominant keyphrases for more long-tail labels. We iterate this process to discover a comprehensive label space and construct a multi-label classifier as a novel method, X-MLClass. X-MLClass exhibits a remarkable increase in ground-truth label space coverage on various datasets, for example, a 40% improvement on the AAPD dataset over topic modeling and keyword extraction methods. Moreover, X-MLClass achieves the best end-to-end multi-label classification accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.842.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.842.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--842 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.842 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.842/><span class=acl-fixed-case>LLM</span>s learn governing principles of dynamical systems, revealing an in-context neural scaling law</a></strong><br><a href=/people/t/toni-j-b-liu/>Toni J.b. Liu</a>
|
<a href=/people/n/nicolas-boulle/>Nicolas Boulle</a>
|
<a href=/people/r/raphael-sarfati/>Raphaël Sarfati</a>
|
<a href=/people/c/christopher-earls/>Christopher Earls</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--842><div class="card-body p-3 small">We study LLMs’ ability to extrapolate the behavior of various dynamical systems, including stochastic, chaotic, continuous, and discrete systems, whose evolution is governed by principles of physical interest. Our results show that LLaMA-2, a language model trained on text, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of a neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.843.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.843.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--843 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.843 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.843.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.843/><span class=acl-fixed-case>AKEW</span>: Assessing Knowledge Editing in the Wild</a></strong><br><a href=/people/x/xiaobao-wu/>Xiaobao Wu</a>
|
<a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a>
|
<a href=/people/l/luu-anh-tuan/>Anh Tuan Luu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--843><div class="card-body p-3 small">Knowledge editing injects knowledge updates into language models to keep them correct and up-to-date. However, its current evaluations deviate significantly from practice: their knowledge updates solely consist of structured facts derived from meticulously crafted datasets, instead of practical sources—unstructured texts like news articles, and they often overlook practical real-world knowledge updates. To address these issues, in this paper we propose AKEW (Assessing Knowledge Editing in the Wild), a new practical benchmark for knowledge editing. AKEW fully covers three editing settings of knowledge updates: structured facts, unstructured texts as facts, and extracted triplets. It further introduces new datasets featuring both counterfactual and real-world knowledge updates. Through extensive experiments, we demonstrate the considerable gap between state-of-the-art knowledge-editing methods and practical scenarios. Our analyses further highlight key insights to motivate future research for practical knowledge editing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.844.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.844.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--844 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.844 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.844/><span class=acl-fixed-case>C</span>opy<span class=acl-fixed-case>B</span>ench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation</a></strong><br><a href=/people/t/tong-chen/>Tong Chen</a>
|
<a href=/people/a/akari-asai/>Akari Asai</a>
|
<a href=/people/n/niloofar-mireshghallah/>Niloofar Mireshghallah</a>
|
<a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/j/james-grimmelmann/>James Grimmelmann</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/p/pang-wei-koh/>Pang Wei Koh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--844><div class="card-body p-3 small">Evaluating the degree of reproduction of copyright-protected content by language models (LMs) is of significant interest to the AI and legal communities. Although both literal and non-literal similarities are considered by courts when assessing the degree of reproduction, prior research has focused only on literal similarities. To bridge this gap, we introduce CopyBench, a benchmark designed to measure both literal and non-literal copying in LM generations. Using copyrighted fiction books as text sources, we provide automatic evaluation protocols to assess literal and non-literal copying, balanced against the model utility in terms of the ability to recall facts from the copyrighted works and generate fluent completions. We find that, although literal copying is relatively rare, two types of non-literal copying—event copying and character copying—occur even in models as small as 7B parameters. Larger models demonstrate significantly more copying, with literal copying rates increasing from 0.2% to 10.5% and non-literal copying from 2.3% to 5.9% when comparing Llama3-8B and 70B models, respectively. We further evaluate the effectiveness of current strategies for mitigating copying and show that (1) training-time alignment can reduce literal copying but may increase non-literal copying, and (2) current inference-time mitigation methods primarily reduce literal but not non-literal copying.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.845.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.845.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--845 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.845 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.845/>Dense <span class=acl-fixed-case>X</span> Retrieval: What Retrieval Granularity Should We Use?</a></strong><br><a href=/people/t/tong-chen/>Tong Chen</a>
|
<a href=/people/h/hongwei-wang/>Hongwei Wang</a>
|
<a href=/people/s/sihao-chen/>Sihao Chen</a>
|
<a href=/people/w/wenhao-yu/>Wenhao Yu</a>
|
<a href=/people/k/kaixin-ma/>Kaixin Ma</a>
|
<a href=/people/x/xinran-zhao/>Xinran Zhao</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--845><div class="card-body p-3 small">Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our experiments reveal that indexing a corpus by fine-grained units such as propositions significantly outperforms passage-level units in retrieval tasks. Moreover, constructing prompts with fine-grained retrieved units for retrieval-augmented language models improves the performance of downstream QA tasks given a specific computation budget.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.846.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.846.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--846 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.846 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.846/>Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach</a></strong><br><a href=/people/y/yanchen-liu/>Yanchen Liu</a>
|
<a href=/people/m/mingyu-derek-ma/>Mingyu Derek Ma</a>
|
<a href=/people/w/wenna-qin/>Wenna Qin</a>
|
<a href=/people/a/azure-zhou/>Azure Zhou</a>
|
<a href=/people/j/jiaao-chen/>Jiaao Chen</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--846><div class="card-body p-3 small">Susceptibility to misinformation describes the degree of belief in unverifiable claims, a latent aspect of individuals’ mental processes that is not observable. Existing susceptibility studies heavily rely on self-reported beliefs, which can be subject to bias, expensive to collect, and challenging to scale for downstream applications. To address these limitations, in this work, we propose a computational approach to efficiently model users’ latent susceptibility levels. As shown in previous work, susceptibility is influenced by various factors (e.g., demographic factors, political ideology), and directly influences people’s reposting behavior on social media. To represent the underlying mental process, our susceptibility modeling incorporates these factors as inputs, guided by the supervision of people’s sharing behavior. Using COVID-19 as a testbed, our experiments demonstrate a significant alignment between the susceptibility scores estimated by our computational modeling and human judgments, confirming the effectiveness of this latent modeling approach. Furthermore, we apply our model to annotate susceptibility scores on a large-scale dataset and analyze the relationships between susceptibility with various factors. Our analysis reveals that political leanings and other psychological factors exhibit varying degrees of association with susceptibility to COVID-19 misinformation, and shows that susceptibility is unevenly distributed across different professional and geographical backgrounds.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.847.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.847.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--847 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.847 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.847/>Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models</a></strong><br><a href=/people/z/zheng-zhao/>Zheng Zhao</a>
|
<a href=/people/y/yftah-ziser/>Yftah Ziser</a>
|
<a href=/people/s/shay-b-cohen/>Shay B Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--847><div class="card-body p-3 small">Fine-tuning pre-trained large language models (LLMs) on a diverse array of tasks has become a common approach for building models that can solve various natural language processing (NLP) tasks. However, where and to what extent these models retain task-specific knowledge remains largely unexplored. This study investigates the task-specific information encoded in pre-trained LLMs and the effects of instruction tuning on their representations across a diverse set of over 60 NLP tasks. We use a set of matrix analysis tools to examine the differences between the way pre-trained and instruction-tuned LLMs store task-specific information. Our findings reveal that while some tasks are already encoded within the pre-trained LLMs, others greatly benefit from instruction tuning. Additionally, we pinpointed the layers in which the model transitions from high-level general representations to more task-oriented representations. This finding extends our understanding of the governing mechanisms of LLMs and facilitates future research in the fields of parameter-efficient transfer learning and multi-task learning. Our code is available at: https://github.com/zsquaredz/layer_by_layer/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.848.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.848.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--848 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.848 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.848/><span class=acl-fixed-case>XD</span>etox: Text Detoxification with Token-Level Toxicity Explanations</a></strong><br><a href=/people/b/beomseok-lee/>Beomseok Lee</a>
|
<a href=/people/h/hyunwoo-kim/>Hyunwoo Kim</a>
|
<a href=/people/k/keon-kim/>Keon Kim</a>
|
<a href=/people/y/yong-suk-choi/>Yong Suk Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--848><div class="card-body p-3 small">Methods for mitigating toxic content through masking and infilling often overlook the decision-making process, leading to either insufficient or excessive modifications of toxic tokens. To address this challenge, we propose XDetox, a novel method that integrates token-level toxicity explanations with the masking and infilling detoxification process. We utilized this approach with two strategies to enhance the performance of detoxification. First, identifying toxic tokens to improve the quality of masking. Second, selecting the regenerated sentence by re-ranking the least toxic sentence among candidates. Our experimental results show state-of-the-art performance across four datasets compared to existing detoxification methods. Furthermore, human evaluations indicate that our method outperforms baselines in both fluency and toxicity reduction. These results demonstrate the effectiveness of our method in text detoxification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.849.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.849.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--849 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.849 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.849/>Optimizing <span class=acl-fixed-case>C</span>hinese Lexical Simplification Across Word Types: A Hybrid Approach</a></strong><br><a href=/people/z/zihao-xiao/>ZiHao Xiao</a>
|
<a href=/people/j/jiefu-gong/>Jiefu Gong</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/w/wei-song/>Wei Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--849><div class="card-body p-3 small">This paper addresses the task of Chinese Lexical Simplification (CLS). A key challenge in CLS is the scarcity of data resources. We begin by evaluating the performance of various language models at different scales in unsupervised and few-shot settings, finding that their effectiveness is sensitive to word types. Expensive large language models (LLMs), such as GPT-4, outperform small models in simplifying complex content words and Chinese idioms from the dictionary.To take advantage of this, we propose an automatic knowledge distillation framework called PivotKD for generating training data to fine-tune small models.In addition, all models face difficulties with out-of-dictionary (OOD) words such as internet slang.To address this, we implement a retrieval-based interpretation augmentation (RIA) strategy, injecting word interpretations from external resources into the context.Experimental results demonstrate that fine-tuned small models outperform GPT-4 in simplifying complex content words and Chinese idioms. Additionally, the RIA strategy enhances the performance of most models, particularly in handling OOD words. Our findings suggest that a hybrid approach could optimize CLS performance while managing inference costs. This would involve configuring choices such as model scale, linguistic resources, and the use of RIA based on specific word types to strike an ideal balance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.850.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.850.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--850 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.850 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.850/>Control Large Language Models via Divide and Conquer</a></strong><br><a href=/people/b/bingxuan-li/>Bingxuan Li</a>
|
<a href=/people/y/yiwei-wang/>Yiwei Wang</a>
|
<a href=/people/t/tao-meng/>Tao Meng</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--850><div class="card-body p-3 small">This paper investigates the capability of LLMs on controllable generation with prompt-based controlling, focusing on Lexically Constrained Generation (LCG). We systematically evaluate the performance of LLMs on satisfying lexical constraints with prompt-based controlling, as well as their efficacy in downstream applications. We identified three key reasons that highlight the limitations of LLMs in LCG, including (1) position bias, where LLMs tend to satisfy constraints that appear in specific positions within the input; (2) low responsiveness to control decoding parameters, which minimally impact the performance of LLMs; and (3) struggle with handling the inherent complexity of certain constraints (e.g. compound word). We conclude that black-box LLMs face significant challenges in consistently satisfying lexical constraints with prompt-based controlling. To address this bottleneck, we introduce the Divide and Conquer Generation strategy, effective for both white-box and black-box LLMs, to enhance LLMs performance in LCG tasks, which demonstrates over 90% improvement on success rate in the most challenging LCG task. Our analysis aims to provide valuable insights into the performance of LLMs in LCG with prompt-based controlling, and our proposed strategy offers a pathway to more sophisticated and customized text generation applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.851.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.851.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--851 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.851 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.851.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.851/>Joint Pre-Encoding Representation and Structure Embedding for Efficient and Low-Resource Knowledge Graph Completion</a></strong><br><a href=/people/c/chenyu-qiu/>Chenyu Qiu</a>
|
<a href=/people/p/pengjiang-qian/>Pengjiang Qian</a>
|
<a href=/people/c/chuang-wang/>Chuang Wang</a>
|
<a href=/people/j/jian-yao/>Jian Yao</a>
|
<a href=/people/l/li-liu/>Li Liu</a>
|
<a href=/people/f/fang-wei/>Fang Wei</a>
|
<a href=/people/e/eddie-y-k-eddie/>Eddie Y.k. Eddie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--851><div class="card-body p-3 small">Knowledge graph completion (KGC) aims to infer missing or incomplete parts in knowledge graph. The existing models are generally divided into structure-based and description-based models, among description-based models often require longer training and inference times as well as increased memory usage. In this paper, we propose Pre-Encoded Masked Language Model (PEMLM) to efficiently solve KGC problem. By encoding textual descriptions into semantic representations before training, the necessary resources are significantly reduced. Furthermore, we introduce a straightforward but effective fusion framework to integrate structural embedding with pre-encoded semantic description, which enhances the model’s prediction performance on 1-N relations. The experimental results demonstrate that our proposed strategy attains state-of-the-art performance on the WN18RR (MRR+5.4% and Hits@1+6.4%) and UMLS datasets. Compared to existing models, we have increased inference speed by 30x and reduced training memory by approximately 60%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.852.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.852.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--852 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.852 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.852/>Improving Discriminative Capability of Reward Models in <span class=acl-fixed-case>RLHF</span> Using Contrastive Learning</a></strong><br><a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/r/rui-zheng/>Rui Zheng</a>
|
<a href=/people/b/binghai-wang/>Binghai Wang</a>
|
<a href=/people/s/senjie-jin/>Senjie Jin</a>
|
<a href=/people/c/caishuang-huang/>Caishuang Huang</a>
|
<a href=/people/j/junjie-ye/>Junjie Ye</a>
|
<a href=/people/z/zhihao-zhang/>Zhihao Zhang</a>
|
<a href=/people/y/yuhao-zhou/>Yuhao Zhou</a>
|
<a href=/people/z/zhiheng-xi/>Zhiheng Xi</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--852><div class="card-body p-3 small">Reinforcement Learning from Human Feedback (RLHF) is a crucial approach to aligning language models with human values and intentions. A fundamental challenge in this method lies in ensuring that the reward model accurately understands and evaluates human preferences. Current methods rely on ranking losses to teach the reward model to assess preferences, but they are susceptible to noise and ambiguous data, often failing to deeply understand human intentions. To address this issue, we introduce contrastive learning into the reward modeling process. In addition to supervised ranking loss, we introduce an unsupervised contrastive loss to enable the reward model to fully capture the distinctions in contrastive data. Experimental results demonstrate that the proposed contrastive learning-based reward modeling method effectively enhances the generalization of the reward model, stabilizes the reinforcement learning training process, and improves the final alignment with human preferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.853.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.853.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--853 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.853 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.853/><span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>CEL</span>: Advancing Table Entity Linking through Distinctive Row and Column Contexts</a></strong><br><a href=/people/y/yuanzheng-wang/>Yuanzheng Wang</a>
|
<a href=/people/y/yixing-fan/>Yixing Fan</a>
|
<a href=/people/j/jiafeng-guo/>Jiafeng Guo</a>
|
<a href=/people/r/ruqing-zhang/>Ruqing Zhang</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--853><div class="card-body p-3 small">Table entity linking (TEL) aims to map entity mentions in the table to their corresponding entities in a knowledge base (KB). The core of this task is to leverage structured contexts, specifically row and column contexts, to enhance the semantics of mentions in entity disambiguation. Most entity linking (EL) methods primarily focus on understanding sequential text contexts, making it difficult to adapt to the row and column structure of tables. Additionally, existing methods for TEL indiscriminately mix row and column contexts together, overlooking their semantic differences. In this paper, we explicitly distinguish the modeling of row and column contexts, and propose a method called RoCEL to capture their distinct semantics. Specifically, for row contexts in tables, we take the attention mechanism to learn the implicit relational dependencies between each cell and the mention. For column contexts in tables, we employ a set-wise encoder to learn the categorical information about the group of mentions. At last, we merge both contexts to obtain the final mention embedding for link prediction. Experiments on four benchmarks show that our approach outperforms the state-of-the-art (SOTA) baseline by about 1.5% on the in-domain dataset, and by 3.7% on average across three out-of-domain datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.854.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.854.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--854 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.854 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.854/>Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models</a></strong><br><a href=/people/z/ziou-zheng/>Zi’ou Zheng</a>
|
<a href=/people/c/christopher-malon/>Christopher Malon</a>
|
<a href=/people/m/martin-renqiang-min/>Martin Renqiang Min</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--854><div class="card-body p-3 small">When performing complex multi-step reasoning tasks, the ability of Large Language Models (LLMs) to derive structured intermediate proof steps is important for ensuring that the models truly perform the desired reasoning and for improving models’ explainability. This paper is centred around a focused study: whether the current state-of-the-art generalist LLMs can leverage the structures in a few examples to better construct the proof structures with in-context learning. Our study specifically focuses on structure-aware demonstration and structure-aware pruning. We demonstrate that they both help improve performance. A detailed analysis is provided to help understand the results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.855.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.855.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--855 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.855 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.855/>Efficient Overshadowed Entity Disambiguation by Mitigating Shortcut Learning</a></strong><br><a href=/people/p/panuthep-tasawong/>Panuthep Tasawong</a>
|
<a href=/people/p/peerat-limkonchotiwat/>Peerat Limkonchotiwat</a>
|
<a href=/people/p/potsawee-manakul/>Potsawee Manakul</a>
|
<a href=/people/c/can-udomcharoenchaikit/>Can Udomcharoenchaikit</a>
|
<a href=/people/e/ekapol-chuangsuwanich/>Ekapol Chuangsuwanich</a>
|
<a href=/people/s/sarana-nutanong/>Sarana Nutanong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--855><div class="card-body p-3 small">Entity disambiguation (ED) is crucial in natural language processing (NLP) for tasks such as question-answering and information extraction. A major challenge in ED is handling overshadowed entities—uncommon entities sharing mention surfaces with common entities. The current approach to enhance performance on these entities involves reasoning over facts in a knowledge base (KB), increasing computational overhead during inference. We argue that the ED performance on overshadowed entities can be enhanced during training by addressing shortcut learning, which does not add computational overhead at inference. We propose a simple yet effective debiasing technique to prevent models from shortcut learning during training. Experiments on a range of ED datasets show that our method achieves state-of-the-art performance without compromising inference speed. Our findings suggest a new research direction for improving entity disambiguation via shortcut learning mitigation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.856.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.856.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--856 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.856 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.856/><span class=acl-fixed-case>A</span>pp<span class=acl-fixed-case>B</span>ench: Planning of Multiple <span class=acl-fixed-case>API</span>s from Various <span class=acl-fixed-case>APP</span>s for Complex User Instruction</a></strong><br><a href=/people/h/hongru-wang/>Hongru Wang</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/b/boyang-xue/>Boyang Xue</a>
|
<a href=/people/h/heming-xia/>Heming Xia</a>
|
<a href=/people/j/jingtao-cao/>Jingtao Cao</a>
|
<a href=/people/z/zeming-liu/>Zeming Liu</a>
|
<a href=/people/j/jeff-z-pan/>Jeff Z. Pan</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--856><div class="card-body p-3 small">Large Language Models (LLMs) can interact with the real world by connecting with versatile external APIs, resulting in better problem-solving and task automation capabilities. Previous research primarily either focuses on APIs with limited arguments from a single source or overlooks the complex dependency relationship between different APIs. However, it is essential to utilize multiple APIs collaboratively from various sources, especially for complex user instructions. In this paper, we introduce MetaBench, the first benchmark to evaluate LLMs’ ability to plan and execute multiple APIs from various sources in order to complete the user’s task. Specifically, we consider two significant challenges in multiple APIs: 1) graph structures: some APIs can be executed independently while others need to be executed one by one, resulting in graph-like execution order; and 2) permission constraints: which source is authorized to execute the API call. We have experimental results on 9 distinct LLMs; e.g., GPT-4o achieves only a 2.0% success rate at the most complex instruction, revealing that the existing state-of-the-art LLMs still cannot perform well in this situation even with the help of in-context learning and finetuning. Our code and data are publicly available at <a href=https://github.com/ruleGreen/AppBench class=acl-markup-url>https://github.com/ruleGreen/AppBench</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.857.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.857.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--857 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.857 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.857.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.857/>Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment</a></strong><br><a href=/people/z/zhipeng-chen/>Zhipeng Chen</a>
|
<a href=/people/k/kun-zhou/>Kun Zhou</a>
|
<a href=/people/w/wayne-xin-zhao/>Xin Zhao</a>
|
<a href=/people/j/jingyuan-wang/>Jingyuan Wang</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--857><div class="card-body p-3 small">Large language models (LLMs) are still struggling in aligning with human preference in complex tasks and scenarios. They are prone to overfit into the unexpected patterns or superficial styles in the training data. We conduct an empirical study that only selects the top-10% most updated parameters in LLMs for alignment training, and see improvements in the convergence process and final performance. It indicates the existence of redundant neurons in LLMs for alignment training. To reduce its influence, we propose a low-redundant alignment method named **ALLO**, focusing on optimizing the most related neurons with the most useful supervised signals. Concretely, we first identify the neurons that are related to the human preference data by a gradient-based strategy, then identify the alignment-related key tokens by reward models for computing loss. Besides, we also decompose the alignment process into the forgetting and learning stages, where we first forget the tokens with unaligned knowledge and then learn aligned knowledge, by updating different ratios of neurons, respectively. Experimental results on 10 datasets have shown the effectiveness of ALLO. Our code and data will be publicly released.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.858.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.858.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--858 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.858 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.858/><span class=acl-fixed-case>A</span>udio<span class=acl-fixed-case>VSR</span>: Enhancing Video Speech Recognition with Audio Data</a></strong><br><a href=/people/x/xiaoda-yang/>Xiaoda Yang</a>
|
<a href=/people/x/xize-cheng/>Xize Cheng</a>
|
<a href=/people/j/jiaqi-duan/>Jiaqi Duan</a>
|
<a href=/people/h/hongshun-qiu/>Hongshun Qiu</a>
|
<a href=/people/m/minjie-hong/>Minjie Hong</a>
|
<a href=/people/m/minghui-fang/>Minghui Fang</a>
|
<a href=/people/s/shengpeng-ji/>Shengpeng Ji</a>
|
<a href=/people/j/jialong-zuo/>Jialong Zuo</a>
|
<a href=/people/z/zhiqing-hong/>Zhiqing Hong</a>
|
<a href=/people/z/zhimeng-zhang/>Zhimeng Zhang</a>
|
<a href=/people/t/tao-jin/>Tao Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--858><div class="card-body p-3 small">Visual Speech Recognition (VSR) aims to predict spoken content by analyzing lip movements in videos. Recently reported state-of-the-art results in VSR often rely on increasingly large amounts of video data, while the publicly available transcribed video datasets are insufficient compared to the audio data. To further enhance the VSR model using the audio data, we employed a generative model for data inflation, integrating the synthetic data with the authentic visual data. Essentially, the generative model incorporates another insight, which enhances the capabilities of the recognition model. For the cross-language issue, previous work has shown poor performance with non-Indo-European languages. We trained a multi-language-family modal fusion model, AudioVSR. Leveraging the concept of modal transfer, we achieved significant results in downstream VSR tasks under conditions of data scarcity. To the best of our knowledge, AudioVSR represents the first work on cross-language-family audio-lip alignment, achieving a new SOTA in the cross-language scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.859.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.859.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--859 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.859 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.859.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.859/><span class=acl-fixed-case>ECCO</span>: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?</a></strong><br><a href=/people/s/siddhant-waghjale/>Siddhant Waghjale</a>
|
<a href=/people/v/vishruth-veerendranath/>Vishruth Veerendranath</a>
|
<a href=/people/z/zhiruo-wang/>Zhiruo Wang</a>
|
<a href=/people/d/daniel-fried/>Daniel Fried</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--859><div class="card-body p-3 small">Although large language models (LLMs) have been largely successful in generating functionally correct programs, conditioning models to produce efficient solutions while ensuring correctness remains a challenge. Further, unreliability in benchmarking code efficiency is a hurdle across varying hardware specifications for popular interpreted languages such as Python. In this paper, we present ECCO, a reproducible benchmark for evaluating program efficiency via two paradigms: natural language (NL) based code generation and history-based code editing. On ECCO, we adapt and thoroughly investigate the three most promising existing LLM-based approaches: in-context learning, iterative refinement with execution or NL feedback, and fine-tuning conditioned on execution and editing history. While most methods degrade functional correctness and moderately increase program efficiency, we find that adding execution information often helps maintain functional correctness, and NL feedback enhances more on efficiency. We release our benchmark to support future work on LLM-based generation of efficient code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.860.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.860.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--860 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.860 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.860/>Ladder: A Model-Agnostic Framework Boosting <span class=acl-fixed-case>LLM</span>-based Machine Translation to the Next Level</a></strong><br><a href=/people/z/zhaopeng-feng/>Zhaopeng Feng</a>
|
<a href=/people/r/ruizhe-chen/>Ruizhe Chen</a>
|
<a href=/people/y/yan-zhang/>Yan Zhang</a>
|
<a href=/people/z/zijie-meng/>Zijie Meng</a>
|
<a href=/people/z/zuozhu-liu/>Zuozhu Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--860><div class="card-body p-3 small">General-purpose Large Language Models (LLMs) like GPT-4 have achieved remarkable advancements in machine translation (MT) by leveraging extensive web content. On the other hand, translation-specific LLMs are built by pre-training on domain-specific monolingual corpora and fine-tuning with human-annotated translation data. Despite the superior performance, these methods either demand an unprecedented scale of computing and data or substantial human editing and annotation efforts. In this paper, we develop MT-Ladder, a novel model-agnostic and cost-effective tool to refine the performance of general LLMs for MT. MT-Ladder is trained on pseudo-refinement triplets which can be easily obtained from existing LLMs without additional human cost. During training, we propose a hierarchical fine-tuning strategy with an easy-to-hard schema, improving MT-Ladder’s refining performance progressively. The trained MT-Ladder can be seamlessly integrated with any general-purpose LLMs to boost their translation performance. By utilizing Gemma-2B/7B as the backbone, MT-Ladder-2B can elevate raw translations to the level of top-tier open-source models (e.g., refining BigTranslate-13B with +6.91 BLEU and +3.52 COMET for XX→En), and MT-Ladder-7B can further enhance model performance to be on par with the state-of-the-art GPT-4. Extensive ablation and analysis corroborate the effectiveness of MT-Ladder in diverse settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.861.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.861.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--861 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.861 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.861/>Re-<span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>ST</span>: Reflection-Reinforced Self-Training for Language Agents</a></strong><br><a href=/people/z/zi-yi-dou/>Zi-Yi Dou</a>
|
<a href=/people/c/cheng-fu-yang/>Cheng-Fu Yang</a>
|
<a href=/people/x/xueqing-wu/>Xueqing Wu</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--861><div class="card-body p-3 small">Finetuning language agents with reasoning-action trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical. In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations. Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks. To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a <i>reflector</i> to refine low-quality generated samples during self-training. The reflector takes the agent’s output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples. This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples. We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6% on HotpotQA and 28.4% on AlfWorld, and Re-ReST further boosting performance by 2.0% and 14.1%, respectively. Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training. Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.862.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.862.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--862 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.862 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.862/>Effective Synthetic Data and Test-Time Adaptation for <span class=acl-fixed-case>OCR</span> Correction</a></strong><br><a href=/people/s/shuhao-guan/>Shuhao Guan</a>
|
<a href=/people/c/cheng-xu/>Cheng Xu</a>
|
<a href=/people/m/moule-lin/>Moule Lin</a>
|
<a href=/people/d/derek-greene/>Derek Greene</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--862><div class="card-body p-3 small">Post-OCR technology is used to correct errors in the text produced by OCR systems. This study introduces a method for constructing post-OCR synthetic data with different noise levels using weak supervision. We define Character Error Rate (CER) thresholds for “effective” and “ineffective” synthetic data, allowing us to create more useful multi-noise level synthetic datasets. Furthermore, we propose Self-Correct-Noise Test-Time Adaptation (SCN-TTA), which combines self-correction and noise generation mechanisms. SCN-TTA allows a model to dynamically adjust to test data without relying on labels, effectively handling proper nouns in long texts and further reducing CER. In our experiments we evaluate a range of models, including multiple PLMs and LLMs. Results indicate that our method yields models that are effective across diverse text types. Notably, the ByT5 model achieves a CER reduction of 68.67% without relying on manually annotated data</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.863.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.863.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--863 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.863 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.863/><span class=acl-fixed-case>SRF</span>: Enhancing Document-Level Relation Extraction with a Novel Secondary Reasoning Framework</a></strong><br><a href=/people/f/fu-zhang/>Fu Zhang</a>
|
<a href=/people/q/qi-miao/>Qi Miao</a>
|
<a href=/people/j/jingwei-cheng/>Jingwei Cheng</a>
|
<a href=/people/h/hongsen-yu/>Hongsen Yu</a>
|
<a href=/people/y/yi-yan/>Yi Yan</a>
|
<a href=/people/x/xin-li/>Xin Li</a>
|
<a href=/people/y/yongxue-wu/>Yongxue Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--863><div class="card-body p-3 small">Document-level Relation Extraction (DocRE) aims to extract relations between entity pairs in a document and poses many challenges as it involves multiple mentions of entities and cross-sentence inference. However, several aspects that are important for DocRE have not been considered and explored. Existing work ignores bidirectional mention interaction when generating relational features for entity pairs. Also, sophisticated neural networks are typically designed for cross-sentence evidence extraction to further enhance DocRE. More interestingly, we reveal a noteworthy finding: If a model has predicted a relation between an entity and other entities, this relation information may help infer and predict more relations between the entity’s adjacent entities and these other entities. Nonetheless, none of existing methods leverage secondary reasoning to exploit results of relation prediction. To this end, we propose a novel Secondary Reasoning Framework (SRF) for DocRE. In SRF, we initially propose a DocRE model that incorporates bidirectional mention fusion and a simple yet effective evidence extraction module (incurring only an additional learnable parameter overhead) for relation prediction. Further, for the first time, we elaborately design and propose a novel secondary reasoning method to discover more relations by exploring the results of the first relation prediction. Extensive experiments show that SRF achieves SOTA performance and our secondary reasoning method is both effective and general when integrated into existing models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.864.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.864.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--864 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.864 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.864/><span class=acl-fixed-case>F</span>ine<span class=acl-fixed-case>C</span>ops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension</a></strong><br><a href=/people/j/junzhuo-liu/>Junzhuo Liu</a>
|
<a href=/people/x/xuzheng-yang/>Xuzheng Yang</a>
|
<a href=/people/w/weiwei-li/>Weiwei Li</a>
|
<a href=/people/p/peng-wang/>Peng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--864><div class="card-body p-3 small">Referring Expression Comprehension (REC) is a crucial cross-modal task that objectively evaluates the capabilities of language understanding, image comprehension, and language-to-image grounding. Consequently, it serves as an ideal testing ground for Multi-modal Large Language Models (MLLMs). In pursuit of this goal, we have established a new REC dataset characterized by two key features: Firstly, it is designed with controllable varying levels of difficulty, necessitating multi-level fine-grained reasoning across object categories, attributes, and multi-hop relationships. Secondly, it includes negative text and images created through fine-grained editing and generation based on existing data, thereby testing the model’s ability to correctly reject scenarios where the target object is not visible in the image—an essential aspect often overlooked in existing datasets and approaches. Utilizing this high-quality dataset, we conducted comprehensive evaluations of both state-of-the-art specialist models and MLLMs. Our findings indicate that there remains a significant gap in achieving satisfactory grounding performance. We anticipate that our dataset will inspire new approaches to enhance visual reasoning and develop more advanced cross-modal interaction strategies, ultimately unlocking the full potential of MLLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.865.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.865.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--865 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.865 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.865/>Exploring the Learning Capabilities of Language Models using <span class=acl-fixed-case>LEVERWORLDS</span></a></strong><br><a href=/people/e/eitan-wagner/>Eitan Wagner</a>
|
<a href=/people/a/amir-feder/>Amir Feder</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--865><div class="card-body p-3 small">Learning a model of a stochastic setting often involves learning both general structure rules and specific properties of the instance. This paper investigates the interplay between learning the general and the specific in various learning methods, with emphasis on sample efficiency. We design a framework called LEVERWORLDS, which allows the generation of simple physics-inspired worlds that follow a similar generative process with different distributions, and their instances can be expressed in natural language. These worlds allow for controlled experiments to assess the sample complexity of different learning methods. We experiment with classic learning algorithms as well as Transformer language models, both with fine-tuning and In-Context Learning (ICL). Our general finding is that (1) Transformers generally succeed in the task; but (2) they are considerably less sample efficient than classic methods that make stronger assumptions about the structure, such as Maximum Likelihood Estimation and Logistic Regression. This finding is in tension with the recent tendency to use Transformers as general-purpose estimators. We propose an approach that leverages the ICL capabilities of contemporary language models to apply simple algorithms for this type of data. Our experiments show that models currently struggle with the task but show promising potential.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.866.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.866.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--866 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.866 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.866/><span class=acl-fixed-case>CONTESTS</span>: a Framework for Consistency Testing of Span Probabilities in Language Models</a></strong><br><a href=/people/e/eitan-wagner/>Eitan Wagner</a>
|
<a href=/people/y/yuli-slavutsky/>Yuli Slavutsky</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--866><div class="card-body p-3 small">Although language model scores are often treated as probabilities, their reliability as probability estimators has mainly been studied through calibration, overlooking other aspects. In particular, it is unclear whether language models produce the same value for different ways of assigning joint probabilities to word spans. Our work introduces a novel framework, ConTestS (Consistency Testing over Spans), involving statistical tests to assess score consistency across interchangeable completion and conditioning orders. We conduct experiments on post-release real and synthetic data to eliminate training effects. Our findings reveal that both Masked Language Models (MLMs) and autoregressive models exhibit inconsistent predictions, with autoregressive models showing larger discrepancies. Larger MLMs tend to produce more consistent predictions, while autoregressive models show the opposite trend. Moreover, for both model types, prediction entropies offer insights into the true word span likelihood and therefore can aid in selecting optimal decoding strategies. The inconsistencies revealed by our analysis, as well their connection to prediction entropies and differences between model types, can serve as useful guides for future research on addressing these limitations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.867.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.867.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--867 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.867 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.867/><span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>E</span>dit-v2: Document Structure Editing Via Multimodal <span class=acl-fixed-case>LLM</span> Grounding</a></strong><br><a href=/people/m/manan-suri/>Manan Suri</a>
|
<a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/r/rajiv-jain/>Rajiv Jain</a>
|
<a href=/people/v/vlad-i-morariu/>Vlad I Morariu</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/d/dinesh-manocha/>Dinesh Manocha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--867><div class="card-body p-3 small">Document structure editing involves manipulating localized textual, visual, and layout components in document images based on the user’s requests. Past works have shown that multimodal grounding of user requests in the document image and identifying the accurate structural components and their associated attributes remain key challenges for this task. To address these, we introduce the DocEditAgent, a novel framework that performs end-to-end document editing by leveraging Large Multimodal Models (LMMs). It consists of three novel components – (1) Doc2Command to simultaneously localize edit regions of interest (RoI) and disambiguate user edit requests into edit commands. (2) LLM-based Command Reformulation prompting to tailor edit commands originally intended for specialized software into edit instructions suitable for generalist LMMs. (3) Moreover, DocEditAgent processes these outputs via Large Multimodal Models like GPT-4V and Gemini, to parse the document layout, execute edits on grounded Region of Interest (RoI), and generate the edited document image. Extensive experiments on the DocEdit dataset show that DocEditAgent significantly outperforms strong baselines on edit command generation (2-33%), RoI bounding box detection (12-31%), and overall document editing (1-12%) tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.868.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.868.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--868 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.868 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.868/><span class=acl-fixed-case>D</span>oge<span class=acl-fixed-case>RM</span>: Equipping Reward Models with Domain Knowledge through Model Merging</a></strong><br><a href=/people/t/tzu-han-lin/>Tzu-Han Lin</a>
|
<a href=/people/c/chen-an-li/>Chen-An Li</a>
|
<a href=/people/h/hung-yi-lee/>Hung-yi Lee</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--868><div class="card-body p-3 small">Reinforcement learning from human feedback (RLHF) is a popular strategy for aligning large language models (LLMs) with desired behaviors. Reward modeling is a crucial step in RLHF. However, collecting paired preference data for training reward models is often costly and time-consuming, especially for domain-specific preferences requiring expert annotation. To address this challenge, we propose the **Do**main knowled**ge** merged **R**eward **M**odel (**DogeRM**), a novel framework that integrates domain-specific knowledge into a general reward model by model merging. The experiments demonstrate that DogeRM enhances performance across different benchmarks and provide a detailed analysis showcasing the effects of model merging, showing the great potential of facilitating model alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.869.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.869.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--869 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.869 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.869/>Understanding Slang with <span class=acl-fixed-case>LLM</span>s: Modelling Cross-Cultural Nuances through Paraphrasing</a></strong><br><a href=/people/i/ifeoluwa-wuraola/>Ifeoluwa Wuraola</a>
|
<a href=/people/n/nina-dethlefs/>Nina Dethlefs</a>
|
<a href=/people/d/daniel-marciniak/>Daniel Marciniak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--869><div class="card-body p-3 small">In the realm of social media discourse, the integration of slang enriches communication, reflecting the sociocultural identities of users. This study investigates the capability of large language models (LLMs) to paraphrase slang within climate-related tweets from Nigeria and the UK, with a focus on identifying emotional nuances. Using DistilRoBERTa as the base-line model, we observe its limited comprehension of slang. To improve cross-cultural understanding, we gauge the effectiveness of leading LLMs ChatGPT 4, Gemini, and LLaMA3 in slang paraphrasing. While ChatGPT 4 and Gemini demonstrate comparable effectiveness in slang paraphrasing, LLaMA3 shows less coverage, with all LLMs exhibiting limitations in coverage, especially of Nigerian slang. Our findings underscore the necessity for culturally sensitive LLM development in emotion classification, particularly in non-anglocentric regions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.870.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.870.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--870 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.870 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.870/>Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding</a></strong><br><a href=/people/l/lifu-tu/>Lifu Tu</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/j/jin-qu/>Jin Qu</a>
|
<a href=/people/j/jiacheng-xu/>Jiacheng Xu</a>
|
<a href=/people/r/rui-meng/>Rui Meng</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/y/yingbo-zhou/>Yingbo Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--870><div class="card-body p-3 small">Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 2020), and factual correctness in question-answering (Gao et al., 2023).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.871.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.871.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--871 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.871 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.871/>Re-Reading Improves Reasoning in Large Language Models</a></strong><br><a href=/people/x/xiaohan-xu/>Xiaohan Xu</a>
|
<a href=/people/c/chongyang-tao/>Chongyang Tao</a>
|
<a href=/people/t/tao-shen/>Tao Shen</a>
|
<a href=/people/c/can-xu/>Can Xu</a>
|
<a href=/people/h/hongbo-xu/>Hongbo Xu</a>
|
<a href=/people/g/guodong-long/>Guodong Long</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/s/shuai-ma/>Shuai Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--871><div class="card-body p-3 small">To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, RE2, i.e., Re-Reading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, RE2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, RE2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, RE2 facilitates a “bidirectional” encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of RE2, illustrating its potential to enable “bidirectional” attention mechanisms. We then evaluate RE2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, RE2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal RE2’s adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.872.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.872.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--872 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.872 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.872/>Adaptive Axes: A Pipeline for In-domain Social Stereotype Analysis</a></strong><br><a href=/people/q/qingcheng-zeng/>Qingcheng Zeng</a>
|
<a href=/people/m/mingyu-jin/>Mingyu Jin</a>
|
<a href=/people/r/rob-voigt/>Rob Voigt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--872><div class="card-body p-3 small">Prior work has explored the possibility of using the semantic information obtained from embedding representations to quantify social stereotypes, leveraging techniques such as word embeddings combined with a list of traits (Garg et al., 2018; Charlesworth et al., 2022) or semantic axes (An et al., 2018; Lucy et al., 2022). However, these approaches have struggled to fully capture the variability in stereotypes across different conceptual domains for the same social group (e.g., black in science, health, and art), in part because the identity of a word and the associations formed during pre-training can dominate its contextual representation (Field and Tsvetkov, 2019). This study explores the ability to recover stereotypes from the contexts surrounding targeted entities by utilizing state-of-the-art text embedding models and adaptive semantic axes enhanced by large language models (LLMs). Our results indicate that the proposed pipeline not only surpasses token-based methods in capturing in-domain framing but also effectively tracks stereotypes over time and along domain-specific semantic axes for in-domain texts. Our research highlights the potential of employing text embedding models to achieve a deeper understanding of nuanced social stereotypes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.873.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.873.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--873 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.873 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.873/><span class=acl-fixed-case>ERVQA</span>: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments</a></strong><br><a href=/people/s/sourjyadip-ray/>Sourjyadip Ray</a>
|
<a href=/people/k/kushal-gupta/>Kushal Gupta</a>
|
<a href=/people/s/soumi-kundu/>Soumi Kundu</a>
|
<a href=/people/d/dr-payal-arvind-kasat/>Dr Payal Arvind Kasat</a>
|
<a href=/people/s/somak-aditya/>Somak Aditya</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--873><div class="card-body p-3 small">The global shortage of healthcare workers has demanded the development of smart healthcare assistants, which can help monitor and alert healthcare workers when necessary. We examine the healthcare knowledge of existing Large Vision Language Models (LVLMs) via the Visual Question Answering (VQA) task in hospital settings through expert annotated open-ended questions. We introduce the Emergency Room Visual Question Answering (ERVQA) dataset, consisting of &lt;image, question, answer> triplets covering diverse emergency room scenarios, a seminal benchmark for LVLMs. By developing a detailed error taxonomy and analyzing answer trends, we reveal the nuanced nature of the task. We benchmark state-of-the-art open-source and closed LVLMs using traditional and adapted VQA metrics: Entailment Score and CLIPScore Confidence. Analyzing errors across models, we infer trends based on properties like decoder type, model size, and in-context examples. Our findings suggest the ERVQA dataset presents a highly complex task, highlighting the need for specialized, domain-specific solutions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.874.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.874.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--874 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.874 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.874/>Human-<span class=acl-fixed-case>LLM</span> Hybrid Text Answer Aggregation for Crowd Annotations</a></strong><br><a href=/people/j/jiyi-li/>Jiyi Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--874><div class="card-body p-3 small">The quality is a crucial issue for crowd annotations. Answer aggregation is an important type of solution. The aggregated answers estimated from multiple crowd answers to the same instance are the eventually collected annotations, rather than the individual crowd answers themselves. Recently, the capability of Large Language Models (LLMs) on data annotation tasks has attracted interest from researchers. Most of the existing studies mainly focus on the average performance of individual crowd workers; several recent works studied the scenarios of aggregation on categorical labels and LLMs used as label creators. However, the scenario of aggregation on text answers and the role of LLMs as aggregators are not yet well-studied. In this paper, we investigate the capability of LLMs as aggregators in the scenario of close-ended crowd text answer aggregation. We propose a human-LLM hybrid text answer aggregation method with a Creator-Aggregator Multi-Stage (CAMS) crowdsourcing framework. We make the experiments based on public crowdsourcing datasets. The results show the effectiveness of our approach based on the collaboration of crowd workers and LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.875.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.875.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--875 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.875 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.875/>Improve Student’s Reasoning Generalizability through Cascading Decomposed <span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>T</span>s Distillation</a></strong><br><a href=/people/c/chengwei-dai/>Chengwei Dai</a>
|
<a href=/people/k/kun-li/>Kun Li</a>
|
<a href=/people/w/wei-zhou/>Wei Zhou</a>
|
<a href=/people/s/songlin-hu/>Songlin Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--875><div class="card-body p-3 small">Large language models (LLMs) exhibit enhanced reasoning at larger scales, driving efforts to distill these capabilities into smaller models via teacher-student learning.Previous works simply fine-tune student models on teachers’ generated Chain-of-Thoughts (CoTs) data. Although these methods enhance in-domain (IND) reasoning performance, they struggle to generalize to out-of-domain (OOD) tasks.We believe that the widespread spurious correlations between questions and answers may lead the model to preset a specific answer which restricts the diversity and generalizability of its reasoning process.In this paper, we propose <b>Cas</b>cading Decomposed <b>Co</b>Ts <b>D</b>istillation (CasCoD) to address these issues by decomposing the traditional single-step learning process into two cascaded learning steps. Specifically, by restructuring the training objectives—removing the answer from outputs and concatenating the question with the rationale as input—CasCoD’s two-step learning process ensures that students focus on learning rationales without interference from the preset answers, thus improving reasoning generalizability. Extensive experiments demonstrate the effectiveness of CasCoD on both IND and OOD benchmark reasoning datasets</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.876.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.876.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--876 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.876 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.876/>Revisiting Supervised Contrastive Learning for Microblog Classification</a></strong><br><a href=/people/j/junbo-huang/>Junbo Huang</a>
|
<a href=/people/r/ricardo-usbeck/>Ricardo Usbeck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--876><div class="card-body p-3 small">Microblog content (e.g., Tweets) is noisy due to its informal use of language and its lack of contextual information within each post. To tackle these challenges, state-of-the-art microblog classification models rely on pre-training language models (LMs). However, pre-training dedicated LMs is resource-intensive and not suitable for small labs. Supervised contrastive learning (SCL) has shown its effectiveness with small, available resources. In this work, we examine the effectiveness of fine-tuning transformer-based language models, regularized with a SCL loss for English microblog classification. Despite its simplicity, the evaluation on two English microblog classification benchmarks (TweetEval and Tweet Topic Classification) shows an improvement over baseline models. The result shows that, across all subtasks, our proposed method has a performance gain of up to 11.9 percentage points. All our models are open source.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.877.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.877.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--877 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.877 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.877/><span class=acl-fixed-case>B</span>ait<span class=acl-fixed-case>A</span>ttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting</a></strong><br><a href=/people/r/rui-pu/>Rui Pu</a>
|
<a href=/people/c/chaozhuo-li/>Chaozhuo Li</a>
|
<a href=/people/r/rui-ha/>Rui Ha</a>
|
<a href=/people/l/litian-zhang/>Litian Zhang</a>
|
<a href=/people/l/lirong-qiu/>Lirong Qiu</a>
|
<a href=/people/x/xi-zhang/>Xi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--877><div class="card-body p-3 small">Jailbreak attacks enable malicious queries to evade detection by LLMs. Existing attacks focus on meticulously constructing prompts to disguise harmful intentions. However, the incorporation of sophisticated disguising prompts may incur the challenge of “intention shift”. Intention shift occurs when the additional semantics within the prompt distract the LLMs, causing the responses to deviate significantly from the original harmful intentions. In this paper, we propose a novel component, “bait”, to alleviate the effects of intention shift. Bait comprises an initial response to the harmful query, prompting LLMs to rectify or supplement the knowledge within the bait. By furnishing rich semantics relevant to the query, the bait helps LLMs focus on the original intention. To conceal the harmful content within the bait, we further propose a novel attack paradigm, BaitAttack. BaitAttack adaptively generates necessary components to persuade targeted LLMs that they are engaging with a legitimate inquiry in a safe context. Our proposal is evaluated on a popular dataset, demonstrating state-of-the-art attack performance and an exceptional capability for mitigating intention shift. The implementation of BaitAttack is accessible at: https://anonymous.4open.science/r/BaitAttack-D1F5.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.878.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.878.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--878 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.878 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.878/>Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective</a></strong><br><a href=/people/z/zhaotian-weng/>Zhaotian Weng</a>
|
<a href=/people/z/zijun-gao/>Zijun Gao</a>
|
<a href=/people/j/jerone-andrews/>Jerone Andrews</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--878><div class="card-body p-3 small">Vision-language models (VLMs) pre-trained on extensive datasets can inadvertently learn biases by correlating gender information with specific objects or scenarios. Current methods, which focus on modifying inputs and monitoring changes in the model’s output probability scores, often struggle to comprehensively understand bias from the perspective of model components. We propose a framework that incorporates causal mediation analysis to measure and map the pathways of bias generation and propagation within VLMs. Our framework is applicable to a wide range of vision-language and multimodal tasks. In this work, we apply it to the object detection task and implement it on the GLIP model. This approach allows us to identify the direct effects of interventions on model bias and the indirect effects of interventions on bias mediated through different model components. Our results show that image features are the primary contributors to bias, with significantly higher impacts than text features, specifically accounting for 32.57% and 12.63% of the bias in the MSCOCO and PASCAL-SENTENCE datasets, respectively. Notably, the image encoder’s contribution surpasses that of the text encoder and the deep fusion encoder. Further experimentation confirms that contributions from both language and vision modalities are aligned and non-conflicting. Consequently, focusing on blurring gender representations within the image encoder which contributes most to the model bias, reduces bias efficiently by 22.03% and 9.04% in the MSCOCO and PASCAL-SENTENCE datasets, respectively, with minimal performance loss or increased computational demands.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.879.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.879.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--879 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.879 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.879/>Mitigating the Language Mismatch and Repetition Issues in <span class=acl-fixed-case>LLM</span>-based Machine Translation via Model Editing</a></strong><br><a href=/people/w/weichuan-wang/>Weichuan Wang</a>
|
<a href=/people/z/zhaoyi-li/>Zhaoyi Li</a>
|
<a href=/people/d/defu-lian/>Defu Lian</a>
|
<a href=/people/c/chen-ma/>Chen Ma</a>
|
<a href=/people/l/linqi-song/>Linqi Song</a>
|
<a href=/people/y/ying-wei/>Ying Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--879><div class="card-body p-3 small">Large Language Models (LLMs) have recently revolutionized the NLP field, while they still fall short in some specific down-stream tasks. In the work, we focus on utilizing LLMs to perform machine translation, where we observe that two patterns of errors frequently occur and drastically affect the translation quality: language mismatch and repetition. The work sets out to explore the potential for mitigating these two issues by leveraging model editing methods, e.g., by locating Feed-Forward Network (FFN) neurons or something that are responsible for the errors and deactivating them in the inference time.We find that directly applying such methods either limited effect on the targeted errors or has significant negative side-effect on the general translation quality, indicating that the located components may also be crucial for ensuring machine translation with LLMs on the rails.To this end, we propose to refine the located components by fetching the intersection of the locating results under different language settings, filtering out the aforementioned information that is irrelevant to targeted errors. The experiment results empirically demonstrate that our methods can effectively reduce the language mismatch and repetition ratios and meanwhile enhance or keep the general translation quality in most cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.880.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.880.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--880 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.880 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.880/><span class=acl-fixed-case>S</span>ci<span class=acl-fixed-case>A</span>gent: Tool-augmented Language Models for Scientific Reasoning</a></strong><br><a href=/people/y/yubo-ma/>Yubo Ma</a>
|
<a href=/people/z/zhibin-gou/>Zhibin Gou</a>
|
<a href=/people/j/junheng-hao/>Junheng Hao</a>
|
<a href=/people/r/ruochen-xu/>Ruochen Xu</a>
|
<a href=/people/s/shuohang-wang/>Shuohang Wang</a>
|
<a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/y/yujiu-yang/>Yujiu Yang</a>
|
<a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/a/aixin-sun/>Aixin Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--880><div class="card-body p-3 small">Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs’ abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Llama3-8B surpasses other LLMs with the comparable size by more than 8.0% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.881.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.881.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--881 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.881 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.881/>Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents</a></strong><br><a href=/people/d/dong-won-lee/>Dong Won Lee</a>
|
<a href=/people/h/hae-won-park/>Hae Won Park</a>
|
<a href=/people/y/yoon-kim/>Yoon Kim</a>
|
<a href=/people/c/cynthia-breazeal/>Cynthia Breazeal</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--881><div class="card-body p-3 small">We describe an approach for aligning an LLM based dialogue agent for long-term social dialogue, where there is only a single global score given by the user at the end of the session. In this paper, we propose the usage of denser naturally-occurring multimodal communicative signals as local implicit feedback to improve the turn-level utterance generation. Therefore, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session level reward, using Local Implicit (LI) multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the RLHF pipeline to improve an LLM-based dialog agent. We run quantitative and qualitative human studies on two large-scale datasets to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.882.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.882.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--882 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.882 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.882/>Towards Measuring and Modeling “Culture” in <span class=acl-fixed-case>LLM</span>s: A Survey</a></strong><br><a href=/people/m/muhammad-farid-adilazuarda/>Muhammad Farid Adilazuarda</a>
|
<a href=/people/s/sagnik-mukherjee/>Sagnik Mukherjee</a>
|
<a href=/people/p/pradhyumna-lavania/>Pradhyumna Lavania</a>
|
<a href=/people/s/siddhant-shivdutt-singh/>Siddhant Shivdutt Singh</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/j/jacki-oneill/>Jacki O’Neill</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--882><div class="card-body p-3 small">We present a survey of more than 90 recent papers that aim to study cultural representation and inclusion in large language models (LLMs). We observe that none of the studies explicitly define “culture, which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of “culture”. We call these aspects the proxies of culture, and organize them across two dimensions of demographic and semantic proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of “culture,” such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness of probing techniques and situated studies on the impact of cultural mis- and under-representation in LLM-based applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.883.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.883.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--883 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.883 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.883/><span class=acl-fixed-case>ESC</span>-Eval: Evaluating Emotion Support Conversations in Large Language Models</a></strong><br><a href=/people/h/haiquan-zhao/>Haiquan Zhao</a>
|
<a href=/people/l/lingyu-li/>Lingyu Li</a>
|
<a href=/people/s/shisong-chen/>Shisong Chen</a>
|
<a href=/people/s/shuqi-kong/>Shuqi Kong</a>
|
<a href=/people/j/jiaan-wang/>Jiaan Wang</a>
|
<a href=/people/k/kexin-huang/>Kexin Huang</a>
|
<a href=/people/t/tianle-gu/>Tianle Gu</a>
|
<a href=/people/y/yixu-wang/>Yixu Wang</a>
|
<a href=/people/j/jian-wang/>Jian Wang</a>
|
<a href=/people/l/liang-dandan/>Liang Dandan</a>
|
<a href=/people/z/zhixu-li/>Zhixu Li</a>
|
<a href=/people/y/yan-teng/>Yan Teng</a>
|
<a href=/people/y/yanghua-xiao/>Yanghua Xiao</a>
|
<a href=/people/y/yingchun-wang/>Yingchun Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--883><div class="card-body p-3 small">Emotion Support Conversation (ESC) is a crucial application, which aims to reduce human stress, offer emotional guidance, and ultimately enhance human mental and physical well-being. With the advancement of Large Language Models (LLMs), many researchers have employed LLMs as the ESC models. However, the evaluation of these LLM-based ESCs remains uncertain. In detail, we first re-organize 2,801 role-playing cards from seven existing datasets to define the roles of the role-playing agent. Second, we train a specific role-playing model called ESC-Role which behaves more like a confused person than GPT-4. Third, through ESC-Role and organized role cards, we systematically conduct experiments using 14 LLMs as the ESC models, including general AI-assistant LLMs (e.g., ChatGPT) and ESC-oriented LLMs (e.g., ExTES-Llama). We conduct comprehensive human annotations on interactive multi-turn dialogues of different ESC models. The results show that ESC-oriented LLMs exhibit superior ESC abilities compared to general AI-assistant LLMs, but there is still a gap behind human performance. Moreover, to automate the scoring process for future ESC models, we developed ESC-RANK, which trained on the annotated data, achieving a scoring performance surpassing 35 points of GPT-4.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.884.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.884.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--884 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.884 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.884/>Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting</a></strong><br><a href=/people/s/sagnik-mukherjee/>Sagnik Mukherjee</a>
|
<a href=/people/m/muhammad-farid-adilazuarda/>Muhammad Farid Adilazuarda</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--884><div class="card-body p-3 small">Socio-demographic prompting is a commonly employed approach to study cultural biases in LLMs as well as for aligning models to certain cultures. In this paper, we systematically probe four LLMs (Llama 3, Mistral v0.2, GPT-3.5 Turbo and GPT4) with prompts that are conditioned on culturally sensitive and non-sensitive cues, on datasets that are supposed to be culturally sensitive (EtiCor and CALI) or neutral (MMLU and ETHICS). We observe that all models except GPT4 show significant variations in their responses on both kinds of datasets for both kinds of prompts, casting doubt on the robustness of the culturally-conditioned prompting as a method for eliciting cultural bias in models that are not sufficiently stable with respect to arbitrary prompting cues. Further, we also show that some of the supposedly culturally neutral datasets have a non-trivial fraction of culturally sensitive questions/tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.885.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.885.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--885 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.885 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.885.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.885.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.885/>Text Fluoroscopy: Detecting <span class=acl-fixed-case>LLM</span>-Generated Text through Intrinsic Features</a></strong><br><a href=/people/x/xiao-yu/>Xiao Yu</a>
|
<a href=/people/k/kejiang-chen/>Kejiang Chen</a>
|
<a href=/people/q/qi-yang/>Qi Yang</a>
|
<a href=/people/w/weiming-zhang/>Weiming Zhang</a>
|
<a href=/people/n/nenghai-yu/>Nenghai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--885><div class="card-body p-3 small">Large language models (LLMs) have revolutionized the domain of natural language processing because of their excellent performance on various tasks. Despite their impressive capabilities, LLMs also have the potential to generate texts that pose risks of misuse. Consequently, detecting LLM-generated text has become increasingly important.Previous LLM-generated text detection methods use semantic features, which are stored in the last layer. This leads to methods that overfit the training set domain and exhibit shortcomings in generalization. Therefore, We argue that utilizing intrinsic features rather than semantic features for detection results in better performance.In this work, we design Text Fluoroscopy, a black-box method with better generalizability for detecting LLM-generated text by mining the intrinsic features of the text to be detected. Our method captures the text’s intrinsic features by identifying the layer with the largest distribution difference from the last and first layers when projected to the vocabulary space.Our method achieves 7.36% and 2.84% average improvement in detection performance compared to the baselines in detecting texts from different domains generated by GPT-4 and Claude3, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.886.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.886.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--886 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.886 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.886.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.886.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.886/>Hate Personified: Investigating the role of <span class=acl-fixed-case>LLM</span>s in content moderation</a></strong><br><a href=/people/s/sarah-masud/>Sarah Masud</a>
|
<a href=/people/s/sahajpreet-singh/>Sahajpreet Singh</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--886><div class="card-body p-3 small">For subjective tasks such as hate detection, where people perceive hate differently, the Large Language Model’s (LLM) ability to represent diverse groups is unclear. By including additional context in prompts, we comprehensively analyze LLM’s sensitivity to geographical priming, persona attributes, and numerical information to assess how well the needs of various groups are reflected. Our findings on two LLMs, five languages, and six datasets reveal that mimicking persona-based attributes leads to annotation variability. Meanwhile, incorporating geographical signals leads to better regional alignment. We also find that the LLMs are sensitive to numerical anchors, indicating the ability to leverage community-based flagging efforts and exposure to adversaries. Our work provides preliminary guidelines and highlights the nuances of applying LLMs in culturally sensitive cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.887.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.887.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--887 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.887 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.887/>Temporally Consistent Factuality Probing for Large Language Models</a></strong><br><a href=/people/a/ashutosh-bajpai/>Ashutosh Bajpai</a>
|
<a href=/people/a/aaryan-goyal/>Aaryan Goyal</a>
|
<a href=/people/a/atif-anwer/>Atif Anwer</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--887><div class="card-body p-3 small">The prolific use of Large Language Models (LLMs) as an alternate knowledge base requires them to be factually consistent, necessitating both correctness and consistency traits for paraphrased queries. Recently, significant attempts have been made to benchmark datasets and metrics to evaluate LLMs for these traits. However, structural simplicity (subject-relation-object) and contemporary association in their query formulation limit the broader definition of factuality and consistency. In this study, we introduce TeCFaP, a novel Temporally Consistent Factuality Probe task to expand the consistent factuality probe in the temporal dimension. To this end, we propose TEMP-COFAC, a high-quality dataset of prefix-style English query paraphrases. Subsequently, we extend the definitions of existing metrics to represent consistent factuality across temporal dimension. We experiment with a diverse set of LLMs and find most of them performing poorly on TeCFaP. Next, we propose a novel solution CoTSeLF (Consistent-Time-Sensitive Learning Framework) combining multi-task instruction tuning (MT-IT) with consistent-time-sensitive reinforcement learning (CTSRL) to improve temporally consistent factuality in LLMs. Our experiments demonstrate the efficacy of CoTSeLF over several baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.888.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.888.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--888 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.888 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.888/>A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives</a></strong><br><a href=/people/z/zihao-li/>Zihao Li</a>
|
<a href=/people/s/shaoxiong-ji/>Shaoxiong Ji</a>
|
<a href=/people/t/timothee-mickus/>Timothee Mickus</a>
|
<a href=/people/v/vincent-segonne/>Vincent Segonne</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--888><div class="card-body p-3 small">Pretrained language models (PLMs) display impressive performances and have captured the attention of the NLP community.Establishing best practices in pretraining has, therefore, become a major focus of NLP research, especially since insights gained from monolingual English models may not necessarily apply to more complex multilingual models.One significant caveat of the current state of the art is that different works are rarely comparable: they often discuss different parameter counts, training data, and evaluation methodology.This paper proposes a comparison of multilingual pretraining objectives in a controlled methodological environment. We ensure that training data and model architectures are comparable, and discuss the downstream performances across 6 languages that we observe in probing and fine-tuning scenarios.We make two key observations: (1) the architecture dictates which pretraining objective is optimal; (2) multilingual translation is a very effective pretraining objective under the right conditions.We make our code, data, and model weights available at https://github.com/Helsinki-NLP/lm-vs-mt.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.889.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.889.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--889 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.889 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.889/>Can <span class=acl-fixed-case>LLM</span>s replace Neil de<span class=acl-fixed-case>G</span>rasse Tyson? Evaluating the Reliability of <span class=acl-fixed-case>LLM</span>s as Science Communicators</a></strong><br><a href=/people/p/prasoon-bajpai/>Prasoon Bajpai</a>
|
<a href=/people/n/niladri-chatterjee/>Niladri Chatterjee</a>
|
<a href=/people/s/subhabrata-dutta/>Subhabrata Dutta</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--889><div class="card-body p-3 small">Large Language Models (LLMs) and AI assistants driven by these models are experiencing exponential growth in usage among both expert and amateur users. In this work, we focus on evaluating the reliability of current LLMs as science communicators. Unlike existing benchmarks, our approach emphasizes assessing these models on scientific question-answering tasks that require a nuanced understanding and awareness of answerability. We introduce a novel dataset, SCiPS-QA, comprising 742 Yes/No queries embedded in complex scientific concepts, along with a benchmarking suite that evaluates LLMs for correctness and consistency across various criteria. We benchmark three proprietary LLMs from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2, Llama-3, and Mistral families. While most open-access models significantly underperform compared to GPT-4 Turbo, our experiments identify Llama-3-70B as a strong competitor, often surpassing GPT-4 Turbo in various evaluation aspects. We also find that even the GPT models exhibit a general incompetence in reliably verifying LLM responses. Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.890.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.890.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--890 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.890 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.890/><span class=acl-fixed-case>LL</span>a<span class=acl-fixed-case>MA</span>-<span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>E</span>: Building Mixture-of-Experts from <span class=acl-fixed-case>LL</span>a<span class=acl-fixed-case>MA</span> with Continual Pre-Training</a></strong><br><a href=/people/t/tong-zhu/>Tong Zhu</a>
|
<a href=/people/x/xiaoye-qu/>Xiaoye Qu</a>
|
<a href=/people/d/daize-dong/>Daize Dong</a>
|
<a href=/people/j/jiacheng-ruan/>Jiacheng Ruan</a>
|
<a href=/people/j/jingqi-tong/>Jingqi Tong</a>
|
<a href=/people/c/conghui-he/>Conghui He</a>
|
<a href=/people/y/yu-cheng/>Yu Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--890><div class="card-body p-3 small">Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, training MoE from scratch in a large-scale setting still suffers from data-hungry and instability problems. Motivated by this limit, we investigate building MoE models from existing dense large language models. Specifically, based on the well-known LLaMA-2 7B model, we obtain an MoE model by: (1) Expert Construction, which partitions the parameters of original Feed-Forward Networks (FFNs) into multiple experts; (2) Continual pre-training, which further trains the transformed MoE model and additional gate networks. In this paper, we comprehensively explore different methods for expert construction and various data sampling strategies for continual pre-training. After these stages, our LLaMA-MoE models could maintain language abilities and route the input tokens to specific experts with part of the parameters activated. Empirically, by training 200B tokens, LLaMA-MoE-3.5B models significantly outperform dense models that contain similar activation parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.891.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.891.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--891 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.891 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.891/>Themis: A Reference-free <span class=acl-fixed-case>NLG</span> Evaluation Language Model with Flexibility and Interpretability</a></strong><br><a href=/people/x/xinyu-hu/>Xinyu Hu</a>
|
<a href=/people/l/li-lin/>Li Lin</a>
|
<a href=/people/m/mingqi-gao/>Mingqi Gao</a>
|
<a href=/people/x/xunjian-yin/>Xunjian Yin</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--891><div class="card-body p-3 small">The evaluation of natural language generation (NLG) tasks is a significant and longstanding research area. With the recent emergence of powerful large language models (LLMs), some studies have turned to LLM-based automatic evaluation methods, which demonstrate great potential to become a new evaluation paradigm following traditional string-based and model-based metrics. However, despite the improved performance of existing methods, they still possess some deficiencies, such as dependency on references and limited evaluation flexibility. Therefore, in this paper, we meticulously construct a large-scale NLG evaluation corpus **NLG-Eval** with annotations from both human and GPT-4 to alleviate the lack of relevant data in this field. Furthermore, we propose **Themis**, an LLM dedicated to NLG evaluation, which has been trained with our designed multi-perspective consistency verification and rating-oriented preference alignment methods. Themis can conduct flexible and interpretable evaluations without references, and it exhibits superior evaluation performance on various NLG tasks, simultaneously generalizing well to unseen tasks and surpassing other evaluation models, including GPT-4.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.892.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.892.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--892 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.892 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.892/>Mitigating Training Imbalance in <span class=acl-fixed-case>LLM</span> Fine-Tuning via Selective Parameter Merging</a></strong><br><a href=/people/y/yiming-ju/>Yiming Ju</a>
|
<a href=/people/z/ziyi-ni/>Ziyi Ni</a>
|
<a href=/people/x/xingrun-xing/>Xingrun Xing</a>
|
<a href=/people/z/zhixiong-zeng/>Zhixiong Zeng</a>
|
<a href=/people/h/hanyu-zhao/>Hanyu Zhao</a>
|
<a href=/people/s/siqi-fan/>Siqi Fan</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--892><div class="card-body p-3 small">Supervised fine-tuning (SFT) is crucial for adapting Large Language Models (LLMs) to specific tasks. In this work, we demonstrate that the order of training data can lead to significant training imbalances, potentially resulting in performance degradation. Consequently, we propose to mitigate this imbalance by merging SFT models fine-tuned with different data orders, thereby enhancing the overall effectiveness of SFT. Additionally, we introduce a novel technique, “parameter-selection merging,” which outperforms traditional weighted-average methods on five datasets. Further, through analysis and ablation studies, we validate the effectiveness of our method and identify the sources of performance improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.893.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.893.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--893 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.893 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.893/>Generating Demonstrations for In-Context Compositional Generalization in Grounded Language Learning</a></strong><br><a href=/people/s/sam-spilsbury/>Sam Spilsbury</a>
|
<a href=/people/p/pekka-marttinen/>Pekka Marttinen</a>
|
<a href=/people/a/alexander-ilin/>Alexander Ilin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--893><div class="card-body p-3 small">In-Context-learning and few-shot prompting are viable methods compositional output generation. However, these methods can be very sensitive to the choice of support examples used. Retrieving good supports from the training data for a given test query is already a difficult problem, but in some cases solving this may not even be enough. We consider the setting of grounded language learning problems where finding relevant supports in the same or similar states as the query may be difficult. We design an agent which instead generates possible supports inputs and targets current state of the world, then uses them in-context-learning to solve the test query. We show substantially improved performance on a previously unsolved compositional generalization test without a loss of performance in other areas. The approach is general and can even scale to instructions expressed in natural language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.894.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.894.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--894 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.894 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.894/><span class=acl-fixed-case>FAME</span>: Towards Factual Multi-Task Model Editing</a></strong><br><a href=/people/l/li-zeng/>Li Zeng</a>
|
<a href=/people/y/yingyu-shan/>Yingyu Shan</a>
|
<a href=/people/z/zeming-liu/>Zeming Liu</a>
|
<a href=/people/j/jiashu-yao/>Jiashu Yao</a>
|
<a href=/people/y/yuhang-guo/>Yuhang Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--894><div class="card-body p-3 small">Large language models (LLMs) embed extensive knowledge and utilize it to perform exceptionally well across various tasks. Nevertheless, outdated knowledge or factual errors within LLMs can lead to misleading or incorrect responses, causing significant issues in practical applications. To rectify the fatal flaw without the necessity for costly model retraining, various model editing approaches have been proposed to correct inaccurate information within LLMs in a cost-efficient way. To evaluate these model editing methods, previous work introduced a series of datasets. However, most of the previous datasets only contain fabricated data in a single format, which diverges from real-world model editing scenarios, raising doubts about their usability in practice. To facilitate the application of model editing in real-world scenarios, we propose the challenge of practicality. To resolve such challenges and effectively enhance the capabilities of LLMs, we present FAME, an authentic, comprehensive, and multi-task dataset, which is designed to enhance the practicality of model editing. We then propose SKEME, a model editing method that uses a novel caching mechanism to ensure synchronization with the real world. The experiments demonstrate that our method performs excellently across various tasks and scenarios, confirming its practicality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.895.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.895.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--895 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.895 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.895/><span class=acl-fixed-case>MLLM</span>-Protector: Ensuring <span class=acl-fixed-case>MLLM</span>’s Safety without Hurting Performance</a></strong><br><a href=/people/r/renjie-pi/>Renjie Pi</a>
|
<a href=/people/t/tianyang-han/>Tianyang Han</a>
|
<a href=/people/j/jianshu-zhang/>Jianshu Zhang</a>
|
<a href=/people/y/yueqi-xie/>Yueqi Xie</a>
|
<a href=/people/r/rui-pan/>Rui Pan</a>
|
<a href=/people/q/qing-lian/>Qing Lian</a>
|
<a href=/people/h/hanze-dong/>Hanze Dong</a>
|
<a href=/people/j/jipeng-zhang/>Jipeng Zhang</a>
|
<a href=/people/t/tong-zhang/>Tong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--895><div class="card-body p-3 small">The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. This paper investigates the novel challenge of defending MLLMs against such attacks. Compared to large language models (LLMs), MLLMs include an additional image modality. We discover that images act as a “foreign language” that is not considered during safety alignment, making MLLMs more prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover all possible scenarios. This vulnerability is exacerbated by the fact that most state-of-the-art MLLMs are fine-tuned on limited image-text pairs that are much fewer than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during safety fine-tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play strategy that solves two subtasks: 1) identifying harmful responses via a lightweight harm detector, and 2) transforming harmful responses into harmless ones via a detoxifier. This approach effectively mitigates the risks posed by malicious visual inputs without compromising the original performance of MLLMs. Our results demonstrate that MLLM-Protector offers a robust solution to a previously unaddressed aspect of MLLM security.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.896.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.896.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--896 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.896 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.896/>Leveraging Large Language Models for <span class=acl-fixed-case>NLG</span> Evaluation: Advances and Challenges</a></strong><br><a href=/people/z/zhen-li/>Zhen Li</a>
|
<a href=/people/x/xiaohan-xu/>Xiaohan Xu</a>
|
<a href=/people/t/tao-shen/>Tao Shen</a>
|
<a href=/people/c/can-xu/>Can Xu</a>
|
<a href=/people/j/jia-chen-gu/>Jia-Chen Gu</a>
|
<a href=/people/y/yuxuan-lai/>Yuxuan Lai</a>
|
<a href=/people/c/chongyang-tao/>Chongyang Tao</a>
|
<a href=/people/s/shuai-ma/>Shuai Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--896><div class="card-body p-3 small">In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This paper aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this paper seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.897.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.897.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--897 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.897 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.897/><span class=acl-fixed-case>I</span>nfini<span class=acl-fixed-case>P</span>ot: Infinite Context Processing on Memory-Constrained <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/m/minsoo-kim/>Minsoo Kim</a>
|
<a href=/people/k/kyuhong-shim/>Kyuhong Shim</a>
|
<a href=/people/j/jungwook-choi/>Jungwook Choi</a>
|
<a href=/people/s/simyung-chang/>Simyung Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--897><div class="card-body p-3 small">Handling long input contexts remains a significant challenge for Large Language Models (LLMs), particularly in resource-constrained environments such as mobile devices. Our work aims to address this limitation by introducing InfiniPot, a novel KV cache control framework designed to enable pre-trained LLMs to manage extensive sequences within fixed memory constraints efficiently, without requiring additional training. InfiniPot leverages Continual Context Distillation (CCD), an iterative process that compresses and retains essential information through novel importance metrics, effectively maintaining critical data even without access to future context. Our comprehensive evaluations indicate that InfiniPot significantly outperforms models trained for long contexts in various NLP tasks, establishing its efficacy and versatility. This work represents a substantial advancement toward making LLMs applicable to a broader range of real-world scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.898.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.898.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--898 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.898 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.898.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.898.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.898/><span class=acl-fixed-case>V</span>ideo<span class=acl-fixed-case>CLIP</span>-<span class=acl-fixed-case>XL</span>: Advancing Long Description Understanding for Video <span class=acl-fixed-case>CLIP</span> Models</a></strong><br><a href=/people/j/jiapeng-wang/>Jiapeng Wang</a>
|
<a href=/people/c/chengyu-wang/>Chengyu Wang</a>
|
<a href=/people/k/kunzhe-huang/>Kunzhe Huang</a>
|
<a href=/people/j/jun-huang/>Jun Huang</a>
|
<a href=/people/l/lianwen-jin/>Lianwen Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--898><div class="card-body p-3 small">Contrastive Language-Image Pre-training (CLIP) has been widely studied and applied in numerous applications. However, the emphasis on brief summary texts during pre-training prevents CLIP from understanding long descriptions. This issue is particularly acute regarding videos given that videos often contain abundant detailed contents. In this paper, we propose the VideoCLIP-XL (eXtra Length) model, which aims to unleash the long-description understanding capability of video CLIP models. Firstly, we establish an automatic data collection system and gather a large-scale VILD pre-training dataset with VIdeo and Long-Description pairs. Then, we propose Text-similarity-guided Primary Component Matching (TPCM) to better learn the distribution of feature space while expanding the long description capability. We also introduce two new tasks namely Detail-aware Description Ranking (DDR) and Hallucination-aware Description Ranking (HDR) for further understanding improvement. Finally, we construct a Long Video Description Ranking (LVDR) benchmark for evaluating the long-description capability more comprehensively. Extensive experimental results on widely-used text-video retrieval benchmarks with both short and long descriptions and our LVDR benchmark can fully demonstrate the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.899.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.899.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--899 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.899 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.899/><span class=acl-fixed-case>C</span>orr<span class=acl-fixed-case>S</span>ynth - A Correlated Sampling Method for Diverse Dataset Generation from <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/s/suhas-s-kowshik/>Suhas S Kowshik</a>
|
<a href=/people/a/abhishek-divekar/>Abhishek Divekar</a>
|
<a href=/people/v/vijit-malik/>Vijit Malik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--899><div class="card-body p-3 small">Large language models (LLMs) have demonstrated remarkable performance in diverse tasks using zero-shot and few-shot prompting. Even though their capabilities of data synthesis have been studied well in recent years, the generated data suffers from a lack of diversity, less adherence to the prompt, and potential biases that creep into the data from the generator model. In this work, we tackle the challenge of generating datasets with high diversity, upon which a student model is trained for downstream tasks. Taking the route of decoding-time guidance-based approaches, we propose CorrSynth, which generates data that is more diverse and faithful to the input prompt using a correlated sampling strategy. Further, our method overcomes the complexity drawbacks of some other guidance-based techniques like classifier-based guidance. With extensive experiments, we show the effectiveness of our approach and substantiate our claims. In particular, we perform intrinsic evaluation to show the improvements in diversity. Our experiments show that CorrSynth improves both student metrics and intrinsic metrics upon competitive baselines across four datasets, showing the innate advantage of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.900.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--900 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.900 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.900/>Defining Knowledge: Bridging Epistemology and Large Language Models</a></strong><br><a href=/people/c/constanza-fierro/>Constanza Fierro</a>
|
<a href=/people/r/ruchira-dhar/>Ruchira Dhar</a>
|
<a href=/people/f/filippos-stamatiou/>Filippos Stamatiou</a>
|
<a href=/people/n/nicolas-garneau/>Nicolas Garneau</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--900><div class="card-body p-3 small">Knowledge claims are abundant in the literature on large language models (LLMs); but can we say that GPT-4 truly “knows” the Earth is round? To address this question, we review standard definitions of knowledge in epistemology and we formalize interpretations applicable to LLMs. In doing so, we identify inconsistencies and gaps in how current NLP research conceptualizes knowledge with respect to epistemological frameworks. Additionally, we conduct a survey of 100 professional philosophers and computer scientists to compare their preferences in knowledge definitions and their views on whether LLMs can really be said to know. Finally, we suggest evaluation protocols for testing knowledge in accordance to the most relevant definitions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.901.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.901.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--901 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.901/><span class=acl-fixed-case>TKGT</span>: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/p/peiwen-jiang/>Peiwen Jiang</a>
|
<a href=/people/x/xinbo-lin/>Xinbo Lin</a>
|
<a href=/people/z/zibo-zhao/>Zibo Zhao</a>
|
<a href=/people/r/ruhui-ma/>Ruhui Ma</a>
|
<a href=/people/y/yvonne-jie-chen/>Yvonne Jie Chen</a>
|
<a href=/people/j/jinhua-cheng/>Jinhua Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--901><div class="card-body p-3 small">The task of text-to-table receives widespread attention, yet its importance and difficulty are underestimated. Existing works use simple datasets similar to table-to-text tasks and employ methods that ignore domain structures. As a bridge between raw text and statistical analysis, the text-to-table task often deals with complex semi-structured texts that refer to specific domain topics in the real world with entities and events, especially from those of social sciences. In this paper, we analyze the limitations of benchmark datasets and methods used in the text-to-table literature and redefine the text-to-table task to improve its compatibility with long text-processing tasks. Based on this redefinition, we propose a new dataset called CPL (Chinese Private Lending), which consists of judgments from China and is derived from a real-world legal academic project. We further propose TKGT (Text-KG-Table), a two stages domain-aware pipeline, which firstly generates domain knowledge graphs (KGs) classes semi-automatically from raw text with the mixed information extraction (Mixed-IE) method, then adopts the hybrid retrieval augmented generation (Hybird-RAG) method to transform it to tables for downstream needs under the guidance of KGs classes. Experiment results show that TKGT achieves state-of-the-art (SOTA) performance on both traditional datasets and the CPL. Our data and main code are available at https://github.com/jiangpw41/TKGT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.902.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--902 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.902.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.902/>Free your mouse! Command Large Language Models to Generate Code to Format Word Documents</a></strong><br><a href=/people/s/shihao-rao/>Shihao Rao</a>
|
<a href=/people/l/liang-li/>Liang Li</a>
|
<a href=/people/j/jiapeng-liu/>Jiapeng Liu</a>
|
<a href=/people/g/guan-weixin/>Guan Weixin</a>
|
<a href=/people/x/xiyan-gao/>Xiyan Gao</a>
|
<a href=/people/b/bing-lim/>Bing Lim</a>
|
<a href=/people/c/can-ma/>Can Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--902><div class="card-body p-3 small">Recently, LLMs have significantly improved code generation, making it increasingly accessible to users. As a result, LLM-powered code generation applications have sprung up, vastly boosting user productivity. This paper mainly explores how to improve the efficiency and experience of users in formatting the document. Specifically, we propose an automatic document formatting method, Text-to-Format, which is driven by various prompting strategies. Text-to-Format takes the user’s formatting instructions and then generates code that can be run in Microsoft Word to format the content in a document. Further, to evaluate automatic document formatting approaches and advance the document formatting task, we built an evaluation specification including a high-quality dataset DocFormEval data, a code runtime environment, and evaluation metrics. Extensive experimental results on data reveal that the prompting strategy’s effect positively correlates with how much knowledge it introduces related to document formatting task. We believe the constructed DocFormEval data and the exploration about Text-to-Format can help developers build more intelligent tools for automatic document formatting, especially in offline scenarios, where the data privacy is the top priority.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.903.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--903 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.903/><span class=acl-fixed-case>CMR</span> Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models</a></strong><br><a href=/people/j/jiawei-gu/>Jiawei Gu</a>
|
<a href=/people/z/zacc-yang/>Zacc Yang</a>
|
<a href=/people/c/chuanghao-ding/>Chuanghao Ding</a>
|
<a href=/people/r/rui-zhao/>Rui Zhao</a>
|
<a href=/people/f/fei-tan/>Fei Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--903><div class="card-body p-3 small">Large Language Models (LLMs) excel in diverse tasks but often underperform in specialized fields due to limited domain-specific or proprietary corpus. Continual pre-training (CPT) enhances LLM capabilities by imbuing new domain-specific or proprietary knowledge while replaying general corpus to prevent catastrophic forgetting. The data mixture ratio of general corpus and domain-specific corpus, however, has been chosen heuristically, leading to sub-optimal training efficiency in practice. In this context, we attempt to re-visit the scaling behavior of LLMs under the hood of CPT, and discover a power-law relationship between loss, mixture ratio, and training tokens scale. We formalize the trade-off between general and domain-specific capabilities, leading to a well-defined Critical Mixture Ratio (CMR) of general and domain data. By striking the balance, CMR maintains the model’s general ability and achieves the desired domain transfer, ensuring the highest utilization of available resources. Considering the balance between efficiency and effectiveness, CMR can be regarded as the optimal mixture ratio. Through extensive experiments, we ascertain the predictability of CMR, propose CMR scaling law and have substantiated its generalization. These findings offer practical guidelines for optimizing LLM training in specialized domains, ensuring both general and domain-specific performance while efficiently managing training resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.904.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--904 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.904/>The Instinctive Bias: Spurious Images lead to Illusion in <span class=acl-fixed-case>MLLM</span>s</a></strong><br><a href=/people/t/tianyang-han/>Tianyang Han</a>
|
<a href=/people/q/qing-lian/>Qing Lian</a>
|
<a href=/people/r/rui-pan/>Rui Pan</a>
|
<a href=/people/r/renjie-pi/>Renjie Pi</a>
|
<a href=/people/j/jipeng-zhang/>Jipeng Zhang</a>
|
<a href=/people/s/shizhe-diao/>Shizhe Diao</a>
|
<a href=/people/y/yong-lin/>Yong Lin</a>
|
<a href=/people/t/tong-zhang/>Tong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--904><div class="card-body p-3 small">Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from visual illusion. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the visual illusion level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs’ robustness in the presence of misleading images. The code and datasets are available at https://github.com/MasaiahHan/CorrelationQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.905.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--905 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.905/>Rationale-Aware Answer Verification by Pairwise Self-Evaluation</a></strong><br><a href=/people/a/akira-kawabata/>Akira Kawabata</a>
|
<a href=/people/s/saku-sugawara/>Saku Sugawara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--905><div class="card-body p-3 small">Answer verification identifies correct solutions among candidates generated by large language models (LLMs). Current approaches typically train verifier models by labeling solutions as correct or incorrect based solely on whether the final answer matches the gold answer. However, this approach neglects any flawed rationale in the solution yielding the correct answer, undermining the verifier’s ability to distinguish between sound and flawed rationales. We empirically show that in StrategyQA, only 19% of LLM-generated solutions with correct answers have valid rationales, thus leading to an unreliable verifier. Furthermore, we demonstrate that training a verifier on valid rationales significantly improves its ability to distinguish valid and flawed rationale. To make a better verifier without extra human supervision, we introduce REPS (Rationale Enhancement through Pairwise Selection), a method for selecting valid rationales from candidates by iteratively applying pairwise self-evaluation using the same LLM that generates the solutions. Verifiers trained on solutions selected by REPS outperform those trained using conventional training methods on three reasoning benchmarks (ARC-Challenge, DROP, and StrategyQA). Our results suggest that training reliable verifiers requires ensuring the validity of rationales in addition to the correctness of the final answers, which would be critical for models assisting humans in solving complex reasoning tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.906.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--906 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.906.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.906/>On the Robustness of Editing Large Language Models</a></strong><br><a href=/people/x/xinbei-ma/>Xinbei Ma</a>
|
<a href=/people/t/tianjie-ju/>Tianjie Ju</a>
|
<a href=/people/j/jiyang-qiu/>Jiyang Qiu</a>
|
<a href=/people/z/zhuosheng-zhang/>Zhuosheng Zhang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/l/lifeng-liu/>Lifeng Liu</a>
|
<a href=/people/y/yulong-wang/>Yulong Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--906><div class="card-body p-3 small">Large language models (LLMs) have played a pivotal role in building communicative AI, yet they encounter the challenge of efficient updates. Model editing enables the manipulation of specific knowledge memories and the behavior of language generation without retraining. However, the robustness of model editing remains an open question. This work seeks to understand the strengths and limitations of editing methods, facilitating practical applications of communicative AI. We focus on three key research questions. RQ1: Can edited LLMs behave consistently resembling communicative AI in realistic situations? RQ2: To what extent does the rephrasing of prompts lead LLMs to deviate from the edited knowledge memory? RQ3: Which knowledge features are correlated with the performance and robustness of editing? Our empirical studies uncover a substantial disparity between existing editing methods and the practical application of LLMs. On rephrased prompts that are flexible but common in realistic applications, the performance of editing experiences a significant decline. Further analysis shows that more popular knowledge is memorized better, easier to recall, and more challenging to edit effectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.907.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--907 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.907.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.907/><span class=acl-fixed-case>IM</span>-<span class=acl-fixed-case>BERT</span>: Enhancing Robustness of <span class=acl-fixed-case>BERT</span> through the Implicit Euler Method</a></strong><br><a href=/people/m/mihyeon-kim/>MiHyeon Kim</a>
|
<a href=/people/j/juhyoung-park/>Juhyoung Park</a>
|
<a href=/people/y/youngbin-kim/>YoungBin Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--907><div class="card-body p-3 small">Pre-trained Language Models (PLMs) have achieved remarkable performance on diverse NLP tasks through pre-training and fine-tuning. However, fine-tuning the model with a large number of parameters on limited downstream datasets often leads to vulnerability to adversarial attacks, causing overfitting of the model on standard datasets. To address these issues, we propose IM-BERT from the perspective of a dynamic system by conceptualizing a layer of BERT as a solution of Ordinary Differential Equations (ODEs). Under the situation of initial value perturbation, we analyze the numerical stability of two main numerical ODE solvers: *the explicit and implicit Euler approaches.* Based on these analyses, we introduce a numerically robust IM-connection incorporating BERT’s layers. This strategy enhances the robustness of PLMs against adversarial attacks, even in low-resource scenarios, without introducing additional parameters or adversarial training strategies. Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the robustness of IM-BERT under various conditions. Compared to the original BERT, IM-BERT exhibits a performance improvement of approximately 8.3%p on the AdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms BERT by achieving 5.9%p higher accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.908.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--908 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.908/>Distract Large Language Models for Automatic Jailbreak Attack</a></strong><br><a href=/people/z/zeguan-xiao/>Zeguan Xiao</a>
|
<a href=/people/y/yan-yang/>Yan Yang</a>
|
<a href=/people/g/guanhua-chen/>Guanhua Chen</a>
|
<a href=/people/y/yun-chen/>Yun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--908><div class="card-body p-3 small">Extensive efforts have been made before the public release of Large language models (LLMs) to align their behaviors with human values. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. In this work, we propose a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, motivated by the research about the distractibility and over-confidence phenomenon of LLMs. Extensive experiments of jailbreaking both open-source and proprietary LLMs demonstrate the superiority of our framework in terms of effectiveness, scalability and transferability. We also evaluate the effectiveness of existing jailbreak defense methods against our attack and highlight the crucial need to develop more effective and practical defense strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.909.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--909 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.909/>Exploring Space Efficiency in a Tree-based Linear Model for Extreme Multi-label Classification</a></strong><br><a href=/people/h/he-zhe-lin/>He-Zhe Lin</a>
|
<a href=/people/c/cheng-hung-liu/>Cheng-Hung Liu</a>
|
<a href=/people/c/chih-jen-lin/>Chih-Jen Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--909><div class="card-body p-3 small">Extreme multi-label classification (XMC) aims to identify relevant subsets from numerous labels. Among the various approaches for XMC, tree-based linear models are effective due to their superior efficiency and simplicity. However, the space complexity of tree-based methods is not well-studied. Many past works assume that storing the model is not affordable and apply techniques such as pruning to save space, which may lead to performance loss. In this work, we conduct both theoretical and empirical analyses on the space to store a tree model under the assumption of sparse data, a condition frequently met in text data. We found that, some features may be unused when training binary classifiers in a tree method, resulting in zero values in the weight vectors. Hence, storing only non-zero elements can greatly save space. Our experimental results indicate that tree models can require less than 10% of the size of the standard one-vs-rest method for multi-label text classification. Our research provides a simple procedure to estimate the size of a tree model before training any classifier in the tree nodes. Then, if the model size is already acceptable, this approach can help avoid modifying the model through weight pruning or other techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.910.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.910.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--910 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.910 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.910/><span class=acl-fixed-case>W</span>orry<span class=acl-fixed-case>W</span>ords: Norms of Anxiety Association for over 44k <span class=acl-fixed-case>E</span>nglish Words</a></strong><br><a href=/people/s/saif-mohammad/>Saif M. Mohammad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--910><div class="card-body p-3 small">Anxiety, the anticipatory unease about a potential negative outcome, is a common and beneficial human emotion. However, there is still much that is not known about anxiety, such as how it relates to our body and how it manifests in language; especially pertinent given the increasing impact of related disorders.In this work,we introduce <i>WorryWords</i>, the first large-scale repository of manually derived word–anxiety associations for over 44,450 English words. We show that the anxiety associations are highly reliable.We use WorryWords to study the relationship between anxiety and other emotion constructs, as well as the rate at which children acquire anxiety words with age. Finally, we show that using WorryWords alone, one can accurately track the change of anxiety in streams of text.WorryWords enables a wide variety of anxiety-related research in psychology, NLP, public health, and social sciences.WorryWords (and its translations to over 100 languages) is freely available. http://saifmohammad.com/worrywords.html</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.911.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--911 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.911.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.911/>Finding Blind Spots in Evaluator <span class=acl-fixed-case>LLM</span>s with Interpretable Checklists</a></strong><br><a href=/people/s/sumanth-doddapaneni/>Sumanth Doddapaneni</a>
|
<a href=/people/m/mohammed-safi-ur-rahman-khan/>Mohammed Safi Ur Rahman Khan</a>
|
<a href=/people/s/sshubam-verma/>Sshubam Verma</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M Khapra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--911><div class="card-body p-3 small">Large Language Models (LLMs) are increasingly relied upon to evaluate text outputs of other LLMs, thereby influencing leaderboards and development decisions. However, concerns persist over the accuracy of these assessments and the potential for misleading conclusions. In this work, we investigate the effectiveness of LLMs as evaluators for text generation tasks. We propose FBI, a novel framework designed to examine the proficiency of Evaluator LLMs in assessing four critical abilities in other LLMs: factual accuracy, instruction following, coherence in long-form writing, and reasoning proficiency. By introducing targeted perturbations in answers generated by LLMs, that clearly impact one of these key capabilities, we test whether an Evaluator LLM can detect these quality drops. By creating a total of 2400 perturbed answers covering 22 perturbation categories, we conduct a comprehensive study using different evaluation strategies on five prominent LLMs commonly used as evaluators in the literature. Our findings reveal significant shortcomings in current Evaluator LLMs, which failed to identify quality drops in over 50% of cases on average. Single-answer and pairwise evaluations demonstrated notable limitations, whereas reference-based evaluations showed comparatively better performance. <i>These results underscore the unreliable nature of current Evaluator LLMs and advocate for cautious implementation in practical applications.</i></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.912.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--912 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.912/><span class=acl-fixed-case>LONGAGENT</span>: Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration</a></strong><br><a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/c/can-zu/>Can Zu</a>
|
<a href=/people/x/xu-hao/>Xu Hao</a>
|
<a href=/people/y/yi-lu/>Yi Lu</a>
|
<a href=/people/w/wei-he/>Wei He</a>
|
<a href=/people/y/yiwen-ding/>Yiwen Ding</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--912><div class="card-body p-3 small">Large language models (LLMs) have achieved tremendous success in understanding language and processing text. However, question-answering (QA) on lengthy documents faces challenges of resource constraints and a high propensity for errors, even for the most advanced models such as GPT-4 and Claude2.In this paper, we introduce _LongAgent_, a multi-agent collaboration method that enables efficient and effective QA over <span class=tex-math>128k</span>-token-long documents. _LongAgent_ adopts a _divide-and-conquer_ strategy, breaking down lengthy documents into shorter, more manageable text chunks. A leader agent comprehends the user’s query and organizes the member agents to read their assigned chunks, reasoning a final answer through multiple rounds of discussion.Due to members’ hallucinations, it’s difficult to guarantee that every response provided by each member is accurate.To address this, we develop an _inter-member communication_ mechanism that facilitates information sharing, allowing for the detection and mitigation of hallucinatory responses.Experimental results show that a LLaMA-2 7B driven by _LongAgent_ can effectively support QA over <span class=tex-math>128k</span>-token documents, achieving 16.42% and 1.63% accuracy gains over GPT-4 on single-hop and multi-hop QA settings, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.913.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--913 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.913.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.913/><span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>P</span>ersuade: A Framework for Evaluating and Explaining Persuasive Arguments</a></strong><br><a href=/people/t/till-raphael-saenger/>Till Raphael Saenger</a>
|
<a href=/people/m/musashi-hinck/>Musashi Hinck</a>
|
<a href=/people/j/justin-grimmer/>Justin Grimmer</a>
|
<a href=/people/b/brandon-m-stewart/>Brandon M. Stewart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--913><div class="card-body p-3 small">We introduce a three-part framework for constructing persuasive messages, AutoPersuade. First, we curate a large collection of arguments and gather human evaluations of their persuasiveness. Next, we introduce a novel topic model to identify the features of these arguments that influence persuasion. Finally, we use the model to predict the persuasiveness of new arguments and to assess the causal effects of argument components, offering an explanation of the results. We demonstrate the effectiveness of AutoPersuade in an experimental study on arguments for veganism, validating our findings through human studies and out-of-sample predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.914.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--914 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.914/>Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs</a></strong><br><a href=/people/s/simone-conia/>Simone Conia</a>
|
<a href=/people/d/daniel-lee/>Daniel Lee</a>
|
<a href=/people/m/min-li/>Min Li</a>
|
<a href=/people/u/umar-farooq-minhas/>Umar Farooq Minhas</a>
|
<a href=/people/s/saloni-potdar/>Saloni Potdar</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--914><div class="card-body p-3 small">Translating text that contains entity names is a challenging task, as cultural-related references can vary significantly across languages. These variations may also be caused by transcreation, an adaptation process that entails more than transliteration and word-for-word translation. In this paper, we address the problem of cross-cultural translation on two fronts: (i) we introduce XC-Translate, the first large-scale, manually-created benchmark for machine translation that focuses on text that contains potentially culturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end method to integrate information from a multilingual knowledge graph into a neural machine translation model by leveraging a dense retrieval mechanism. Our experiments and analyses show that current machine translation systems and large language models still struggle to translate texts containing entity names, whereas KG-MT outperforms state-of-the-art approaches by a large margin, obtaining a 129% and 62% relative improvement compared to NLLB-200 and GPT-4, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.915.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--915 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.915/>Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems</a></strong><br><a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/j/jingqi-tong/>Jingqi Tong</a>
|
<a href=/people/y/yurong-mou/>Yurong Mou</a>
|
<a href=/people/m/ming-zhang/>Ming Zhang</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--915><div class="card-body p-3 small">Human cognition exhibits systematic compositionality, the algebraic ability to generate infinite novel combinations from finite learned components, which is the key to understanding and reasoning about complex logic. In this work, we investigate the compositionality of large language models (LLMs) in mathematical reasoning. Specifically, we construct a new dataset MathTrap by introducing carefully designed logical traps into the problem descriptions of MATH and GSM8K. Since problems with logical flaws are quite rare in the real world, these represent “unseen” cases to LLMs. Solving these requires the models to systematically compose (1) the mathematical knowledge involved in the original problems with (2) knowledge related to the introduced traps. Our experiments show that while LLMs possess both components of requisite knowledge, they do not <b>spontaneously</b> combine them to handle these novel cases. We explore several methods to mitigate this deficiency, such as natural language prompts, few-shot demonstrations, and fine-tuning. We find that LLMs’ performance can be improved through the above external intervention. Overall, systematic compositionality remains an open challenge for large language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.916.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.916.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--916 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.916 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.916/>Scaling Laws for Linear Complexity Language Models</a></strong><br><a href=/people/x/xuyang-shen/>Xuyang Shen</a>
|
<a href=/people/d/dong-li/>Dong Li</a>
|
<a href=/people/r/ruitao-leng/>Ruitao Leng</a>
|
<a href=/people/z/zhen-qin/>Zhen Qin</a>
|
<a href=/people/w/weigao-sun/>Weigao Sun</a>
|
<a href=/people/y/yiran-zhong/>Yiran Zhong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--916><div class="card-body p-3 small">The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, we examine the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay. We also include LLaMA as a baseline architecture for comparison with softmax attention. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.917.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--917 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.917/>Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards</a></strong><br><a href=/people/h/heejin-do/>Heejin Do</a>
|
<a href=/people/s/sangwon-ryu/>Sangwon Ryu</a>
|
<a href=/people/g/gary-lee/>Gary Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--917><div class="card-body p-3 small">Recent advances in automated essay scoring (AES) have shifted towards evaluating multiple traits to provide enriched feedback. Like typical AES systems, multi-trait AES employs the quadratic weighted kappa (QWK) to measure agreement with human raters, aligning closely with the rating schema; however, its non-differentiable nature prevents its direct use in neural network training. In this paper, we propose Scoring-aware Multi-reward Reinforcement Learning (SaMRL), which integrates actual evaluation schemes into the training process by designing QWK-based rewards with a mean-squared error penalty for multi-trait AES. Existing reinforcement learning (RL) applications in AES are limited to classification models despite associated performance degradation, as RL requires probability distributions; instead, we adopt an autoregressive score generation framework to leverage token generation probabilities for robust multi-trait score predictions. Empirical analyses demonstrate that SaMRL facilitates model training, notably enhancing scoring of previously inferior prompts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.918.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--918 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.918/>Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis</a></strong><br><a href=/people/g/guangliang-liu/>Guangliang Liu</a>
|
<a href=/people/h/haitao-mao/>Haitao Mao</a>
|
<a href=/people/j/jiliang-tang/>Jiliang Tang</a>
|
<a href=/people/k/kristen-johnson/>Kristen Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--918><div class="card-body p-3 small">Large Language Models (LLMs) are capable of producing content that perpetuates stereotypes, discrimination, and toxicity.The recently proposed <i>moral self-correction</i> is a computationally efficient method for reducing harmful content in the responses of LLMs. However, the process of how injecting self-correction instructions can modify the behavior of LLMs remains under-explored. In this paper, we explore the effectiveness of moral self-correction by answering three research questions: (1) In what scenarios does moral self-correction work? (2) What are the internal mechanisms of LLMs, e.g., hidden states, that are influenced by moral self-correction instructions? (3) Is intrinsic moral self-correction actually superficial in terms of reduced immorality in hidden states? We argue that self-correction can help LLMs find a shortcut to more morally correct output, rather than truly reducing the immorality stored in hidden states.Through empirical investigation with tasks of language generation and multi-choice question answering, we conclude: (i) LLMs exhibit good performance across both tasks, and self-correction instructions are particularly beneficial when the correct answer is already top-ranked; (ii) The morality levels in intermediate hidden states are strong indicators as to whether one instruction would be more effective than another; (iii) Based on our analysis of intermediate hidden states and task case studies of self-correction behaviors, we are first to propose the hypothesis that intrinsic moral self-correction is in fact superficial.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.919.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.919.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--919 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.919 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.919/><span class=acl-fixed-case>ATAP</span>: Automatic Template-Augmented Commonsense Knowledge Graph Completion via Pre-Trained Language Models</a></strong><br><a href=/people/f/fu-zhang/>Fu Zhang</a>
|
<a href=/people/y/yifan-ding/>Yifan Ding</a>
|
<a href=/people/j/jingwei-cheng/>Jingwei Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--919><div class="card-body p-3 small">The mission of commonsense knowledge graph completion (CKGC) is to infer missing facts from known commonsense knowledge. CKGC methods can be roughly divided into two categories: triple-based methods and text-based methods. Due to the imbalanced distribution of entities and limited structural information, triple-based methods struggle with long-tail entities. Text-based methods alleviate this issue, but require extensive training and fine-tuning of language models, which reduces efficiency. To alleviate these problems, we propose ATAP, the first CKGC framework that utilizes automatically generated continuous prompt templates combined with pre-trained language models (PLMs). Moreover, ATAP uses a carefully designed new prompt template training strategy, guiding PLMs to generate optimal prompt templates for CKGC tasks. Combining the rich knowledge of PLMs with the template automatic augmentation strategy, ATAP effectively mitigates the long-tail problem and enhances CKGC performance. Results on benchmark datasets show that ATAP achieves state-of-the-art performance overall.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.920.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.920.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--920 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.920 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.920/><span class=acl-fixed-case>LM</span>2: A Simple Society of Language Models Solves Complex Reasoning</a></strong><br><a href=/people/g/gurusha-juneja/>Gurusha Juneja</a>
|
<a href=/people/s/subhabrata-dutta/>Subhabrata Dutta</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--920><div class="card-body p-3 small">Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in LLM reasoning – a decomposer generates the subproblems, and a solver solves each of these subproblems. However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) – the decomposer does not keep track of the ability of the solver to follow the decomposed reasoning. In this paper, we propose LM2 to address these challenges. LM2 modularizes the decomposition, solution, and verification into three different language models. The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the reasoning requirement. The solver model generates the solution to the subproblems that are then checked by the verifier module; depending upon the feedback from the verifier, the reasoning context is constructed using the subproblems and the solutions. These models are trained to coordinate using policy learning. Exhaustive experimentation suggests the superiority of LM2 over existing methods on in- and out-domain reasoning problems, outperforming the best baselines by 8.1% on MATH, 7.71% on JEEBench, and 9.7% on MedQA problems (code available at https://github.com/ LCS2-IIITD/Language_Model_Multiplex).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.921.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.921.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--921 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.921 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.921/>Towards a Similarity-adjusted Surprisal Theory</a></strong><br><a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>
|
<a href=/people/t/tiago-pimentel/>Tiago Pimentel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--921><div class="card-body p-3 small">Surprisal theory posits that the cognitive effort required to comprehend a word is determined by its contextual predictability, quantified assurprisal. Traditionally, surprisal theory treats words as distinct entities, overlooking any potential similarity between them. Giulianelli et al. (2023) address this limitation by introducing information value, a measure of predictability designed to account for similarities between communicative units. Our work leverages Ricotta and Szeidl’s (2006) diversity index to extend surprisal into a metric that we term similarity-adjusted surprisal, exposing a mathematical relationship between surprisal and information value. Similarity-adjusted surprisal aligns with information value when considering graded similarities and reduces to standard surprisal when words are treated as distinct. Experimental results with reading time data indicate that similarity-adjusted surprisal adds predictive power beyond standard surprisal for certain datasets, suggesting it serves as a complementary measure of comprehension effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.922.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.922.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--922 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.922 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.922/>Multi-Level Information Retrieval Augmented Generation for Knowledge-based Visual Question Answering</a></strong><br><a href=/people/a/adjali-omar/>Adjali Omar</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/s/sahar-ghannay/>Sahar Ghannay</a>
|
<a href=/people/h/herve-le-borgne/>Hervé Le Borgne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--922><div class="card-body p-3 small">The Knowledge-Aware Visual Question Answering about Entity task aims to disambiguate entities using textual and visual information, as well as knowledge. It usually relies on two independent steps, information retrieval then reading comprehension, that do not benefit each other. Retrieval Augmented Generation (RAG) offers a solution by using generated answers as feedback for retrieval training. RAG usually relies solely on pseudo-relevant passages retrieved from external knowledge bases which can lead to ineffective answer generation. In this work, we propose a multi-level information RAG approach that enhances answer generation through entity retrieval and query expansion. We formulate a joint-training RAG loss such that answer generation is conditioned on both entity and passage retrievals. We show through experiments new state-of-the-art performance on the VIQuAE KB-VQA benchmark and demonstrate that our approach can help retrieve more actual relevant knowledge to generate accurate answers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.923.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.923.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--923 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.923 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.923.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.923/>Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?</a></strong><br><a href=/people/j/jianfeng-he/>Jianfeng He</a>
|
<a href=/people/r/runing-yang/>Runing Yang</a>
|
<a href=/people/l/linlin-yu/>Linlin Yu</a>
|
<a href=/people/c/changbin-li/>Changbin Li</a>
|
<a href=/people/r/ruoxi-jia/>Ruoxi Jia</a>
|
<a href=/people/f/feng-chen/>Feng Chen</a>
|
<a href=/people/m/ming-jin/>Ming Jin</a>
|
<a href=/people/c/chang-tien-lu/>Chang-Tien Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--923><div class="card-body p-3 small">Text summarization, a key natural language generation (NLG) task, is vital in various domains. However, the high cost of inaccurate summaries in risk-critical applications, particularly those involving human-in-the-loop decision-making, raises concerns about the reliability of uncertainty estimation on text summarization (UE-TS) evaluation methods. This concern stems from the dependency of uncertainty model metrics on diverse and potentially conflicting NLG metrics. To address this issue, we introduce a comprehensive UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The benchmark evaluates the uncertainty estimation capabilities of two large language models and one pre-trained language model on three datasets, with human-annotation analysis incorporated where applicable. We also assess the performance of 14 common uncertainty estimation methods within this benchmark. Our findings emphasize the importance of considering multiple uncorrelated NLG metrics and diverse uncertainty estimation methods to ensure reliable and efficient evaluation of UE-TS techniques. Our code and data are available: https://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.924.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.924.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--924 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.924 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.924/>Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/o/omer-goldman/>Omer Goldman</a>
|
<a href=/people/a/alon-jacovi/>Alon Jacovi</a>
|
<a href=/people/a/aviv-slobodkin/>Aviv Slobodkin</a>
|
<a href=/people/a/aviya-maimon/>Aviya Maimon</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--924><div class="card-body p-3 small">Improvements in language models’ capabilities have pushed their applications towards longer contexts, making long-context evaluation and development an active research area. However, many disparate use-cases are grouped together under the umbrella term of “long-context”, defined simply by the total length of the model’s input, including - for example - Needle-in-a-Haystack tasks, book summarization, and information aggregation. Given their varied difficulty, in this position paper we argue that conflating different tasks by their context length is unproductive. As a community, we require a more precise vocabulary to understand what makes long-context tasks similar or different. We propose to unpack the taxonomy of long-context based on the properties that make them more difficult with longer contexts. We propose two orthogonal axes of difficulty: (I) Diffusion: How hard is it to find the necessary information in the context? (II) Scope: How much necessary information is there to find? We survey the literature on long-context, provide justification for this taxonomy as an informative descriptor, and situate the literature with respect to it. We conclude that the most difficult and interesting settings, whose necessary information is very long and highly diffused within the input, is severely under-explored. By using a descriptive vocabulary and discussing the relevant properties of difficulty in long-context, we can implement more informed research in this area. We call for a careful design of tasks and benchmarks with distinctly long context, taking into account the characteristics that make it qualitatively different from shorter context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.925.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.925.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--925 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.925 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.925.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.925/><span class=acl-fixed-case>BPE</span> Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training</a></strong><br><a href=/people/p/pavel-chizhov/>Pavel Chizhov</a>
|
<a href=/people/c/catherine-arnett/>Catherine Arnett</a>
|
<a href=/people/e/elizaveta-korotkova/>Elizaveta Korotkova</a>
|
<a href=/people/i/ivan-p-yamshchikov/>Ivan P. Yamshchikov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--925><div class="card-body p-3 small">Language models can greatly benefit from efficient tokenization. However, they still mostly utilize the classical Byte-Pair Encoding (BPE) algorithm, a simple and reliable method. BPE has been shown to cause such issues as under-trained tokens and sub-optimal compression that may affect the downstream performance. We introduce PickyBPE, a modified BPE algorithm that carries out vocabulary refinement during tokenizer training by removing merges that leave intermediate “junk” tokens. Our method improves vocabulary efficiency, eliminates under-trained tokens, and does not compromise text compression. Our experiments show that this method either improves downstream performance or does not harm it.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.926.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.926.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--926 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.926 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.926/><span class=acl-fixed-case>SEGMENT</span>+: Long Text Processing with Short-Context Language Models</a></strong><br><a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/s/shuang-li/>Shuang Li</a>
|
<a href=/people/k/kerun-yu/>Kerun Yu</a>
|
<a href=/people/j/jinglei-chen/>Jinglei Chen</a>
|
<a href=/people/z/zujie-liang/>Zujie Liang</a>
|
<a href=/people/x/xinhui-wu/>Xinhui Wu</a>
|
<a href=/people/y/yuxi-qian/>Yuxi Qian</a>
|
<a href=/people/f/feng-wei/>Feng Wei</a>
|
<a href=/people/b/bo-zheng/>Bo Zheng</a>
|
<a href=/people/j/jiaqing-liang/>Jiaqing Liang</a>
|
<a href=/people/j/jiangjie-chen/>Jiangjie Chen</a>
|
<a href=/people/y/yanghua-xiao/>Yanghua Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--926><div class="card-body p-3 small">There is a growing interest in expanding the input capacity of language models (LMs) across various domains. However, simply increasing the context window does not guarantee robust performance across diverse long-input processing tasks, such as understanding extensive documents and extracting detailed information from lengthy and noisy data. In response, we introduce Segment+, a general framework that enables LMs to handle extended inputs within limited context windows efficiently. Segment+ utilizes structured notes and a filtering module to manage information flow, resulting in a system that is both controllable and interpretable. Our extensive experiments across various model sizes, focusing on long-document question-answering and Needle-in-a-Haystack tasks, demonstrate the effectiveness of Segment+ in improving performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.927.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.927.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.927/>Explicit Memory Learning with Expectation Maximization</a></strong><br><a href=/people/z/zhangyue-yin/>Zhangyue Yin</a>
|
<a href=/people/q/qiushi-sun/>Qiushi Sun</a>
|
<a href=/people/q/qipeng-guo/>Qipeng Guo</a>
|
<a href=/people/z/zhiyuan-zeng/>Zhiyuan Zeng</a>
|
<a href=/people/q/qinyuan-cheng/>Qinyuan Cheng</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.928.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.928.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--928 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.928 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.928/>Closing the Loop: Learning to Generate Writing Feedback via Language Model Simulated Student Revisions</a></strong><br><a href=/people/i/inderjeet-jayakumar-nair/>Inderjeet Jayakumar Nair</a>
|
<a href=/people/j/jiaye-tan/>Jiaye Tan</a>
|
<a href=/people/x/xiaotian-su/>Xiaotian Su</a>
|
<a href=/people/a/anne-gere/>Anne Gere</a>
|
<a href=/people/x/xu-wang/>Xu Wang</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--928><div class="card-body p-3 small">Providing feedback is widely recognized as crucial for refining students’ writing skills. Recent advances in language models (LMs) have made it possible to automatically generate feedback that is actionable and well-aligned with human-specified attributes. However, it remains unclear whether the feedback generated by these models is truly effective in enhancing the quality of student revisions. Moreover, prompting LMs with a precise set of instructions to generate feedback is nontrivial due to the lack of consensus regarding the specific attributes that can lead to improved revising performance. To address these challenges, we propose PROF that PROduces Feedback via learning from LM simulated student revisions. PROF aims to iteratively optimize the feedback generator by directly maximizing the effectiveness of students’ overall revising performance as simulated by LMs. Focusing on an economic essay assignment, we empirically test the efficacy of PROF and observe that our approach not only surpasses a variety of baseline methods in effectiveness of improving students’ writing but also demonstrates enhanced pedagogical values, even though it was not explicitly trained for this aspect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.929.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.929.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--929 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.929 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.929.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.929/>Small <span class=acl-fixed-case>LLM</span>s Are Weak Tool Learners: A Multi-<span class=acl-fixed-case>LLM</span> Agent</a></strong><br><a href=/people/w/weizhou-shen/>Weizhou Shen</a>
|
<a href=/people/c/chenliang-li/>Chenliang Li</a>
|
<a href=/people/h/hongzhan-chen/>Hongzhan Chen</a>
|
<a href=/people/m/ming-yan/>Ming Yan</a>
|
<a href=/people/x/xiaojun-quan/>Xiaojun Quan</a>
|
<a href=/people/h/hehong-chen/>Hehong Chen</a>
|
<a href=/people/j/ji-zhang/>Ji Zhang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--929><div class="card-body p-3 small">Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.930.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.930.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--930 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.930 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.930/>Interpreting Context Look-ups in Transformers: Investigating Attention-<span class=acl-fixed-case>MLP</span> Interactions</a></strong><br><a href=/people/c/clement-neo/>Clement Neo</a>
|
<a href=/people/s/shay-b-cohen/>Shay B Cohen</a>
|
<a href=/people/f/fazl-barez/>Fazl Barez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--930><div class="card-body p-3 small">Understanding the inner workings of large language models (LLMs) is crucial for advancing their theoretical foundations and real-world applications. While the attention mechanism and multi-layer perceptrons (MLPs) have been studied independently, their interactions remain largely unexplored. This study investigates how attention heads and next-token neurons interact in LLMs to predict new words. We propose a methodology to identify next-token neurons, find prompts that highly activate them, and determine the upstream attention heads responsible. We then generate and evaluate explanations for the activity of these attention heads in an automated manner. Our findings reveal that some attention heads recognize specific contexts relevant to predicting a token and activate a downstream token-predicting neuron accordingly. This mechanism provides a deeper understanding of how attention heads work with MLP neurons to perform next-token prediction. Our approach offers a foundation for further research into the intricate workings of LLMs and their impact on text generation and understanding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.931.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.931.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--931 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.931 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.931.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.931/>Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis</a></strong><br><a href=/people/a/amey-hengle/>Amey Hengle</a>
|
<a href=/people/a/atharva-kulkarni/>Atharva Kulkarni</a>
|
<a href=/people/s/shantanu-deepak-patankar/>Shantanu Deepak Patankar</a>
|
<a href=/people/m/madhumitha-chandrasekaran/>Madhumitha Chandrasekaran</a>
|
<a href=/people/s/sneha-dsilva/>Sneha D’silva</a>
|
<a href=/people/j/jemima-s-jacob/>Jemima S. Jacob</a>
|
<a href=/people/r/rashmi-gupta/>Rashmi Gupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--931><div class="card-body p-3 small">In this study, we introduce ANGST, a novel, first of its kind benchmark for depression-anxiety comorbidity classification from social media posts. Unlike contemporary datasets that often oversimplify the intricate interplay between different mental health disorders by treating them as isolated conditions, ANGST enables multi-label classification, allowing each post to be simultaneously identified as indicating depression and/or anxiety. Comprising 2876 meticulously annotated posts by expert psychologists and an additional 7667 silver-labeled posts, ANGST posits a more representative sample of online mental health discourse. Moreover, we benchmark ANGST using various state-of-the-art language models, ranging from Mental-BERT to GPT-4. Our results provide significant insights into the capabilities and limitations of these models in complex diagnostic scenarios. While GPT-4 generally outperforms other models, none achieve an F1 score exceeding 72% in multi-class comorbid classification, underscoring the ongoing challenges in applying language models to mental health diagnostics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.932.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.932.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--932 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.932 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.932/>The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning</a></strong><br><a href=/people/s/shaobo-cui/>Shaobo Cui</a>
|
<a href=/people/z/zhijing-jin/>Zhijing Jin</a>
|
<a href=/people/b/bernhard-scholkopf/>Bernhard Schölkopf</a>
|
<a href=/people/b/boi-faltings/>Boi Faltings</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--932><div class="card-body p-3 small">Understanding commonsense causality is a unique mark of intelligence for humans. It helps people understand the principles of the real world better and benefits the decision-making process related to causation. For instance, commonsense causality is crucial in judging whether a defendant’s action causes the plaintiff’s loss in determining legal liability. Despite its significance, a systematic exploration of this topic is notably lacking. Our comprehensive survey bridges this gap by focusing on taxonomies, benchmarks, acquisition methods, qualitative reasoning, and quantitative measurements in commonsense causality, synthesizing insights from over 200 representative articles. Our work aims to provide a systematic overview, update scholars on recent advancements, provide a practical guide for beginners, and highlight promising future research directions in this vital field. A summary of the related literature is available at https://github.com/cui-shaobo/causality-papers .</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.933.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.933.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--933 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.933 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.933/>Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups</a></strong><br><a href=/people/r/razvan-alexandru-smadu/>Răzvan-Alexandru Smădu</a>
|
<a href=/people/d/david-gabriel-ion/>David-Gabriel Ion</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a>
|
<a href=/people/f/florin-pop/>Florin Pop</a>
|
<a href=/people/m/mihaela-claudia-cercel/>Mihaela-Claudia Cercel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--933><div class="card-body p-3 small">Complex Word Identification (CWI) is an essential step in the lexical simplification task and has recently become a task on its own. Some variations of this binary classification task have emerged, such as lexical complexity prediction (LCP) and complexity evaluation of multi-word expressions (MWE). Large language models (LLMs) recently became popular in the Natural Language Processing community because of their versatility and capability to solve unseen tasks in zero/few-shot settings. Our work investigates LLM usage, specifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and closed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE settings. We evaluate zero-shot, few-shot, and fine-tuning settings and show that LLMs struggle in certain conditions or achieve comparable results against existing methods. In addition, we provide some views on meta-learning combined with prompt learning. In the end, we conclude that the current state of LLMs cannot or barely outperform existing methods, which are usually much smaller.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.934.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.934.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--934 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.934 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.934/>Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue</a></strong><br><a href=/people/j/jia-chen-gu/>Jia-Chen Gu</a>
|
<a href=/people/h/hao-xiang-xu/>Hao-Xiang Xu</a>
|
<a href=/people/j/jun-yu-ma/>Jun-Yu Ma</a>
|
<a href=/people/p/pan-lu/>Pan Lu</a>
|
<a href=/people/z/zhen-hua-ling/>Zhen-Hua Ling</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--934><div class="card-body p-3 small">Model editing is a technique that edits the large language models (LLMs) with updated knowledge to alleviate hallucinations without resource-intensive retraining. While current model editing methods can effectively modify a model’s behavior within a specific area of interest, they often overlook the potential unintended side effects on the general abilities of LLMs such as reasoning, natural language inference, and question answering. In this paper, we raise concerns that model editing’s improvements on factuality may come at the cost of a significant degradation of the model’s general abilities. We systematically analyze the side effects by evaluating four popular editing methods on three LLMs across eight representative tasks. Our extensive empirical experiments show that it is challenging for current editing methods to simultaneously improve factuality of LLMs and maintain their general abilities. Our analysis reveals that the side effects are caused by model editing altering the original model weights excessively, leading to overfitting to the edited facts. To mitigate this, a method named RECT is proposed to regularize the edit update weights by imposing constraints on their complexity based on the RElative Change in weighT. Evaluation results show that RECT can significantly mitigate the side effects of editing while still maintaining over 94% editing performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.935.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.935.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.935/>Are Large Language Models In-Context Personalized Summarizers? Get an i<span class=acl-fixed-case>COPERNICUS</span> Test Done!</a></strong><br><a href=/people/d/divya-patel/>Divya Patel</a>
|
<a href=/people/p/pathik-patel/>Pathik Patel</a>
|
<a href=/people/a/ankush-chander/>Ankush Chander</a>
|
<a href=/people/s/sourish-dasgupta/>Sourish Dasgupta</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.936.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.936.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--936 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.936 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.936.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.936/><span class=acl-fixed-case>M</span>edi<span class=acl-fixed-case>TOD</span>: An <span class=acl-fixed-case>E</span>nglish Dialogue Dataset for Medical History Taking with Comprehensive Annotations</a></strong><br><a href=/people/v/vishal-vivek-saley/>Vishal Vivek Saley</a>
|
<a href=/people/g/goonjan-saha/>Goonjan Saha</a>
|
<a href=/people/r/rocktim-jyoti-das/>Rocktim Jyoti Das</a>
|
<a href=/people/d/dinesh-raghu/>Dinesh Raghu</a>
|
<a href=/people/m/mausam/>Mausam .</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--936><div class="card-body p-3 small">Medical task-oriented dialogue systems can assist doctors by collecting patient medical history, aiding in diagnosis, or guiding treatment selection, thereby reducing doctor burnout and expanding access to medical services. However, doctor-patient dialogue datasets are not readily available, primarily due to privacy regulations. Moreover, existing datasets lack comprehensive annotations involving medical slots and their different attributes, such as symptoms and their onset, progression, and severity. These comprehensive annotations are crucial for accurate diagnosis. Finally, most existing datasets are non-English, limiting their utility for the larger research community.In response, we introduce MediTOD, a new dataset of doctor-patient dialogues in English for the medical history-taking task. Collaborating with doctors, we devise a questionnaire-based labeling scheme tailored to the medical domain. Then, medical professionals create the dataset with high-quality comprehensive annotations, capturing medical slots and their attributes. We establish benchmarks in supervised and few-shot settings on MediTOD for natural language understanding, policy learning, and natural language generation subtasks, evaluating models from both TOD and biomedical domains. We make MediTOD publicly available for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.937.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.937.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--937 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.937 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.937.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.937/>***<span class=acl-fixed-case>Y</span>es<span class=acl-fixed-case>B</span>ut***: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models</a></strong><br><a href=/people/a/abhilash-nandy/>Abhilash Nandy</a>
|
<a href=/people/y/yash-agarwal/>Yash Agarwal</a>
|
<a href=/people/a/ashish-patwa/>Ashish Patwa</a>
|
<a href=/people/m/millon-madhur-das/>Millon Madhur Das</a>
|
<a href=/people/a/aman-bansal/>Aman Bansal</a>
|
<a href=/people/a/ankit-raj/>Ankit Raj</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a>
|
<a href=/people/n/niloy-ganguly/>Niloy Ganguly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--937><div class="card-body p-3 small">Understanding satire and humor is a challenging task for even current Vision-Language models. In this paper, we propose the challenging tasks of Satirical Image Detection (detecting whether an image is satirical), Understanding (generating the reason behind the image being satirical), and Completion (given one half of the image, selecting the other half from 2 given options, such that the complete image is satirical) and release a high-quality dataset ***YesBut***, consisting of 2547 images, 1084 satirical and 1463 non-satirical, containing different artistic styles, to evaluate those tasks. Each satirical image in the dataset depicts a normal scenario, along with a conflicting scenario which is funny or ironic. Despite the success of current Vision-Language Models on multimodal tasks such as Visual QA and Image Captioning, our benchmarking experiments show that such models perform poorly on the proposed tasks on the ***YesBut*** Dataset in Zero-Shot Settings w.r.t both automated as well as human evaluation. Additionally, we release a dataset of 119 real, satirical photographs for further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.938.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.938.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--938 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.938 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.938/>Working Memory Identifies Reasoning Limits in Language Models</a></strong><br><a href=/people/c/chunhui-zhang/>Chunhui Zhang</a>
|
<a href=/people/y/yiren-jian/>Yiren Jian</a>
|
<a href=/people/z/zhongyu-ouyang/>Zhongyu Ouyang</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--938><div class="card-body p-3 small">This study explores the inherent limitations of large language models (LLMs) from a scaling perspective, focusing on the upper bounds of their cognitive capabilities. We integrate insights from cognitive science to quantitatively examine how LLMs perform on n-back tasks—a benchmark used to assess working memory, which involves temporarily holding and manipulating information. Our findings reveal that despite the increased model size, LLMs still face significant challenges in holding and processing information effectively, especially under complex task conditions. We also assess various prompting strategies, revealing their diverse impacts on LLM performance. The results highlight the struggle of current LLMs to autonomously discover optimal problem-solving patterns without heavily relying on manually corrected prompts. To move beyond these constraints, fundamental improvements in the planning and search of LLMs are essential for them to reason autonomously. Improving these capabilities will reduce the reliance on external corrections and enable LLMs to become more autonomous in their problem-solving processes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.939.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.939.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--939 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.939 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.939/><span class=acl-fixed-case>RAFT</span>: Realistic Attacks to Fool Text Detectors</a></strong><br><a href=/people/j/james-liyuan-wang/>James Liyuan Wang</a>
|
<a href=/people/r/ran-li/>Ran Li</a>
|
<a href=/people/j/junfeng-yang/>Junfeng Yang</a>
|
<a href=/people/c/chengzhi-mao/>Chengzhi Mao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--939><div class="card-body p-3 small">Large language models (LLMs) have exhibited remarkable fluency across various tasks. However, their unethical applications, such as disseminating disinformation, have become a growing concern. Although recent works have proposed a number of LLM detection methods, their robustness and reliability remain unclear. In this paper, we present RAFT: a grammar error-free black-box attack against existing LLM detectors. In contrast to previous attacks for language models, our method exploits the transferability of LLM embeddings at the word-level while preserving the original text quality. We leverage an auxiliary embedding to greedily select candidate words to perturb against the target detector. Experiments reveal that our attack effectively compromises all detectors in the study across various domains by up to 99%, and are transferable across source models. Manual human evaluation studies show our attacks are realistic and indistinguishable from original human-written text. We also show that examples generated by RAFT can be used to train adversarially robust detectors. Our work shows that current LLM detectors are not adversarially robust, underscoring the urgent need for more resilient detection mechanisms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.940.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.940.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--940 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.940 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.940/><span class=acl-fixed-case>LLM</span>-Evolve: Evaluation for <span class=acl-fixed-case>LLM</span>’s Evolving Capability on Benchmarks</a></strong><br><a href=/people/j/jiaxuan-you/>Jiaxuan You</a>
|
<a href=/people/m/mingjie-liu/>Mingjie Liu</a>
|
<a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/m/mostofa-patwary/>Mostofa Patwary</a>
|
<a href=/people/m/mohammad-shoeybi/>Mohammad Shoeybi</a>
|
<a href=/people/b/bryan-catanzaro/>Bryan Catanzaro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--940><div class="card-body p-3 small">The advancement of large language models (LLMs) has extended their use to dynamic and interactive real-world applications, where models engage continuously with their environment and potentially enhance their performance over time. Most existing LLM benchmarks evaluate LLMs on i.i.d. tasks, overlooking their ability to learn iteratively from past experiences. Our paper bridges this evaluation gap by proposing a novel framework, LLM-Evolve, which extends established benchmarks to sequential problem-solving settings. LLM-Evolve evaluates LLMs over multiple rounds, providing feedback after each round to build a demonstration memory that the models can query in future tasks. We applied LLM-Evolve to the MMLU, GSM8K, and AgentBench benchmarks, testing 8 state-of-the-art open-source and closed-source models. Results show that LLMs can achieve performance improvements of up to 17% by learning from past interactions, with the quality of retrieval algorithms and feedback significantly influencing this capability. These insights advocate for more understanding and benchmarks for LLMs’ performance in evolving interactive scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.941.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.941.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--941 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.941 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.941/><span class=acl-fixed-case>FFN</span>-<span class=acl-fixed-case>S</span>kip<span class=acl-fixed-case>LLM</span>: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping</a></strong><br><a href=/people/a/ajay-kumar-jaiswal/>Ajay Kumar Jaiswal</a>
|
<a href=/people/b/bodun-hu/>Bodun Hu</a>
|
<a href=/people/l/lu-yin/>Lu Yin</a>
|
<a href=/people/y/yeonju-ro/>Yeonju Ro</a>
|
<a href=/people/t/tianlong-chen/>Tianlong Chen</a>
|
<a href=/people/s/shiwei-liu/>Shiwei Liu</a>
|
<a href=/people/a/aditya-akella/>Aditya Akella</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--941><div class="card-body p-3 small">Autoregressive Large Language Models (e.g., LLaMa, GPTs) are omnipresent achieving remarkable success in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges for autoregressive token-by-token generation. To mitigate computation overload incurred during generation, several early-exit and layer-dropping strategies have been proposed. Despite some promising success due to the redundancy across LLMs layers on metrics like Rough-L/BLUE, our careful knowledge-intensive evaluation unveils issues such as generation collapse, hallucination, and noticeable performance drop even at the trivial exit ratio of ~10-15% of layers. We attribute these errors primarily to ineffective handling of the KV cache through state copying during early exit. In this work, we observe the saturation of computationally expensive feed-forward blocks of LLM layers and propose FFN-SkipLLM, which is a novel fine-grained skip strategy for autoregressive LLMs. FFN-SkipLLM leverages an input-adaptive feed-forward skipping approach that can skip ~25-30% of FFN blocks of LLMs with marginal change in performance on knowledge-intensive generation tasks without any requirement to handle the KV cache. Our extensive experiments and ablation studies across benchmarks like MT-Bench, Factoid-QA, and variable-length text summarization illustrate how our simple and easy-to-use method can facilitate faster autoregressive decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.942.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.942.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--942 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.942 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.942/><span class=acl-fixed-case>LLM</span>-based Code-Switched Text Generation for Grammatical Error Correction</a></strong><br><a href=/people/t/tom-potter/>Tom Potter</a>
|
<a href=/people/z/zheng-yuan/>Zheng Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--942><div class="card-body p-3 small">With the rise of globalisation, code-switching (CSW) has become a ubiquitous part of multilingual conversation, posing new challenges for natural language processing (NLP), especially in Grammatical Error Correction (GEC). This work explores the complexities of applying GEC systems to CSW texts. Our objectives include evaluating the performance of state-of-the-art GEC systems on an authentic CSW dataset from English as a Second Language (ESL) learners, exploring synthetic data generation as a solution to data scarcity, and developing a model capable of correcting grammatical errors in monolingual and CSW texts. We generated synthetic CSW GEC data, resulting in one of the first substantial datasets for this task, and showed that a model trained on this data is capable of significant improvements over existing systems. This work targets ESL learners, aiming to provide educational technologies that aid in the development of their English grammatical correctness without constraining their natural multilingualism.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.943.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.943.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--943 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.943 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.943/>Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models</a></strong><br><a href=/people/m/mehrdad-farahani/>Mehrdad Farahani</a>
|
<a href=/people/r/richard-johansson/>Richard Johansson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--943><div class="card-body p-3 small">Generative language models often struggle with specialized or less-discussed knowledge. A potential solution is found in Retrieval-Augmented Generation (RAG) models which act like retrieving information before generating responses. In this study, we explore how the Atlas approach, a RAG model, decides between what it already knows (parametric) and what it retrieves (non-parametric). We use causal mediation analysis and controlled experiments to examine how internal representations influence information processing. Our findings disentangle the effects of parametric knowledge and the retrieved context. They indicate that in cases where the model can choose between both types of information (parametric and non-parametric), it relies more on the context than the parametric knowledge. Furthermore, the analysis investigates the computations involved in <i>how</i> the model uses the information from the context. We find that multiple mechanisms are active within the model and can be detected with mediation analysis: first, the decision of <i>whether the context is relevant</i>, and second, how the encoder computes output representations to support copying when relevant.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.944.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.944.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--944 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.944 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.944.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.944.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.944/>On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning</a></strong><br><a href=/people/g/geewook-kim/>Geewook Kim</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--944><div class="card-body p-3 small">Recent advancements in language and vision assistants have showcased impressive capabilities but suffer from a lack of transparency, limiting broader research and reproducibility. While open-source models handle general image tasks effectively, they face challenges with the high computational demands of complex visually-situated text understanding. Such tasks often require increased token inputs and large vision modules to harness high-resolution information. Striking a balance between model size and data importance remains an open question. This study aims to redefine the design of vision-language models by identifying key components and creating efficient models with constrained inference costs. By strategically formulating datasets, optimizing vision modules, and enhancing supervision techniques, we achieve significant improvements in inference throughput while maintaining high performance. Extensive experiments across models ranging from 160M to 13B parameters offer insights into model optimization.We will fully open-source our codebase, models, and datasets at https://github.com/naver-ai/elva.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.945.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.945.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--945 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.945 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.945.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.945/>Community-Cross-Instruct: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities</a></strong><br><a href=/people/z/zihao-he/>Zihao He</a>
|
<a href=/people/m/minh-duc-chu/>Minh Duc Chu</a>
|
<a href=/people/r/rebecca-dorn/>Rebecca Dorn</a>
|
<a href=/people/s/siyi-guo/>Siyi Guo</a>
|
<a href=/people/k/kristina-lerman/>Kristina Lerman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--945><div class="card-body p-3 small">Social scientists use surveys to probe the opinions and beliefs of populations, but these methods are slow, costly, and prone to biases. Recent advances in large language models (LLMs) enable the creating of computational representations or “digital twins” of populations that generate human-like responses mimicking the population’s language, styles, and attitudes. We introduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs to online communities to elicit their beliefs. Given a corpus of a community’s online discussions, Community-Cross-Instruct automatically generates instruction-output pairs by an advanced LLM to (1) finetune a foundational LLM to faithfully represent that community, and (2) evaluate the alignment of the finetuned model to the community. We demonstrate the method’s utility in accurately representing political and diet communities on Reddit. Unlike prior methods requiring human-authored instructions, Community-Cross-Instruct generates instructions in a fully unsupervised manner, enhancing scalability and generalization across domains. This work enables cost-effective and automated surveying of diverse online communities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.946.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.946.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--946 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.946 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.946.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.946/>Mathador-<span class=acl-fixed-case>LM</span>: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models</a></strong><br><a href=/people/e/eldar-kurtic/>Eldar Kurtic</a>
|
<a href=/people/a/amir-moeini/>Amir Moeini</a>
|
<a href=/people/d/dan-alistarh/>Dan Alistarh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--946><div class="card-body p-3 small">We introduce Mathador-LM, a new benchmark for evaluating the mathematical reasoning on large language models (LLMs), combining ruleset interpretation, planning, and problem-solving. This benchmark is inspired by the Mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances dynamically, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. Additionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary models struggle with Mathador-LM, scoring significantly lower than average 3rd graders. This stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks. The implementation of Mathador-LM benchmark is available at https://github.com/IST-DASLab/Mathador-LM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.947.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.947.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--947 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.947 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.947/>Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models</a></strong><br><a href=/people/y/yuan-hong-liao/>Yuan-Hong Liao</a>
|
<a href=/people/r/rafid-mahmood/>Rafid Mahmood</a>
|
<a href=/people/s/sanja-fidler/>Sanja Fidler</a>
|
<a href=/people/d/david-acuna/>David Acuna</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--947><div class="card-body p-3 small">Despite recent advances demonstrating vision- language models’ (VLMs) abilities to describe complex relationships among objects in images using natural language, their capability to quantitatively reason about object sizes and distances remains underexplored. In this work, we introduce a manually annotated benchmark of 241 questions across five categories specifically designed for quantitative spatial reasoning, and systematically investigate the performance of SoTA VLMs on this task. Our analysis reveals that questions involving reasoning about distances between objects are particularly challenging for SoTA VLMs; however, some VLMs perform significantly better at this task than others, with an almost 40 points gap between the two best performing models. We also make the surprising observation that the success rate of the top-performing VLM increases by 19 points when a reasoning path using a reference object emerges naturally in the response. Inspired by this observation, we develop a zero-shot prompting technique, SpatialPrompt, that encourages VLMs to answer quantitative spatial questions using references objects as visual cues. Specifically, we demonstrate that instruct- ing VLMs to use reference objects in their reasoning paths significantly improves their quantitative spatial reasoning performance, bypassing the need for external data, architectural modifications, or fine-tuning. Remarkably, by solely using SpatialPrompt, Gemini 1.5 Pro, GPT-4V, and GPT-4o improve by 56.2, 28.5, and 6.7 points on average in Q-Spatial Bench without the need for more data, model architectural modifications, or fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.948.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.948.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--948 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.948 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.948/>One Thousand and One Pairs: A “novel” challenge for long-context language models</a></strong><br><a href=/people/m/marzena-karpinska/>Marzena Karpinska</a>
|
<a href=/people/k/katherine-thai/>Katherine Thai</a>
|
<a href=/people/k/kyle-lo/>Kyle Lo</a>
|
<a href=/people/t/tanya-goyal/>Tanya Goyal</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--948><div class="card-body p-3 small">Synthetic long-context LLM benchmarks (e.g., “needle-in-the-haystack”) test only surface-level retrieval capabilities; but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest pair accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.949.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.949.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--949 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.949 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.949/>Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation</a></strong><br><a href=/people/t/tu-vu/>Tu Vu</a>
|
<a href=/people/k/kalpesh-krishna/>Kalpesh Krishna</a>
|
<a href=/people/s/salaheddin-alzubi/>Salaheddin Alzubi</a>
|
<a href=/people/c/chris-tar/>Chris Tar</a>
|
<a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/y/yun-hsuan-sung/>Yun-Hsuan Sung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--949><div class="card-body p-3 small">As large language models (LLMs) evolve, evaluating their output reliably becomes increasingly difficult due to the high cost of human evaluation. To address this, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on a diverse set of over 100 quality assessment tasks, incorporating 5M+ human judgments curated from publicly released human evaluations. FLAMe outperforms models like GPT-4 and Claude-3 on various held-out tasks, and serves as a powerful starting point for fine-tuning, as shown in our reward model evaluation case study (FLAMe-RM). On Reward-Bench, FLAMe-RM-24B achieves 87.8% accuracy, surpassing GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we introduce FLAMe-Opt-RM, an efficient tail-patch fine-tuning approach that offers competitive RewardBench performance using 25×fewer training datapoints. Our FLAMe variants outperform popular proprietary LLM-as-a-Judge models on 8 of 12 autorater benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis shows that FLAMe is significantly less biased than other LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.950.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.950.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--950 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.950 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.950/>Do <span class=acl-fixed-case>LLM</span>s learn a true syntactic universal?</a></strong><br><a href=/people/j/john-hale/>John T. Hale</a>
|
<a href=/people/m/milos-stanojevic/>Miloš Stanojević</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--950><div class="card-body p-3 small">Do large multilingual language models learn language universals? We consider a candidate universal much-discussed in the linguistics literature, the Final-over-Final Condition (Sheehan et al., 2017b). This Condition is syntactic in the sense that it can only be stated by reference to abstract sentence properties such as nested phrases and head direction. A study of typologically diverse “mixed head direction” languages confirms that the Condition holds in corpora. But in a targeted syntactic evaluation, Gemini Pro only seems to respect the Condition in German, Russian, Hungarian and Serbian. These relatively high-resource languages contrast with Basque, where Gemini Pro does not seem to have learned the Condition at all. This result suggests that modern language models may need additional sources of bias in order to become truly human-like, within a developmentally-realistic budget of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.951.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.951.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--951 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.951 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.951/><span class=acl-fixed-case>GDPO</span>: Learning to Directly Align Language Models with Diversity Using <span class=acl-fixed-case>GF</span>low<span class=acl-fixed-case>N</span>ets</a></strong><br><a href=/people/o/oh-joon-kwon/>Oh Joon Kwon</a>
|
<a href=/people/d/daiki-e-matsunaga/>Daiki E. Matsunaga</a>
|
<a href=/people/k/kee-eung-kim/>Kee-Eung Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--951><div class="card-body p-3 small">A critical component of the current generation of language models is preference alignment, which aims to precisely control the model’s behavior to meet human needs and values. The most notable among such methods is Reinforcement Learning with Human Feedback (RLHF) and its offline variant Direct Preference Optimization (DPO), both of which seek to maximize a reward model based on human preferences. In particular, DPO derives reward signals directly from the offline preference data, but in doing so overfits the reward signals and generates suboptimal responses that may contain human biases in the dataset. In this work, we propose a practical application of a diversity-seeking RL algorithm called GFlowNet-DPO (GDPO) in an offline preference alignment setting to curtail such challenges. Empirical results show GDPO can generate far more diverse responses than the baseline methods that are still relatively aligned with human values in dialog generation and summarization tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.952.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.952.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--952 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.952 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.952.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.952/>How Susceptible are Large Language Models to Ideological Manipulation?</a></strong><br><a href=/people/k/kai-chen/>Kai Chen</a>
|
<a href=/people/z/zihao-he/>Zihao He</a>
|
<a href=/people/j/jun-yan/>Jun Yan</a>
|
<a href=/people/t/taiwei-shi/>Taiwei Shi</a>
|
<a href=/people/k/kristina-lerman/>Kristina Lerman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--952><div class="card-body p-3 small">Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs’ ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.953.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.953.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--953 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.953 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.953/>Measuring Psychological Depth in Language Models</a></strong><br><a href=/people/f/fabrice-y-harel-canada/>Fabrice Y Harel-Canada</a>
|
<a href=/people/h/hanyu-zhou/>Hanyu Zhou</a>
|
<a href=/people/s/sreya-muppalla/>Sreya Muppalla</a>
|
<a href=/people/z/zeynep-senahan-yildiz/>Zeynep Senahan Yildiz</a>
|
<a href=/people/m/miryung-kim/>Miryung Kim</a>
|
<a href=/people/a/amit-sahai/>Amit Sahai</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--953><div class="card-body p-3 small">Evaluations of creative stories generated by large language models (LLMs) often focus on objective properties of the text, such as its style, coherence, and diversity. While these metrics are indispensable, they do not speak to a story’s subjective, psychological impact from a reader’s perspective. We introduce the Psychological Depth Scale (PDS), a novel framework rooted in literary theory that measures an LLM’s ability to produce authentic and narratively complex stories that provoke emotion, empathy, and engagement. We empirically validate our framework by showing that humans can consistently evaluate stories based on PDS (0.72 Krippendorff’s alpha). We also explore techniques for automating the PDS to easily scale future analyses. GPT-4o, combined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an average Spearman correlation of 0.51 with human judgment while Llama-3-70B with constrained decoding scores as high as 0.68 for empathy. Finally, we compared the depth of stories authored by both humans and LLMs. Surprisingly, GPT-4 stories either surpassed or were statistically indistinguishable from highly-rated human-written stories sourced from Reddit. By shifting the focus from text to reader, the Psychological Depth Scale is a validated, automated, and systematic means of measuring the capacity of LLMs to connect with humans through the stories they tell.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.954.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.954.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--954 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.954 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.954/>Media Attitude Detection via Framing Analysis with Events and their Relations</a></strong><br><a href=/people/j/jin-zhao/>Jin Zhao</a>
|
<a href=/people/j/jingxuan-tu/>Jingxuan Tu</a>
|
<a href=/people/h/han-du/>Han Du</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--954><div class="card-body p-3 small">Framing is used to present some selective aspects of an issue and making them more salient, which aims to promote certain values, interpretations, or solutions (Entman, 1993). This study investigates the nuances of media framing on public perception and understanding by examining how events are presented within news articles. Unlike previous research that primarily focused on word choice as a framing device, this work explores the comprehensive narrative construction through events and their relations. Our method integrates event extraction, cross-document event coreference, and causal relationship mapping among events to extract framing devices employed by media to assess their role in framing the narrative. We evaluate our approach with a media attitude detection task and show that the use of event mentions, event cluster descriptors, and their causal relations effectively captures the subtle nuances of framing, thereby providing deeper insights into the attitudes conveyed by news articles. The experimental results show the framing device models surpass the baseline models and offers a more detailed and explainable analysis of media framing effects. We make the source code and dataset publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.955.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.955.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--955 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.955 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.955/>Fill In The Gaps: Model Calibration and Generalization with Synthetic Data</a></strong><br><a href=/people/y/yang-ba/>Yang Ba</a>
|
<a href=/people/m/michelle-v-mancenido/>Michelle V Mancenido</a>
|
<a href=/people/r/rong-pan/>Rong Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--955><div class="card-body p-3 small">As machine learning models continue to swiftly advance, calibrating their performance has become a major concern prior to practical and widespread implementation. Most existing calibration methods often negatively impact model accuracy due to the lack of diversity of validation data, resulting in reduced generalizability. To address this, we propose a calibration method that incorporates synthetic data without compromising accuracy. We derive the expected calibration error (ECE) bound using the Probably Approximately Correct (PAC) learning framework. Large language models (LLMs), known for their ability to mimic real data and generate text with mixed class labels, are utilized as a synthetic data generation strategy to lower the ECE bound and improve model accuracy on real test data. Additionally, we propose data generation mechanisms for efficient calibration. Testing our method on four different natural language processing tasks, we observed an average up to 34% increase in accuracy and 33% decrease in ECE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.956.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.956.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--956 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.956 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.956/>Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations</a></strong><br><a href=/people/s/sagi-shaier/>Sagi Shaier</a>
|
<a href=/people/a/ari-kobren/>Ari Kobren</a>
|
<a href=/people/p/philip-ogren/>Philip V. Ogren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--956><div class="card-body p-3 small">Resolving knowledge conflicts is a crucial challenge in Question Answering (QA) tasks, as the internet contains numerous conflicting facts and opinions. While some research has made progress in tackling ambiguous settings where multiple valid answers exist, these approaches often neglect to provide source citations, leaving users to evaluate the factuality of each answer. On the other hand, existing work on citation generation has focused on unambiguous settings with single answers, failing to address the complexity of real-world scenarios. Despite the importance of both aspects, no prior research has combined them, leaving a significant gap in the development of QA systems. In this work, we bridge this gap by proposing the novel task of QA with source citation in ambiguous settings, where multiple valid answers exist. To facilitate research in this area, we create a comprehensive framework consisting of: (1) five novel datasets, obtained by augmenting three existing reading comprehension datasets with citation meta-data across various ambiguous settings, such as distractors and paraphrasing; (2) the first ambiguous multi-hop QA dataset featuring real-world, naturally occurring contexts; (3) two new metrics to evaluate models’ performances; and (4) several strong baselines using rule-based, prompting, and finetuning approaches over five large language models. We hope that this new task, datasets, metrics, and baselines will inspire the community to push the boundaries of QA research and develop more trustworthy and interpretable systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.957.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.957.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--957 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.957 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.957/>Granular Privacy Control for Geolocation with Vision Language Models</a></strong><br><a href=/people/e/ethan-mendes/>Ethan Mendes</a>
|
<a href=/people/y/yang-chen/>Yang Chen</a>
|
<a href=/people/j/james-hays/>James Hays</a>
|
<a href=/people/s/sauvik-das/>Sauvik Das</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--957><div class="card-body p-3 small">Vision Language Models (VLMs) are rapidly advancing in their capability to answer information-seeking questions. As these models are widely deployed in consumer applications, they could lead to new privacy risks due to emergent abilities to identify people in photos, geolocate images, etc. As we demonstrate, somewhat surprisingly, current open-source and proprietary VLMs are very capable image geolocators, making widespread geolocation with VLMs an immediate privacy risk, rather than merely a theoretical future concern. As a first step to address this challenge, we develop a new benchmark, GPTGeoChat, to test the capability of VLMs to moderate geolocation dialogues with users. We collect a set of 1,000 image geolocation conversations between in-house annotators and GPT-4v, which are annotated with the granularity of location information revealed at each turn. Using this new dataset we evaluate the ability of various VLMs to moderate GPT-4v geolocation conversations by determining when too much location information has been revealed. We find that custom fine-tuned models perform on par with prompted API-based models when identifying leaked location information at the country or city level, however fine-tuning on supervised data appears to be needed to accurately moderate finer granularities, such as the name of a restaurant or building.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.958.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.958.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--958 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.958 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.958/><span class=acl-fixed-case>M</span>ed<span class=acl-fixed-case>R</span>ead<span class=acl-fixed-case>M</span>e: A Systematic Study for Fine-grained Sentence Readability in Medical Domain</a></strong><br><a href=/people/c/chao-jiang/>Chao Jiang</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--958><div class="card-body p-3 small">Medical texts are notoriously challenging to read. Properly measuring their readability is the first step towards making them more accessible. Here, we present the first systematic study on fine-grained readability measurements in the medical domain, at both sentence-level and span-level. We first introduce a new dataset MedReadMe, which consists of manually annotated readability ratings and fine-grained complex span annotation for 4,520 sentences, featuring two novel “Google-Easy” and “Google-Hard” categories. It supports our quantitative analysis, which covers 650 linguistic features and additional complex span features, to answer “why medical sentences are so hard.” Enabled by our high-quality annotation, we benchmark several state-of-the-art sentence-level readability metrics, including unsupervised, supervised, and prompting-based methods using recently developed large language models (LLMs). Informed by our fine-grained complex span annotation, we find that adding a single feature, capturing the number of jargon spans, into existing readability formulas can significantly improve their correlation with human judgments, and also make them more stable. We will publicly release data and code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.959.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.959.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--959 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.959 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.959.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.959.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.959/><span class=acl-fixed-case>M</span>eme<span class=acl-fixed-case>CLIP</span>: Leveraging <span class=acl-fixed-case>CLIP</span> Representations for Multimodal Meme Classification</a></strong><br><a href=/people/s/siddhant-bikram-shah/>Siddhant Bikram Shah</a>
|
<a href=/people/s/shuvam-shiwakoti/>Shuvam Shiwakoti</a>
|
<a href=/people/m/maheep-chaudhary/>Maheep Chaudhary</a>
|
<a href=/people/h/haohan-wang/>Haohan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--959><div class="card-body p-3 small">The complexity of text-embedded images presents a formidable challenge in machine learning given the need for multimodal understanding of multiple aspects of expression conveyed by them. While previous research in multimodal analysis has primarily focused on singular aspects such as hate speech and its subclasses, this study expands this focus to encompass multiple aspects of linguistics: hate, targets of hate, stance, and humor. We introduce a novel dataset PrideMM comprising 5,063 text-embedded images associated with the LGBTQ+ Pride movement, thereby addressing a serious gap in existing resources. We conduct extensive experimentation on PrideMM by using unimodal and multimodal baseline methods to establish benchmarks for each task. Additionally, we propose a novel framework MemeCLIP for efficient downstream learning while preserving the knowledge of the pre-trained CLIP model. The results of our experiments show that MemeCLIP achieves superior performance compared to previously proposed frameworks on two real-world datasets. We further compare the performance of MemeCLIP and zero-shot GPT-4 on the hate classification task. Finally, we discuss the shortcomings of our model by qualitatively analyzing misclassified samples. Our code and dataset are publicly available at: https://github.com/SiddhantBikram/MemeCLIP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.960.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.960.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.960/><span class=acl-fixed-case>F</span>lip<span class=acl-fixed-case>G</span>uard: Defending Preference Alignment against Update Regression with Constrained Optimization</a></strong><br><a href=/people/m/mingye-zhu/>Mingye Zhu</a>
|
<a href=/people/y/yi-liu/>Yi Liu</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/j/junbo-guo/>Junbo Guo</a>
|
<a href=/people/z/zhendong-mao/>Zhendong Mao</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.961.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.961.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--961 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.961 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.961/><span class=acl-fixed-case>S</span>tory<span class=acl-fixed-case>S</span>park<span class=acl-fixed-case>QA</span>: Expert-Annotated <span class=acl-fixed-case>QA</span> Pairs with Real-World Knowledge for Children’s Story-Based Learning</a></strong><br><a href=/people/j/jiaju-chen/>Jiaju Chen</a>
|
<a href=/people/y/yuxuan-lu/>Yuxuan Lu</a>
|
<a href=/people/s/shao-zhang/>Shao Zhang</a>
|
<a href=/people/b/bingsheng-yao/>Bingsheng Yao</a>
|
<a href=/people/y/yuanzhe-dong/>Yuanzhe Dong</a>
|
<a href=/people/y/ying-xu/>Ying Xu</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/q/qianwen-wang/>Qianwen Wang</a>
|
<a href=/people/d/dakuo-wang/>Dakuo Wang</a>
|
<a href=/people/y/yuling-sun/>Yuling Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--961><div class="card-body p-3 small">Interactive story reading is common in early childhood education, where teachers expect to teach both language skills and real-world knowledge beyond the story. While many story reading systems have been developed for this activity, they often fail to infuse real-world knowledge into the conversation. This limitation can be attributed to the existing question-answering (QA) datasets used for children’s education, upon which the systems are built, failing to capture the nuances of how education experts think when conducting interactive story reading activities. To bridge this gap, we design an annotation framework, empowered by existing knowledge graph to capture experts’ annotations and thinking process, and leverage this framework to construct StorySparkQA dataset, which comprises 5, 868 expert-annotated QA pairs with real-world knowledge. We conduct automated and human expert evaluations across various QA pair generation settings to demonstrate that our StorySparkQA can effectively support models in generating QA pairs that target real-world knowledge beyond story content. StorySparkQA is available at https://huggingface.co/datasets/NEU-HAI/StorySparkQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.962.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.962.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--962 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.962 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.962.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.962.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.962/><span class=acl-fixed-case>M</span>ed<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>T</span>: Medical Chain of Thought via Hierarchical Expert</a></strong><br><a href=/people/j/jiaxiang-liu/>Jiaxiang Liu</a>
|
<a href=/people/y/yuan-wang/>Yuan Wang</a>
|
<a href=/people/j/jiawei-du/>Jiawei Du</a>
|
<a href=/people/j/joey-tianyi-zhou/>Joey Tianyi Zhou</a>
|
<a href=/people/z/zuozhu-liu/>Zuozhu Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--962><div class="card-body p-3 small">Artificial intelligence has advanced in Medical Visual Question Answering (Med-VQA), but prevalent research tends to focus on the accuracy of the answers, often overlooking the reasoning paths and interpretability, which are crucial in clinical settings. Besides, current Med-VQA algorithms, typically reliant on singular models, lack the robustness needed for real-world medical diagnostics which usually require collaborative expert evaluation. To address these shortcomings, this paper presents MedCoT, a novel hierarchical expert verification reasoning chain method designed to enhance interpretability and accuracy in biomedical imaging inquiries. MedCoT is predicated on two principles: The necessity for explicit reasoning paths in Med-VQA and the requirement for multi-expert review to formulate accurate conclusions. The methodology involves an Initial Specialist proposing diagnostic rationales, followed by a Follow-up Specialist who validates these rationales, and finally, a consensus is reached through a vote among a sparse Mixture of Experts within the locally deployed Diagnostic Specialist, which then provides the definitive diagnosis. Experimental evaluations on four standard Med-VQA datasets demonstrate that MedCoT surpasses existing state-of-the-art approaches, providing significant improvements in performance and interpretability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.963.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.963.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--963 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.963 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.963/>Varying Sentence Representations via Condition-Specified Routers</a></strong><br><a href=/people/z/ziyong-lin/>Ziyong Lin</a>
|
<a href=/people/q/quansen-wang/>Quansen Wang</a>
|
<a href=/people/z/zixia-jia/>Zixia Jia</a>
|
<a href=/people/z/zilong-zheng/>Zilong Zheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--963><div class="card-body p-3 small">Semantic similarity between two sentences is inherently subjective and can vary significantly based on the specific aspects emphasized. Consequently, traditional sentence encoders must be capable of generating conditioned sentence representations that account for diverse conditions or aspects. In this paper, we propose a novel yet efficient framework based on transformer-style language models that facilitates advanced conditioned sentence representation while maintaining model parameters and computational efficiency. Empirical evaluations on the Conditional Semantic Textual Similarity and Knowledge Graph Completion tasks demonstrate the superiority of our proposed framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.964.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.964.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--964 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.964 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.964.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.964/>Inductive-Deductive Strategy Reuse for Multi-Turn Instructional Dialogues</a></strong><br><a href=/people/j/jiao-ou/>Jiao Ou</a>
|
<a href=/people/j/jiayu-wu/>Jiayu Wu</a>
|
<a href=/people/c/che-liu/>Che Liu</a>
|
<a href=/people/f/fuzheng-zhang/>Fuzheng Zhang</a>
|
<a href=/people/d/di-zhang/>Di Zhang</a>
|
<a href=/people/k/kun-gai/>Kun Gai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--964><div class="card-body p-3 small">Aligning large language models (LLMs) with human expectations requires high-quality instructional dialogues, which can be achieved by raising diverse, in-depth, and insightful instructions that deepen interactions. Existing methods target instructions from real instruction dialogues as a learning goal and fine-tune a user simulator for posing instructions. However, the user simulator struggles to implicitly model complex dialogue flows and pose high-quality instructions. In this paper, we take inspiration from the cognitive abilities inherent in human learning and propose the explicit modeling of complex dialogue flows through instructional strategy reuse. Specifically, we first induce high-level strategies from various real instruction dialogues. These strategies are applied to new dialogue scenarios deductively, where the instructional strategies facilitate high-quality instructions. Experimental results show that our method can generate diverse, in-depth, and insightful instructions for a given dialogue history. The constructed multi-turn instructional dialogues can outperform competitive baselines on the downstream chat model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.965.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.965.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--965 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.965 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.965/>Information Flow Routes: Automatically Interpreting Language Models at Scale</a></strong><br><a href=/people/j/javier-ferrando/>Javier Ferrando</a>
|
<a href=/people/e/elena-voita/>Elena Voita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--965><div class="card-body p-3 small">Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to computations. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Unlike with patching, we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can analyze model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that some attention head roles are overall important, e.g. previous token heads and subword merging heads. Next, we find similarities in Llama 2 behavior when handling tokens of the same part of speech. Finally, we show that some model components can be specialized on domains such as coding or multilingual texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.966.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.966.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--966 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.966 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.966/>A Simple yet Effective Training-free Prompt-free Approach to <span class=acl-fixed-case>C</span>hinese Spelling Correction Based on Large Language Models</a></strong><br><a href=/people/h/houquan-zhou/>Houquan Zhou</a>
|
<a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/b/bo-zhang/>Bo Zhang</a>
|
<a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/s/shaopeng-lai/>Shaopeng Lai</a>
|
<a href=/people/j/ji-zhang/>Ji Zhang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--966><div class="card-body p-3 small">This work proposes a simple training-free prompt-free approach to leverage large language models (LLMs) for the Chinese spelling correction (CSC) task, which is totally different from all previous CSC approaches. The key idea is to use an LLM as a pure language model in a conventional manner. The LLM goes through the input sentence from the beginning, and at each inference step, produces a distribution over its vocabulary for deciding the next token, given a partial sentence. To ensure that the output sentence remains faithful to the input sentence, we design a minimal distortion model that utilizes pronunciation or shape similarities between the original and replaced characters. Furthermore, we propose two useful reward strategies to address practical challenges specific to the CSC task. Experiments on five public datasets demonstrate that our approach significantly improves LLM performance, enabling them to compete with state-of-the-art domain-general CSC models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.967.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.967.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--967 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.967 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.967.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.967/>Representational Analysis of Binding in Language Models</a></strong><br><a href=/people/q/qin-dai/>Qin Dai</a>
|
<a href=/people/b/benjamin-heinzerling/>Benjamin Heinzerling</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--967><div class="card-body p-3 small">Entity tracking is essential for complex reasoning. To perform in-context entity tracking, language models (LMs) must bind an entity to its attribute (e.g., bind a container to its content) to recall attribute for a given entity. For example, given a context mentioning “The coffee is in Box Z, the stone is in Box M, the map is in Box H”, to infer “Box Z contains the coffee” later, LMs must bind “Box Z” to “coffee”. To explain the binding behaviour of LMs, existing research introduces a Binding ID mechanism and states that LMs use a abstract concept called Binding ID (BI) to internally mark entity-attribute pairs. However, they have not directly captured the BI information from entity activations. In this work, we provide a novel view of the Binding ID mechanism by localizing the BI information. Specifically, we discover that there exists a low-rank subspace in the hidden state (or activation) of LMs, that primarily encodes BIs. To identify this subspace, we take principle component analysis as our first attempt and it is empirically proven to be effective. Moreover, we also discover that when editing representations along directions in the subspace, LMs tend to bind a given entity to other attributes accordingly. For example, by patching activations along the BI encoding direction we can make the LM to infer “Box Z contains the stone” and “Box Z contains the map”.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.968.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.968.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--968 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.968 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.968/><span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>S</span>afe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference</a></strong><br><a href=/people/e/erxin-yu/>Erxin Yu</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/m/ming-liao/>Ming Liao</a>
|
<a href=/people/s/siqi-wang/>Siqi Wang</a>
|
<a href=/people/g/gao-zuchen/>Gao Zuchen</a>
|
<a href=/people/f/fei-mi/>Fei Mi</a>
|
<a href=/people/l/lanqing-hong/>Lanqing Hong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--968><div class="card-body p-3 small">As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research issue. Previous red teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our knowledge, we are the first to study LLM safety in multi-turn dialogue coreference. We created a dataset of 1,400 questions across 14 categories, each featuring multi-turn coreference safety attacks. We then conducted detailed evaluations on five widely used open-source LLMs. The results indicated that under multi-turn coreference safety attacks, the highest attack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was 13.9% with the Mistral-7B-Instruct model. These findings highlight the safety vulnerabilities in LLMs during dialogue coreference interactions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.969.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.969.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--969 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.969 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.969/><span class=acl-fixed-case>C</span>lim<span class=acl-fixed-case>R</span>etrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures</a></strong><br><a href=/people/t/tobias-schimanski/>Tobias Schimanski</a>
|
<a href=/people/j/jingwei-ni/>Jingwei Ni</a>
|
<a href=/people/r/roberto-spacey-martin/>Roberto Spacey Martín</a>
|
<a href=/people/n/nicola-ranger/>Nicola Ranger</a>
|
<a href=/people/m/markus-leippold/>Markus Leippold</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--969><div class="card-body p-3 small">To handle the vast amounts of qualitative data produced in corporate climate communication, stakeholders increasingly rely on Retrieval Augmented Generation (RAG) systems. However, a significant gap remains in evaluating domain-specific information retrieval – the basis for answer generation. To address this challenge, this work simulates the typical tasks of a sustainability analyst by examining 30 sustainability reports with 16 detailed climate-related questions. As a result, we obtain a dataset with over 8.5K unique question-source-answer pairs labeled by different levels of relevance. Furthermore, we develop a use case with the dataset to investigate the integration of expert knowledge into information retrieval with embeddings. Although we show that incorporating expert knowledge works, we also outline the critical limitations of embeddings in knowledge-intensive downstream domains like climate change communication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.970.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.970.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--970 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.970 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.970.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.970.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.970/>Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs</a></strong><br><a href=/people/l/liu-ran/>Liu Ran</a>
|
<a href=/people/z/zhongzhou-liu/>Zhongzhou Liu</a>
|
<a href=/people/x/xiaoli-li/>Xiaoli Li</a>
|
<a href=/people/y/yuan-fang/>Yuan Fang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--970><div class="card-body p-3 small">Knowledge graphs (KGs) are instrumental in various real-world applications, yet they often suffer from incompleteness due to missing relations. To predict instances for novel relations with limited training examples, few-shot relation learning approaches have emerged, utilizing techniques such as meta-learning. However, the assumption is that novel relations in meta-testing and base relations in meta-training are independently and identically distributed, which may not hold in practice. To address the limitation, we propose RelAdapter, a context-aware adapter for few-shot relation learning in KGs designed to enhance the adaptation process in meta-learning. First, RelAdapter is equipped with a lightweight adapter module that facilitates relation-specific, tunable adaptation of meta-knowledge in a parameter-efficient manner. Second, RelAdapter is enriched with contextual information about the target relation, enabling enhanced adaptation to each distinct relation. Extensive experiments on three benchmark KGs validate the superiority of RelAdapter over state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.971.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.971.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--971 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.971 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.971.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.971/>Zero-Shot Detection of <span class=acl-fixed-case>LLM</span>-Generated Text using Token Cohesiveness</a></strong><br><a href=/people/s/shixuan-ma/>Shixuan Ma</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--971><div class="card-body p-3 small">The increasing capability and widespread usage of large language models (LLMs) highlight the desirability of automatic detection of LLM-generated text. Zero-shot detectors, due to their training-free nature, have received considerable attention and notable success. In this paper, we identify a new feature, token cohesiveness, that is useful for zero-shot detection, and we demonstrate that LLM-generated text tends to exhibit higher token cohesiveness than human-written text. Based on this observation, we devise TOCSIN, a generic dual-channel detection paradigm that uses token cohesiveness as a plug-and-play module to improve existing zero-shot detectors. To calculate token cohesiveness, TOCSIN only requires a few rounds of random token deletion and semantic difference measurement, making it particularly suitable for a practical black-box setting where the source model used for generation is not accessible. Extensive experiments with four state-of-the-art base detectors on various datasets, source models, and evaluation settings demonstrate the effectiveness and generality of the proposed approach. Code available at: https://github.com/Shixuan-Ma/TOCSIN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.972.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.972.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--972 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.972 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.972/>Dual-oriented Disentangled Network with Counterfactual Intervention for Multimodal Intent Detection</a></strong><br><a href=/people/z/zhanpeng-chen/>Zhanpeng Chen</a>
|
<a href=/people/z/zhihong-zhu/>Zhihong Zhu</a>
|
<a href=/people/x/xianwei-zhuang/>Xianwei Zhuang</a>
|
<a href=/people/z/zhiqi-huang/>Zhiqi Huang</a>
|
<a href=/people/y/yuexian-zou/>Yuexian Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--972><div class="card-body p-3 small">Multimodal intent detection is designed to leverage diverse modalities for a comprehensive understanding of user intentions in real-world scenarios, thus playing a critical role in modern task-oriented dialogue systems. Existing methods have made great progress in modal alignment and fusion, however, two vital limitations are neglected: (I) close entanglement of multimodal semantics with modal structures; (II) insufficient learning of the causal effects of semantic and modality-specific information on the final predictions under the end-to-end training fashion. To alleviate the above limitations, we introduce the Dual-oriented Disentangled Network with Counterfactual Intervention (DuoDN). DuoDN addresses key limitations in current systems by effectively disentangling and utilizing modality-specific and multimodal semantic information. The model consists of a Dual-oriented Disentangled Encoder that decouples semantics-oriented and modality-oriented representations, alongside a Counterfactual Intervention Module that applies causal inference to understand causal effects by injecting confounders. Experiments on three benchmark datasets demonstrate DuoDN’s superiority over existing methods, with extensive analysis validating its advantages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.973.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.973.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--973 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.973 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.973/>From <span class=acl-fixed-case>LLM</span>s to <span class=acl-fixed-case>MLLM</span>s: Exploring the Landscape of Multimodal Jailbreaking</a></strong><br><a href=/people/s/siyuan-wang/>Siyuan Wang</a>
|
<a href=/people/z/zhuohan-long/>Zhuohan Long</a>
|
<a href=/people/z/zhihao-fan/>Zhihao Fan</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--973><div class="card-body p-3 small">The rapid development of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has exposed vulnerabilities to various adversarial attacks. This paper provides a comprehensive overview of jailbreaking research targeting both LLMs and MLLMs, highlighting recent advancements in evaluation benchmarks, attack techniques and defense strategies. Compared to the more advanced state of unimodal jailbreaking, multimodal domain remains underexplored. We summarize the limitations and potential research directions of multimodal jailbreaking, aiming to inspire future research and further enhance the robustness and security of MLLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.974.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.974.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--974 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.974 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.974/>Symbolic Working Memory Enhances Language Models for Complex Rule Application</a></strong><br><a href=/people/s/siyuan-wang/>Siyuan Wang</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--974><div class="card-body p-3 small">Large Language Models (LLMs) have shown remarkable reasoning performance but struggle with multi-step deductive reasoning involving a series of rule application steps, especially when rules are presented non-sequentially. Our preliminary analysis shows that while LLMs excel in single-step rule application, their performance drops significantly in multi-step scenarios due to the challenge in rule grounding. It requires anchoring the applicable rule and supporting facts at each step, amidst multiple input rules, facts, and inferred facts. To address this, we propose augmenting LLMs with external working memory and introduce a neurosymbolic framework for rule application. The memory stores facts and rules in both natural language and symbolic forms, enabling precise tracking. Utilizing this memory, our framework iteratively performs symbolic rule grounding and LLM-based rule implementation. The former matches predicates and variables of symbolic rules and facts to ground applicable rules at each step. Experiments indicate our framework’s effectiveness in rule application and its robustness across various steps and settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.975.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.975.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--975 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.975 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.975/><span class=acl-fixed-case>LL</span>o<span class=acl-fixed-case>CO</span>: Learning Long Contexts Offline</a></strong><br><a href=/people/s/sijun-tan/>Sijun Tan</a>
|
<a href=/people/x/xiuyu-li/>Xiuyu Li</a>
|
<a href=/people/s/shishir-g-patil/>Shishir G Patil</a>
|
<a href=/people/z/ziyang-wu/>Ziyang Wu</a>
|
<a href=/people/t/tianjun-zhang/>Tianjun Zhang</a>
|
<a href=/people/k/kurt-keutzer/>Kurt Keutzer</a>
|
<a href=/people/j/joseph-e-gonzalez/>Joseph E. Gonzalez</a>
|
<a href=/people/r/raluca-ada-popa/>Raluca Ada Popa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--975><div class="card-body p-3 small">Processing long contexts remains a challenge for large language models (LLMs) due to the quadratic computational and memory overhead of the self-attention mechanism and the substantial KV cache sizes during generation. We propose LLoCO, a novel approach to address this problem by learning contexts offline through context compression and in-domain parameter-efficient finetuning with LoRA. Our method enables an LLM to create a concise representation of the original context and efficiently retrieve relevant information to answer questions accurately. Our approach extends the effective context window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on several long-context question-answering datasets, demonstrating that LLoCO significantly outperforms in-context learning while using <span class=tex-math>30 ×</span> fewer tokens during inference. LLoCO achieves up to <span class=tex-math>7.62 ×</span> speed-up during inference and <span class=tex-math>11.52 ×</span> higher throughput during finetuning, substantially reduces the cost of long document question answering. This makes it a promising solution for efficient long context processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.976.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.976.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--976 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.976 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.976.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.976/>Don’t Forget Your Reward Values: Language Model Alignment via Value-based Calibration</a></strong><br><a href=/people/x/xinnian-mao/>Xin Mao</a>
|
<a href=/people/f/feng-lin-li/>Feng-Lin Li</a>
|
<a href=/people/h/huimin-xu/>Huimin Xu</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/w/wang-chen/>Wang Chen</a>
|
<a href=/people/l/luu-anh-tuan/>Anh Tuan Luu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--976><div class="card-body p-3 small">While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based alignment methods as viable alternatives. This paper delves into existing order-based methods, unifying them into one framework and examining their inefficiencies in utilizing reward values. Building upon these findings, we propose a new Value-based Calibration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and diversity in different settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.977.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.977.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--977 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.977 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.977/>Mentor-<span class=acl-fixed-case>KD</span>: Making Small Language Models Better Multi-step Reasoners</a></strong><br><a href=/people/h/hojae-lee/>Hojae Lee</a>
|
<a href=/people/j/junho-kim/>Junho Kim</a>
|
<a href=/people/s/sangkeun-lee/>SangKeun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--977><div class="card-body p-3 small">Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine-tuning language models of multi-step rationales generated by LLM teachers. However, they have inadequately considered two challenges regarding insufficient distillation sets from the LLM teacher model, in terms of 1) data quality and 2) soft label provision. In this paper, we propose Mentor-KD, which effectively distills the multi-step reasoning capability of LLMs to smaller LMs while addressing the aforementioned challenges. Specifically, we exploit a mentor, intermediate-sized task-specific fine-tuned model, to augment additional CoT annotations and provide soft labels for the student model during reasoning distillation. We conduct extensive experiments and confirm Mentor-KD’s effectiveness across various models and complex reasoning tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.978.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.978.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--978 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.978 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.978/>Are Large Language Models Capable of Generating Human-Level Narratives?</a></strong><br><a href=/people/y/yufei-tian/>Yufei Tian</a>
|
<a href=/people/t/tenghao-huang/>Tenghao Huang</a>
|
<a href=/people/m/miri-liu/>Miri Liu</a>
|
<a href=/people/d/derek-jiang/>Derek Jiang</a>
|
<a href=/people/a/alexander-spangher/>Alexander Spangher</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--978><div class="card-body p-3 small">As daily reliance on large language models (LLMs) grows, assessing their generation quality is crucial to understanding how they might impact on our communications. This paper investigates the capability of LLMs in storytelling, focusing on narrative development and plot progression. We introduce a novel computational framework to analyze narratives through three discourse-level aspects: i) story arcs, ii) turning points, and iii) affective dimensions, including arousal and valence. By leveraging expert and automatic annotations, we uncover significant discrepancies between the LLM- and human- written stories. While human-written stories are suspenseful, arousing, and diverse in narrative structures, LLM stories are homogeneously positive and lack tension. Next, we measure narrative reasoning skills as a precursor to generative capacities, concluding that most LLMs fall short of human abilities in discourse understanding. Finally, we show that explicit integration of aforementioned discourse features can enhance storytelling, as is demonstrated by over 40% improvement in neural storytelling in terms of diversity, suspense, and arousal. Such advances promise to facilitate greater and more natural roles LLMs in human communication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.979.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.979.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--979 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.979 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.979.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.979.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.979/><span class=acl-fixed-case>MP</span>2<span class=acl-fixed-case>D</span>: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs</a></strong><br><a href=/people/y/yerin-hwang/>Yerin Hwang</a>
|
<a href=/people/y/yongil-kim/>Yongil Kim</a>
|
<a href=/people/y/yunah-jang/>Yunah Jang</a>
|
<a href=/people/j/jeesoo-bang/>Jeesoo Bang</a>
|
<a href=/people/h/hyunkyung-bae/>Hyunkyung Bae</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--979><div class="card-body p-3 small">Despite advancements in on-topic dialogue systems, effectively managing topic shifts within dialogues remains a persistent challenge, largely attributed to the limited availability of training datasets. To address this issue, we propose Multi-Passage to Dialogue (MP2D), a data generation framework that automatically creates conversational question-answering datasets with natural topic transitions. By leveraging the relationships between entities in a knowledge graph, MP2D maps the flow of topics within a dialogue, effectively mirroring the dynamics of human conversation. It retrieves relevant passages corresponding to the topics and transforms them into dialogues through the passage-to-dialogue method. Through quantitative and qualitative experiments, we demonstrate MP2D’s efficacy in generating dialogue with natural topic shifts. Furthermore, this study introduces a novel benchmark for topic shift dialogues, TS-WikiDialog. Utilizing the dataset, we demonstrate that even Large Language Models (LLMs) struggle to handle topic shifts in dialogue effectively, and we showcase the performance improvements of models trained on datasets generated by MP2D across diverse topic shift dialogue tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.980.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.980.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--980 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.980 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.980/>Can Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction</a></strong><br><a href=/people/h/haohui-lu/>Haohui Lu</a>
|
<a href=/people/u/usman-naseem/>Usman Naseem</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--980><div class="card-body p-3 small">Large Language Models (LLMs) have made significant strides in various tasks, yet their effectiveness in predicting disease progression remains relatively unexplored. To fill this gap, we use LLMs and employ advanced graph prompting and Retrieval-Augmented Generation (RAG) to predict disease comorbidity within disease networks. Specifically, we introduce a disease Comorbidity prediction model using LLM, named ComLLM, which leverages domain knowledge to enhance the prediction performance. Based on the comprehensive experimental results, ComLLM consistently outperforms conventional models, such as Graph Neural Networks, achieving average area under the curve (AUC) improvements of 10.70% and 6.07% over the best baseline models in two distinct disease networks. ComLLM is evaluated across multiple settings for disease progression prediction, employing various prompting strategies, including zero-shot, few-shot, Chain-of-Thought, graph prompting and RAG. Our results show that graph prompting and RAG enhance LLM performance in disease progression prediction tasks. ComLLM exhibits superior predictive capabilities and serves as a proof-of-concept for LLM-based systems in disease progression prediction, highlighting its potential for broad applications in healthcare.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.981.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.981.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--981 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.981 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.981/>Searching for Best Practices in Retrieval-Augmented Generation</a></strong><br><a href=/people/x/xiaohua-wang/>Xiaohua Wang</a>
|
<a href=/people/z/zhenghua-wang/>Zhenghua Wang</a>
|
<a href=/people/x/xuan-gao/>Xuan Gao</a>
|
<a href=/people/f/feiran-zhang/>Feiran Zhang</a>
|
<a href=/people/y/yixin-wu/>Yixin Wu</a>
|
<a href=/people/z/zhibo-xu/>Zhibo Xu</a>
|
<a href=/people/t/tianyuan-shi/>Tianyuan Shi</a>
|
<a href=/people/z/zhengyuan-wang/>Zhengyuan Wang</a>
|
<a href=/people/s/shizheng-li/>Shizheng Li</a>
|
<a href=/people/q/qi-qian/>Qi Qian</a>
|
<a href=/people/r/ruicheng-yin/>Ruicheng Yin</a>
|
<a href=/people/c/changze-lv/>Changze Lv</a>
|
<a href=/people/x/xiaoqing-zheng/>Xiaoqing Zheng</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--981><div class="card-body p-3 small">Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a “retrieval as generation” strategy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.982.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.982.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--982 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.982 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.982/>Moral Foundations of Large Language Models</a></strong><br><a href=/people/m/marwa-abdulhai/>Marwa Abdulhai</a>
|
<a href=/people/g/gregory-serapio-garcia/>Gregory Serapio-García</a>
|
<a href=/people/c/clement-crepy/>Clement Crepy</a>
|
<a href=/people/d/daria-valter/>Daria Valter</a>
|
<a href=/people/j/john-canny/>John Canny</a>
|
<a href=/people/n/natasha-jaques/>Natasha Jaques</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--982><div class="card-body p-3 small">Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary in the weight they place on these dimensions when making moral decisions, in part due to their cultural upbringing and political ideology. As large language models (LLMs) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values. We analyze known LLMs and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. Finally, we show that we can adversarially select prompts that encourage the moral to exhibit a particular set of moral foundations, and that this can affect the model’s behavior on downstream tasks. These findings help illustrate the potential risks and unintended consequences of LLMs assuming a particular moral stance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.983.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.983.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--983 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.983 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.983/>The Zeno’s Paradox of ‘Low-Resource’ Languages</a></strong><br><a href=/people/h/hellina-hailu-nigatu/>Hellina Hailu Nigatu</a>
|
<a href=/people/a/atnafu-lambebo-tonja/>Atnafu Lambebo Tonja</a>
|
<a href=/people/b/benjamin-rosman/>Benjamin Rosman</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--983><div class="card-body p-3 small">The disparity in the languages commonly studied in Natural Language Processing (NLP) is typically reflected by referring to languages as low vs high-resourced. However, there is limited consensus on what exactly qualifies as a ‘low-resource language.’ To understand how NLP papers define and study ‘low resource’ languages, we qualitatively analyzed 150 papers from the ACL Anthology and popular speech-processing conferences that mention the keyword ‘low-resource.’ Based on our analysis, we show how several interacting axes contribute to ‘low-resourcedness’ of a language and why that makes it difficult to track progress for each individual language. We hope our work (1) elicits explicit definitions of the terminology when it is used in papers and (2) provides grounding for the different axes to consider when connoting a language as low-resource.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.984.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.984.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--984 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.984 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.984.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.984/>Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization</a></strong><br><a href=/people/a/aseem-srivastava/>Aseem Srivastava</a>
|
<a href=/people/s/smriti-joshi/>Smriti Joshi</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a>
|
<a href=/people/m/md-shad-akhtar/>Md Shad Akhtar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--984><div class="card-body p-3 small">In mental health counseling, condensing dialogues into concise and relevant summaries (aka counseling notes) holds pivotal significance. Large Language Models (LLMs) exhibit remarkable capabilities in various generative tasks; however, their adaptation to domain-specific intricacies remains challenging, especially within mental health contexts. Unlike standard LLMs, mental health experts first plan to apply domain knowledge in writing summaries. Our work enhances LLMs’ ability by introducing a novel planning engine to orchestrate structuring knowledge alignment. To achieve high-order planning, we divide knowledge encapsulation into two major phases: (i) holding dialogue structure and (ii) incorporating domain-specific knowledge. We employ a planning engine on Llama-2, resulting in a novel framework, PIECE. Our proposed system employs knowledge filtering-cum-scaffolding to encapsulate domain knowledge. Additionally, PIECE leverages sheaf convolution learning to enhance its understanding of the dialogue’s structural nuances. We compare PIECE with 14 baseline methods and observe a significant improvement across ROUGE and Bleurt scores. Further, expert evaluation and analyses validate the generation quality to be effective, sometimes even surpassing the gold standard. We further benchmark PIECE with other LLMs and report improvement, including Llama-2 (+2.72%), Mistral (+2.04%), and Zephyr (+1.59%), to justify the generalizability of the planning engine.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.985.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.985.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--985 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.985 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.985/>Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition</a></strong><br><a href=/people/p/pritika-ramu/>Pritika Ramu</a>
|
<a href=/people/k/koustava-goswami/>Koustava Goswami</a>
|
<a href=/people/a/apoorv-saxena/>Apoorv Saxena</a>
|
<a href=/people/b/balaji-vasan-srinivasan/>Balaji Vasan Srinivasan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--985><div class="card-body p-3 small">Accurately attributing answer text to its source document is crucial for developing a reliable question-answering system. However, attribution for long documents remains largely unexplored. Post-hoc attribution systems are designed to map answer text back to the source document, yet the granularity of this mapping has not been addressed. Furthermore, a critical question arises: What exactly should be attributed? This involves identifying the specific information units within an answer that require grounding. In this paper, we propose and investigate a novel approach to the factual decomposition of generated answers for attribution, employing template-based in-context learning. To accomplish this, we utilize the question and integrate negative sampling during few-shot in-context learning for decomposition. This approach enhances the semantic understanding of both abstractive and extractive answers. We examine the impact of answer decomposition by providing a thorough examination of various attribution approaches, ranging from retrieval-based techniques to LLM-based attributors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.986.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.986.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--986 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.986 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.986/>From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment</a></strong><br><a href=/people/y/yusuke-hirota/>Yusuke Hirota</a>
|
<a href=/people/r/ryo-hachiuma/>Ryo Hachiuma</a>
|
<a href=/people/c/chao-han-huck-yang/>Chao-Han Huck Yang</a>
|
<a href=/people/y/yuta-nakashima/>Yuta Nakashima</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--986><div class="card-body p-3 small">Large language models (LLMs) have enhanced the capacity of vision-language models to caption visual text. This generative approach to image caption enrichment further makes textual captions more descriptive, improving alignment with the visual context. However, while many studies focus on the benefits of generative caption enrichment (GCE), are there any negative side effects? We compare standard-format captions and recent GCE processes from the perspectives of gender bias and hallucination, showing that enriched captions suffer from increased gender bias and hallucination. Furthermore, models trained on these enriched captions amplify gender bias by an average of 30.9% and increase hallucination by 59.5%. This study serves as a caution against the trend of making captions more descriptive.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.987.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.987.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--987 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.987 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.987/>Pruning via Merging: Compressing <span class=acl-fixed-case>LLM</span>s via Manifold Alignment Based Layer Merging</a></strong><br><a href=/people/d/deyuan-liu/>Deyuan Liu</a>
|
<a href=/people/z/zhanyue-qin/>Zhanyue Qin</a>
|
<a href=/people/h/hairu-wang/>Hairu Wang</a>
|
<a href=/people/z/zhao-yang/>Zhao Yang</a>
|
<a href=/people/z/zecheng-wang/>Zecheng Wang</a>
|
<a href=/people/f/fangying-rong/>Fangying Rong</a>
|
<a href=/people/q/qingbin-liu/>Qingbin Liu</a>
|
<a href=/people/y/yanchao-hao/>Yanchao Hao</a>
|
<a href=/people/b/bo-li/>Bo Li</a>
|
<a href=/people/x/xi-chen/>Xi Chen</a>
|
<a href=/people/c/cunhang-fan/>Cunhang Fan</a>
|
<a href=/people/z/zhao-lv/>Zhao Lv</a>
|
<a href=/people/d/dianhui-chu/>Dianhui Chu</a>
|
<a href=/people/z/zhiying-tu/>Zhiying Tu</a>
|
<a href=/people/d/dianbo-sui/>Dianbo Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--987><div class="card-body p-3 small">While large language models (LLMs) excel in many domains, their complexity and scale challenge deployment in resource-limited environments. Current compression techniques, such as parameter pruning, often fail to effectively utilize the knowledge from pruned parameters. To address these challenges, we propose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA), a novel approach that uses manifold learning and the Information Bottleneck (IB) measure to merge similar layers, reducing model size while preserving essential performance. We evaluate MKA on multiple benchmark datasets and various LLMs. Our findings show that MKA not only preserves model performance but also achieves substantial compression ratios, outperforming traditional pruning methods. Moreover, when coupled with quantization, MKA delivers even greater compression. Specifically, on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75% with a minimal performance decrease of only 2.82%. The proposed MKA method offers a resource-efficient and performance-preserving model compression technique for LLMs. We make our code available at https://github.com/SempraETY/Pruning-via-Merging</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.988.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.988.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--988 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.988 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.988/>Embedded Named Entity Recognition using Probing Classifiers</a></strong><br><a href=/people/n/nicholas-popovic/>Nicholas Popovic</a>
|
<a href=/people/m/michael-farber/>Michael Färber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--988><div class="card-body p-3 small">Streaming text generation, has become a common way of increasing the responsiveness of language model powered applications such as chat assistants. At the same time, extracting semantic information from generated text is a useful tool for applications such as automated fact checking or retrieval augmented generation. Currently, this requires either separate models during inference, which increases computational cost, or destructive fine-tuning of the language model. Instead, we propose an approach called EMBER which enables streaming named entity recognition in decoder-only language models without fine-tuning them and while incurring minimal additional computational cost at inference time. Specifically, our experiments show that EMBER maintains high token generation rates, with only a negligible decrease in speed of around 1% compared to a 43.64% slowdown measured for a baseline. We make our code and data available online, including a toolkit for training, testing, and deploying efficient token classification models optimized for streaming text generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.989.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.989.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--989 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.989 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.989/>Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training</a></strong><br><a href=/people/z/zhou-zhang/>Zhou Zhang</a>
|
<a href=/people/d/dongzeng-tan/>Dongzeng Tan</a>
|
<a href=/people/j/jiaan-wang/>Jiaan Wang</a>
|
<a href=/people/y/yilong-chen/>Yilong Chen</a>
|
<a href=/people/j/jiarong-xu/>Jiarong Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--989><div class="card-body p-3 small">Emojis have gained immense popularity on social platforms, serving as a common means to supplement or replace text. However, existing data mining approaches generally either completely ignore or simply treat emojis as ordinary Unicode characters, which may limit the model’s ability to grasp the rich semantic information in emojis and the interaction between emojis and texts. Thus, it is necessary to release the emoji’s power in social media data mining. To this end, we first construct a heterogeneous graph consisting of three types of nodes, i.e. post, word and emoji nodes to improve the representation of different elements in posts. The edges are also well-defined to model how these three elements interact with each other. To facilitate the sharing of information among post, word and emoji nodes, we propose a graph pre-train framework for text and emoji co-modeling, which contains two graph pre-training tasks: node-level graph contrastive learning and edge-level link reconstruction learning. Extensive experiments on the Xiaohongshu and Twitter datasets with two types of downstream tasks demonstrate that our approach proves significant improvement over previous strong baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.990.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.990.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--990 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.990 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.990.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.990.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.990/>Data Contamination Can Cross Language Barriers</a></strong><br><a href=/people/f/feng-yao/>Feng Yao</a>
|
<a href=/people/y/yufan-zhuang/>Yufan Zhuang</a>
|
<a href=/people/z/zihao-sun/>Zihao Sun</a>
|
<a href=/people/s/sunan-xu/>Sunan Xu</a>
|
<a href=/people/a/animesh-kumar/>Animesh Kumar</a>
|
<a href=/people/j/jingbo-shang/>Jingbo Shang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--990><div class="card-body p-3 small">The opacity in developing large language models (LLMs) is raising growing concerns about the potential contamination of public benchmarks in the pre-training data. Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination. In this paper, we first present a cross-lingual form of contamination that inflates LLMs’ performance while evading current detection methods, deliberately injected by overfitting LLMs on the translated versions of benchmark test sets. Then, we propose generalization-based approaches to unmask such deeply concealed contamination. Specifically, we examine the LLM’s performance change after modifying the original benchmark by replacing the false answer choices with correct ones from other questions. Contaminated models can hardly generalize to such easier situations, where the false choices can be <i>not even wrong</i>, as all choices are correct in their memorization. Experimental results demonstrate that cross-lingual contamination can easily fool existing detection methods, but not ours. In addition, we discuss the potential utilization of cross-lingual contamination in interpreting LLMs’ working mechanisms and in post-training LLMs for enhanced multilingual capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.991.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.991.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--991 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.991 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.991/>Automated Essay Scoring: A Reflection on the State of the Art</a></strong><br><a href=/people/s/shengjie-li/>Shengjie Li</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--991><div class="card-body p-3 small">While steady progress has been made on the task of automated essay scoring (AES) in the past decade, much of the recent work in this area has focused on developing models that beat existing models on a standard evaluation dataset. While improving performance numbers remains an important goal in the short term, such a focus is not necessarily beneficial for the long-term development of the field. We reflect on the state of the art in AES research, discussing issues that we believe can encourage researchers to think bigger than improving performance numbers with the ultimate goal of triggering discussion among AES researchers on how we should move forward.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.992.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.992.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--992 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.992 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.992/>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</a></strong><br><a href=/people/t/tian-liang/>Tian Liang</a>
|
<a href=/people/z/zhiwei-he/>Zhiwei He</a>
|
<a href=/people/w/wenxiang-jiao/>Wenxiang Jiao</a>
|
<a href=/people/x/xing-wang/>Xing Wang</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/y/yujiu-yang/>Yujiu Yang</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--992><div class="card-body p-3 small">Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of “tit for tat” and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of “tit for tat” state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.993.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.993.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--993 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.993 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.993/>Unveiling and Consulting Core Experts in Retrieval-Augmented <span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>E</span>-based <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/x/xin-zhou/>Xin Zhou</a>
|
<a href=/people/p/ping-nie/>Ping Nie</a>
|
<a href=/people/y/yiwen-guo/>Yiwen Guo</a>
|
<a href=/people/h/haojie-wei/>Haojie Wei</a>
|
<a href=/people/z/zhanqiu-zhang/>Zhanqiu Zhang</a>
|
<a href=/people/p/pasquale-minervini/>Pasquale Minervini</a>
|
<a href=/people/r/ruotian-ma/>Ruotian Ma</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--993><div class="card-body p-3 small">Retrieval-Augmented Generation (RAG) significantly improved the ability of Large Language Models (LLMs) to solve knowledge-intensive tasks. While existing research seeks to enhance RAG performance by retrieving higher-quality documents or designing RAG-specific LLMs, the internal mechanisms within LLMs that contribute to RAG’s effectiveness remain underexplored. In this paper, we aim to investigate these internal mechanisms within the popular Mixture-of-Expert (MoE)-based LLMs and demonstrate how to improve RAG by examining expert activations in these LLMs. Our controlled experiments reveal that several core groups of experts are primarily responsible for RAG-related behaviors. The activation of these core experts can signify the model’s inclination towards external/internal knowledge and adjust its behavior. For instance, we identify core experts that can (1) indicate the sufficiency of the model’s internal knowledge, (2) assess the quality of retrieved documents, and (3) enhance the model’s ability to utilize context. Based on these findings, we propose several strategies to enhance RAG’s efficiency and effectiveness through expert activation. Experimental results across various datasets and MoE LLMs show the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.994.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.994.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--994 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.994 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.994.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.994/><span class=acl-fixed-case>CURE</span>: Context- and Uncertainty-Aware Mental Disorder Detection</a></strong><br><a href=/people/m/migyeong-kang/>Migyeong Kang</a>
|
<a href=/people/g/goun-choi/>Goun Choi</a>
|
<a href=/people/h/hyolim-jeon/>Hyolim Jeon</a>
|
<a href=/people/j/ji-hyun-an/>Ji Hyun An</a>
|
<a href=/people/d/daejin-choi/>Daejin Choi</a>
|
<a href=/people/j/jinyoung-han/>Jinyoung Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--994><div class="card-body p-3 small">As the explainability of mental disorder detection models has become important, symptom-based methods that predict disorders from identified symptoms have been widely utilized. However, since these approaches focused on the presence of symptoms, the context of symptoms can be often ignored, leading to missing important contextual information related to detecting mental disorders. Furthermore, the result of disorder detection can be vulnerable to errors that may occur in identifying symptoms. To address these issues, we propose a novel framework that detects mental disorders by leveraging symptoms and their context while mitigating potential errors in symptom identification. In this way, we propose to use large language models to effectively extract contextual information and introduce an uncertainty-aware decision fusion network that combines predictions of multiple models based on quantified uncertainty values. To evaluate the proposed method, we constructed a new Korean mental health dataset annotated by experts, named KoMOS. Experimental results demonstrate that the proposed model accurately detects mental disorders even in situations where symptom information is incomplete.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.995.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.995.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--995 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.995 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.995/><span class=acl-fixed-case>P</span>ep<span class=acl-fixed-case>R</span>ec: Progressive Enhancement of Prompting for Recommendation</a></strong><br><a href=/people/y/yakun-yu/>Yakun Yu</a>
|
<a href=/people/s/shi-ang-qi/>Shi-ang Qi</a>
|
<a href=/people/b/baochun-li/>Baochun Li</a>
|
<a href=/people/d/di-niu/>Di Niu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--995><div class="card-body p-3 small">With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, recent researchers have actively explored the potential of LLMs for recommendation systems by converting the input data into textual sentences through prompt templates. Although semantic knowledge from LLMs can help enrich the content information of items, to date it is still hard for them to achieve comparable performance to traditional deep learning recommendation models, partly due to a lack of ability to leverage collaborative filtering. In this paper, we propose a novel training-free prompting framework, PepRec, which aims to capture knowledge from both content-based filtering and collaborative filtering to boost recommendation performance with LLMs, while providing interpretation for the recommendation. Experiments based on two real-world datasets from different domains show that PepRec significantly outperforms various traditional deep learning recommendation models and prompt-based recommendation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.996.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.996.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--996 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.996 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.996/>In-Context Compositional Generalization for Large Vision-Language Models</a></strong><br><a href=/people/c/chuanhao-li/>Chuanhao Li</a>
|
<a href=/people/c/chenchen-jing/>Chenchen Jing</a>
|
<a href=/people/z/zhen-li/>Zhen Li</a>
|
<a href=/people/m/mingliang-zhai/>Mingliang Zhai</a>
|
<a href=/people/y/yuwei-wu/>Yuwei Wu</a>
|
<a href=/people/y/yunde-jia/>Yunde Jia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--996><div class="card-body p-3 small">Recent work has revealed that in-context learning for large language models exhibits compositional generalization capacity, which can be enhanced by selecting in-context demonstrations similar to test cases to provide contextual information. However, how to exhibit in-context compositional generalization (ICCG) of large vision-language models (LVLMs) is non-trival. Due to the inherent asymmetry between visual and linguistic modalities, ICCG in LVLMs faces an inevitable challenge—redundant information on the visual modality. The redundant information affects in-context learning from two aspects: (1) Similarity calculation may be dominated by redundant information, resulting in sub-optimal demonstration selection. (2) Redundant information in in-context demonstrations brings misleading contextual information to in-context learning. To alleviate these problems, we propose a demonstration selection method to achieve ICCG for LVLMs, by considering two key factors of demonstrations: content and structure, from a multimodal perspective. Specifically, we design a diversity-coverage-based matching score to select demonstrations with maximum coverage, and avoid selecting demonstrations with redundant information via their content redundancy and structural complexity. We build a GQA-ICCG dataset to simulate the ICCG setting, and conduct experiments on GQA-ICCG and the VQA v2 dataset. Experimental results demonstrate the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.997.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.997.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.997.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.997.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.997/>Improving Zero-shot <span class=acl-fixed-case>LLM</span> Re-Ranker with Risk Minimization</a></strong><br><a href=/people/x/xiaowei-yuan/>Xiaowei Yuan</a>
|
<a href=/people/z/zhao-yang/>Zhao Yang</a>
|
<a href=/people/y/yequan-wang/>Yequan Wang</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.998.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.998.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--998 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.998 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.998/>Game on Tree: Visual Hallucination Mitigation via Coarse-to-Fine View Tree and Game Theory</a></strong><br><a href=/people/x/xianwei-zhuang/>Xianwei Zhuang</a>
|
<a href=/people/z/zhihong-zhu/>Zhihong Zhu</a>
|
<a href=/people/z/zhanpeng-chen/>Zhanpeng Chen</a>
|
<a href=/people/y/yuxin-xie/>Yuxin Xie</a>
|
<a href=/people/l/liming-liang/>Liming Liang</a>
|
<a href=/people/y/yuexian-zou/>Yuexian Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--998><div class="card-body p-3 small">Large Vision-Language Models (LVLMs) may produce outputs that are unfaithful to reality, also known as visual hallucinations (VH), which hinders their application in multimodal understanding and decision-making. In this work, we introduce a novel plug-and-play train-free decoding algorithm named Game and Tree based Hallucination Mitigation (GTHM), designed for mitigating VH. GTHM is inspired by empirical observations that the fuzziness of multi-granularity view perception exacerbates VH. Based on this, GTHM leverages visual information to construct a coarse-to-fine visual view tree (CFTree) that organizes visual objects, attributes, and relationships in a hierarchical manner. Additionally, we innovatively model the optimal visual-token matching process on the CFTree as the cooperative game. Specifically, we define the Tree-based Shapley Value (TSV) for each visual view on the CFTree to assess its significant contribution to the overall visual understanding, thereby determining the optimal visual granularity. Subsequently, we utilize the TSV as guidance to implement adaptive weight contrastive decoding to achieve vision-aware decoding. Extensive experiments on four popular benchmarks confirm the effectiveness of our GTHM in alleviating VH across different LVLM families without additional training or post-processing. Our code is published at https://github.com/mengchuang123/GTHM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.999.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.999.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--999 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.999 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.999/>Label Confidence Weighted Learning for Target-level Sentence Simplification</a></strong><br><a href=/people/x/xin-ying-qiu/>Xin Ying Qiu</a>
|
<a href=/people/j/jingshen-zhang/>Jingshen Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--999><div class="card-body p-3 small">Multi-level sentence simplification generates simplified sentences with varying language proficiency levels. We propose Label Confidence Weighted Learning (LCWL), a novel approach that incorporates a label confidence weighting scheme in the training loss of the encoder-decoder model, setting it apart from existing confidence-weighting methods primarily designed for classification. Experimentation on English grade-level simplification dataset shows that LCWL outperforms state-of-the-art unsupervised baselines. Fine-tuning the LCWL model on in-domain data and combining with Symmetric Cross Entropy (SCE) consistently delivers better simplifications compared to strong supervised methods. Our results highlight the effectiveness of label confidence weighting techniques for text simplification tasks with encoder-decoder architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1000.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1000 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1000 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1000/>Quantum Recurrent Architectures for Text Classification</a></strong><br><a href=/people/w/wenduan-xu/>Wenduan Xu</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/d/douglas-brown/>Douglas Brown</a>
|
<a href=/people/g/gabriel-matos/>Gabriel Matos</a>
|
<a href=/people/k/konstantinos-meichanetzidis/>Konstantinos Meichanetzidis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1000><div class="card-body p-3 small">We develop quantum RNNs with cells based on Parametrised Quantum Circuits (PQCs). PQCs can provide a form of hybrid quantum-classical computation where the input and the output is in the form of classical data. The previous “hidden” state is the quantum state from the previous time-step, and an angle encoding is used to define a (non-linear) mapping from a classical word embedding into the quantum Hilbert space. Measurements of the quantum state provide classical statistics which are used for classification. We report results which are competitive with various RNN baselines on the Rotten Tomatoes dataset, as well as emulator results which demonstrate the feasibility of running such models on quantum hardware.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1001.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1001/>Tree of Problems: Improving structured problem solving with compositionality</a></strong><br><a href=/people/a/armel-randy-zebaze/>Armel Randy Zebaze</a>
|
<a href=/people/b/benoit-sagot/>Benoît Sagot</a>
|
<a href=/people/r/rachel-bawden/>Rachel Bawden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1001><div class="card-body p-3 small">Large Language Models (LLMs) have demonstrated remarkable performance across multipletasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition per forms better than CoT on complex reasoning tasks. All code for this paper will be made available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1002.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1002/>What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study</a></strong><br><a href=/people/b/beatrice-savoldi/>Beatrice Savoldi</a>
|
<a href=/people/s/sara-papi/>Sara Papi</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/ana-guerberof-arenas/>Ana Guerberof-Arenas</a>
|
<a href=/people/l/luisa-bentivogli/>Luisa Bentivogli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1002><div class="card-body p-3 small">Gender bias in machine translation (MT) is recognized as an issue that can harm people and society. And yet, advancements in the field rarely involve people, the final MT users, or inform how they might be impacted by biased technologies. Current evaluations are often restricted to automatic methods, which offer an opaque estimate of what the downstream impact of gender disparities might be. We conduct an extensive human-centered study to examine if and to what extent bias in MT brings harms with tangible costs, such as quality of service gaps across women and men. To this aim, we collect behavioral data from ~90 participants, who post-edited MT outputs to ensure correct gender translation. Across multiple datasets, languages, and types of users, our study shows that feminine post-editing demands significantly more technical and temporal effort, also corresponding to higher financial costs. Existing bias measurements, however, fail to reflect the found disparities. Our findings advocate for human-centered approaches that can inform the societal impact of bias.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1003.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1003.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1003/><span class=acl-fixed-case>S</span>eg2<span class=acl-fixed-case>A</span>ct: Global Context-aware Action Generation for Document Logical Structuring</a></strong><br><a href=/people/z/zichao-li/>Zichao Li</a>
|
<a href=/people/s/shaojie-he/>Shaojie He</a>
|
<a href=/people/m/meng-liao/>Meng Liao</a>
|
<a href=/people/x/xuanang-chen/>Xuanang Chen</a>
|
<a href=/people/y/yaojie-lu/>Yaojie Lu</a>
|
<a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/y/yanxiong-lu/>Yanxiong Lu</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1003><div class="card-body p-3 small">Document logical structuring aims to extract the underlying hierarchical structure of documents, which is crucial for document intelligence. Traditional approaches often fall short in handling the complexity and the variability of lengthy documents. To address these issues, we introduce Seg2Act, an end-to-end, generation-based method for document logical structuring, revisiting logical structure extraction as an action generation task. Specifically, given the text segments of a document, Seg2Act iteratively generates the action sequence via a global context-aware generative model, and simultaneously updates its global context and current logical structure based on the generated actions. Experiments on ChCatExt and HierDoc datasets demonstrate the superior performance of Seg2Act in both supervised and transfer learning settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1004.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1004/>Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for <span class=acl-fixed-case>LLM</span> Pruning</a></strong><br><a href=/people/a/abhinav-bandari/>Abhinav Bandari</a>
|
<a href=/people/l/lu-yin/>Lu Yin</a>
|
<a href=/people/c/cheng-yu-hsieh/>Cheng-Yu Hsieh</a>
|
<a href=/people/a/ajay-kumar-jaiswal/>Ajay Kumar Jaiswal</a>
|
<a href=/people/t/tianlong-chen/>Tianlong Chen</a>
|
<a href=/people/l/li-shen/>Li Shen</a>
|
<a href=/people/r/ranjay-krishna/>Ranjay Krishna</a>
|
<a href=/people/s/shiwei-liu/>Shiwei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1004><div class="card-body p-3 small">Network pruning has emerged as a potential solution to make LLMs cheaper to deploy. However, existing LLM pruning approachesuniversally rely on the C4 dataset as the calibration data for calculating pruning scores, leaving its optimality unexplored. In this study, we evaluate the choice of calibration data on LLM pruning, across a wide range of datasets that are most commonly used in LLM training and evaluation, including four pertaining datasets as well as three categories of downstream tasks encompassing nine datasets. Each downstream dataset is prompted with In-Context Learning (ICL) and Chain-of-Thought (CoT), respectively. Besides the already intriguingobservation that the choice of calibration data significantly impacts the performance of pruned LLMs, our results also uncover several subtle and often unexpected findings, summarized as follows: (1) C4 is not the optimal choice for LLM pruning, even among commonly used pre-training datasets; (2) arithmetic datasets—when used as calibration data—performs on par or even better than pre-training datasets; (3) pruning with downstream datasets does not necessarily help the corresponding downstream task, compared to pre-training data; (4) ICL is widely beneficial to all data categories, whereas CoT is only useful on certain tasks. Our findings shed light on the importance of carefully selecting calibration data for LLM pruning and pave the way for more efficient deployment of these powerfulmodels in real-world applications. We release our code at: https://github.com/abx393/llm-pruning-calibration-data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1005.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1005/>Revisiting the Robustness of Watermarking to Paraphrasing Attacks</a></strong><br><a href=/people/s/saksham-rastogi/>Saksham Rastogi</a>
|
<a href=/people/d/danish-pruthi/>Danish Pruthi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1005><div class="card-body p-3 small">Amidst rising concerns about the internet being proliferated with content generated from language models (LMs), watermarking is seen as a principled way to certify whether text was generated from a model. Many recent watermarking techniques slightly modify the output probabilities of LMs to embed a signal in the generated output that can later be detected. Since early proposals for text watermarking, questions about their robustness to paraphrasing have been prominently discussed. Lately, some techniques are deliberately designed and claimed to be robust to paraphrasing. Particularly, a recent approach trains a model to produce a watermarking signal that is invariant to semantically-similar inputs. However, such watermarking schemes do not adequately account for the ease with which they can be reverse-engineered. We show that with limited access to model generations, we can undo the effects of watermarking and drastically improve the effectiveness of paraphrasing attacks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1006.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1006/>A Survey of Ontology Expansion for Conversational Understanding</a></strong><br><a href=/people/j/jinggui-liang/>Jinggui Liang</a>
|
<a href=/people/y/yuxia-wu/>Yuxia Wu</a>
|
<a href=/people/y/yuan-fang/>Yuan Fang</a>
|
<a href=/people/h/hao-fei/>Hao Fei</a>
|
<a href=/people/l/lizi-liao/>Lizi Liao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1006><div class="card-body p-3 small">In the rapidly evolving field of conversational AI, Ontology Expansion (OnExp) is crucial for enhancing the adaptability and robustness of conversational agents. Traditional models rely on static, predefined ontologies, limiting their ability to handle new and unforeseen user needs. This survey paper provides a comprehensive review of the state-of-the-art techniques in OnExp for conversational understanding. It categorizes the existing literature into three main areas: (1) New Intent Discovery, (2) New Slot-Value Discovery, and (3) Joint OnExp. By examining the methodologies, benchmarks, and challenges associated with these areas, we highlight several emerging frontiers in OnExp to improve agent performance in real-world scenarios and discuss their corresponding challenges. This survey aspires to be a foundational reference for researchers and practitioners, promoting further exploration and innovation in this crucial domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1007.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1007/>Calibrating Language Models with Adaptive Temperature Scaling</a></strong><br><a href=/people/j/johnathan-xie/>Johnathan Xie</a>
|
<a href=/people/a/annie-s-chen/>Annie S Chen</a>
|
<a href=/people/y/yoonho-lee/>Yoonho Lee</a>
|
<a href=/people/e/eric-mitchell/>Eric Mitchell</a>
|
<a href=/people/c/chelsea-finn/>Chelsea Finn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1007><div class="card-body p-3 small">The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration—how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly. In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1008.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1008/>Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?</a></strong><br><a href=/people/f/fumiya-uchiyama/>Fumiya Uchiyama</a>
|
<a href=/people/t/takeshi-kojima/>Takeshi Kojima</a>
|
<a href=/people/a/andrew-gambardella/>Andrew Gambardella</a>
|
<a href=/people/q/qi-cao/>Qi Cao</a>
|
<a href=/people/y/yusuke-iwasawa/>Yusuke Iwasawa</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1008><div class="card-body p-3 small">Recent large language models (LLMs) have demonstrated remarkable generalization abilities in mathematics and logical reasoning tasks.Prior research indicates that LLMs pre-trained with programming language data exhibit high mathematical and reasoning abilities; however, this causal relationship has not been rigorously tested. Our research aims to verify which programming languages and features during pre-training affect logical inference performance. Specifically, we pre-trained decoder-based language models from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under identical conditions. Thereafter, we evaluated the trained models in a few-shot in-context learning setting on logical reasoning tasks: FLD and bAbi, which do not require commonsense or world knowledge. The results demonstrate that nearly all models trained with programming languages consistently outperform those trained with natural languages, indicating that programming languages contain factors that elicit logic inference performance. In addition, we found that models trained with programming languages exhibit a better ability to follow instructions compared to those trained with natural languages. Further analysis reveals that the depth of Abstract Syntax Trees representing parsed results of programs also affects logical reasoning performance. These findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1009.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1009/>Why do objects have many names? A study on word informativeness in language use and lexical systems</a></strong><br><a href=/people/e/eleonora-gualdoni/>Eleonora Gualdoni</a>
|
<a href=/people/g/gemma-boleda/>Gemma Boleda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1009><div class="card-body p-3 small">Human lexicons contain many different words that speakers can use to refer to the same object, e.g., *purple* or *magenta* for the same shade of color. On the one hand, studies on language use have explored how speakers adapt their referring expressions to successfully communicate in context, without focusing on properties of the lexical system. On the other hand, studies in language evolution have discussed how competing pressures for informativeness and simplicity shape lexical systems, without tackling in-context communication. We aim at bridging the gap between these traditions, and explore why a soft mapping between referents and words is a good solution for communication, by taking into account both in-context communication and the structure of the lexicon. We propose a simple measure of informativeness for words and lexical systems, grounded in a visual space, and analyze color naming data for English and Mandarin Chinese. We conclude that optimal lexical systems are those where multiple words can apply to the same referent, conveying different amounts of information. Such systems allow speakers to maximize communication accuracy and minimize the amount of information they convey when communicating about referents in contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1010.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1010/>Dual-Space Knowledge Distillation for Large Language Models</a></strong><br><a href=/people/s/songming-zhang/>Songming Zhang</a>
|
<a href=/people/x/xue-zhang/>Xue Zhang</a>
|
<a href=/people/z/zengkui-sun/>Zengkui Sun</a>
|
<a href=/people/y/yufeng-chen/>Yufeng Chen</a>
|
<a href=/people/j/jinan-xu/>Jinan Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1010><div class="card-body p-3 small">Knowledge distillation (KD) is known as a promising solution to compress large language models (LLMs) via transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output distributions of the two models so that more knowledge can be transferred. However, in the current white-box KD framework, the output distributions are from the respective output spaces of the two models, using their own prediction heads. We argue that the space discrepancy will lead to low similarity between the teacher model and the student model on both representation and distribution levels. Furthermore, this discrepancy also hinders the KD process between models with different vocabularies, which is common for current LLMs. To address these issues, we propose a dual-space knowledge distillation (DSKD) framework that unifies the output spaces of the two models for KD. On the basis of DSKD, we further develop a cross-model attention mechanism, which can automatically align the representations of the two models with different vocabularies. Thus, our framework is not only compatible with various distance functions for KD (e.g., KL divergence) like the current framework, but also supports KD between any two LLMs regardless of their vocabularies. Experiments on task-agnostic instruction-following benchmarks show that DSKD significantly outperforms the current white-box KD framework with various distance functions, and also surpasses existing KD methods for LLMs with different vocabularies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1011.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1011.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1011/><span class=acl-fixed-case>N</span>oise<span class=acl-fixed-case>B</span>ench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition</a></strong><br><a href=/people/e/elena-merdjanovska/>Elena Merdjanovska</a>
|
<a href=/people/a/ansar-aynetdinov/>Ansar Aynetdinov</a>
|
<a href=/people/a/alan-akbik/>Alan Akbik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1011><div class="card-body p-3 small">Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries. Such label noise poses challenges for supervised learning and may significantly deteriorate model quality. To address this, prior work proposed various noise-robust learning approaches capable of learning from data with partially incorrect labels. These approaches are typically evaluated using simulated noise where the labels in a clean dataset are automatically corrupted. However, as we show in this paper, this leads to unrealistic noise that is far easier to handle than real noise caused by human error or semi-automatic annotation. To enable the study of the impact of various types of real noise, we introduce NoiseBench, an NER benchmark consisting of clean training data corrupted with 6 types of real noise, including expert errors, crowdsourcing errors, automatic annotation errors and LLM errors. We present an analysis that shows that real noise is significantly more challenging than simulated noise, and show that current state-of-the-art models for noise-robust learning fall far short of their achievable upper bound. We release NoiseBench for both English and German to the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1012.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1012/>On the Universal Truthfulness Hyperplane Inside <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/j/junteng-liu/>Junteng Liu</a>
|
<a href=/people/s/shiqi-chen/>Shiqi Chen</a>
|
<a href=/people/y/yu-cheng/>Yu Cheng</a>
|
<a href=/people/j/junxian-he/>Junxian He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1012><div class="card-body p-3 small">While large language models (LLMs) have demonstrated remarkable abilities across various fields, hallucination remains a significant challenge. Recent studies have explored hallucinations through the lens of internal representations, proposing mechanisms to decipher LLMs’ adherence to facts. However, these approaches often fail to generalize to out-of-distribution data, leading to concerns about whether internal representation patterns reflect fundamental factual awareness, or only overfit spurious correlations on the specific datasets. In this work, we investigate whether a universal truthfulness hyperplane that distinguishes the model’s factually correct and incorrect outputs exists within the model. To this end, we scale up the number of training datasets and conduct an extensive evaluation – we train the truthfulness hyperplane on a diverse collection of over 40 datasets and examine its cross-task, cross-domain, and in-domain generalization. Our results indicate that increasing the diversity of the training datasets significantly enhances the performance in all scenarios, while the volume of data samples plays a less critical role. This finding supports the optimistic hypothesis that a universal truthfulness hyperplane may indeed exist within the model, offering promising directions for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1013.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1013/><span class=acl-fixed-case>P</span>air<span class=acl-fixed-case>D</span>istill: Pairwise Relevance Distillation for Dense Retrieval</a></strong><br><a href=/people/c/chao-wei-huang/>Chao-Wei Huang</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1013><div class="card-body p-3 small">Effective information retrieval (IR) from vast datasets relies on advanced techniques to extract relevant information in response to queries. Recent advancements in dense retrieval have showcased remarkable efficacy compared to traditional sparse retrieval methods. To further enhance retrieval performance, knowledge distillation techniques, often leveraging robust cross-encoder rerankers, have been extensively explored. However, existing approaches primarily distill knowledge from pointwise rerankers, which assign absolute relevance scores to documents, thus facing challenges related to inconsistent comparisons. This paper introduces Pairwise Relevance Distillation (PairDistill) to leverage pairwise reranking, offering fine-grained distinctions between similarly relevant documents to enrich the training of dense retrieval models. Our experiments demonstrate that PairDistill outperforms existing methods, achieving new state-of-the-art results across multiple benchmarks. This highlights the potential of PairDistill in advancing dense retrieval techniques effectively. Our source code and trained models are released at https://github.com/MiuLab/PairDistill</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1014.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1014/>User Inference Attacks on Large Language Models</a></strong><br><a href=/people/n/nikhil-kandpal/>Nikhil Kandpal</a>
|
<a href=/people/k/krishna-pillutla/>Krishna Pillutla</a>
|
<a href=/people/a/alina-oprea/>Alina Oprea</a>
|
<a href=/people/p/peter-kairouz/>Peter Kairouz</a>
|
<a href=/people/c/christopher-a-choquette-choo/>Christopher A. Choquette-Choo</a>
|
<a href=/people/z/zheng-xu/>Zheng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1014><div class="card-body p-3 small">Text written by humans makes up the vast majority of the data used to pre-train and fine-tune large language models (LLMs). Many sources of this data—like code, forum posts, personal websites, and books—are easily attributed to one or a few “users”. In this paper, we ask if it is possible to infer if any of a _user’s_ data was used to train an LLM. Not only would this constitute a breach of privacy, but it would also enable users to detect when their data was used for training. We develop the first effective attacks for _user inference_—at times, with near-perfect success—against LLMs. Our attacks are easy to employ, requiring only black-box access to an LLM and a few samples from the user, which _need not be the ones that were trained on_. We find, both theoretically and empirically, that certain properties make users more susceptible to user inference: being an outlier, having highly correlated examples, and contributing a larger fraction of data. Based on these findings, we identify several methods for mitigating user inference including training with example-level differential privacy, removing within-user duplicate examples, and reducing a user’s contribution to the training data. Though these provide partial mitigation, our work highlights the need to develop methods to fully protect LLMs from user inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1015.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1015/><span class=acl-fixed-case>H</span>i<span class=acl-fixed-case>FT</span>: A Hierarchical Full Parameter Fine-Tuning Strategy</a></strong><br><a href=/people/y/yongkang-liu/>YongKang Liu</a>
|
<a href=/people/y/yiqun-zhang/>Yiqun Zhang</a>
|
<a href=/people/q/qian-li/>Qian Li</a>
|
<a href=/people/t/tong-liu/>Tong Liu</a>
|
<a href=/people/s/shi-feng/>Shi Feng</a>
|
<a href=/people/d/daling-wang/>Daling Wang</a>
|
<a href=/people/y/yifei-zhang/>Yifei Zhang</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schuetze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1015><div class="card-body p-3 small">Full-parameter fine-tuning (FPFT) has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which potentially compromises the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. We propose a novel, memory-efficient, optimizer-independent, end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT significantly reduces the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance with parameter-efficient fine-tuning and standard FPFT. (2) Results on six models show that HiFT reduces the number of trainable parameters by about 89.18% on average compared to FPFT. (3) HiFT supports FPFT of 7B models for 24G GPU memory devices under mixed precision without using any memory saving techniques. (4) HiFT supports various optimizers including AdamW, AdaGrad, SGD, etc. The source code link is https://github.com/misonsky/HiFT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1016.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1016/>Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (<span class=acl-fixed-case>CLIP</span>) Models</a></strong><br><a href=/people/y/yufang-liu/>Yufang Liu</a>
|
<a href=/people/t/tao-ji/>Tao Ji</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/a/aimin-zhou/>Aimin Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1016><div class="card-body p-3 small">Large Vision-Language Models (LVLMs) have achieved impressive performance, yet research has pointed out a serious issue with object hallucinations within these models. However, there is no clear conclusion as to which part of the model these hallucinations originate from. In this paper, we present an in-depth investigation into the object hallucination problem specifically within the CLIP model, which serves as the backbone for many state-of-the-art vision-language systems. We unveil that even in isolation, the CLIP model is prone to object hallucinations, suggesting that the hallucination problem is not solely due to the interaction between vision and language modalities. To address this, we propose a counterfactual data augmentation method by creating negative samples with a variety of hallucination issues. We demonstrate that our method can effectively mitigate object hallucinations for CLIP model, and we show the the enhanced model can be employed as a visual encoder, effectively alleviating the object hallucination issue in LVLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1017.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1017/>Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning <span class=acl-fixed-case>LLM</span>s for Simultaneous Translation</a></strong><br><a href=/people/m/matthew-raffel/>Matthew Raffel</a>
|
<a href=/people/v/victor-agostinelli/>Victor Agostinelli</a>
|
<a href=/people/l/lizhong-chen/>Lizhong Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1017><div class="card-body p-3 small">Large language models (LLMs) have achieved state-of-the-art performance in various language processing tasks, motivating their adoption in simultaneous translation. Current fine-tuning methods to adapt LLMs for simultaneous translation focus on prompting optimization strategies using either data augmentation or prompt structure modifications. However, these methods suffer from several issues, such as unnecessarily expanded training sets, computational inefficiency from dumping the key and value cache, increased prompt sizes, or restriction to a single decision policy. To eliminate these issues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs for simultaneous translation. It utilizes a novel attention mask approach that models simultaneous translation during fine-tuning by masking attention for a desired decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT 2017 dataset, we have observed a significant translation quality improvement compared to state-of-the-art prompting optimization strategies on five language pairs while reducing the computational cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1018.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1018/><span class=acl-fixed-case>T</span>ool<span class=acl-fixed-case>P</span>lanner: A Tool Augmented <span class=acl-fixed-case>LLM</span> for Multi Granularity Instructions with Path Planning and Feedback</a></strong><br><a href=/people/q/qinzhuo-wu/>Qinzhuo Wu</a>
|
<a href=/people/w/wei-liu/>Wei Liu</a>
|
<a href=/people/j/jian-luan/>Jian Luan</a>
|
<a href=/people/b/bin-wang/>Bin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1018><div class="card-body p-3 small">Recently, tool-augmented LLMs have gained increasing attention. Given an instruction, tool-augmented LLMs can interact with various external tools in multiple rounds and provide a final answer. However, previous LLMs were trained on overly detailed instructions, which included API names or parameters, while real users would not explicitly mention these API details. This leads to a gap between trained LLMs and real-world scenarios. In addition, most works ignore whether the interaction process follows the instruction. To address these issues, we constructed a training dataset called MGToolBench, which contains statement and category-level instructions to better reflect real-world scenarios. In addition, we propose ToolPlanner, a two-stage reinforcement learning framework that utilizes path planning and two feedback mechanisms to enhance the LLM’s task completion and instruction-following capabilities. Experimental results show that ToolPlanner significantly improves the Match Rate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA model. Human evaluation verifies that the multi-granularity instructions can better align with users’ usage habits. Our data and code will be released upon acceptance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1019.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1019/>Please note that <span class=acl-fixed-case>I</span>’m just an <span class=acl-fixed-case>AI</span>: Analysis of Behavior Patterns of <span class=acl-fixed-case>LLM</span>s in (Non-)offensive Speech Identification</a></strong><br><a href=/people/e/esra-donmez/>Esra Dönmez</a>
|
<a href=/people/t/thang-vu/>Thang Vu</a>
|
<a href=/people/a/agnieszka-falenska/>Agnieszka Falenska</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1019><div class="card-body p-3 small">Offensive speech is highly prevalent on online platforms. Being trained on online data, Large Language Models (LLMs) display undesirable behaviors, such as generating harmful text or failing to recognize it. Despite these shortcomings, the models are becoming a part of our everyday lives by being used as tools for information search, content creation, writing assistance, and many more. Furthermore, the research explores using LLMs in applications with immense social risk, such as late-life companions and online content moderators. Despite the potential harms from LLMs in such applications, whether LLMs can reliably identify offensive speech and how they behave when they fail are open questions. This work addresses these questions by probing sixteen widely used LLMs and showing that most fail to identify (non-)offensive online language. Our experiments reveal undesirable behavior patterns in the context of offensive speech detection, such as erroneous response generation, over-reliance on profanity, and failure to recognize stereotypes. Our work highlights the need for extensive documentation of model reliability, particularly in terms of the ability to detect offensive language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1020.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1020/>How to Compute the Probability of a Word</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/c/clara-meister/>Clara Meister</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1020><div class="card-body p-3 small">Language models (LMs) estimate a probability distribution over strings in a natural language; these distributions are crucial for computing perplexity and surprisal in linguistics research. While we are usually concerned with measuring these values for words, most LMs operate over subwords. Despite seemingly straightforward, accurately computing probabilities over one unit given probabilities over the other requires care. Indeed, we show here that many recent linguistic studies have been incorrectly computing these values. This paper derives the correct methods for computing word probabilities, highlighting issues when relying on language models that use beginning-of-word (bow)-marking tokenisers, e.g., the GPT family. Empirically, we show that correcting the widespread bug in probability computations affects measured outcomes in sentence comprehension and lexical optimisation analyses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1021.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1021/>A linguistically-motivated evaluation methodology for unraveling model’s abilities in reading comprehension tasks</a></strong><br><a href=/people/e/elie-antoine/>Elie Antoine</a>
|
<a href=/people/f/frederic-bechet/>Frederic Bechet</a>
|
<a href=/people/g/geraldine-damnati/>Géraldine Damnati</a>
|
<a href=/people/p/philippe-langlais/>Philippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1021><div class="card-body p-3 small">We introduce an evaluation methodology for reading comprehension tasks based on the intuition that certain examples, by the virtue of their linguistic complexity, consistently yield lower scores regardless of model size or architecture. We capitalize on semantic frame annotation for characterizing this complexity, and study seven complexity factors that may account for model’s difficulty. We first deploy this methodology on a carefully annotated French reading comprehension benchmark showing that two of those complexity factors are indeed good predictors of models’ failure, while others are less so. We further deploy our methodology on a well studied English benchmark by using chatGPT as a proxy for semantic annotation.Our study reveals that fine-grained linguistically-motivated automatic evaluation of a reading comprehension task is not only possible, but helps understand models’ abilities to handle specific linguistic characteristics of input examples. It also shows that current state-of-the-art models fail with some for those characteristics which suggests that adequately handling them requires more than merely increasing model size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1022.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1022.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1022.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1022/><span class=acl-fixed-case>G</span>uard<span class=acl-fixed-case>B</span>ench: A Large-Scale Benchmark for Guardrail Models</a></strong><br><a href=/people/e/elias-bassani/>Elias Bassani</a>
|
<a href=/people/i/ignacio-sanchez/>Ignacio Sanchez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1022><div class="card-body p-3 small">Generative AI systems powered by Large Language Models have become increasingly popular in recent years. Lately, due to the risk of providing users with unsafe information, the adoption of those systems in safety-critical domains has raised significant concerns. To respond to this situation, input-output filters, commonly called guardrail models, have been proposed to complement other measures, such as model alignment. Unfortunately, the lack of a standard benchmark for guardrail models poses significant evaluation issues and makes it hard to compare results across scientific publications. To fill this gap, we introduce GuardBench, a large-scale benchmark for guardrail models comprising 40 safety evaluation datasets. To facilitate the adoption of GuardBench, we release a Python library providing an automated evaluation pipeline built on top of it. With our benchmark, we also share the first large-scale prompt moderation datasets in German, French, Italian, and Spanish. To assess the current state-of-the-art, we conduct an extensive comparison of recent guardrail models and show that a general-purpose instruction-following model of comparable size achieves competitive results without the need for specific fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1023.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1023/>Generate-on-Graph: Treat <span class=acl-fixed-case>LLM</span> as both Agent and <span class=acl-fixed-case>KG</span> for Incomplete Knowledge Graph Question Answering</a></strong><br><a href=/people/y/yao-xu/>Yao Xu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/j/jiabei-chen/>Jiabei Chen</a>
|
<a href=/people/z/zihao-wang/>Zihao Wang</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a>
|
<a href=/people/h/hanghang-tong/>Hanghang Tong</a>
|
<a href=/people/g/guang-liu/>Guang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1023><div class="card-body p-3 small">To address the issues of insufficient knowledge and hallucination in Large Language Models (LLMs), numerous studies have explored integrating LLMs with Knowledge Graphs (KGs). However, these methods are typically evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where all factual triples required for each question are entirely covered by the given KG. In such cases, LLMs primarily act as an agent to find answer entities within the KG, rather than effectively integrating the internal knowledge of LLMs and external knowledge sources such as KGs. In fact, KGs are often incomplete to cover all the knowledge required to answer questions. To simulate these real-world scenarios and evaluate the ability of LLMs to integrate internal and external knowledge, we propose leveraging LLMs for QA under Incomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the factual triples for each question, and construct corresponding datasets. To handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG), which can generate new factual triples while exploring KGs. Specifically, GoG performs reasoning through a Thinking-Searching-Generating framework, which treats LLM as both Agent and KG in IKGQA. Experimental results on two datasets demonstrate that our GoG outperforms all previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1024.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1024/>Language models and brains align due to more than next-word prediction and word-level information</a></strong><br><a href=/people/g/gabriele-merlin/>Gabriele Merlin</a>
|
<a href=/people/m/mariya-toneva/>Mariya Toneva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1024><div class="card-body p-3 small">Pretrained language models have been shown to significantly predict brain recordings of people comprehending language. Recent work suggests that the prediction of the next word is a key mechanism that contributes to this alignment. What is not yet understood is whether prediction of the next word is necessary for this observed alignment or simply sufficient, and whether there are other shared mechanisms or information that are similarly important. In this work, we take a step towards understanding the reasons for brain alignment via two simple perturbations in popular pretrained language models. These perturbations help us design contrasts that can control for different types of information. By contrasting the brain alignment of these differently perturbed models, we show that improvements in alignment with brain recordings are due to more than improvements in next-word prediction and word-level information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1025.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1025/><span class=acl-fixed-case>LLME</span>dge<span class=acl-fixed-case>R</span>efine: Enhancing Text Clustering with <span class=acl-fixed-case>LLM</span>-Based Boundary Point Refinement</a></strong><br><a href=/people/z/zijin-feng/>Zijin Feng</a>
|
<a href=/people/l/luyang-lin/>Luyang Lin</a>
|
<a href=/people/l/lingzhi-wang/>Lingzhi Wang</a>
|
<a href=/people/h/hong-cheng/>Hong Cheng</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1025><div class="card-body p-3 small">Text clustering is a fundamental task in natural language processing with numerous applications. However, traditional clustering methods often struggle with domain-specific fine-tuning and the presence of outliers. To address these challenges, we introduce LLMEdgeRefine, an iterative clustering method enhanced by large language models (LLMs), focusing on edge points refinement. LLMEdgeRefine enhances current clustering methods by creating super-points to mitigate outliers and iteratively refining clusters using LLMs for improved semantic coherence. Our method demonstrates superior performance across multiple datasets, outperforming state-of-the-art techniques, and offering robustness, adaptability, and cost-efficiency for diverse text clustering applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1026.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1026/><span class=acl-fixed-case>C</span>asi<span class=acl-fixed-case>M</span>edicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures</a></strong><br><a href=/people/e/ekaterina-sviridova/>Ekaterina Sviridova</a>
|
<a href=/people/a/anar-yeginbergen/>Anar Yeginbergen</a>
|
<a href=/people/a/ainara-estarrona/>Ainara Estarrona</a>
|
<a href=/people/e/elena-cabrio/>Elena Cabrio</a>
|
<a href=/people/s/serena-villata/>Serena Villata</a>
|
<a href=/people/r/rodrigo-agerri/>Rodrigo Agerri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1026><div class="card-body p-3 small">Explaining Artificial Intelligence (AI) decisions is a major challenge nowadays in AI, in particular when applied to sensitive scenarios like medicine and law. However, the need to explain the rationale behind decisions is a main issues also for human-based deliberation as it is important to justify why a certain decision has been taken. Resident medical doctors for instance are required not only to provide a (possibly correct) diagnosis, but also to explain how they reached a certain conclusion. Developing new tools to aid residents to train their explanation skills is therefore a central objective of AI in education. In this paper, we follow this direction, and we present, to the best of our knowledge, the first multilingual dataset for Medical Question Answering where correct and incorrect diagnoses for a clinical case are enriched with a natural language explanation written by doctors. These explanations have been manually annotated with argument components (i.e., premise, claim) and argument relations (i.e., attack, support). The Multilingual CasiMedicos-arg dataset consists of 558 clinical cases (English, Spanish, French, Italian) with explanations, where we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106 attack relations. We conclude by showing how competitive baselines perform over this challenging dataset for the argument mining task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1027.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1027.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1027/>A Simple and Effective <span class=tex-math>L_2</span> Norm-Based Strategy for <span class=acl-fixed-case>KV</span> Cache Compression</a></strong><br><a href=/people/a/alessio-devoto/>Alessio Devoto</a>
|
<a href=/people/y/yu-zhao/>Yu Zhao</a>
|
<a href=/people/s/simone-scardapane/>Simone Scardapane</a>
|
<a href=/people/p/pasquale-minervini/>Pasquale Minervini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1027><div class="card-body p-3 small">The deployment of large language models (LLMs) is often hindered by the extensive memory requirements of the Key-Value (KV) cache, especially as context lengths increase. Existing approaches to reduce the KV cache size involve either fine-tuning the model to learn a compression strategy or leveraging attention scores to reduce the sequence length. We analyse the attention distributions in decoder-only Transformers-based models and observe that attention allocation patterns stay consistent across most layers. Surprisingly, we find a clear correlation between the <span class=tex-math>L<sub>2</sub></span> norm and the attention scores over cached KV pairs, where a low <span class=tex-math>L<sub>2</sub></span> norm of a key embedding usually leads to a high attention score during decoding. This finding indicates that the influence of a KV pair is potentially determined by the key embedding itself before being queried. Based on this observation, we compress the KV cache based on the <span class=tex-math>L<sub>2</sub></span> norm of key embeddings. Our experimental results show that this simple strategy can reduce the KV cache size by 50% on language modelling and needle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing accuracy. Moreover, without relying on the attention scores, this approach remains compatible with FlashAttention, enabling broader applicability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1028.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1028/><span class=acl-fixed-case>GOME</span>: Grounding-based Metaphor Binding With Conceptual Elaboration For Figurative Language Illustration</a></strong><br><a href=/people/l/linhao-zhang/>Linhao Zhang</a>
|
<a href=/people/j/jintao-liu/>Jintao Liu</a>
|
<a href=/people/l/li-jin/>Li Jin</a>
|
<a href=/people/h/hao-wang/>Hao Wang</a>
|
<a href=/people/k/kaiwen-wei/>Kaiwen Wei</a>
|
<a href=/people/g/guangluan-xu/>Guangluan Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1028><div class="card-body p-3 small">The illustration or visualization of figurative language, such as linguistic metaphors, is an emerging challenge for existing Large Language Models (LLMs) and multimodal models. Due to their comparison of seemingly unrelated concepts in metaphors, existing LLMs have a tendency of over-literalization, which illustrates figurative language solely based on literal objects, ignoring the underlying groundings and associations across disparate metaphorical domains. Furthermore, prior approaches have ignored the binding process between visual objects and metaphorical attributes, which further intensifies the infidelity of visual metaphors. To address the issues above, we propose GOME (Grounding-based Metaphor Binding), which illustrates linguistic metaphors from the grounding perspective elaborated through LLMs. GOME consists of two steps for metaphor illustration, including grounding-based elaboration and scenario visualization. In the elaboration step, metaphorical knowledge is integrated into systematic instructions for LLMs, which employs a CoT prompting method rooted in rhetoric. This approach specifies metaphorical devices such as vehicles and groundings, to ensure accurate and faithful descriptions consumed by text-to-image models. In the visualization step, an inference-time metaphor binding method is realized based on elaboration outputs, which register attentional control during the diffusion process, and captures the underlying attributes from the abstract metaphorical domain. Comprehensive evaluations using multiple downstream tasks confirm that, GOME is superior to isolated LLMs, diffusion models, or their direct collaboration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1029.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1029/><span class=acl-fixed-case>D</span>3<span class=acl-fixed-case>CODE</span>: Disentangling Disagreements in Data across Cultures on Offensiveness Detection and Evaluation</a></strong><br><a href=/people/a/aida-mostafazadeh-davani/>Aida Mostafazadeh Davani</a>
|
<a href=/people/m/mark-diaz/>Mark Diaz</a>
|
<a href=/people/d/dylan-k-baker/>Dylan K Baker</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1029><div class="card-body p-3 small">While human annotations play a crucial role in language technologies, annotator subjectivity has long been overlooked in data collection. Recent studies that critically examine this issue are often focused on Western contexts, and solely document differences across age, gender, or racial groups. Consequently, NLP research on subjectivity have failed to consider that individuals within demographic groups may hold diverse values, which influence their perceptions beyond group norms. To effectively incorporate these considerations into NLP pipelines, we need datasets with extensive parallel annotations from a variety of social and cultural groups.In this paper we introduce the D3CODE dataset: a large-scale cross-cultural dataset of parallel annotations for offensive language in over 4.5K English sentences annotated by a pool of more than 4k annotators, balanced across gender and age, from across 21 countries, representing eight geo-cultural regions. The dataset captures annotators’ moral values along six moral foundations: care, equality, proportionality, authority, loyalty, and purity. Our analyses reveal substantial regional variations in annotators’ perceptions that are shaped by individual moral values, providing crucial insights for developing pluralistic, culturally sensitive NLP models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1030.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1030/><span class=acl-fixed-case>PALM</span>: Few-Shot Prompt Learning for Audio Language Models</a></strong><br><a href=/people/a/asif-hanif/>Asif Hanif</a>
|
<a href=/people/m/maha-tufail-agro/>Maha Tufail Agro</a>
|
<a href=/people/m/mohammad-areeb-qazi/>Mohammad Areeb Qazi</a>
|
<a href=/people/h/hanan-aldarmaki/>Hanan Aldarmaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1030><div class="card-body p-3 small">Audio-Language Models (ALMs) have recently achieved remarkable success in zero-shot audio recognition tasks, which match features of audio waveforms with class-specific text prompt features, inspired by advancements in Vision-Language Models (VLMs). Given the sensitivity of zero-shot performance to the choice of hand-crafted text prompts, many prompt learning techniques have been developed for VLMs. We explore the efficacy of these approaches in ALMs and propose a novel method, Prompt Learning in Audio Language Models (PALM), which optimizes the feature space of the text encoder branch. Unlike existing methods that work in the input space, our approach results in greater training efficiency. We demonstrate the effectiveness of our approach on 11 audio recognition datasets, encompassing a variety of speech-processing tasks, and compare the results with three baselines in a few-shot learning setup. Our method is either on par with or outperforms other approaches while being computationally less demanding. Our code is publicly available at <a href=https://asif-hanif.github.io/palm/ class=acl-markup-url>https://asif-hanif.github.io/palm/</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1031.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1031.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1031/>Annotator-Centric Active Learning for Subjective <span class=acl-fixed-case>NLP</span> Tasks</a></strong><br><a href=/people/m/michiel-van-der-meer/>Michiel Van Der Meer</a>
|
<a href=/people/n/neele-falk/>Neele Falk</a>
|
<a href=/people/p/pradeep-k-murukannaiah/>Pradeep K. Murukannaiah</a>
|
<a href=/people/e/enrico-liscio/>Enrico Liscio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1031><div class="card-body p-3 small">Active Learning (AL) addresses the high costs of collecting human annotations by strategically annotating the most informative samples. However, for subjective NLP tasks, incorporating a wide range of perspectives in the annotation process is crucial to capture the variability in human judgments. We introduce Annotator-Centric Active Learning (ACAL), which incorporates an annotator selection strategy following data sampling. Our objective is two-fold: (1) to efficiently approximate the full diversity of human judgments, and (2) to assess model performance using annotator-centric metrics, which value minority and majority perspectives equally. We experiment with multiple annotator selection strategies across seven subjective NLP tasks, employing both traditional and novel, human-centered evaluation metrics. Our findings indicate that ACAL improves data efficiency and excels in annotator-centric performance evaluations. However, its success depends on the availability of a sufficiently large and diverse pool of annotators to sample from.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1032.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1032/>On the Proper Treatment of Tokenization in Psycholinguistics</a></strong><br><a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>
|
<a href=/people/l/luca-malagutti/>Luca Malagutti</a>
|
<a href=/people/j/juan-luis-gastaldi/>Juan Luis Gastaldi</a>
|
<a href=/people/b/brian-dusell/>Brian DuSell</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1032><div class="card-body p-3 small">Language models are widely used in computational psycholinguistics to test theories that relate the negative log probability (the surprisal) of a region of interest (a substring of characters) under a language model to its cognitive cost experienced by readers, as operationalized, for example, by gaze duration on the region. However, the application of modern language models to psycholinguistic studies is complicated by the practice of using tokenization as an intermediate step in training a model. Doing so results in a language model over *token* strings rather than one over character strings. Vexingly, regions of interest are generally misaligned with these token strings. The paper argues that token-level language models should be (approximately) marginalized into character-level language models before they are used in psycholinguistic studies to compute the surprisal of a region of interest; then, the marginalized character-level language model can be used to compute the surprisal of an arbitrary character substring, which we term a focal area, that the experimenter may wish to use as a predictor. Our proposal of marginalizing a token-level model into a character-level one solves this misalignment issue independently of the tokenization scheme. Empirically, we discover various focal areas whose surprisal is a better psychometric predictor than the surprisal of the region of interest itself.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1033.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1033.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1033/>Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation</a></strong><br><a href=/people/a/anas-himmi/>Anas Himmi</a>
|
<a href=/people/g/guillaume-staerman/>Guillaume Staerman</a>
|
<a href=/people/m/marine-picot/>Marine Picot</a>
|
<a href=/people/p/pierre-colombo/>Pierre Colombo</a>
|
<a href=/people/n/nuno-m-guerreiro/>Nuno M Guerreiro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1033><div class="card-body p-3 small">Hallucinated translations pose significant threats and safety concerns when it comes to practical deployment of machine translation systems. Previous research works have identified that detectors exhibit complementary performance — different detectors excel at detecting different types of hallucinations. In this paper, we propose to address the limitations of individual detectors by combining them and introducing a straightforward method for aggregating multiple detectors. Our results demonstrate the efficacy of our aggregated detector, providing a promising step towards evermore reliable machine translation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1034.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1034.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1034.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1034/>Jailbreaking <span class=acl-fixed-case>LLM</span>s with <span class=acl-fixed-case>A</span>rabic Transliteration and <span class=acl-fixed-case>A</span>rabizi</a></strong><br><a href=/people/m/mansour-al-ghanim/>Mansour Al Ghanim</a>
|
<a href=/people/s/saleh-almohaimeed/>Saleh Almohaimeed</a>
|
<a href=/people/m/mengxin-zheng/>Mengxin Zheng</a>
|
<a href=/people/y/yan-solihin/>Yan Solihin</a>
|
<a href=/people/q/qian-lou/>Qian Lou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1034><div class="card-body p-3 small">This study identifies the potential vulnerabilities of Large Language Models (LLMs) to ‘jailbreak’ attacks, specifically focusing on the Arabic language and its various forms. While most research has concentrated on English-based prompt manipulation, our investigation broadens the scope to investigate the Arabic language. We initially tested the AdvBench benchmark in Standardized Arabic, finding that even with prompt manipulation techniques like prefix injection, it was insufficient to provoke LLMs into generating unsafe content. However, when using Arabic transliteration and chatspeak (or arabizi), we found that unsafe content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3 Sonnet. Our findings suggest that using Arabic and its various forms could expose information that might remain hidden, potentially increasing the risk of jailbreak attacks. We hypothesize that this exposure could be due to the model’s learned connection to specific words, highlighting the need for more comprehensive safety training across all language forms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1035.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1035.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1035.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1035/>Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models</a></strong><br><a href=/people/z/zara-siddique/>Zara Siddique</a>
|
<a href=/people/l/liam-turner/>Liam Turner</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1035><div class="card-body p-3 small">Large language models (LLMs) have been shown to propagate and amplify harmful stereotypes, particularly those that disproportionately affect marginalised communities. To understand the effect of these stereotypes more comprehensively, we introduce GlobalBias, a dataset of 876k sentences incorporating 40 distinct gender-by-ethnicity groups alongside descriptors typically used in bias literature, which enables us to study a broad set of stereotypes from around the world. We use GlobalBias to directly probe a suite of LMs via perplexity, which we use as a proxy to determine how certain stereotypes are represented in the model’s internal representations. Following this, we generate character profiles based on given names and evaluate the prevalence of stereotypes in model outputs. We find that the demographic groups associated with various stereotypes remain consistent across model likelihoods and model outputs. Furthermore, larger models consistently display higher levels of stereotypical outputs, even when explicitly instructed not to.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1036.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1036/>Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks</a></strong><br><a href=/people/c/changho-lee/>Changho Lee</a>
|
<a href=/people/j/janghoon-han/>Janghoon Han</a>
|
<a href=/people/s/seonghyeon-ye/>Seonghyeon Ye</a>
|
<a href=/people/s/stanley-jungkyu-choi/>Stanley Jungkyu Choi</a>
|
<a href=/people/h/honglak-lee/>Honglak Lee</a>
|
<a href=/people/k/kyunghoon-bae/>Kyunghoon Bae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1036><div class="card-body p-3 small">Instruction tuning has been proven effective in enhancing zero-shot generalization across various tasks and in improving the performance of specific tasks. For task-specific improvements, strategically selecting and training on related tasks that provide meaningful supervision is crucial, as this approach enhances efficiency and prevents performance degradation from learning irrelevant tasks. In this light, we introduce a simple yet effective task selection method that leverages instruction information alone to identify relevant tasks, optimizing instruction tuning for specific tasks. Our method is significantly more efficient than traditional approaches, which require complex measurements of pairwise transferability between tasks or the creation of data samples for the target task. Additionally, by aligning the model with the unique instructional template style of the meta-dataset, we enhance its ability to granularly discern relevant tasks, leading to improved overall performance. Experimental results demonstrate that training on a small set of tasks, chosen solely based on the instructions, results in substantial improvements in performance on benchmarks such as P3, Big-Bench, NIV2, and Big-Bench Hard. Significantly, these improvements surpass those achieved by prior task selection methods, highlighting the superiority of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1037.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1037/>Recurrent Alignment with Hard Attention for Hierarchical Text Rating</a></strong><br><a href=/people/c/chenxi-lin/>Chenxi Lin</a>
|
<a href=/people/r/ren-jiayu/>Ren Jiayu</a>
|
<a href=/people/g/guoxiu-he/>Guoxiu He</a>
|
<a href=/people/z/zhuoren-jiang/>Zhuoren Jiang</a>
|
<a href=/people/h/haiyan-yu/>Haiyan Yu</a>
|
<a href=/people/x/xiaomin-zhu/>Xiaomin Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1037><div class="card-body p-3 small">While large language models (LLMs) excel at understanding and generating plain text, they are not tailored to handle hierarchical text structures or directly predict task-specific properties such as text rating. In fact, selectively and repeatedly grasping the hierarchical structure of large-scale text is pivotal for deciphering its essence. To this end, we propose a novel framework for hierarchical text rating utilizing LLMs, which incorporates Recurrent Alignment with Hard Attention (RAHA). Particularly, hard attention mechanism prompts a frozen LLM to selectively focus on pertinent leaf texts associated with the root text and generate symbolic representations of their relationships. Inspired by the gradual stabilization of the Markov Chain, recurrent alignment strategy involves feeding predicted ratings iteratively back into the prompts of another trainable LLM, aligning it to progressively approximate the desired target. Experimental results demonstrate that RAHA outperforms existing state-of-the-art methods on three hierarchical text rating datasets. Theoretical and empirical analysis confirms RAHA’s ability to gradually converge towards the underlying target through multiple inferences. Additional experiments on plain text rating datasets verify the effectiveness of this Markov-like alignment. Our data and code can be available in https://github.com/ECNU-Text-Computing/Markov-LLM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1038.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1038/><span class=acl-fixed-case>CHESS</span>: Optimizing <span class=acl-fixed-case>LLM</span> Inference via Channel-Wise Thresholding and Selective Sparsification</a></strong><br><a href=/people/j/junhui-he/>Junhui He</a>
|
<a href=/people/s/shangyu-wu/>Shangyu Wu</a>
|
<a href=/people/w/weidong-wen/>Weidong Wen</a>
|
<a href=/people/c/chun-jason-xue/>Chun Jason Xue</a>
|
<a href=/people/q/qingan-li/>Qingan Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1038><div class="card-body p-3 small">Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these resource challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, they do not model the impact of activation sparsification on performance, resulting in suboptimal performance degradation. To address the limitations, this paper reformulates the activation sparsification problem to explicitly capture the relationship between activation sparsity and model performance. Then, this paper proposes CHESS, a general activation sparsification approach via CHannel-wise thrEsholding and Selective Sparsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers. Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules. Finally, we detail the implementation of sparse kernels to accelerate LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over eight downstream tasks while activating fewer parameters than existing methods, thus speeding up the LLM inference by up to 1.27x.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1039.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1039/>Semformer: Transformer Language Models with Semantic Planning</a></strong><br><a href=/people/y/yongjing-yin/>Yongjing Yin</a>
|
<a href=/people/j/junran-ding/>Junran Ding</a>
|
<a href=/people/k/kai-song/>Kai Song</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1039><div class="card-body p-3 small">Next-token prediction serves as the dominant component in current neural language models.During the training phase, the model employs teacher forcing, which predicts tokens based on all preceding ground truth tokens.However, this approach has been found to create shortcuts, utilizing the revealed prefix to spuriously fit future tokens, potentially compromising the accuracy of the next-token predictor.In this paper, we introduce Semformer, a novel method of training a Transformer language model that explicitly models the semantic planning of response.Specifically, we incorporate a sequence of planning tokens into the prefix, guiding the planning token representations to predict the latent semantic representations of the response, which are induced by an autoencoder.In a minimal planning task (i.e., graph path-finding), our model exhibits near-perfect performance and effectively mitigates shortcut learning, a feat that standard training methods and baseline models have been unable to accomplish.Furthermore, we pretrain Semformer from scratch with 125M parameters, demonstrating its efficacy through measures of perplexity, in-context learning, and fine-tuning on summarization tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1040.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1040/><span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>CG</span>en: Document-based Controlled Code Generation</a></strong><br><a href=/people/s/sameer-pimparkhede/>Sameer Pimparkhede</a>
|
<a href=/people/m/mehant-kammakomati/>Mehant Kammakomati</a>
|
<a href=/people/s/srikanth-g-tamilselvam/>Srikanth G. Tamilselvam</a>
|
<a href=/people/p/prince-kumar/>Prince Kumar</a>
|
<a href=/people/a/ashok-pon-kumar/>Ashok Pon Kumar</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1040><div class="card-body p-3 small">Recent developments show that Large Language Models (LLMs) produce state-of-the-art performance on natural language (NL) to code generation for resource-rich general-purpose languages like C++, Java, and Python. However, their practical usage for structured domain-specific languages (DSLs) such as YAML, JSON is limited due to domain-specific schema, grammar, and customizations generally unseen by LLMs during pre-training. Efforts have been made to mitigate this challenge via in-context learning through relevant examples or by fine-tuning. However, it suffers from problems, such as limited DSL samples and prompt sensitivity but enterprises maintain good documentation of the DSLs. Therefore, we propose DocCGen, a framework that can leverage such rich knowledge by breaking the NL-to-Code generation task for structured code languages into a two-step process. First, it detects the correct libraries using the library documentation that best matches the NL query. Then, it utilizes schema rules extracted from the documentation of these libraries to constrain the decoding. We evaluate our framework for two complex structured languages, Ansible YAML and Bash command, consisting of two settings: Out-of-domain (OOD) and In domain (ID). Our extensive experiments show that DocCGen consistently improves different sized language models across all six evaluation metrics, reducing syntactic and semantic errors in structured code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1041.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1041/>Semantics and Sentiment: Cross-lingual Variations in Emoji Use</a></strong><br><a href=/people/g/giulio-zhou/>Giulio Zhou</a>
|
<a href=/people/s/sydelle-de-souza/>Sydelle De Souza</a>
|
<a href=/people/e/ella-markham/>Ella Markham</a>
|
<a href=/people/o/oghenetekevwe-kwakpovwe/>Oghenetekevwe Kwakpovwe</a>
|
<a href=/people/s/sumin-zhao/>Sumin Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1041><div class="card-body p-3 small">Over the past decade, the use of emojis in social media has seen a rapid increase. Despite their popularity and image-grounded nature, previous studies have found that people interpret emojis inconsistently when presented in context and in isolation. In this work, we explore whether emoji semantics differ across languages and how semantics interacts with sentiment in emoji use across languages. To do so, we developed a corpus containing the literal meanings for a set of emojis, as defined by L1 speakers in English, Portuguese and Chinese. We then use these definitions to assess whether speakers of different languages agree on whether an emoji is being used literally or figuratively in the context where they are grounded in, as well as whether this literal and figurative use correlates with the sentiment of the context itself. We found that there were varying levels of disagreement on the definition for each emoji but that these stayed fairly consistent across languages. We also demonstrated a correlation between the sentiment of a tweet and the figurative use of an emoji, providing theoretical underpinnings for empirical results in NLP tasks, particularly offering insights that can benefit sentiment analysis models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1042.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1042.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1042.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1042/>The Emergence of Compositional Languages in Multi-entity Referential Games: from Image to Graph Representations</a></strong><br><a href=/people/d/daniel-akkerman/>Daniel Akkerman</a>
|
<a href=/people/p/phong-le/>Phong Le</a>
|
<a href=/people/r/raquel-g-alhama/>Raquel G. Alhama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1042><div class="card-body p-3 small">To study the requirements needed for a human-like language to develop, Language Emergence research uses jointly trained artificial agents which communicate to solve a task, the most popular of which is a referential game. The targets that agents refer to typically involve a single entity, which limits their ecological validity and the complexity of the emergent languages. Here, we present a simple multi-entity game in which targets include multiple entities that are spatially related. We ask whether agents dealing with multi-entity targets benefit from the use of graph representations, and explore four different graph schemes. Our game requires more sophisticated analyses to capture the extent to which the emergent languages are compositional, and crucially, what the decomposed features are. We find that emergent languages from our setup exhibit a considerable degree of compositionality, but not over all features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1043.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1043/>Transformers are Multi-State <span class=acl-fixed-case>RNN</span>s</a></strong><br><a href=/people/m/matanel-oren/>Matanel Oren</a>
|
<a href=/people/m/michael-hassid/>Michael Hassid</a>
|
<a href=/people/n/nir-yarden/>Nir Yarden</a>
|
<a href=/people/y/yossi-adi/>Yossi Adi</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1044.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1044/>Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization</a></strong><br><a href=/people/n/niyati-bafna/>Niyati Bafna</a>
|
<a href=/people/k/kenton-murray/>Kenton Murray</a>
|
<a href=/people/d/david-yarowsky/>David Yarowsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1044><div class="card-body p-3 small">While large language models exhibit certain cross-lingual generalization capabilities, they suffer from performance degradation (PD) on unseen closely-related languages (CRLs) and dialects relative to their high-resource language neighbour (HRLN). However, we currently lack a fundamental understanding of what kinds of linguistic distances contribute to PD, and to what extent. Furthermore, studies of cross-lingual generalization are confounded by unknown quantities of CRL language traces in the training data, and by the frequent lack of availability of evaluation data in lower-resource related languages and dialects. To address these issues, we model phonological, morphological, and lexical distance as Bayesian noise processes to synthesize artificial languages that are controllably distant from the HRLN. We analyse PD as a function of underlying noise parameters, offering insights on model robustness to isolated and composed linguistic phenomena, and the impact of task and HRL characteristics on PD. We calculate parameter posteriors on real CRL-HRLN pair data and show that they follow computed trends of artificial languages, demonstrating the viability of our noisers. Our framework offers a cheap solution for estimating task performance on an unseen CRL given HRLN performance using its posteriors, as well as for diagnosing observed PD on a CRL in terms of its linguistic distances from its HRLN, and opens doors to principled methods of mitigating performance degradation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1045.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1045.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1045/>Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion</a></strong><br><a href=/people/k/kerem-zaman/>Kerem Zaman</a>
|
<a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/s/shashank-srivastava/>Shashank Srivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1045><div class="card-body p-3 small">Model fusion research aims to aggregate the knowledge of multiple individual models to enhance performance by combining their weights. In this work, we study the inverse problem: investigating whether model fusion can be used to reduce unwanted knowledge. We investigate the effects of model fusion in three scenarios: the learning of shortcuts, social biases, and memorization of training data in fine-tuned language models. Through experiments covering classification and generation tasks, our analysis highlights that shared knowledge among models is enhanced during model fusion, while unshared knowledge is usually forgotten. Based on this observation, we demonstrate the potential of model fusion as a debiasing tool and showcase its efficacy in addressing privacy concerns associated with language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1046.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1046.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1046/>Collective Critics for Creative Story Generation</a></strong><br><a href=/people/m/minwook-bae/>Minwook Bae</a>
|
<a href=/people/h/hyounghun-kim/>Hyounghun Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1046><div class="card-body p-3 small">Generating a long story of several thousand words with narrative coherence using Large Language Models (LLMs) has been a challenging task. Previous research has addressed this challenge by proposing different frameworks that create a story plan and generate a long story based on that plan. However, these frameworks have been mainly focusing on maintaining narrative coherence in stories, often overlooking creativity in story planning and the expressiveness of the stories generated from those plans, which are desirable properties to captivate readers’ interest. In this paper, we propose Collective Critics for Creative Story Generation framework (CritiCS), which is composed of plan refining stage (CrPlan) and story generation stage (CrText), to integrate a collective revision mechanism that promotes those properties into long-form story generation process. Specifically, in each stage, a group of LLM critics and one leader collaborate to incrementally refine drafts of plan and story throughout multiple rounds. Extensive human evaluation shows that the CritiCS can significantly enhance story creativity and reader engagement, while also maintaining narrative coherence. Furthermore, the design of the framework allows active participation from human writers in any role within the critique process, enabling interactive human-machine collaboration in story writing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1047.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1047 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1047/>Surprise! <span class=acl-fixed-case>U</span>niform <span class=acl-fixed-case>I</span>nformation <span class=acl-fixed-case>D</span>ensity Isn’t the Whole Story: Predicting Surprisal Contours in Long-form Discourse</a></strong><br><a href=/people/e/eleftheria-tsipidi/>Eleftheria Tsipidi</a>
|
<a href=/people/f/franz-nowak/>Franz Nowak</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/e/ethan-wilcox/>Ethan Wilcox</a>
|
<a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>
|
<a href=/people/a/alex-warstadt/>Alex Warstadt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1047><div class="card-body p-3 small">The Uniform Information Density (UID) hypothesis posits that speakers tend to distribute information evenly across linguistic units to achieve efficient communication. Of course, information rate in texts and discourses is not perfectly uniform. While these fluctuations can be viewed as theoretically uninteresting noise on top of a uniform target, another explanation is that UID is not the only functional pressure regulating information content in a language. Speakers may also seek to maintain interest, adhere to writing conventions, and build compelling arguments. In this paper, we propose one such functional pressure; namely that speakers modulate information rate based on location within a hierarchically-structured model of discourse. We term this the Structured Context Hypothesis and test it by predicting the surprisal contours of naturally occurring discourses extracted from large language models using predictors derived from discourse structure. We find that hierarchical predictors are significant predictors of a discourse’s information contour and that deeply nested hierarchical predictors are more predictive than shallow ones. This work takes an initial step beyond UID to propose testable hypotheses for why the information rate fluctuates in predictable ways.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1048.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1048/>Model-based Preference Optimization in Abstractive Summarization without Human Feedback</a></strong><br><a href=/people/j/jaepill-choi/>Jaepill Choi</a>
|
<a href=/people/k/kyubyung-chae/>Kyubyung Chae</a>
|
<a href=/people/j/jiwoo-song/>Jiwoo Song</a>
|
<a href=/people/y/yohan-jo/>Yohan Jo</a>
|
<a href=/people/t/taesup-kim/>Taesup Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1048><div class="card-body p-3 small">In abstractive summarization, the challenge of producing concise and accurate summaries arises from the vast amount of information contained in the source document. Consequently, although Large Language Models (LLMs) can generate fluent text, they often introduce inaccuracies by hallucinating content not found in the original source. While supervised fine-tuning methods that maximize likelihood contribute to this issue, they do not consistently enhance the faithfulness of the summaries. Preference-based optimization methods, such as Direct Preference Optimization (DPO), can further refine the model to align with human preferences. However, these methods still heavily depend on costly human feedback. In this work, we introduce a novel and straightforward approach called Model-based Preference Optimization (MPO) to fine-tune LLMs for improved summarization abilities without any human feedback. By leveraging the model’s inherent summarization capabilities, we create a preference dataset that is fully generated by the model using different decoding strategies. Our experiments on standard summarization datasets and various metrics demonstrate that our proposed MPO significantly enhances the quality of generated summaries without relying on human feedback. The code is publicly available at <a href=https://github.com/cjaep/MPO class=acl-markup-url>https://github.com/cjaep/MPO</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1049.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1049/>Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?</a></strong><br><a href=/people/w/wataru-hashimoto/>Wataru Hashimoto</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1049><div class="card-body p-3 small">This work investigates the impact of data augmentation on confidence calibration and uncertainty estimation in Named Entity Recognition (NER) tasks. For the future advance of NER in safety-critical fields like healthcare and finance, it is essential to achieve accurate predictions with calibrated confidence when applying Deep Neural Networks (DNNs), including Pre-trained Language Models (PLMs), as a real-world application. However, DNNs are prone to miscalibration, which limits their applicability. Moreover, existing methods for calibration and uncertainty estimation are computational expensive. Our investigation in NER found that data augmentation improves calibration and uncertainty in cross-genre and cross-lingual setting, especially in-domain setting. Furthermore, we showed that the calibration for NER tends to be more effective when the perplexity of the sentences generated by data augmentation is lower, and that increasing the size of the augmentation further improves calibration and uncertainty.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1050.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1050/><span class=acl-fixed-case>N</span>euro<span class=acl-fixed-case>T</span>rial<span class=acl-fixed-case>NER</span>: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries</a></strong><br><a href=/people/s/simona-emilova-doneva/>Simona Emilova Doneva</a>
|
<a href=/people/t/tilia-ellendorff/>Tilia Ellendorff</a>
|
<a href=/people/b/beate-sick/>Beate Sick</a>
|
<a href=/people/j/jean-philippe-goldman/>Jean-Philippe Goldman</a>
|
<a href=/people/a/amelia-elaine-cannon/>Amelia Elaine Cannon</a>
|
<a href=/people/g/gerold-schneider/>Gerold Schneider</a>
|
<a href=/people/b/benjamin-victor-ineichen/>Benjamin Victor Ineichen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1050><div class="card-body p-3 small">Extracting and aggregating information from clinical trial registries could provide invaluable insights into the drug development landscape and advance the treatment of neurologic diseases. However, achieving this at scale is hampered by the volume of available data and the lack of an annotated corpus to assist in the development of automation tools. Thus, we introduce NeuroTrialNER, a new and fully open corpus for named entity recognition (NER). It comprises 1093 clinical trial summaries sourced from ClinicalTrials.gov, annotated for neurological diseases, therapeutic interventions, and control treatments. We describe our data collection process and the corpus in detail. We demonstrate its utility for NER using large language models and achieve a close-to-human performance. By bridging the gap in data resources, we hope to foster the development of text processing tools that help researchers navigate clinical trials data more easily.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1051.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1051/>Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting</a></strong><br><a href=/people/m/maxime-guillaume-kayser/>Maxime Guillaume Kayser</a>
|
<a href=/people/b/bayar-menzat/>Bayar Menzat</a>
|
<a href=/people/c/cornelius-emde/>Cornelius Emde</a>
|
<a href=/people/b/bogdan-alexandru-bercean/>Bogdan Alexandru Bercean</a>
|
<a href=/people/a/alex-novak/>Alex Novak</a>
|
<a href=/people/a/abdala-trinidad-espinosa-morgado/>Abdalá Trinidad Espinosa Morgado</a>
|
<a href=/people/b/bartlomiej-papiez/>Bartlomiej Papiez</a>
|
<a href=/people/s/susanne-gaube/>Susanne Gaube</a>
|
<a href=/people/t/thomas-lukasiewicz/>Thomas Lukasiewicz</a>
|
<a href=/people/o/oana-maria-camburu/>Oana-Maria Camburu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1051><div class="card-body p-3 small">The growing capabilities of AI models are leading to their wider use, including in safety-critical domains. Explainable AI (XAI) aims to make these models safer to use by making their inference process more transparent. However, current explainability methods are seldom evaluated in the way they are intended to be used: by real-world end users. To address this, we conducted a large-scale user study with 85 healthcare practitioners in the context of human-AI collaborative chest X-ray analysis. We evaluated three types of explanations: visual explanations (saliency maps), natural language explanations, and a combination of both modalities. We specifically examined how different explanation types influence users depending on whether the AI advice and explanations are factually correct. We find that text-based explanations lead to significant over-reliance, which is alleviated by combining them with saliency maps. We also observe that the quality of explanations, that is, how much factually correct information they entail, and how much this aligns with AI correctness, significantly impacts the usefulness of the different explanation types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1052.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1052/>Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering</a></strong><br><a href=/people/w/weihe-zhai/>Weihe Zhai</a>
|
<a href=/people/a/arkaitz-zubiaga/>Arkaitz Zubiaga</a>
|
<a href=/people/b/bingquan-liu/>Bingquan Liu</a>
|
<a href=/people/c/cheng-jie-sun/>Chengjie Sun</a>
|
<a href=/people/y/yalong-zhao/>Yalong Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1052><div class="card-body p-3 small">The fusion of language models (LMs) and knowledge graphs (KGs) is widely used in commonsense question answering, but generating faithful explanations remains challenging. Current methods often overlook path decoding faithfulness, leading to divergence between graph encoder outputs and model predictions. We identify confounding effects and LM-KG misalignment as key factors causing spurious explanations. To address this, we introduce the LM-KG Fidelity metric to assess KG representation reliability and propose the LM-KG Distribution-aware Alignment (LKDA) algorithm to improve explanation faithfulness. Without ground truth, we evaluate KG explanations using the proposed Fidelity-Sparsity Trade-off Curve. Experiments on CommonsenseQA and OpenBookQA show that LKDA significantly enhances explanation fidelity and model performance, highlighting the need to address distributional misalignment for reliable commonsense reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1053.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1053/>Generation with Dynamic Vocabulary</a></strong><br><a href=/people/y/yanting-liu/>Yanting Liu</a>
|
<a href=/people/t/tao-ji/>Tao Ji</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/x/xiaoling-wang/>Xiaoling Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1053><div class="card-body p-3 small">We introduce a new dynamic vocabulary for language models. It can involve arbitrary text spans during generation. These text spans act as basic generation bricks, akin to tokens in the traditional static vocabularies. We show that, the ability to generate multi-tokens atomically improve both generation quality and efficiency (compared to the standard language model, the MAUVE metric is increased by 25%, the latency is decreased by 20 %). The dynamic vocabulary can be deployed in a plug-and-play way, thus is attractive for various downstream applications. For example, we demonstrate that dynamic vocabulary can be applied to different domains in a training-free manner. It also helps to generate reliable citations in question answering tasks (substantially enhancing citation results without compromising answer accuracy).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1054.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1054/>Argument Relation Classification through Discourse Markers and Adversarial Training</a></strong><br><a href=/people/m/michele-luca-contalbo/>Michele Luca Contalbo</a>
|
<a href=/people/f/francesco-guerra/>Francesco Guerra</a>
|
<a href=/people/m/matteo-paganelli/>Matteo Paganelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1054><div class="card-body p-3 small">Argument relation classification (ARC) identifies supportive, contrasting and neutral relations between argumentative units. The current approaches rely on transformer architectures which have proven to be more effective than traditional methods based on hand-crafted linguistic features. In this paper, we introduce DISARM, which advances the state of the art with a training procedure combining multi-task and adversarial learning strategies. By jointly solving the ARC and discourse marker detection tasks and aligning their embedding spaces into a unified latent space, DISARM outperforms the accuracy of existing approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1055.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1055/>Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection</a></strong><br><a href=/people/a/abhishek-purushothama/>Abhishek Purushothama</a>
|
<a href=/people/a/adam-wiemerslage/>Adam Wiemerslage</a>
|
<a href=/people/k/katharina-von-der-wense/>Katharina Von Der Wense</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1055><div class="card-body p-3 small">Pre-trained transformers such as BERT have been shown to be effective in many natural language tasks. However, they are under-explored for character-level sequence to sequence tasks. In this work, we investigate pre-training transformers for the character-level task of morphological inflection in several languages. We compare various training setups and secondary tasks where unsupervised data taken directly from the target task is used. We show that training on secondary unsupervised tasks increases inflection performance even without any external data, suggesting that models learn from additional unsupervised tasks themselves—not just from additional data. We also find that this does not hold true for specific combinations of secondary task and training setup, which has interesting implications for denoising objectives in character-level tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1056.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1056.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1056/>Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval</a></strong><br><a href=/people/d/dae-yon-hwang/>Dae Yon Hwang</a>
|
<a href=/people/b/bilal-taha/>Bilal Taha</a>
|
<a href=/people/h/harshit-pande/>Harshit Pande</a>
|
<a href=/people/y/yaroslav-nechaev/>Yaroslav Nechaev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1056><div class="card-body p-3 small">Despite the recent advancements in information retrieval (IR), zero-shot IR remains a significant challenge, especially when dealing with new domains, languages, and newly-released use cases that lack historical query traffic from existing users. For such cases, it is common to use query augmentations followed by fine-tuning pre-trained models on the document data paired with synthetic queries. In this work, we propose a novel Universal Document Linking (UDL) algorithm, which links similar documents to enhance synthetic query generation across multiple datasets with different characteristics. UDL leverages entropy for the choice of similarity models and named entity recognition (NER) for the link decision of documents using similarity scores. Our empirical studies demonstrate the effectiveness and universality of the UDL across diverse datasets and IR models, surpassing state-of-the-art methods in zero-shot cases. The developed code for reproducibility is included in https://github.com/eoduself/UDL</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1057.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1057/>Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models</a></strong><br><a href=/people/p/po-heng-chen/>Po-Heng Chen</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1057><div class="card-body p-3 small">Multilingual pre-trained language models (mPLMs) have demonstrated notable effectiveness in zero-shot cross-lingual transfer tasks. Specifically, they can be fine-tuned solely on tasks in the source language and subsequently applied to tasks in the target language. However, for low-resource languages unseen during pre-training, relying solely on zero-shot language transfer often yields sub-optimal results. One common strategy is to continue training PLMs using masked language modeling objectives on the target language. Nonetheless, this approach can be inefficient due to the need to adjust all parameters for language adaptation. In this paper, we propose a more efficient solution: soft-prompt tuning for language adaptation. Our experiments demonstrate that with carefully designed prompts, soft-prompt tuning enables mPLMs to achieve effective zero-shot cross-lingual transfer to downstream tasks in previously unseen languages. Notably, we found that prompt tuning outperforms continuously trained baselines on two text classification benchmarks, encompassing 20 low-resource languages while utilizing a mere 0.28% of the tuned parameters. These results underscore the superior adaptability of mPLMs to previously unseen languages afforded by soft-prompt tuning compared to traditional fine-tuning methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1058.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1058/>Prove Your Point!: Bringing Proof-Enhancement Principles to Argumentative Essay Generation</a></strong><br><a href=/people/r/ruiyu-xiao/>Ruiyu Xiao</a>
|
<a href=/people/l/lei-wu/>Lei Wu</a>
|
<a href=/people/y/yuhang-gou/>Yuhang Gou</a>
|
<a href=/people/w/weinan-zhang/>Weinan Zhang</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1058><div class="card-body p-3 small">Argumentative essay generation (AEG) aims to generate complete texts on specific controversial topics or debates. Although current AEG methods can generate individual opinions, they often overlook the high-level connections between these opinions. This often leads to the generated results being mired in logical confusion, unable to proof their own arguments effectively. The generated essay may present evidence that contradicts the claims or they may fail to assemble the claims into logical flow. In this paper, we present a unified two-stage framework: Proof-Enhancement and Self-Annotation (PESA) for AEG with a focus on logical enhancement. Specifically, we first construct pseudo-labels for logical information,claims and grounds, using a large language model. We then propose a tree planning approach that introduces proof principles and ensures logical consistency. Extensive experimental results show that, benefiting from proof principle guidance, PESA generates argumentative essays with better logical validity and persuasiveness than strong baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1059.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1059/><span class=acl-fixed-case>TV</span>-<span class=acl-fixed-case>TREES</span>: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning</a></strong><br><a href=/people/k/kate-sanders/>Kate Sanders</a>
|
<a href=/people/n/nathaniel-weir/>Nathaniel Weir</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1059><div class="card-body p-3 small">It is challenging for models to understand complex, multimodal content such as television clips, and this is in part because video-language models often rely on single-modality reasoning and lack interpretability. To combat these issues we propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by searching for trees of entailment relationships between simple text-video evidence and higher-level conclusions that prove question-answer pairs. We also introduce the task of multimodal entailment tree generation to evaluate reasoning quality. Our method’s performance on the challenging TVQA benchmark demonstrates interpretable, state-of-the-art zero-shot performance on full clips, illustrating that multimodal entailment tree generation can be a best-of-both-worlds alternative to black-box systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1060.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1060/>Unsupervised Extraction of Dialogue Policies from Conversations</a></strong><br><a href=/people/m/makesh-narsimhan-sreedhar/>Makesh Narsimhan Sreedhar</a>
|
<a href=/people/t/traian-rebedea/>Traian Rebedea</a>
|
<a href=/people/c/christopher-parisien/>Christopher Parisien</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1060><div class="card-body p-3 small">Dialogue policies play a crucial role in developing task-oriented dialogue systems, yet their development and maintenance are challenging and typically require substantial effort from experts in dialogue modeling. While in many situations, large amounts of conversational data are available for the task at hand, people lack an effective solution able to extract dialogue policies from this data. In this paper, we address this gap by first illustrating how Large Language Models (LLMs) can be instrumental in extracting dialogue policies from datasets, through the conversion of conversations into a unified intermediate representation consisting of canonical forms. We then propose a novel method for generating dialogue policies utilizing a controllable and interpretable graph-based methodology. By combining canonical forms across conversations into a flow network, we find that running graph traversal algorithms helps in extracting dialogue flows. These flows are a better representation of the underlying interactions than flows extracted by prompting LLMs. Our technique focuses on giving conversation designers greater control, offering a productivity tool to improve the process of developing dialogue policies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1061.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1061/><span class=acl-fixed-case>GRIZAL</span>: Generative Prior-guided Zero-Shot Temporal Action Localization</a></strong><br><a href=/people/o/onkar-kishor-susladkar/>Onkar Kishor Susladkar</a>
|
<a href=/people/g/gayatri-sudhir-deshmukh/>Gayatri Sudhir Deshmukh</a>
|
<a href=/people/v/vandan-gorade/>Vandan Gorade</a>
|
<a href=/people/s/sparsh-mittal/>Sparsh Mittal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1061><div class="card-body p-3 small">Zero-shot temporal action localization (TAL) aims to temporally localize actions in videos without prior training examples. To address the challenges of TAL, we offer GRIZAL, a model that uses multimodal embeddings and dynamic motion cues to localize actions effectively. GRIZAL achieves sample diversity by using large-scale generative models such as GPT-4 for generating textual augmentations and DALL-E for generating image augmentations. Our model integrates vision-language embeddings with optical flow insights, optimized through a blend of supervised and self-supervised loss functions. On ActivityNet, Thumos14 and Charades-STA datasets, GRIZAL greatly outperforms state-of-the-art zero-shot TAL models, demonstrating its robustness and adaptability across a wide range of video content. We will make all the models and code publicly available by open-sourcing them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1062.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1062/>Preserving Multi-Modal Capabilities of Pre-trained <span class=acl-fixed-case>VLM</span>s for Improving Vision-Linguistic Compositionality</a></strong><br><a href=/people/y/youngtaek-oh/>Youngtaek Oh</a>
|
<a href=/people/j/jae-won-cho/>Jae Won Cho</a>
|
<a href=/people/d/dong-jin-kim/>Dong-Jin Kim</a>
|
<a href=/people/i/in-so-kweon/>In So Kweon</a>
|
<a href=/people/j/junmo-kim/>Junmo Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1062><div class="card-body p-3 small">In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the model’s multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the model’s representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities. Code is available at: https://github.com/ytaek-oh/fsc-clip.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1063.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1063/><span class=acl-fixed-case>F</span>oodie<span class=acl-fixed-case>QA</span>: A Multimodal Dataset for Fine-Grained Understanding of <span class=acl-fixed-case>C</span>hinese Food Culture</a></strong><br><a href=/people/w/wenyan-li/>Wenyan Li</a>
|
<a href=/people/c/crystina-zhang/>Crystina Zhang</a>
|
<a href=/people/j/jiaang-li/>Jiaang Li</a>
|
<a href=/people/q/qiwei-peng/>Qiwei Peng</a>
|
<a href=/people/r/raphael-tang/>Raphael Tang</a>
|
<a href=/people/l/li-zhou/>Li Zhou</a>
|
<a href=/people/w/weijia-zhang/>Weijia Zhang</a>
|
<a href=/people/g/guimin-hu/>Guimin Hu</a>
|
<a href=/people/y/yifei-yuan/>Yifei Yuan</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1063><div class="card-body p-3 small">Food is a rich and varied dimension of cultural heritage, crucial to both individuals and social groups. To bridge the gap in the literature on the often-overlooked regional diversity in this domain, we introduce FoodieQA, a manually curated, fine-grained image-text dataset capturing the intricate features of food cultures across various regions in China. We evaluate vision–language Models (VLMs) and large language models (LLMs) on newly collected, unseen food images and corresponding questions. FoodieQA comprises three multiple-choice question-answering tasks where models need to answer questions based on multiple images, a single image, and text-only descriptions, respectively. While LLMs excel at text-based question answering, surpassing human accuracy, the open-sourced VLMs still fall short by 41% on multi-image and 21% on single-image VQA tasks, although closed-weights models perform closer to human levels (within 10%). Our findings highlight that understanding food and its cultural implications remains a challenging and under-explored direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1064.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1064/>A Two-Step Approach for Data-Efficient <span class=acl-fixed-case>F</span>rench Pronunciation Learning</a></strong><br><a href=/people/h/hoyeon-lee/>Hoyeon Lee</a>
|
<a href=/people/h/hyeeun-jang/>Hyeeun Jang</a>
|
<a href=/people/j/jonghwan-kim/>Jonghwan Kim</a>
|
<a href=/people/j/jaemin-kim/>Jaemin Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1064><div class="card-body p-3 small">Recent studies have addressed intricate phonological phenomena in French, relying on either extensive linguistic knowledge or a significant amount of sentence-level pronunciation data. However, creating such resources is expensive and non-trivial. To this end, we propose a novel two-step approach that encompasses two pronunciation tasks: grapheme-to-phoneme and post-lexical processing. We then investigate the efficacy of the proposed approach with a notably limited amount of sentence-level pronunciation data. Our findings demonstrate that the proposed two-step approach effectively mitigates the lack of extensive labeled data, and serves as a feasible solution for addressing French phonological phenomena even under resource-constrained environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1065.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1065 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1065/>Exploring Intra and Inter-language Consistency in Embeddings with <span class=acl-fixed-case>ICA</span></a></strong><br><a href=/people/r/rongzhi-li/>Rongzhi Li</a>
|
<a href=/people/t/takeru-matsuda/>Takeru Matsuda</a>
|
<a href=/people/h/hitomi-yanaka/>Hitomi Yanaka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1065><div class="card-body p-3 small">Word embeddings represent words as multidimensional real vectors, facilitating data analysis and processing, but are often challenging to interpret. Independent Component Analysis (ICA) creates clearer semantic axes by identifying independent key features. Previous research has shown ICA’s potential to reveal universal semantic axes across languages. However, it lacked verification of the consistency of independent components within and across languages. We investigated the consistency of semantic axes in two ways: both within a single language and across multiple languages. We first probed into intra-language consistency, focusing on the reproducibility of axes by performing ICA multiple times and clustering the outcomes. Then, we statistically examined inter-language consistency by verifying those axes’ correspondences using statistical tests. We newly applied statistical methods to establish a robust framework that ensures the reliability and universality of semantic axes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1066.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1066 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1066/><span class=acl-fixed-case>D</span>etox<span class=acl-fixed-case>LLM</span>: A Framework for Detoxification with Explanations</a></strong><br><a href=/people/m/md-tawkat-islam-khondaker/>Md Tawkat Islam Khondaker</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/l/laks-v-s-lakshmanan/>Laks V. S. Lakshmanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1066><div class="card-body p-3 small">Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose DetoxLLM, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus. We further introduce explanation to promote transparency and trustworthiness. DetoxLLM additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases. Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of DetoxLLM against adversarial toxicity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1067.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1067.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1067.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1067/>Comparing a <span class=acl-fixed-case>BERT</span> Classifier and a <span class=acl-fixed-case>GPT</span> classifier for Detecting Connective Language Across Multiple Social Media</a></strong><br><a href=/people/j/josephine-lukito/>Josephine Lukito</a>
|
<a href=/people/b/bin-chen/>Bin Chen</a>
|
<a href=/people/g/gina-m-masullo/>Gina M. Masullo</a>
|
<a href=/people/n/natalie-jomini-stroud/>Natalie Jomini Stroud</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1067><div class="card-body p-3 small">This study presents an approach for detecting connective language—defined as language that facilitates engagement, understanding, and conversation—from social media discussions. We developed and evaluated two types of classifiers: BERT and GPT-3.5 turbo. Our results demonstrate that the BERT classifier significantly outperforms GPT-3.5 turbo in detecting connective language. Furthermore, our analysis confirms that connective language is distinct from related concepts measuring discourse qualities, such as politeness and toxicity. We also explore the potential of BERT-based classifiers for platform-agnostic tools. This research advances our understanding of the linguistic dimensions of online communication and proposes practical tools for detecting connective language across diverse digital environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1068.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1068/><span class=acl-fixed-case>S</span>hadow<span class=acl-fixed-case>LLM</span>: Predictor-based Contextual Sparsity for Large Language Models</a></strong><br><a href=/people/y/yash-akhauri/>Yash Akhauri</a>
|
<a href=/people/a/ahmed-f-abouelhamayed/>Ahmed F AbouElhamayed</a>
|
<a href=/people/j/jordan-dotzel/>Jordan Dotzel</a>
|
<a href=/people/z/zhiru-zhang/>Zhiru Zhang</a>
|
<a href=/people/a/alexander-m-rush/>Alexander M Rush</a>
|
<a href=/people/s/safeen-huda/>Safeen Huda</a>
|
<a href=/people/m/mohamed-s-abdelfattah/>Mohamed S. Abdelfattah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1068><div class="card-body p-3 small">The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated efficiency techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy. Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We develop a novel predictor called ShadowLLM, which can shadow the LLM behavior and enforce better sparsity patterns, resulting in over 15% improvement in end-to-end accuracy compared to prior methods. In addition, ShadowLLM achieves up to a 20% speed-up over the state-of-the-art DejaVu framework. These enhancements are validated on Llama-2 and OPT models with up to 30 billion parameters. Our code is available at https://github.com/abdelfattah-lab/shadow_llm/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1069.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1069 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1069/>Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health</a></strong><br><a href=/people/k/krishnapriya-vishnubhotla/>Krishnapriya Vishnubhotla</a>
|
<a href=/people/d/daniela-teodorescu/>Daniela Teodorescu</a>
|
<a href=/people/m/mallory-j-feldman/>Mallory J Feldman</a>
|
<a href=/people/k/kristen-lindquist/>Kristen Lindquist</a>
|
<a href=/people/s/saif-mohammad/>Saif M. Mohammad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1069><div class="card-body p-3 small">We are united in how emotions are central to shaping our experiences; yet, individuals differ greatly in how we each identify, categorize, and express emotions. In psychology, variation in the ability of individuals to differentiate between emotion concepts is called emotion granularity (determined through self-reports of one’s emotions). High emotion granularity has been linked with better mental and physical health; whereas low emotion granularity has been linked with maladaptive emotion regulation strategies and poor health outcomes. In this work, we propose computational measures of emotion granularity derived from temporally-ordered speaker utterances in social media (in lieu of self reports that suffer from various biases). We then investigate the effectiveness of such text-derived measures of emotion granularity in functioning as markers of various mental health conditions (MHCs). We establish baseline measures of emotion granularity derived from textual utterances, and show that, at an aggregate level, emotion granularities are significantly lower for people self-reporting as having an MHC than for the control population. This paves the way towards a better understanding of the MHCs, and specifically the role emotions play in our well-being.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1070.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1070 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1070/><span class=acl-fixed-case>BLSP</span>-Emo: Towards Empathetic Large Speech-Language Models</a></strong><br><a href=/people/c/chen-wang/>Chen Wang</a>
|
<a href=/people/m/minpeng-liao/>Minpeng Liao</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/j/junhong-wu/>Junhong Wu</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1070><div class="card-body p-3 small">The recent release of GPT-4o showcased the potential of end-to-end multimodal models, not just in terms of low latency but also in their ability to understand and generate expressive speech with rich emotions. While the details are unknown to the open research community, it likely involves significant amounts of curated data and compute, neither of which is readily accessible. In this paper, we present BLSP-Emo (Bootstrapped Language-Speech Pretraining with Emotion support), a novel approach to developing an end-to-end speech-language model capable of understanding both semantics and emotions in speech and generate empathetic responses. BLSP-Emo utilizes existing speech recognition (ASR) and speech emotion recognition (SER) datasets through a two-stage process. The first stage focuses on semantic alignment, following recent work on pretraining speech-language models using ASR data. The second stage performs emotion alignment with the pretrained speech-language model on an emotion-aware continuation task constructed from SER data. Our experiments demonstrate that the BLSP-Emo model excels in comprehending speech and delivering empathetic responses, both in instruction-following tasks and conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1071.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1071.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1071/><span class=acl-fixed-case>S</span>ynthesiz<span class=acl-fixed-case>RR</span>: Generating Diverse Datasets with Retrieval Augmentation</a></strong><br><a href=/people/a/abhishek-divekar/>Abhishek Divekar</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1071><div class="card-body p-3 small">It is often desirable to distill the capabilities of large language models (LLMs) into smaller student models due to compute and memory constraints. One way to do this for classification tasks is via dataset synthesis, which can be accomplished by generating examples of each label from the LLM. Prior approaches to synthesis use few-shot prompting, which relies on the LLM’s parametric knowledge to generate usable examples. However, this leads to issues of repetition, bias towards popular entities, and stylistic differences from human text. In this work, we propose Synthesize by Retrieval and Refinement (SynthesizRR), which uses retrieval augmentation to introduce variety into the dataset synthesis process: as retrieved passages vary, the LLM is seeded with different content to generate its examples. We empirically study the synthesis of six datasets, covering topic classification, sentiment analysis, tone detection, and humor, requiring complex synthesis strategies. We find SynthesizRR greatly improves lexical and semantic diversity, similarity to human-written text, and distillation performance, when compared to 32-shot prompting and four prior approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1072.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1072.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1072/>Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model</a></strong><br><a href=/people/w/wenqi-zhang/>Wenqi Zhang</a>
|
<a href=/people/z/zhenglin-cheng/>Zhenglin Cheng</a>
|
<a href=/people/y/yuanyu-he/>Yuanyu He</a>
|
<a href=/people/m/mengna-wang/>Mengna Wang</a>
|
<a href=/people/y/yongliang-shen/>Yongliang Shen</a>
|
<a href=/people/z/zeqi-tan/>Zeqi Tan</a>
|
<a href=/people/g/guiyang-hou/>Guiyang Hou</a>
|
<a href=/people/m/mingqian-he/>Mingqian He</a>
|
<a href=/people/y/yanna-ma/>Yanna Ma</a>
|
<a href=/people/w/weiming-lu/>Weiming Lu</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1072><div class="card-body p-3 small">Although most current large multimodal models (LMMs) can already understand photos of natural scenes and portraits, their understanding of abstract images, e.g., charts, maps, or layouts, and visual reasoning capabilities remains quite rudimentary. They often struggle with simple daily tasks, such as reading time from a clock, understanding a flowchart, or planning a route using a road map. In light of this, we design a multi-modal self-instruct, utilizing large language models and their code capabilities to synthesize massive abstract images and visual reasoning instructions across daily scenarios. Our strategy effortlessly creates a multimodal benchmark with 11,193 instructions for eight visual scenarios: charts, tables, simulated maps, dashboards, flowcharts, relation graphs, floor plans, and visual puzzles. <b>This benchmark, constructed with simple lines and geometric elements, exposes the shortcomings of most advanced LMMs</b> like GPT-4V and Llava in abstract image understanding, spatial relations reasoning, and visual element induction. Besides, to verify the quality of our synthetic data, we fine-tune an LMM using 62,476 synthetic chart, table and road map instructions. The results demonstrate improved chart understanding and map navigation performance, and also demonstrate potential benefits for other visual reasoning tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1073.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1073/><span class=acl-fixed-case>D</span>ata<span class=acl-fixed-case>N</span>arrative: Automated Data-Driven Storytelling with Visualizations and Texts</a></strong><br><a href=/people/m/mohammed-saidul-islam/>Mohammed Saidul Islam</a>
|
<a href=/people/m/md-tahmid-rahman-laskar/>Md Tahmid Rahman Laskar</a>
|
<a href=/people/m/md-rizwan-parvez/>Md Rizwan Parvez</a>
|
<a href=/people/e/enamul-hoque/>Enamul Hoque</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1073><div class="card-body p-3 small">Data-driven storytelling is a powerful method for conveying insights by combining narrative techniques with visualizations and text. These stories integrate visual aids, such as highlighted bars and lines in charts, along with textual annotations explaining insights. However, creating such stories requires a deep understanding of the data and meticulous narrative planning, often necessitating human intervention, which can be time-consuming and mentally taxing. While Large Language Models (LLMs) excel in various NLP tasks, their ability to generate coherent and comprehensive data stories remains underexplored. In this work, we introduce a novel task for data story generation and a benchmark containing 1,449 stories from diverse sources. To address the challenges of crafting coherent data stories, we propose a multi-agent framework employing two LLM agents designed to replicate the human storytelling process: one for understanding and describing the data (Reflection), generating the outline, and narration, and another for verification at each intermediary step. While our agentic framework generally outperforms non-agentic counterparts in both model-based and human evaluations, the results also reveal unique challenges in data story generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1074.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1074/><span class=acl-fixed-case>DEM</span>: Distribution Edited Model for Training with Mixed Data Distributions</a></strong><br><a href=/people/d/dhananjay-ram/>Dhananjay Ram</a>
|
<a href=/people/a/aditya-rawal/>Aditya Rawal</a>
|
<a href=/people/m/momchil-hardalov/>Momchil Hardalov</a>
|
<a href=/people/n/nikolaos-pappas/>Nikolaos Pappas</a>
|
<a href=/people/s/sheng-zha/>Sheng Zha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1074><div class="card-body p-3 small">Training with mixed data distributions is a common and important part of creating multi-task and instruction-following models. The diversity of the data distributions and cost of joint training makes the optimization procedure extremely challenging. Data mixing methods partially address this problem, albeit having a sub-optimal performance across data sources and require multiple expensive training runs. In this paper, we propose a simple and efficient alternative for better optimization of the data sources by combining models individually trained on each data source with the base model using basic element-wise vector operations. The resulting model, namely Distribution Edited Model (DEM), is cheaper than standard data mixing and outperforms strong baselines on a variety of benchmarks, yielding upto 6.2% improvement on MMLU, 11.5% on BBH, 16.1% on DROP, 6% MathQA and 9.3% on HELM with models of size 3B to 13B. Notably, DEM does not require full re-training when modifying a single data-source, thus making it very flexible and scalable for training with diverse data sources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1075.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1075.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1075 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1075 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1075/>Altogether: Image Captioning via Re-aligning Alt-text</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/p/po-yao-huang/>Po-Yao Huang</a>
|
<a href=/people/x/xiaoqing-tan/>Xiaoqing Tan</a>
|
<a href=/people/c/ching-feng-yeh/>Ching-Feng Yeh</a>
|
<a href=/people/j/jacob-kahn/>Jacob Kahn</a>
|
<a href=/people/c/christine-jou/>Christine Jou</a>
|
<a href=/people/g/gargi-ghosh/>Gargi Ghosh</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a>
|
<a href=/people/s/shang-wen-li/>Shang-Wen Li</a>
|
<a href=/people/s/saining-xie/>Saining Xie</a>
|
<a href=/people/c/christoph-feichtenhofer/>Christoph Feichtenhofer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1075><div class="card-body p-3 small">This paper focuses on creating synthetic data to improve the quality of image captions. Existing works typically have two shortcomings. First, they caption images from scratch, ignoring existing alt-text metadata, and second, lack transparency if the captioners’ training data (e.g. GPT) is unknown. In this paper, we study a principled approach Altogether based on the key idea to edit and re-align existing alt-texts associated with the images. To generate training data, we perform human annotation where annotators start with the existing alt-text and re-align it to the image content in multiple rounds, consequently constructing captions with rich visual concepts. This differs from prior work that carries out human annotation as a one-time description task solely based on images and annotator knowledge. We train a captioner on this data that generalizes the process of re-aligning alt-texts at scale. Our results show our Altogether approach leads to richer image captions that also improve text-to-image generation and zero-shot image classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1076.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1076.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1076/><span class=acl-fixed-case>V</span>erify<span class=acl-fixed-case>M</span>atch: A Semi-Supervised Learning Paradigm for Natural Language Inference with Confidence-Aware <span class=acl-fixed-case>M</span>ix<span class=acl-fixed-case>U</span>p</a></strong><br><a href=/people/s/seo-yeon-park/>Seo Yeon Park</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1076><div class="card-body p-3 small">While natural language inference (NLI) has emerged as a prominent task for evaluating a model’s capability to perform natural language understanding, creating large benchmarks for training deep learning models imposes a significant challenge since it requires extensive human annotations. To overcome this, we propose to construct pseudo-generated samples (premise-hypothesis pairs) using class-specific fine-tuned large language models (LLMs) thereby reducing the human effort and the costs in annotating large amounts of data. However, despite the impressive performance of LLMs, it is necessary to verify that the pseudo-generated labels are actually correct. Towards this goal, in this paper, we propose VerifyMatch, a semi-supervised learning (SSL) approach in which the LLM pseudo-labels guide the training of the SSL model and, at the same time, the SSL model acts as a verifier of the LLM-generated data. In our approach, we retain all pseudo-labeled samples, but to ensure unlabeled data quality, we further propose to use MixUp whenever the verifier does not agree with the LLM-generated label or when they both agree on the label but the verifier has a low confidence—lower than an adaptive confidence threshold. We achieve competitive accuracy compared to strong baselines for NLI datasets in low-resource settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1077.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1077.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1077.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1077/><span class=acl-fixed-case>C</span>a<span class=acl-fixed-case>T</span>-Bench: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans</a></strong><br><a href=/people/y/yash-kumar-lal/>Yash Kumar Lal</a>
|
<a href=/people/v/vanya-cohen/>Vanya Cohen</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a>
|
<a href=/people/r/ray-mooney/>Ray Mooney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1077><div class="card-body p-3 small">Understanding the abilities of LLMs to reason about natural language plans, such as instructional text and recipes, is critical to reliably using them in decision-making systems. A fundamental aspect of plans is the temporal order in which their steps need to be executed, which reflects the underlying causal dependencies between them. We introduce CaT-Bench, a benchmark of Step Order Prediction questions, which test whether a step must necessarily occur before or after another in cooking recipe plans. We use this to evaluate how well frontier LLMs understand causal and temporal dependencies. We find that SOTA LLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased towards predicting dependence more often, perhaps relying on temporal order of steps as a heuristic. While prompting for explanations and using few-shot examples improve performance, the best F1 result is only 0.73. Further, human evaluation of explanations along with answer correctness show that, on average, humans do not agree with model reasoning. Surprisingly, we also find that explaining after answering leads to better performance than normal chain-of-thought prompting, and LLM answers are not consistent across questions about the same step pairs. Overall, results show that LLMs’ ability to detect dependence between steps has significant room for improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1078.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1078 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1078.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1078/>Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics</a></strong><br><a href=/people/t/theo-gigant/>Théo Gigant</a>
|
<a href=/people/c/camille-guinaudeau/>Camille Guinaudeau</a>
|
<a href=/people/m/marc-decombas/>Marc Decombas</a>
|
<a href=/people/f/frederic-dufaux/>Frederic Dufaux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1078><div class="card-body p-3 small">Automatic metrics are used as proxies to evaluate abstractive summarization systems when human annotations are too expensive. To be useful, these metrics should be fine-grained, show a high correlation with human annotations, and ideally be independant of reference quality; however, most standard evaluation metrics for summarization are reference-based, and existing reference-free metrics correlates poorly with relevance, especially on summaries of longer documents. In this paper, we introduce a reference-free metric that correlates well with human evaluated relevance, while being very cheap to compute. We show that this metric can also be used along reference-based metrics to improve their robustness in low quality reference settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1079.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1079/>An Empirical Analysis of the Writing Styles of Persona-Assigned <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/m/manuj-malik/>Manuj Malik</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a>
|
<a href=/people/k/kian-ming-a-chai/>Kian Ming A. Chai</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1080.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1080 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1080/>Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks</a></strong><br><a href=/people/a/amit-parekh/>Amit Parekh</a>
|
<a href=/people/n/nikolas-vitsakis/>Nikolas Vitsakis</a>
|
<a href=/people/a/alessandro-suglia/>Alessandro Suglia</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1080><div class="card-body p-3 small">Evaluating the generalisation capabilities of multimodal models based solely on their performance on out-of-distribution data fails to capture their true robustness. This work introduces a comprehensive evaluation framework that systematically examines the role of instructions and inputs in the generalisation abilities of such models, considering architectural design, input perturbations across language and vision modalities, and increased task complexity. The proposed framework uncovers the resilience of multimodal models to extreme instruction perturbations and their vulnerability to observational changes, raising concerns about overfitting to spurious correlations. By employing this evaluation framework on current Transformer-based multimodal models for robotic manipulation tasks, we uncover limitations and suggest future advancements should focus on architectural and training innovations that better integrate multimodal inputs, enhancing a model’s generalisation prowess by prioritising sensitivity to input content over incidental correlations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1081.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1081/><span class=acl-fixed-case>GPT</span> vs <span class=acl-fixed-case>RETRO</span>: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning</a></strong><br><a href=/people/a/aleksander-ficek/>Aleksander Ficek</a>
|
<a href=/people/j/jiaqi-zeng/>Jiaqi Zeng</a>
|
<a href=/people/o/oleksii-kuchaiev/>Oleksii Kuchaiev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1081><div class="card-body p-3 small">Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT. Additionally, our study indicates that 8B parameter models strike an optimal balance between cost and performance and P-tuning lags behind other PEFT techniques. We further provide a comparative analysis of between applying PEFT to Instruction-tuned RETRO model and base RETRO model. This work presents the first comprehensive comparison of various PEFT methods integrated with RAG, applied to both GPT and RETRO models, highlighting their relative performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1082.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1082 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1082.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1082/><span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>ST</span>: Automatic Complex Code Generation with Online Searching and Correctness Testing</a></strong><br><a href=/people/x/xinyi-he/>Xinyi He</a>
|
<a href=/people/j/jiaru-zou/>Jiaru Zou</a>
|
<a href=/people/y/yun-lin/>Yun Lin</a>
|
<a href=/people/m/mengyu-zhou/>Mengyu Zhou</a>
|
<a href=/people/s/shi-han/>Shi Han</a>
|
<a href=/people/z/zejian-yuan/>Zejian Yuan</a>
|
<a href=/people/d/dongmei-zhang/>Dongmei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1082><div class="card-body p-3 small">Large Language Models have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CoCoST framework, which enhances complex code generation by online searching for more information with planned queries and correctness testing for code refinement. Moreover, CoCoST serializes the complex inputs and outputs to improve comprehension and generates test cases to ensure the adaptability for real-world applications. CoCoST is validated through rigorous experiments on the DS-1000 and ClassEval datasets. Experimental results show that CoCoST substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality of LLMs in generating complex code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1083.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1083.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1083.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1083/>Sequential <span class=acl-fixed-case>API</span> Function Calling Using <span class=acl-fixed-case>G</span>raph<span class=acl-fixed-case>QL</span> Schema</a></strong><br><a href=/people/a/avirup-saha/>Avirup Saha</a>
|
<a href=/people/l/lakshmi-mandal/>Lakshmi Mandal</a>
|
<a href=/people/b/balaji-ganesan/>Balaji Ganesan</a>
|
<a href=/people/s/sambit-ghosh/>Sambit Ghosh</a>
|
<a href=/people/r/renuka-sindhgatta/>Renuka Sindhgatta</a>
|
<a href=/people/c/carlos-eberhardt/>Carlos Eberhardt</a>
|
<a href=/people/d/dan-debrunner/>Dan Debrunner</a>
|
<a href=/people/s/sameep-mehta/>Sameep Mehta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1083><div class="card-body p-3 small">Function calling using Large Language Models (LLMs) is an active research area that aims to empower LLMs with the ability to execute APIs to perform real-world tasks. However, sequential function calling using LLMs with interdependence between functions is still under-explored. To this end, we introduce GraphQLRestBench, a dataset consisting of natural language utterances paired with function call sequences representing real-world REST API calls with variable mapping between functions. In order to represent the response structure of the functions in the LLM prompt, we use the GraphQL schema of the REST APIs. We also introduce a custom evaluation framework for our dataset consisting of four specially designed metrics. We evaluate various open-source LLMs on our dataset using few-shot Chain-of-Thought and ReAct prompting to establish a reasonable baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1084.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1084/>The Illusion of Competence: Evaluating the Effect of Explanations on Users’ Mental Models of Visual Question Answering Systems</a></strong><br><a href=/people/j/judith-sieker/>Judith Sieker</a>
|
<a href=/people/s/simeon-junker/>Simeon Junker</a>
|
<a href=/people/r/ronja-utescher/>Ronja Utescher</a>
|
<a href=/people/n/nazia-attari/>Nazia Attari</a>
|
<a href=/people/h/heiko-wersing/>Heiko Wersing</a>
|
<a href=/people/h/hendrik-buschmeier/>Hendrik Buschmeier</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1084><div class="card-body p-3 small">We examine how users perceive the limitations of an AI system when it encounters a task that it cannot perform perfectly and whether providing explanations alongside its answers aids users in constructing an appropriate mental model of the system’s capabilities and limitations. We employ a visual question answer and explanation task where we control the AI system’s limitations by manipulating the visual inputs: during inference, the system either processes full-color or grayscale images. Our goal is to determine whether participants can perceive the limitations of the system. We hypothesize that explanations will make limited AI capabilities more transparent to users. However, our results show that explanations do not have this effect. Instead of allowing users to more accurately assess the limitations of the AI system, explanations generally increase users’ perceptions of the system’s competence – regardless of its actual performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1085.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1085/>Re-Evaluating Evaluation for Multilingual Summarization</a></strong><br><a href=/people/j/jessica-zosa-forde/>Jessica Zosa Forde</a>
|
<a href=/people/r/ruochen-zhang/>Ruochen Zhang</a>
|
<a href=/people/l/lintang-sutawika/>Lintang Sutawika</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/s/samuel-cahyawijaya/>Samuel Cahyawijaya</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/m/minghao-wu/>Minghao Wu</a>
|
<a href=/people/c/carsten-eickhoff/>Carsten Eickhoff</a>
|
<a href=/people/s/stella-biderman/>Stella Biderman</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1085><div class="card-body p-3 small">Automatic evaluation approaches (ROUGE, BERTScore, LLM-based evaluators) have been widely used to evaluate summarization tasks. Despite the complexities of script differences and tokenization, these approaches have been indiscriminately applied to summarization across multiple languages. While previous works have argued that these approaches correlate strongly with human ratings in English, it remains unclear whether the conclusion holds for other languages. To answer this question, we construct a small-scale pilot dataset containing article-summary pairs and human ratings in English, Chinese and Indonesian. To measure the strength of summaries, our ratings are measured as head-to-head comparisons with resulting Elo scores across four dimensions. Our analysis reveals that standard metrics are unreliable measures of quality, and that these problems are exacerbated in Chinese and Indonesian. We advocate for more nuanced and careful considerations in designing a robust evaluation framework for multiple languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1086.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1086/>Video-Text Prompting for Weakly Supervised Spatio-Temporal Video Grounding</a></strong><br><a href=/people/h/heng-zhao/>Heng Zhao</a>
|
<a href=/people/z/zhao-yinjie/>Zhao Yinjie</a>
|
<a href=/people/b/bihan-wen/>Bihan Wen</a>
|
<a href=/people/y/yew-soon-ong/>Yew-Soon Ong</a>
|
<a href=/people/j/joey-tianyi-zhou/>Joey Tianyi Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1086><div class="card-body p-3 small">Weakly-supervised Spatio-Temporal Video Grounding(STVG) aims to localize target object tube given a text query, without densely annotated training data. Existing methods extract each candidate tube feature independently by cropping objects from video frame feature, discarding all contextual information such as position change and inter-entity relationship. In this paper, we propose Video-Text Prompting(VTP) to construct candidate feature. Instead of cropping tube region from feature map, we draw visual markers(e.g. red circle) over objects tubes as video prompts; corresponding text prompt(e.g. in red circle) is also inserted after the subject word of query text to highlight its presence. Nevertheless, each candidate feature may look similar without cropping. To address this, we further propose Contrastive VTP(CVTP) by introducing negative contrastive samples whose candidate object is erased instead of being highlighted; by comparing the difference between VTP candidate and the contrastive sample, the gap of matching score between correct candidate and the rest is enlarged. Extensive experiments and ablations are conducted on several STVG datasets and our results surpass existing weakly-supervised methods by a great margin, demonstrating the effectiveness of our proposed methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1087.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1087/>A Fast and Sound Tagging Method for Discontinuous Named-Entity Recognition</a></strong><br><a href=/people/c/caio-filippo-corro/>Caio Filippo Corro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1087><div class="card-body p-3 small">We introduce a novel tagging scheme for discontinuous named entity recognition based on an explicit description of the inner structure of discontinuous mentions. We rely on a weighted finite state automaton for both marginal and maximum a posteriori inference. As such, our method is sound in the sense that (1) well-formedness of predicted tag sequences is ensured via the automaton structure and (2) there is an unambiguous mapping between well-formed sequences of tags and (discontinuous) mentions. We evaluate our approach on three English datasets in the biomedical domain, and report comparable results to state-of-the-art while having a way simpler and faster model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1088.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1088/>Factuality of Large Language Models: A Survey</a></strong><br><a href=/people/y/yuxia-wang/>Yuxia Wang</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/m/muhammad-arslan-manzoor/>Muhammad Arslan Manzoor</a>
|
<a href=/people/f/fei-liu/>Fei Liu</a>
|
<a href=/people/g/georgi-nenkov-georgiev/>Georgi Nenkov Georgiev</a>
|
<a href=/people/r/rocktim-jyoti-das/>Rocktim Jyoti Das</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1088><div class="card-body p-3 small">Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1089.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1089/>Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation</a></strong><br><a href=/people/y/youngwoo-kim/>Youngwoo Kim</a>
|
<a href=/people/r/razieh-rahimi/>Razieh Rahimi</a>
|
<a href=/people/j/james-allan/>James Allan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1089><div class="card-body p-3 small">Most of the efforts in interpreting neural relevance models have been on local explanations, which explain the relevance of a document to a query. However, local explanations are not effective in predicting the model’s behavior on unseen texts. We aim at explaining a neural relevance model by providing lexical explanations that can be globally generalized. Specifically, we construct a relevance thesaurus containing semantically relevant query term and document term pairs, which can augment BM25 scoring functions to better approximate the neural model’s predictions. We propose a novel method to build a relevance thesaurus construction. Our method involves training a neural relevance model which can score the relevance for partial segments of query and documents. The trained model is used to identify relevant terms over the vocabulary space. The resulting thesaurus explanation is evaluated based on ranking effectiveness and fidelity to the targeted neural ranking model. Finally, our thesaurus reveals the existence of brand name bias in ranking models, which further supports the utility of our explanation method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1090.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1090 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1090/>Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse</a></strong><br><a href=/people/r/rongchen-guo/>Rongchen Guo</a>
|
<a href=/people/i/isar-nejadgholi/>Isar Nejadgholi</a>
|
<a href=/people/h/hillary-dawkins/>Hillary Dawkins</a>
|
<a href=/people/k/kathleen-c-fraser/>Kathleen C. Fraser</a>
|
<a href=/people/s/svetlana-kiritchenko/>Svetlana Kiritchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1090><div class="card-body p-3 small">This work provides an explanatory view of how LLMs can apply moral reasoning to both criticize and defend sexist language. We assessed eight large language models, all of which demonstrated the capability to provide explanations grounded in varying moral perspectives for both critiquing and endorsing views that reflect sexist assumptions. With both human and automatic evaluation, we show that all eight models produce comprehensible and contextually relevant text, which is helpful in understanding diverse views on how sexism is perceived. Also, through analysis of moral foundations cited by LLMs in their arguments, we uncover the diverse ideological perspectives in models’ outputs, with some models aligning more with progressive or conservative views on gender roles and sexism.Based on our observations, we caution against the potential misuse of LLMs to justify sexist language. We also highlight that LLMs can serve as tools for understanding the roots of sexist beliefs and designing well-informed interventions. Given this dual capacity, it is crucial to monitor LLMs and design safety mechanisms for their use in applications that involve sensitive societal topics, such as sexism.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1091.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1091 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1091/><span class=acl-fixed-case>DISCERN</span>: Decoding Systematic Errors in Natural Language for Text Classifiers</a></strong><br><a href=/people/r/rakesh-r-menon/>Rakesh R Menon</a>
|
<a href=/people/s/shashank-srivastava/>Shashank Srivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1091><div class="card-body p-3 small">Despite their high predictive accuracies, current machine learning systems often exhibit systematic biases stemming from annotation artifacts or insufficient support for certain classes in the dataset. Recent work proposes automatic methods for identifying and explaining systematic biases using keywords. We introduce DISCERN, a framework for interpreting systematic biases in text classifiers using language explanations. DISCERN iteratively generates precise natural language descriptions of systematic errors by employing an interactive loop between two large language models. Finally, we use the descriptions to improve classifiers by augmenting classifier training sets with synthetically generated instances or annotated examples via active learning. On three text-classification datasets, we demonstrate that language explanations from our framework induce consistent performance improvements that go beyond what is achievable with exemplars of systematic bias. Finally, in human evaluations, we show that users can interpret systematic biases more effectively (by over 25% relative) and efficiently when described through language explanations as opposed to cluster exemplars.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1092.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1092 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1092/><span class=acl-fixed-case>I</span>nt<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>O</span>p: Interpretability-Aware Vision-Language Prompt Tuning</a></strong><br><a href=/people/s/soumya-suvra-ghosal/>Soumya Suvra Ghosal</a>
|
<a href=/people/s/samyadeep-basu/>Samyadeep Basu</a>
|
<a href=/people/s/soheil-feizi/>Soheil Feizi</a>
|
<a href=/people/d/dinesh-manocha/>Dinesh Manocha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1092><div class="card-body p-3 small">Image-text contrastive models such as CLIP learn transferable and robust representations for zero-shot transfer to a variety of downstream tasks. However, to obtain strong downstream performances, prompts need to be carefully curated, which can be a tedious engineering task. To address the issue of manual prompt engineering, prompt-tuning is used where a set of contextual vectors are learned by leveraging information from the training data. Despite their effectiveness, existing prompt-tuning frameworks often lack interpretability, thus limiting their ability to understand the compositional nature of images. In this work, we first identify that incorporating compositional attributes (e.g., a “green” tree frog) in the design of manual prompts can significantly enhance image-text alignment scores. Building upon this observation, we propose a novel and interpretable prompt-tuning method named IntCoOp, which learns to jointly align attribute-level inductive biases and class embeddings during prompt-tuning. To assess the effectiveness of our approach, we evaluate IntCoOp across two representative tasks in a few-shot learning setup: generalization to novel classes, and unseen domain shifts. Through extensive experiments across 10 downstream datasets on CLIP, we find that introducing attribute-level inductive biases leads to superior performance against state-of-art prompt tuning frameworks. Notably, in a 16-shot setup, IntCoOp improves CoOp by 7.35% in average performance across 10 diverse datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1093.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1093 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1093.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1093/>Scope-enhanced Compositional Semantic Parsing for <span class=acl-fixed-case>DRT</span></a></strong><br><a href=/people/x/xiulin-yang/>Xiulin Yang</a>
|
<a href=/people/j/jonas-groschwitz/>Jonas Groschwitz</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1093><div class="card-body p-3 small">Discourse Representation Theory (DRT) distinguishes itself from other semantic representation frameworks by its ability to model complex semantic and discourse phenomena through structural nesting and variable binding. While seq2seq models hold the state of the art on DRT parsing, their accuracy degrades with the complexity of the sentence, and they sometimes struggle to produce well-formed DRT representations. We introduce the AMS parser, a compositional, neurosymbolic semantic parser for DRT. It rests on a novel mechanism for predicting quantifier scope. We show that the AMS parser reliably produces well-formed outputs and performs well on DRT parsing, especially on complex sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1094.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1094 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1094/>The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models</a></strong><br><a href=/people/s/siyang-liu/>Siyang Liu</a>
|
<a href=/people/t/trisha-maturi/>Trisha Maturi</a>
|
<a href=/people/b/bowen-yi/>Bowen Yi</a>
|
<a href=/people/s/siqi-shen/>Siqi Shen</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1094><div class="card-body p-3 small">We explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories. Through a diverse set of prompts tailored to ensure response robustness, we find a general inclination of LLM values towards younger demographics, especially when compared to the US population. Although a general inclination can be observed, we also found that this inclination toward younger groups can be different across different value categories. Additionally, we explore the impact of incorporating age identity information in prompts and observe challenges in mitigating value discrepancies with different age cohorts. Our findings highlight the age bias in LLMs and provide insights for future work. Materials for our analysis will be available via <a href=https://github.com/anonymous class=acl-markup-url>https://github.com/anonymous</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1095.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1095 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1095/><span class=acl-fixed-case>T</span>empo<span class=acl-fixed-case>F</span>ormer: A Transformer for Temporally-aware Representations in Change Detection</a></strong><br><a href=/people/t/talia-tseriotou/>Talia Tseriotou</a>
|
<a href=/people/a/adam-tsakalidis/>Adam Tsakalidis</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1095><div class="card-body p-3 small">Dynamic representation learning plays a pivotal role in understanding the evolution of linguistic content over time. On this front both context and time dynamics as well as their interplay are of prime importance. Current approaches model context via pre-trained representations, which are typically temporally agnostic. Previous work on modelling context and temporal dynamics has used recurrent methods, which are slow and prone to overfitting. Here we introduce TempoFormer, the first task-agnostic transformer-based and temporally-aware model for dynamic representation learning. Our approach is jointly trained on inter and intra context dynamics and introduces a novel temporal variation of rotary positional embeddings. The architecture is flexible and can be used as the temporal representation foundation of other models or applied to different transformer-based architectures. We show new SOTA performance on three different real-time change detection tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1096.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1096/>Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?</a></strong><br><a href=/people/g/guillermo-marco/>Guillermo Marco</a>
|
<a href=/people/j/julio-gonzalo/>Julio Gonzalo</a>
|
<a href=/people/m/m-teresa-mateo-girona/>M.Teresa Mateo-Girona</a>
|
<a href=/people/r/ramon-del-castillo-santos/>Ramón Del Castillo Santos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1096><div class="card-body p-3 small">Are LLMs ready to compete in creative writing skills with a top (rather than average) novelist? To provide an initial answer for this question, we have carried out a contest between Patricio Pron (an awarded novelist, considered one of the best of his generation) and GPT-4 (one of the top performing LLMs), in the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee Sidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write short stories for both their titles and their opponent’s. Then, we prepared an evaluation rubric inspired by Boden’s definition of creativity, and we collected several detailed expert assessments of the texts, provided by literature critics and scholars. The results of our experimentation indicate that LLMs are still far from challenging a top human creative writer. We also observed that GPT-4 writes more creatively using Pron’s titles than its own titles (which is an indication of the potential for human-machine co-creation). Additionally, we found that GPT-4 has a more creative writing style in English than in Spanish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1097.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1097/>Evaluating Diversity in Automatic Poetry Generation</a></strong><br><a href=/people/y/yanran-chen/>Yanran Chen</a>
|
<a href=/people/h/hannes-groner/>Hannes Gröner</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1097><div class="card-body p-3 small">Natural Language Generation (NLG), and more generally generative AI, are among the currently most impactful research fields. Creative NLG, such as automatic poetry generation, is a fascinating niche in this area. While most previous research has focused on forms of the Turing test when evaluating automatic poetry generation — can humans distinguish between automatic and human generated poetry — we evaluate the diversity of automatically generated poetry (with a focus on quatrains), by comparing distributions of generated poetry to distributions of human poetry along structural, lexical, semantic and stylistic dimensions, assessing different model types (word vs. character-level, general purpose LLMs vs. poetry-specific models), including the very recent LLaMA3-8B, and types of fine-tuning (conditioned vs. unconditioned). We find that current automatic poetry systems are considerably underdiverse along multiple dimensions — they often do not rhyme sufficiently, are semantically too uniform and even do not match the length distribution of human poetry. Our experiments reveal, however, that style-conditioning and character-level modeling clearly increases diversity across virtually all dimensions we explore. Our identified limitations may serve as the basis for more genuinely diverse future poetry generation models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1098.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1098.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1098/>Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models</a></strong><br><a href=/people/y/yi-zhou/>Yi Zhou</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1098><div class="card-body p-3 small">Social biases such as gender or racial biases have been reported in language models (LMs), including Masked Language Models (MLMs). Given that MLMs are continuously trained with increasing amounts of additional data collected over time, an important yet unanswered question is how the social biases encoded with MLMs vary over time. In particular, the number of social media users continues to grow at an exponential rate, and it is a valid concern for the MLMs trained specifically on social media data whether their social biases (if any) would also amplify over time. To empirically analyse this problem, we use a series of MLMs pretrained on chronologically ordered temporal snapshots of corpora. Our analysis reveals that, although social biases are present in all MLMs, most types of social bias remain relatively stable over time (with a few exceptions). To further understand the mechanisms that influence social biases in MLMs, we analyse the temporal corpora used to train the MLMs. Our findings show that some demographic groups, such as male, obtain higher preference over the other, such as female on the training corpora constantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1099.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1099.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1099/>Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection</a></strong><br><a href=/people/c/camilla-casula/>Camilla Casula</a>
|
<a href=/people/s/sebastiano-vecellio-salto/>Sebastiano Vecellio Salto</a>
|
<a href=/people/a/alan-ramponi/>Alan Ramponi</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1099><div class="card-body p-3 small">The use of synthetic data for training models for a variety of NLP tasks is now widespread. However, previous work reports mixed results with regards to its effectiveness on highly subjective tasks such as hate speech detection. In this paper, we present an in-depth qualitative analysis of the potential and specific pitfalls of synthetic data for hate speech detection in English, with 3,500 manually annotated examples. We show that, across different models, synthetic data created through paraphrasing gold texts can improve out-of-distribution robustness from a computational standpoint. However, this comes at a cost: synthetic data fails to reliably reflect the characteristics of real-world data on a number of linguistic dimensions, it results in drastically different class distributions, and it heavily reduces the representation of both specific identity groups and intersectional hate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1100.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1100 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1100/>Grounding Language in Multi-Perspective Referential Communication</a></strong><br><a href=/people/z/zineng-tang/>Zineng Tang</a>
|
<a href=/people/l/lingjun-mao/>Lingjun Mao</a>
|
<a href=/people/a/alane-suhr/>Alane Suhr</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1100><div class="card-body p-3 small">We introduce a task and dataset for referring expression generation and comprehension in multi-agent embodied environments.In this task, two agents in a shared scene must take into account one another’s visual perspective, which may be different from their own, to both produce and understand references to objects in a scene and the spatial relations between them.We collect a dataset of 2,970 human-written referring expressions, each paired with human comprehension judgments, and evaluate the performance of automated models as speakers and listeners paired with human partners, finding that model performance in both reference generation and comprehension lags behind that of pairs of human agents.Finally, we experiment training an open-weight speaker model with evidence of communicative success when paired with a listener, resulting in an improvement from 58.9 to 69.3% in communicative success and even outperforming the strongest proprietary model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1101.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1101/>Threshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval</a></strong><br><a href=/people/y/yifan-qiao/>Yifan Qiao</a>
|
<a href=/people/p/parker-carlson/>Parker Carlson</a>
|
<a href=/people/s/shanxiu-he/>Shanxiu He</a>
|
<a href=/people/y/yingrui-yang/>Yingrui Yang</a>
|
<a href=/people/t/tao-yang/>Tao Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1101><div class="card-body p-3 small">This paper revisits dynamic pruning through rank score thresholding in cluster-based sparse retrieval to skip the index partially at cluster and document levels during inference. It proposes a two-parameter pruning control scheme called ASC with a probabilistic guarantee on rank-safeness competitiveness. ASC uses cluster-level maximum weight segmentation to improve accuracy of rank score bound estimation and threshold-driven pruning, and is targeted for speeding up retrieval applications requiring high relevance competitiveness. The experiments with MS MARCO and BEIR show that ASC improves the accuracy and safeness of pruning for better relevance while delivering a low latency on a single-threaded CPU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1102.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1102/>Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>A</span>mharic Translation</a></strong><br><a href=/people/h/hizkiel-mitiku-alemayehu/>Hizkiel Mitiku Alemayehu</a>
|
<a href=/people/h/hamada-m-zahera/>Hamada M Zahera</a>
|
<a href=/people/a/axel-cyrille-ngonga-ngomo/>Axel-Cyrille Ngonga Ngomo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1102><div class="card-body p-3 small">Multilingual large language models (mLLMs) have significantly advanced machine translation, yet challenges remain for low-resource languages like Amharic. This study evaluates the performance of state-of-the-art mLLMs, specifically NLLB-200 (NLLB3.3, NLLB1.3 Distilled1.3, NLB600) and M2M (M2M1.2B, M2M418), in English-Amharic bidirectional translation using the Lesan AI dataset. We employed both automatic and human evaluation methods to analyze translation errors. Automatic evaluation used BLEU, METEOR, chrF, and TER metrics, while human evaluation assessed translation quality at both word and sentence levels. Sentence-level accuracy was rated by annotators on a scale from 0 to 5, and word-level quality was evaluated using Multidimensional Quality Metrics. Our findings indicate that the NLLB3.3B model consistently outperformed other mLLMs across all evaluation methods. Common error included mistranslation, omission, untranslated segments, and additions, with mistranslation being particularly common. Punctuation and spelling errors were rare in our experiment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1103.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1103.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1103.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1103/><span class=acl-fixed-case>MIPD</span>: Exploring Manipulation and Intention In a Novel Corpus of <span class=acl-fixed-case>P</span>olish Disinformation</a></strong><br><a href=/people/a/arkadiusz-modzelewski/>Arkadiusz Modzelewski</a>
|
<a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a>
|
<a href=/people/p/pavel-savov/>Pavel Savov</a>
|
<a href=/people/m/magdalena-anna-wilczynska/>Magdalena Anna Wilczyńska</a>
|
<a href=/people/a/adam-wierzbicki/>Adam Wierzbicki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1103><div class="card-body p-3 small">This study presents a novel corpus of 15,356 Polish web articles, including articles identified as containing disinformation. Our dataset enables a multifaceted understanding of disinformation. We present a distinctive multilayered methodology for annotating disinformation in texts. What sets our corpus apart is its focus on uncovering hidden intent and manipulation in disinformative content. A team of experts annotated each article with multiple labels indicating both disinformation creators’ intents and the manipulation techniques employed. Additionally, we set new baselines for binary disinformation detection and two multiclass multilabel classification tasks: manipulation techniques and intention types classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1104.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1104/>Unsupervised Discrete Representations of <span class=acl-fixed-case>A</span>merican <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage</a></strong><br><a href=/people/a/artem-abzaliev/>Artem Abzaliev</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1104><div class="card-body p-3 small">Many modalities are naturally represented as continuous signals, making it difficult to use them with models that expect discrete units, such as LLMs. In this paper, we explore the use of audio compression techniques for the discrete representation of the gestures used in sign language. We train a tokenizer for American Sign Language (ASL) fingerspelling, which discretizes sequences of fingerspelling signs into tokens. We also propose a loss function to improve the interpretability of these tokens such that they preserve both the semantic and the visual information of the signal. We show that the proposed method improves the performance of the discretized sequence on downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1105.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1105 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1105/>Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models</a></strong><br><a href=/people/c/chani-jung/>Chani Jung</a>
|
<a href=/people/d/dongkwan-kim/>Dongkwan Kim</a>
|
<a href=/people/j/jiho-jin/>Jiho Jin</a>
|
<a href=/people/j/jiseon-kim/>Jiseon Kim</a>
|
<a href=/people/y/yeon-seonwoo/>Yeon Seonwoo</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a>
|
<a href=/people/h/hyunwoo-kim/>Hyunwoo Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1105><div class="card-body p-3 small">While humans naturally develop theory of mind (ToM), the capability to understand other people’s mental states and beliefs, state-of-the-art large language models (LLMs) underperform on simple ToM benchmarks. We posit that we can extend our understanding of LLMs’ ToM abilities by evaluating key human ToM precursors<span class=tex-math>-</span>perception inference and perception-to-belief inference<span class=tex-math>-</span>in LLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate these precursory inferences for ToM in LLMs by annotating characters’ perceptions on ToMi and FANToM, respectively.Our evaluation of eight state-of-the-art LLMs reveals that the models generally perform well in perception inference while exhibiting limited capability in perception-to-belief inference (e.g., lack of inhibitory control).Based on these results, we present PercepToM, a novel ToM method leveraging LLMs’ strong perception inference capability while supplementing their limited perception-to-belief inference. Experimental results demonstrate that PercepToM significantly enhances LLM’s performance, especially in false belief scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1106.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1106/>Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/m/mihir-parmar/>Mihir Parmar</a>
|
<a href=/people/h/hanieh-deilamsalehy/>Hanieh Deilamsalehy</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/s/seunghyun-yoon/>Seunghyun Yoon</a>
|
<a href=/people/r/ryan-a-rossi/>Ryan A. Rossi</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1106><div class="card-body p-3 small">Extractive summarization plays a pivotal role in natural language processing due to its wide-range applications in summarizing diverse content efficiently, while also being faithful to the original content. Despite significant advancement achieved in extractive summarization by Large Language Models (LLMs), these summaries frequently exhibit incoherence. An important aspect of the coherent summary is its readability for intended users. Although there have been many datasets and benchmarks proposed for creating coherent extractive summaries, none of them currently incorporate user intent to improve coherence in extractive summarization. Motivated by this, we propose a systematically created human-annotated dataset consisting of coherent summaries for five publicly available datasets and natural language user feedback, offering valuable insights into how to improve coherence in extractive summaries. We utilize this dataset for aligning LLMs through supervised fine-tuning with natural language human feedback to enhance the coherence of their generated summaries. Preliminary experiments with Falcon-40B and Llama-2-13B show significant performance improvements (~10% Rouge-L) in terms of producing coherent summaries. We further utilize human feedback to benchmark results over instruction-tuned models such as FLAN-T5 which resulted in several interesting findings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1107.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1107/>Jump Starting Bandits with <span class=acl-fixed-case>LLM</span>-Generated Prior Knowledge</a></strong><br><a href=/people/p/parand-a-alamdari/>Parand A. Alamdari</a>
|
<a href=/people/y/yanshuai-cao/>Yanshuai Cao</a>
|
<a href=/people/k/kevin-h-wilson/>Kevin H. Wilson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1107><div class="card-body p-3 small">We present substantial evidence demonstrating the benefits of integrating Large Language Models (LLMs) with a Contextual Multi-Armed Bandit framework. Contextual bandits have been widely used in recommendation systems to generate personalized suggestions based on user-specific contexts. We show that LLMs, pre-trained on extensive corpora rich in human knowledge and preferences, can simulate human behaviours well enough to jump-start contextual multi-armed bandits to reduce online learning regret. We propose an initialization algorithm for contextual bandits by prompting LLMs to produce a pre-training dataset of approximate human preferences for the bandit. This significantly reduces online learning regret and data-gathering costs for training such models. Our approach is validated empirically through two sets of experiments with different bandit setups: one which utilizes LLMs to serve as an oracle and a real-world experiment utilizing data from a conjoint survey experiment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1108.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1108/>Adaptation Odyssey in <span class=acl-fixed-case>LLM</span>s: Why Does Additional Pretraining Sometimes Fail to Improve?</a></strong><br><a href=/people/f/firat-oncel/>Fırat Öncel</a>
|
<a href=/people/m/matthias-bethge/>Matthias Bethge</a>
|
<a href=/people/b/beyza-ermis/>Beyza Ermis</a>
|
<a href=/people/m/mirco-ravanelli/>Mirco Ravanelli</a>
|
<a href=/people/c/cem-subakan/>Cem Subakan</a>
|
<a href=/people/c/cagatay-yildiz/>Çağatay Yıldız</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1108><div class="card-body p-3 small">In the last decade, the generalization and adaptation abilities of deep learning models were typically evaluated on fixed training and test distributions. Contrary to traditional deep learning, large language models (LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text corpora curated from the Internet with minimal human intervention, and (iii) trained in an online fashion. These stark contrasts prevent researchers from transferring lessons learned on model generalization and adaptation in deep learning contexts to LLMs.To this end, our short paper introduces empirical observations that aim to shed light on further training of already pretrained language models. Specifically, we demonstrate that training a model on a text domain could degrade its perplexity on the test portion of the same domain. We observe with our subsequent analysis that the performance degradation is positively correlated with the similarity between the additional and the original pretraining dataset of the LLM. Our further token-level perplexity analysis reveals that the perplexity degradation is due to a handful of tokens that are not informative about the domain. We hope these findings will guide us in determining when to adapt a model vs when to rely on its foundational capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1109.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1109.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1109.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1109/>Not All Contexts Are Equal: Teaching <span class=acl-fixed-case>LLM</span>s Credibility-aware Generation</a></strong><br><a href=/people/r/ruotong-pan/>Ruotong Pan</a>
|
<a href=/people/b/boxi-cao/>Boxi Cao</a>
|
<a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/j/jia-zheng/>Jia Zheng</a>
|
<a href=/people/s/sirui-wang/>Sirui Wang</a>
|
<a href=/people/x/xunliang-cai/>Xunliang Cai</a>
|
<a href=/people/l/le-sun/>Le Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1109><div class="card-body p-3 small">The rapid development of large language models has led to the widespread adoption of Retrieval-Augmented Generation (RAG), which integrates external knowledge to alleviate knowledge bottlenecks and mitigate hallucinations. However, the existing RAG paradigm inevitably suffers from the impact of flawed information introduced during the retrieval phrase, thereby diminishing the reliability and correctness of the generated outcomes. In this paper, we propose Credibility-aware Generation (CAG), a universally applicable framework designed to mitigate the impact of flawed information in RAG. At its core, CAG aims to equip models with the ability to discern and process information based on its credibility. To this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG. Furthermore, to accurately evaluate the models’ capabilities of CAG, we construct a comprehensive benchmark covering three critical real-world scenarios. Experimental results demonstrate that our model can effectively understand and employ credibility for generation, significantly outperform other models with retrieval augmentation, and exhibit robustness despite the increasing noise in the context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1110.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1110/>Virtual Personas for Language Models via an Anthology of Backstories</a></strong><br><a href=/people/s/suhong-moon/>Suhong Moon</a>
|
<a href=/people/m/marwa-abdulhai/>Marwa Abdulhai</a>
|
<a href=/people/m/minwoo-kang/>Minwoo Kang</a>
|
<a href=/people/j/joseph-suh/>Joseph Suh</a>
|
<a href=/people/w/widyadewi-soedarmadji/>Widyadewi Soedarmadji</a>
|
<a href=/people/e/eran-kohen-behar/>Eran Kohen Behar</a>
|
<a href=/people/d/david-chan/>David Chan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1110><div class="card-body p-3 small">Large language models (LLMs) are trained from vast repositories of text authored by millions of distinct authors, reflecting an enormous diversity of human traits. While these models bear the potential to be used as approximations of human subjects in behavioral studies, prior efforts have been limited in steering model responses to match individual human users. In this work, we introduce Anthology, a method for conditioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as backstories. We show that our methodology enhances the consistency and reliability of experimental outcomes while ensuring better representation of diverse sub-populations. Across three nationally representative human surveys conducted as part of Pew Research Center’s American Trends Panel (ATP), we demonstrate that Anthology achieves up to 18% improvement in matching the response distributions of human respondents and 27% improvement in consistency metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1111.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1111/>Step-by-Step Reasoning to Solve Grid Puzzles: Where do <span class=acl-fixed-case>LLM</span>s Falter?</a></strong><br><a href=/people/n/nemika-tyagi/>Nemika Tyagi</a>
|
<a href=/people/m/mihir-parmar/>Mihir Parmar</a>
|
<a href=/people/m/mohith-kulkarni/>Mohith Kulkarni</a>
|
<a href=/people/a/aswin-rrv/>Aswin Rrv</a>
|
<a href=/people/n/nisarg-patel/>Nisarg Patel</a>
|
<a href=/people/m/mutsumi-nakamura/>Mutsumi Nakamura</a>
|
<a href=/people/a/arindam-mitra/>Arindam Mitra</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1111><div class="card-body p-3 small">Solving grid puzzles involves a significant amount of logical reasoning. Hence, it is a good domain to evaluate reasoning capability of a model which can then guide us to improve the reasoning ability of models. However, most existing works evaluate only the final predicted answer of a puzzle, without delving into an in-depth analysis of the LLMs’ reasoning chains (such as where they falter) or providing any finer metrics to evaluate them. Since LLMs may rely on simple heuristics or artifacts to predict the final answer, it is crucial to evaluate the generated reasoning chain beyond overall correctness measures, for accurately evaluating the reasoning abilities of LLMs. To this end, we first develop GridPuzzle, an evaluation dataset comprising of 274 grid-based puzzles with different complexities. Second, we propose a new error taxonomy derived from manual analysis of reasoning chains from LLMs including GPT-4, Claude-3, Gemini, Mistral, and Llama-2. Then, we develop a LLM-based framework for large-scale subjective evaluation (i.e., identifying errors) and an objective metric, PuzzleEval, to evaluate the correctness of reasoning chains. Evaluating reasoning chains from LLMs leads to several interesting findings. We further show that existing prompting methods used for enhancing models’ reasoning abilities do not improve performance on GridPuzzle. This highlights the importance of understanding fine-grained errors and presents a challenge for future research to enhance LLMs’ puzzle-solving abilities by developing methods that address these errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1112.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1112.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1112/>Reasoning in Token Economies: Budget-Aware Evaluation of <span class=acl-fixed-case>LLM</span> Reasoning Strategies</a></strong><br><a href=/people/j/junlin-wang/>Junlin Wang</a>
|
<a href=/people/s/siddhartha-jain/>Siddhartha Jain</a>
|
<a href=/people/d/dejiao-zhang/>Dejiao Zhang</a>
|
<a href=/people/b/baishakhi-ray/>Baishakhi Ray</a>
|
<a href=/people/v/varun-kumar/>Varun Kumar</a>
|
<a href=/people/b/ben-athiwaratkun/>Ben Athiwaratkun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1112><div class="card-body p-3 small">A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don’t surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1113.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1113/>The Empirical Variability of Narrative Perceptions of Social Media Texts</a></strong><br><a href=/people/j/joel-mire/>Joel Mire</a>
|
<a href=/people/m/maria-antoniak/>Maria Antoniak</a>
|
<a href=/people/e/elliott-ash/>Elliott Ash</a>
|
<a href=/people/a/andrew-piper/>Andrew Piper</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1113><div class="card-body p-3 small">Most NLP work on narrative detection has focused on prescriptive definitions of stories crafted by researchers, leaving open the questions: how do crowd workers perceive texts to be a story, and why? We investigate this by building StoryPerceptions, a dataset of 2,496 perceptions of storytelling in 502 social media texts from 255 crowd workers, including categorical labels along with free-text storytelling rationales, authorial intent, and more. We construct a fine-grained bottom-up taxonomy of crowd workers’ varied and nuanced perceptions of storytelling by open-coding their free-text rationales. Through comparative analyses at the label and code level, we illuminate patterns of disagreement among crowd workers and across other annotation contexts, including prescriptive labeling from researchers and LLM-based predictions. Notably, plot complexity, references to generalized or abstract actions, and holistic aesthetic judgments (such as a sense of cohesion) are especially important in disagreements. Our empirical findings broaden understanding of the types, relative importance, and contentiousness of features relevant to narrative detection, highlighting opportunities for future work on reader-contextualized models of narrative reception.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1114.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1114/>Which questions should <span class=acl-fixed-case>I</span> answer? Salience Prediction of Inquisitive Questions</a></strong><br><a href=/people/y/yating-wu/>Yating Wu</a>
|
<a href=/people/r/ritika-rajesh-mangla/>Ritika Rajesh Mangla</a>
|
<a href=/people/a/alex-dimakis/>Alex Dimakis</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1114><div class="card-body p-3 small">Inquisitive questions — open-ended, curiosity-driven questions people ask as they read — are an integral part of discourse processing and comprehension. Recent work in NLP has taken advantage of question generation capabilities of LLMs to enhance a wide range of applications. But the space of inquisitive questions is vast: many questions can be evoked from a given context. So which of those should be prioritized to find answers? Linguistic theories, unfortunately, have not yet provided an answer to this question. This paper presents QSalience, a salience predictor of inquisitive questions. QSalience is instruction-tuned over our dataset of linguist-annotated salience scores of 1,766 (context, question) pairs. A question scores high on salience if answering it would greatly enhance the understanding of the text. We show that highly salient questions are empirically more likely to be answered in the same article, bridging potential questions with Questions Under Discussion. We further validate our findings by showing that answering salient questions is an indicator of summarization quality in news.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1115.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1115/>Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues</a></strong><br><a href=/people/l/lei-sun/>Lei Sun</a>
|
<a href=/people/j/jinming-zhao/>Jinming Zhao</a>
|
<a href=/people/q/qin-jin/>Qin Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1115><div class="card-body p-3 small">Personality recognition aims to identify the personality traits implied in user data such as dialogues and social media posts. Current research predominantly treats personality recognition as a classification task, failing to reveal the supporting evidence for the recognized personality. In this paper, we propose a novel task named Explainable Personality Recognition, aiming to reveal the reasoning process as supporting evidence of the personality trait. Inspired by personality theories, personality traits are made up of stable patterns of personality state, where the states are short-term characteristic patterns of thoughts, feelings, and behaviors in a concrete situation at a specific moment in time. We propose an explainable personality recognition framework called Chain-of-Personality-Evidence (CoPE), which involves a reasoning process from specific contexts to short-term personality states to long-term personality traits. Furthermore, based on the CoPE framework, we construct an explainable personality recognition dataset from dialogues, PersonalityEvd. We introduce two explainable personality state recognition and explainable personality trait recognition tasks, which require models to recognize the personality state and trait labels and their corresponding support evidence. Our extensive experiments based on Large Language Models on the two tasks show that revealing personality traits is very challenging and we present some insights for future research. We will release our dataset and source code to facilitate further studies in this direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1116.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1116/>Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech</a></strong><br><a href=/people/g/guan-ting-lin/>Guan-Ting Lin</a>
|
<a href=/people/w/wei-ping-huang/>Wei Ping Huang</a>
|
<a href=/people/h/hung-yi-lee/>Hung-yi Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1116><div class="card-body p-3 small">Deep Learning-based end-to-end Automatic Speech Recognition (ASR) has made significant strides but still struggles with performance on out-of-domain samples due to domain shifts in real-world scenarios. Test-Time Adaptation (TTA) methods address this issue by adapting models using test samples at inference time. However, current ASR TTA methods have largely focused on non-continual TTA, which limits cross-sample knowledge learning compared to continual TTA. In this work, we first propose a Fast-slow TTA framework for ASR that leverages the advantage of continual and non-continual TTA. Following this framework, we introduce Dynamic SUTA (DSUTA), an entropy-minimization-based continual TTA method for ASR. To enhance DSUTA’s robustness for time-varying data, we design a dynamic reset strategy to automatically detect domain shifts and reset the model, making it more effective at handling multi-domain data. Our method demonstrates superior performance on various noisy ASR datasets, outperforming both non-continual and continual TTA baselines while maintaining robustness to domain changes without requiring domain boundary information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1117.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1117 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1117/>Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities</a></strong><br><a href=/people/s/sachit-menon/>Sachit Menon</a>
|
<a href=/people/r/richard-zemel/>Richard Zemel</a>
|
<a href=/people/c/carl-vondrick/>Carl Vondrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1117><div class="card-body p-3 small">When presented with questions involving visual thinking, humans naturally switch reasoning modalities, often forming mental images or drawing visual aids. Large language models have shown promising results in arithmetic and symbolic reasoning by expressing intermediate reasoning in text as a chain of thought, yet struggle to extend this capability to answer text queries that are easily solved by visual reasoning, even with extensive multimodal pretraining. We introduce a simple method, <i>whiteboard-of-thought</i> prompting, to unlock the visual reasoning capabilities of multimodal large language models across modalities. Whiteboard-of-thought prompting provides multimodal large language models with a metaphorical ‘whiteboard’ to draw out reasoning steps as images, then returns these images back to the model for further processing. We find this can be accomplished with no demonstrations or specialized modules, instead leveraging models’ existing ability to write code with libraries such as Matplotlib and Turtle. This simple approach shows state-of-the-art results on four difficult natural language tasks that involve visual and spatial reasoning. We identify multiple settings where GPT-4o using chain-of-thought fails dramatically, including more than one where it achieves 0% accuracy, while whiteboard-of-thought enables up to 92% accuracy in these same settings. We present a detailed exploration of where the technique succeeds as well as its sources of error.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1118.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1118.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1118.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1118/><span class=acl-fixed-case>C</span>ode<span class=acl-fixed-case>J</span>udge: Evaluating Code Generation with Large Language Models</a></strong><br><a href=/people/w/weixi-tong/>Weixi Tong</a>
|
<a href=/people/t/tianyi-zhang/>Tianyi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1118><div class="card-body p-3 small">Large Language Models (LLMs) have shown promising performance in code generation. However, how to reliably evaluate code generated by LLMs remains an unresolved problem. This paper presents CodeJudge, a code evaluation framework that leverages LLMs to evaluate the semantic correctness of generated code without the need for test cases. We investigate different ways to guide the LLM in performing “slow thinking” to arrive at an in-depth and reliable evaluation. We experimented with four LLMs as evaluators on four code generation datasets and five programming languages. The results show that CodeJudge significantly outperformed existing methods in most settings. Furthermore, compared with a SOTA GPT-3.5-based code evaluation method, CodeJudge achieved better results even when using a much smaller model, Llama-3-8B-Instruct. Our code and datasets are available on GitHub https://github.com/VichyTong/CodeJudge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1119.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1119/>Self-Training Large Language and Vision Assistant for Medical Question Answering</a></strong><br><a href=/people/g/guohao-sun/>Guohao Sun</a>
|
<a href=/people/c/can-qin/>Can Qin</a>
|
<a href=/people/h/huazhu-fu/>Huazhu Fu</a>
|
<a href=/people/l/linwei-wang/>Linwei Wang</a>
|
<a href=/people/z/zhiqiang-tao/>Zhiqiang Tao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1119><div class="card-body p-3 small">Large Vision-Language Models (LVLMs) have shown significant potential in assisting medical diagnosis by leveraging extensive biomedical datasets. However, the advancement of medical image understanding and reasoning critically depends on building high-quality visual instruction data, which is costly and labor-intensive to obtain, particularly in the medical domain. To mitigate this data-starving issue, we introduce Self-Training Large Language and Vision Assistant for Medical (STLLaVA-Med). The proposed method is designed to train a policy model (an LVLM) capable of auto-generating medical visual instruction data to improve data efficiency, guided through Direct Preference Optimization (DPO). Specifically, a more powerful and larger LVLM (e.g., GPT-4o) is involved as a biomedical expert to oversee the DPO fine-tuning process on the auto-generated data, encouraging the policy model to align efficiently with human preferences. We validate the efficacy and data efficiency of STLLaVA-Med across three major medical Visual Question Answering (VQA) benchmarks, demonstrating competitive zero-shot performance with the utilization of only 9% of the medical data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1120.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1120/><span class=acl-fixed-case>SYNFAC</span>-<span class=acl-fixed-case>EDIT</span>: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization</a></strong><br><a href=/people/p/prakamya-mishra/>Prakamya Mishra</a>
|
<a href=/people/z/zonghai-yao/>Zonghai Yao</a>
|
<a href=/people/p/parth-vashisht/>Parth Vashisht</a>
|
<a href=/people/f/feiyun-ouyang/>Feiyun Ouyang</a>
|
<a href=/people/b/beining-wang/>Beining Wang</a>
|
<a href=/people/v/vidhi-dhaval-mody/>Vidhi Dhaval Mody</a>
|
<a href=/people/h/hong-yu/>Hong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1120><div class="card-body p-3 small">Large Language Models (LLMs) such as GPT & Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes >100B parameter GPT variants like GPT-3.5 & GPT-4 to act as synthetic experts to generate high-quality synthetics feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback generated by these synthetic feedback experts without additional human annotations, mirroring and optimizing the practical scenario in which medical professionals refine AI system outputs. Although such 100B+ parameter GPT variants have proven to demonstrate expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on their capacity to act as synthetic feedback experts and deliver expert-level edit feedback for improving the generation quality of weaker (&lt;10B parameter) LLMs like GPT-2 (1.5B) & Llama 2 (7B) in clinical domain. So in this work, we leverage 100B+ GPT variants to act as synthetic feedback experts offering expert-level edit feedback, that is used to reduce hallucinations and align weaker (&lt;10B parameter) LLMs with medical facts using two distinct alignment algorithms (DPO & SALT), endeavoring to narrow the divide between AI-generated content and factual accuracy. This highlights the substantial potential of LLM-based synthetic edits in enhancing the alignment of clinical factuality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1121.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1121/>Defending Jailbreak Prompts via In-Context Adversarial Game</a></strong><br><a href=/people/y/yujun-zhou/>Yujun Zhou</a>
|
<a href=/people/y/yufei-han/>Yufei Han</a>
|
<a href=/people/h/haomin-zhuang/>Haomin Zhuang</a>
|
<a href=/people/k/kehan-guo/>Kehan Guo</a>
|
<a href=/people/z/zhenwen-liang/>Zhenwen Liang</a>
|
<a href=/people/h/hongyan-bao/>Hongyan Bao</a>
|
<a href=/people/x/xiangliang-zhang/>Xiangliang Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1121><div class="card-body p-3 small">Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG’s efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover, ICAG demonstrates remarkable transferability to other LLMs, indicating its potential as a versatile defense mechanism. The code is available at https://github.com/YujunZhou/In-Context-Adversarial-Game.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1122.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1122 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1122/>Detecting Online Community Practices with Large Language Models: A Case Study of Pro-<span class=acl-fixed-case>U</span>krainian Publics on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/k/kateryna-kasianenko/>Kateryna Kasianenko</a>
|
<a href=/people/s/shima-khanehzar/>Shima Khanehzar</a>
|
<a href=/people/s/stephen-wan/>Stephen Wan</a>
|
<a href=/people/e/ehsan-dehghan/>Ehsan Dehghan</a>
|
<a href=/people/a/axel-bruns/>Axel Bruns</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1122><div class="card-body p-3 small">Communities on social media display distinct patterns of linguistic expression and behaviour, collectively referred to as practices. These practices can be traced in textual exchanges, and reflect the intentions, knowledge, values, and norms of users and communities. This paper introduces a comprehensive methodological workflow for computational identification of such practices within social media texts. By focusing on supporters of Ukraine during the Russia-Ukraine war in (1) the activist collective NAFO and (2) the Eurovision Twitter community, we present a gold-standard data set capturing their unique practices. Using this corpus, we perform practice prediction experiments with both open-source baseline models and OpenAI’s large language models (LLMs). Our results demonstrate that closed-source models, especially GPT-4, achieve superior performance, particularly with prompts that incorporate salient features of practices, or utilize Chain-of-Thought prompting. This study provides a detailed error analysis and offers valuable insights into improving the precision of practice identification, thereby supporting context-sensitive moderation and advancing the understanding of online community dynamics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1123.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1123 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1123/>Multilingual Topic Classification in <span class=acl-fixed-case>X</span>: Dataset and Analysis</a></strong><br><a href=/people/d/dimosthenis-antypas/>Dimosthenis Antypas</a>
|
<a href=/people/a/asahi-ushio/>Asahi Ushio</a>
|
<a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1123><div class="card-body p-3 small">In the dynamic realm of social media, diverse topics are discussed daily, transcending linguistic boundaries. However, the complexities of understanding and categorising this content across various languages remain an important challenge with traditional techniques like topic modelling often struggling to accommodate this multilingual diversity. In this paper, we introduce X-Topic, a multilingual dataset featuring content in four distinct languages (English, Spanish, Japanese, and Greek), crafted for the purpose of tweet topic classification. Our dataset includes a wide range of topics, tailored for social media content, making it a valuable resource for scientists and professionals working on cross-linguistic analysis, the development of robust multilingual models, and computational scientists studying online dialogue. Finally, we leverage X-Topic to perform a comprehensive cross-linguistic and multilingual analysis, and compare the capabilities of current general- and domain-specific language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1124.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1124/><span class=acl-fixed-case>MT</span>-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models</a></strong><br><a href=/people/w/wai-chung-kwan/>Wai-Chung Kwan</a>
|
<a href=/people/x/xingshan-zeng/>Xingshan Zeng</a>
|
<a href=/people/y/yuxin-jiang/>Yuxin Jiang</a>
|
<a href=/people/y/yufei-wang/>Yufei Wang</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1124><div class="card-body p-3 small">Large language models (LLMs) are increasingly used for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks mainly focus on single-turn evaluations, overlooking the models’ capabilities in multi-turn interactions. To address this gap, we introduce , a comprehensive benchmark to evaluate the multi-turn conversational abilities of LLMs. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or creating new examples using GPT-4 with a human-in-the-loop process to avoid data leakage. To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 10 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in specific tasks. We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models’ fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1125.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1125/>Updating <span class=acl-fixed-case>CLIP</span> to Prefer Descriptions Over Captions</a></strong><br><a href=/people/a/amir-zur/>Amir Zur</a>
|
<a href=/people/e/elisa-kreiss/>Elisa Kreiss</a>
|
<a href=/people/k/karel-doosterlinck/>Karel D’Oosterlinck</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/a/atticus-geiger/>Atticus Geiger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1125><div class="card-body p-3 small">Although CLIPScore is a powerful generic metric that captures the similarity between a text and an image, it fails to distinguish between a caption that is meant to complement the information in an image and a description that is meant to replace an image entirely, e.g., for accessibility. We address this shortcoming by updating the CLIP model with the Concadia dataset to assign higher scores to descriptions than captions using parameter efficient fine-tuning and a loss objective derived from work on causal interpretability. This model correlates with the judgements of blind and low-vision people while preserving transfer capabilities and has interpretable structure that sheds light on the caption–description distinction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1126.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1126.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1126.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1126/><span class=acl-fixed-case>C</span>md<span class=acl-fixed-case>C</span>aliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research</a></strong><br><a href=/people/s/sian-yao-huang/>Sian-Yao Huang</a>
|
<a href=/people/c/cheng-lin-yang/>Cheng-Lin Yang</a>
|
<a href=/people/c/che-yu-lin/>Che-Yu Lin</a>
|
<a href=/people/c/chun-ying-huang/>Chun-Ying Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1126><div class="card-body p-3 small">This research addresses command-line embedding in cybersecurity, a field obstructed by the lack of comprehensive datasets due to privacy and regulation concerns. We propose the first dataset of similar command lines, named CyPHER, for training and unbiased evaluation. The training set is generated using a set of large language models (LLMs) comprising 28,520 similar command-line pairs. Our testing dataset consists of 2,807 similar command-line pairs sourced from authentic command-line data.In addition, we propose a command-line embedding model named CmdCaliper, enabling the computation of semantic similarity with command lines. Performance evaluations demonstrate that the smallest version of CmdCaliper (30 million parameters) suppresses state-of-the-art (SOTA) sentence embedding models with ten times more parameters across various tasks (e.g., malicious command-line detection and similar command-line retrieval).Our study explores the feasibility of data generation using LLMs in the cybersecurity domain. Furthermore, we release our proposed command-line dataset, embedding models’ weights and all program codes to the public. This advancement paves the way for more effective command-line embedding for future researchers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1127.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1127/>Back to School: Translation Using Grammar Books</a></strong><br><a href=/people/j/jonathan-hus/>Jonathan Hus</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1127><div class="card-body p-3 small">Machine translation systems for high resource languages perform exceptionally well and produce high quality translations. Unfortunately, the vast majority of languages are not considered high resource and lack the quantity of parallel sentences needed to train such systems. These under-represented languages are not without resources, however, and bilingual dictionaries and grammar books are available as linguistic reference material. With current large language models (LLMs) supporting near book-length contexts, we can begin to use the available material to ensure advancements are shared among all of the world’s languages. In this paper, we demonstrate incorporating grammar books in the prompt of GPT-4 to improve machine translation and evaluate the performance on 16 topologically diverse low-resource languages, using a combination of reference material to show that the machine translation performance of LLMs can be improved using this method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1128.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1128/><span class=acl-fixed-case>VIEWS</span>: Entity-Aware News Video Captioning</a></strong><br><a href=/people/h/hammad-ayyubi/>Hammad Ayyubi</a>
|
<a href=/people/t/tianqi-liu/>Tianqi Liu</a>
|
<a href=/people/a/arsha-nagrani/>Arsha Nagrani</a>
|
<a href=/people/x/xudong-lin/>Xudong Lin</a>
|
<a href=/people/m/mingda-zhang/>Mingda Zhang</a>
|
<a href=/people/a/anurag-arnab/>Anurag Arnab</a>
|
<a href=/people/f/feng-han/>Feng Han</a>
|
<a href=/people/y/yukun-zhu/>Yukun Zhu</a>
|
<a href=/people/x/xuande-feng/>Xuande Feng</a>
|
<a href=/people/k/kevin-zhang/>Kevin Zhang</a>
|
<a href=/people/j/jialu-liu/>Jialu Liu</a>
|
<a href=/people/s/shih-fu-chang/>Shih-Fu Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1128><div class="card-body p-3 small">Existing popular video captioning benchmarks and models often produce generic captions for videos that lack specific identification of individuals, locations, or organizations (named entities). However, in the case of news videos, the setting is more demanding, requiring the inclusion of such named entities for meaningful summarization. Therefore, we introduce the task of directly summarizing news videos into captions that are entity-aware. To facilitate research in this area, we have collected a large-scale dataset named VIEWS (VIdeo NEWS). Within this task, we face challenges inherent to recognizing named entities and navigating diverse, dynamic contexts, all while relying solely on visual cues. To address these challenges, we propose a model-agnostic approach that enriches visual information extracted from videos with context sourced from external knowledge, enabling the generation of entity-aware captions. We validate the effectiveness of our approach across three video captioning models. Additionally, we conduct a critical analysis of our methodology to gain insights into the complexity of the task, the challenges it presents, and potential avenues for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1129.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1129/>Towards Aligning Language Models with Textual Feedback</a></strong><br><a href=/people/s/sauc-abadal-lloret/>Saüc Abadal Lloret</a>
|
<a href=/people/s/shehzaad-dhuliawala/>Shehzaad Dhuliawala</a>
|
<a href=/people/k/keerthiram-murugesan/>Keerthiram Murugesan</a>
|
<a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1129><div class="card-body p-3 small">We present ALT (ALignment with Textual feedback), an approach that aligns language models with user preferences expressed in text. We argue that text offers greater expressiveness, enabling users to provide richer feedback than simple comparative preferences and this richer feedback can lead to more efficient and effective alignment. ALT aligns the model by conditioning its generation on the textual feedback. Our method relies solely on language modeling techniques and requires minimal hyper-parameter tuning, though it still presents the main benefit of RL-based algorithms and can effectively learn from textual feedback. We explore the efficacy and efficiency of textual feedback across different tasks such as toxicity reduction, summarization, and dialog response. We find that ALT outperforms PPO for the task of toxicity reduction while being able to match its performance on summarization with only 20% of the samples. We also explore how ALT can be used with feedback provided by an existing LLM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1130.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1130/><span class=acl-fixed-case>AMPO</span>: Automatic Multi-Branched Prompt Optimization</a></strong><br><a href=/people/s/sheng-yang/>Sheng Yang</a>
|
<a href=/people/y/yurong-wu/>Yurong Wu</a>
|
<a href=/people/y/yan-gao/>Yan Gao</a>
|
<a href=/people/z/zineng-zhou/>Zineng Zhou</a>
|
<a href=/people/b/bin-benjamin-zhu/>Bin Benjamin Zhu</a>
|
<a href=/people/x/xiaodi-sun/>Xiaodi Sun</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/z/zhiming-ding/>Zhiming Ding</a>
|
<a href=/people/a/anbang-hu/>Anbang Hu</a>
|
<a href=/people/y/yuan-fang/>Yuan Fang</a>
|
<a href=/people/y/yunsong-li/>Yunsong Li</a>
|
<a href=/people/j/junyan-chen/>Junyan Chen</a>
|
<a href=/people/l/linjun-yang/>Linjun Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1130><div class="card-body p-3 small">Prompt engineering is very important to enhance the performance of large language models (LLMs). When dealing with complex issues, prompt engineers tend to distill multiple patterns from examples and inject relevant solutions to optimize the prompts, achieving satisfying results. However, existing automatic prompt optimization techniques are only limited to producing single flow instructions, struggling with handling diverse patterns. In this paper, we present AMPO, an automatic prompt optimization method that can iteratively develop a multi-branched prompt using failure cases as feedback. Our goal is to explore a novel way of structuring prompts with multi-branches to better handle multiple patterns in complex tasks, for which we introduce three modules: Pattern Recognition, Branch Adjustment, and Branch Pruning. In experiments across five tasks, AMPO consistently achieves the best results. Additionally, our approach demonstrates significant optimization efficiency due to our adoption of a minimal search strategy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1131.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1131/><span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>MPT</span>: Decoding-enhanced Multi-phase Prompt Tuning for Making <span class=acl-fixed-case>LLM</span>s Be Better Context-aware Translators</a></strong><br><a href=/people/x/xinglin-lyu/>Xinglin Lyu</a>
|
<a href=/people/j/junhui-li/>Junhui Li</a>
|
<a href=/people/y/yanqing-zhao/>Yanqing Zhao</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/s/shimin-tao/>Shimin Tao</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1132.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1132/><span class=acl-fixed-case>DEFT</span>-<span class=acl-fixed-case>UCS</span>: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing</a></strong><br><a href=/people/d/devleena-das/>Devleena Das</a>
|
<a href=/people/v/vivek-khetan/>Vivek Khetan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1132><div class="card-body p-3 small">Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset to fine-tune PLMs for text-generation needed for text editing tasks such as simplification, grammar correction, clarity, etc. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFT-UCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1133.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1133/>Unveiling Multi-level and Multi-modal Semantic Representations in the Human Brain using Large Language Models</a></strong><br><a href=/people/y/yuko-nakagi/>Yuko Nakagi</a>
|
<a href=/people/t/takuya-matsuyama/>Takuya Matsuyama</a>
|
<a href=/people/n/naoko-koide-majima/>Naoko Koide-Majima</a>
|
<a href=/people/h/hiroto-q-yamaguchi/>Hiroto Q. Yamaguchi</a>
|
<a href=/people/r/rieko-kubo/>Rieko Kubo</a>
|
<a href=/people/s/shinji-nishimoto/>Shinji Nishimoto</a>
|
<a href=/people/y/yu-takagi/>Yu Takagi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1133><div class="card-body p-3 small">In recent studies, researchers have used large language models (LLMs) to explore semantic representations in the brain; however, they have typically assessed different levels of semantic content, such as speech, objects, and stories, separately. In this study, we recorded brain activity using functional magnetic resonance imaging (fMRI) while participants viewed 8.3 hours of dramas and movies. We annotated these stimuli at multiple semantic levels, which enabled us to extract latent representations of LLMs for this content. Our findings demonstrate that LLMs predict human brain activity more accurately than traditional language models, particularly for complex background stories. Furthermore, we identify distinct brain regions associated with different semantic representations, including multi-modal vision-semantic representations, which highlights the importance of modeling multi-level and multi-modal semantic representations simultaneously. We will make our fMRI dataset publicly available to facilitate further research on aligning LLMs with human brain function.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1134.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1134/>“They are uncultured”: Unveiling Covert Harms and Social Threats in <span class=acl-fixed-case>LLM</span> Generated Conversations</a></strong><br><a href=/people/p/preetam-prabhu-srikar-dammu/>Preetam Prabhu Srikar Dammu</a>
|
<a href=/people/h/hayoung-jung/>Hayoung Jung</a>
|
<a href=/people/a/anjali-singh/>Anjali Singh</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/t/tanu-mitra/>Tanu Mitra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1134><div class="card-body p-3 small">Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools. Despite their utility, research indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world. Additionally, these studies typically investigate “harm” as a singular dimension, ignoring the various and subtle forms in which harms manifest. To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature. We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1135.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1135 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1135/>Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models</a></strong><br><a href=/people/d/do-xuan-long/>Do Xuan Long</a>
|
<a href=/people/d/duong-ngoc-yen/>Duong Ngoc Yen</a>
|
<a href=/people/l/luu-anh-tuan/>Anh Tuan Luu</a>
|
<a href=/people/k/kenji-kawaguchi/>Kenji Kawaguchi</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/n/nancy-chen/>Nancy F. Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1135><div class="card-body p-3 small">We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69% with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1136.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1136/>Will <span class=acl-fixed-case>LLM</span>s Replace the Encoder-Only Models in Temporal Relation Classification?</a></strong><br><a href=/people/g/gabriel-roccabruna/>Gabriel Roccabruna</a>
|
<a href=/people/m/massimo-rizzoli/>Massimo Rizzoli</a>
|
<a href=/people/g/giuseppe-riccardi/>Giuseppe Riccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1136><div class="card-body p-3 small">The automatic detection of temporal relations among events has been mainly investigated with encoder-only models such as RoBERTa. Large Language Models (LLM) have recently shown promising performance in temporal reasoning tasks such as temporal question answering. Nevertheless, recent studies have tested the LLMs’ performance in detecting temporal relations of closed-source models only, limiting the interpretability of those results. In this work, we investigate LLMs’ performance and decision process in the Temporal Relation Classification task. First, we assess the performance of seven open and closed-sourced LLMs experimenting with in-context learning and lightweight fine-tuning approaches. Results show that LLMs with in-context learning significantly underperform smaller encoder-only models based on RoBERTa. Then, we delve into the possible reasons for this gap by applying explainable methods. The outcome suggests a limitation of LLMs in this task due to their autoregressive nature, which causes them to focus only on the last part of the sequence. Additionally, we evaluate the word embeddings of these two models to better understand their pre-training differences. The code and the fine-tuned models can be found respectively on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1137.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1137/>Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties</a></strong><br><a href=/people/k/keunwoo-peter-yu/>Keunwoo Peter Yu</a>
|
<a href=/people/z/zheyuan-zhang/>Zheyuan Zhang</a>
|
<a href=/people/f/fengyuan-hu/>Fengyuan Hu</a>
|
<a href=/people/s/shane-storks/>Shane Storks</a>
|
<a href=/people/j/joyce-chai/>Joyce Chai</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1138.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1138 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1138.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1138/>Waterfall: Scalable Framework for Robust Text Watermarking and Provenance for <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/g/gregory-kang-ruey-lau/>Gregory Kang Ruey Lau</a>
|
<a href=/people/x/xinyuan-niu/>Xinyuan Niu</a>
|
<a href=/people/h/hieu-dao/>Hieu Dao</a>
|
<a href=/people/j/jiangwei-chen/>Jiangwei Chen</a>
|
<a href=/people/c/chuan-sheng-foo/>Chuan-Sheng Foo</a>
|
<a href=/people/b/bryan-kian-hsiang-low/>Bryan Kian Hsiang Low</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1138><div class="card-body p-3 small">Protecting intellectual property (IP) of text such as articles and code is increasingly important, especially as sophisticated attacks become possible, such as paraphrasing by large language models (LLMs) or even unauthorized training of LLMs on copyrighted text to infringe such IP. However, existing text watermarking methods are not robust enough against such attacks nor scalable to millions of users for practical implementation. In this paper, we propose Waterfall, the first training-free framework for robust and scalable text watermarking applicable across multiple text types (e.g., articles, code) and languages supportable by LLMs, for general text and LLM data provenance. Waterfall comprises several key innovations, such as being the first to use LLM as paraphrasers for watermarking along with a novel combination of techniques that are surprisingly effective in achieving robust verifiability and scalability. We empirically demonstrate that Waterfall achieves significantly better scalability, robust verifiability, and computational efficiency compared to SOTA article-text watermarking methods, and also showed how it could be directly applied to the watermarking of code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1139.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1139/><span class=acl-fixed-case>MASIVE</span>: Open-Ended Affective State Identification in <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>S</span>panish</a></strong><br><a href=/people/n/nicholas-deas/>Nicholas Deas</a>
|
<a href=/people/e/elsbeth-turcan/>Elsbeth Turcan</a>
|
<a href=/people/i/ivan-ernesto-perez-mejia/>Ivan Ernesto Perez Mejia</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1139><div class="card-body p-3 small">In the field of emotion analysis, much NLP research focuses on identifying a limited number of discrete emotion categories, often applied across languages. These basic sets, however, are rarely designed with textual data in mind, and culture, language, and dialect can influence how particular emotions are interpreted. In this work, we broaden our scope to a practically unbounded set of affective states, which includes any terms that humans use to describe their experiences of feeling. We collect and publish MASIVE, a dataset of Reddit posts in English and Spanish containing over 1,000 unique affective states each. We then define the new problem of affective state identification for language generation models framed as a masked span prediction task. On this task, we find that smaller finetuned multilingual models outperform much larger LLMs, even on region-specific Spanish affective states. Additionally, we show that pretraining on MASIVE improves model performance on existing emotion benchmarks. Finally, through machine translation experiments, we find that native speaker-written data is vital to good performance on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1140.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1140/>You Make me Feel like a Natural Question: Training <span class=acl-fixed-case>QA</span> Systems on Transformed Trivia Questions</a></strong><br><a href=/people/t/tasnim-kabir/>Tasnim Kabir</a>
|
<a href=/people/y/yoo-yeon-sung/>Yoo Yeon Sung</a>
|
<a href=/people/s/saptarashmi-bandyopadhyay/>Saptarashmi Bandyopadhyay</a>
|
<a href=/people/h/hao-zou/>Hao Zou</a>
|
<a href=/people/a/abhranil-chandra/>Abhranil Chandra</a>
|
<a href=/people/j/jordan-lee-boyd-graber/>Jordan Lee Boyd-Graber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1140><div class="card-body p-3 small">Training question-answering QA and information retrieval systems for web queries require large, expensive datasets that are difficult to annotate and time-consuming to gather. Moreover, while natural datasets of information-seeking questions are often prone to ambiguity or ill-formed, there are troves of freely available, carefully crafted question datasets for many languages. Thus, we automatically generate shorter, information-seeking questions, resembling web queries in the style of the Natural Questions (NQ) dataset from longer trivia data. Training a QA system on these transformed questions is a viable strategy for alternating to more expensive training setups showing the F1 score difference of less than six points and contrasting the final systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1141.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1141/><span class=acl-fixed-case>A</span>lpha<span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>RA</span>: Assigning <span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>RA</span> Experts Based on Layer Training Quality</a></strong><br><a href=/people/p/peijun-qing/>Peijun Qing</a>
|
<a href=/people/c/chongyang-gao/>Chongyang Gao</a>
|
<a href=/people/y/yefan-zhou/>Yefan Zhou</a>
|
<a href=/people/x/xingjian-diao/>Xingjian Diao</a>
|
<a href=/people/y/yaoqing-yang/>Yaoqing Yang</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1141><div class="card-body p-3 small">Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), are known to enhance training efficiency in Large Language Models (LLMs). Due to the limited parameters of LoRA, recent studies seek to combine LoRA with Mixture-of-Experts (MoE) to boost performance across various tasks. However, inspired by the observed redundancy in traditional MoE structures, prior studies find that LoRA experts within the MoE architecture also exhibit redundancy, suggesting a need to vary the allocation of LoRA experts across different layers. In this paper, we leverage Heavy-Tailed Self-Regularization (HT-SR) Theory to design a fine-grained allocation strategy. Our analysis reveals that the number of experts per layer correlates with layer training quality, which exhibits significant variability across layers. Based on this, we introduce AlphaLoRA, a theoretically principled and training-free method for allocating LoRA experts to reduce redundancy further. Experiments on three models across ten language processing and reasoning benchmarks demonstrate that AlphaLoRA achieves comparable or superior performance over all baselines. Our code is available at https://github.com/morelife2017/alphalora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1142.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1142 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1142/>Flee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling</a></strong><br><a href=/people/i/irfan-robbani/>Irfan Robbani</a>
|
<a href=/people/p/paul-reisert/>Paul Reisert</a>
|
<a href=/people/s/surawat-pothong/>Surawat Pothong</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/c/camelia-guerraoui/>Camélia Guerraoui</a>
|
<a href=/people/w/wenzhi-wang/>Wenzhi Wang</a>
|
<a href=/people/s/shoichi-naito/>Shoichi Naito</a>
|
<a href=/people/j/jungmin-choi/>Jungmin Choi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1142><div class="card-body p-3 small">Prior research in computational argumentation has mainly focused on scoring the quality of arguments, with less attention on explicating logical errors. In this work, we introduce four sets of explainable templates for common informal logical fallacies designed to explicate a fallacy’s implicit logic. Using our templates, we conduct an annotation study on top of 400 fallacious arguments taken from LOGIC dataset and achieve a high agreement score (Krippendorf’s <span class=tex-math>𝛼</span> of 0.54) and reasonable coverage 83%. Finally, we conduct an experiment for detecting the structure of fallacies and discover that state-of-the-art language models struggle with detecting fallacy templates (0.47 accuracy). To facilitate research on fallacies, we make our dataset and guidelines publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1143.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1143.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1143/>Advancing Social Intelligence in <span class=acl-fixed-case>AI</span> Agents: Technical Challenges and Open Questions</a></strong><br><a href=/people/l/leena-mathur/>Leena Mathur</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1143><div class="card-body p-3 small">Building socially-intelligent AI agents (Social-AI) is a multidisciplinary, multimodal research goal that involves creating agents that can sense, perceive, reason about, learn from, and respond to affect, behavior, and cognition of other agents (human or artificial). Progress towards Social-AI has accelerated in the past decade across several computing communities, including natural language processing, machine learning, robotics, human-machine interaction, computer vision, and speech. Natural language processing, in particular, has been prominent in Social-AI research, as language plays a key role in constructing the social world. In this position paper, we identify a set of underlying technical challenges and open questions for researchers across computing communities to advance Social-AI. We anchor our discussion in the context of social intelligence concepts and prior progress in Social-AI research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1144.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1144 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1144/><span class=acl-fixed-case>RA</span>t: Injecting Implicit Bias for Text-To-Image Prompt Refinement Models</a></strong><br><a href=/people/z/ziyi-kou/>Ziyi Kou</a>
|
<a href=/people/s/shichao-pei/>Shichao Pei</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a>
|
<a href=/people/x/xiangliang-zhang/>Xiangliang Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1144><div class="card-body p-3 small">Text-to-image prompt refinement (T2I-Refine) aims to rephrase or extend an input prompt with more descriptive details that can be leveraged to generate images with higher quality. In this paper, we study an adversarial prompt attacking problem for T2I-Refine, where to goal is to implicitly inject specific concept bias to the input prompts during the refinement process so that the generated images, still with higher quality, are explicitly biased to the target group. Our study is motivated by the limitation of current T2I-Refine research that lacks of explorations on the potential capacity of T2I-Refine models to provide prompt refinement service in a biased or advertising manner. To address the limitations, we develop RAt, a prompt refinement and attacking framework that attacks input prompts with intentionally selected adversarial replacements by optimizing a token distribution matrix based on the text-to-image finetuning strategy with a token-level bias obfuscation loss as regularization. We evaluate RAt on a large-scale text-to-image dataset with various concepts as target in both in-domain and transfer-domain scenarios. The evaluation results demonstrate that, compared to other T2I-Refine schemes, RAt is well capable of implicitly attacking input prompts to generate images with higher quality and explicit visual bias towards specific concept group.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1145.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1145.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1145.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1145/>Can <span class=acl-fixed-case>LLM</span> Generate Culturally Relevant Commonsense <span class=acl-fixed-case>QA</span> Data? Case Study in <span class=acl-fixed-case>I</span>ndonesian and <span class=acl-fixed-case>S</span>undanese</a></strong><br><a href=/people/r/rifki-afina-putri/>Rifki Afina Putri</a>
|
<a href=/people/f/faiz-ghifari-haznitrama/>Faiz Ghifari Haznitrama</a>
|
<a href=/people/d/dea-adhista/>Dea Adhista</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1145><div class="card-body p-3 small">Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in 4.5K questions per language (9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally ‘deep’ as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1146.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1146 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1146/>Can Language Models Induce Grammatical Knowledge from Indirect Evidence?</a></strong><br><a href=/people/m/miyu-oba/>Miyu Oba</a>
|
<a href=/people/y/yohei-oseki/>Yohei Oseki</a>
|
<a href=/people/a/akiyo-fukatsu/>Akiyo Fukatsu</a>
|
<a href=/people/a/akari-haga/>Akari Haga</a>
|
<a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a>
|
<a href=/people/s/saku-sugawara/>Saku Sugawara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1146><div class="card-body p-3 small">What kinds of and how much data is necessary for language models to induce grammatical knowledge to judge sentence acceptability? Recent language models still have much room for improvement in their data efficiency compared to humans. This paper investigates whether language models efficiently use indirect data (indirect evidence), from which they infer sentence acceptability. In contrast, humans use indirect evidence efficiently, which is considered one of the inductive biases contributing to efficient language acquisition. To explore this question, we introduce the Wug InDirect Evidence Test (WIDET), a dataset consisting of training instances inserted into the pre-training data and evaluation instances. We inject synthetic instances with newly coined wug words into pretraining data and explore the model’s behavior on evaluation data that assesses grammatical acceptability regarding those words. We prepare the injected instances by varying their levels of indirectness and quantity. Our experiments surprisingly show that language models do not induce grammatical knowledge even after repeated exposure to instances with the same structure but differing only in lexical items from evaluation instances in certain language phenomena. Our findings suggest a potential direction for future research: developing models that use latent indirect evidence to induce grammatical knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1147.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1147/>Do <span class=acl-fixed-case>LLM</span>s Know to Respect Copyright Notice?</a></strong><br><a href=/people/j/jialiang-xu/>Jialiang Xu</a>
|
<a href=/people/s/shenglan-li/>Shenglan Li</a>
|
<a href=/people/z/zhaozhuo-xu/>Zhaozhuo Xu</a>
|
<a href=/people/d/denghui-zhang/>Denghui Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1147><div class="card-body p-3 small">Prior study shows that LLMs sometimes generate content that violates copyright. In this paper, we study another important yet underexplored problem, i.e., will LLMs respect copyright information in user input, and behave accordingly? The research problem is critical, as a negative answer would imply that LLMs will become the primary facilitator and accelerator of copyright infringement behavior. We conducted a series of experiments using a diverse set of language models, user prompts, and copyrighted materials, including books, news articles, API documentation, and movie scripts. Our study offers a conservative evaluation of the extent to which language models may infringe upon copyrights when processing user input containing protected material. This research emphasizes the need for further investigation and the importance of ensuring LLMs respect copyright regulations when handling user input to prevent unauthorized use or reproduction of protected content. We also release a benchmark dataset serving as a test bed for evaluating infringement behaviors by LLMs and stress the need for future alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1148.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1148/><span class=acl-fixed-case>S</span>pec<span class=acl-fixed-case>H</span>ub: Provable Acceleration to Multi-Draft Speculative Decoding</a></strong><br><a href=/people/r/ryan-sun/>Ryan Sun</a>
|
<a href=/people/t/tianyi-zhou/>Tianyi Zhou</a>
|
<a href=/people/x/xun-chen/>Xun Chen</a>
|
<a href=/people/l/lichao-sun/>Lichao Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1148><div class="card-body p-3 small">Large Language Models (LLMs) have become essential in advancing natural language processing (NLP) tasks, but their sequential token generation limits inference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising solution by using a smaller draft model to generate multiple token sequences, which the target LLM verifies in parallel.However, current heuristic approaches, such as Recursive Rejection Sampling (RRS), suffer from low acceptance rates in subsequent drafts, limiting the advantages of using multiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can theoretically improve acceptance rates, but its computational cost is too high for real-time use.We present SpecHub, a novel, efficient sampling-verification method for MDSD that improves acceptance rates with only linear computational overhead. By simplifying the OTM problem into a compact Linear Programming model, SpecHub significantly reduces computational complexity. It further accelerates sampling by leveraging a sparse joint distribution, focusing computation on high-probability token sequences.%It integrates seamlessly into existing MDSD frameworks.In extensive experiments, Spechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step than RRS and RRS without replacement. We attach our code at https://github.com/MasterGodzilla/Speculative_decoding_OT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1149.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1149 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1149.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1149/>Interventional Speech Noise Injection for <span class=acl-fixed-case>ASR</span> Generalizable Spoken Language Understanding</a></strong><br><a href=/people/y/yeonjoon-jung/>YeonJoon Jung</a>
|
<a href=/people/j/jaeseong-lee/>Jaeseong Lee</a>
|
<a href=/people/s/seungtaek-choi/>Seungtaek Choi</a>
|
<a href=/people/d/dohyeon-lee/>Dohyeon Lee</a>
|
<a href=/people/m/minsoo-kim/>Minsoo Kim</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1149><div class="card-body p-3 small">Recently, pre-trained language models (PLMs) have been increasingly adopted in spoken language understanding (SLU). However, automatic speech recognition (ASR) systems frequently produce inaccurate transcriptions, leading to noisy inputs for SLU models, which can significantly degrade their performance. To address this, our objective is to train SLU models to withstand ASR errors by exposing them to noises commonly observed in ASR systems, referred to as ASR-plausible noises. Speech noise injection (SNI) methods have pursued this objective by introducing ASR-plausible noises, but we argue that these methods are inherently biased towards specific ASR systems, or ASR-specific noises. In this work, we propose a novel and less biased augmentation method of introducing the noises that are plausible to any ASR system, by cutting off the non-causal effect of noises. Experimental results and analyses demonstrate the effectiveness of our proposed methods in enhancing the robustness and generalizability of SLU models against unseen ASR systems by introducing more diverse and plausible ASR noises in advance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1150.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1150 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1150/>Rethinking the Role of Proxy Rewards in Language Model Alignment</a></strong><br><a href=/people/s/sungdong-kim/>Sungdong Kim</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1150><div class="card-body p-3 small">Learning from human feedback via proxy reward modeling has been studied to align Large Language Models (LLMs) with human values. However, achieving reliable training through that proxy reward model (RM) is not a trivial problem, and its behavior remained as a black-box. In this paper, we study the role of proxy rewards in the LLM alignment via ‘reverse reward engineering’ by composing interpretable features as a white-box reward function. We aim to replicate the ground truth (gold) reward signal by achieving a monotonic relationship between the proxy and gold reward signals after training the model using the proxy reward in reinforcement learning (RL). Our findings indicate that successfully emulating the gold reward requires generating responses that are relevant with enough length to open-ended questions, while also ensuring response consistency in closed-ended questions. Furthermore, resulting models optimizing our devised white-box reward show competitive performances with strong open-source RMs in alignment benchmarks. We highlight its potential usage as a simple but strong reward baseline for the LLM alignment, not requiring explicit human feedback dataset and RM training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1151.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1151.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1151/>Visual Text Matters: Improving Text-<span class=acl-fixed-case>KVQA</span> with Visual Text Entity Knowledge-aware Large Multimodal Assistant</a></strong><br><a href=/people/a/abhirama-subramanyam-penamakuri/>Abhirama Subramanyam Penamakuri</a>
|
<a href=/people/a/anand-mishra/>Anand Mishra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1151><div class="card-body p-3 small">We revisit knowledge-aware text-based visual question answering, also known as Text-KVQA in the light of modern advancements in large multimodal models (LMMs), and make the following contributions: (i) We propose VisTEL – a principled approach to perform visual text entity linking. The proposed VisTEL module harnesses a state-of-the-art visual text recognition engine and the power of a large multimodal model to jointly reason using textual and visual context obtained using surrounding cues in the image to link visual text entity to the correct knowledge base entity. (ii) We present KaLMA – knowledge-aware large multimodal assistant that augments an LMM with knowledge associated with visual text entity in the image to arrive at an accurate answer. Further, we provide a comprehensive experimental analysis and comparison of our approach with traditional visual question answering, pre-large multimodal models, and large multimodal models, as well as prior top-performing approaches. Averaging over three splits of Text-KVQA, our proposed approach surpasses the previous best approach by a substantial 23.3% on an absolute scale and establishes a new state of the art. We make our implementation publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1152.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1152/>Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics</a></strong><br><a href=/people/s/stefano-perrella/>Stefano Perrella</a>
|
<a href=/people/l/lorenzo-proietti/>Lorenzo Proietti</a>
|
<a href=/people/p/pere-lluis-huguet-cabot/>Pere-Lluís Huguet Cabot</a>
|
<a href=/people/e/edoardo-barba/>Edoardo Barba</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1152><div class="card-body p-3 small">Machine Translation (MT) evaluation metrics assess translation quality automatically. Recently, researchers have employed MT metrics for various new use cases, such as data filtering and translation re-ranking. However, most MT metrics return assessments as scalar scores that are difficult to interpret, posing a challenge to making informed design choices. Moreover, MT metrics’ capabilities have historically been evaluated using correlation with human judgment, which, despite its efficacy, falls short of providing intuitive insights into metric performance, especially in terms of new metric use cases. To address these issues, we introduce an interpretable evaluation framework for MT metrics. Within this framework, we evaluate metrics in two scenarios that serve as proxies for the data filtering and translation re-ranking use cases. Furthermore, by measuring the performance of MT metrics using Precision, Recall, and F-score, we offer clearer insights into their capabilities than correlation with human judgments. Finally, we raise concerns regarding the reliability of manually curated data following the Direct Assessments+Scalar Quality Metrics (DA+SQM) guidelines, reporting a notably low agreement with Multidimensional Quality Metrics (MQM) annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1153.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1153 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1153/><span class=acl-fixed-case>IFC</span>ap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning</a></strong><br><a href=/people/s/soeun-lee/>Soeun Lee</a>
|
<a href=/people/s/si-woo-kim/>Si-Woo Kim</a>
|
<a href=/people/t/taewhan-kim/>Taewhan Kim</a>
|
<a href=/people/d/dong-jin-kim/>Dong-Jin Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1153><div class="card-body p-3 small">Recent advancements in image captioning have explored text-only training methods to overcome the limitations of paired image-text data. However, existing text-only training methods often overlook the modality gap between using text data during training and employing images during inference. To address this issue, we propose a novel approach called Image-like Retrieval, which aligns text features with visually relevant features to mitigate the modality gap. Our method further enhances the accuracy of generated captions by designing a fusion module that integrates retrieved captions with input features. Additionally, we introduce a Frequency-based Entity Filtering technique that significantly improves caption quality. We integrate these methods into a unified framework, which we refer to as IFCap (**I**mage-like Retrieval and **F**requency-based Entity Filtering for Zero-shot **Cap**tioning). Through extensive experimentation, our straightforward yet powerful approach has demonstrated its efficacy, outperforming the state-of-the-art methods by a significant margin in both image captioning and video captioning compared to zero-shot captioning based on text-only training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1154.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1154 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1154.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1154.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1154/>Encoding Spreadsheets for Large Language Models</a></strong><br><a href=/people/h/haoyu-dong/>Haoyu Dong</a>
|
<a href=/people/j/jianbo-zhao/>Jianbo Zhao</a>
|
<a href=/people/y/yuzhang-tian/>Yuzhang Tian</a>
|
<a href=/people/j/junyu-xiong/>Junyu Xiong</a>
|
<a href=/people/m/mengyu-zhou/>Mengyu Zhou</a>
|
<a href=/people/y/yun-lin/>Yun Lin</a>
|
<a href=/people/j/jose-cambronero/>José Cambronero</a>
|
<a href=/people/y/yeye-he/>Yeye He</a>
|
<a href=/people/s/shi-han/>Shi Han</a>
|
<a href=/people/d/dongmei-zhang/>Dongmei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1154><div class="card-body p-3 small">Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SheetEncoder, pioneering an efficient encoding method designed to unleash and optimize LLMs’ powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs’ token constraints, making it impractical for most applications. To tackle this challenge, three innovative modules are proposed to compress spreadsheets effectively: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4’s in-context learning setting. Moreover, fine-tuned LLM with SheetEncoder has an average compression ratio of 25×, but achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%, demonstrating that SheetEncoder greatly boosts LLMs’s performance on spreadsheet data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1155.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1155/>Let’s discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment</a></strong><br><a href=/people/r/rositsa-v-ivanova/>Rositsa V Ivanova</a>
|
<a href=/people/t/thomas-huber/>Thomas Huber</a>
|
<a href=/people/c/christina-niklaus/>Christina Niklaus</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1155><div class="card-body p-3 small">Research in the computational assessment of Argumentation Quality has gained popularity over the last ten years. Various quality dimensions have been explored through the creation of domain-specific datasets and assessment methods. We survey the related literature (211 publications and 32 datasets), while addressing potential overlaps and blurry boundaries to related domains. This paper provides a representative overview of the state of the art in Computational Argument Quality Assessment with a focus on quality dimensions and annotated datasets. The aim of the survey is to identify research gaps and to aid future discussions and work in the domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1156.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1156 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1156/>Automatic sentence segmentation of clinical record narratives in real-world data</a></strong><br><a href=/people/d/dongfang-xu/>Dongfang Xu</a>
|
<a href=/people/d/davy-weissenbacher/>Davy Weissenbacher</a>
|
<a href=/people/k/karen-oconnor/>Karen O’Connor</a>
|
<a href=/people/s/siddharth-rawal/>Siddharth Rawal</a>
|
<a href=/people/g/graciela-gonzalez-hernandez/>Graciela Gonzalez Hernandez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1156><div class="card-body p-3 small">Sentence segmentation is a linguistic task and is widely used as a pre-processing step in many NLP applications. The need for sentence segmentation is particularly pronounced in clinical notes, where ungrammatical and fragmented texts are common. We propose a straightforward and effective sequence labeling classifier to predict sentence spans using a dynamic sliding window based on the prediction of each input sequence. This sliding window algorithm allows our approach to segment long text sequences on the fly. To evaluate our approach, we annotated 90 clinical notes from the MIMIC-III dataset. Additionally, we tested our approach on five other datasets to assess its generalizability and compared its performance against state-of-the-art systems on these datasets. Our approach outperformed all the systems, achieving an F1 score that is 15% higher than the next best-performing system on the clinical dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1157.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1157.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1157/>One-to-Many Communication and Compositionality in Emergent Communication</a></strong><br><a href=/people/h/heeyoung-lee/>Heeyoung Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1157><div class="card-body p-3 small">Compositional languages leverage rules that derive meaning from combinations of simpler constituents. This property is considered to be the hallmark of human language as it enables the ability to express novel concepts and ease of learning. As such, numerous studies in the emergent communication field explore the prerequisite conditions for emergence of compositionality. Most of these studies set out one-to-one communication environment wherein a speaker interacts with a single listener during a single round of communication game. However, real-world communications often involve multiple listeners; their interests may vary and they may even need to coordinate among themselves to be successful at a given task. This work investigates the effects of one-to-many communication environment on emergent languages where a single speaker broadcasts its message to multiple listeners to cooperatively solve a task. We observe that simply broadcasting the speaker’s message to multiple listeners does not induce more compositional languages. We then find and analyze two axes of environmental pressures that facilitate emergence of compositionality: listeners of *different interests* and *coordination* among listeners.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1158.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1158 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1158/><span class=acl-fixed-case>B</span>ayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities</a></strong><br><a href=/people/s/siyin-wang/>Siyin Wang</a>
|
<a href=/people/c/chao-han-huck-yang/>Chao-Han Huck Yang</a>
|
<a href=/people/j/ji-wu/>Ji Wu</a>
|
<a href=/people/c/chao-zhang-tu/>Chao Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1158><div class="card-body p-3 small">Large language models (LLMs) can adapt to new tasks through in-context learning (ICL) based on a few examples presented in dialogue history without any model parameter update. Despite such convenience, the performance of ICL heavily depends on the quality of the in-context examples presented, which makes the in-context example selection approach a critical choice. This paper proposes a novel eBayesian in-Context example Selection method (ByCS) for ICL. Extending the inference probability conditioned on in-context examples based on Bayes’ theorem, ByCS focuses on the inverse inference conditioned on test input. Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), in-context examples are selected based on their inverse inference results. Diverse and extensive cross-tasking and cross-modality experiments are performed with speech, text, and image examples. Experimental results show the efficacy and robustness of our ByCS method on various models, tasks and modalities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1159.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1159 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1159.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1159.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1159/>Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?</a></strong><br><a href=/people/a/alexander-arno-weber/>Alexander Arno Weber</a>
|
<a href=/people/k/klaudia-thellmann/>Klaudia Thellmann</a>
|
<a href=/people/j/jan-ebert/>Jan Ebert</a>
|
<a href=/people/n/nicolas-flores-herr/>Nicolas Flores-Herr</a>
|
<a href=/people/j/jens-lehmann/>Jens Lehmann</a>
|
<a href=/people/m/michael-fromm/>Michael Fromm</a>
|
<a href=/people/m/mehdi-ali/>Mehdi Ali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1159><div class="card-body p-3 small">The adaption of multilingual pre-trained LLMs into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models instruction-tuned on different language compositions on <i>parallel</i> <i>instruction-tuning</i> benchmarks across a selection of the most spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized and a large, multilingual LLMs by instruction-tuning them on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 9.9%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets. Finally, we conduct a human annotation study to understand the alignment between human-based and GPT-4-based evaluation within multilingual chat scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1160.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1160/>Multi-<span class=acl-fixed-case>L</span>ogi<span class=acl-fixed-case>E</span>val: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models</a></strong><br><a href=/people/n/nisarg-patel/>Nisarg Patel</a>
|
<a href=/people/m/mohith-kulkarni/>Mohith Kulkarni</a>
|
<a href=/people/m/mihir-parmar/>Mihir Parmar</a>
|
<a href=/people/a/aashna-budhiraja/>Aashna Budhiraja</a>
|
<a href=/people/m/mutsumi-nakamura/>Mutsumi Nakamura</a>
|
<a href=/people/n/neeraj-varshney/>Neeraj Varshney</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1160><div class="card-body p-3 small">As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning. Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multi-step reasoning with a limited set of inference rules. Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning. To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multi-step logical reasoning with various inference rules and depths. Multi-LogiEval covers three logic types — propositional, first-order, and non-monotonic consisting of more than 30 inference rules and more than 60 of their combinations with various depths. Leveraging this dataset, we conduct evaluations on a range of LLMs such as GPT-4, ChatGPT, Gemini-Pro, Orca, and Mistral, employing a zero-shot chain-of-thought. Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5). We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings. We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1161.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1161 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1161/>Linear Layer Extrapolation for Fine-Grained Emotion Classification</a></strong><br><a href=/people/m/mayukh-sharma/>Mayukh Sharma</a>
|
<a href=/people/s/sean-obrien/>Sean O’Brien</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1161><div class="card-body p-3 small">Certain abilities of Transformer-based language models consistently emerge in their later layers. Previous research has leveraged this phenomenon to improve factual accuracy through self-contrast, penalizing early-exit predictions based on the premise that later-layer updates are more factually reliable than earlier-layer associations. We observe a similar pattern for fine-grained emotion classification in text, demonstrating that self-contrast can enhance encoder-based text classifiers. Additionally, we reinterpret self-contrast as a form of linear extrapolation, which motivates a refined approach that dynamically adjusts the contrastive strength based on the selected intermediate layer. Experiments across multiple models and emotion classification datasets show that our method outperforms standard classification techniques in fine-grained emotion classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1162.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1162/>Task Oriented In-Domain Data Augmentation</a></strong><br><a href=/people/x/xiao-liang/>Xiao Liang</a>
|
<a href=/people/x/xinyu-hu/>Xinyu Hu</a>
|
<a href=/people/s/simiao-zuo/>Simiao Zuo</a>
|
<a href=/people/y/yeyun-gong/>Yeyun Gong</a>
|
<a href=/people/q/qiang-lou/>Qiang Lou</a>
|
<a href=/people/y/yi-liu/>Yi Liu</a>
|
<a href=/people/s/shao-lun-huang/>Shao-Lun Huang</a>
|
<a href=/people/j/jian-jiao/>Jian Jiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1162><div class="card-body p-3 small">Large Language Models (LLMs) have shown superior performance in various applications and fields. To achieve better performance on specialized domains such as law and advertisement, LLMs are often continue pre-trained on in-domain data. However, existing approaches suffer from two major issues. First, in-domain data are scarce compared with general domain-agnostic data. Second, data used for continual pre-training are not task-aware, such that they may not be helpful to downstream applications. We propose TRAIT, a task-oriented in-domain data augmentation framework. Our framework is divided into two parts: in-domain data selection and task-oriented synthetic passage generation. The data selection strategy identifies and selects a large amount of in-domain data from general corpora, and thus significantly enriches domain knowledge in the continual pre-training data. The synthetic passages contain guidance on how to use domain knowledge to answer questions about downstream tasks. By training on such passages, the model aligns with the need of downstream applications. We adapt LLMs to two domains: advertisement and math. On average, TRAIT improves LLM performance by 8% in the advertisement domain and 7.5% in the math domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1163.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1163 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1163/><span class=acl-fixed-case>S</span>ci<span class=acl-fixed-case>DQA</span>: A Deep Reading Comprehension Dataset over Scientific Papers</a></strong><br><a href=/people/s/shruti-singh/>Shruti Singh</a>
|
<a href=/people/n/nandan-sarkar/>Nandan Sarkar</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1163><div class="card-body p-3 small">Scientific literature is typically dense, requiring significant background knowledge and deep comprehension for effective engagement. We introduce SciDQA, a new dataset for reading comprehension that challenges language models to deeply understand scientific articles, consisting of 2,937 QA pairs. Unlike other scientific QA datasets, SciDQA sources questions from peer reviews by domain experts and answers by paper authors, ensuring a thorough examination of the literature. We enhance the dataset’s quality through a process that carefully decontextualizes the content, tracks the source document across different versions, and incorporates a bibliography for multi-document question-answering. Questions in SciDQA necessitate reasoning across figures, tables, equations, appendices, and supplementary materials, and require multi-document reasoning. We evaluate several open-source and proprietary LLMs across various configurations to explore their capabilities in generating relevant and factual responses, as opposed to simple review memorization. Our comprehensive evaluation, based on metrics for surface-level and semantic similarity, highlights notable performance discrepancies. SciDQA represents a rigorously curated, naturally derived scientific QA dataset, designed to facilitate research on complex reasoning within the domain of question answering for scientific texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1164.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1164/>Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules</a></strong><br><a href=/people/z/zhuocheng-gong/>Zhuocheng Gong</a>
|
<a href=/people/a/ang-lv/>Ang Lv</a>
|
<a href=/people/j/jian-guan/>Jian Guan</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/h/huishuai-zhang/>Huishuai Zhang</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1164><div class="card-body p-3 small">Is it always necessary to compute tokens from shallow to deep layers in Transformers? The continued success of vanilla Transformers and their variants suggests an undoubted “yes”. In this work, however, we attempt to break the depth-ordered convention by proposing a novel architecture dubbed mixture-of-modules (MoM), which is motivated by an intuition that any layer, regardless of its position, can be used to compute a token as long as it possesses the needed processing capabilities. The construction of MoM starts from a finite set of modules defined by multi-head attention and feed-forward networks, each distinguished by its unique parameterization. Two routers then iteratively select attention modules and feed-forward modules from the set to process a token. The selection dynamically expands the computation graph in the forward pass of the token, culminating in an assembly of modules. We show that MoM provides not only a unified framework for Transformers and their numerous variants but also a flexible and learnable approach for reducing redundancy in Transformer parameterization. We pre-train various MoMs using OpenWebText. Empirical results demonstrate that MoMs, of different sizes, consistently outperform vanilla transformers. More interestingly, after removing 50% of the multi-head attention modules and 25% of the feed-forward modules, an MoM model still holds comparable performance. Additionally, by properly adjusting the number of modules and compressing the model depth, one can have an MoM that achieves comparable performance to GPT-2 (774M) while saving 16% TFLOPs and 42% memory usage during forward computation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1165.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1165 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1165/>No Culture Left Behind: <span class=acl-fixed-case>A</span>rt<span class=acl-fixed-case>EL</span>ingo-28, a Benchmark of <span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>A</span>rt with Captions in 28 Languages</a></strong><br><a href=/people/y/youssef-mohamed/>Youssef Mohamed</a>
|
<a href=/people/r/runjia-li/>Runjia Li</a>
|
<a href=/people/i/ibrahim-said-ahmad/>Ibrahim Said Ahmad</a>
|
<a href=/people/k/kilichbek-haydarov/>Kilichbek Haydarov</a>
|
<a href=/people/p/philip-torr/>Philip Torr</a>
|
<a href=/people/k/kenneth-church/>Kenneth Church</a>
|
<a href=/people/m/mohamed-elhoseiny/>Mohamed Elhoseiny</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1165><div class="card-body p-3 small">Research in vision and language has made considerable progress thanks to benchmarks such as COCO. COCO captions focused on unambiguous facts in English; ArtEmis introduced subjective emotions and ArtELingo introduced some multilinguality (Chinese and Arabic). However we believe there should be more multilinguality. Hence, we present ArtELingo-28, a vision-language benchmark that spans 28 languages and encompasses approximately 200,000 annotations (140 annotations per image). Traditionally, vision research focused on unambiguous class labels, whereas ArtELingo-28 emphasizes diversity of opinions over languages and cultures. The challenge is to build machine learning systems that assign emotional captions to images. Baseline results will be presented for three novel conditions: Zero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual transfer is more successful for culturally-related languages. Data and code will be made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1166.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1166 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1166/><span class=acl-fixed-case>PREDICT</span>: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection</a></strong><br><a href=/people/s/someen-park/>Someen Park</a>
|
<a href=/people/j/jaehoon-kim/>Jaehoon Kim</a>
|
<a href=/people/s/seungwan-jin/>Seungwan Jin</a>
|
<a href=/people/s/sohyun-park/>Sohyun Park</a>
|
<a href=/people/k/kyungsik-han/>Kyungsik Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1166><div class="card-body p-3 small">While a few public benchmarks have been proposed for training hate speech detection models, the differences in labeling criteria between these benchmarks pose challenges for generalized learning, limiting the applicability of the models. Previous research has presented methods to generalize models through data integration or augmentation, but overcoming the differences in labeling criteria between datasets remains a limitation. To address these challenges, we propose PREDICT, a novel framework that uses the notion of multi-agent for hate speech detection. PREDICT consists of two phases: (1) PRE (Perspective-based REasoning): Multiple agents are created based on the induced labeling criteria of given datasets, and each agent generates stances and reasons; (2) DICT (Debate using InCongruenT references): Agents representing hate and non-hate stances conduct the debate, and a judge agent classifies hate or non-hate and provides a balanced reason. Experiments on five representative public benchmarks show that PREDICT achieves superior cross-evaluation performance compared to methods that focus on specific labeling criteria or majority voting methods. Furthermore, we validate that PREDICT effectively mediates differences between agents’ opinions and appropriately incorporates minority opinions to reach a consensus. Our code is available at https://github.com/Hanyang-HCC-Lab/PREDICT</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1167.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1167 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1167/><span class=acl-fixed-case>T</span>oken<span class=acl-fixed-case>V</span>erse: Towards Unifying Speech and <span class=acl-fixed-case>NLP</span> Tasks via Transducer-based <span class=acl-fixed-case>ASR</span></a></strong><br><a href=/people/s/shashi-kumar/>Shashi Kumar</a>
|
<a href=/people/s/srikanth-madikeri/>Srikanth Madikeri</a>
|
<a href=/people/j/juan-pablo-zuluaga-gomez/>Juan Pablo Zuluaga Gomez</a>
|
<a href=/people/i/iuliia-thorbecke/>Iuliia Thorbecke</a>
|
<a href=/people/e/esau-villatoro-tello/>Esaú Villatoro-tello</a>
|
<a href=/people/s/sergio-burdisso/>Sergio Burdisso</a>
|
<a href=/people/p/petr-motlicek/>Petr Motlicek</a>
|
<a href=/people/k/karthik-pandia-d-s/>Karthik Pandia D S</a>
|
<a href=/people/a/aravind-ganapathiraju/>Aravind Ganapathiraju</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1167><div class="card-body p-3 small">In traditional conversational intelligence from speech, a cascaded pipeline is used, involving tasks such as voice activity detection, diarization, transcription, and subsequent processing with different NLP models for tasks like semantic endpointing and named entity recognition (NER). Our paper introduces TokenVerse, a single Transducer-based model designed to handle multiple tasks. This is achieved by integrating task-specific tokens into the reference text during ASR model training, streamlining the inference and eliminating the need for separate NLP models. In addition to ASR, we conduct experiments on 3 different tasks: speaker change detection, endpointing, and NER. Our experiments on a public and a private dataset show that the proposed method improves ASR by up to 7.7% in relative WER while outperforming the cascaded pipeline approach in individual task performance. Our code is publicly available: https://github.com/idiap/tokenverse-unifying-speech-nlp</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1168.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1168 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1168/><span class=acl-fixed-case>A</span>pi<span class=acl-fixed-case>Q</span>: Finetuning of 2-Bit Quantized Large Language Model</a></strong><br><a href=/people/b/baohao-liao/>Baohao Liao</a>
|
<a href=/people/c/christian-herold/>Christian Herold</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1168><div class="card-body p-3 small">Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the effectiveness of these methods compared to full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing the LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM’s activation precision while mitigating the error propagation from shallower into deeper layers. Through comprehensive evaluations conducted on a spectrum of language tasks with various LLMs, ApiQ demonstrably minimizes activation error during quantization. Consequently, it consistently achieves superior finetuning results across various bit-widths. Notably, one can even finetune a 2-bit Llama-2-70b with ApiQ on a single NVIDIA A100-80GB GPU without any memory-saving techniques, and achieve promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1169.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1169/>Memorize Step by Step: Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk</a></strong><br><a href=/people/z/zhiyuan-zeng/>Zhiyuan Zeng</a>
|
<a href=/people/q/qipeng-guo/>Qipeng Guo</a>
|
<a href=/people/x/xiaoran-liu/>Xiaoran Liu</a>
|
<a href=/people/z/zhangyue-yin/>Zhangyue Yin</a>
|
<a href=/people/w/wentao-shu/>Wentao Shu</a>
|
<a href=/people/m/mianqiu-huang/>Mianqiu Huang</a>
|
<a href=/people/b/bo-wang/>Bo Wang</a>
|
<a href=/people/y/yunhua-zhou/>Yunhua Zhou</a>
|
<a href=/people/l/linlin-li/>Linlin Li</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1169><div class="card-body p-3 small">The evolution of Large Language Models (LLMs) has led to significant advancements, with models like Claude and Gemini capable of processing contexts up to 1 million tokens. However, efficiently handling long sequences remains challenging, particularly during the prefilling stage when input lengths exceed GPU memory capacity. Traditional methods often segment sequence into chunks and compress them iteratively with fixed-size memory. However, our empirical analysis shows that the fixed-size memory results in wasted computational and GPU memory resources. Therefore, we introduces Incremental Memory (IM), a method that starts with a small memory size and gradually increases it, optimizing computational efficiency. Additionally, we propose Decremental Chunk based on Incremental Memory (IMDC), which reduces chunk size while increasing memory size, ensuring stable and lower GPU memory usage. Our experiments demonstrate that IMDC is consistently faster (1.45x) and reduces GPU memory consumption by 23.3% compared to fixed-size memory, achieving comparable performance on the LongBench Benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1170.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1170/>A Morphology-Based Investigation of Positional Encodings</a></strong><br><a href=/people/p/poulami-ghosh/>Poulami Ghosh</a>
|
<a href=/people/s/shikhar-vashishth/>Shikhar Vashishth</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1170><div class="card-body p-3 small">Contemporary deep learning models effectively handle languages with diverse morphology despite not being directly integrated into them. Morphology and word order are closely linked, with the latter incorporated into transformer-based models through positional encodings. This prompts a fundamental inquiry: Is there a correlation between the morphological complexity of a language and the utilization of positional encoding in pre-trained language models? In pursuit of an answer, we present the first study addressing this question, encompassing 22 languages and 5 downstream tasks. Our findings reveal that the importance of positional encoding diminishes with increasing morphological complexity in languages. Our study motivates the need for a deeper understanding of positional encoding, augmenting them to better reflect the different languages under consideration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1171.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1171 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1171/><span class=acl-fixed-case>I</span> love pineapple on pizza != <span class=acl-fixed-case>I</span> hate pineapple on pizza: Stance-Aware Sentence Transformers for Opinion Mining</a></strong><br><a href=/people/v/vahid-ghafouri/>Vahid Ghafouri</a>
|
<a href=/people/j/jose-such/>Jose Such</a>
|
<a href=/people/g/guillermo-suarez-tangil/>Guillermo Suarez-Tangil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1171><div class="card-body p-3 small">Sentence transformers excel at grouping topically similar texts, but struggle to differentiate opposing viewpoints on the same topic. This shortcoming hinders their utility in applications where understanding nuanced differences in opinion is essential, such as those related to social and political discourse analysis. This paper addresses this issue by fine-tuning sentence transformers with arguments for and against human-generated controversial claims. We demonstrate how our fine-tuned model enhances the utility of sentence transformers for social computing tasks such as opinion mining and stance detection. We elaborate that applying stance-aware sentence transformers to opinion mining is a more computationally efficient and robust approach in comparison to the classic classification-based approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1172.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1172 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1172/><span class=acl-fixed-case>B</span>ias<span class=acl-fixed-case>W</span>ipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability</a></strong><br><a href=/people/m/mamta-mamta/>Mamta Mamta</a>
|
<a href=/people/r/rishikant-chigrupaatii/>Rishikant Chigrupaatii</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1172><div class="card-body p-3 small">Toxic content detection plays a vital role in addressing the misuse of social media platforms to harm people or groups due to their race, gender or ethnicity. However, due to the nature of the datasets, systems develop an unintended bias due to the over-generalization of the model to the training data. This compromises the fairness of the systems, which can impact certain groups due to their race, gender, etc.Existing methods mitigate bias using data augmentation, adversarial learning, etc., which require re-training and adding extra parameters to the model.In this work, we present a robust and generalizable technique <i>BiasWipe</i> to mitigate unintended bias in language models. <i>BiasWipe</i> utilizes model interpretability using Shapley values, which achieve fairness by pruning the neuron weights responsible for unintended bias. It first identifies the neuron weights responsible for unintended bias and then achieves fairness by pruning them without loss of original performance. It does not require re-training or adding extra parameters to the model. To show the effectiveness of our proposed technique for bias unlearning, we perform extensive experiments for Toxic content detection for BERT, RoBERTa, and GPT models. .</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1173.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1173.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1173.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1173/><span class=acl-fixed-case>A</span>r<span class=acl-fixed-case>M</span>eme: Propagandistic Content in <span class=acl-fixed-case>A</span>rabic Memes</a></strong><br><a href=/people/f/firoj-alam/>Firoj Alam</a>
|
<a href=/people/a/abul-hasnat/>Abul Hasnat</a>
|
<a href=/people/f/fatema-ahmad/>Fatema Ahmad</a>
|
<a href=/people/m/md-arid-hasan/>Md. Arid Hasan</a>
|
<a href=/people/m/maram-hasanain/>Maram Hasanain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1173><div class="card-body p-3 small">With the rise of digital communication memes have become a significant medium for cultural and political expression that is often used to mislead audience. Identification of such misleading and persuasive multimodal content become more important among various stakeholders, including social media platforms, policymakers, and the broader society as they often cause harm to the individuals, organizations and/or society. While there has been effort to develop AI based automatic system for resource rich languages (e.g., English), it is relatively little to none for medium to low resource languages. In this study, we focused on developing an Arabic memes dataset with manual annotations of propagandistic content. We annotated <span class=tex-math>∼6K</span> Arabic memes collected from various social media platforms, which is a first resource for Arabic multimodal research. We provide a comprehensive analysis aiming to develop computational tools for their detection. We made the dataset publicly available for the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1174.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1174.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1174/>Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic Reasoning with Argumentation Theory-Driven Prompts</a></strong><br><a href=/people/a/arianna-muti/>Arianna Muti</a>
|
<a href=/people/f/federico-ruggeri/>Federico Ruggeri</a>
|
<a href=/people/k/khalid-al-khatib/>Khalid Al Khatib</a>
|
<a href=/people/a/alberto-barron-cedeno/>Alberto Barrón-Cedeño</a>
|
<a href=/people/t/tommaso-caselli/>Tommaso Caselli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1174><div class="card-body p-3 small">We propose misogyny detection as an Argumentative Reasoning task and we investigate the capacity of large language models (LLMs) to understand the implicit reasoning used to convey misogyny in both Italian and English. The central aim is to generate the missing reasoning link between a message and the implied meanings encoding the misogyny. Our study uses argumentation theory as a foundation to form a collection of prompts in both zero-shot and few-shot settings. These prompts integrate different techniques, including chain-of-thought reasoning and augmented knowledge. Our findings show that LLMs fall short on reasoning capabilities about misogynistic comments and that they mostly rely on their implicit knowledge derived from internalized common stereotypes about women to generate implied assumptions, rather than on inductive reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1175.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1175 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1175/>Thoughts to Target: Enhance Planning for Target-driven Conversation</a></strong><br><a href=/people/z/zhonghua-zheng/>Zhonghua Zheng</a>
|
<a href=/people/l/lizi-liao/>Lizi Liao</a>
|
<a href=/people/y/yang-deng/>Yang Deng</a>
|
<a href=/people/e/ee-peng-lim/>Ee-Peng Lim</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/l/liqiang-nie/>Liqiang Nie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1175><div class="card-body p-3 small">In conversational AI, large-scale models excel in various tasks but struggle with target-driven conversation planning. Current methods, such as chain-of-thought reasoning and tree-search policy learning techniques, either neglect plan rationality or require extensive human simulation procedures. Addressing this, we propose a novel two-stage framework, named EnPL, to improve the LLMs’ capability in planning conversations towards designated targets, including (1) distilling natural language plans from target-driven conversation corpus and (2) generating new plans with demonstration-guided in-context learning. Specifically, we first propose a filter approach to distill a high-quality plan dataset, ConvPlan (Resources of this paper can be found at https://github.com/pandazzh2020/ConvPlan). With the aid of corresponding conversational data and support from relevant knowledge bases, we validate the quality and rationality of these plans. Then, these plans are leveraged to help guide LLMs to further plan for new targets. Empirical results demonstrate that our method significantly improves the planning ability of LLMs, especially in target-driven conversations. Furthermore, EnPL is demonstrated to be quite effective in collecting target-driven conversation datasets and enhancing response generation, paving the way for constructing extensive target-driven conversational models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1176.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1176/>Scalable Data Ablation Approximations for Language Models through Modular Training and Merging</a></strong><br><a href=/people/c/clara-na/>Clara Na</a>
|
<a href=/people/i/ian-magnusson/>Ian Magnusson</a>
|
<a href=/people/a/ananya-harsh-jha/>Ananya Harsh Jha</a>
|
<a href=/people/t/tom-sherborne/>Tom Sherborne</a>
|
<a href=/people/e/emma-strubell/>Emma Strubell</a>
|
<a href=/people/j/jesse-dodge/>Jesse Dodge</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1176><div class="card-body p-3 small">Training data compositions for Large Language Models (LLMs) can significantly affect their downstream performance. However, a thorough data ablation study exploring large sets of candidate data mixtures is typically prohibitively expensive since the full effect is seen only after training the models; this can lead practitioners to settle for sub-optimal data mixtures. We propose an efficient method for approximating data ablations which trains individual models on subsets of a training corpus and reuses them across evaluations of combinations of subsets.In continued pre-training experiments, we find that, given an arbitrary evaluation set, the perplexity score of a single model trained on a candidate set of data is strongly correlated with perplexity scores of parameter averages of models trained on distinct partitions of that data. From this finding, we posit that researchers and practitioners can conduct inexpensive simulations of data ablations by maintaining a pool of models that were each trained on partitions of a large training corpus, and assessing candidate data mixtures by evaluating parameter averages of combinations of these models. This approach allows for substantial improvements in amortized training efficiency – scaling only linearly with respect to new data – by enabling reuse of previous training computation, opening new avenues for improving model performance through rigorous, incremental data assessment and mixing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1177.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1177 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1177/>Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation</a></strong><br><a href=/people/z/zhe-cao/>Zhe Cao</a>
|
<a href=/people/z/zhi-qu/>Zhi Qu</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1177><div class="card-body p-3 small">Multilingual neural machine translation models support fine-tuning hundreds of languages simultaneously. However, fine-tuning on full parameters solely is inefficient potentially leading to negative interactions among languages. In this work, we demonstrate that the fine-tuning for a language occurs in its intrinsic language-specific subspace with a tiny fraction of entire parameters. Thus, we propose language-specific LoRA to isolate intrinsic language-specific subspaces. Furthermore, we propose architecture learning techniques and introduce a gradual pruning schedule during fine-tuning to exhaustively explore the optimal setting and the minimal intrinsic subspaces for each language, resulting in a lightweight yet effective fine-tuning procedure. The experimental results on a 12-language subset and a 30-language subset of FLORES-101 show that our methods not only outperform full-parameter fine-tuning up to 2.25 spBLEU scores but also reduce trainable parameters to 0.4% for high and medium-resource languages and 1.6% for low-resource ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1178.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1178/>Attention Score is not All You Need for Token Importance Indicator in <span class=acl-fixed-case>KV</span> Cache Reduction: Value Also Matters</a></strong><br><a href=/people/z/zhiyu-guo/>Zhiyu Guo</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1179.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1179 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1179/>Generative Subgraph Retrieval for Knowledge Graph–Grounded Dialog Generation</a></strong><br><a href=/people/j/jinyoung-park/>Jinyoung Park</a>
|
<a href=/people/m/minseok-joo/>Minseok Joo</a>
|
<a href=/people/j/joo-kyung-kim/>Joo-Kyung Kim</a>
|
<a href=/people/h/hyunwoo-j-kim/>Hyunwoo J. Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1179><div class="card-body p-3 small">Knowledge graph–grounded dialog generation requires retrieving a dialog-relevant subgraph from the given knowledge base graph and integrating it with the dialog history. Previous works typically represent the graph using an external encoder, such as graph neural networks, and retrieve relevant triplets based on the similarity between single-vector representations of triplets and the dialog history. However, these external encoders fail to leverage the rich knowledge of pretrained language models, and the retrieval process is also suboptimal due to the information bottleneck caused by the single-vector abstraction of the dialog history. In this work, we propose Dialog generation with Generative Subgraph Retrieval (DialogGSR), which retrieves relevant knowledge subgraphs by directly generating their token sequences on top of language models. For effective generative subgraph retrieval, we introduce two key methods: (i) structure-aware knowledge graph linearization with self-supervised graph-specific tokens and (ii) graph-constrained decoding utilizing graph structural proximity-based entity informativeness scores for valid and relevant generative retrieval. DialogGSR achieves state-of-the-art performance in knowledge graph–grounded dialog generation, as demonstrated on OpenDialKG and KOMODIS datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1180.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1180 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1180/>Adapters Mixup: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers</a></strong><br><a href=/people/t/tuc-van-nguyen/>Tuc Van Nguyen</a>
|
<a href=/people/t/thai-le/>Thai Le</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1180><div class="card-body p-3 small">Existing works show that augmenting the training data of pre-trained language models (PLMs) for classification tasks fine-tuned via parameter-efficient fine-tuning methods (PEFT) using both clean and adversarial examples can enhance their robustness under adversarial attacks. However, this adversarial training paradigm often leads to performance degradation on clean inputs and requires frequent re-training on the entire data to account for new, unknown attacks. To overcome these challenges while still harnessing the benefits of adversarial training and the efficiency of PEFT, this work proposes a novel approach, called AdpMixup, that combines two paradigms: (1) fine-tuning through adapters and (2) adversarial augmentation via mixup to dynamically leverage existing knowledge from a set of pre-known attacks for robust inference. Intuitively, AdpMixup fine-tunes PLMs with multiple adapters with both clean and pre-known adversarial examples and intelligently mixes them up in different ratios during prediction. Our experiments show AdpMixup achieves the best trade-off between training efficiency and robustness under both pre-known and unknown attacks, compared to existing baselines on five downstream tasks across six varied black-box attacks and 2 PLMs. The code is available at https://github.com/nguyentuc/adapters_mixup.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1181.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1181.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1181/>Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using <span class=acl-fixed-case>GPT</span>-4</a></strong><br><a href=/people/w/woojin-kim/>Woojin Kim</a>
|
<a href=/people/s/sungeun-hahm/>Sungeun Hahm</a>
|
<a href=/people/j/jaejin-lee/>Jaejin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1181><div class="card-body p-3 small">De-identification (de-ID) refers to removing the association between a set of identifying data and the data subject. In clinical data management, the de-ID of Protected Health Information (PHI) is critical for patient confidentiality. However, state-of-the-art de-ID models show poor generalization on a new dataset. This is due to the difficulty of retaining training corpora. Additionally, labeling standards and the formats of patient records vary across different institutions. Our study addresses these issues by exploiting GPT-4 for data augmentation through one-shot and zero-shot prompts. Our approach effectively circumvents the problem of PHI leakage, ensuring privacy by redacting PHI before processing. To evaluate the effectiveness of our proposal, we conduct cross-dataset testing. The experimental result demonstrates significant improvements across three types of F1 scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1182.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1182.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1182/>Connecting the Dots: Evaluating Abstract Reasoning Capabilities of <span class=acl-fixed-case>LLM</span>s Using the <span class=acl-fixed-case>N</span>ew <span class=acl-fixed-case>Y</span>ork <span class=acl-fixed-case>T</span>imes Connections Word Game</a></strong><br><a href=/people/p/prisha-samdarshi/>Prisha Samdarshi</a>
|
<a href=/people/m/mariam-mustafa/>Mariam Mustafa</a>
|
<a href=/people/a/anushka-kulkarni/>Anushka Kulkarni</a>
|
<a href=/people/r/raven-rothkopf/>Raven Rothkopf</a>
|
<a href=/people/t/tuhin-chakrabarty/>Tuhin Chakrabarty</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1182><div class="card-body p-3 small">The New York Times Connections game has emerged as a popular and challenging pursuit for word puzzle enthusiasts. We collect438 Connections games to evaluate the performance of state-of-the-art large language models (LLMs) against expert and novice humanplayers. Our results show that even the best-performing LLM, Claude 3.5 Sonnet, which has otherwise shown impressive reasoning abilities on a wide variety of benchmarks, can only fully solve 18% of the games. Novice and expert players perform better than Claude 3.5 Sonnet, with expert human players significantly outperforming it. We create a taxonomy of the knowledge types required to successfully cluster and categorize words in the Connections game. We find that while LLMs are decent at categorizing words based on semantic relations they struggle with other types of knowledge such as Encyclopedic Knowledge, Multiword Expressions or knowledge that combines both Word Form and Meaning. Our results establish the New York Times Connections game as a challenging benchmark for evaluating abstract reasoning capabilities in humans and AI systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1183.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1183/><span class=acl-fixed-case>G</span>ott<span class=acl-fixed-case>BERT</span>: a pure <span class=acl-fixed-case>G</span>erman Language Model</a></strong><br><a href=/people/r/raphael-scheible/>Raphael Scheible</a>
|
<a href=/people/j/johann-frei/>Johann Frei</a>
|
<a href=/people/f/fabian-thomczyk/>Fabian Thomczyk</a>
|
<a href=/people/h/henry-he/>Henry He</a>
|
<a href=/people/p/patric-tippmann/>Patric Tippmann</a>
|
<a href=/people/j/jochen-knaus/>Jochen Knaus</a>
|
<a href=/people/v/victor-jaravine/>Victor Jaravine</a>
|
<a href=/people/f/frank-kramer/>Frank Kramer</a>
|
<a href=/people/m/martin-boeker/>Martin Boeker</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1184.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1184/>Computational Meme Understanding: A Survey</a></strong><br><a href=/people/k/khoi-p-n-nguyen/>Khoi P. N. Nguyen</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1184><div class="card-body p-3 small">Computational Meme Understanding, which concerns the automated comprehension of memes, has garnered interest over the last four years and is facing both substantial opportunities and challenges. We survey this emerging area of research by first introducing a comprehensive taxonomy for memes along three dimensions – forms, functions, and topics. Next, we present three key tasks in Computational Meme Understanding, namely, classification, interpretation, and explanation, and conduct a comprehensive review of existing datasets and models, discussing their limitations. Finally, we highlight the key challenges and recommend avenues for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1185.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1185.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1185/><span class=acl-fixed-case>C</span>over<span class=acl-fixed-case>ICL</span>: Selective Annotation for In-Context Learning via Active Graph Coverage</a></strong><br><a href=/people/c/costas-mavromatis/>Costas Mavromatis</a>
|
<a href=/people/b/balasubramaniam-srinivasan/>Balasubramaniam Srinivasan</a>
|
<a href=/people/z/zhengyuan-shen/>Zhengyuan Shen</a>
|
<a href=/people/j/jiani-zhang/>Jiani Zhang</a>
|
<a href=/people/h/huzefa-rangwala/>Huzefa Rangwala</a>
|
<a href=/people/c/christos-faloutsos/>Christos Faloutsos</a>
|
<a href=/people/g/george-karypis/>George Karypis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1185><div class="card-body p-3 small">In-context learning (ICL) adapts Large Language Models (LLMs) to new tasks, without requiring any parameter updates, but few annotated examples as input. In this work, we investigate selective annotation for ICL, where there is a limited budget for annotating examples, similar to low-budget active learning (AL). Although uncertainty-based selection is unreliable with few annotated data, we present CoverICL, an adaptive graph-based selection algorithm, that effectively incorporates uncertainty sampling into selective annotation for ICL. First, CoverICL builds a nearest-neighbor graph based on the semantic similarity between candidate ICL examples. Then, CoverICL employs uncertainty estimation by the LLM to identify hard examples for the task. Selective annotation is performed over the active graph of the hard examples, adapting the process to the particular LLM used and the task tackled. CoverICL selects the most representative examples by solving a Maximum Coverage problem, approximating diversity-based sampling. Extensive experiments on ten datasets and seven LLMs show that, by incorporating uncertainty via coverage on the active graph, CoverICL (1) outperforms existing AL methods for ICL by 2–4.6% accuracy points, (2) is up to 2x more budget-efficient than SOTA methods for low-budget AL, and (3) generalizes better across tasks compared to non-graph alternatives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1186.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1186 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1186/>Retrieval-enriched zero-shot image classification in low-resource domains</a></strong><br><a href=/people/n/nicola-dallasen/>Nicola Dall’Asen</a>
|
<a href=/people/y/yiming-wang/>Yiming Wang</a>
|
<a href=/people/e/enrico-fini/>Enrico Fini</a>
|
<a href=/people/e/elisa-ricci/>Elisa Ricci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1186><div class="card-body p-3 small">Low-resource domains, characterized by scarce data and annotations, present significant challenges for language and visual understanding tasks, with the latter much under-explored in the literature. Recent advancements in Vision-Language Models (VLM) have shown promising results in high-resource domains but fall short in low-resource concepts that are under-represented (e.g. only a handful of images per category) in the pre-training set. We tackle the challenging task of zero-shot low-resource image classification from a novel perspective. By leveraging a retrieval-based strategy, we achieve this in a training-free fashion. Specifically, our method, named CoRE (Combination of Retrieval Enrichment), enriches the representation of both query images and class prototypes by retrieving relevant textual information from large web-crawled databases. This retrieval-based enrichment significantly boosts classification performance by incorporating the broader contextual information relevant to the specific class. We validate our method on a newly established benchmark covering diverse low-resource domains, including medical imaging, rare plants, and circuits. Our experiments demonstrate that CoRE outperforms existing state-of-the-art methods that rely on synthetic data generation and model fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1187.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1187 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1187/><span class=acl-fixed-case>I</span>-<span class=acl-fixed-case>AM</span>-<span class=acl-fixed-case>G</span>: Interest Augmented Multimodal Generator for Item Personalization</a></strong><br><a href=/people/x/xianquan-wang/>Xianquan Wang</a>
|
<a href=/people/l/likang-wu/>Likang Wu</a>
|
<a href=/people/s/shukang-yin/>Shukang Yin</a>
|
<a href=/people/z/zhi-li/>Zhi Li</a>
|
<a href=/people/y/yanjiang-chen/>Yanjiang Chen</a>
|
<a href=/people/h/hufeng-hufeng/>Hufeng Hufeng</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/q/qi-liu/>Qi Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1187><div class="card-body p-3 small">The emergence of personalized generation has made it possible to create texts or images that meet the unique needs of users. Recent advances mainly focus on style or scene transfer based on given keywords. However, in e-commerce and recommender systems, it is almost an untouched area to explore user historical interactions, automatically mine user interests with semantic associations, and create item representations that closely align with user individual interests.In this paper, we propose a brand new framework called **I**nterest-**A**ugmented **M**ultimodal **G**enerator (**I-AM-G**). The framework first extracts tags from the multimodal information of items that the user has interacted with, and the most frequently occurred ones are extracted to rewrite the text description of the item. Then, the framework uses a decoupled text-to-text and image-to-image retriever to search for the top-<span class=tex-math>K</span> similar item text and image embeddings from the item pool. Finally, the Attention module for user interests fuses the retrieved information in a cross-modal manner and further guides the personalized generation process in collaboration with the rewritten text.We conducted extensive and comprehensive experiments to demonstrate that our framework can effectively generate results aligned with user preferences, which potentially provides a new paradigm of **Rewrite and Retrieve** for personalized generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1188.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1188 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1188/>Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps</a></strong><br><a href=/people/g/giuseppe-attanasio/>Giuseppe Attanasio</a>
|
<a href=/people/b/beatrice-savoldi/>Beatrice Savoldi</a>
|
<a href=/people/d/dennis-fucci/>Dennis Fucci</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1188><div class="card-body p-3 small">Current automatic speech recognition (ASR) models are designed to be used across many languages and tasks without substantial changes. However, this broad language coverage hides performance gaps within languages, for example, across genders. Our study systematically evaluates the performance of two widely used multilingual ASR models on three datasets, encompassing 19 languages from eight language families and two speaking conditions. Our findings reveal clear gender disparities, with the advantaged group varying across languages and models. Surprisingly, those gaps are not explained by acoustic or lexical properties. However, probing internal model states reveals a correlation with gendered performance gap. That is, the easier it is to distinguish speaker gender in a language using probes, the more the gap reduces, favoring female speakers. Our results show that gender disparities persist even in state-of-the-art models. Our findings have implications for the improvement of multilingual ASR systems, underscoring the importance of accessibility to training data and nuanced evaluation to predict and mitigate gender gaps. We release all code and artifacts at https://github.com/g8a9/multilingual-asr-gender-gap.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1189.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1189/>Enhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing</a></strong><br><a href=/people/b/baihe-huang/>Baihe Huang</a>
|
<a href=/people/h/hiteshi-sharma/>Hiteshi Sharma</a>
|
<a href=/people/y/yi-mao/>Yi Mao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1189><div class="card-body p-3 small">In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains. Within the training pipeline of LLMs, the Reinforcement Learning with Human Feedback (RLHF) phase is crucial for aligning LLMs with human preferences and values. Label smoothing, a technique that replaces hard labels with soft labels, emerges as promising techniques to enhance RLHF training. Despite the benefits, the choice of label smoothing parameters often relies on heuristic approaches and lack theoretical understanding. This paper addresses the challenge of selecting the label smoothing parameter in a principled manner. We introduce Confidence Aware Label Smoothing (CALS), a method that iteratively updates the label smoothing parameter based on preference labels and model forecasts. Our theoretical analysis characterizes the optimal label smoothing parameter, demonstrates its dependence on the confidence level, and reveals its influence on training dynamics and equilibrium. Empirical evaluations on state-of-the-art alignment tasks show that CALS achieves competitive performance, highlighting its potential for improving alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1190.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1190 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1190/>Contrastive Policy Gradient: Aligning <span class=acl-fixed-case>LLM</span>s on sequence-level scores in a supervised-friendly fashion</a></strong><br><a href=/people/y/yannis-flet-berliac/>Yannis Flet-Berliac</a>
|
<a href=/people/n/nathan-grinsztajn/>Nathan Grinsztajn</a>
|
<a href=/people/f/florian-strub/>Florian Strub</a>
|
<a href=/people/e/eugene-choi/>Eugene Choi</a>
|
<a href=/people/b/bill-wu/>Bill Wu</a>
|
<a href=/people/c/chris-cremer/>Chris Cremer</a>
|
<a href=/people/a/arash-ahmadian/>Arash Ahmadian</a>
|
<a href=/people/y/yash-chandak/>Yash Chandak</a>
|
<a href=/people/m/mohammad-gheshlaghi-azar/>Mohammad Gheshlaghi Azar</a>
|
<a href=/people/o/olivier-pietquin/>Olivier Pietquin</a>
|
<a href=/people/m/matthieu-geist/>Matthieu Geist</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1190><div class="card-body p-3 small">Reinforcement Learning (RL) has been used to finetune Large Language Models (LLMs) using a reward model trained from preference data, to better align with human judgment. The recently introduced direct alignment methods, which are often simpler, more stable, and computationally lighter, can more directly achieve this. However, these approaches cannot optimize arbitrary rewards, and the preference-based ones are not the only rewards of interest for LLMs (eg, unit tests for code generation or textual entailment for summarization, among others). RL-finetuning is usually done with a variation of policy gradient, which calls for on-policy or near-on-policy samples, requiring costly generations. We introduce *Contrastive Policy Gradient*, or CoPG, a simple and mathematically principled new RL algorithm that can estimate the optimal policy even from off-policy data. It can be seen as an off-policy policy gradient approach that does not rely on important sampling techniques and highlights the importance of using (the right) state baseline. We show this approach to generalize the direct alignment method IPO (identity preference optimization) and classic policy gradient. We experiment with the proposed CoPGon a toy bandit problem to illustrate its properties, as well as for finetuning LLMs on a summarization task, using a learned reward function considered as ground truth for the purpose of the experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1191.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1191/>Show and Guide: Instructional-Plan Grounded Vision and Language Model</a></strong><br><a href=/people/d/diogo-gloria-silva/>Diogo Glória-Silva</a>
|
<a href=/people/d/david-semedo/>David Semedo</a>
|
<a href=/people/j/joao-magalhaes/>Joao Magalhaes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1191><div class="card-body p-3 small">Guiding users through complex procedural plans is an inherently multimodal task in which having visually illustrated plan steps is crucial to deliver an effective plan guidance. However, existing works on plan-following language models (LMs) often are not capable of multimodal input and output. In this work, we present MM-PlanLLM, the first multimodal LLM designed to assist users in executing instructional tasks by leveraging both textual plans and visual information. Specifically, we bring cross-modality through two key tasks: Conversational Video Moment Retrieval, where the model retrieves relevant step-video segments based on user queries, and Visually-Informed Step Generation, where the model generates the next step in a plan, conditioned on an image of the user’s current progress. MM-PlanLLM is trained using a novel multitask-multistage approach, designed to gradually expose the model to multimodal instructional-plans semantic layers, achieving strong performance on both multimodal and textual dialogue in a plan-grounded setting. Furthermore, we show that the model delivers cross-modal temporal and plan-structure representations aligned between textual plan steps and instructional video moments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1192.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1192.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1192/>Beyond Turn-Based Interfaces: Synchronous <span class=acl-fixed-case>LLM</span>s as Full-Duplex Dialogue Agents</a></strong><br><a href=/people/b/bandhav-veluri/>Bandhav Veluri</a>
|
<a href=/people/b/benjamin-n-peloquin/>Benjamin N Peloquin</a>
|
<a href=/people/b/bokai-yu/>Bokai Yu</a>
|
<a href=/people/h/hongyu-gong/>Hongyu Gong</a>
|
<a href=/people/s/shyamnath-gollakota/>Shyamnath Gollakota</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1192><div class="card-body p-3 small">Despite broad interest in modeling spoken dialogue agents, most approaches are inherently “half-duplex” – restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is “full-duplex” allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of “time”. To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model’s ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1193.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1193.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1193/><span class=acl-fixed-case>Q</span>u<span class=acl-fixed-case>BE</span>: Question-based Belief Enhancement for Agentic <span class=acl-fixed-case>LLM</span> Reasoning</a></strong><br><a href=/people/m/minsoo-kim/>Minsoo Kim</a>
|
<a href=/people/j/jongyoon-kim/>Jongyoon Kim</a>
|
<a href=/people/j/jihyuk-kim/>Jihyuk Kim</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1193><div class="card-body p-3 small">Despite advancements in Large Language Models (LLMs), many complex tasks are not easily solved in a single inference step, requiring the use of agentic LLMs in interactive environments. However, agentic LLMs suffer from a phenomenon known as reasoning derailment, due to the indiscriminate incorporation of observations from partially observable environments. We introduce QuBE, a method that enhances agents’ focus on task-relevant contexts, by constructing a belief state via question answering. We validate QuBE through experiments in two agentic LLM scenarios with partial observability: 1) a canonical interactive decision-making scenario using text-based game engines, and 2) an interactive retrieval-augmented generation (RAG) scenario using search engines. In the AlfWorld text-based game, QuBE outperforms established baselines by substantial margins, and in the search engine scenario, it achieves marked improvements on the BeIR zero-shot retrieval benchmark. The results demonstrate that QuBE significantly mitigates reasoning derailment, refining the decision-making process of LLM agents in partially observed environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1194.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1194 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1194/><span class=acl-fixed-case>C</span>omp<span class=acl-fixed-case>A</span>ct: Compressing Retrieved Documents Actively for Question Answering</a></strong><br><a href=/people/c/chanwoong-yoon/>Chanwoong Yoon</a>
|
<a href=/people/t/taewhoo-lee/>Taewhoo Lee</a>
|
<a href=/people/h/hyeon-hwang/>Hyeon Hwang</a>
|
<a href=/people/m/minbyul-jeong/>Minbyul Jeong</a>
|
<a href=/people/j/jaewoo-kang/>Jaewoo Kang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1194><div class="card-body p-3 small">Retrieval-augmented generation supports language models to strengthen their factual groundings by providing external contexts. However, language models often face challenges when given extensive information, diminishing their effectiveness in solving questions. Context compression tackles this issue by filtering out irrelevant information, but current methods still struggle in realistic scenarios where crucial information cannot be captured with a single-step approach. To overcome this limitation, we introduce CompAct, a novel framework that employs an active strategy to condense extensive documents without losing key information. Our experiments demonstrate that CompAct brings significant improvements in both performance and compression rate on multi-hop question-answering benchmarks. CompAct flexibly operates as a cost-efficient plug-in module with various off-the-shelf retrievers or readers, achieving exceptionally high compression rates (47x).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1195.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1195/>An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models</a></strong><br><a href=/people/f/fatemeh-shiri/>Fatemeh Shiri</a>
|
<a href=/people/x/xiao-yu-guo/>Xiao-Yu Guo</a>
|
<a href=/people/m/mona-golestan-far/>Mona Golestan Far</a>
|
<a href=/people/x/xin-yu/>Xin Yu</a>
|
<a href=/people/r/reza-haf/>Reza Haf</a>
|
<a href=/people/y/yuan-fang-li/>Yuan-Fang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1195><div class="card-body p-3 small">Large Multimodal Models (LMMs) have achieved strong performance across a range of vision and language tasks. However, their spatial reasoning capabilities are under-investigated. In this paper, we construct a novel VQA dataset, Spatial-MM, to comprehensively study LMMs’ spatial understanding and reasoning capabilities. Our analyses on object-relationship and multi-hop reasoning reveal several important findings. Firstly, bounding boxes and scene graphs, even synthetic ones, can significantly enhance LMMs’ spatial reasoning. Secondly, LMMs struggle more with questions posed from the human perspective than the camera perspective about the image. Thirdly, chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations. Moreover, spatial reasoning steps are much less accurate than non-spatial ones across MLLMs. Lastly, our perturbation analysis on GQA-spatial reveals that LMMs are much stronger at basic object detection than complex spatial reasoning. We believe our new benchmark dataset and in-depth analyses can spark further research on LMMs spatial reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1196.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1196.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1196 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1196 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1196/>Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models</a></strong><br><a href=/people/j/jiaxin-zhang/>Jiaxin Zhang</a>
|
<a href=/people/w/wendi-cui/>Wendi Cui</a>
|
<a href=/people/y/yiran-huang/>Yiran Huang</a>
|
<a href=/people/k/kamalika-das/>Kamalika Das</a>
|
<a href=/people/s/sricharan-kumar/>Sricharan Kumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1196><div class="card-body p-3 small">Large language models (LLMs) are proficient in capturing factual knowledge across various domains. However, refining their capabilities on previously seen knowledge or integrating new knowledge from external sources remains a significant challenge. In this work, we propose a novel synthetic knowledge ingestion method called , which leverages fine-grained synthesis, interleaved generation, and assemble augmentation strategies to construct high-quality data representations from raw knowledge sources. We then integrate and its variations with three knowledge injection techniques: Retrieval Augmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual Pre-training (CPT) to inject and refine knowledge in language models. Extensive empirical experiments are conducted on various question-answering tasks spanning finance, biomedicine, and open-generation domains to demonstrate that significantly outperforms baseline methods by facilitating effective knowledge injection. We believe that our work is an important step towards enhancing the factual accuracy of LLM outputs by refining knowledge representation and injection capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1197.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1197/>Local Contrastive Editing of Gender Stereotypes</a></strong><br><a href=/people/m/marlene-lutz/>Marlene Lutz</a>
|
<a href=/people/r/rochelle-choenni/>Rochelle Choenni</a>
|
<a href=/people/m/markus-strohmaier/>Markus Strohmaier</a>
|
<a href=/people/a/anne-lauscher/>Anne Lauscher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1197><div class="card-body p-3 small">Stereotypical bias encoded in language models (LMs) poses a threat to safe language technology, yet our understanding of how bias manifests in the parameters of LMs remains incomplete. We introduce local contrastive editing that enables the localization and editing of a subset of weights in a target model in relation to a reference model. We deploy this approach to identify and modify subsets of weights that are associated with gender stereotypes in LMs. Through a series of experiments we demonstrate that local contrastive editing can precisely localize and control a small subset (<span class=tex-math>&lt; 0.5%</span>) of weights that encode gender bias. Our work (i) advances our understanding of how stereotypical biases can manifest in the parameter space of LMs and (ii) opens up new avenues for developing parameter-efficient strategies for controlling model properties in a contrastive manner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1198.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1198 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1198/>De-Identification of Sensitive Personal Data in Datasets Derived from <span class=acl-fixed-case>IIT</span>-<span class=acl-fixed-case>CDIP</span></a></strong><br><a href=/people/s/stefan-larson/>Stefan Larson</a>
|
<a href=/people/n/nicole-cornehl-lima/>Nicole Cornehl Lima</a>
|
<a href=/people/s/santiago-pedroza-diaz/>Santiago Pedroza Diaz</a>
|
<a href=/people/a/amogh-manoj-joshi/>Amogh Manoj Joshi</a>
|
<a href=/people/s/siddharth-betala/>Siddharth Betala</a>
|
<a href=/people/j/jamiu-tunde-suleiman/>Jamiu Tunde Suleiman</a>
|
<a href=/people/y/yash-mathur/>Yash Mathur</a>
|
<a href=/people/k/kaushal-kumar-prajapati/>Kaushal Kumar Prajapati</a>
|
<a href=/people/r/ramla-alakraa/>Ramla Alakraa</a>
|
<a href=/people/j/junjie-shen/>Junjie Shen</a>
|
<a href=/people/t/temi-okotore/>Temi Okotore</a>
|
<a href=/people/k/kevin-leach/>Kevin Leach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1198><div class="card-body p-3 small">The IIT-CDIP document collection is the source of several widely used and publicly accessible document understanding datasets. In this paper, manual inspection of 5 datasets derived from IIT-CDIP uncovers the presence of thousands of instances of sensitive personal data, including US Social Security Numbers (SSNs), birth places and dates, and home addresses of individuals. The presence of such sensitive personal data in commonly-used and publicly available datasets is startling and has ethical and potentially legal implications; we believe such sensitive data ought to be removed from the internet. Thus, in this paper, we develop a modular data de-identification pipeline that replaces sensitive data with synthetic, but realistic, data. Via experiments, we demonstrate that this de-identification method preserves the utility of the de-identified documents so that they can continue be used in various document understanding applications. We will release redacted versions of these datasets publicly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1199.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1199.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1199.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1199/><span class=acl-fixed-case>RAR</span>: Retrieval-augmented retrieval for code generation in low resource languages</a></strong><br><a href=/people/a/avik-dutta/>Avik Dutta</a>
|
<a href=/people/m/mukul-singh/>Mukul Singh</a>
|
<a href=/people/g/gust-verbruggen/>Gust Verbruggen</a>
|
<a href=/people/s/sumit-gulwani/>Sumit Gulwani</a>
|
<a href=/people/v/vu-le/>Vu Le</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1199><div class="card-body p-3 small">Language models struggle in generating code for low-resource programming languages, since these are underrepresented in training data. Either examples or documentation are commonly used for improved code generation. We propose to use both types of information together and present retrieval augmented retrieval (RAR) as a two-step method for selecting relevant examples and documentation. Experiments on three low-resource languages (Power Query M, OfficeScript and Excel formulas) show that RAR outperforms independently example and grammar retrieval (+2.81–26.14%). Interestingly, we show that two-step retrieval selects better examples and documentation when used independently as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1200.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1200 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1200/><span class=acl-fixed-case>STAR</span>: <span class=acl-fixed-case>S</span>ocio<span class=acl-fixed-case>T</span>echnical Approach to Red Teaming Language Models</a></strong><br><a href=/people/l/laura-weidinger/>Laura Weidinger</a>
|
<a href=/people/j/john-f-j-mellor/>John F J Mellor</a>
|
<a href=/people/b/bernat-guillen-pegueroles/>Bernat Guillén Pegueroles</a>
|
<a href=/people/n/nahema-marchal/>Nahema Marchal</a>
|
<a href=/people/r/ravin-kumar/>Ravin Kumar</a>
|
<a href=/people/k/kristian-lum/>Kristian Lum</a>
|
<a href=/people/c/canfer-akbulut/>Canfer Akbulut</a>
|
<a href=/people/m/mark-diaz/>Mark Diaz</a>
|
<a href=/people/a/a-stevie-bergman/>A. Stevie Bergman</a>
|
<a href=/people/m/mikel-d-rodriguez/>Mikel D. Rodriguez</a>
|
<a href=/people/v/verena-rieser/>Verena Rieser</a>
|
<a href=/people/w/william-isaac/>William Isaac</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1200><div class="card-body p-3 small">This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for human red teamers, leading to improved coverage of the risk surface. Parameterised instructions also provide more detailed insights into model failures at no increased cost. Second, STAR improves signal quality by matching demographics to assess harms for specific groups, resulting in more sensitive annotations. STAR further employs a novel step of arbitration to leverage diverse viewpoints and improve label reliability, treating disagreement not as noise but as a valuable contribution to signal quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1201.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1201/>Do great minds think alike? Investigating Human-<span class=acl-fixed-case>AI</span> Complementarity in Question Answering with <span class=acl-fixed-case>CAIMIRA</span></a></strong><br><a href=/people/m/maharshi-gor/>Maharshi Gor</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé Iii</a>
|
<a href=/people/t/tianyi-zhou/>Tianyi Zhou</a>
|
<a href=/people/j/jordan-lee-boyd-graber/>Jordan Lee Boyd-Graber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1201><div class="card-body p-3 small">Recent advancements of large language models (LLMs)have led to claims of AI surpassing humansin natural language processing NLP tasks such as textual understanding and reasoning.%This work investigates these assertions by introducingCAIMIRA, a novel framework rooted in item response theory IRTthat enables quantitative assessment and comparison of problem-solving abilities inquestion-answering QA agents.%Through analysis of over 300,000 responses from ~ 70 AI systemsand 155 humans across thousands of quiz questions, CAIMIRA uncovers distinctproficiency patterns in knowledge domains and reasoning skills. %Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning,while state-of-the-art LLMs like GPT-4 Turbo and Llama-3-70B demonstrate superior performance ontargeted information retrieval and fact-based reasoning, particularly when information gapsare well-defined and addressable through pattern matching or data retrieval.%These findings identify key areas for future QA tasks and model development,highlighting the critical need for questions that not only challengehigher-order reasoning and scientific thinking, but also demand nuanced linguisticand cross-contextual application.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1202.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1202 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1202/>Memory-Efficient Fine-Tuning of Transformers via Token Selection</a></strong><br><a href=/people/a/antoine-simoulin/>Antoine Simoulin</a>
|
<a href=/people/n/namyong-park/>Namyong Park</a>
|
<a href=/people/x/xiaoyi-liu/>Xiaoyi Liu</a>
|
<a href=/people/g/grey-yang/>Grey Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1202><div class="card-body p-3 small">Fine-tuning provides an effective means to specialize pre-trained models for various downstream tasks. However, fine-tuning often incurs high memory overhead, especially for large transformer-based models, such as LLMs. While existing methods may reduce certain parts of the memory required for fine-tuning, they still require caching all intermediate activations computed in the forward pass to update weights during the backward pass. In this work, we develop TokenTune, a method to reduce memory usage, specifically the memory to store intermediate activations, in the fine-tuning of transformer-based models. During the backward pass, TokenTune approximates the gradient computation by backpropagating through just a subset of input tokens. Thus, with TokenTune, only a subset of intermediate activations are cached during the forward pass. Also, TokenTune can be easily combined with existing methods like LoRA, further reducing the memory cost. We evaluate our approach on pre-trained transformer models with up to billions of parameters, considering the performance on multiple downstream tasks such as text classification and question answering in a few-shot learning setup. Overall, TokenTune achieves performance on par with full fine-tuning or representative memory-efficient fine-tuning methods, while greatly reducing the memory footprint, especially when combined with other methods with complementary memory reduction mechanisms. We hope that our approach will facilitate the fine-tuning of large transformers, in specializing them for specific domains or co-training them with other neural components from a larger system. Our code is available at https://github.com/facebookresearch/tokentune.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1203.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1203/>Unveiling the mystery of visual attributes of concrete and abstract concepts: Variability, nearest neighbors, and challenging categories</a></strong><br><a href=/people/t/tarun-tater/>Tarun Tater</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte Im Walde</a>
|
<a href=/people/d/diego-frassinelli/>Diego Frassinelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1203><div class="card-body p-3 small">The visual representation of a concept varies significantly depending on its meaning and the context where it occurs; this poses multiple challenges both for vision and multimodal models. Our study focuses on concreteness, a well-researched lexical-semantic variable, using it as a case study to examine the variability in visual representations. We rely on images associated with approximately 1,000 abstract and concrete concepts extracted from two different datasets: Bing and YFCC. Our goals are: (i) evaluate whether visual diversity in the depiction of concepts can reliably distinguish between concrete and abstract concepts; (ii) analyze the variability of visual features across multiple images of the same concept through a nearest neighbor analysis; and (iii) identify challenging factors contributing to this variability by categorizing and annotating images. Our findings indicate that for classifying images of abstract versus concrete concepts, a combination of basic visual features such as color and texture is more effective than features extracted by more complex models like Vision Transformer (ViT). However, ViTs show better performances in the nearest neighbor analysis, emphasizing the need for a careful selection of visual features when analyzing conceptual variables through modalities other than text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1204.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1204.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1204/>Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark</a></strong><br><a href=/people/e/elizabeth-fons/>Elizabeth Fons</a>
|
<a href=/people/r/rachneet-kaur/>Rachneet Kaur</a>
|
<a href=/people/s/soham-palande/>Soham Palande</a>
|
<a href=/people/z/zhen-zeng/>Zhen Zeng</a>
|
<a href=/people/t/tucker-balch/>Tucker Balch</a>
|
<a href=/people/m/manuela-veloso/>Manuela Veloso</a>
|
<a href=/people/s/svitlana-vyetrenko/>Svitlana Vyetrenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1204><div class="card-body p-3 small">Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more. In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms. We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data. Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features, each accompanied by textual descriptions. This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series. Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter. In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1205.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1205.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1205/>Can <span class=acl-fixed-case>LLM</span>s Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner</a></strong><br><a href=/people/s/shudong-liu/>Shudong Liu</a>
|
<a href=/people/z/zhaocong-li/>Zhaocong Li</a>
|
<a href=/people/x/xuebo-liu/>Xuebo Liu</a>
|
<a href=/people/r/runzhe-zhan/>Runzhe Zhan</a>
|
<a href=/people/d/derek-f-wong/>Derek F. Wong</a>
|
<a href=/people/l/lidia-s-chao/>Lidia S. Chao</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1205><div class="card-body p-3 small">Large language models (LLMs) often exhibit excessive, random, and uninformative uncertainty, rendering them unsuitable for decision-making in human-computer interactions. In this paper, we aim to instigate a heightened awareness of self-uncertainty in LLMs, enabling them to express uncertainty more effectively. To accomplish this, we propose an uncertainty-aware instruction tuning (UaIT) method, aligning LLMs’ perception with the probabilistic uncertainty of the generation. We conducted experiments using LLaMA2 and Mistral on multiple free-form QA tasks. Experimental results revealed a surprising 45.2% improvement in the effectiveness of uncertainty expression by LLMs, accompanied by reasonably good out-of-domain generalization capabilities. Moreover, this uncertainty expression can serve as a valuable real-time basis for human decision-making, e.g., retrieving external documents and incorporating stronger LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1206.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1206/>Preference-Guided Reflective Sampling for Aligning Language Models</a></strong><br><a href=/people/h/hai-ye/>Hai Ye</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1206><div class="card-body p-3 small">Iterative data generation and model re-training can effectively align large language models (LLMs) to human preferences. The process of data sampling is crucial, as it significantly influences the success of policy improvement. Repeated random sampling is a widely used method that independently queries the model multiple times to generate outputs. In this work, we propose a more effective sampling method, named Preference-Guided Reflective Sampling (PRS). Unlike random sampling, PRS employs a tree-based generation framework to enable more efficient sampling. It leverages adaptive self-refinement techniques to better explore the sampling space. By specifying user preferences in natural language, PRS can further optimize response generation according to these preferences. As a result, PRS can align models to diverse user preferences. Our experiments demonstrate that PRS generates higher-quality responses with significantly higher rewards. On AlpacaEval and Arena-Hard, PRS substantially outperforms repeated random sampling in best-of-<span class=tex-math>N</span> sampling. Moreover, PRS shows strong performance when applied in iterative offline RL training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1207.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1207/>Metrics for What, Metrics for Whom: Assessing Actionability of Bias Evaluation Metrics in <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/p/pieter-delobelle/>Pieter Delobelle</a>
|
<a href=/people/g/giuseppe-attanasio/>Giuseppe Attanasio</a>
|
<a href=/people/d/debora-nozza/>Debora Nozza</a>
|
<a href=/people/s/su-lin-blodgett/>Su Lin Blodgett</a>
|
<a href=/people/z/zeerak-talat/>Zeerak Talat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1207><div class="card-body p-3 small">This paper introduces the concept of actionability in the context of bias measures in natural language processing (NLP). We define actionability as the degree to which a measure’s results enable informed action and propose a set of desiderata for assessing it. Building on existing frameworks such as measurement modeling, we argue that actionability is a crucial aspect of bias measures that has been largely overlooked in the literature.We conduct a comprehensive review of 146 papers proposing bias measures in NLP, examining whether and how they provide the information required for actionable results. Our findings reveal that many key elements of actionability, including a measure’s intended use and reliability assessment, are often unclear or entirely absent.This study highlights a significant gap in the current approach to developing and reporting bias measures in NLP. We argue that this lack of clarity may impede the effective implementation and utilization of these measures. To address this issue, we offer recommendations for more comprehensive and actionable metric development and reporting practices in NLP bias research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1208.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1208/>Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/x/xuhui-zhou/>Xuhui Zhou</a>
|
<a href=/people/z/zhe-su/>Zhe Su</a>
|
<a href=/people/t/tiwalayo-eisape/>Tiwalayo Eisape</a>
|
<a href=/people/h/hyunwoo-kim/>Hyunwoo Kim</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1208><div class="card-body p-3 small">Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena. However, most recent work has used a more omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that involve humans and AI agents in the real world. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that LLMs perform better in unrealistic, omniscient simulation settings but struggle in ones that more accurately reflect real-world conditions with information asymmetry. Moreover, we illustrate the limitations inherent in learning from omniscient simulations. Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1209.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1209 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1209.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1209/>A Simple <span class=acl-fixed-case>LLM</span> Framework for Long-Range Video Question-Answering</a></strong><br><a href=/people/c/ce-zhang/>Ce Zhang</a>
|
<a href=/people/t/taixi-lu/>Taixi Lu</a>
|
<a href=/people/m/md-mohaiminul-islam/>Md Mohaiminul Islam</a>
|
<a href=/people/z/ziyang-wang/>Ziyang Wang</a>
|
<a href=/people/s/shoubin-yu/>Shoubin Yu</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/g/gedas-bertasius/>Gedas Bertasius</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1209><div class="card-body p-3 small">We present LLoVi, a simple yet effective **L**anguage-based **Lo**ng-range **Vi**deo question-answering (LVQA) framework. Our method decomposes the short- and long-range modeling aspects of LVQA into two stages. First, we use a short-term visual captioner to generate textual descriptions of short video clips (0.5-8 seconds in length) densely sampled from a long input video. Afterward, an LLM aggregates the densely extracted short-term captions to answer a given question. Furthermore, we propose a novel multi-round summarization prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question. To analyze what makes our simple framework so effective, we thoroughly evaluate various components of our framework. Our empirical analysis reveals that the choice of the visual captioner and LLM is critical for good LVQA performance. The proposed multi-round summarization prompt also leads to a significant LVQA performance boost. Our method achieves the best-reported results on the EgoSchema dataset, best known for very long-form video question-answering. LLoVi also outperforms the previous state-of-the-art by **10.2%** and **6.2%** on NExT-QA and IntentQA for LVQA. Finally, we extend LLoVi to grounded VideoQA, which requires both QA and temporal localization, and show that it outperforms all prior methods on NExT-GQA. Code is available at https://github.com/CeeZh/LLoVi.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1210.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1210/>Rebuilding <span class=acl-fixed-case>ROME</span> : Resolving Model Collapse during Sequential Model Editing</a></strong><br><a href=/people/a/akshat-gupta/>Akshat Gupta</a>
|
<a href=/people/s/sidharth-baskaran/>Sidharth Baskaran</a>
|
<a href=/people/g/gopala-anumanchipalli/>Gopala Anumanchipalli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1210><div class="card-body p-3 small">Recent work using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we show that disabling edits are an artifact of irregularities in the implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that model collapse is no longer observed when making large scale sequential edits with r-ROME, while further improving generalization and locality of model editing compared to the original implementation of ROME. We also provide a detailed mathematical explanation of the reason behind disabling edits.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1211.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1211 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1211/><span class=acl-fixed-case>C</span>asablanca: Data and Models for Multidialectal <span class=acl-fixed-case>A</span>rabic Speech Recognition</a></strong><br><a href=/people/b/bashar-talafha/>Bashar Talafha</a>
|
<a href=/people/k/karima-kadaoui/>Karima Kadaoui</a>
|
<a href=/people/s/samar-mohamed-magdy/>Samar Mohamed Magdy</a>
|
<a href=/people/m/mariem-habiboullah/>Mariem Habiboullah</a>
|
<a href=/people/c/chafei-mohamed-chafei/>Chafei Mohamed Chafei</a>
|
<a href=/people/a/ahmed-oumar-el-shangiti/>Ahmed Oumar El-Shangiti</a>
|
<a href=/people/h/hiba-zayed/>Hiba Zayed</a>
|
<a href=/people/m/mohamedou-cheikh-tourad/>Mohamedou Cheikh Tourad</a>
|
<a href=/people/r/rahaf-alhamouri/>Rahaf Alhamouri</a>
|
<a href=/people/r/rwaa-assi/>Rwaa Assi</a>
|
<a href=/people/a/aisha-alraeesi/>Aisha Alraeesi</a>
|
<a href=/people/h/hour-mohamed/>Hour Mohamed</a>
|
<a href=/people/f/fakhraddin-alwajih/>Fakhraddin Alwajih</a>
|
<a href=/people/a/abdelrahman-mohamed/>Abdelrahman Mohamed</a>
|
<a href=/people/a/abdellah-el-mekki/>Abdellah El Mekki</a>
|
<a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/b/benelhadj-djelloul-mama-saadia/>Benelhadj Djelloul Mama Saadia</a>
|
<a href=/people/h/hamzah-a-alsayadi/>Hamzah A. Alsayadi</a>
|
<a href=/people/w/walid-al-dhabyani/>Walid Al-Dhabyani</a>
|
<a href=/people/s/sara-shatnawi/>Sara Shatnawi</a>
|
<a href=/people/y/yasir-ech-chammakhy/>Yasir Ech-chammakhy</a>
|
<a href=/people/a/amal-makouar/>Amal Makouar</a>
|
<a href=/people/y/yousra-berrachedi/>Yousra Berrachedi</a>
|
<a href=/people/m/mustafa-jarrar/>Mustafa Jarrar</a>
|
<a href=/people/s/shady-shehata/>Shady Shehata</a>
|
<a href=/people/i/ismail-berrada/>Ismail Berrada</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1211><div class="card-body p-3 small">In spite of the recent progress in speech processing, the majority of world languages and dialects remain uncovered. This situation only furthers an already wide technological divide, thereby hindering technological and socioeconomic inclusion. This challenge is largely due to the absence of datasets that can empower diverse speech systems. In this paper, we seek to mitigate this obstacle for a number of Arabic dialects by presenting Casablanca, a large-scale community-driven effort to collect and transcribe a multi-dialectal Arabic dataset. The dataset covers eight dialects: Algerian, Egyptian, Emirati, Jordanian, Mauritanian, Moroccan, Palestinian, and Yemeni, and includes annotations for transcription, gender, dialect, and code-switching. We also develop a number of strong baselines exploiting Casablanca. The project page for Casablanca is accessible at: www.dlnlp.ai/speech/casablanca.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1212.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1212 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1212/>Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations</a></strong><br><a href=/people/r/rima-hazra/>Rima Hazra</a>
|
<a href=/people/s/sayan-layek/>Sayan Layek</a>
|
<a href=/people/s/somnath-banerjee/>Somnath Banerjee</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1212><div class="card-body p-3 small">Ensuring the safe alignment of large language models (LLMs) with human values is critical as they become integral to applications like translation and question answering. Current alignment methods struggle with dynamic user intentions and complex objectives, making models vulnerable to generating harmful content. We propose Safety Arithmetic, a training-free framework enhancing LLM safety across different scenarios: Base models, Supervised fine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm Direction Removal to avoid harmful content and Safety Alignment to promote safe responses. Additionally, we present NoIntentEdit, a dataset highlighting edit instances that could compromise model safety if used unintentionally. Our experiments show that Safety Arithmetic significantly improves safety measures, reduces over-safety, and maintains model utility, outperforming existing methods in ensuring safe content generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1213.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1213.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1213/>Communicating with Speakers and Listeners of Different Pragmatic Levels</a></strong><br><a href=/people/k/kata-naszadi/>Kata Naszadi</a>
|
<a href=/people/f/frans-a-oliehoek/>Frans A Oliehoek</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1213><div class="card-body p-3 small">This paper explores the impact of variable pragmatic competence on communicative success through simulating language learning and conversing between speakers and listeners with different levels of reasoning abilities. Through studying this interaction, we hypothesize that matching levels of reasoning between communication partners would create a more beneficial environment for communicative success and language learning. Our research findings indicate that learning from more explicit, literal language is advantageous, irrespective of the learner’s level of pragmatic competence. Furthermore, we find that integrating pragmatic reasoning during language learning, not just during evaluation, significantly enhances overall communication performance. This paper provides key insights into the importance of aligning reasoning levels and incorporating pragmatic reasoning in optimizing communicative interactions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1214.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1214 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1214/><span class=acl-fixed-case>RECANTF</span>ormer: Referring Expression Comprehension with Varying Numbers of Targets</a></strong><br><a href=/people/b/bhathiya-hemanthage/>Bhathiya Hemanthage</a>
|
<a href=/people/h/hakan-bilen/>Hakan Bilen</a>
|
<a href=/people/p/phil-bartie/>Phil Bartie</a>
|
<a href=/people/c/christian-dondrup/>Christian Dondrup</a>
|
<a href=/people/o/oliver-lemon/>Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1214><div class="card-body p-3 small">The Generalized Referring Expression Comprehension (GREC) task extends classic REC by generating image bounding boxes for objects referred to in natural language expressions, which may indicate zero, one, or multiple targets. This generalization enhances the practicality of REC models for diverse real-world applications. However, the presence of varying numbers of targets in samples makes GREC a more complex task, both in terms of training supervision and final prediction selection strategy. Addressing these challenges, we introduce RECANTFormer, a one-stage method for GREC that combines a decoder-free (encoder-only) transformer architecture with DETR-like Hungarian matching. Our approach consistently outperforms baselines by significant margins in three GREC datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1215.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1215 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1215/>Sprout: Green Generative <span class=acl-fixed-case>AI</span> with Carbon-Efficient <span class=acl-fixed-case>LLM</span> Inference</a></strong><br><a href=/people/b/baolin-li/>Baolin Li</a>
|
<a href=/people/y/yankai-jiang/>Yankai Jiang</a>
|
<a href=/people/v/vijay-gadepally/>Vijay Gadepally</a>
|
<a href=/people/d/devesh-tiwari/>Devesh Tiwari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1215><div class="card-body p-3 small">The rapid advancement of generative AI has heightened environmental concerns, particularly regarding carbon emissions. Our framework, Sprout, addresses these challenges by reducing the carbon footprint of inference in large language models (LLMs). Sprout introduces “generation directives” to guide the autoregressive generation process, achieving a balance between ecological sustainability and high-quality outputs. By employing a strategic optimizer for directive assignment and a novel offline quality evaluator, Sprout reduces the carbon footprint of generative LLM inference by over 40% in real-world evaluations, using the Llama model and global electricity grid data. This work is crucial as the rising interest in inference time compute scaling laws amplifies environmental concerns, emphasizing the need for eco-friendly AI solutions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1216.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1216/>Do <span class=acl-fixed-case>LLM</span>s Plan Like Human Writers? Comparing Journalist Coverage of Press Releases with <span class=acl-fixed-case>LLM</span>s</a></strong><br><a href=/people/a/alexander-spangher/>Alexander Spangher</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1216><div class="card-body p-3 small">Journalists engage in multiple steps in the news writing process that depend on human creativity, like exploring different “angles” (i.e. the specific perspectives a reporter takes). These can potentially be aided by large language models (LLMs). By affecting planning decisions, such interventions can have an outsize impact on creative output. We advocate a careful approach to evaluating these interventions to ensure alignment with human values.In a case study of journalistic coverage of press releases, we assemble a large dataset of 250k press releases and 650k articles covering them. We develop methods to identify news articles that _challenge and contextualize_ press releases. Finally, we evaluate suggestions made by LLMs for these articles and compare these with decisions made by human journalists. Our findings are three-fold: (1) Human-written news articles that challenge and contextualize press releases more take more creative angles and use more informational sources. (2) LLMs align better with humans when recommending angles, compared with informational sources. (3) Both the angles and sources LLMs suggest are significantly less creative than humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1217.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1217 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1217/><span class=acl-fixed-case>T</span>-<span class=acl-fixed-case>FREE</span>: Subword Tokenizer-Free Generative <span class=acl-fixed-case>LLM</span>s via Sparse Representations for Memory-Efficient Embeddings</a></strong><br><a href=/people/b/bjorn-deiseroth/>Björn Deiseroth</a>
|
<a href=/people/m/manuel-brack/>Manuel Brack</a>
|
<a href=/people/p/patrick-schramowski/>Patrick Schramowski</a>
|
<a href=/people/k/kristian-kersting/>Kristian Kersting</a>
|
<a href=/people/s/samuel-weinbach/>Samuel Weinbach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1217><div class="card-body p-3 small">Tokenizers are crucial for encoding information in Large Language Models, but their development has recently stagnated, and they contain inherent weaknesses. Major limitations include computational overhead, ineffective vocabulary use, and unnecessarily large embedding and head layers. Additionally, their performance is biased towards a reference corpus, leading to reduced effectiveness for underrepresented languages.To remedy these issues, we propose T-Free, which directly embeds words through sparse activation patterns over character triplets and does not require a reference corpus. T-Free inherently exploits morphological similarities and allows for strong compression of embedding layers. In our exhaustive experimental evaluation, we achieve competitive downstream performance with a parameter reduction of more than 85% on these layers. Further, T-Free shows significant improvements in cross-lingual transfer learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1218.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1218 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1218/><span class=acl-fixed-case>S</span>peech<span class=acl-fixed-case>QE</span>: Estimating the Quality of Direct Speech Translation</a></strong><br><a href=/people/h/hyojung-han/>HyoJung Han</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1218><div class="card-body p-3 small">Recent advances in automatic quality estimation for machine translation have exclusively focused on written language, leaving the speech modality underexplored. In this work, we formulate the task of quality estimation for speech translation (SpeechQE), construct a benchmark, and evaluate a family of systems based on cascaded and end-to-end architectures. In this process, we introduce a novel end-to-end system leveraging pre-trained text LLM. Results suggest that end-to-end approaches are better suited to estimating the quality of direct speech translation than using quality estimation systems designed for text in cascaded systems. More broadly, we argue that quality estimation of speech translation needs to be studied as a separate problem from that of text, and release our [data and models](https://github.com/h-j-han/SpeechQE) to guide further research in this space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1219.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1219 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1219/>Assessing and Verifying Task Utility in <span class=acl-fixed-case>LLM</span>-Powered Applications</a></strong><br><a href=/people/n/negar-arabzadeh/>Negar Arabzadeh</a>
|
<a href=/people/s/siqing-huo/>Siqing Huo</a>
|
<a href=/people/n/nikhil-mehta/>Nikhil Mehta</a>
|
<a href=/people/q/qingyun-wu/>Qingyun Wu</a>
|
<a href=/people/c/chi-wang/>Chi Wang</a>
|
<a href=/people/a/ahmed-hassan/>Ahmed Hassan Awadallah</a>
|
<a href=/people/c/charles-l-a-clarke/>Charles L. A. Clarke</a>
|
<a href=/people/j/julia-kiseleva/>Julia Kiseleva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1219><div class="card-body p-3 small">The rapid development of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents, assisting humans in their daily tasks. However, a significant gap remains in assessing to what extent LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the need to verify utility of LLM-powered applications, particularly by ensuring alignment between the application’s functionality and end-user needs. We introduce AgentEval, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the effectiveness and robustness of AgentEval for two open source datasets including Math Problem solving and ALFWorld House-hold related tasks. For reproducibility purposes, we make the data, code and all the logs publicly available at https://github.com/Narabzad/AgentEval</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1220.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1220/>Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models</a></strong><br><a href=/people/s/somanshu-singla/>Somanshu Singla</a>
|
<a href=/people/z/zhen-wang/>Zhen Wang</a>
|
<a href=/people/t/tianyang-liu/>Tianyang Liu</a>
|
<a href=/people/a/abdullah-ashfaq/>Abdullah Ashfaq</a>
|
<a href=/people/z/zhiting-hu/>Zhiting Hu</a>
|
<a href=/people/e/eric-xing/>Eric P. Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1220><div class="card-body p-3 small">Aligning Large Language Models (LLMs) traditionally relies on complex and costly training processes like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To address the challenge of achieving alignment without these extensive tuning costs and expensive annotations, we present a novel, tuning-free approach for self-alignment called Dynamic Rewarding with Prompt Optimization (DRPO). Our approach enables self-alignment through a search-based prompt optimization framework, allowing the model to self-improve and generate optimized prompts without additional training or human supervision. The core of DRPO leverages a dynamic rewarding mechanism to identify and rectify model-specific alignment weaknesses, enabling LLMs to adapt quickly to various alignment challenges. Empirical evaluations on eight recent LLMs, including both open- and closed-source, reveal that DRPO significantly enhances alignment performance, enabling base models to outperform their SFT/RLHF-tuned counterparts. Moreover, DRPO’s automatically optimized prompts surpass those curated by human experts, demonstrating its superior alignment capabilities. Our findings envision a highly cost-effective and adaptable solution for future alignment research to be further explored.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1221.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1221 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1221/>Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree</a></strong><br><a href=/people/h/harbani-jaggi/>Harbani Jaggi</a>
|
<a href=/people/k/kashyap-coimbatore-murali/>Kashyap Coimbatore Murali</a>
|
<a href=/people/e/eve-fleisig/>Eve Fleisig</a>
|
<a href=/people/e/erdem-biyik/>Erdem Biyik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1221><div class="card-body p-3 small">When annotators disagree, predicting the labels given by individual annotators can capture nuances overlooked by traditional label aggregation. We introduce three approaches to predict individual annotator ratings on the toxicity of text by incorporating individual annotator-specific information: a neural collaborative filtering (NCF) approach, an in-context learning (ICL) approach, and an intermediate embedding-based architecture. We also study the utility of demographic information for rating prediction. NCF showed limited utility; however, integrating annotator history, demographics, and survey information permits both the embedding-based architecture and ICL to substantially improve prediction accuracy, with the embedding-based architecture outperforming the other methods. We also find that, if demographics are predicted from survey information, using these imputed demographics as features performs comparably to using true demographic data. This suggests that demographics may not provide substantial information for modeling ratings beyond what is captured in survey responses. Our findings raise considerations about the relative utility of different types of annotator information and provide new approaches for modeling annotators in subjective NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1222.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1222 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1222/>Adversarial Text Generation using Large Language Models for Dementia Detection</a></strong><br><a href=/people/y/youxiang-zhu/>Youxiang Zhu</a>
|
<a href=/people/n/nana-lin/>Nana Lin</a>
|
<a href=/people/k/kiran-sandilya-balivada/>Kiran Sandilya Balivada</a>
|
<a href=/people/d/daniel-haehn/>Daniel Haehn</a>
|
<a href=/people/x/xiaohui-liang/>Xiaohui Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1222><div class="card-body p-3 small">Although large language models (LLMs) excel in various text classification tasks, regular prompting strategies (e.g., few-shot prompting) do not work well with dementia detection via picture description. The challenge lies in the language marks for dementia are unclear, and LLM may struggle with relating its internal knowledge to dementia detection. In this paper, we present an accurate and interpretable classification approach by Adversarial Text Generation (ATG), a novel decoding strategy that could relate dementia detection with other tasks. We further develop a comprehensive set of instructions corresponding to various tasks and use them to guide ATG, achieving the best accuracy of 85%, >10% improvement compared to the regular prompting strategies. In addition, we introduce feature context, a human-understandable text that reveals the underlying features of LLM used for classifying dementia. From feature contexts, we found that dementia detection can be related to tasks such as assessing attention to detail, language, and clarity with specific features of the environment, character, and other picture content or language-related features. Future work includes incorporating multi-modal LLMs to interpret speech and picture information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1223.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1223 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1223.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1223.data.tgz data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1223/>x<span class=acl-fixed-case>COMET</span>-lite: Bridging the Gap Between Efficiency and Quality in Learned <span class=acl-fixed-case>MT</span> Evaluation Metrics</a></strong><br><a href=/people/d/daniil-larionov/>Daniil Larionov</a>
|
<a href=/people/m/mikhail-seleznyov/>Mikhail Seleznyov</a>
|
<a href=/people/v/vasiliy-viskov/>Vasiliy Viskov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1223><div class="card-body p-3 small">State-of-the-art trainable machine translation evaluation metrics like xCOMET achieve high correlation with human judgment but rely on large encoders (up to 10.7B parameters), making them computationally expensive and inaccessible to researchers with limited resources. To address this issue, we investigate whether the knowledge stored in these large encoders can be compressed while maintaining quality. We employ distillation, quantization, and pruning techniques to create efficient xCOMET alternatives and introduce a novel data collection pipeline for efficient black-box distillation. Our experiments show that, using quantization, xCOMET can be compressed up to three times with no quality degradation. Additionally, through distillation, we create an 278M-sized xCOMET-lite metric, which has only 2.6% of xCOMET-XXL parameters, but retains 92.1% of its quality. Besides, it surpasses strong small-scale metrics like COMET-22 and BLEURT-20 on the WMT22 metrics challenge dataset by 6.4%, despite using 50% fewer parameters. All code, dataset, and models are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1224.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1224.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1224/>The Greatest Good Benchmark: Measuring <span class=acl-fixed-case>LLM</span>s’ Alignment with Utilitarian Moral Dilemmas</a></strong><br><a href=/people/g/giovanni-franco-gabriel-marraffini/>Giovanni Franco Gabriel Marraffini</a>
|
<a href=/people/a/andres-cotton/>Andrés Cotton</a>
|
<a href=/people/n/noe-fabian-hsueh/>Noe Fabian Hsueh</a>
|
<a href=/people/a/axel-fridman/>Axel Fridman</a>
|
<a href=/people/j/juan-wisznia/>Juan Wisznia</a>
|
<a href=/people/l/luciano-del-corro/>Luciano Del Corro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1224><div class="card-body p-3 small">The question of how to make decisions that maximise the well-being of all persons is very relevant to design language models that are beneficial to humanity and free from harm. We introduce the Greatest Good Benchmark to evaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis across 15 diverse LLMs reveals consistently encoded moral preferences that diverge from established moral theories and lay population moral standards. Most LLMs have a marked preference for impartial beneficence and rejection of instrumental harm. These findings showcase the ‘artificial moral compass’ of LLMs, offering insights into their moral alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1225.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1225 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1225/><span class=acl-fixed-case>F</span>air<span class=acl-fixed-case>F</span>low: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding</a></strong><br><a href=/people/j/jiali-cheng/>Jiali Cheng</a>
|
<a href=/people/h/hadi-amiri/>Hadi Amiri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1225><div class="card-body p-3 small">Language models are prone to dataset biases, known as shortcuts and spurious correlations in data, which often result in performance drop on new data. We present a new debiasing framework called FairFlow that mitigates dataset biases by learning to be <i>undecided</i> in its predictions for data samples or representations associated with known or unknown biases. The framework introduces two key components: a suite of data and model perturbation operations that generate different biased views of input samples, and a contrastive objective that learns debiased and robust representations from the resulting biased views of samples. Experiments show that FairFlow outperforms existing debiasing methods, particularly against out-of-domain and hard test samples without compromising the in-domain performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1226.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1226 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1226.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1226.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1226/>Style-Shifting Behaviour of the Manosphere on <span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/j/jai-aggarwal/>Jai Aggarwal</a>
|
<a href=/people/s/suzanne-stevenson/>Suzanne Stevenson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1226><div class="card-body p-3 small">Hate speech groups (HSGs) may negatively influence online platforms through their distinctive language, which may affect the tone and topics of other spaces if spread beyond the HSGs. We explore the linguistic style of the Manosphere, a misogynistic HSG, on Reddit. We find that Manospheric authors have a distinct linguistic style using not only uncivil language, but a greater focus on gendered topics, which are retained when posting in other communities. Thus, potentially harmful aspects of Manospheric style carry over into posts on non-Manospheric subreddits, motivating future work to explore how this stylistic spillover may negatively influence community health.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1227.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1227/>The Death and Life of Great Prompts: Analyzing the Evolution of <span class=acl-fixed-case>LLM</span> Prompts from the Structural Perspective</a></strong><br><a href=/people/y/yihan-ma/>Yihan Ma</a>
|
<a href=/people/x/xinyue-shen/>Xinyue Shen</a>
|
<a href=/people/y/yixin-wu/>Yixin Wu</a>
|
<a href=/people/b/boyang-zhang/>Boyang Zhang</a>
|
<a href=/people/m/michael-backes/>Michael Backes</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1227><div class="card-body p-3 small">Effective utilization of large language models (LLMs), such as ChatGPT, relies on the quality of input prompts. This paper explores prompt engineering, specifically focusing on the disparity between experimentally designed prompts and real-world “in-the-wild” prompts. We analyze 10,538 in-the-wild prompts collected from various platforms and develop a framework that decomposes the prompts into eight key components. Our analysis shows that and Requirement are the most prevalent two components. Roles specified in the prompts, along with their capabilities, have become increasingly varied over time, signifying a broader range of application scenarios for LLMs. However, from the response of GPT-4, there is a marginal improvement with a specified role, whereas leveraging less prevalent components such as Capability and Demonstration can result in a more satisfying response. Overall, our work sheds light on the essential components of in-the-wild prompts and the effectiveness of these components on the broader landscape of LLM prompt engineering, providing valuable guidelines for the LLM community to optimize high-quality prompts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1228.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1228/>Holistic Evaluation for Interleaved Text-and-Image Generation</a></strong><br><a href=/people/m/minqian-liu/>Minqian Liu</a>
|
<a href=/people/z/zhiyang-xu/>Zhiyang Xu</a>
|
<a href=/people/z/zihao-lin/>Zihao Lin</a>
|
<a href=/people/t/trevor-ashby/>Trevor Ashby</a>
|
<a href=/people/j/joy-rimchala/>Joy Rimchala</a>
|
<a href=/people/j/jiaxin-zhang/>Jiaxin Zhang</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1228><div class="card-body p-3 small">Interleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate and explainable evaluation. We carefully define five essential evaluation aspects for InterleavedEval, including text quality, perceptual quality, image coherence, text-image coherence, and helpfulness, to ensure a comprehensive and fine-grained assessment. Through extensive experiments and rigorous human evaluation, we show that our benchmark and metric can effectively evaluate the existing models with a strong correlation with human judgments surpassing previous reference-based metrics. We also provide substantial findings and insights to foster future research in interleaved generation and its evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1229.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1229.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1229/><span class=acl-fixed-case>FOLIO</span>: Natural Language Reasoning with First-Order Logic</a></strong><br><a href=/people/s/simeng-han/>Simeng Han</a>
|
<a href=/people/h/hailey-schoelkopf/>Hailey Schoelkopf</a>
|
<a href=/people/y/yilun-zhao/>Yilun Zhao</a>
|
<a href=/people/z/zhenting-qi/>Zhenting Qi</a>
|
<a href=/people/m/martin-riddell/>Martin Riddell</a>
|
<a href=/people/w/wenfei-zhou/>Wenfei Zhou</a>
|
<a href=/people/j/james-coady/>James Coady</a>
|
<a href=/people/d/david-peng/>David Peng</a>
|
<a href=/people/y/yujie-qiao/>Yujie Qiao</a>
|
<a href=/people/l/luke-benson/>Luke Benson</a>
|
<a href=/people/l/lucy-sun/>Lucy Sun</a>
|
<a href=/people/a/alexander-wardle-solano/>Alexander Wardle-Solano</a>
|
<a href=/people/h/hannah-szabo/>Hannah Szabó</a>
|
<a href=/people/e/ekaterina-zubova/>Ekaterina Zubova</a>
|
<a href=/people/m/matthew-burtell/>Matthew Burtell</a>
|
<a href=/people/j/jonathan-fan/>Jonathan Fan</a>
|
<a href=/people/y/yixin-liu/>Yixin Liu</a>
|
<a href=/people/b/brian-wong/>Brian Wong</a>
|
<a href=/people/m/malcolm-sailor/>Malcolm Sailor</a>
|
<a href=/people/a/ansong-ni/>Ansong Ni</a>
|
<a href=/people/l/linyong-nan/>Linyong Nan</a>
|
<a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/w/wojciech-maciej-kryscinski/>Wojciech Maciej Kryscinski</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/y/ye-liu/>Ye Liu</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/y/yingbo-zhou/>Yingbo Zhou</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/rex-ying/>Rex Ying</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1229><div class="card-body p-3 small">Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1230.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1230/>The <span class=acl-fixed-case>LLM</span> Effect: Are Humans Truly Using <span class=acl-fixed-case>LLM</span>s, or Are They Being Influenced By Them Instead?</a></strong><br><a href=/people/a/alexander-choi/>Alexander Choi</a>
|
<a href=/people/s/syeda-sabrina-akter/>Syeda Sabrina Akter</a>
|
<a href=/people/j/j-p-singh/>J.p. Singh</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1230><div class="card-body p-3 small">Large Language Models (LLMs) have shown capabilities close to human performance in various analytical tasks, leading researchers to use them for time and labor-intensive analyses. However, their capability to handle highly specialized and open-ended tasks in domains like policy studies remains in question. This paper investigates the efficiency and accuracy of LLMs in specialized tasks through a structured user study focusing on Human-LLM partnership. The study, conducted in two stages—Topic Discovery and Topic Assignment—integrates LLMs with expert annotators to observe the impact of LLM suggestions on what is usually human-only analysis. Results indicate that LLM-generated topic lists have significant overlap with human generated topic lists, with minor hiccups in missing document-specific topics. However, LLM suggestions may significantly improve task completion speed, but at the same time introduce anchoring bias, potentially affecting the depth and nuance of the analysis, raising a critical question about the trade-off between increased efficiency and the risk of biased analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1231.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1231 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1231/>Is Child-Directed Speech Effective Training Data for Language Models?</a></strong><br><a href=/people/s/steven-y-feng/>Steven Y. Feng</a>
|
<a href=/people/n/noah-goodman/>Noah Goodman</a>
|
<a href=/people/m/michael-c-frank/>Michael Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1231><div class="card-body p-3 small">While high-performing language models are typically trained on hundreds of billions of words, human children become fluent language users with a much smaller amount of data. What are the features of the data they receive, and how do these features support language modeling objectives? To investigate this question, we train GPT-2 and RoBERTa models on 29M words of English child-directed speech and a new matched, synthetic dataset (TinyDialogues), comparing to OpenSubtitles, Wikipedia, and a heterogeneous blend of datasets from the BabyLM challenge. We evaluate the syntactic and semantic knowledge of these models using developmentally-inspired evaluations. Through pretraining experiments, we test whether the global developmental ordering or the local discourse ordering of children’s training data supports high performance relative to other datasets. The local properties of the data affect model results, but surprisingly, global properties do not. Further, child language input is not uniquely valuable for training language models. These findings support the hypothesis that, rather than proceeding from better data, the child’s learning algorithm is substantially more data-efficient than current language modeling techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1232.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1232 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1232/><span class=acl-fixed-case>R</span>ev<span class=acl-fixed-case>MUX</span>: Data Multiplexing with Reversible Adapters for Efficient <span class=acl-fixed-case>LLM</span> Batch Inference</a></strong><br><a href=/people/y/yige-xu/>Yige Xu</a>
|
<a href=/people/x/xu-guo/>Xu Guo</a>
|
<a href=/people/z/zhiwei-zeng/>Zhiwei Zeng</a>
|
<a href=/people/c/chunyan-miao/>Chunyan Miao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1232><div class="card-body p-3 small">Large language models (LLMs) have brought a great breakthrough to the natural language processing (NLP) community, while leading the challenge of handling concurrent customer queries due to their high throughput demands. Data multiplexing addresses this by merging multiple inputs into a single composite input, allowing more efficient inference through a shared forward pass. However, as distinguishing individuals from a composite input is challenging, conventional methods typically require training the entire backbone, yet still suffer from performance degradation. In this paper, we introduce RevMUX, a parameter-efficient data multiplexing framework that incorporates a reversible design in the multiplexer, which can be reused by the demultiplexer to perform reverse operations and restore individual samples for classification. Extensive experiments on four datasets and three types of LLM backbones demonstrate the effectiveness of RevMUX for enhancing LLM inference efficiency while retaining a satisfactory classification performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1233.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1233 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1233/>Inference Helps <span class=acl-fixed-case>PLM</span>s’ Conceptual Understanding: Improving the Abstract Inference Ability with Hierarchical Conceptual Entailment Graphs</a></strong><br><a href=/people/j/juncai-li/>Juncai Li</a>
|
<a href=/people/r/ru-li/>Ru Li</a>
|
<a href=/people/x/xiaoli-li/>Xiaoli Li</a>
|
<a href=/people/q/qinghua-chai/>Qinghua Chai</a>
|
<a href=/people/j/jeff-z-pan/>Jeff Z. Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1233><div class="card-body p-3 small">The abstract inference capability of the Language Model plays a pivotal role in boosting its generalization and reasoning prowess in Natural Language Inference (NLI). Entailment graphs are crafted precisely for this purpose, focusing on learning entailment relations among predicates. Yet, prevailing approaches overlook the *polysemy* and *hierarchical nature of concepts* during entity conceptualization. This oversight disregards how arguments might entail differently across various concept levels, thereby missing potential entailment connections. To tackle this hurdle, we introduce the *concept pyramid* and propose the HiCon-EG (Hierarchical Conceptual Entailment Graph) framework, which organizes arguments hierarchically, delving into entailment relations at diverse concept levels. By learning entailment relationships at different concept levels, the model is guided to better understand concepts so as to improve its abstract inference capabilities. Our method enhances scalability and efficiency in acquiring common-sense knowledge through leveraging statistical language distribution instead of manual labeling, Experimental results show that entailment relations derived from HiCon-EG significantly bolster abstract detection tasks. Our code is available at https://github.com/SXUCFN/HiCon-EG</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1234.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1234.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1234/><span class=acl-fixed-case>M</span>3<span class=acl-fixed-case>H</span>op-<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>T</span>: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought</a></strong><br><a href=/people/g/gitanjali-kumari/>Gitanjali Kumari</a>
|
<a href=/people/k/kirtan-jain/>Kirtan Jain</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1234><div class="card-body p-3 small">In recent years, there has been a significant rise in the phenomenon of hate against women on social media platforms, particularly through the use of misogynous memes. These memes often target women with subtle and obscure cues, making their detection a challenging task for automated systems. Recently, Large Language Models (LLMs) have shown promising results in reasoning using Chain-of-Thought (CoT) prompting to generate the intermediate reasoning chains as the rationale to facilitate multimodal tasks, but often neglect cultural diversity and key aspects like emotion and contextual knowledge hidden in the visual modalities. To address this gap, we introduce a **M**ultimodal **M**ulti-hop CoT (M3Hop-CoT) framework for **M**isogynous meme identification, combining a CLIP-based classifier and a multimodal CoT module with entity-object-relationship integration. M3Hop-CoT employs a three-step multimodal prompting principle to induce emotions, target awareness, and contextual knowledge for meme analysis. Our empirical evaluation, including both qualitative and quantitative analysis, validates the efficacy of the M3Hop-CoT framework on the SemEval-2022 Task 5 (**MAMI task**) dataset, highlighting its strong performance in the macro-F1 score. Furthermore, we evaluate the model’s generalizability by evaluating it on various benchmark meme datasets, offering a thorough insight into the effectiveness of our approach across different datasets. Codes are available at this link: https://github.com/Gitanjali1801/LLM_CoT</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1235.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1235 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1235/><span class=acl-fixed-case>GPT</span>-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation</a></strong><br><a href=/people/g/govind-ramesh/>Govind Ramesh</a>
|
<a href=/people/y/yao-dou/>Yao Dou</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1235><div class="card-body p-3 small">Research on jailbreaking has been valuable for testing and understanding the safety and security issues of large language models (LLMs). In this paper, we introduce Iterative Refinement Induced Self-Jailbreak (IRIS), a novel approach that leverages the reflective capabilities of LLMs for jailbreaking with only black-box access. Unlike previous methods, IRIS simplifies the jailbreaking process by using a single model as both the attacker and target. This method first iteratively refines adversarial prompts through self-explanation, which is crucial for ensuring that even well-aligned LLMs obey adversarial instructions. IRIS then rates and enhances the output given the refined prompt to increase its harmfulness. We find that IRIS achieves jailbreak success rates of 98% on GPT-4, 92% on GPT-4 Turbo, and 94% on Llama-3.1-70B in under 7 queries. It significantly outperforms prior approaches in automatic, black-box, and interpretable jailbreaking, while requiring substantially fewer queries, thereby establishing a new standard for interpretable jailbreaking methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1236.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1236 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1236/><span class=acl-fixed-case>RE</span>-<span class=acl-fixed-case>RAG</span>: Improving Open-Domain <span class=acl-fixed-case>QA</span> Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation</a></strong><br><a href=/people/k/kiseung-kim/>Kiseung Kim</a>
|
<a href=/people/j/jay-yoon-lee/>Jay-Yoon Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1236><div class="card-body p-3 small">The Retrieval Augmented Generation (RAG) framework utilizes a combination of parametric knowledge and external knowledge to demonstrate state-of-the-art performance on open-domain question answering tasks. However, the RAG framework suffers from performance degradation when the query is accompanied by irrelevant contexts. In this work, we propose the RE-RAG framework, which introduces a relevance estimator (RE) that not only provides relative relevance between contexts as previous rerankers did, but also provide confidence, which can be used to classify whether given context is useful for answering the given question. We propose a weakly supervised method for training the RE simply utilizing question-answer data without any labels for correct contexts. We show that RE trained with a small generator (sLM) can not only improve the sLM fine-tuned together with RE but also improve previously unreferenced large language models (LLMs). Furthermore, we investigate new decoding strategies that utilize the proposed confidence measured by RE such as choosing to let the user know that it is “unanswerable” to answer the question given the retrieved contexts or choosing to rely on LLM’s parametric knowledge rather than unrelated contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1237.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1237/>Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets</a></strong><br><a href=/people/v/vatsal-gupta/>Vatsal Gupta</a>
|
<a href=/people/p/pranshu-pandya/>Pranshu Pandya</a>
|
<a href=/people/t/tushar-kataria/>Tushar Kataria</a>
|
<a href=/people/v/vivek-gupta/>Vivek Gupta</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1237><div class="card-body p-3 small">Language models, characterized by their black-box nature, often hallucinate and display sensitivity to input perturbations, causing concerns about trust. To enhance trust, it is imperative to gain a comprehensive understanding of the model’s failure modes and develop effective strategies to improve their performance. In this study, we introduce a methodology designed to examine how input perturbations affect language models across various scales, including pre-trained models and large language models (LLMs). Utilizing fine-tuning, we enhance the model’s robustness to input perturbations. Additionally, we investigate whether exposure to one perturbation enhances or diminishes the model’s performance with respect to other perturbations. To address robustness against multiple perturbations, we present three distinct fine-tuning strategies. Furthermore, we broaden the scope of our methodology to encompass large language models (LLMs) by leveraging a chain of thought (CoT) prompting approach augmented with exemplars. We employ the Tabular-NLI task to showcase how our proposed strategies adeptly train a robust model, enabling it to address diverse perturbations while maintaining accuracy on the original dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1238.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1238/>Simul-<span class=acl-fixed-case>M</span>u<span class=acl-fixed-case>ST</span>-<span class=acl-fixed-case>C</span>: Simultaneous Multilingual Speech Translation Corpus Using Large Language Model</a></strong><br><a href=/people/m/mana-makinae/>Mana Makinae</a>
|
<a href=/people/y/yusuke-sakai/>Yusuke Sakai</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1238><div class="card-body p-3 small">Simultaneous Speech Translation (SiST) begins translating before the entire source input is received, making it crucial to balance quality and latency. In real interpreting situations, interpreters manage this simultaneity by breaking sentences into smaller segments and translating them while maintaining the source order as much as possible. SiST could benefit from this approach to balance quality and latency. However, current corpora used for simultaneous tasks often involve significant word reordering in translation, which is not ideal given that interpreters faithfully follow source syntax as much as possible. Inspired by conference interpreting by humans utilizing the salami technique, we introduce the Simul-MuST-C, a dataset created by leveraging the Large Language Model (LLM), specifically GPT-4o, which aligns the target text as closely as possible to the source text by using minimal chunks that contain enough information to be interpreted. Experiments on three language pairs show that the effectiveness of segmented-base monotonicity in training data varies with the grammatical distance between the source and the target, with grammatically distant language pairs benefiting the most in achieving quality while minimizing latency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1239.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1239 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1239/>Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text</a></strong><br><a href=/people/p/pritika-ramu/>Pritika Ramu</a>
|
<a href=/people/a/aparna-garimella/>Aparna Garimella</a>
|
<a href=/people/s/sambaran-bandyopadhyay/>Sambaran Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1239><div class="card-body p-3 small">Understanding whether a generated table is of good quality is important to be able to use it in creating or editing documents using automatic methods. In this work, we underline that existing measures for table quality evaluation fail to capture the overall semantics of the tables, and sometimes unfairly penalize good tables and reward bad ones. We propose TabEval, a novel table evaluation strategy that captures table semantics by first breaking down a table into a list of natural language atomic statements and then compares them with ground truth statements using entailment-based measures. To validate our approach, we curate a dataset comprising of text descriptions for 1,250 diverse Wikipedia tables, covering a range of topics and structures, in contrast to the limited scope of existing datasets. We compare TabEval with existing metrics using unsupervised and supervised text-to-table generation methods, demonstrating its stronger correlation with human judgments of table quality across four datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1240.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1240 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1240/>On the Fragility of Active Learners for Text Classification</a></strong><br><a href=/people/a/abhishek-ghose/>Abhishek Ghose</a>
|
<a href=/people/e/emma-thuong-nguyen/>Emma Thuong Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1240><div class="card-body p-3 small">Active learning (AL) techniques optimally utilize a labeling budget by iteratively selecting instances that are most valuable for learning. However, they lack “prerequisite checks”, i.e., there are no prescribed criteria to pick an AL algorithm best suited for a dataset. A practitioner must pick a technique they trust would beat random sampling, based on prior reported results, and hope that it is resilient to the many variables in their environment: dataset, labeling budget and prediction pipelines. The important questions then are: how often on average, do we expect any AL technique to reliably beat the computationally cheap and easy-to-implement strategy of random sampling? Does it at least make sense to use AL in an “Always ON” mode in a prediction pipeline, so that while it might not always help, it never under-performs random sampling? How much of a role does the prediction pipeline play in AL’s success?We examine these questions in detail for the task of text classification using pre-trained representations, which are ubiquitous today.Our primary contribution here is a rigorous evaluation of AL techniques, old and new, across setups that vary wrt datasets, text representations and classifiers. This unlocks multiple insights around warm-up times, i.e., number of labels before gains from AL are seen, viability of an “Always ON” mode and the relative significance of different factors.Additionally, we release a framework for rigorous benchmarking of AL techniques for text classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1241.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1241 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1241.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1241/><span class=acl-fixed-case>BMR</span>etriever: Tuning Large Language Models as Better Biomedical Text Retrievers</a></strong><br><a href=/people/r/ran-xu/>Ran Xu</a>
|
<a href=/people/w/wenqi-shi/>Wenqi Shi</a>
|
<a href=/people/y/yue-yu/>Yue Yu</a>
|
<a href=/people/y/yuchen-zhuang/>Yuchen Zhuang</a>
|
<a href=/people/y/yanqiao-zhu/>Yanqiao Zhu</a>
|
<a href=/people/m/may-dongmei-wang/>May Dongmei Wang</a>
|
<a href=/people/j/joyce-c-ho/>Joyce C. Ho</a>
|
<a href=/people/c/chao-zhang-tu/>Chao Zhang</a>
|
<a href=/people/c/carl-yang/>Carl Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1241><div class="card-body p-3 small">Developing effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the lack of sufficient publicly annotated biomedical data and computational resources. We present BMRetriever, a series of dense retrievers for enhancing biomedical retrieval via unsupervised pre-training on large biomedical corpora, followed by instruction fine-tuning on a combination of labeled datasets and synthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify BMRetriever’s efficacy on various biomedical applications. BMRetriever also exhibits strong parameter efficiency, with the 410M variant outperforming baselines up to 11.7 times larger, and the 2B variant matching the performance of models with over 5B parameters. The training data and model checkpoints are released at https://huggingface.co/BMRetriever to ensure transparency, reproducibility, and application to new domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1242.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1242/>Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval</a></strong><br><a href=/people/j/jonghyun-song/>Jonghyun Song</a>
|
<a href=/people/c/cheyon-jin/>Cheyon Jin</a>
|
<a href=/people/w/wenlong-zhao/>Wenlong Zhao</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a>
|
<a href=/people/j/jay-yoon-lee/>Jay-Yoon Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1242><div class="card-body p-3 small">A common retrieve-and-rerank paradigm involves retrieving relevant candidates from a broad set using a fast bi-encoder (BE), followed by applying expensive but accurate cross-encoders (CE) to a limited candidate set. However, relying on this small subset is often susceptible to error propagation from the bi-encoders, which limits the overall performance. To address these issues, we propose the Comparing Multiple Candidates (CMC) framework. CMC compares a query and multiple embeddings of similar candidates (i.e., neighbors) through shallow self-attention layers, delivering rich representations contextualized to each other. Furthermore, CMC is scalable enough to handle multiple comparisons simultaneously. For example, comparing ~10K candidates with CMC takes a similar amount of time as comparing 16 candidates with CE. Experimental results on the ZeSHEL dataset demonstrate that CMC, when plugged in between bi-encoders and cross-encoders as a seamless intermediate reranker (BE-CMC-CE), can effectively improve recall@k (+6.7%-p, +3.5%-p for R@16, R@64) compared to using only bi-encoders (BE-CE), with negligible slowdown (&lt;7%). Additionally, to verify CMC’s effectiveness as the final-stage reranker in improving top-1 accuracy, we conduct experiments on downstream tasks such as entity, passage, and dialogue ranking. The results indicate that CMC is not only faster (11x) but also often more effective than CE, with improved prediction accuracy in Wikipedia entity linking (+0.7%-p) and DSTC7 dialogue ranking (+3.3%-p).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1243.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1243 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1243/><span class=acl-fixed-case>M</span>3<span class=acl-fixed-case>D</span>: <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>M</span>odal <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>D</span>ocument Fine-Grained Inconsistency Detection</a></strong><br><a href=/people/c/chia-wei-tang/>Chia-Wei Tang</a>
|
<a href=/people/t/ting-chih-chen/>Ting-Chih Chen</a>
|
<a href=/people/k/kiet-a-nguyen/>Kiet A. Nguyen</a>
|
<a href=/people/k/kazi-sajeed-mehrab/>Kazi Sajeed Mehrab</a>
|
<a href=/people/a/alvi-md-ishmam/>Alvi Md Ishmam</a>
|
<a href=/people/c/chris-thomas/>Chris Thomas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1243><div class="card-body p-3 small">Fact-checking claims is a highly laborious task that involves understanding how each factual assertion within the claim relates to a set of trusted source materials. Existing approaches make sample-level predictions but fail to identify the specific aspects of the claim that are troublesome and the specific evidence relied upon. In this paper, we introduce a method and new benchmark for this challenging task. Our method predicts the fine-grained logical relationship of each aspect of the claim from a set of multimodal documents, which include text, image(s), video(s), and audio(s). We also introduce a new benchmark (M3DC) of claims requiring multimodal multidocument reasoning, which we construct using a novel claim synthesis technique. Experiments show that our approach outperforms other models on this challenging task on two benchmarks while providing finer-grained predictions, explanations, and evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1244.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1244.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1244 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1244 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1244.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1244/><span class=acl-fixed-case>M</span>ed<span class=acl-fixed-case>A</span>dapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning</a></strong><br><a href=/people/w/wenqi-shi/>Wenqi Shi</a>
|
<a href=/people/r/ran-xu/>Ran Xu</a>
|
<a href=/people/y/yuchen-zhuang/>Yuchen Zhuang</a>
|
<a href=/people/y/yue-yu/>Yue Yu</a>
|
<a href=/people/h/haotian-sun/>Haotian Sun</a>
|
<a href=/people/h/hang-wu/>Hang Wu</a>
|
<a href=/people/c/carl-yang/>Carl Yang</a>
|
<a href=/people/m/may-dongmei-wang/>May Dongmei Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1244><div class="card-body p-3 small">Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and privacy concerns. In this study, we propose MedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards biomedical applications. Instead of fine-tuning the entire LLM, MedAdapter effectively adapts the original model by fine-tuning only a small BERT-sized adapter to rank candidate solutions generated by LLMs. Experiments on four biomedical tasks across eight datasets demonstrate that MedAdapter effectively adapts both white-box and black-box LLMs in biomedical reasoning, achieving average performance improvements of 18.24% and 10.96%, respectively, without requiring extensive computational resources or sharing data with third parties. MedAdapter also yields enhanced performance when combined with train-time adaptation, highlighting a flexible and complementary solution to existing adaptation methods. Faced with the challenges of balancing model performance, computational resources, and data privacy, MedAdapter provides an efficient, privacy-preserving, cost-effective, and transparent solution for adapting LLMs to the biomedical domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1245.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1245.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1245 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1245 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1245.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1245.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1245/><span class=acl-fixed-case>EHRA</span>gent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records</a></strong><br><a href=/people/w/wenqi-shi/>Wenqi Shi</a>
|
<a href=/people/r/ran-xu/>Ran Xu</a>
|
<a href=/people/y/yuchen-zhuang/>Yuchen Zhuang</a>
|
<a href=/people/y/yue-yu/>Yue Yu</a>
|
<a href=/people/j/jieyu-zhang/>Jieyu Zhang</a>
|
<a href=/people/h/hang-wu/>Hang Wu</a>
|
<a href=/people/y/yuanda-zhu/>Yuanda Zhu</a>
|
<a href=/people/j/joyce-c-ho/>Joyce C. Ho</a>
|
<a href=/people/c/carl-yang/>Carl Yang</a>
|
<a href=/people/m/may-dongmei-wang/>May Dongmei Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1245><div class="card-body p-3 small">Clinicians often rely on data engineers to retrieve complex patient information from electronic health record (EHR) systems, a process that is both inefficient and time-consuming. We propose EHRAgent, a large language model (LLM) agent empowered with accumulative domain knowledge and robust coding capability. EHRAgent enables autonomous code generation and execution to facilitate clinicians in directly interacting with EHRs using natural language. Specifically, we formulate a multi-tabular reasoning task based on EHRs as a tool-use planning process, efficiently decomposing a complex task into a sequence of manageable actions with external toolsets. We first inject relevant medical information to enable EHRAgent to effectively reason about the given query, identifying and extracting the required records from the appropriate tables. By integrating interactive coding and execution feedback, EHRAgent then effectively learns from error messages and iteratively improves its originally generated code. Experiments on three real-world EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate, verifying its strong capacity to tackle complex clinical tasks with minimal demonstrations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1246.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1246 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1246.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1246/><span class=acl-fixed-case>S</span>im<span class=acl-fixed-case>LLM</span>: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation</a></strong><br><a href=/people/h/hoang-quoc-nguyen-son/>Hoang-Quoc Nguyen-Son</a>
|
<a href=/people/m/minh-son-dao/>Minh-Son Dao</a>
|
<a href=/people/k/koji-zettsu/>Koji Zettsu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1246><div class="card-body p-3 small">Large language models have emerged as a significant phenomenon due to their ability to produce natural text across various applications. However, the proliferation of generated text raises concerns regarding its potential misuse in fraudulent activities such as academic dishonesty, spam dissemination, and misinformation propagation. Prior studies have detected the generation of non-analogous text, which manifests numerous differences between original and generated text. We have observed that the similarity between the original text and its generation is notably higher than that between the generated text and its subsequent regeneration. To address this, we propose a novel approach named SimLLM, aimed at estimating the similarity between an input sentence and its generated counterpart to detect analogous machine-generated sentences that closely mimic human-written ones. Our empirical analysis demonstrates SimLLM’s superior performance compared to existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1247.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1247 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1247/><span class=acl-fixed-case>CELLO</span>: Causal Evaluation of Large Vision-Language Models</a></strong><br><a href=/people/m/meiqi-chen/>Meiqi Chen</a>
|
<a href=/people/b/bo-peng/>Bo Peng</a>
|
<a href=/people/y/yan-zhang/>Yan Zhang</a>
|
<a href=/people/c/chaochao-lu/>Chaochao Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1247><div class="card-body p-3 small">Causal reasoning is fundamental to human intelligence and crucial for effective decision-making in real-world environments. Despite recent advancements in large vision-language models (LVLMs), their ability to comprehend causality remains unclear. Previous work typically focuses on commonsense causality between events and/or actions, which is insufficient for applications like embodied agents and lacks the explicitly defined causal graphs required for formal causal reasoning. To overcome these limitations, we introduce a fine-grained and unified definition of causality involving interactions between humans and/or objects. Building on the definition, we construct a novel dataset, CELLO, consisting of 14,094 causal questions across all four levels of causality: discovery, association, intervention, and counterfactual. This dataset surpasses traditional commonsense causality by including explicit causal graphs that detail the interactions between humans and objects. Extensive experiments on CELLO reveal that current LVLMs still struggle with causal reasoning tasks, but they can benefit significantly from our proposed CELLO-CoT, a causally inspired chain-of-thought prompting strategy. Both quantitative and qualitative analyses from this study provide valuable insights for future research. Our project page is at https://github.com/OpenCausaLab/CELLO.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1248.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1248 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1248/>Simultaneous Interpretation Corpus Construction by Large Language Models in Distant Language Pair</a></strong><br><a href=/people/y/yusuke-sakai/>Yusuke Sakai</a>
|
<a href=/people/m/mana-makinae/>Mana Makinae</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1248><div class="card-body p-3 small">In Simultaneous Machine Translation (SiMT), training with a simultaneous interpretation (SI) corpus is an effective method for achieving high-quality yet low-latency. However, constructing such a corpus is challenging due to high costs, and limitations in annotator capabilities, and as a result, existing SI corpora are limited. Therefore, we propose a method to convert existing speech translation (ST) corpora into interpretation-style corpora, maintaining the original word order and preserving the entire source content using Large Language Models (LLM-SI-Corpus). We demonstrate that fine-tuning SiMT models using the LLM-SI-Corpus reduces latency while achieving better quality compared to models fine-tuned with other corpora in both speech-to-text and text-to-text settings. The LLM-SI-Corpus is available at https://github.com/yusuke1997/LLM-SI-Corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1249.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1249 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1249/>Training-free Deep Concept Injection Enables Language Models for Video Question Answering</a></strong><br><a href=/people/x/xudong-lin/>Xudong Lin</a>
|
<a href=/people/m/manling-li/>Manling Li</a>
|
<a href=/people/r/richard-zemel/>Richard Zemel</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/s/shih-fu-chang/>Shih-Fu Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1249><div class="card-body p-3 small">Recently, enabling pretrained language models (PLMs) to perform zero-shot crossmodal tasks such as video question answering has been extensively studied. A popular approach is to learn a projection network that projects visual features into the input text embedding space of a PLM, as well as feed-forward adaptation layers, with the weights of the PLM frozen. However, is it really necessary to learn such additional layers? In this paper, we make the first attempt to demonstrate that the PLM is able to perform zero-shot crossmodal tasks without any crossmodal pretraining, when the observed visual concepts are injected as both additional input text tokens and augmentation in the intermediate features within each feed-forward network for the PLM. Specifically, inputting observed visual concepts as text tokens helps to inject them through the self-attention layers in the PLM; to augment the intermediate features in a way that is compatible with the PLM, we propose to construct adaptation layers based on the intermediate representation of concepts (obtained by solely inputting them to the PLM). These two complementary injection mechanisms form the proposed Deep Concept Injection, which comprehensively enables the PLM to perceive instantly without crossmodal pretraining. Extensive empirical analysis on zero-shot video question answering, as well as visual question answering, shows Deep Concept Injection achieves competitive or even better results in both zero-shot and fine-tuning settings, compared to state-of-the-art methods that require crossmodal pretraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1250.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1250.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1250 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1250 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1250/><span class=acl-fixed-case>MIB</span>ench: Evaluating Multimodal Large Language Models over Multiple Images</a></strong><br><a href=/people/h/haowei-liu/>Haowei Liu</a>
|
<a href=/people/x/xi-zhang/>Xi Zhang</a>
|
<a href=/people/h/haiyang-xu/>Haiyang Xu</a>
|
<a href=/people/y/yaya-shi/>Yaya Shi</a>
|
<a href=/people/c/chaoya-jiang/>Chaoya Jiang</a>
|
<a href=/people/m/ming-yan/>Ming Yan</a>
|
<a href=/people/j/ji-zhang/>Ji Zhang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/c/chunfeng-yuan/>Chunfeng Yuan</a>
|
<a href=/people/b/bing-li/>Bing Li</a>
|
<a href=/people/w/weiming-hu/>Weiming Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1250><div class="card-body p-3 small">Built on the power of LLMs, numerous multimodal large language models (MLLMs) have recently achieved remarkable performance on various vision-language tasks. However, most existing MLLMs and benchmarks primarily focus on single-image input scenarios, leaving the performance of MLLMs when handling realistic multiple images underexplored. Although a few benchmarks consider multiple images, their evaluation dimensions and samples are very limited. In this paper, we propose a new benchmark MIBench, to comprehensively evaluate fine-grained abilities of MLLMs in multi-image scenarios. Specifically, MIBench categorizes the multi-image abilities into three scenarios: multi-image instruction (MII), multimodal knowledge-seeking (MKS) and multimodal in-context learning (MIC), and constructs 13 tasks with a total of 13K annotated samples. During data construction, for MII and MKS, we extract correct options from manual annotations and create challenging distractors to obtain multiple-choice questions. For MIC, to enable an in-depth evaluation, we set four sub-tasks and transform the original datasets into in-context learning formats. We evaluate several open-source and closed-source MLLMs on the proposed MIBench. The results reveal that although current models excel in single-image tasks, they exhibit significant shortcomings when faced with multi-image inputs, such as limited fine-grained perception, multi-image reasoning and in-context learning abilities. The annotated data of MIBench is available at https://huggingface.co/datasets/StarBottle/MIBench.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1251.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1251 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1251/><span class=acl-fixed-case>ZEBRA</span>: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering</a></strong><br><a href=/people/f/francesco-maria-molfese/>Francesco Maria Molfese</a>
|
<a href=/people/s/simone-conia/>Simone Conia</a>
|
<a href=/people/r/riccardo-orlando/>Riccardo Orlando</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1251><div class="card-body p-3 small">Current Large Language Models (LLMs) have shown strong reasoning capabilities in commonsense question answering benchmarks, but the process underlying their success remains largely opaque. As a consequence, recent approaches have equipped LLMs with mechanisms for knowledge retrieval, reasoning and introspection, not only to improve their capabilities but also to enhance the interpretability of their outputs. However, these methods require additional training, hand-crafted templates or human-written explanations. To address these issues, we introduce ZEBRA, a zero-shot question answering framework that combines retrieval, case-based reasoning and introspection and dispenses with the need for additional training of the LLM. Given an input question, ZEBRA retrieves relevant question-knowledge pairs from a knowledge base and generates new knowledge by reasoning over the relationships in these pairs. This generated knowledge is then used to answer the input question, improving the model’s performance and interpretability. We evaluate our approach across 8 well-established commonsense reasoning benchmarks, demonstrating that ZEBRA consistently outperforms strong LLMs and previous knowledge integration approaches, achieving an average accuracy improvement of up to 4.5 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1252.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1252 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1252/><span class=acl-fixed-case>ABLE</span>: Personalized Disability Support with Politeness and Empathy Integration</a></strong><br><a href=/people/k/kshitij-mishra/>Kshitij Mishra</a>
|
<a href=/people/m/manisha-burja/>Manisha Burja</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1252><div class="card-body p-3 small">In today’s dynamic world, providing inclusive and personalized support for individuals with physical disabilities is imperative. With diverse needs and preferences, tailored assistance according to user personas is crucial. In this paper, we introduce ABLE (Adaptive, Bespoke, Listen and Empathetic), a Conversational Support System for Physical Disabilities. By tracking user personas, including gender, age, and personality traits based on the OCEAN model, ABLE ensures that support interactions are uniquely tailored to each user’s characteristics and preferences. Moreover, integrating politeness and empathy levels in responses enhances user satisfaction and engagement, fostering a supportive and respectful environment. The development of ABLE involves compiling a comprehensive conversational dataset enriched with user profile annotations. Leveraging reinforcement learning techniques and diverse reward mechanisms, ABLE trains a model to generate responses aligned with individual user profiles while maintaining appropriate levels of politeness and empathy. Based on rigorous empirical analysis encompassing automatic and human evaluation metrics based on persona-consistency, politeness accuracy, empathy accuracy, perplexity, and conversation coherence, the efficacy of ABLE is assessed. Our findings underscore ABLE’s success in delivering tailored support to individuals grappling with physical disabilities. To the best of our knowledge, this is the very first attempt towards building a user’s persona-oriented physical disability support system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1253.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1253.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1253 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1253 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1253.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1253/>Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models</a></strong><br><a href=/people/h/hyungjoo-chae/>Hyungjoo Chae</a>
|
<a href=/people/y/yeonghyeon-kim/>Yeonghyeon Kim</a>
|
<a href=/people/s/seungone-kim/>Seungone Kim</a>
|
<a href=/people/k/kai-tzu-iunn-ong/>Kai Tzu-iunn Ong</a>
|
<a href=/people/b/beong-woo-kwak/>Beong-woo Kwak</a>
|
<a href=/people/m/moohyeon-kim/>Moohyeon Kim</a>
|
<a href=/people/s/sunghwan-mac-kim/>Sunghwan Kim</a>
|
<a href=/people/t/taeyoon-kwon/>Taeyoon Kwon</a>
|
<a href=/people/j/jiwan-chung/>Jiwan Chung</a>
|
<a href=/people/y/youngjae-yu/>Youngjae Yu</a>
|
<a href=/people/j/jinyoung-yeo/>Jinyoung Yeo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1253><div class="card-body p-3 small">Algorithmic reasoning tasks that involve complex logical patterns, such as completing Dyck language, pose challenges for large language models (LLMs), despite their recent success. Prior work has used LLMs to generate programming language and applied external compilers for such tasks. Yet, when on the fly, it is hard to generate an executable code with the correct logic for the solution. Even so, code for one instance cannot be reused for others, although they might require the same logic to solve. We present Think-and-Execute, a novel framework that improves LLMs’ algorithmic reasoning: (1) In Think, we discover task-level logic shared across all instances, and express such logic with pseudocode; (2) In Execute, we tailor the task-level pseudocode to each instance and simulate the execution of it. Think-and-Execute outperforms several strong baselines (including CoT and PoT) in diverse algorithmic reasoning tasks. We manifest the advantage of using task-level pseudocode over generating instance-specific solutions one by one. Also, we show that pseudocode can better improve LMs’ reasoning than natural language (NL) guidance, even though they are trained with NL instructions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1254.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1254 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1254.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1254.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1254/>Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code</a></strong><br><a href=/people/h/hyungjoo-chae/>Hyungjoo Chae</a>
|
<a href=/people/t/taeyoon-kwon/>Taeyoon Kwon</a>
|
<a href=/people/s/seungjun-moon/>Seungjun Moon</a>
|
<a href=/people/y/yongho-song/>Yongho Song</a>
|
<a href=/people/d/dongjin-kang/>Dongjin Kang</a>
|
<a href=/people/k/kai-tzu-iunn-ong/>Kai Tzu-iunn Ong</a>
|
<a href=/people/b/beong-woo-kwak/>Beong-woo Kwak</a>
|
<a href=/people/s/seonghyeon-bae/>Seonghyeon Bae</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a>
|
<a href=/people/j/jinyoung-yeo/>Jinyoung Yeo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1254><div class="card-body p-3 small">This paper presents Coffee-Gym, a comprehensive RL environment for training models that provide feedback on code editing. Coffee-Gym includes two major components: (1) Coffee, a dataset containing humans’ code edit traces for coding questions and human-written feedback for editing erroneous code; (2) CoffeeEval, a reward function that faithfully reflects the helpfulness of feedback by assessing the performance of the revised code in unit tests. With them, Coffee-Gym addresses the unavailability of high-quality datasets for training feedback models with RL, and provides more accurate rewards than the SOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback models that outperform baselines in enhancing open-source code LLMs’ code editing, making them comparable with closed-source LLMs. We make the dataset and the model checkpoint publicly available in https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1255.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1255 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1255/>Improving Minimum <span class=acl-fixed-case>B</span>ayes Risk Decoding with Multi-Prompt</a></strong><br><a href=/people/d/david-heineman/>David Heineman</a>
|
<a href=/people/y/yao-dou/>Yao Dou</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1255><div class="card-body p-3 small">While instruction fine-tuned LLMs are effective text generators, sensitivity to prompt construction makes performance unstable and sub-optimal in practice. Relying on a single ‘best’ prompt cannot capture all differing approaches to a generation problem. Using this observation, we propose multi-prompt decoding, where many candidate generations are decoded from a prompt bank at inference-time. To ensemble candidates, we use Minimum Bayes Risk (MBR) decoding, which selects a final output using a trained value metric. We show multi-prompt improves MBR across a comprehensive set of conditional generation tasks, and show this is a result of estimating a more diverse and higher quality candidate space than that of a single prompt. Our experiments confirm multi-prompt improves generation across tasks, models and metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1256.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1256 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1256.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1256.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1256/>Deciphering Cognitive Distortions in Patient-Doctor Mental Health Conversations: A Multimodal <span class=acl-fixed-case>LLM</span>-Based Detection and Reasoning Framework</a></strong><br><a href=/people/g/gopendra-vikram-singh/>Gopendra Vikram Singh</a>
|
<a href=/people/s/sai-vardhan-vemulapalli/>Sai Vardhan Vemulapalli</a>
|
<a href=/people/m/mauajama-firdaus/>Mauajama Firdaus</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1256><div class="card-body p-3 small">Cognitive distortion research holds increasing significance as it sheds light on pervasive errors in thinking patterns, providing crucial insights into mental health challenges and fostering the development of targeted interventions and therapies. This paper delves into the complex domain of cognitive distortions which are prevalent distortions in cognitive processes often associated with mental health issues. Focusing on patient-doctor dialogues, we introduce a pioneering method for detecting and reasoning about cognitive distortions utilizing Large Language Models (LLMs). Operating within a multimodal context encompassing audio, video, and textual data, our approach underscores the critical importance of integrating diverse modalities for a comprehensive understanding of cognitive distortions. By leveraging multimodal information, including audio, video, and textual data, our method offers a nuanced perspective that enhances the accuracy and depth of cognitive distortion detection and reasoning in a zero-shot manner. Our proposed hierarchical framework adeptly tackles both detection and reasoning tasks, showcasing significant performance enhancements compared to current methodologies. Through comprehensive analysis, we elucidate the efficacy of our approach, offering promising insights into the diagnosis and understanding of cognitive distortions in multimodal settings.The code and dataset can be found here: <a href=https://github.com/clang1234/ZS-CoDR.git class=acl-markup-url>https://github.com/clang1234/ZS-CoDR.git</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1257.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1257.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1257 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1257 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1257/>Nearest Neighbor Normalization Improves Multimodal Retrieval</a></strong><br><a href=/people/n/neil-chowdhury/>Neil Chowdhury</a>
|
<a href=/people/f/franklin-wang/>Franklin Wang</a>
|
<a href=/people/s/sumedh-shenoy/>Sumedh Shenoy</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/s/sarah-schwettmann/>Sarah Schwettmann</a>
|
<a href=/people/t/tristan-thrush/>Tristan Thrush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1257><div class="card-body p-3 small">Multimodal models leverage large-scale pretraining to achieve strong but still imperfect performance on tasks such as image captioning, visual question answering, and cross-modal retrieval. In this paper, we present a simple and efficient method for correcting errors in trained contrastive image-text retrieval models with no additional training, called Nearest Neighbor Normalization (NNN). We show an improvement on retrieval metrics in both text retrieval and image retrieval for all of the contrastive models that we tested (CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used (MS-COCO and Flickr30k). NNN requires a reference database, but does not require any training on this database, and can even increase the retrieval accuracy of a model after finetuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1258.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1258 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1258/>Rethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning</a></strong><br><a href=/people/s/shengguang-wu/>Shengguang Wu</a>
|
<a href=/people/s/shusheng-yang/>Shusheng Yang</a>
|
<a href=/people/z/zhenglun-chen/>Zhenglun Chen</a>
|
<a href=/people/q/qi-su/>Qi Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1258><div class="card-body p-3 small">This study addresses the challenges of assessing and enhancing social-pragmatic inference in large language models (LLMs). We first highlight the inadequacy of current accuracy-based multiple choice question answering (MCQA) formats in assessing social-pragmatic reasoning, and propose the direct evaluation of models’ free-form responses as measure, which correlates better with human judgment. Furthermore, we explore methods to improve pragmatic abilities in LLMs, advocating for preference optimization (PO) over supervised finetuning (SFT), given the absence of a definitive “gold” answer in social contexts. Our results show that preferential tuning consistently outperforms SFT across pragmatic phenomena and offers a near-free launch in pragmatic abilities without compromising general capabilities. Lastly, we examine the internal structure of LLMs, revealing that the significant boost in pragmatic reasoning is tied to deeper layer representations, analogous to human high-level thinking. Our experiments span a variety of pragmatic and social reasoning datasets, as well as an image referential game requiring a multimodal theory of mind (ToM). With our refined paradigms for evaluating and enhancing pragmatic inference, this paper offers key insights into building more socially aware language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1259.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1259 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1259/><span class=acl-fixed-case>L</span>ong<span class=acl-fixed-case>RAG</span>: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering</a></strong><br><a href=/people/q/qingfei-zhao/>Qingfei Zhao</a>
|
<a href=/people/r/ruobing-wang/>Ruobing Wang</a>
|
<a href=/people/y/yukuo-cen/>Yukuo Cen</a>
|
<a href=/people/d/daren-zha/>Daren Zha</a>
|
<a href=/people/s/shicheng-tan/>Shicheng Tan</a>
|
<a href=/people/y/yuxiao-dong/>Yuxiao Dong</a>
|
<a href=/people/j/jie-tang/>Jie Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1259><div class="card-body p-3 small">Long-Context Question Answering (LCQA), a challenging task, aims to reason over long-context documents to yield accurate answers to questions. Existing long-context Large Language Models (LLMs) for LCQA often struggle with the “lost in the middle” issue. Retrieval-Augmented Generation (RAG) mitigates this issue by providing external factual evidence. However, its chunking strategy disrupts the global long-context information, and its low-quality retrieval in long contexts hinders LLMs from identifying effective factual details due to substantial noise. To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG’s understanding of complex long-context knowledge (i.e., global information and factual details). We design LongRAG as a plug-and-play paradigm, facilitating adaptation to various domains and LLMs. Extensive experiments on three multi-hop datasets demonstrate that LongRAG significantly outperforms long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%). Furthermore, we conduct quantitative ablation studies and multi-dimensional analyses, highlighting the effectiveness of the system’s components and fine-tuning strategies.Data and code are available at [https://github.com/QingFei1/LongRAG](https://github.com/QingFei1/LongRAG).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1260.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1260.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1260 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1260 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1260/>Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models</a></strong><br><a href=/people/y/yuxuan-guo/>Yuxuan Guo</a>
|
<a href=/people/z/zhiliang-tian/>Zhiliang Tian</a>
|
<a href=/people/y/yiping-song/>Yiping Song</a>
|
<a href=/people/t/tianlun-liu/>Tianlun Liu</a>
|
<a href=/people/l/liang-ding/>Liang Ding</a>
|
<a href=/people/d/dongsheng-li/>Dongsheng Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1260><div class="card-body p-3 small">Watermarking enables people to determine whether the text is generated by a specific model. It injects a unique signature based on the “green-red” list that can be tracked during detection, where the words in green lists are encouraged to be generated. Recent researchers propose to fix the green/red lists or increase the proportion of green tokens to defend against paraphrasing attacks. However, these methods cause degradation of text quality due to semantic disparities between the watermarked text and the unwatermarked text. In this paper, we propose a semantic-aware watermark method that considers contexts to generate a semantic-aware key to split a semantically balanced green/red list for watermark injection. The semantic balanced list reduces the performance drop due to adding bias on green lists. To defend against paraphrasing attacks, we generate the watermark key considering the semantics of contexts via locally sensitive hashing. To improve the text quality, we propose to split green/red lists considering semantics to enable the green list to cover almost all semantics. We also dynamically adapt the bias to balance text quality and robustness. The experiments show our advantages in both robustness and text quality comparable to existing baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1261.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1261.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1261 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1261 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1261.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1261.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1261/>Knowledge Graph Enhanced Large Language Model Editing</a></strong><br><a href=/people/m/mengqi-zhang/>Mengqi Zhang</a>
|
<a href=/people/x/xiaotian-ye/>Xiaotian Ye</a>
|
<a href=/people/q/qiang-liu/>Qiang Liu</a>
|
<a href=/people/p/pengjie-ren/>Pengjie Ren</a>
|
<a href=/people/s/shu-wu/>Shu Wu</a>
|
<a href=/people/z/zhumin-chen/>Zhumin Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1261><div class="card-body p-3 small">Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge. Model editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of post-edit LLMs in processing edited knowledge. To tackle these problems, we propose a novel model editing method that leverages knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we first utilize a knowledge graph augmentation module to uncover associated knowledge that has changed due to editing, obtaining its internal representations within LLMs. This approach allows knowledge alterations within LLMs to be reflected through an external graph structure. Subsequently, we design a graph-based knowledge edit module to integrate structured knowledge into the model editing. This ensures that the updated parameters reflect not only the modifications of the edited knowledge but also the changes in other associated knowledge resulting from the editing process. Comprehensive experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME significantly improves the generalization capabilities of post-edit LLMs in employing edited knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1262.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1262/>‘Quis custodiet ipsos custodes?’ Who will watch the watchmen? On Detecting <span class=acl-fixed-case>AI</span>-generated peer-reviews</a></strong><br><a href=/people/s/sandeep-kumar/>Sandeep Kumar</a>
|
<a href=/people/m/mohit-sahu/>Mohit Sahu</a>
|
<a href=/people/v/vardhan-gacche/>Vardhan Gacche</a>
|
<a href=/people/t/tirthankar-ghosal/>Tirthankar Ghosal</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1262><div class="card-body p-3 small">The integrity of the peer-review process is vital for maintaining scientific rigor and trust within the academic community. With the steady increase in the usage of large language models (LLMs) like ChatGPT in academic writing, there is a growing concern that AI-generated texts could compromise the scientific publishing including peer-reviews. Previous works have focused on generic AI-generated text detection or have presented an approach for estimating the fraction of peer-reviews that can be AI-generated. Our focus here is to solve a real-world problem by assisting the editor or chair in determining whether a review is written by ChatGPT or not. To address this, we introduce the Term Frequency (TF) model, which posits that AI often repeats tokens, and the Review Regeneration (RR) model which is based on the idea that ChatGPT generates similar outputs upon re-prompting. We stress test these detectors against token attack and paraphrasing. Finally we propose an effective defensive strategy to reduce the effect of paraphrasing on our models. Our findings suggest both our proposed methods perform better than other AI text detectors. Our RR model is more robust, although our TF model performs better than the RR model without any attacks. We make our code, dataset, model public.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1263.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1263 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1263.data.zip data-toggle=tooltip data-placement=top title=Data><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1263/>Mitigating Open-Vocabulary Caption Hallucinations</a></strong><br><a href=/people/a/assaf-ben-kish/>Assaf Ben-Kish</a>
|
<a href=/people/m/moran-yanuka/>Moran Yanuka</a>
|
<a href=/people/m/morris-alper/>Morris Alper</a>
|
<a href=/people/r/raja-giryes/>Raja Giryes</a>
|
<a href=/people/h/hadar-averbuch-elor/>Hadar Averbuch-Elor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1263><div class="card-body p-3 small">While recent years have seen rapid progress in image-conditioned text generation, image captioning still suffers from the fundamental issue of hallucinations, namely, the generation of spurious details that cannot be inferred from the given image. Existing methods largely use closed-vocabulary object lists to mitigate or evaluate hallucinations in image captioning, ignoring the long-tailed nature of hallucinations that occur in practice. To this end, we propose a framework for addressing hallucinations in image captioning in the open-vocabulary setting. Our framework includes a new benchmark, OpenCHAIR, that leverages generative foundation models to evaluate open-vocabulary object hallucinations for image captioning, surpassing the popular and similarly-sized CHAIR benchmark in both diversity and accuracy. Furthermore, to mitigate open-vocabulary hallucinations without using a closed object list, we propose MOCHa, an approach harnessing advancements in reinforcement learning. Our multi-objective reward function explicitly targets the trade-off between fidelity and adequacy in generations without requiring any strong supervision. MOCHa improves a large variety of image captioning models, as captured by our OpenCHAIR benchmark and other existing metrics. We will release our code and models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1264.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1264/>Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes</a></strong><br><a href=/people/k/kosuke-nishida/>Kosuke Nishida</a>
|
<a href=/people/k/kyosuke-nishida/>Kyosuke Nishida</a>
|
<a href=/people/k/kuniko-saito/>Kuniko Saito</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1264><div class="card-body p-3 small">Loss spikes, a phenomenon in which the loss value diverges suddenly, is a fundamental issue in the pre-training of large language models. This paper supposes that the non-uniformity of the norm of the parameters is one of the causes of loss spikes. Here, in training of neural networks, the scale of the gradients is required to be kept constant throughout the layers to avoid the vanishing and exploding gradients problem. However, to meet these requirements in the Transformer model, the norm of the model parameters must be non-uniform, and thus, parameters whose norm is smaller are more sensitive to the parameter update. To address this issue, we propose a novel technique, weight scaling as reparameterization (WeSaR). WeSaR introduces a gate parameter per parameter matrix and adjusts it to the value satisfying the requirements. Because of the gate parameter, WeSaR sets the norm of the original parameters uniformly, which results in stable training. Experimental results with the Transformer decoders consisting of 130 million, 1.3 billion, and 13 billion parameters showed that WeSaR stabilizes and accelerates training and that it outperformed compared methods including popular initialization methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1265.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1265.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1265 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1265 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1265/><span class=acl-fixed-case>ALVIN</span>: Active Learning Via <span class=acl-fixed-case>IN</span>terpolation</a></strong><br><a href=/people/m/michalis-korakakis/>Michalis Korakakis</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/a/adrian-weller/>Adrian Weller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1265><div class="card-body p-3 small">Active Learning aims to minimize annotation effort by selecting the most useful instances from a pool of unlabeled data. However, typical active learning methods overlook the presence of distinct example groups within a class, whose prevalence may vary, e.g., in occupation classification datasets certain demographics are disproportionately represented in specific classes. This oversight causes models to rely on shortcuts for predictions, i.e., spurious correlations between input attributes and labels occurring in well-represented groups. To address this issue, we propose Active Learning Via INterpolation (ALVIN), which conducts intra-class interpolations between examples from under-represented and well-represented groups to create anchors, i.e., artificial points situated between the example groups in the representation space. By selecting instances close to the anchors for annotation, ALVIN identifies informative examples exposing the model to regions of the representation space that counteract the influence of shortcuts. Crucially, since the model considers these examples to be of high certainty, they are likely to be ignored by typical active learning methods. Experimental results on six datasets encompassing sentiment analysis, natural language inference, and paraphrase detection demonstrate that ALVIN outperforms state-of-the-art active learning methods in both in-distribution and out-of-distribution generalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1266.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1266 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1266/>Filtered Direct Preference Optimization</a></strong><br><a href=/people/t/tetsuro-morimura/>Tetsuro Morimura</a>
|
<a href=/people/m/mitsuki-sakamoto/>Mitsuki Sakamoto</a>
|
<a href=/people/y/yuu-jinnai/>Yuu Jinnai</a>
|
<a href=/people/k/kenshi-abe/>Kenshi Abe</a>
|
<a href=/people/k/kaito-ariu/>Kaito Ariu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1266><div class="card-body p-3 small">Reinforcement learning from human feedback (RLHF) plays a crucial role in aligning language models with human preferences. While the significance of dataset quality is generally recognized, explicit investigations into its impact within the RLHF framework, to our knowledge, have been limited. This paper addresses the issue of text quality within the preference dataset by focusing on direct preference optimization (DPO), an increasingly adopted reward-model-free RLHF method. We confirm that text quality significantly influences the performance of models optimized with DPO more than those optimized with reward-model-based RLHF. Building on this new insight, we propose an extension of DPO, termed filtered direct preference optimization (fDPO). fDPO uses a trained reward model to monitor the quality of texts within the preference dataset during DPO training. Samples of lower quality are discarded based on comparisons with texts generated by the model being optimized, resulting in a more accurate dataset. Experimental results demonstrate that fDPO enhances the final model performance. Our code is available at https://github.com/CyberAgentAILab/filtered-dpo.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1267.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1267 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1267/>Instruction Fine-Tuning: Does Prompt Loss Matter?</a></strong><br><a href=/people/m/mathew-huerta-enochian/>Mathew Huerta-Enochian</a>
|
<a href=/people/s/seung-yong-ko/>Seung Yong Ko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1267><div class="card-body p-3 small">We present a novel study analyzing the effects of various prompt loss token weights (PLW) for supervised instruction fine-tuning (SIFT). While prompt-masking (PLW = 0) is common for SIFT, some fine-tuning APIs support fractional PLWs and suggest that using a small non-zero PLW can help stabilize learning when fine-tuning on short-completion data. However, there has never been a study confirming this claim, and OpenAI, a major cloud-based SIFT provider, recently removed this parameter from their fine-tuning API. We found that performance of models fine-tuned on short-completion data had a statistically-significant negative quadratic relationship with PLW. Using small values (0.01 − 0.5) of PLW produced better results on multiple-choice and short-generation benchmarks (outperforming models fine-tuned on long-completion data) while large values (≈ 1.0) of PLW produced better results on long-generation benchmarks. We explained this effect and verified its importance through additional experiments. This research serves as a warning to API providers about the importance of providing a PLW parameter for SIFT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2024.emnlp-main.1268.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2024.emnlp-main.1268.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2024--emnlp-main--1268 data-toggle=collapse aria-expanded=false aria-controls=abstract-2024.emnlp-main.1268 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2024.emnlp-main.1268.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2024.emnlp-main.1268/>Entity Insertion in Multilingual Linked Corpora: The Case of <span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/t/tomas-feith/>Tomás Feith</a>
|
<a href=/people/a/akhil-arora/>Akhil Arora</a>
|
<a href=/people/m/martin-gerlach/>Martin Gerlach</a>
|
<a href=/people/d/debjit-paul/>Debjit Paul</a>
|
<a href=/people/r/robert-west/>Robert West</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2024--emnlp-main--1268><div class="card-body p-3 small">Links are a fundamental part of information networks, turning isolated pieces of knowledge into a network of information that is much richer than the sum of its parts. However, adding a new link to the network is not trivial: it requires not only the identification of a suitable pair of source and target entities but also the understanding of the content of the source to locate a suitable position for the link in the text. The latter problem has not been addressed effectively, particularly in the absence of text spans in the source that could serve as anchors to insert a link to the target entity. To bridge this gap, we introduce and operationalize the task of entity insertion in information networks. Focusing on the case of Wikipedia, we empirically show that this problem is, both, relevant and challenging for editors. We compile a benchmark dataset in 105 languages and develop a framework for entity insertion called LocEI (Localized Entity Insertion) and its multilingual variant XLocEI. We show that XLocEI outperforms all baseline models (including state-of-the-art prompt-based ranking with LLMs such as GPT-4) and that it can be applied in a zero-shot manner on languages not seen during training with minimal performance drop. These findings are important for applying entity insertion models in practice, e.g., to support editors in adding links across the more than 300 language versions of Wikipedia.</div></div></div></section></div><footer class="bg-gradient-light py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5"><div class=container><p class="text-muted small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2024 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="text-muted small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="text-muted small px-1"><i>Site last built on 15 November 2024 at 05:39 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/49daaa082e5f4df3054c713e20ac52ca44873487>commit 49daaa0</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>