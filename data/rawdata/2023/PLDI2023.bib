@article{10.1145/3591220,
author = {Bagnall, Alexander and Stewart, Gordon and Banerjee, Anindya},
title = {Formally Verified Samplers from Probabilistic Programs with Loops and Conditioning},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591220},
doi = {10.1145/3591220},
abstract = {We present Zar: a formally verified compiler pipeline from discrete probabilistic programs with unbounded loops in the conditional probabilistic guarded command language (cpGCL) to proved-correct executable samplers in the random bit model. We exploit the key idea that all discrete probability distributions can be reduced to unbiased coin-flipping schemes. The compiler pipeline first translates cpGCL programs into choice-fix trees, an intermediate representation suitable for reduction of biased probabilistic choices. Choice-fix trees are then translated to coinductive interaction trees for execution within the random bit model. The correctness of the composed translations establishes the sampling equidistribution theorem: compiled samplers are correct wrt. the conditional weakest pre-expectation semantics of cpGCL source programs. Zar is implemented and fully verified in the Coq proof assistant. We extract verified samplers to OCaml and Python and empirically validate them on a number of illustrative examples.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {106},
numpages = {24},
keywords = {Probabilistic Programming, Verified Compilers}
}

@software{10.5281/zenodo.7809333,
author = {Bagnall, Alexander and Stewart, Gordon and Banerjee, Anindya},
title = {Artifact for PLDI'23 paper 'Formally Verified Samplers From Probabilistic Programs With Loops and Conditioning'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7809333},
abstract = {
    <p>The artifact contains the Coq proof development and empirical evaluation scripts for reproducing the results of the paper ‘Formally Verified Samplers From Probabilistic Programs With Loops and Conditioning’. A Dockerfile is included for building a Docker container image with all dependencies installed.</p>

},
keywords = {Probabilistic Programming, Verified Compilers}
}

@article{10.1145/3591221,
author = {Gopinathan, Kiran and Keoliya, Mayank and Sergey, Ilya},
title = {Mostly Automated Proof Repair for Verified Libraries},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591221},
doi = {10.1145/3591221},
abstract = {The cost of maintaining formally specified and verified software is widely considered prohibitively high due to the need to constantly keep code and the proofs of its correctness in sync—the problem known as proof repair. One of the main challenges in automated proof repair for evolving code is to infer invariants for a new version of a once verified program that are strong enough to establish its full functional correctness. In this work, we present the first proof repair methodology for higher-order imperative functions, whose initial versions were verified in the Coq proof assistant and whose specifications remained unchanged. Our proof repair procedure is based on the combination of dynamic program alignment, enumerative invariant synthesis, and a novel technique for efficiently pruning the space of invariant candidates, dubbed proof-driven testing, enabled by the constructive nature of Coq’s proof certificates. We have implemented our approach in a mostly-automated proof repair tool called Sisyphus. Given an OCaml function verified in Coq and its unverified new version, Sisyphus produces a Coq proof for the new version, discharging most of the new proof goals automatically and suggesting high-confidence obligations for the programmer to prove for the cases when automation fails. We have evaluated Sisyphus on 10 OCaml programs taken from popular libraries, that manipulate arrays and mutable data structures, considering their verified original and unverified evolved versions. Sisyphus has managed to repair proofs for all those functions, suggesting correct invariants and generating a small number of easy-to-prove residual obligations.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {107},
numpages = {25},
keywords = {invariant inference, mechanised proofs, proof repair, separation logic}
}

@software{10.5281/zenodo.7703886,
author = {Gopinathan, Kiran and Keoliya, Mayank and Sergey, Ilya},
title = {Reproduction Artefact for Article 'Mostly Automated Proof Repair for Verified Libraries'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7703886},
abstract = {
    <p>This artefact is for our tool, Sisyphus, which is a functional, reusable, and extensible framework for automated repair of Coq proofs.</p>
<p>The artefact contains the source code and build scripts for Sisyphus, a corpus of individual OCaml programs which can be used to reproduce the experimental results in the paper, and a self-contained Docker file to automate setting up the development environment.</p>
<p>The artefact also contains a README in markdown that provides detailed step-by-step instructions for running Sisyphus and reproducing the experimental results.</p>

},
keywords = {invariant inference, mechanised proofs, proof repair, separation logic}
}

@article{10.1145/3591222,
author = {Alberdingk Thijm, Timothy and Beckett, Ryan and Gupta, Aarti and Walker, David},
title = {Modular Control Plane Verification via Temporal Invariants},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591222},
doi = {10.1145/3591222},
abstract = {Monolithic control plane verification cannot scale to hyperscale network architectures with tens of thousands of nodes, heterogeneous network policies and thousands of network changes a day. Instead, modular verification offers improved scalability, reasoning over diverse behaviors, and robustness following policy updates. We introduce Timepiece, a new modular control plane verification system. While one class of verifiers, starting with Minesweeper, were based on analysis of stable paths, we show that such models, when deployed na\"{\i}vely for modular verification, are unsound. To rectify the situation, we adopt a routing model based around a logical notion of time and develop a sound, expressive, and scalable verification engine.  

Our system requires that a user specifies interfaces between module components. We develop methods for defining these interfaces using predicates inspired by temporal logic, and show how to use those interfaces to verify a range of network-wide properties such as reachability or access control. Verifying a prefix-filtering policy using a non-modular verification engine times out on an 80-node fattree network after 2 hours. However, Timepiece verifies a 2,000-node fattree in 2.37 minutes on a 96-core virtual machine. Modular verification of individual routers is embarrassingly parallel and completes in seconds, which allows verification to scale beyond non-modular engines, while still allowing the full power of SMT-based symbolic reasoning.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {108},
numpages = {26},
keywords = {compositional reasoning, formal network verification, modular verification}
}

@software{10.5281/zenodo.7799158,
author = {Alberdingk Thijm, Timothy and Beckett, Ryan and Gupta, Aarti and Walker, David},
title = {Artifact associated with the PLDI 2023 submission "Modular Control Plane Verification via Temporal Invariants".},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7799158},
abstract = {
    <p>The artifact provides the code of the Timepiece GitHub repository, which includes the code implementing our modular verification procedure, along with libraries to reproduce and run the benchmarks given in the paper. Fattree benchmarks are contained in the Timepiece.Benchmarks subproject. The Internet2 benchmark can be run by using the Timepiece.Angler subproject to convert an .angler.json file into a Timepiece benchmark: such a .json file can be created from the included Angler repository, which translates network configurations read by Batfish to the .angler.json format. The core of our implementation can be found in the Timepiece subproject of the repository.</p>
<p>Tooling is included to generate the plots from our paper via pgfplots, using a template plot.tex file. We also include a run_all.py Python script for conveniently running and gathering data on our fattree benchmarks, and a Dockerfile for building a Docker image to run the code.</p>

},
keywords = {control plane, dotnet, modular verification, network configurations, network verification, python}
}

@article{10.1145/3591223,
author = {Cascaval, Dan and Bodik, Rastislav and Schulz, Adriana},
title = {A Lineage-Based Referencing DSL for Computer-Aided Design},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591223},
doi = {10.1145/3591223},
abstract = {3D Computer-Aided Design (CAD) modeling is ubiquitous in mechanical engineering and design. Modern CAD models are programs that produce geometry and can be used to implement high-level geometric changes by modifying input parameters. While there has been a surge of recent interest in program-based tooling for the CAD domain, one fundamental problem remains unsolved. CAD programs pass geometric arguments to operations using references, which are queries that select elements from the constructed geometry according to programmer intent. The challenge is designing reference semantics that can express programmer intent across all geometric topologies achievable with model parameters, including topologies where the desired elements are not present. In current systems, both users and automated tools may create invalid models when parameters are changed, as references to geometric elements are lost or silently and arbitrarily switched. While existing CAD systems use heuristics to attempt to infer user intent in cases of this undefined behavior, this best-effort solution is not suitable for constructing automated tools to edit and optimize CAD programs. We analyze the failure modes of existing referencing schemes and formalize a set of criteria on which to evaluate solutions to the CAD referencing problem. In turn, we propose a domain-specific language that exposes references as a first-class language construct, using user-authored queries to introspect element history and define references safely over all paths. We give a semantics for fine-grained element lineage that can subsequently be queried; and show that our language meets the desired properties. Finally, we provide an implementation of a lineage-based referencing system in a 2.5D CAD kernel, demonstrating realistic referencing scenarios and illustrating how our system safely represents models that cause reference breakage in existing CAD systems.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {109},
numpages = {24},
keywords = {computer-aided design programs, persistent naming, shape modeling}
}

@article{10.1145/3591224,
author = {Watt, Conrad and Trela, Maja and Lammich, Peter and M\"{a}rkl, Florian},
title = {WasmRef-Isabelle: A Verified Monadic Interpreter and Industrial Fuzzing Oracle for WebAssembly},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591224},
doi = {10.1145/3591224},
abstract = {We present WasmRef-Isabelle, a monadic interpreter for WebAssembly written in Isabelle/HOL and proven correct with respect to the WasmCert-Isabelle mechanisation of WebAssembly. WasmRef-Isabelle has been adopted and deployed as a fuzzing oracle in the continuous integration infrastructure of Wasmtime, a widely used WebAssembly implementation. Previous efforts to fuzz Wasmtime against WebAssembly's official OCaml reference interpreter were abandoned by Wasmtime's developers after the reference interpreter exhibited unacceptable performance characteristics, which its maintainers decided not to fix in order to preserve the interpreter's close definitional correspondence with the official specification. With WasmRef-Isabelle, we achieve the best of both worlds - an interpreter fast enough to be useable as a fuzzing oracle that also maintains a close correspondence with the specification through a mechanised proof of correctness.  

We verify the correctness of WasmRef-Isabelle through a two-step refinement proof in Isabelle/HOL. We demonstrate that WasmRef-Isabelle significantly outperforms the official reference interpreter, has performance comparable to a Rust debug build of the industry WebAssembly interpreter Wasmi, and competes with unverified oracles on fuzzing throughput when deployed in Wasmtime's fuzzing infrastructure. We also present several new extensions to WasmCert-Isabelle which enhance WasmRef-Isabelle's utility as a fuzzing oracle: we add support for a number of upcoming WebAssembly features, and fully mechanise the numeric semantics of WebAssembly's integer operations.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {110},
numpages = {24},
keywords = {WasmCert, refinement, theorem proving, virtual machine}
}

@software{10.5281/zenodo.7815663,
author = {Watt, Conrad and Trela, Maja and Lammich, Peter and M\"{a}rkl, Florian},
title = {Supplementary material for WasmRef-Isabelle},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7815663},
abstract = {
    <p>Supplementary figures and graphs as mentioned in the main publication, and an archive of our Isabelle/HOL code.</p>

},
keywords = {WasmCert}
}

@article{10.1145/3591225,
author = {Tarek Ibn Ziad, Mohamed and Damani, Sana and Jaleel, Aamer and Keckler, Stephen W. and Stephenson, Mark},
title = {cuCatch: A Debugging Tool for Efficiently Catching Memory Safety Violations in CUDA Applications},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591225},
doi = {10.1145/3591225},
abstract = {CUDA, OpenCL, and OpenACC are the primary means of writing general-purpose software for NVIDIA GPUs, all of which are subject to the same well-documented memory safety vulnerabilities currently plaguing software written in C and C++. One can argue that the GPU execution environment makes software development more error prone. Unlike C and C++, CUDA features multiple, distinct memory spaces to map to the GPU’s unique memory hierarchy, and a typical CUDA program has thousands of concurrently executing threads. Furthermore, the CUDA platform has fewer guardrails than CPU platforms that have been forced to incrementally adjust to a barrage of security attacks. Unfortunately, the peculiarities of the GPU make it difficult to directly port memory safety solutions from the CPU space. This paper presents cuCatch, a new memory safety error detection tool designed specifically for the CUDA programming model. cuCatch combines optimized compiler instrumentation with driver support to implement a novel algorithm for catching spatial and temporal memory safety errors with low performance overheads. Our experimental results on a wide set of GPU applications show that cuCatch incurs a 19\% runtime slowdown on average, which is orders of magnitude faster than state-of-the-art debugging tools on GPUs. Moreover, our quantitative evaluation demonstrates cuCatch’s higher error detection coverage compared to prior memory safety tools. The combination of high error detection coverage and low runtime overheads makes cuCatch an ideal candidate for accelerating memory safety debugging for GPU applications.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {111},
numpages = {24},
keywords = {GPU, bug detection, memory safety}
}

@article{10.1145/3591226,
author = {Li, John M. and Ahmed, Amal and Holtzen, Steven},
title = {Lilac: A Modal Separation Logic for Conditional Probability},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591226},
doi = {10.1145/3591226},
abstract = {We present Lilac, a separation logic for reasoning about probabilistic programs where separating conjunction captures probabilistic independence. Inspired by an analogy with mutable state where sampling corresponds to dynamic allocation, we show how probability spaces over a fixed, ambient sample space appear to be the natural analogue of heap fragments, and present a new combining operation on them such that probability spaces behave like heaps and measurability of random variables behaves like ownership. This combining operation forms the basis for our model of separation, and produces a logic with many pleasant properties. In particular, Lilac has a frame rule identical to the ordinary one, and naturally accommodates advanced features like continuous random variables and reasoning about quantitative properties of programs. Then we propose a new modality based on disintegration theory for reasoning about conditional probability. We show how the resulting modal logic validates examples from prior work, and give a formal verification of an intricate weighted sampling algorithm whose correctness depends crucially on conditional independence structure.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {112},
numpages = {24},
keywords = {probabilistic programming, separation logic}
}

@article{10.1145/3591227,
author = {Gao, Fengjuan and Wang, Yu and Wang, Ke},
title = {Discrete Adversarial Attack to Models of Code},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591227},
doi = {10.1145/3591227},
abstract = {The pervasive brittleness of deep neural networks has attracted significant attention in recent years. A particularly interesting finding is the existence of adversarial examples, imperceptibly perturbed natural inputs that induce erroneous predictions in state-of-the-art neural models. In this paper, we study a different type of adversarial examples specific to code models, called discrete adversarial examples, which are created through program transformations that preserve the semantics of original inputs.In particular, we propose a novel, general method that is highly effective in attacking a broad range of code models. From the defense perspective, our primary contribution is a theoretical foundation for the application of adversarial training — the most successful algorithm for training robust classifiers — to defending code models against discrete adversarial attack. Motivated by the theoretical results, we present a simple realization of adversarial training that substantially improves the robustness of code models against adversarial attacks in practice. We extensively evaluate both our attack and defense methods. Results show that our discrete attack is significantly more effective than state-of-the-art whether or not defense mechanisms are in place to aid models in resisting attacks. In addition, our realization of adversarial training improves the robustness of all evaluated models by the widest margin against state-of-the-art adversarial attacks as well as our own.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {113},
numpages = {24},
keywords = {Adversarial Training, Discrete Adversarial Attack, Models of code}
}

@article{10.1145/3591228,
author = {Park, Sunjae and Song, Woosung and Nam, Seunghyeon and Kim, Hyeongyu and Shin, Junbum and Lee, Juneyoung},
title = {HEaaN.MLIR: An Optimizing Compiler for Fast Ring-Based Homomorphic Encryption},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591228},
doi = {10.1145/3591228},
abstract = {Homomorphic encryption (HE) is an encryption scheme that provides arithmetic operations on the encrypted data without doing decryption.  
For Ring-based HE, an encryption scheme that uses arithmetic operations on a polynomial ring as building blocks, performance improvement of unit HE operations has been achieved by two kinds of efforts.  
The first one is through accelerating the building blocks, polynomial operations.  
However, it does not facilitate optimizations across polynomial operations such as fusing two polynomial operations.  
The second one is implementing highly optimized HE operations in an amalgamated manner.  
The written codes have superior performance, but they are hard to maintain.  

To resolve these challenges, we propose HEaaN.MLIR, a compiler that performs optimizations across polynomial operations.  
Also, we propose Poly and ModArith, compiler intermediate representations (IRs) for integer polynomial arithmetic and modulus arithmetic on integer arrays.  
HEaaN.MLIR has compiler optimizations that are motivated by manual optimizations that HE developers do.  
These include optimizing modular arithmetic operations, fusing loops, and vectorizing integer arithmetic instructions.  
HEaaN.MLIR can parse a program consisting of the Poly and ModArith instructions and generate a high-performance, multithreaded machine code for a CPU.  
Our experiment shows that the compiled operations outperform heavily optimized open-source and commercial HE libraries by up to 3.06x in a single thread and 4.55x in multiple threads.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {114},
numpages = {25},
keywords = {Compiler, Homomorphic Encryption}
}

@article{10.1145/3591229,
author = {Elsman, Martin},
title = {Garbage-Collection Safety for Region-Based Type-Polymorphic Programs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591229},
doi = {10.1145/3591229},
abstract = {Region inference offers a mechanism to reduce (and sometimes entirely remove) the need for reference-tracing garbage collection by inferring where to insert allocation and deallocation instructions in a program at compile time. When the mechanism is combined with techniques for reference-tracing garbage collection, which is helpful in general to support programs with very dynamic memory behaviours, it turns out that region-inference is complementary to adding generations to a reference-tracing collector. However, region-inference and the associated region-representation analyses that make such a memory management strategy perform well in practice are complex, both from a theoretical point-of-view and from an implementation point-of-view. 

In this paper, we demonstrate a soundness problem with existing theoretical developments, which have to do with ensuring that, even for higher-order polymorphic programs, no dangling-pointers appear during a reference-tracing collection. This problem has materialised as a practical soundness problem in a real implementation based on region inference. As a solution, we present a modified, yet simple, region type-system that captures garbage-collection effects, even for polymorphic higher-order code, and outline how region inference and region-representation analyses are adapted to the new type system. The new type system allows for associating simpler region type-schemes with functions, compared to original work, makes it possible to combine region-based memory management with partly tag-free reference-tracing (and generational) garbage-collection, and repairs previously derived work that is based on the erroneous published results.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {115},
numpages = {23},
keywords = {Standard ML, garbage-collection, region-inference}
}

@software{10.5281/zenodo.7803910,
author = {Elsman, Martin},
title = {Artifact for the PLDI 2023 paper 'Garbage-Collection Safety for Region-Based Type-Polymorphic Programs'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7803910},
abstract = {
    <p>This artifact reproduces the content of Figure 9 in the paper. For a detailed overview of the artifact, including instructions on how to use it, please consult the Zenodo page.</p>

},
keywords = {Garbage collection, PLDI, Region inference, SML}
}

@article{10.1145/3591230,
author = {Koval, Nikita and Khalanskiy, Dmitry and Alistarh, Dan},
title = {CQS: A Formally-Verified Framework for Fair and Abortable Synchronization},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591230},
doi = {10.1145/3591230},
abstract = {Writing concurrent code that is both correct and efficient is notoriously difficult. Thus, programmers often prefer to use synchronization abstractions, which render code simpler and easier to reason about. Despite a wealth of work on this topic, there is still a gap between the rich semantics provided by synchronization abstractions in modern programming languages—specifically, fair FIFO ordering of synchronization requests and support for abortable operations—and frameworks for implementing it correctly and efficiently. Supporting such semantics is critical given the rising popularity of constructs for asynchronous programming, such as coroutines, which abort frequently and are cheaper to suspend and resume compared to native threads. This paper introduces a new framework called CancellableQueueSynchronizer (CQS), which enables simple yet efficient implementations of a wide range of fair and abortable synchronization primitives: mutexes, semaphores, barriers, count-down latches, and blocking pools. Our main contribution is algorithmic, as implementing both fairness and abortability efficiently at this level of generality is non-trivial. Importantly, all our algorithms, including the CQS framework and the primitives built on top of it, come with formal proofs in the Iris framework for Coq for many of their properties. These proofs are modular, so it is easy to show correctness for new primitives implemented on top of CQS. From a practical perspective, implementation of CQS for native threads on the JVM improves throughput by up to two orders of magnitude over Java’s AbstractQueuedSynchronizer, the only practical abstraction offering similar semantics. Further, we successfully integrated CQS as a core component of the popular Kotlin Coroutines library, validating the framework’s practical impact and expressiveness in a real-world environment. In sum, CancellableQueueSynchronizer is the first framework to combine expressiveness with formal guarantees and solid practical performance. Our approach should be extensible to other languages and families of synchronization primitives.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {116},
numpages = {23},
keywords = {Abortability, Concurrent Data Structures, Iris, Kotlin Coroutines, Synchronization Primitives}
}

@article{10.1145/3591231,
author = {Mangipudi, Shamiek and Chuprikov, Pavel and Eugster, Patrick and Viering, Malte and Savvides, Savvas},
title = {Generalized Policy-Based Noninterference for Efficient Confidentiality-Preservation},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591231},
doi = {10.1145/3591231},
abstract = {As more organizations are leveraging third-party cloud and edge data centers to process data efficiently, the issue of preserving data confidentiality becomes increasingly important. In response, numerous security mechanisms have been introduced and promoted in recent years including software-based ones such as homomorphic encryption, as well as hardware-based ones such as Intel SGX and AMD SEV. However these mechanisms vary in their security properties, performance characteristics, availability, and application modalities, making it hard for programmers to judiciously choose and correctly employ the right one for a given data query.  

This paper presents a mechanism-independent approach to distributed confidentiality-preserving data analytics. Our approach hinges on a core programming language which abstracts the intricacies of individual security mechanisms. Data is labeled using custom confidentiality levels arranged along a lattice in order to capture its exact confidentiality constraints. High-level mappings between available mechanisms and these labels are captured through a novel expressive form of security policy. Confidentiality is guaranteed through a type system based on a novel formulation of noninterference, generalized to support our security policy definition. Queries written in a largely security-agnostic subset of our language are transformed to the full language to automatically use mechanisms in an efficient, possibly combined manner, while provably preserving confidentiality in data queries end-to-end. We prototype our approach as an extension to the popular Apache Spark analytics engine, demonstrating the significant versatility and performance benefits of our approach over single hardwired mechanisms --- including in existing systems --- without compromising on confidentiality.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {117},
numpages = {25},
keywords = {anguage-based security, enclave, homomorphic encryption, noninterference, secure computing, type system}
}

@article{10.1145/3591232,
author = {Cho, Kyeongmin and Jeon, Seungmin and Raad, Azalea and Kang, Jeehoon},
title = {Memento: A Framework for Detectable Recoverability in Persistent Memory},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591232},
doi = {10.1145/3591232},
abstract = {Persistent memory (PM) is an emerging class of storage technology that combines the performance of DRAM with the durability of SSD, offering the best of both worlds. This had led to a surge of research on persistent objects in PM. Among such persistent objects, concurrent data structures (DSs) are particularly interesting thanks to their performance and scalability. One of the most widely used correctness criteria for persistent concurrent DSs is detectable recoverability, ensuring both thread safety (for correctness in non-crashing concurrent executions) and crash consistency (for correctness in crashing executions). However, the existing approaches to designing detectably recoverable concurrent DSs are either limited to simple algorithms or suffer from high runtime overheads. We present Memento: a general and high-performance programming framework for detectably recoverable concurrent DSs in PM. To ensure general applicability to various DSs, Memento supports primitive operations such as checkpoint and compare-and-swap and their composition with control constructs. To ensure high performance, Memento employs a timestamp-based recovery strategy that requires fewer writes and flushes to PM than the existing approaches. We formally prove that Memento ensures detectable recoverability in the presence of crashes. To showcase Memento, we implement a lock-free stack, list, queue, and hash table, and a combining queue that detectably recovers from random crashes in stress tests and performs comparably to existing hand-tuned persistent DSs with and without detectable recoverability.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {118},
numpages = {26},
keywords = {concurrent data structure, detectable recovery, persistent memory}
}

@software{10.5281/zenodo.7811928,
author = {Cho, Kyeongmin and Jeon, Seungmin and Raad, Azalea and Kang, Jeehoon},
title = {Artifact for Article "Memento: A Framework for Detectable Recoverability in Persistent Memory"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7811928},
abstract = {
    <p>The accompanying artifact (also available at https://github.com/kaist-cp/memento) provides the implementation and the experimental results. The artifact is structured as follows.</p>
<ul>
<li><code>/memento/src/</code>: the implementation of the Memento framework and its primitives (§4).</li>
<li><code>/memento/src/</code>: the implementation of detectably persistent data structures (§5).</li>
<li><code>/memento/evaluation/</code>: the experiment script for correctness and performance (§6).</li>
<li><code>/evaluation_data/</code>: the complete experimental results (§6).</li>
</ul>
<p>Please refer to the artifact’s <code>README.md</code> for detailed instructions on how to reproduce the results.</p>

},
keywords = {concurrent data structure, detectable recovery, persistent memory}
}

@article{10.1145/3591233,
author = {Lei, Yuxiang and Sui, Yulei and Tan, Shin Hwei and Zhang, Qirun},
title = {Recursive State Machine Guided Graph Folding for Context-Free Language Reachability},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591233},
doi = {10.1145/3591233},
abstract = {Context-free language reachability (CFL-reachability) is a fundamental framework for program analysis. A large variety of static analyses can be formulated as CFL-reachability problems, which determines whether specific source-sink pairs in an edge-labeled graph are connected by a reachable path, i.e., a path whose edge labels form a string accepted by the given CFL. Computing CFL-reachability is expensive. The fastest algorithm exhibits a slightly subcubic time complexity with respect to the input graph size. Improving the scalability of CFL-reachability is of practical interest, but reducing the time complexity is inherently difficult.  

In this paper, we focus on improving the scalability of CFL-reachability from a more practical perspective---reducing the input graph size. Our idea arises from the existence of trivial edges, i.e., edges that do not affect any reachable path in CFL-reachability. We observe that two nodes joined by trivial edges can be folded---by merging the two nodes with all the edges joining them removed---without affecting the CFL-reachability result. By studying the characteristic of the recursive state machines (RSMs), an alternative form of CFLs, we propose an approach to identify foldable node pairs without the need to verify the underlying reachable paths (which is equivalent to solving the CFL-reachability problem). In particular, given a CFL-reachability problem instance with an input graph G and an RSM, based on the correspondence between paths in G and state transitions in RSM, we propose a graph folding principle, which can determine whether two adjacent nodes are foldable by examining only their incoming and outgoing edges.  

On top of the graph folding principle, we propose an efficient graph folding algorithm GF. The time complexity of GF is linear with respect to the number of nodes in the input graph. Our evaluations on two clients (alias analysis and value-flow analysis) show that GF significantly accelerates RSM/CFL-reachability by reducing the input graph size. On average, for value-flow analysis, GF reduces 60.96\% of nodes and 42.67\% of edges of the input graphs, obtaining a speedup of 4.65\texttimes{} and a memory usage reduction of 57.35\%. For alias analysis, GF reduces 38.93\% of nodes and 35.61\% of edges of the input graphs, obtaining a speedup of 3.21\texttimes{} and a memory usage reduction of 65.19\%.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {119},
numpages = {25},
keywords = {CFL-reachability, graph simplification, recursive state machines}
}

@software{10.5281/zenodo.7787371,
author = {Lei, Yuxiang and Sui, Yulei and Tan, Shin Hwei and Zhang, Qirun},
title = {Artifact of "Recursive State Machine Guided Graph Folding for Context-Free Language Reachability"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7787371},
abstract = {
    <p>This is the artifact of the paper “Recursive State Machine Guided Graph Folding for Context-Free Language Reachability” accepted to PLDI 2023. The artifact is packaged as a Docker image “gf.tar.gz”, which is to reproduce the experiment results of the paper. Please see README.pdf for detailed usage of the artifact.</p>

},
keywords = {CFL-reachability, graph simplification, recursive state machines}
}

@article{10.1145/3591234,
author = {Nigam, Rachit and Azevedo de Amorim, Pedro Henrique and Sampson, Adrian},
title = {Modular Hardware Design with Timeline Types},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591234},
doi = {10.1145/3591234},
abstract = {Modular design is a key challenge for enabling large-scale reuse of hardware modules. Unlike software, however, hardware designs correspond to physical circuits and inherit constraints from them. Timing constraints—which cycle a signal arrives, when an input is read—and structural constraints—how often a multiplier accepts new inputs—are fundamental to hardware interfaces. Existing hardware design languages do not provide a way to encode these constraints; a user must read documentation, build scripts, or in the worst case, a module’s implementation to understand how to use it. We present Filament, a language for modular hardware design that supports the specification and enforcement of timing and structural constraints for statically scheduled pipelines. Filament uses timeline types, which describe the intervals of clock-cycle time when a given signal is available or required. Filament enables safe composition of hardware modules, ensures that the resulting designs are correctly pipelined, and predictably lowers them to efficient hardware.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {120},
numpages = {25},
keywords = {Hardware Description Language, Type Systems}
}

@software{10.5281/zenodo.7709916,
author = {Nigam, Rachit and Azevedo de Amorim, Pedro Henrique and Sampson, Adrian},
title = {Reproduction Package for "Modular Hardware Design with Timeline Types"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7709916},
abstract = {
    <p>The artifact contains the source code for the language and compiler presented in the paper (Filament) along with Verilog modules generated from the Aetherling and Reticle compilers. It provides scripts to regenerate the tables and results in the paper. The reproducer must manually install the Vivado toolchain, as instructed in the README, in order to obtain the resource usage numbers.</p>

},
keywords = {Hardware design language, type system}
}

@article{10.1145/3591235,
author = {Zhang, Tony Nuda and Sharma, Upamanyu and Kapritsos, Manos},
title = {Performal: Formal Verification of Latency Properties for Distributed Systems},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591235},
doi = {10.1145/3591235},
abstract = {Understanding and debugging the performance of distributed systems is a notoriously hard task, but a critical one. Traditional techniques like logging, tracing, and benchmarking represent a best-effort way to find performance bugs, but they either require a full deployment to be effective or can only find bugs after they manifest. Even with such techniques in place, real deployments often exhibit performance bugs that cause unwanted behavior.  

In this paper, we present Performal, a novel methodology that leverages the recent advances in formal verification to provide rigorous latency guarantees for real, complex distributed systems. The task is not an easy one: it requires carefully decoupling the formal proofs from the execution environment, formally defining latency properties, and proving them on real, distributed implementations. We used Performal to prove rigorous upper bounds for the latency of three applications: a distributed lock, ZooKeeper and a MultiPaxos-based State Machine Replication system. Our experimental evaluation shows that these bounds are a good proxy for the behavior of the deployed system and can be used to identify performance bugs in real-world systems.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {121},
numpages = {26},
keywords = {distributed systems, latency, performance, systems verification}
}

@software{10.5281/zenodo.7812534,
author = {Zhang, Tony Nuda and Sharma, Upamanyu and Kapritsos, Manos},
title = {Artifact for Article `Performal: Formal Verification of Latency Properties for Distributed Systems'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7812534},
abstract = {
    <p>This artifact includes source code used to produce the results the paper `Performal: Formal Verification of Latency Properties for Distributed Systems’. Instructions to reproduce the results are documented in the artifact.</p>

},
keywords = {distributed systems, formal software verification, latency, performance, systems verification}
}

@article{10.1145/3591236,
author = {Bansal, Manya and Hsu, Olivia and Olukotun, Kunle and Kjolstad, Fredrik},
title = {Mosaic: An Interoperable Compiler for Tensor Algebra},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591236},
doi = {10.1145/3591236},
abstract = {We introduce Mosaic, a sparse tensor algebra compiler that can bind tensor expressions to external functions of other tensor algebra libraries and compilers. Users can extend Mosaic by adding new functions and bind a sub-expression to a function using a scheduling API. Mosaic substitutes the bound sub-expressions with calls to the external functions and automatically generates the remaining code using a default code generator. As the generated code is fused by default, users can productively leverage both fusion and calls to specialized functions within the same compiler. We demonstrate the benefits of our dual approach by showing that calling hand-written CPU and specialized hardware functions can provide speedups of up to 206\texttimes{} against fused code in some cases, while generating fused code can provide speedups of up to 3.57\texttimes{} against code that calls external functions in other cases. Mosaic also offers a search system that can automatically map an expression to a set of registered external functions. Both the explicit binding and automatic search are verified by Mosaic. Additionally, the interface for adding new external functions is simple and general. Currently, 38 external functions have been added to Mosaic, with each addition averaging 20 lines of code.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {122},
numpages = {26},
keywords = {automated search, compilation, external functions, sparse tensor algebra}
}

@software{10.5281/zenodo.7814275,
author = {Bansal, Manya and Hsu, Olivia and Olukotun, Kunle and Kjolstad, Fredrik},
title = {Artifact for Mosaic: An Interoperable Compiler for Tensor Algebra},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7814275},
abstract = {
    <p>This artifact describes how to set up and run Mosaic, a compiler that can compose externally defined library functions to implement an arbitrary sparse tensor algebra expression. Mosaic fills in the gaps that are not provided by the libraries, guaranteeing generality in both expressions and data structures. The artifact also describes how to reproduce the quantitative experimental results presented in the paper.</p>

},
keywords = {Code Optimization, Compiler, External Functions, Sparse Tensor Algebra}
}

@article{10.1145/3591237,
author = {Sisco, Zachary D. and Balkind, Jonathan and Sherwood, Timothy and Hardekopf, Ben},
title = {Loop Rerolling for Hardware Decompilation},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591237},
doi = {10.1145/3591237},
abstract = {We introduce the new problem of hardware decompilation. Analogous to software decompilation, hardware decompilation is about analyzing a low-level artifact—in this case a netlist, i.e., a graph of wires and logical gates representing a digital circuit—in order to recover higher-level programming abstractions, and using those abstractions to generate code written in a hardware description language (HDL). The overall problem of hardware decompilation requires a number of pieces. In this paper we focus on one specific piece of the puzzle: a technique we call hardware loop rerolling. Hardware loop rerolling leverages clone detection and program synthesis techniques to identify repeated logic in netlists (such as would be synthesized from loops in the original HDL code) and reroll them into syntactic loops in the recovered HDL code. We evaluate hardware loop rerolling for hardware decompilation over a set of hardware design benchmarks written in the PyRTL HDL and industry standard SystemVerilog. Our implementation identifies and rerolls loops in 52 out of 53 of the netlists in our benchmark suite, and we show three examples of how hardware decompilation can provide concrete benefits: transpilation between HDLs, faster simulation times over netlists (with mean speedup of 6x), and artifact compaction (39\% smaller on average).},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {123},
numpages = {23},
keywords = {hardware decompilation, loop rerolling, program synthesis}
}

@software{10.5281/zenodo.7823993,
author = {Sisco, Zachary D. and Balkind, Jonathan and Sherwood, Timothy and Hardekopf, Ben},
title = {Artifact for "Loop Rerolling for Hardware Decompilation"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7823993},
abstract = {
    <p>The artifact consists of four components: (1) source code for hardware loop identification over the benchmark suite of netlists; (2) source code for hardware loop rerolling over the benchmark suite; (3) scripts for comparing simulation times between decompiled HDL code with rerolled loops and the original netlist using Verilator; and (4) Yosys scripts for converting Verilog designs to netlists in BLIF. We provide instructions to reproduce the results reported in the evaluation.</p>

},
keywords = {hardware decompilation, loop rerolling, program synthesis}
}

@article{10.1145/3591238,
author = {Tao, Zhe and Nawas, Stephanie and Mitchell, Jacqueline and Thakur, Aditya V.},
title = {Architecture-Preserving Provable Repair of Deep Neural Networks},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591238},
doi = {10.1145/3591238},
abstract = {Deep neural networks (DNNs) are becoming increasingly important components  
of software, and are considered the state-of-the-art solution for a number  
of problems, such as image recognition. However, DNNs are far from  
infallible, and incorrect behavior of DNNs can have disastrous real-world  
consequences. This paper addresses the problem of architecture-preserving  
V-polytope provable repair of DNNs.  
A V-polytope defines a convex bounded polytope using its vertex representation.  
V-polytope provable repair guarantees that the repaired DNN  
satisfies the given specification on the infinite set of points in the given V-polytope.  
An architecture-preserving repair only modifies the parameters of the DNN, without  
modifying its architecture. The repair has the flexibility to  
modify multiple layers of the DNN, and runs in polynomial time.  
It supports DNNs with activation functions that have some linear pieces,  
as well as fully-connected, convolutional, pooling and residual layers.  
To the best our knowledge, this is the first provable repair approach that  
has all of these features.  
We implement our approach in a tool called APRNN. Using  
MNIST, ImageNet, and ACAS Xu DNNs, we show that  
it has better efficiency, scalability, and generalization  
compared to PRDNN and REASSURE, prior provable repair methods that are  
not architecture preserving.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {124},
numpages = {25},
keywords = {Bug fixing, Deep Neural Networks, Repair, Synthesis}
}

@software{10.5281/zenodo.7807290,
author = {Tao, Zhe and Nawas, Stephanie and Mitchell, Jacqueline and Thakur, Aditya V.},
title = {Reproduction Package for the PLDI 2023 Article "Architecture-Preserving Provable Repair of Deep Neural Networks"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7807290},
abstract = {
    <p>This is the artifact of the PLDI 2023 article “Architecture-Preserving Provable Repair of Deep Neural Networks”. Please refer to the README.md file for the instructions. The latest version of the artifact can be found at https://github.com/95616ARG/APRNN/</p>

},
keywords = {Bug fixing, Deep Neural Networks, Repair, Synthesis}
}

@article{10.1145/3591239,
author = {Zhang, Yihong and Wang, Yisu Remy and Flatt, Oliver and Cao, David and Zucker, Philip and Rosenthal, Eli and Tatlock, Zachary and Willsey, Max},
title = {Better Together: Unifying Datalog and Equality Saturation},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591239},
doi = {10.1145/3591239},
abstract = {We present egglog, a fixpoint reasoning system that unifies Datalog and equality saturation (EqSat). Like Datalog, egglog supports efficient incremental execution, cooperating analyses, and lattice-based reasoning. Like EqSat, egglog supports term rewriting, efficient congruence closure, and extraction of optimized terms.  

We identify two recent applications -- a unification-based pointer analysis in Datalog and an EqSat-based floating-point term rewriter -- that have been hampered by features missing from Datalog but found in EqSat or vice-versa. We evaluate our system by reimplementing those projects in egglog. The resulting systems in egglog are faster, simpler, and fix bugs found in the original systems.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {125},
numpages = {25},
keywords = {Datalog, Equality saturation, Program optimization, Rewrite systems}
}

@software{10.5281/zenodo.7709794,
author = {Zhang, Yihong and Wang, Yisu Remy and Flatt, Oliver and Cao, David and Zucker, Philip and Rosenthal, Eli and Tatlock, Zachary and Willsey, Max},
title = {Artifact for "Better Together: Unifying Datalog and Equality Saturation"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7709794},
abstract = {
    <p>This artifact contains the egglog system, as well as data and scripts needed to reproduce the microbenchmarks and two case studies as described in the paper.</p>

},
keywords = {Datalog, program optimization, program synthesis}
}

@article{10.1145/3591240,
author = {Park, Jihyeok and Youn, Dongjun and Lee, Kanguk and Ryu, Sukyoung},
title = {Feature-Sensitive Coverage for Conformance Testing of Programming Language Implementations},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591240},
doi = {10.1145/3591240},
abstract = {The conformance testing of programming language implementations is crucial to support correct and consistent execution environments. Because manually maintaining conformance tests for real-world programming languages is cumbersome and labor-intensive, researchers have presented various ways to make conformance tests effective and efficient. One such approach is to use graph coverage, one of the most widely-used coverage criteria, to generate tests that reach different parts of a mechanized language specification. Since mechanized specifications use functions or inductive definitions to describe the semantics of language features, traditional graph coverage criteria for software work as they are. However, they may not produce high-quality conformance tests because language implementations often have specialized execution paths for different features, even when their semantics descriptions use the same functions. Traditional graph coverage may not distinguish test requirements of such language features, which degrades the quality of conformance testing. Similarly, it may not distinguish test requirements of different parts of the same language feature when their semantics descriptions use the same functions.  

We present feature-sensitive (FS) coverage as a novel coverage criterion to generate high-quality conformance tests for language implementations. It is a general extension of graph coverage, refining conventional test requirements using the innermost enclosing language features. We also introduce feature-call-path-sensitive (FCPS) coverage, a variant of FS coverage, and extend both coverage criteria using the 𝑘-limiting approach. To evaluate the effectiveness of the new coverage criteria for language implementations, we apply them to a mechanized specification of JavaScript. We extend JEST, the state-of-the-art JavaScript conformance test synthesizer using coverage-guided mutational fuzzing, with various FS and FCPS coverage criteria. For the latest JavaScript language specification (ES13, 2022), our tool automatically synthesizes 237,981 conformance tests in 50 hours with five coverage criteria. We evaluated the conformance of eight mainstream JavaScript implementations (four engines and four transpilers) with the synthesized conformance tests and discovered bugs in all of them. The tool detected 143 distinct conformance bugs (42 in engines and 101 in transpilers), 85 of which were confirmed by the developers and 83 of which were newly discovered bugs.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {126},
numpages = {23},
keywords = {conformance test synthesis, coverage-guided fuzzing, feature-sensitive coverage, mechanized specification}
}

@software{10.5281/zenodo.7787547,
author = {Park, Jihyeok and Youn, Dongjun and Lee, Kanguk and Ryu, Sukyoung},
title = {Artifact For "Feature-Sensitive Coverage for Conformance Testing of Programming Language Implementations"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7787547},
abstract = {
    <p>Artifact For “Feature-Sensitive Coverage for Conformance Testing of Programming Language Implementations”</p>

},
keywords = {conformance test synthesis, coverage-guided fuzzing, feature-sensitive coverage, mechanized specification}
}

@article{10.1145/3591241,
author = {Huang, Yulong and Yallop, Jeremy},
title = {Defunctionalization with Dependent Types},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591241},
doi = {10.1145/3591241},
abstract = {The defunctionalization translation that eliminates higher-order functions from programs forms a key part of many compilers. However, defunctionalization for dependently-typed languages has not been formally studied. We present the first formally-specified defunctionalization translation for a dependently-typed language and establish key metatheoretical properties such as soundness and type preservation. The translation is suitable for incorporation into type-preserving compilers for dependently-typed languages},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {127},
numpages = {23},
keywords = {compilation, dependent types, type preservation, type systems}
}

@software{10.5281/zenodo.7709681,
author = {Huang, Yulong and Yallop, Jeremy},
title = {Defunctionalization with Dependent Types: Artifact},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7709681},
abstract = {
    <p>We provide a portable standalone implementation of the defunctionalization translation, written in OCaml and compiled to run in a web browser using js_of_ocaml. The implementation performs type checking of CC and DCC terms, abstract defunctionalization and backwards translation from DCC to CC, allowing the interested reader to experiment with the effects of the translation on real examples. We include several ready-made examples, including dependent composition, dependent pairs and finite sets.</p>

},
keywords = {compilation, dependent types, type preservation, type systems}
}

@article{10.1145/3591242,
author = {Ma, Wenjie and Yang, Shengyuan and Tan, Tian and Ma, Xiaoxing and Xu, Chang and Li, Yue},
title = {Context Sensitivity without Contexts: A Cut-Shortcut Approach to Fast and Precise Pointer Analysis},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591242},
doi = {10.1145/3591242},
abstract = {Over the past decades, context sensitivity has been considered as one of the most effective ideas for improving the precision of pointer analysis for Java. Different from the extremely fast context-insensitivity approach, context sensitivity requires every program method to be analyzed under different contexts for separating the static abstractions of different dynamic instantiations of the method’s variables and heap objects, and thus reducing spurious object flows introduced by method calls. However, despite great precision benefits, as each method is equivalently cloned and analyzed under each context, context sensitivity brings heavy efficiency costs. Recently, numerous selective context-sensitive approaches have been put forth for scaling pointer analysis to large and complex Java programs by applying contexts only to the selected methods while analyzing the remaining ones context-insensitively; however, because the selective approaches do not fundamentally alter the primary methodology of context sensitivity (and do not thus remove its efficiency bottleneck), they produce much improved but still limited results.  

In this work, we present a fundamentally different approach called Cut-Shortcut for fast and precise pointer analysis for Java. Its insight is simple: the main effect of cloning methods under different contexts is to filter spurious object flows that have been merged inside a callee method; from the view of a typical pointer flow graph (PFG), such effect can be simulated by cutting off (Cut) the edges that introduce precision loss to certain pointers and adding Shortcut edges directly from source pointers to the target ones circumventing the method on PFG. As a result, we can achieve the effect of context sensitivity without contexts. We identify three general program patterns and develop algorithms based on them to safely cut off and add shortcut edges on PFG, formalize them and formally prove the soundness. To comprehensively validate Cut-Shortcut’s effectiveness, we implement two versions of Cut-Shortcut for two state-of-the-art pointer analysis frameworks for Java, one in Datalog for the declarative Doop and the other in Java for the imperative Tai-e, and we consider all the large and complex programs used in recent literatures that meet the experimental requirements. The evaluation results are extremely promising: Cut-Shortcut is even able to run faster than context insensitivity for most evaluated programs while obtaining high precision that is comparable to context sensitivity (if scalable) in both frameworks. This is for the first time that we have been able to achieve such a good efficiency and precision trade-off for those hard-to-analyze programs, and we hope Cut-Shortcut could offer new perspectives for developing more effective pointer analysis for Java in the future.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {128},
numpages = {26},
keywords = {Alias Analysis, Context Sensitivity, Java, Pointer Analysis}
}

@software{10.5281/zenodo.7808384,
author = {Ma, Wenjie and Yang, Shengyuan and Tan, Tian and Ma, Xiaoxing and Xu, Chang and Li, Yue},
title = {Context Sensitivity without Contexts: A Cut-Shortcut Approach to Fast and Precise Pointer Analysis (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7808384},
abstract = {
    <p>This is the artifact of PLDI’23 paper Context Sensitivity without Contexts: A Cut-Shortcut Approach to Fast and Precise Pointer Analysis.</p>

},
keywords = {Alias Analysis, Context Sensitivity, Java, Pointer Analysis}
}

@article{10.1145/3591243,
author = {Bouajjani, Ahmed and Enea, Constantin and Rom\'{a}n-Calvo, Enrique},
title = {Dynamic Partial Order Reduction for Checking Correctness against Transaction Isolation Levels},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591243},
doi = {10.1145/3591243},
abstract = {Modern applications, such as social networking systems and e-commerce platforms are centered around using large-scale databases for storing and retrieving data. Accesses to the database are typically enclosed in transactions that allow computations on shared data to be isolated from other concurrent computations and resilient to failures. Modern databases trade isolation for performance. The weaker the isolation level is, the more behaviors a database is allowed to exhibit and it is up to the developer to ensure that their application can tolerate those behaviors.  

In this work, we propose stateless model checking algorithms for studying correctness of such applications that rely on dynamic partial order reduction. These algorithms work for a number of widely-used weak isolation levels, including Read Committed, Causal Consistency, Snapshot Isolation and Serializability. We show that they are complete, sound and optimal, and run with polynomial memory consumption in all cases. We report on an implementation of these algorithms in the context of Java Pathfinder applied to a number of challenging applications drawn from the literature of distributed systems and databases.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {129},
numpages = {26},
keywords = {Applications of Storage Systems, Dynamic Partial-Order Reduction, Transactional Databases, Weak Isolation Levels}
}

@software{10.5281/zenodo.7824546,
author = {Bouajjani, Ahmed and Enea, Constantin and Rom\'{a}n-Calvo, Enrique},
title = {Transactional JPF - Artifact for "Dynamic Partial Order Reduction for Checking Correctness against Transaction Isolation Levels"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7824546},
abstract = {
    <p>Extension of Java PathFinder (JPF) for managing transactions as well as EXPLORE-CE algorithm’s implementation. It includes several benchmarks used in the article “Dynamic Partial Order Reduction for Checking Correctness against Transaction Isolation Levels” as well as graphical scripts for plotting the results.</p>

},
keywords = {Dynamic Partial-Order Reduction, Java PathFinder, Transactional Databases, Weak Isolation Levels}
}

@article{10.1145/3591244,
author = {Yamazaki, Tetsuro and Nakamaru, Tomoki and Shioya, Ryota and Ugawa, Tomoharu and Chiba, Shigeru},
title = {Collecting Cyclic Garbage across Foreign Function Interfaces: Who Takes the Last Piece of Cake?},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591244},
doi = {10.1145/3591244},
abstract = {A growing number of libraries written in managed languages, such as Python and JavaScript, are bringing about new demand for a foreign language interface (FFI) between two managed languages. Such an FFI allows a host-language program to seamlessly call a library function written in a foreign language and exchange objects. It is often implemented by a user-level library but such implementation cannot reclaim cyclic garbage, or a group of objects with circular references, across the language boundary. This paper proposes Refgraph GC, which enables FFI implementation that can reclaim cyclic garbage. Refgraph GC coordinates the garbage collectors of two languages and it needs to modify the managed runtime of one language only. It does not modify that of the other language. This paper discusses the soundness and completeness of the proposed algorithm and also shows the results of the experiments with our implementation of FFI with Refgraph GC. This FFI allows a Ruby program to access a JavaScript library.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {130},
numpages = {24},
keywords = {FFI, garbage collection, memory management}
}

@software{10.5281/zenodo.7811907,
author = {Yamazaki, Tetsuro and Nakamaru, Tomoki and Shioya, Ryota and Ugawa, Tomoharu and Chiba, Shigeru},
title = {Artifact - Collecting cyclic garbage across foreign function interfaces},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7811907},
abstract = {
    <p>All the programs to reproduce the evaluation presented in the corresponding paper.</p>

},
keywords = {FFI, Garbage collection, Memory Management}
}

@article{10.1145/3591245,
author = {Tassarotti, Joseph and Tristan, Jean-Baptiste},
title = {Verified Density Compilation for a Probabilistic Programming Language},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591245},
doi = {10.1145/3591245},
abstract = {This paper presents ProbCompCert, a compiler for a subset of the Stan probabilistic programming language (PPL), in which several key compiler passes have been formally verified using the Coq proof assistant. Because of the probabilistic nature of PPLs, bugs in their compilers can be difficult to detect and fix, making verification an interesting possibility. However, proving correctness of PPL compilation requires new techniques because certain transformations performed by compilers for PPLs are quite different from other kinds of languages. This paper describes techniques for verifying such transformations and their application in ProbCompCert. In the course of verifying ProbCompCert, we found an error in the Stan language reference manual related to the semantics and implementation of a key language construct.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {131},
numpages = {23},
keywords = {compilers. probabilistic programming, formal verification}
}

@software{10.5281/zenodo.7812119,
author = {Tassarotti, Joseph and Tristan, Jean-Baptiste},
title = {Verified Density Compilation for a Probabilistic Programming Language (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7812119},
abstract = {
    <p>The artifact consists of the Coq and OCaml source code for ProbCompCert, the compiler described in the accompanying paper. ProbCompcert is implemented as an extension to CompCert. The artifact is available in 3 different forms: a copy of the source code, a Docker image, and a Qemu image. The latter two contain the compiler pre-built, along with scripts for re-running the experiments described in the paper.</p>

},
keywords = {compilers, formal verification, probabilistic programming}
}

@article{10.1145/3591246,
author = {Liu, Junrui and Chen, Yanju and Atkinson, Eric and Feng, Yu and Bodik, Rastislav},
title = {Conflict-Driven Synthesis for Layout Engines},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591246},
doi = {10.1145/3591246},
abstract = {Modern web browsers rely on layout engines to convert HTML documents to layout trees that specify color, size, and position. However, existing layout engines are notoriously difficult to maintain because of the complexity of web standards. This is especially true for incremental layout engines, which are designed to improve performance by updating only the parts of the layout tree that need to be changed. In this paper, we propose Medea, a new framework for automatically generating incremental layout engines. Medea separates the specification of the layout engine from its incremental implementation, and guarantees correctness through layout engine synthesis. The synthesis is driven by a new iterative algorithm based on detecting conflicts that prevent optimality of the incremental algorithm. We evaluated Medea on a fragment of HTML layout that includes challenging features such as margin collapse, floating layout, and absolute positioning. Medea successfully synthesized an incremental layout engine for this fragment. The synthesized layout engine is both correct and efficient. In particular, we demonstrated that it avoids real-world bugs that have been reported in the layout engines of Chrome, Firefox, and Safari. The incremental layout engine synthesized by Medea is up to 1.82\texttimes{} faster than a naive incremental baseline. We also demonstrated that our conflict-driven algorithm produces engines that are 2.74\texttimes{} faster than a baseline without conflict analysis.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {132},
numpages = {22},
keywords = {program synthesis}
}

@article{10.1145/3591247,
author = {Pick, Lauren and Desai, Ankush and Gupta, Aarti},
title = {Psym: Efficient Symbolic Exploration of Distributed Systems},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591247},
doi = {10.1145/3591247},
abstract = {Verification of distributed systems using systematic exploration is daunting because of the many possible interleavings of messages and failures.  
When faced with this scalability challenge, existing approaches have traditionally mitigated state space explosion by avoiding exploration of redundant states (e.g., via state hashing) and redundant interleavings of transitions (e.g., via partial-order reductions).  
In this paper, we present an efficient symbolic exploration method that not only avoids redundancies in states and interleavings, but additionally  
avoids redundant computations that are performed during updates to states on transitions.  
Our symbolic explorer leverages a novel, fine-grained, canonical representation of distributed system configurations (states) to identify opportunities for avoiding such redundancies on-the-fly.  
The explorer also includes an interface that is compatible with abstractions for state-space reduction and with partial-order and other reductions for avoiding redundant interleavings.  
We implement our approach in the tool Psym and empirically demonstrate that it outperforms a state-of-the-art exploration tool, can successfully verify many common distributed protocols, and can scale to multiple real-world industrial case studies across},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {133},
numpages = {26},
keywords = {binary decision diagrams, distributed systems, systematic exploration}
}

@software{10.5281/zenodo.7814715,
author = {Pick, Lauren and Desai, Ankush and Gupta, Aarti},
title = {Software for `Psym: Efficient Symbolic Exploration of Distributed Systems'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7814715},
abstract = {
    <p>The artifact contains the software used to achieve experimental results the paper `Psym: Efficient Symbolic Exploration of Distributed Systems’ as well some of the benchmarks that can be used to reproduce the results.</p>

},
keywords = {binary decision diagrams, distributed systems, systematic exploration}
}

@article{10.1145/3591248,
author = {Barnaby, Celeste and Chen, Qiaochu and Samanta, Roopsha and Dillig, I\c{s}\i{}l},
title = {ImageEye: Batch Image Processing using Program Synthesis},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591248},
doi = {10.1145/3591248},
abstract = {This paper presents a new synthesis-based approach for batch image processing. Unlike existing tools that can only apply global edits to the entire image, our method can apply fine-grained edits to individual objects within the image. For example, our method can selectively blur or crop specific objects that have a certain property. To facilitate such fine-grained image editing tasks, we propose a neuro-symbolic domain-specific language (DSL) that combines pre-trained neural networks for image classification with other language constructs that enable symbolic reasoning. Our method can automatically learn programs in this DSL from user demonstrations by utilizing a novel synthesis algorithm. We have implemented the proposed technique in a tool called ImageEye and evaluated it on 50 image editing tasks. Our evaluation shows that ImageEye is able to automate 96\% of these tasks.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {134},
numpages = {26},
keywords = {Computer Vision, Neuro-symbolic Synthesis, Program Synthesis}
}

@software{10.5281/zenodo.7810841,
author = {Barnaby, Celeste and Chen, Qiaochu and Samanta, Roopsha and Dillig, I\c{s}\i{}l},
title = {Reproduction Package for 'ImageEye: Batch Image Editing with Program Synthesis'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7810841},
abstract = {
    <p>This artifact contains the source code for ImageEye, along with a Docker image that runs the experiments described in the paper. In particular, the Docker image runs the benchmarks, ablations, and comparison with EUSolver.</p>

},
keywords = {computer vision, neuro-symbolic synthesis, program synthesis}
}

@article{10.1145/3591249,
author = {Muller, Stefan K. and Singer, Kyle and Keeney, Devyn Terra and Neth, Andrew and Agrawal, Kunal and Lee, I-Ting Angelina and Acar, Umut A.},
title = {Responsive Parallelism with Synchronization},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591249},
doi = {10.1145/3591249},
abstract = {Many concurrent programs assign priorities to threads to improve responsiveness. When used in conjunction with synchronization mechanisms such as mutexes and condition variables, however, priorities can lead to priority inversions, in which high-priority threads are delayed by low-priority ones. Priority inversions in the use of mutexes are easily handled using dynamic techniques such as priority inheritance, but priority inversions in the use of condition variables are not well-studied and dynamic techniques are not suitable. 

In this work, we use a combination of static and dynamic techniques to prevent priority inversion in code that uses mutexes and condition variables. A type system ensures that condition variables are used safely, even while dynamic techniques change thread priorities at runtime to eliminate priority inversions in the use of mutexes. We prove the soundness of our system, using a model of priority inversions based on cost models for parallel programs. To show that the type system is practical to implement, we encode it within the type systems of Rust and C++, and show that the restrictions are not overly burdensome by writing sizeable case studies using these encodings, including porting the Memcached object server to use our C++ implementation.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {135},
numpages = {24},
keywords = {condition variables, cost semantics, priority inversions, type systems}
}

@software{10.5281/zenodo.7706984,
author = {Muller, Stefan K. and Singer, Kyle and Keeney, Devyn Terra and Neth, Andrew and Agrawal, Kunal and Lee, I-Ting Angelina and Acar, Umut A.},
title = {Responsive Parallelism with Synchronization Case Studies},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7706984},
abstract = {
    <p>Implementations of the type system described in “Responsive Parallelism with Synchronization” in C++ and Rust, as well as implementations of the case studies. Included are instructions on how to build and execute the case studies, as well as a Docker container environment that can be used for compilation and execution.</p>

},
keywords = {C++, Cilk, condition variable, cost semantics, priority inversion, Rust, type system}
}

@article{10.1145/3591250,
author = {Surbatovich, Milijana and Spargo, Naomi and Jia, Limin and Lucia, Brandon},
title = {A Type System for Safe Intermittent Computing},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591250},
doi = {10.1145/3591250},
abstract = {Batteryless energy-harvesting devices enable computing in inaccessible environments, at a cost to programmability and correctness. These devices operate intermittently as energy is available, using a recovery system to save and restore state. Some program tasks must execute atomically w.r.t. power failures, re-executing if power fails before completion. Any re-execution should typically be idempotent—its behavior should match the behavior of a single execution. Thus, a key aspect of correct intermittent execution is identifying and recovering state causing undesired non-idempotence. Unfortunately, past intermittent systems take an ad-hoc approach, using unsound dataflow analyses or conservatively recovering all written state. Moreover, no prior work allows the programmer to directly specify idempotence requirements (including allowable non-idempotence). We present curricle, the first type system approach to safe intermittence, for Rust. Type level reasoning allows programmers to express requirements and retains alias information crucial for sound analyses. Curricle uses information flow and type qualifiers to reject programs causing undesired non-idempotence. We implement Curricle’s type system on top of Rust’s compiler, evaluating the prototype on benchmarks from prior work. We find that Curricle benefits application programmers by allowing them to express idempotence requirements that are checked to be satisfied, and that targeting programs checked with Curricle allows intermittent system designers to write simpler recovery systems that perform better.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {136},
numpages = {25},
keywords = {energy harvesting, information flow, intermittent computing, type systems}
}

@article{10.1145/3591251,
author = {Tun\c{c}, H\"{u}nkar Can and Abdulla, Parosh Aziz and Chakraborty, Soham and Krishna, Shankaranarayanan and Mathur, Umang and Pavlogiannis, Andreas},
title = {Optimal Reads-From Consistency Checking for C11-Style Memory Models},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591251},
doi = {10.1145/3591251},
abstract = {Over the years, several memory models have been proposed to capture the subtle concurrency semantics of C/C++. One of the most fundamental problems associated with a memory model M is consistency checking: given an execution X, is X consistent with M? This problem lies at the heart of numerous applications, including specification testing and litmus tests, stateless model checking, and dynamic analyses. As such, it has been explored extensively and its complexity is well-understood for traditional models like SC and TSO. However, less is known for the numerous model variants of C/C++, for which the problem becomes challenging due to the intricacies of their concurrency primitives. In this work we study the problem of consistency checking for popular variants of the C11 memory model, in particular, the RC20 model, its release-acquire (RA) fragment, the strong and weak variants of RA (SRA and WRA), as well as the Relaxed fragment of RC20. Motivated by applications in testing and model checking, we focus on reads-from consistency checking. The input is an execution X specifying a set of events, their program order and their reads-from relation, and the task is to decide the existence of a modification order on the writes of X that makes X consistent in a memory model. We draw a rich complexity landscape for this problem; our results include (i) nearly-linear-time algorithms for certain variants, which improve over prior results, (ii) fine-grained optimality results, as well as (iii) matching upper and lower bounds (NP-hardness) for other variants. To our knowledge, this is the first work to characterize the complexity of consistency checking for C11 memory models. We have implemented our algorithms inside the TruSt model checker and the C11Tester testing tool. Experiments on standard benchmarks show that our new algorithms improve consistency checking, often by a significant margin.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {137},
numpages = {25},
keywords = {complexity, concurrency, weak memory models}
}

@software{10.5281/zenodo.7816526,
author = {Tun\c{c}, H\"{u}nkar Can and Abdulla, Parosh Aziz and Chakraborty, Soham and Krishna, Shankaranarayanan and Mathur, Umang and Pavlogiannis, Andreas},
title = {Artifact for Article "Optimal Reads-From Consistency Checking for C11-Style Memory Models "},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7816526},
abstract = {
    <p>This artifact contains all the source codes and experimental data for replicating our evaluation in the paper. We implemented our programs as an extension to the C11Tester and GenMC tools. The provided experimental data contains all the benchmarks used in our evaluation. The artifact also contains Python scripts that fully automate the process of replicating our evaluation.</p>

},
keywords = {complexity, concurrency, weak memory models}
}

@article{10.1145/3591252,
author = {M\"{u}ller, Mark Niklas and Fischer, Marc and Staab, Robin and Vechev, Martin},
title = {Abstract Interpretation of Fixpoint Iterators with Applications to Neural Networks},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591252},
doi = {10.1145/3591252},
abstract = {We present a new abstract interpretation framework for the precise over-approximation of numerical fixpoint iterators.  
Our key observation is that unlike in standard abstract interpretation (AI), typically used to over-approximate all reachable program states, in this setting, one only needs to abstract the concrete fixpoints, i.e., the final program states. Our framework targets numerical fixpoint iterators with convergence and uniqueness guarantees in the concrete and is based on two major technical contributions: (i) theoretical insights which allow us to compute sound and precise fixpoint abstractions without using joins, and (ii) a new abstract domain, CH-Zonotope, which admits efficient propagation and inclusion checks while retaining high precision.  

We implement our framework in a tool called CRAFT and evaluate it on a novel fixpoint-based neural network architecture (monDEQ) that is particularly challenging to verify. Our extensive evaluation demonstrates that CRAFT exceeds the state-of-the-art performance in terms of speed (two orders of magnitude), scalability (one order of magnitude), and precision (25\% higher certified accuracies).},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {138},
numpages = {25},
keywords = {abstract interpretation, adversarial robustness, equlibrium models, fixpoint}
}

@software{10.5281/zenodo.7997778,
author = {M\"{u}ller, Mark Niklas and Fischer, Marc and Staab, Robin and Vechev, Martin},
title = {Abstract Interpretation of Fixpoint Iterators with Applications to Neural Networks - Artifact},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7997778},
abstract = {
    <p>An implementation of the abstract fixpoint iterator framework for monDEQs, CRAFT, as well as the CH-Zonotope domain. Included are the code, trained models, expected results, and detailed instructions on how to reproduce all results from the PLDI’23 paper “Abstract Interpretation of Fixpoint Iterators with Applications to Neural Networks”.</p>

},
keywords = {abstract interpretation, adversarial robustness, equlibrium models, fixpoint}
}

@article{10.1145/3591253,
author = {Lee, Dongjae and Cho, Minki and Kim, Jinwoo and Moon, Soonwon and Song, Youngju and Hur, Chung-Kil},
title = {Fair Operational Semantics},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591253},
doi = {10.1145/3591253},
abstract = {Fairness properties, which state that a sequence of bad events cannot happen infinitely before a good event takes place, are often crucial in program verification. However, general methods for expressing and reasoning about various kinds of fairness properties are relatively underdeveloped compared to those for safety properties. 
This paper proposes FOS (Fair Operational Semantics), a theory capable of expressing arbitrary notions of fairness as an operational semantics and reasoning about these notions of fairness. In addition, FOS enables thread-local reasoning about fairness by providing thread-local simulation relations equipped with separation- logic-style resource algebras. We verify a ticket lock implementation and a client of the ticket lock under weak memory concurrency as an example, which requires reasoning about different notions of fairness including fairness of a scheduler, fairness of the ticket lock implementation, and even fairness of weak memory. The theory of FOS, as well as the examples in the paper, are fully formalized in Coq.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {139},
numpages = {24},
keywords = {Fair Operational Semantics, Fairness, Fairness Logic}
}

@software{10.5281/zenodo.7711063,
author = {Lee, Dongjae and Cho, Minki and Kim, Jinwoo and Moon, Soonwon and Song, Youngju and Hur, Chung-Kil},
title = {Coq development for Fair Operational Semantics},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7711063},
abstract = {
    <p>This artifact is the Coq development for the 2023 PLDI paper `Fair Operational Semantics’. It contains the formalization of the theory of FOS and proofs for the examples in the paper.</p>

},
keywords = {Concurrency, Coq Proof Assistant, Fairness, Separation Logic}
}

@article{10.1145/3591254,
author = {Xu, Amanda and Molavi, Abtin and Pick, Lauren and Tannu, Swamit and Albarghouthi, Aws},
title = {Synthesizing Quantum-Circuit Optimizers},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591254},
doi = {10.1145/3591254},
abstract = {Near-term quantum computers are expected to work in an environment where each operation is noisy, with no error correction. Therefore, quantum-circuit optimizers are applied to minimize the number of noisy operations. Today, physicists are constantly experimenting with novel devices and architectures. For every new physical substrate and for every modification of a quantum computer, we need to modify or rewrite major pieces of the optimizer to run successful experiments. In this paper, we present QUESO, an efficient approach for automatically synthesizing a quantum-circuit optimizer for a given quantum device. For instance, in 1.2 minutes, QUESO can synthesize an optimizer with high-probability correctness guarantees for IBM computers that significantly outperforms leading compilers, such as IBM's Qiskit and TKET, on the majority (85\%) of the circuits in a diverse benchmark suite.A number of theoretical and algorithmic insights underlie QUESO: (1) An algebraic approach for representing rewrite rules and their semantics. This facilitates reasoning about complex symbolic rewrite rules that are beyond the scope of existing techniques. (2) A fast approach for probabilistically verifying equivalence of quantum circuits by reducing the problem to a special form of polynomial identity testing. (3) A novel probabilistic data structure, called a polynomial identity filter (PIF), for efficiently synthesizing rewrite rules. (4) A beam-search-based algorithm that efficiently applies the synthesized symbolic rewrite rules to optimize quantum circuits.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {140},
numpages = {25},
keywords = {probabilistic verification, quantum computing}
}

@software{10.5281/zenodo.7809285,
author = {Xu, Amanda and Molavi, Abtin and Pick, Lauren and Tannu, Swamit and Albarghouthi, Aws},
title = {Synthesizing Quantum-Circuit Optimizers Artifact (QUESO)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7809285},
abstract = {
    <p>This software artifact includes the source code for QUESO (a tool for synthesizing quantum-circuit optimizers) as well as a README.md with instructions for reproducing results using the provided scripts and benchmarks in the Docker image. See https://arxiv.org/abs/2211.09691 for the full version of the paper.</p>

},
keywords = {probabilistic verification, quantum circuit optimization, quantum computing}
}

@article{10.1145/3591255,
author = {Yuan, Yongwei and Radhakrishna, Arjun and Samanta, Roopsha},
title = {Trace-Guided Inductive Synthesis of Recursive Functional Programs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591255},
doi = {10.1145/3591255},
abstract = {We propose a novel trace-guided approach to tackle the challenges of ambiguity and generalization in synthesis of recursive functional programs from input-output examples. Our approach augments the search space of programs with recursion traces consisting of recursive subcalls of the programs. Our method is based on a new version space algebra (VSA) for succinct representation and efficient manipulation of pairs of recursion traces and programs that are consistent with each other. We have implemented this approach in a tool called SyRup and evaluated it on benchmarks from prior work. Our evaluation demonstrates that SyRup not only requires fewer examples to achieve a certain success rate than existing synthesizers, but is also less sensitive to the quality of the examples.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {141},
numpages = {24},
keywords = {Program Synthesis, Recursive Functional Programs}
}

@software{10.5281/zenodo.7812616,
author = {Yuan, Yongwei and Radhakrishna, Arjun and Samanta, Roopsha},
title = {Artifact for "Trace-Guided Inductive Synthesis of Recursive Functional Programs"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7812616},
abstract = {
    <p>The artifact includes the implementation of the synthesis algorithm, and necessary code to reproduce the experimental results.</p>

},
keywords = {Program Synthesis, Recursive Functional Programs}
}

@article{10.1145/3591256,
author = {Elsman, Martin and Henriksen, Troels},
title = {Parallelism in a Region Inference Context},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591256},
doi = {10.1145/3591256},
abstract = {Region inference is a type-based program analysis that takes a non-annotated program as input and constructs a program that explicitly manages memory allocation and deallocation by dividing the heap into a stack of regions, each of which can grow and shrink independently from other regions, using constant-time operations. 

Whereas region-based memory management has shown useful in the contexts of explicit region-based memory management, and in particular, in combination with parallel execution of code, combining region inference with techniques for higher-order parallel programming has not been investigated. 

In this paper, we present an implementation of a fork-join parallel construct suitable for a compiler based on region inference. We present a minimal higher-order language incorporating the parallel construct, including typing rules and a dynamic semantics for the language, and demonstrate type soundness. We present a novel effect-based region-protection inference algorithm and discuss benefits and shortcomings of the approach. We also describe an efficient implementation embedded in the MLKit Standard ML compiler. Finally, we evaluate the approach and the implementation based on a number of parallel benchmarks, and thereby demonstrate that the technique effectively utilises multi-core architectures in a higher-order functional setting.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {142},
numpages = {23},
keywords = {Memory Management, Parallelism, Region Inference}
}

@software{10.5281/zenodo.7810545,
author = {Elsman, Martin and Henriksen, Troels},
title = {Artifact for the PLDI 2023 paper 'Parallelism in a Region Inference Context'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7810545},
abstract = {
    <p>The artifact reproduces Figures 6(a), 6(b) and Tables 1 and 2 of the paper. For a detailed overview, including instructions for using the artifact, please consult the README.md file at the Zenodo page.</p>

},
keywords = {Parallelism, PLDI, SML}
}

@article{10.1145/3591257,
author = {Isemann, Raphael and Giuffrida, Cristiano and Bos, Herbert and van der Kouwe, Erik and Gleissenthall, Klaus von},
title = {Don’t Look UB: Exposing Sanitizer-Eliding Compiler Optimizations},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591257},
doi = {10.1145/3591257},
abstract = {Sanitizers are widely used compiler features that detect undefined behavior and resulting vulnerabilities by injecting runtime checks into programs. For better performance, sanitizers are often used in conjunction with optimization passes. But doing so combines two compiler features with conflicting objectives. While sanitizers want to expose undefined behavior, optimizers often exploit these same properties for performance. In this paper, we show that this clash can have serious consequences: optimizations can remove sanitizer failures, thereby hiding the presence of bugs or even introducing new ones.  

We present LookUB, a differential-testing based framework for finding optimizer transformations that elide sanitizer failures. We used our method to find 17 such sanitizer-eliding optimizations in Clang. Next, we used static analysis and fuzzing to search for bugs in open-source projects that were previously hidden due to sanitizer-eliding optimizations. This led us to discover 20 new bugs in Linux Containers, libmpeg2, NTFS-3G, and WINE. Finally, we present an effective mitigation strategy based on a customization of the Clang optimizer with an overhead increase of 4\%.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {143},
numpages = {21},
keywords = {Fuzzing, Optimizations, Sanitizers}
}

@software{10.5281/zenodo.7684001,
author = {Isemann, Raphael and Giuffrida, Cristiano and Bos, Herbert and van der Kouwe, Erik and Gleissenthall, Klaus von},
title = {Artifact for "Don’t Look UB: Exposing Sanitizer-Eliding Compiler Optimizations"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7684001},
abstract = {
    <p>This artifact contains the fuzzing and static analysis setups of the respective paper.</p>

},
keywords = {compilers, fuzzing, sanitizers}
}

@article{10.1145/3591258,
author = {Milovan\v{c}evi\'{c}, Dragana and Kun\v{c}ak, Viktor},
title = {Proving and Disproving Equivalence of Functional Programming Assignments},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591258},
doi = {10.1145/3591258},
abstract = {We present an automated approach to verify the correctness of programming assignments, such as the ones that arise in a functional programming course. Our approach takes as input student submissions and reference solutions, and uses equivalence checking to automatically prove or disprove correctness of each submission. To be effective in the context of a real-world programming course, an automated grading system must be both robust, to support programs written in a variety of style, and scalable, to treat hundreds of submissions at once. We achieve robustness by handling recursion using functional induction and by handling auxiliary functions using function call matching. We achieve scalability using a clustering algorithm that leverages the transitivity of equivalence to discover intermediate reference solutions among student submissions. We implement our approach on top of the Stainless verification system, to support equivalence checking of Scala programs. We evaluate our system and its components on over 4000 programs drawn from a functional programming course and from the program equivalence checking literature; this is the largest such evaluation to date. We show that our system is capable of proving program correctness by generating inductive equivalence proofs, and providing counterexamples for incorrect programs, with a high success rate.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {144},
numpages = {24},
keywords = {automated grading, equivalence checking, functional induction}
}

@software{10.5281/zenodo.7810840,
author = {Milovan\v{c}evi\'{c}, Dragana and Kun\v{c}ak, Viktor},
title = {Proving and Disproving Equivalence of Functional Programming Assignments (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7810840},
abstract = {
    <p>This artifact contains the complete data set and instructions for reproducing the results from the PLDI’23 paper Proving and Disproving Equivalence of Functional Programming Assignments</p>

},
keywords = {automated grading, equivalence checking, functional induction}
}

@article{10.1145/3591259,
author = {Kanabar, Hrutvik and Vivien, Samuel and Abrahamsson, Oskar and Myreen, Magnus O. and Norrish, Michael and Pohjola, Johannes \r{A}man and Zanetti, Riccardo},
title = {PureCake: A Verified Compiler for a Lazy Functional Language},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591259},
doi = {10.1145/3591259},
abstract = {We present PureCake, a mechanically-verified compiler for PureLang, a lazy, purely functional programming language with monadic effects. PureLang syntax is Haskell-like and indentation-sensitive, and its constraint-based Hindley-Milner type system guarantees safe execution. We derive sound equational reasoning principles over its operational semantics, dramatically simplifying some proofs. We prove end-to-end correctness for the compilation of PureLang down to machine code---the first such result for any lazy language---by targeting CakeML and composing with its verified compiler. Multiple optimisation passes are necessary to handle realistic lazy idioms effectively. We develop PureCake entirely within the HOL4 interactive theorem prover.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {145},
numpages = {25},
keywords = {HOL4, Haskell, compiler verification, interactive theorem proving}
}

@software{10.5281/zenodo.7782305,
author = {Kanabar, Hrutvik and Vivien, Samuel and Abrahamsson, Oskar and Myreen, Magnus O. and Norrish, Michael and Pohjola, Johannes \r{A}man and Zanetti, Riccardo},
title = {Artifact for “PureCake: A Verified Compiler for a Lazy Functional Language”},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7782305},
abstract = {
    <p><code>README.md</code> describes how to understand and use this artifact. <code>correspondences.md</code> links the artifact to the paper.</p>

},
keywords = {compiler verification, Haskell, HOL4, interactive theorem proving}
}

@article{10.1145/3591260,
author = {Brandon, William and Driscoll, Benjamin and Dai, Frank and Berkow, Wilson and Milano, Mae},
title = {Better Defunctionalization through Lambda Set Specialization},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591260},
doi = {10.1145/3591260},
abstract = {Higher-order functions pose a challenge for both static program analyses and optimizing compilers.  
To simplify the analysis and compilation of languages with higher-order functions, a rich body of  
prior work has proposed a variety of defunctionalization techniques, which can eliminate  
higher-order functions from a program by transforming the program to a semantically-equivalent  
first-order representation. Several modern languages take this a step further, specializing  
higher-order functions with respect to the functions on which they operate, and in turn allowing  
compilers to generate more efficient code. However, existing specializing defunctionalization  
techniques restrict how function values may be used, forcing implementations to fall back on costly  
dynamic alternatives. We propose lambda set specialization (LSS), the first specializing  
defunctionalization technique which imposes no restrictions on how function values may be used. We  
formulate LSS in terms of a polymorphic type system which tracks the flow of function values through  
the program, and use this type system to recast specialization of higher-order functions with  
respect to their arguments as a form of type monomorphization. We show that our type system admits a  
simple and tractable type inference algorithm, and give a formalization and fully-mechanized proof  
in the Isabelle/HOL proof assistant showing soundness and completeness of the type inference  
algorithm with respect to the type system. To show the benefits of LSS, we evaluate its impact on  
the run time performance of code generated by the MLton compiler for Standard ML, the OCaml  
compiler, and the new Morphic functional programming language. We find that pre-processing with LSS  
achieves run time speedups of up to 6.85x under MLton, 3.45x for OCaml, and 78.93x for Morphic.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {146},
numpages = {24},
keywords = {defunctionalization, monomorphization, type systems}
}

@software{10.5281/zenodo.7712285,
author = {Brandon, William and Driscoll, Benjamin and Dai, Frank and Berkow, Wilson and Milano, Mae},
title = {Reproduction Package for Article "Better Defunctionalization Through Lambda Set Specialization"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7712285},
abstract = {
    <p>The repository contains a compressed docker image file, containing the artifact for the PLDI 2023 paper “Better Defunctionalization through Lambda Set Specialization.” To use this artifact, first decompress the file (using <code>tar</code> or an archiving program like 7zip), and then use <code>docker load</code> to load up the decompressed docker image. Information for reproducing the results from our PLDI paper is available in README.md files in the ‘morphic/’ and ‘LSSIsabelle/’ directories inside the Docker image. Note: the Docker archive must be decompressed and then loaded with <code>docker load</code> (<em>not</em> <code>docker import</code>, as our archive does not use squashed layers).</p>

},
keywords = {defunctionalization, monomorphization, type systems}
}

@article{10.1145/3591261,
author = {Ye, Qianchuan and Delaware, Benjamin},
title = {Taype: A Policy-Agnostic Language for Oblivious Computation},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591261},
doi = {10.1145/3591261},
abstract = {Secure multiparty computation (MPC) allows for joint computation over private data from multiple entities, usually backed by powerful cryptographic techniques that protect sensitive data. Several high-level programming languages have been proposed to make writing MPC applications accessible to non-experts. These languages typically require developers to enforce security policies within the logic of the secure application itself, making it difficult to update security requirements, or to experiment with different policies. This paper presents the design and implementation of Taype, a language that permits security concerns to be decoupled from the program logic. To do so, Taype provides the first implementation of oblivious algebraic data types and tape semantics, two language features recently proposed by a core calculus for oblivious computation, λOADT+. We evaluate our implementation of Taype on a range of benchmarks, demonstrating its ability to encode a range of security polices for a rich class of data types.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {147},
numpages = {25},
keywords = {Algebraic Data Types, Dependent type systems, Oblivious computation}
}

@software{10.5281/zenodo.7806981,
author = {Ye, Qianchuan and Delaware, Benjamin},
title = {Taype: A Policy-Agnostic Language for Oblivious Computation: PLDI23 Artifact},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7806981},
abstract = {
    <p>This is the artifact for the PLDI23 paper “Taype: A Policy-Agnostic Language for Oblivious Computation”. Visit the Zenodo link for more details.</p>

},
keywords = {Algebraic Data Types, Dependent type systems, Oblivious computation}
}

@article{10.1145/3591262,
author = {Moseley, Dan and Nishio, Mario and Perez Rodriguez, Jose and Saarikivi, Olli and Toub, Stephen and Veanes, Margus and Wan, Tiki and Xu, Eric},
title = {Derivative Based Nonbacktracking Real-World Regex Matching with Backtracking Semantics},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591262},
doi = {10.1145/3591262},
abstract = {We develop a new derivative based theory and algorithm for nonbacktracking regex matching that supports anchors and counting, preserves backtracking semantics, and can be extended with lookarounds. The algorithm has been implemented as a new regex backend in .NET and was extensively tested as part of the formal release process of .NET7. We present a formal proof of the correctness of the algorithm, which we believe to be the first of its kind concerning industrial implementations of regex matchers. The paper describes the complete foundation, the matching algorithm, and key aspects of the implementation involving a regex rewrite system, as well as a comprehensive evaluation over industrial case studies and other regex engines.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {148},
numpages = {24},
keywords = {PCRE, automata, derivative, matching, regex, symbolic}
}

@software{10.5281/zenodo.7709500,
author = {Moseley, Dan and Nishio, Mario and Perez Rodriguez, Jose and Saarikivi, Olli and Toub, Stephen and Veanes, Margus and Wan, Tiki and Xu, Eric},
title = {Artifact for "Derivative Based Nonbacktracking Real-World Regex Matching with Backtracking Semantics"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7709500},
abstract = {
    <p>This artifact contains the necessary software and data for reproducing results for the paper “Derivative Based Nonbacktracking Real-World Regex Matching with Backtracking Semantics”. It is based on a benchmark of regex engines available in various programming languages by Mario Ju\'{a}rez. A docker image with all the necessary software for running the benchmark is included. The work described in the paper is part of .NET 7, the source code of which is also included.</p>

},
keywords = {.net, benchmark, dotnet, regex, regular expression, source code}
}

@article{10.1145/3591263,
author = {Avanzini, Martin and Moser, Georg and Schaper, Michael},
title = {Automated Expected Value Analysis of Recursive Programs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591263},
doi = {10.1145/3591263},
abstract = {In this work, we study the fully automated inference of expected result values of probabilistic programs in the presence of natural programming constructs such as procedures, local variables and recursion. While crucial, capturing these constructs becomes highly non-trivial. The key contribution is the definition of a term representation, denoted as infer[.], translating a pre-expectation semantics into first-order constraints, susceptible to automation via standard methods. A crucial step is the use of logical variables, inspired by previous work on Hoare logics for recursive programs. Noteworthy, our methodology is not restricted to tail-recursion, which could unarguably be replaced by iteration and wouldn't need additional insights. We have implemented this analysis in our prototype ev-imp. We provide ample experimental evidence of the prototype's algorithmic expressibility.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {149},
numpages = {23},
keywords = {automation, expected value analysis, probabilistic programming, weakest pre-expectation semantics}
}

@software{10.5281/zenodo.7801911,
author = {Avanzini, Martin and Moser, Georg and Schaper, Michael},
title = {Ev-Imp: Research Artifact},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7801911},
abstract = {
    <p>Ev-imp implements an expected value analysis for probabilistic, imperative programs featuring dynamic sampling instructions, non-deterministic choice, nested loops and most crucially recursive procedure declarations. Concretely, it estimates the value returned by a procedure, in average, as a function of the initial state. To this end, it implements the inference machinery described in Section 5 of our paper. For an overview of the concrete syntax of programs and usage of the tool, we kindly refer the reader to the accompanying README.md.</p>

},
keywords = {automation, expected value analysis, probabilistic programming, weakest pre-expectation semantics}
}

@article{10.1145/3591264,
author = {Zhang, Jialun and Morrisett, Greg and Tan, Gang},
title = {Interval Parsing Grammars for File Format Parsing},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591264},
doi = {10.1145/3591264},
abstract = {File formats specify how data is encoded for persistent storage. They cannot be formalized as context-free grammars since their specifications include context-sensitive patterns such as the random access pattern and the type-length-value pattern. We propose a new grammar mechanism called Interval Parsing Grammars IPGs) for file format specifications. An IPG attaches to every nonterminal/terminal an interval, which specifies the range of input the nonterminal/terminal consumes. By connecting intervals and attributes, the context-sensitive patterns in file formats can be well handled. In this paper, we formalize IPGs' syntax as well as its semantics, and its semantics naturally leads to a parser generator that generates a recursive-descent parser from an IPG. In general, IPGs are declarative, modular, and enable termination checking. We have used IPGs to specify a number of file formats including ZIP, ELF, GIF, PE, and part of PDF; we have also evaluated the performance of the generated parsers.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {150},
numpages = {23},
keywords = {Context-sensitive Grammars, File Formats}
}

@software{10.5281/zenodo.7811236,
author = {Zhang, Jialun and Morrisett, Greg and Tan, Gang},
title = {Reproduction Package for Article "Interval Parsing Grammars for File Format Parsing"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7811236},
abstract = {
    <p>This package includes: (1) The parser generator for IPGs; (2) IPGs implementation for ELF, ZIP, PE, GIF and PDF; (3) A test script to reproduce all the evaluation results shown in the paper.</p>

},
keywords = {Context-sensitive Grammars, File Formats}
}

@article{10.1145/3591265,
author = {Rao, Xiaojia and Georges, A\"{\i}na Linn and Legoupil, Maxime and Watt, Conrad and Pichon-Pharabod, Jean and Gardner, Philippa and Birkedal, Lars},
title = {Iris-Wasm: Robust and Modular Verification of WebAssembly Programs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591265},
doi = {10.1145/3591265},
abstract = {WebAssembly makes it possible to run C/C++ applications on the web with near-native performance. A WebAssembly program is expressed as a collection of higher-order ML-like modules, which are composed together through a system of explicit imports and exports using a host language, enabling a form of higher- order modular programming. We present Iris-Wasm, a mechanized higher-order separation logic building on a specification of Wasm 1.0 mechanized in Coq and the Iris framework. Using Iris-Wasm, we are able to specify and verify individual modules separately, and then compose them modularly in a simple host language featuring the core operations of the WebAssembly JavaScript Interface. Building on Iris-Wasm, we develop a logical relation that enforces robust safety: unknown, adversarial code can only affect other modules through the functions that they explicitly export. Together, the program logic and the logical relation allow us to formally verify functional correctness of WebAssembly programs, even when they invoke and are invoked by unknown code, thereby demonstrating that WebAssembly enforces strong isolation between modules.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {151},
numpages = {25},
keywords = {WebAssembly, formal verification, higher-order logic, separation logic}
}

@software{10.5281/zenodo.7808708,
author = {Rao, Xiaojia and Georges, A\"{\i}na Linn and Legoupil, Maxime and Watt, Conrad and Pichon-Pharabod, Jean and Gardner, Philippa and Birkedal, Lars},
title = {Iris-Wasm: Robust and Modular Verification of WebAssembly Programs (Artefact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7808708},
abstract = {
    <p>This is the artefact for the paper “Iris-Wasm: Robust and Modular Verification of WebAssembly Programs”.</p>
<p>The artefact contains the Coq proofs accompanying the paper. These proofs are built using the Iris framework.</p>
<p>These proofs are available either as a .tar.gz archive, which can be compiled following the instructions in the README contained, or as a virtual machine image for VirtualBox (7.0.6) containing the already compiled Coq proofs with browsing tools (Emacs + Proof General) installed. The credentials for the account of the VM are provided in the README inside the .tar.gz archive.</p>

},
keywords = {Coq, Iris, Mechanized proofs, Separation logic, WebAssembly}
}

@article{10.1145/3591266,
author = {Sewell, Thomas and Myreen, Magnus O. and Tan, Yong Kiam and Kumar, Ramana and Mihajlovic, Alexander and Abrahamsson, Oskar and Owens, Scott},
title = {Cakes That Bake Cakes: Dynamic Computation in CakeML},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591266},
doi = {10.1145/3591266},
abstract = {We have extended the verified CakeML compiler with a new language primitive,  
Eval, which permits evaluation of new CakeML syntax at runtime. This new  
implementation supports an ambitious form of compilation at runtime and dynamic  
execution, where the original and dynamically added code can share  
(higher-order) values and recursively call each other. This is, to our  
knowledge, the first verified run-time environment capable of supporting a  
standard LCF-style theorem prover design.  

Modifying the modern CakeML compiler pipeline and proofs to support  
a dynamic computation semantics was an extensive project. We review the  
design decisions, proof techniques, and proof engineering lessons  
from the project, and highlight some unexpected complications.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {152},
numpages = {24},
keywords = {compiler verification, dynamic computation, interactive theorem proving}
}

@software{10.5281/zenodo.7813942,
author = {Sewell, Thomas and Myreen, Magnus O. and Tan, Yong Kiam and Kumar, Ramana and Mihajlovic, Alexander and Abrahamsson, Oskar and Owens, Scott},
title = {Cakeml+Eval Artifact},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7813942},
abstract = {
    <p>This is a collection of CakeML and Candle artefacts to accompany “Cakes that Bake Cakes: Dynamic Computation in CakeML”.</p>
<p>In this modified version of CakeML, the REPL and Candle modes are built into the standard bootstrapped CakeML compiler. The compiler is built into a binary via the in-HOL4 verified self-bootstrap mechanism.</p>
<p>The resulting executable REPL, and the proof repositories it is built from, are provided pre-built in this artefact.</p>

},
keywords = {compiler verification, dynamic computation, interactive theorem proving}
}

@article{10.1145/3591267,
author = {Goens, Andr\'{e}s and Chakraborty, Soham and Sarkar, Susmit and Agarwal, Sukarn and Oswald, Nicolai and Nagarajan, Vijay},
title = {Compound Memory Models},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591267},
doi = {10.1145/3591267},
abstract = {Today's mobile, desktop, and server processors are heterogeneous, consisting not only of CPUs but also GPUs and other accelerators. Such heterogeneous processors are starting to expose a shared memory interface across these devices.Given that each of these individual devices typically supports a distinct instruction set architecture and a distinct memory consistency model, it is not clear what the memory consistency model of the heterogeneous machine should be. In this paper, we answer this question by formalizing "compound" memory models: we present a compositional operational model describing the resulting model when devices with distinct consistency models are fused together. We instantiate our model with the compound x86TSO/PTX model -- a CPU enforcing x86TSO and a GPU enforcing the PTX model. A key result is that the x86TSO/PTX compound model retains compiler mappings from the language-based (scoped) C memory model. This means that threads mapped to the x86TSO device can continue to use the already proven C-to-x86TSO compiler mapping, and the same for PTX.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {153},
numpages = {24},
keywords = {coherence protocols, compound memory models, consistency models}
}

@software{10.5281/zenodo.7798646,
author = {Goens, Andr\'{e}s and Chakraborty, Soham and Sarkar, Susmit and Agarwal, Sukarn and Oswald, Nicolai and Nagarajan, Vijay},
title = {Compound Memory Models: Artifact},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7798646},
abstract = {
    <p>This is the artifact to the paper “Compound Memory Models”. It includes the LOST-POP model implementation in Lean 4, the CMM implementation in Alloy, multiple litmus tests for both and accompanying gem5 simulations for some litmus tests.</p>

},
keywords = {alloy, axiomatic, compositional, gem5, lean4, memory models, operational, simulator}
}

@article{10.1145/3591268,
author = {Kovach, Scott and Kolichala, Praneeth and Gu, Tiancheng and Kjolstad, Fredrik},
title = {Indexed Streams: A Formal Intermediate Representation for Fused Contraction Programs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591268},
doi = {10.1145/3591268},
abstract = {We introduce indexed streams, a formal operational model and intermediate representation that describes the fused execution of a contraction language that encompasses both sparse tensor algebra and relational algebra. We prove that the indexed stream model is correct with respect to a functional semantics. We also develop a compiler for contraction expressions that uses indexed streams as an intermediate representation. The compiler is only 540 lines of code, but we show that its performance can match both the TACO compiler for sparse tensor algebra and the SQLite and DuckDB query processing libraries for relational algebra.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {154},
numpages = {25},
keywords = {contractions, functional programming, operational semantics, streams}
}

@software{10.5281/zenodo.7809339,
author = {Kovach, Scott and Kolichala, Praneeth and Gu, Tiancheng and Kjolstad, Fredrik},
title = {Benchmark Reproduction for "Indexed Streams: A Formal Intermediate Representation for Fused Contraction Programs"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7809339},
abstract = {
    <p>Contains formal proofs in Lean3 and benchmarking code for evaluation (section 8).</p>

},
keywords = {contractions, functional programming, operational semantics, streams}
}

@article{10.1145/3591269,
author = {Yallop, Jeremy and Xie, Ningning and Krishnaswami, Neel},
title = {flap: A Deterministic Parser with Fused Lexing},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591269},
doi = {10.1145/3591269},
abstract = {Lexers and parsers are typically defined separately and connected by a token stream. This separate definition is important for modularity and reduces the potential for parsing ambiguity. However, materializing tokens as data structures and case-switching on tokens comes with a cost. We show how to fuse separately-defined lexers and parsers, drastically improving performance without compromising modularity or increasing ambiguity. We propose a deterministic variant of Greibach Normal Form that ensures deterministic parsing with a single token of lookahead and makes fusion strikingly simple, and prove that normalizing context free expressions into the deterministic normal form is semantics-preserving. Our staged parser combinator library, flap, provides a standard interface, but generates specialized token-free code that runs two to six times faster than ocamlyacc on a range of benchmarks.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {155},
numpages = {24},
keywords = {fusion, lexing, multi-stage programming, optimization, parsing}
}

@software{10.5281/zenodo.7824835,
author = {Yallop, Jeremy and Xie, Ningning and Krishnaswami, Neel},
title = {flap: A Deterministic Parser with Fused Lexing (artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7824835},
abstract = {
    <p>Artifact accompanying the paper “flap: A Deterministic Parser with Fused Lexing”. Please see the latest version: https://doi.org/10.5281/zenodo.7712770</p>

},
keywords = {fusion, lexing, multi-stage programming, optimization, parsing}
}

@article{10.1145/3591270,
author = {Chen, Yu-Fang and Chung, Kai-Min and Leng\'{a}l, Ond\v{r}ej and Lin, Jyun-Ao and Tsai, Wei-Lun and Yen, Di-De},
title = {An Automata-Based Framework for Verification and Bug Hunting in Quantum Circuits},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591270},
doi = {10.1145/3591270},
abstract = {We introduce a new paradigm for analysing and finding bugs in quantum circuits. In our approach, the problem is given by a ‍triple {P} C {Q} and the question is whether, given a set P of quantum states on the input of a circuit C, the set of quantum states on the output is equal to (or included in) a set Q. While this is not suitable to specify, e.g., functional correctness of a quantum circuit, it is sufficient to detect many bugs in quantum circuits. We propose a technique based on tree automata to compactly represent sets of quantum states and develop transformers to implement the semantics of quantum gates over this representation. Our technique computes with an algebraic representation of quantum states, avoiding the inaccuracy of working with floating-point numbers. We implemented the proposed approach in a prototype tool and evaluated its performance against various benchmarks from the literature. The evaluation shows that our approach is quite scalable, e.g., we managed to verify a large circuit with 40 qubits and 141,527 gates, or catch bugs injected into a circuit with 320 qubits and 1,758 gates, where all tools we compared with failed. In addition, our work establishes a connection between quantum program verification and automata, opening new possibilities to exploit the richness of automata theory and automata-based verification in the world of quantum computing.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {156},
numpages = {26},
keywords = {quantum circuits, tree automata, verification}
}

@software{10.5281/zenodo.7811406,
author = {Chen, Yu-Fang and Chung, Kai-Min and Leng\'{a}l, Ond\v{r}ej and Lin, Jyun-Ao and Tsai, Wei-Lun and Yen, Di-De},
title = {An Automata-based Framework for Verification and Bug Hunting in Quantum Circuits},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7811406},
abstract = {
    <p>We introduce a new paradigm for analysing and finding bugs in quantum circuits. In our approach, the problem is given by a triple <span class="math inline">{<em>P</em>}&nbsp;<em>C</em>&nbsp;{<em>Q</em>}</span> and the question is whether, given a set <span class="math inline"><em>P</em></span> of quantum states on the input of a circuit <span class="math inline"><em>C</em></span>, the set of quantum states on the output is equal to (or included in) a set <span class="math inline"><em>Q</em></span>. While this is not suitable to specify, e.g., functional correctness of a quantum circuit, it is sufficient to detect many bugs in quantum circuits. We propose a technique based on tree automata to compactly represent sets of quantum states and develop transformers to implement the semantics of quantum gates over this representation. Our technique computes with an algebraic representation of quantum states, avoiding the inaccuracy of working with floating-point numbers. We implemented the proposed approach in a prototype tool and evaluated its performance against various benchmarks from the literature. The evaluation shows that our approach is quite scalable, e.g., we managed to verify a large circuit with 40 qubits and 141,527 gates, or catch bugs injected into a circuit with 320 qubits and 1,758 gates, where all tools we compared with failed. In addition, our work establishes a connection between quantum program verification and automata, opening new possibilities to exploit the richness of automata theory and automata-based verification in the world of quantum computing.</p>

},
keywords = {quantum circuits, tree automata, verification}
}

@article{10.1145/3591271,
author = {Zhou, Zhe and Mishra, Ashish and Delaware, Benjamin and Jagannathan, Suresh},
title = {Covering All the Bases: Type-Based Verification of Test Input Generators},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591271},
doi = {10.1145/3591271},
abstract = {Test input generators are an important part of property-based testing (PBT) frameworks. Because PBT is intended to test deep semantic and structural properties of a program, the outputs produced by these generators can be complex data structures, constrained to satisfy properties the developer believes is most relevant to testing the function of interest. An important feature expected of these generators is that they be capable of producing all acceptable elements that satisfy the function’s input type and generator-provided constraints. However, it is not readily apparent how we might validate whether a particular generator’s output satisfies this coverage requirement. Typically, developers must rely on manual inspection and post-mortem analysis of test runs to determine if the generator is providing sufficient coverage; these approaches are error-prone and difficult to scale as generators become more complex. To address this important concern, we present a new refinement type-based verification procedure for validating the coverage provided by input test generators, based on a novel interpretation of types that embeds “must-style” underapproximate reasoning principles as a fundamental part of the type system. The types associated with expressions now capture the set of values guaranteed to be produced by the expression, rather than the typical formulation that uses types to represent the set of values an expression may produce. Beyond formalizing the notion of coverage types in the context of a rich core language with higher-order procedures and inductive datatypes, we also present a detailed evaluation study to justify the utility of our ideas.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {157},
numpages = {24},
keywords = {property-based testing, refinement types, underapproximate reasoning}
}

@software{10.5281/zenodo.7811004,
author = {Zhou, Zhe and Mishra, Ashish and Delaware, Benjamin and Jagannathan, Suresh},
title = {PLDI2023 Artifact: Covering All the Bases: Type-Based Verification of Test Input Generators},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7811004},
abstract = {
    <p>This artifact contains:</p>
<ol type="1">
<li>README.md : the artifact guide.</li>
<li>poirot-full.pdf: full paper with the appendix.</li>
<li>poirot_pldi-2023.tar.gz: the docker image (optional, we recommend to pull from the docker hub, see README.md).</li>
<li>Dockerfile: the docker file that can reproduce the docker image (optional, we recommend to pull from the docker hub, see README.md).</li>
</ol>

},
keywords = {property-based testing, refinement types, underapproximate reasoning}
}

@article{10.1145/3591272,
author = {Kuepper, Joel and Erbsen, Andres and Gross, Jason and Conoly, Owen and Sun, Chuyue and Tian, Samuel and Wu, David and Chlipala, Adam and Chuengsatiansup, Chitchanok and Genkin, Daniel and Wagner, Markus and Yarom, Yuval},
title = {CryptOpt: Verified Compilation with Randomized Program Search for Cryptographic Primitives},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591272},
doi = {10.1145/3591272},
abstract = {Most software domains rely on compilers to translate high-level code to multiple different machine languages,  
with performance not too much worse than what developers would have the patience to write directly  
in assembly language. However, cryptography has been an exception, where many performance-critical  
routines have been written directly in assembly (sometimes through metaprogramming layers). Some past  
work has shown how to do formal verification of that assembly, and other work has shown how to generate  
C code automatically along with formal proof, but with consequent performance penalties vs. the best-  
known assembly. We present CryptOpt, the first compilation pipeline that specializes high-level cryptographic  
functional programs into assembly code significantly faster than what GCC or Clang produce, with mechanized  
proof (in Coq) whose final theorem statement mentions little beyond the input functional program and the  
operational semantics of x86-64 assembly. On the optimization side, we apply randomized search through the  
space of assembly programs, with repeated automatic benchmarking on target CPUs. On the formal-verification  
side, we connect to the Fiat Cryptography framework (which translates functional programs into C-like IR  
code) and extend it with a new formally verified program-equivalence checker, incorporating a modest subset  
of known features of SMT solvers and symbolic-execution engines. The overall prototype is quite practical,  
e.g. producing new fastest-known implementations of finite-field arithmetic for both Curve25519 (part of the  
TLS standard) and the Bitcoin elliptic curve secp256k1 for the Intel 12𝑡ℎ and 13𝑡ℎ generations.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {158},
numpages = {25},
keywords = {assembly, elliptic-curve cryptography, search-based software engineering}
}

@software{10.5281/zenodo.7710435,
author = {Kuepper, Joel and Erbsen, Andres and Gross, Jason and Conoly, Owen and Sun, Chuyue and Tian, Samuel and Wu, David and Chlipala, Adam and Chuengsatiansup, Chitchanok and Genkin, Daniel and Wagner, Markus and Yarom, Yuval},
title = {Evaluation package for ‘ CryptOpt: Verified Compilation with Randomized Program Search for Cryptographic Primitives’},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7710435},
abstract = {
    <p>This artifact contains copies of CryptOpt, Fiat Cryptography and AssemblyLine. It also contains Dockerfiles to create a Docker container to run optimizations for the cryptographic primitives mentioned in the paper. It contains scripts to validate the claims, along with instructions how to build, run and evaluate. To create and build, an Internet connection is required to download dependencies.</p>

},
keywords = {Assembly, asymmetric cryptography, bet-and-run, C, Coq, cryptography, Docker, formal verification, Intel x86-64, Node.js, performance, performance measurements, random local search, straight line code}
}

@article{10.1145/3591273,
author = {Tardieu, Olivier and Grove, David and Bercea, Gheorghe-Teodor and Castro, Paul and Cwiklik, Jaroslaw and Epstein, Edward},
title = {Reliable Actors with Retry Orchestration},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591273},
doi = {10.1145/3591273},
abstract = {Cloud developers have to build applications that are resilient to failures and 
interruptions. We advocate for a fault-tolerant programming model for the cloud 
based on actors, retry orchestration, and tail calls. This model builds upon 
persistent data stores and message queues readily available on the cloud. Retry 
orchestration not only guarantees that (1) failed actor invocations will be 
retried but also that (2) completed invocations are never repeated and (3) it 
preserves a strict happen-before relationship across failures within call 
stacks. Tail calls can break complex tasks into simple steps to minimize 
re-execution during recovery. We review key application patterns and failure 
scenarios. We formalize a process calculus to precisely capture the mechanisms 
of fault tolerance in this model. We briefly describe our implementation. Using 
an application inspired by a typical enterprise scenario, we validate 
the functional correctness of our implementation and assess the impact 
of fault preparedness and recovery on performance.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {159},
numpages = {24},
keywords = {actors, distributed systems, fault tolerance, workflows}
}

@software{10.5281/zenodo.7805564,
author = {Tardieu, Olivier and Grove, David and Bercea, Gheorghe-Teodor and Castro, Paul and Cwiklik, Jaroslaw and Epstein, Edward},
title = {Software Artifact for PACMPL Article "Reliable Actors with Retry Orchestration"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7805564},
abstract = {
    <p>Software artifact supporting the claims of the PACMPL (PLDI) 2023 paper “Reliable Actors with Retry Orchestration”.</p>

},
keywords = {actors, distributed systems, fault tolerance, workflows}
}

@article{10.1145/3591274,
author = {Valizadeh, Mojtaba and Berger, Martin},
title = {Search-Based Regular Expression Inference on a GPU},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591274},
doi = {10.1145/3591274},
abstract = {Regular expression inference (REI) is a supervised machine learning and program synthesis problem that takes a cost metric for regular expressions, and positive and negative examples of strings as input. It outputs a regular expression that is precise (i.e., accepts all positive and rejects all negative examples), and minimal w.r.t. to the cost metric. We present a novel algorithm for REI over arbitrary alphabets that is enumerative and trades off time for space. Our main algorithmic idea is to implement the search space of regular expressions succinctly as a contiguous matrix of bitvectors. Collectively, the bitvectors represent, as characteristic sequences, all sub-languages of the infix-closure of the union of positive and negative examples. Mathematically, this is a semiring of (a variant of) formal power series. Infix-closure enables bottom-up compositional construction of larger from smaller regular expressions using the operations of our semiring. This minimises data movement and data-dependent branching, hence maximises data-parallelism. In addition, the infix-closure remains unchanged during the search, hence search can be staged: first pre-compute various expensive operations, and then run the compute intensive search process. We provide two C++ implementations, one for general purpose CPUs and one for Nvidia GPUs (using CUDA). We benchmark both on Google Colab Pro: the GPU implementation is on average over 1000x faster than the CPU implementation on the hardest benchmarks.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {160},
numpages = {23},
keywords = {GPU, Grammar inference, machine learning, program synthesis, regular expression inference}
}

@article{10.1145/3591275,
author = {Mulder, Ike and Czajka, \L{}ukasz and Krebbers, Robbert},
title = {Beyond Backtracking: Connections in Fine-Grained Concurrent Separation Logic},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591275},
doi = {10.1145/3591275},
abstract = {Concurrent separation logic has been responsible for major advances in the formal verification of fine-grained concurrent algorithms and data structures such as locks, barriers, queues, and reference counters. The key ingredient of the verification of a fine-grained program is an invariant, which relates the physical data representation (on the heap) to a logical representation (in mathematics) and to the state of the threads (using a form of ghost state). An invariant is typically represented as a disjunction of logical states, but this disjunctive nature makes invariants a difficult target for automated verification. Current approaches roughly suffer from two problems. They use backtracking to introduce disjunctions in an uninformed manner, which can lead to unprovable goals if an appropriate case analysis has not been made before choosing the disjunct. Moreover, they eliminate disjunctions too eagerly, which can cause poor efficiency. While disjunctions are no problem for automated provers based on classical (i.e., non-separating) logic, the challenges with disjunctions are prominent in the study of proof automation for intuitionistic logic. We take inspiration from that area—specifically, based on ideas from connection calculus, we design a simple multi-succedent calculus for separation logic with disjunctions featuring a novel concept of a connection. While our calculus is not complete, it has the advantage that it can be extended with features of the state-of-the-art concurrent separation logic Iris (such as modalities, higher-order quantification, ghost state, and invariants), and can be implemented effectively in the Coq proof assistant with little need for backtracking. We evaluate the practicality on 24 challenging benchmarks, 14 of which we can verify fully automatically.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {161},
numpages = {25},
keywords = {Coq, Iris, Separation logic, backtracking, disjunctions, fine-grained concurrency, proof automation}
}

@software{10.5281/zenodo.7799173,
author = {Mulder, Ike and Czajka, \L{}ukasz and Krebbers, Robbert},
title = {Artifact and Appendix of 'Beyond Backtracking: Connections in Fine-Grained Concurrent Separation Logic'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7799173},
abstract = {
    <p>This is the artifact and appendix for the PLDI ‘23 paper ’Beyond Backtracking: Connections in Fine-Grained Concurrent Separation Logic’. It contains the source code of an extension of Diaframe that has better support for disjunctions, a VM containing a compiled version of this source code, instructions for evaluation, and the technical appendix.</p>

},
keywords = {backtracking, Coq, disjunctions, fine-grained concurrency, Iris, proof automation, Separation logic}
}

@article{10.1145/3591276,
author = {Zakhour, George and Weisenburger, Pascal and Salvaneschi, Guido},
title = {Type-Checking CRDT Convergence},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591276},
doi = {10.1145/3591276},
abstract = {Conflict-Free Replicated Data Types (CRDTs) are a recent approach for keeping replicated data consistent while guaranteeing the absence of conflicts among replicas. For correct operation, CRDTs rely on a merge function that is commutative, associative and idempotent. Ensuring that such algebraic properties are satisfied by implementations, however, is left to the programmer, resulting in a process that is complex and error-prone. While techniques based on testing, automatic verification of a model, and mechanized or handwritten proofs are available, we lack an approach that is able to verify such properties on concrete CRDT implementations. 

In this paper, we present Propel, a programming language with a type system that captures the algebraic properties required by a correct CRDT implementation. The Propel type system deduces such properties by case analysis and induction: sum types guide the case analysis and algebraic properties in function types enable induction for free. Propel’s key feature is its capacity to reason about algebraic properties (a) in terms of rewrite rules and (b) to derive the equality or inequality of expressions from the properties. We provide an implementation of Propel as a Scala embedding, we implement several CRDTs, verify them with Propel and compare the verification process with four state-of-the-art verification tools. Our evaluation shows that Propel is able to automatically deduce the properties that are relevant for common CRDT implementations found in open-source libraries even in cases in which competitors timeout.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {162},
numpages = {24},
keywords = {Conflict-Free Replicated Data Types, Type Systems, Verification}
}

@software{10.5281/zenodo.7817421,
author = {Zakhour, George and Weisenburger, Pascal and Salvaneschi, Guido},
title = {Type-Checking CRDT Convergence},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7817421},
abstract = {
    <p>Propel – Verifying the algebraic and relational properties of functions</p>
<p>Artifact for the paper “Type-Checking CRDT Convergence”</p>
<h2 id="getting-started">GETTING STARTED</h2>
<h3 id="building-and-loading-the-docker-image">BUILDING AND LOADING THE DOCKER IMAGE</h3>
<p>We provide you with <code>propel.tar.xz</code>, which is a pre-built container image that contains all necessary programs. To load, run the following command:</p>
<pre><code>$ docker load &lt; propel.tar.xz</code></pre>
<p>Further, we also provide the option to build the contain anew. To build, run the following command which takes between 10 and 20 minutes:</p>
<pre><code>$ docker build -t propel .</code></pre>
<p>Rebuilding the image may not work on Apple M1 machines because of incomplete emulation of system calls (specifically the inotify kernel subsystem). Hence, we recommend rebuilding the image on a platform fully supported by Docker, like x86-64 systems.</p>
<h3 id="checking-if-the-container-and-the-relevant-programs-run-correctly">CHECKING IF THE CONTAINER AND THE RELEVANT PROGRAMS RUN CORRECTLY</h3>
<p>We provide a script that runs fast checks on Propel and the other provers (HipSpec, Zeno, cvc5, Vampire) used in the evaluation.</p>
<p>The check verifies commutativity of natural number addition – a task which all programs are able to prove correct quickly. The following command runs the check:</p>
<pre><code>$ docker run -it --rm propel /check_image/check</code></pre>
<p>If you see in green the line “Check Done” at the end, the container is behaving as expected.</p>
<p>The check will show the provers’ output, which should look similar to the following (shortened) excerpt:</p>
<pre><code>Checking Zeno

[...]

Searching for proofs... 
Proved "CommutativityAddition.prop_comm_add : add x y = add y x"

[...]

Checking HipSpec

[...]

Proved:
    add m n == add n m
    add m (add n o) == add n (add m o)
    prop_comm_add {- add x y == add y x -}


Checking CVC5
"commutativity nat_add2p"
unsat

Checking Vampire

[...]

\% Termination reason: Refutation

[...]

Checking Propel

✔ Check successful.

Check Done</code></pre>
<p>Note that CVC5 and and Vampire report <code>unsat</code> or <code>Refutation</code>, respectively. This is because properties are verified by SMT solvers by finding a counterexample for their negation.</p>
<h2 id="step-by-step-instructions">STEP-BY-STEP INSTRUCTIONS</h2>
<h3 id="compiling-propel">COMPILING PROPEL</h3>
<p>The provided container already contains a binary executable of Propel.</p>
<p>To compile Propel to Java bytecode yourself, run the following command:</p>
<pre><code>$ docker run -it --rm propel bash -c 'cd /propel; sbt clean compile'</code></pre>
<p>To compile Propel to a native binary yourself, run the following command:</p>
<pre><code>$ docker run -it --rm propel bash -c 'cd /propel; sbt clean nativeLink'</code></pre>
<p>Compiling Propel, to bytecode or to a native executable, may not work inside the Docker container on Apple M1 machines for the reasons mentioned earlier.</p>
<p>The resulting binary is at <code>/propel/.native/target/scala-3.2.2/propel</code>. The <code>propel</code> executable in the PATH is already symlinked to that binary file. Hence, by default, you can just run <code>propel</code>.</p>
<h3 id="testing-propel">TESTING PROPEL</h3>
<p>To run the tests in Propel, execute:</p>
<pre><code>$ docker run -it --rm propel bash -c 'cd /propel &amp;\&amp; sbt test'</code></pre>
<p>Running the Propel tests may not work inside the Docker container on Apple M1 machines for the reasons mentioned earlier.</p>
<p>Note that running all unit tests can take several minutes. The output should look similar to the following (shortened) excerpt:</p>
<pre><code>[info] SuccessfulPropertyChecks:
[info] - nat_add2p
[info] - nat_add3p
[info] - nat_mult2p
[info] - bv_add

[...]

[info] FailingPropertyChecks:
[info] - nat_add2p_acc !!! IGNORED !!!
[info] - nat_add3p_acc !!! IGNORED !!!

[...]

[info] Total number of tests run: 49
[info] Suites: completed 2, aborted 0
[info] Tests: succeeded 49, failed 0, canceled 0, ignored 15, pending 0
</code></pre>
<p>The <code>SuccessfulPropertyChecks</code> contain the examples for which Propel can verify all properties. The <code>FailingPropertyChecks</code> contain the examples for which Propel is unable to verify all properties, hence their unit tests are disabled (<code>IGNORED</code>).</p>
<h3 id="running-the-benchmarks">RUNNING THE BENCHMARKS</h3>
<p>The benchmarks in Tables 2 and 3, and Figure 1 can be re-executed with the container. The number of the properties that (1) could be proven,(2) could not be proven and (3) timed out should match the content of tables and the figures. The given time may differ depending on the system where the benchmarks are run. Due the the timeout of one minute, not only the amount of seconds can differ but also the type of the result. It could be the case that the benchmark succeeds or fails in less than 60s on one setup but takes more than 60s on a different setup, in which case it would time out.</p>
<p>To execute the benchmarks on HipSpec, run the following command:</p>
<pre><code>$ docker run -it --rm propel /benchmarks/hipspec/run</code></pre>
<p>To execute the benchmarks on Zeno, run the following command:</p>
<pre><code>$ docker run -it --rm propel /benchmarks/zeno/run</code></pre>
<p>To execute the benchmarks on cvc5, run the following command:</p>
<pre><code>$ docker run -it --rm propel /benchmarks/cvc5/run</code></pre>
<p>To execute the benchmarks on Vampire, run the following command:</p>
<pre><code>$ docker run -it --rm propel /benchmarks/vampire/run</code></pre>
<p>To execute the benchmarks on Propel, run the following command:</p>
<pre><code>$ docker run -it --rm propel /benchmarks/propel/run</code></pre>
<p>The results in Table 2 correspond to one line from the output of each command. We list each CRDT benchmark and the token its corresponding line starts with:</p>
<ol type="1">
<li>GCounter (Peano number list)
<ul>
<li>commutativity: <code>natlist_gcounter_comm</code></li>
<li>associativity: <code>natlist_gcounter_assoc</code></li>
<li>idempotency: <code>natlist_gcounter_idem</code></li>
</ul></li>
<li>GCounter (bit vector list)
<ul>
<li>commutativity: <code>bvlist_gcounter_comm</code></li>
<li>associativity: <code>bvlist_gcounter_assoc</code></li>
<li>idempotency: <code>bvlist_gcounter_idem</code></li>
</ul></li>
<li>BCounter (Peano number list)
<ul>
<li>commutativity: <code>natlist_bcounter_comm</code></li>
<li>associativity: <code>natlist_bcounter_assoc</code></li>
<li>idempotency: <code>natlist_bcounter_idem</code></li>
</ul></li>
<li>BCounter (bit vector list)
<ul>
<li>commutativity: <code>bvlist_bcounter_comm</code></li>
<li>associativity: <code>bvlist_bcounter_assoc</code></li>
<li>idempotency: <code>bvlist_bcounter_idem</code></li>
</ul></li>
<li>PNCounter (Peano number list)
<ul>
<li>commutativity: <code>natlist_pncounter_comm</code></li>
<li>associativity: <code>natlist_pncounter_assoc</code></li>
<li>idempotency: <code>natlist_pncounter_idem</code></li>
</ul></li>
<li>PNCounter (bit vector list)
<ul>
<li>commutativity: <code>bvlist_pncounter_comm</code></li>
<li>associativity: <code>bvlist_pncounter_assoc</code></li>
<li>idempotency: <code>bvlist_pncounter_idem</code></li>
</ul></li>
<li>LWW Register (Peano numbers)
<ul>
<li>commutativity: <code>nat_lwwreg_comm</code></li>
<li>associativity: <code>nat_lwwreg_assoc</code></li>
<li>idempotency: <code>nat_lwwreg_idem</code></li>
</ul></li>
<li>LWW Register (bit vectors)
<ul>
<li>commutativity: <code>bv_lwwreg_comm</code></li>
<li>associativity: <code>bv_lwwreg_assoc</code></li>
<li>idempotency: <code>bv_lwwreg_idem</code></li>
</ul></li>
<li>GSet
<ul>
<li>commutativity: <code>gset_comm</code></li>
<li>associativity: <code>gset_assoc</code></li>
<li>idempotency: <code>gset_idem</code></li>
</ul></li>
<li>ORSet
<ul>
<li>commutativity: <code>orset_comm</code></li>
<li>associativity: <code>orset_assoc</code></li>
<li>idempotency: <code>orset_idem</code></li>
</ul></li>
<li>2PSet
<ul>
<li>commutativity: <code>twophaseset_comm</code></li>
<li>associativity: <code>twophaseset_assoc</code></li>
<li>idempotency: <code>twophaseset_idem</code></li>
</ul></li>
</ol>
<p>The results in Table 3 correspond to one line from the output of each command. We list each CRDT benchmark and the token its corresponding line starts with:</p>
<ol type="1">
<li>add2p
<ul>
<li>commutativity: <code>nat_add2p_comm</code></li>
<li>associativity: <code>nat_add2p_assoc</code></li>
</ul></li>
<li>add3p
<ul>
<li>commutativity: <code>nat_add3p_comm</code></li>
<li>associativity: <code>nat_add3p_assoc</code></li>
</ul></li>
</ol>
<p>Figure 2 provides further benchmark results from those benchmarks of the TIP 2015 (Tons of Inductive Problems, https://tip-org.github.io/) benchmark suite, which check algebraic and relational properties supported by Propel. They can be run using the following commands:</p>
<pre><code>$ docker run -it --rm propel /tip2015/hipspec/run
$ docker run -it --rm propel /tip2015/zeno/run
$ docker run -it --rm propel /tip2015/cvc5/run
$ docker run -it --rm propel /tip2015/vampire/run
$ docker run -it --rm propel /tip2015/propel/run</code></pre>
<h3 id="using-propel-as-a-scala-dsl">USING PROPEL AS A SCALA DSL</h3>
<p>Propel as described in Section 3 is a DSL in Scala. To experiment with the DSL, we invite you take a look into <code>/propel/src/test/scala/propel/ScalaExamplesNat.scala</code>, <code>/propel/src/test/scala/propel/ScalaExamplesNum.scala</code> and <code>/propel/src/test/scala/propel/ScalaExamplesList.scala</code> inside the container.</p>
<p>As an example, you can execute the following commands to run a shell, explore the files and recompile the project:</p>
<pre><code>$ docker run -it --rm propel bash                               # open a shell
$ nano /propel/src/test/scala/propel/ScalaExamplesList.scala    # open the file

# edit and save the file

$ cd /propel &amp;\&amp; sbt Test/compile                                # recompile</code></pre>
<p>Compiling the examples may not work inside the Docker container on Apple M1 machines for the reasons mentioned earlier.</p>
<p>You may define your own function using the following syntax:</p>
<pre><code>def myFunction = prop[(FunctionProperties) ::= (T1, T1) =&gt;: T2] { (x, y) =&gt; body }
// or
def myRecursiveFunction = prop.rec[(FunctionProperties) ::= (T1, T1) =&gt;: T2] { myRecursiveFunction =&gt; (x, y) =&gt; body }</code></pre>
<p>Here, <code>myFunction</code> is the name of the function, <code>FunctionProperties</code> is a list of function properties the function has (separated by <code>&amp;</code>), <code>T1</code> is the type of the arguments of the binary function, <code>T2</code> is the return type of the function, <code>x</code> and <code>y</code> are the names of the function arguments, and <code>body</code> is the function body.</p>
<p>The function properties are chosen from the following list: <code>Comm</code>, <code>Assoc</code>, <code>Idem</code>, <code>Sel</code>, <code>Refl</code>, <code>Antisym</code>, <code>Trans</code>, <code>Conn</code>, and <code>Sym</code>. Their semantics is defined in Table 1.</p>
<p>If Propel is able to prove the properties that the function is annotated with, then compilation succeeds. If the properties cannot be proven, then a compilation error indicates which property could not be proven</p>
<p>For example, you can add the GCounter CRDT example from the paper to one of the files in <code>/propel/src/test/scala/propel</code></p>
<pre><code>def mergeGCounter = prop[(Comm \&amp; Assoc \&amp; Idem) := (List[Num], List[Num]) =&gt;: List[Num]] { (x, y) =&gt; zipWith(maxNum)(x, y) }</code></pre>
<p>We hope that the integration into Scala makes the artifact easily usable by other researchers, either (1) by directly using the DSL to check algebraic and relational properties of their programs or (2) by building on Propel’s verification engine. To facilitate the latter, the implementation of Propel’s Scala DSL (<code>propel.dsl</code> package) is separated from the verification mechanism (<code>propel.evaluator</code> package), which researchers can adopt independently of the Scala integration (an overview of the package structure is in the last section).</p>
<h3 id="using-propel-standalone-outside-of-scala">USING PROPEL STANDALONE (OUTSIDE OF SCALA)</h3>
<p>Propel can be directly reused as a verification tool in other projects (without the Scala and JVM dependency) through the <code>propel</code> binary. The binary consumes ASTs of Propel’s calculus in an S-expression-based syntax.</p>
<p>Our Scala implementation of the full surface language also follows the approach of translating Scala programs to terms in the calculus and passing them to the verification mechanism. A similar approach can be adopted by other tools that use Propel. Note that the AST is a bit more low-level then the Scala implementation and the calculus presented in the paper. In particular, the properties that are captured in the type of a function need to be propagated to the call sites of the function, i.e., function calls are syntactically annotated with the properties that should hold for them. The concrete format is described in the FORMAT.md file.</p>
<p>We provide all benchmarks in this format in the <code>/benchmarks/propel</code> directory. For example, the <code>nat_add2p_comm.propel</code> is a direct translation of the <code>add2p</code> function of Listing 4. This file can be checked by running:</p>
<pre><code>propel -f /benchmarks/propel/nat_add2p_comm.propel</code></pre>
<p>Additional information about the proof attempts can be shown using the <code>-d</code> and <code>-r</code> flags.</p>
<h3 id="structure-of-the-propel-source-code">STRUCTURE OF THE PROPEL SOURCE CODE</h3>
<p>Propel is organized into the following packages:</p>
<ul>
<li><code>ast</code>: Abstract syntax tree definitions for the verifier</li>
<li><code>dsl</code>: Scala DSL</li>
<li><code>evaluator</code>: Rewrite engine (used by an implementation of the calculus’ dynamic semantics and by the verifier)</li>
<li><code>evaluator.properties</code>: Verifier for algebraic and relational properties (call <code>evaluator.properties.check</code> on an <code>ast.Term</code> to verify properties)</li>
<li><code>parser</code>: Parser for Propel’s serialization format (as used by the benchmarks)</li>
<li><code>printing</code>: Pretty-printer for Propel ASTs</li>
<li><code>typer</code>: Standard type checker (not checking algebraic and relational properties)</li>
<li><code>util</code>: Small, useful definitions</li>
</ul>

},
keywords = {Conflict-Free Replicated Data Types, Type Systems, Verification}
}

@article{10.1145/3591277,
author = {Mordido, Andreia and Spaderna, Janek and Thiemann, Peter and Vasconcelos, Vasco T.},
title = {Parameterized Algebraic Protocols},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591277},
doi = {10.1145/3591277},
abstract = {We propose algebraic protocols that enable the definition of protocol templates and session types analogous to the definition of domain-specific types with algebraic datatypes. Parameterized algebraic protocols subsume all regular as well as most context-free and nested session types and, at the same time, replace the expensive superlinear algorithms for type checking by a nominal check that runs in linear time. Algebraic protocols in combination with polymorphism increase expressiveness and modularity by facilitating new ways of parameterizing and composing session types.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {163},
numpages = {25},
keywords = {Algebraic datatypes, Isorecursive types, Nominal types, Parameterized protocols, Session types}
}

@software{10.5281/zenodo.7804667,
author = {Mordido, Andreia and Spaderna, Janek and Thiemann, Peter and Vasconcelos, Vasco T.},
title = {AlgST},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7804667},
abstract = {
    <p>Implementation of “Parameterized Algebraic Protocols”</p>

},
keywords = {algebraic datatypes, algebraic session types, interpreter, nominal types, parameterized protocols, polymorphism, session types, type checker}
}

@article{10.1145/3591278,
author = {Fiala, Jon\'{a}\v{s} and Itzhaky, Shachar and M\"{u}ller, Peter and Polikarpova, Nadia and Sergey, Ilya},
title = {Leveraging Rust Types for Program Synthesis},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591278},
doi = {10.1145/3591278},
abstract = {The Rust type system guarantees memory safety and data-race freedom. However, to satisfy Rust's type rules, many familiar implementation patterns must be adapted substantially. These necessary adaptations complicate programming and might hinder language adoption. In this paper, we demonstrate that, in contrast to manual programming, automatic synthesis is not complicated by Rust's type system, but rather benefits in two major ways. First, a Rust synthesizer can get away with significantly simpler specifications. While in more traditional imperative languages, synthesizers often require lengthy annotations in a complex logic to describe the shape of data structures, aliasing, and potential side effects, in Rust, all this information can be inferred from the types, letting the user focus on specifying functional properties using a slight extension of Rust expressions. Second, the Rust type system reduces the search space for synthesis, which improves performance.  

In this work, we present the first approach to automatically synthesizing correct-by-construction programs in safe Rust. The key ingredient of our synthesis procedure is Synthetic Ownership Logic, a new program logic for deriving programs that are guaranteed to satisfy both a user-provided functional specification and, importantly, Rust's intricate type system. We implement this logic in a new tool called RusSOL. Our evaluation shows the effectiveness of RusSOL, both in terms of annotation burden and performance, in synthesizing provably correct solutions to common problems faced by new Rust developers.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {164},
numpages = {24},
keywords = {Rust, program logic, program synthesis, type systems}
}

@software{10.5281/zenodo.7811786,
author = {Fiala, Jon\'{a}\v{s} and Itzhaky, Shachar and M\"{u}ller, Peter and Polikarpova, Nadia and Sergey, Ilya},
title = {Reproduction Package for Article ``Leveraging Rust Types for Program Synthesis''},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7811786},
abstract = {
    <p>The purpose of this artifact is to reproduce the results presented in the PLDI 2023 paper titled “Leveraging Rust Types for Program Synthesis”. The artifact contains the instructions, tool, and Docker images to re-run the evaluation described in the paper. It also contains the appendix for the paper. The structure of the tool is described in the <code>sources/STRUCTURE.md</code> file.</p>

},
keywords = {ownership types, rust, synthesis}
}

@article{10.1145/3591279,
author = {Liu, Zongyuan and Stepanenko, Sergei and Pichon-Pharabod, Jean and Timany, Amin and Askarov, Aslan and Birkedal, Lars},
title = {VMSL: A Separation Logic for Mechanised Robust Safety of Virtual Machines Communicating above FF-A},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591279},
doi = {10.1145/3591279},
abstract = {Thin hypervisors make it possible to isolate key security components like keychains, fingerprint readers, and digital wallets from the easily-compromised operating system.  
To work together, virtual machines running on top of the hypervisor can make hypercalls to the hypervisor to share pages between each other in a controlled way.  
However, the design of such hypercall ABIs remains a delicate balancing task between conflicting needs for expressivity, performance, and security.  
In particular, it raises the question of what makes the specification of a hypervisor, and of its hypercall ABIs, good enough for the virtual machines.  
In this paper, we validate the expressivity and security of the design of the hypercall ABIs of Arm's FF-A.  
We formalise a substantial fragment of FF-A as a machine with a simplified ISA in which hypercalls are steps of the machine.  
We then develop VMSL, a novel separation logic, which we prove sound with respect to the machine execution model, and use it to reason modularly about virtual machines which communicate through the hypercall ABIs, demonstrating the hypercall ABIs' expressivity.  
Moreover, we use the logic to prove robust safety of communicating virtual machines, that is, the guarantee that even if some of the virtual machines are compromised and execute unknown code, they cannot break the safety properties of other virtual machines running known code.  
This demonstrates the intended security guarantees of the hypercall ABIs.  
All the results in the paper have been formalised in Coq using the Iris framework.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {165},
numpages = {25},
keywords = {FF-A, Iris, hypercall, logical relation, robust safety, separation logic}
}

@software{10.5281/zenodo.7813157,
author = {Liu, Zongyuan and Stepanenko, Sergei and Pichon-Pharabod, Jean and Timany, Amin and Askarov, Aslan and Birkedal, Lars},
title = {Artifact of "VMSL: A Separation Logic for Mechanised Robust Safety of Virtual Machines Communicating above FF-A"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7813157},
abstract = {
    <p>This is the artifact for the PLDI’23 paper “VMSL: A Separation Logic for Mechanised Robust Safety of Virtual Machines Communicating above FF-A”. It is the Coq mechanisation of all results presented in the paper.</p>

},
keywords = {FF-A, hypercall, Iris, logical relation, robust safety, separation logic}
}

@article{10.1145/3591280,
author = {Li, Ziyang and Huang, Jiani and Naik, Mayur},
title = {Scallop: A Language for Neurosymbolic Programming},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591280},
doi = {10.1145/3591280},
abstract = {We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-the-art models in terms of accuracy. Furthermore, Scallop's solutions outperform these models in aspects such as runtime and data efficiency, interpretability, and generalizability.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {166},
numpages = {25},
keywords = {Differentiable reasoning, Neurosymbolic methods}
}

@software{10.5281/zenodo.7804200,
author = {Li, Ziyang and Huang, Jiani and Naik, Mayur},
title = {Reproduction package for article "Scallop: A Language for Neurosymbolic Programming"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7804200},
abstract = {
    <p>This artifact includes source code of the software and experiments presented in the paper `Scallop: A Language for Neurosymbolic Programming’. Instructions to reproduce the results are documented in the artifact.</p>

},
keywords = {Differentiable Programming, Logic Programming, Neurosymbolic Method, Programming Language}
}

@article{10.1145/3591281,
author = {Saha, Seemanta and Ghentiyala, Surendra and Lu, Shihua and Bang, Lucas and Bultan, Tevfik},
title = {Obtaining Information Leakage Bounds via Approximate Model Counting},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591281},
doi = {10.1145/3591281},
abstract = {Information leaks are a significant problem in modern software systems. In recent years, information theoretic concepts, such as Shannon entropy, have been applied to quantifying information leaks in programs. One recent approach is to use symbolic execution together with model counting constraints solvers in order to quantify information leakage. There are at least two reasons for unsoundness in quantifying information leakage using this approach: 1) Symbolic execution may not be able to explore all execution paths, 2) Model counting constraints solvers may not be able to provide an exact count. We present a sound symbolic quantitative information flow analysis that bounds the information leakage both for the cases where the program behavior is not fully explored and the model counting constraint solver is unable to provide a precise model count but provides an upper and a lower bound. We implemented our approach as an extension to KLEE for computing sound bounds for information leakage in C programs.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {167},
numpages = {22},
keywords = {Information Leakage, Model Counting, Optimization, Quantitative Program Analysis, Symbolic Quantitative Information Flow Analysis}
}

@article{10.1145/3591282,
author = {Pailoor, Shankara and Chen, Yanju and Wang, Franklyn and Rodr\'{\i}guez, Clara and Van Geffen, Jacob and Morton, Jason and Chu, Michael and Gu, Brian and Feng, Yu and Dillig, I\c{s}\i{}l},
title = {Automated Detection of Under-Constrained Circuits in Zero-Knowledge Proofs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591282},
doi = {10.1145/3591282},
abstract = {As zero-knowledge proofs gain increasing adoption, the cryptography community has designed domain-specific languages (DSLs) that facilitate the construction of zero-knowledge proofs (ZKPs). Many of these DSLs, such as Circom, facilitate the construction of arithmetic circuits, which are essentially polynomial equations over a finite field. In particular, given a program in a zero-knowledge proof DSL, the compiler automatically produces the corresponding arithmetic circuit. However, a common and serious problem is that the generated circuit may be underconstrained, either due to a bug in the program or a bug in the compiler itself. Underconstrained circuits admit multiple witnesses for a given input, so a malicious party can generate bogus witnesses, thereby causing the verifier to accept a proof that it should not. Because of the increasing prevalence of such arithmetic circuits in blockchain applications, several million dollars worth of cryptocurrency have been stolen due to underconstrained arithmetic circuits. Motivated by this problem, we propose a new technique for finding ZKP bugs caused by underconstrained polynomial equations over finite fields. Our method performs semantic reasoning over the finite field equations generated by the compiler to prove whether or not each signal is uniquely determined by the input. Our proposed approach combines SMT solving with lightweight uniqueness inference to effectively reason about underconstrained circuits. We have implemented our proposed approach in a tool called QED2 and evaluate it on 163 Circom circuits. Our evaluation shows that QED2 can successfully solve 70\% of these benchmarks, meaning that it either verifies the uniqueness of the output signals or finds a pair of witnesses that demonstrate non-uniqueness of the circuit. Furthermore, QED2 has found 8 previously unknown vulnerabilities in widely-used circuits.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {168},
numpages = {23},
keywords = {SNARKs, program verification, zero-knowledge proofs}
}

@software{10.5281/zenodo.7776035,
author = {Pailoor, Shankara and Chen, Yanju and Wang, Franklyn and Rodr\'{\i}guez, Clara and Van Geffen, Jacob and Morton, Jason and Chu, Michael and Gu, Brian and Feng, Yu and Dillig, I\c{s}\i{}l},
title = {Automated Detection of Under-constrained Circuits in Zero-Knowledge Proofs},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7776035},
abstract = {
    <p>Research Artifact for PLDI’23 Paper “Automated Detection of Under-constrained Circuits in Zero-Knowledge Proofs”</p>

},
keywords = {Automated reasoning, Cryptographic protocols, Program analysis, Program verification}
}

@article{10.1145/3591283,
author = {Lehmann, Nico and Geller, Adam T. and Vazou, Niki and Jhala, Ranjit},
title = {Flux: Liquid Types for Rust},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591283},
doi = {10.1145/3591283},
abstract = {We introduce Flux, which shows how logical refinements can work hand in glove with Rust's ownership mechanisms to yield ergonomic type-based verification of low-level pointer manipulating programs. First, we design a novel refined type system for Rust that indexes mutable locations, with pure (immutable) values that can appear in refinements, and then exploits Rust's ownership mechanisms to abstract sub-structural reasoning about locations within Rust's polymorphic type constructors, while supporting strong updates. We formalize the crucial dependency upon Rust's strong aliasing guarantees by exploiting the Stacked Borrows aliasing model to prove that "well-borrowed evaluations of well-typed programs do not get stuck". Second, we implement our type system in Flux, a plug-in to the Rust compiler that exploits the factoring of complex invariants into types and refinements to efficiently synthesize loop annotations-including complex quantified invariants describing the contents of containers-via liquid inference. Third, we evaluate Flux with a benchmark suite of vector manipulating programs and parts of a previously verified secure sandboxing library to demonstrate the advantages of refinement types over program logics as implemented in the state-of-the-art Prusti verifier. While Prusti's more expressive program logic can, in general, verify deep functional correctness specifications, for the lightweight but ubiquitous and important verification use-cases covered by our benchmarks, liquid typing makes verification ergonomic by slashing specification lines by a factor of two, verification time by an order of magnitude, and annotation overhead from up to 24\% of code size (average 14\%), to nothing at all.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {169},
numpages = {25},
keywords = {Rust, heap-manipulating programs, liquid types}
}

@article{10.1145/3591284,
author = {Arora, Jatin and Westrick, Sam and Acar, Umut A.},
title = {Efficient Parallel Functional Programming with Effects},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591284},
doi = {10.1145/3591284},
abstract = {Although functional programming languages simplify writing safe  
parallel programs by helping programmers to avoid data races,  
they have traditionally delivered poor performance.  
Recent work improved performance by using a hierarchical memory  
architecture that allows processors to allocate and reclaim memory  
independently without any synchronization, solving thus the key  
performance challenge afflicting functional programs.  
The approach, however, restricts mutation, or memory effects, so as to  
ensure "disentanglement", a low-level memory property that  
guarantees independence between different heaps in the hierarchy.  

This paper proposes techniques for supporting entanglement and for  
allowing functional programs to use mutation at will.  
Our techniques manage entanglement by distinguishing between  
disentangled and entangled objects and shielding disentangled objects  
from the cost of entanglement management.  
We present a semantics that formalizes entanglement as a property at  
the granularity of memory objects, and define several cost metrics  
to reason about and bound the time and space cost of entanglement.  
We present an implementation of the techniques by extending the MPL  
compiler for Parallel ML.  
The extended compiler supports all features of the Parallel ML  
language, including unrestricted effects.  
Our experiments using a variety of benchmarks show that MPL incurs a  
small time and space overhead compared to sequential runs, scales  
well, and is competitive with languages such as C++, Go, Java, OCaml.  
These results show that our techniques can marry the safety benefits  
of functional programming with performance.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {170},
numpages = {26},
keywords = {concurrent, functional programming, memory management, parallel}
}

@software{10.5281/zenodo.7824069,
author = {Arora, Jatin and Westrick, Sam and Acar, Umut A.},
title = {Replication instructions for Article: Efficient Parallel Functional Programming with Effects},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7824069},
abstract = {
    <p>The artifact contains the implementation of our language MPL and also contains benchmarks for its evaluation w.r.t languages MLton, C/C++, Go, Java, and OCaml.</p>

},
keywords = {functional languages, memory management, parallel programming, parallelism}
}

@article{10.1145/3591285,
author = {Guria, Sankha Narayan and Foster, Jeffrey S. and Van Horn, David},
title = {Absynthe: Abstract Interpretation-Guided Synthesis},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591285},
doi = {10.1145/3591285},
abstract = {Synthesis tools have seen significant success in recent times. However,  
past approaches often require a complete and accurate embedding of the source  
language in the logic of the underlying solver, an approach difficult for  
industrial-grade languages. Other approaches couple the semantics of the source  
language with purpose-built synthesizers, necessarily tying the synthesis engine  
to a particular language model.  
In this paper, we propose Absynthe, an alternative approach based on  
user-defined abstract semantics that aims to be both lightweight and language  
agnostic, yet effective in guiding the search for programs.  
A synthesis goal in Absynthe is specified as an abstract  
specification in a lightweight user-defined abstract domain and concrete test  
cases.  
The synthesis engine is parameterized by the abstract semantics and independent  
of the source language.  
Absynthe validates candidate programs against test cases using the  
actual concrete language implementation to ensure correctness.  
We formalize the synthesis rules for Absynthe  
and describe how the key ideas are scaled-up in our implementation in Ruby. We  
evaluated Absynthe on SyGuS strings benchmark and found it competitive with  
other enumerative search solvers. Moreover, Absynthe's ability to combine  
abstract domains allows the user to move along a cost spectrum, i.e., expressive  
domains prune more programs but require more time. Finally, to verify  
Absynthe can act as a general purpose synthesis tool, we use Absynthe to  
synthesize Pandas data frame manipulating programs in Python using simple  
abstractions like types and column labels of a data frame. Absynthe reaches  
parity with AutoPandas, a deep learning based tool for the same benchmark  
suite. In summary, our results demonstrate Absynthe is a  
promising step forward towards a general-purpose approach to synthesis that may  
broaden the applicability of synthesis to more full-featured languages.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {171},
numpages = {24},
keywords = {abstract interpretation, program synthesis}
}

@software{10.5281/zenodo.7824175,
author = {Guria, Sankha Narayan and Foster, Jeffrey S. and Van Horn, David},
title = {Artifact for "Absynthe: Abstract Interpretation-Guided Synthesis"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7824175},
abstract = {
    <p>The artifact is a Docker image that contains all of the source code, benchmarks, and experiment harnesses used in the development of the paper (set-up and ready to run). The README contains instructions to reproduce results from the paper, as well as pointers for how to use, extend or modify the tool and benchmarks.</p>

},
keywords = {abstract interpretation, program synthesis, Ruby}
}

@article{10.1145/3591286,
author = {Jin, Ende and Amin, Nada and Zhang, Yizhou},
title = {Extensible Metatheory Mechanization via Family Polymorphism},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591286},
doi = {10.1145/3591286},
abstract = {With the growing practice of mechanizing language metatheories, it has become ever more pressing that interactive theorem provers make it easy to write reusable, extensible code and proofs. This paper presents a novel language design geared towards extensible metatheory mechanization in a proof assistant. The new design achieves reuse and extensibility via a form of family polymorphism, an object-oriented idea, that allows code and proofs to be polymorphic to their enclosing families. Our development addresses technical challenges that arise from the underlying language of a proof assistant being simultaneously functional, dependently typed, a logic, and an interactive tool. Our results include (1) a prototypical implementation of the language design as a Coq plugin, (2) a dependent type theory capturing the essence of the language mechanism and its consistency and canonicity results, and (3) case studies showing how the new expressiveness naturally addresses real programming challenges in metatheory mechanization.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {172},
numpages = {25},
keywords = {Coq, Proof engineering, dependent type theory, expression problem, extensible frameworks, inductive types, interactive theorem proving, late binding, mixins, modules, reuse}
}

@software{10.5281/zenodo.7800226,
author = {Jin, Ende and Amin, Nada and Zhang, Yizhou},
title = {Artifact for Extensible Metatheory Mechanization via Family Polymorphism},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7800226},
abstract = {
    <p>This artifact includes 3 components: (1) a complete document for interacting with this artifact; (2) a docker image for directly interacting with our plugin without setup; and (3) source code to build our plugin and work with it without virtualization</p>

},
keywords = {Coq., dependent type theory, expression problem, extensible frameworks, inductive types, interactive theorem proving, late binding, mixins, modules, Proof engineering, reuse}
}

@article{10.1145/3591287,
author = {Chida, Nariyoshi and Terauchi, Tachio},
title = {Repairing Regular Expressions for Extraction},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591287},
doi = {10.1145/3591287},
abstract = {While synthesizing and repairing regular expressions (regexes) based on Programming-by-Examples (PBE) methods have seen rapid progress in recent years, all existing works only support synthesizing or repairing regexes for membership testing, and the support for extraction is still an open problem. This paper fills the void by proposing the first PBE-based method for synthesizing and repairing regexes for extraction. Our work supports regexes that have real-world extensions such as backreferences and lookarounds. The extensions significantly affect the PBE-based synthesis and repair problem. In fact, we show that there are unsolvable instances of the problem if the synthesized regexes are not allowed to use the extensions, i.e., there is no regex without the extensions that correctly classify the given set of examples, whereas every problem instance is solvable if the extensions are allowed. This is in stark contrast to the case for the membership where every instance is guaranteed to have a solution expressible by a pure regex without the extensions. The main contribution of the paper is an algorithm to solve the PBE-based synthesis and repair problem for extraction. Our algorithm builds on existing methods for synthesizing and repairing regexes for membership testing, i.e., the enumerative search algorithms with SMT constraint solving. However, significant extensions are needed because the SMT constraints in the previous works are based on a non-deterministic semantics of regexes. Non-deterministic semantics is sound for membership but not for extraction, because which substrings are extracted depends on the deterministic behavior of actual regex engines. To address the issue, we propose a new SMT constraint generation method that respects the deterministic behavior of regex engines. For this, we first define a novel formal semantics of an actual regex engine as a deterministic big-step operational semantics, and use it as a basis to design the new SMT constraint generation method. The key idea to simulate the determinism in the formal semantics and the constraints is to consider continuations of regex matching and use them for disambiguation. We also propose two new search space pruning techniques called approximation-by-pure-regex and approximation-by-backreferences that make use of the extraction information in the examples.　We have implemented the synthesis and repair algorithm in a tool called R3 (Repairing Regex for extRaction) and evaluated it on 50 regexes that contain real-world extensions. Our evaluation shows the effectiveness of the algorithm and that our new pruning techniques substantially prune the search space.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {173},
numpages = {24},
keywords = {Program Repair, Programming by Example, Regular Expression}
}

@article{10.1145/3591288,
author = {Yoon, Yongho and Lee, Woosuk and Yi, Kwangkeun},
title = {Inductive Program Synthesis via Iterative Forward-Backward Abstract Interpretation},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591288},
doi = {10.1145/3591288},
abstract = {A key challenge in example-based program synthesis is the gigantic search space of programs. To address this challenge, various work proposed to use abstract interpretation to prune the search space. However, most of existing approaches have focused only on forward abstract interpretation, and thus cannot fully exploit the power of abstract interpretation. In this paper, we propose a novel approach to inductive program synthesis via iterative forward-backward abstract interpretation. The forward abstract interpretation computes possible outputs of a program given inputs, while the backward abstract interpretation computes possible inputs of a program given outputs. By iteratively performing the two abstract interpretations in an alternating fashion, we can effectively determine if any completion of each partial program as a candidate can satisfy the input-output examples. We apply our approach to a standard formulation, syntax-guided synthesis (SyGuS), thereby supporting a wide range of inductive synthesis tasks. We have implemented our approach and evaluated it on a set of benchmarks from the prior work. The experimental results show that our approach significantly outperforms the state-of-the-art approaches thanks to the sophisticated abstract interpretation techniques.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {174},
numpages = {25},
keywords = {Abstract Interpretation, Program Synthesis, Programming by Example}
}

@software{10.5281/zenodo.7816533,
author = {Yoon, Yongho and Lee, Woosuk and Yi, Kwangkeun},
title = {Artifact of Inductive Program Synthesis via Iterative Forward-Backward Abstract Interpretation},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7816533},
abstract = {
    <p>The artifacts include the main tool(Simba), the other baseline solvers(Duet, Probe), benchmarks and evaluation scripts.</p>

},
keywords = {Abstract Interpretation, Program Synthesis}
}

@article{10.1145/3591289,
author = {Eilers, Marco and Dardinier, Thibault and M\"{u}ller, Peter},
title = {CommCSL: Proving Information Flow Security for Concurrent Programs using Abstract Commutativity},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591289},
doi = {10.1145/3591289},
abstract = {Information flow security ensures that the secret data manipulated by a program does not influence its observable output. Proving information flow security is especially challenging for concurrent programs, where operations on secret data may influence the execution time of a thread and, thereby, the interleaving between different threads. Such internal timing channels may affect the observable outcome of a program even if an attacker does not observe execution times. Existing verification techniques for information flow security in concurrent programs attempt to prove that secret data does not influence the relative timing of threads. However, these techniques are often restrictive (for instance because they disallow branching on secret data) and make strong assumptions about the execution platform (ignoring caching, processor instructions with data-dependent runtime, and other common features that affect execution time).  

In this paper, we present a novel verification technique for secure information flow in concurrent programs that lifts these restrictions and does not make any assumptions about timing behavior. The key idea is to prove that all mutating operations performed on shared data commute, such that different thread interleavings do not influence its final value. Crucially, commutativity is required only for an abstraction of the shared data that contains the information that will be leaked to a public output. Abstract commutativity is satisfied by many more operations than standard commutativity, which makes our technique widely applicable.  

We formalize our technique in CommCSL, a relational concurrent separation logic with support for commutativity-based reasoning, and prove its soundness in Isabelle/HOL. We implemented CommCSL in HyperViper, an automated verifier based on the Viper verification infrastructure, and demonstrate its ability to verify challenging examples.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {175},
numpages = {26},
keywords = {Commutativity, concurrency, information flow, separation logic}
}

@software{10.5281/zenodo.7813862,
author = {Eilers, Marco and Dardinier, Thibault and M\"{u}ller, Peter},
title = {Artifact of paper "CommCSL: Proving Information Flow Security for Concurrent Programs using Abstract Commutativity"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7813862},
abstract = {
    <p>The artifact is a VirtualBox VM that contains the Isabelle/HOL formalization and soundness proof of CommCSL as well as the implementation of CommCSL in the tool HyperViper, and the evaluation presented in the paper.</p>

},
keywords = {Commutativity, concurrency, information flow security, separation logic}
}

@article{10.1145/3591290,
author = {Lew, Alexander K. and Ghavamizadeh, Matin and Rinard, Martin C. and Mansinghka, Vikash K.},
title = {Probabilistic Programming with Stochastic Probabilities},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591290},
doi = {10.1145/3591290},
abstract = {We present a new approach to the design and implementation of probabilistic programming languages (PPLs), based on the idea of stochastically estimating the probability density ratios necessary for probabilistic inference. By relaxing the usual PPL design constraint that these densities be computed exactly, we are able to eliminate many common restrictions in current PPLs, to deliver a language that, for the first time, simultaneously supports first-class constructs for marginalization and nested inference, unrestricted stochastic control flow, continuous and discrete sampling, and programmable inference with custom proposals. At the heart of our approach is a new technique for compiling these expressive probabilistic programs into randomized algorithms for unbiasedly estimating their densities and density reciprocals. We employ these stochastic probability estimators within modified Monte Carlo inference algorithms that are guaranteed to be sound despite their reliance on inexact estimates of density ratios. We establish the correctness of our compiler using logical relations over the semantics of λSP, a new core calculus for modeling and inference with stochastic probabilities. We also implement our approach in an open-source extension to Gen, called GenSP, and evaluate it on six challenging inference problems adapted from the modeling and inference literature. We find that: (1)  ‍can automate fast density estimators for programs with very expensive exact densities; (2) convergence of inference is mostly unaffected by the noise from these estimators; and (3) our sound-by-construction estimators are competitive with hand-coded density estimators, incurring only a small constant-factor overhead.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {176},
numpages = {25},
keywords = {approximate computing, probabilistic programming, semantics}
}

@article{10.1145/3591291,
author = {Tun\c{c}, H\"{u}nkar Can and Mathur, Umang and Pavlogiannis, Andreas and Viswanathan, Mahesh},
title = {Sound Dynamic Deadlock Prediction in Linear Time},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591291},
doi = {10.1145/3591291},
abstract = {Deadlocks are one of the most notorious concurrency bugs, and significant research has focused on detecting them efficiently. Dynamic predictive analyses work by observing concurrent executions, and reason about alternative interleavings that can witness concurrency bugs. Such techniques offer scalability and sound bug reports, and have emerged as an effective approach for concurrency bug detection, such as data races. Effective dynamic deadlock prediction, however, has proven a challenging task, as no deadlock predictor currently meets the requirements of soundness, high-precision, and efficiency.  

In this paper, we first formally establish that this tradeoff is unavoidable, by showing that (a) sound and complete deadlock prediction is intractable, in general, and (b) even the seemingly simpler task of determining the presence of potential deadlocks, which often serve as unsound witnesses for actual predictable deadlocks, is intractable. The main contribution of this work is a new class of predictable deadlocks, called sync(hronization)-preserving deadlocks. Informally, these are deadlocks that can be predicted by reordering the observed execution while preserving the relative order of conflicting critical sections. We present two algorithms for sound deadlock prediction based on this notion. Our first algorithm SPDOffline detects all sync-preserving deadlocks, with running time that is linear per abstract deadlock pattern, a novel notion also introduced in this work. Our second algorithm SPDOnline predicts all sync-preserving deadlocks that involve two threads in a strictly online fashion, runs in overall linear time, and is better suited for a runtime monitoring setting.  

We implemented both our algorithms and evaluated their ability to perform offline and online deadlock-prediction on a large dataset of standard benchmarks. Our results indicate that our new notion of sync-preserving deadlocks is highly effective, as (i) it can characterize the vast majority of deadlocks and (ii) it can be detected using an online, sound, complete and highly efficient algorithm.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {177},
numpages = {26},
keywords = {concurrency, predictive analyses, runtime analyses}
}

@software{10.5281/zenodo.7809600,
author = {Tun\c{c}, H\"{u}nkar Can and Mathur, Umang and Pavlogiannis, Andreas and Viswanathan, Mahesh},
title = {Artifact for Article "Sound Dynamic Deadlock Prediction in Linear Time"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7809600},
abstract = {
    <p>The artifact contains the source codes of the deadlock prediction tools developed in our paper. Moreover, the artifact contains the compared tools (partially), experimental data, and scripts that can reproduce the experimental evaluation performed in the paper. The artifact does not contain all the compared tools as we lack the necessary rights to redistribute certain tools.</p>

},
keywords = {concurrency, predictive analyses, runtime analyses}
}

@article{10.1145/3591292,
author = {Prinz, Jacob and Lampropoulos, Leonidas},
title = {Merging Inductive Relations},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591292},
doi = {10.1145/3591292},
abstract = {Inductive relations offer a powerful and expressive way of writing  
 program specifications while facilitating compositional reasoning.  
 Their widespread use by proof assistant users has made them a  
 particularly attractive target for proof engineering tools such as  
 QuickChick, a property-based testing tool for Coq which can  
 automatically derive generators for values satisfying an inductive  
 relation.  
 However, while such generators are generally efficient, there is  
 an infrequent yet seemingly inevitable situation where their  
 performance greatly degrades: when multiple inductive relations  
 constrain the same piece of data.  

 In this paper, we introduce an algorithm for merging two such  
 inductively defined properties that share an index. The algorithm  
 finds shared structure between the two relations, and creates a  
 single merged relation that is provably equivalent to the  
 conjunction of the two.  
 We demonstrate, through a series of case studies,  
 that the merged relations can improve the performance of automatic  
 generation by orders of magnitude, as well as simplify mechanized  
 proofs by getting rid of the need for nested induction and tedious  
 low-level book-keeping.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {178},
numpages = {20},
keywords = {QuickChick, inductive relations, merging}
}

@software{10.5281/zenodo.7709704,
author = {Prinz, Jacob and Lampropoulos, Leonidas},
title = {Reproduction Package for "Merging Inductive Relations"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7709704},
abstract = {
    <p>VM that allows for replicating all the experiments in the PLDI 2023 paper “Merging Inductive Relations”.</p>

},
keywords = {Merging Inductive Relations, Property-Based Testing, QuickChick}
}

@article{10.1145/3591293,
author = {Bertram, Noah and Levinson, Alex and Hsu, Justin},
title = {Cutting the Cake: A Language for Fair Division},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591293},
doi = {10.1145/3591293},
abstract = {The fair division literature in economics considers how to divide resources between multiple agents such that the allocation is envy-free: each agent receives their favorite piece. Researchers have developed a variety of fair division protocols for the most standard setting, where the agents want to split a single item, however, the protocols are highly intricate and the proofs of envy-freeness involve tedious case analysis.  

We propose Slice, a domain specific language for fair-division. Programs in our language can be converted to logical formulas encoding envy-freeness and other target properties. Then, the constraints can be dispatched to automated solvers. We prove that our constraint generation procedure is sound and complete. We also report on a prototype implementation of Slice, which we have used to automatically check envy-freeness for several protocols from the fair division literature.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {179},
numpages = {22},
keywords = {Fair division, automated verification}
}

@software{10.5281/zenodo.7814374,
author = {Bertram, Noah and Levinson, Alex and Hsu, Justin},
title = {Prototype implementation of Slice, appearing in "Cutting the Cake: A Language for Fair Division"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7814374},
abstract = {
    <p>This is a prototype implementation of the cake-cutting language, Slice. For more details, view the readme.</p>

},
keywords = {automatic verification, fair division}
}

@article{10.1145/3591294,
author = {Lecoeur, Bastien and Mohsin, Hasan and Donaldson, Alastair F.},
title = {Program Reconditioning: Avoiding Undefined Behaviour When Finding and Reducing Compiler Bugs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591294},
doi = {10.1145/3591294},
abstract = {We introduce program reconditioning, a method for allowing program generation and differential testing to be used to find miscompilation bugs, and test-case reduction to be used to simplify bug-triggering programs, even when (a) the programming language of interest features undefined behaviour (UB) and (b) no tools exist to detect and avoid this UB. We present two program generation tools based on our reconditioning idea: GLSLsmith for the OpenGL Shading Language (GLSL), a widely-used language for graphics programming, and WGSLsmith for the WebGPU Shading Language (WGSL), a new language for web-based graphics rendering. GLSL features many UBs, but unlike for languages such as C and C++ no tools exist to detect them automatically. While the WGSL language specification features very limited UB, early WGSL implementations do exhibit UB, for reasons of initial implementation simplicity, making it challenging to test them to quickly detect and eliminate unrelated miscompilation bugs. Thanks to reconditioning, we show that GLSLsmith and WGSLsmith allow differential testing and test-case reduction to be applied to compilers for GLSL and WGSL for the first time, despite the unavailability of UB detection techniques for these languages. Through a large testing campaign, we have found 24 and 33 bugs in GLSL and WGSL compilers, respectively. We present experiments showing that when reconditioning is disabled, compiler testing leads to a high rate of test programs that appear to trigger miscompilation bugs, but actually just feature UB. We also present a novel approach to managing floating-point roundoff error using reconditioning, implemented for both GLSL and WGSL.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {180},
numpages = {25},
keywords = {OpenGL, Randomised testing, WebGPU, compiler testing, test-case reduction, undefined behaviour}
}

@software{10.5281/zenodo.7819755,
author = {Lecoeur, Bastien and Mohsin, Hasan and Donaldson, Alastair F.},
title = {Artifact for "Program Reconditioning: Avoiding Undefined Behaviour When Finding and Reducing Compiler Bugs", PLDI 2023},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7819755},
abstract = {
    <p>The artifact includes runnable versions of the GLSLsmith and WGSLsmith tools (both source code and binary distributions), together with instructions showing how to use them to find and reduce compiler bugs, plus data sets related to controlled experiments described in the paper.</p>

},
keywords = {compiler testing, OpenGL, Randomised testing, test-case reduction, undefined behaviour, WebGPU}
}

@article{10.1145/3591295,
author = {Livinskii, Vsevolod and Babokin, Dmitry and Regehr, John},
title = {Fuzzing Loop Optimizations in Compilers for C++ and Data-Parallel Languages},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591295},
doi = {10.1145/3591295},
abstract = {Compilers are part of the foundation upon which software systems are built; they need to be as correct as possible. This paper is about stress-testing loop optimizers; it presents a major reimplementation of Yet Another Random Program Generator (YARPGen), an open-source generative compiler fuzzer. This new version has found 122 bugs, both in compilers for data-parallel languages, such as the Intel® Implicit SPMD Program Compiler and the Intel® oneAPI DPC++ compiler, and in C++ compilers such as GCC and Clang/LLVM. The first main contribution of our work is a novel method for statically avoiding undefined behavior when generating loops; the resulting programs conform to the relevant language standard, enabling automated testing. The second main contribution is a collection of mechanisms for increasing the diversity of generated loop code; in our evaluation, we demonstrate that these make it possible to trigger loop optimizations significantly more often, providing opportunities to discover bugs in the optimizers.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {181},
numpages = {22},
keywords = {YARPGen, automated testing, compiler defect, compiler testing, random program generation, random testing}
}

@article{10.1145/3591296,
author = {Meyer, Roland and Wies, Thomas and Wolff, Sebastian},
title = {Embedding Hindsight Reasoning in Separation Logic},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591296},
doi = {10.1145/3591296},
abstract = {Automatically proving linearizability of concurrent data structures remains a key challenge for verification. We present temporal interpolation as a new proof principle to guide automated proof search using hindsight arguments within concurrent separation logic. Temporal interpolation offers an easy-to-automate alternative to prophecy variables and has the advantage of structuring proofs into easy-to-discharge hypotheses. Additionally, we advance hindsight theory by integrating it into a program logic, bringing formal rigor and complementary proof machinery. We substantiate the usefulness of temporal interpolation by implementing it in a tool and using it to automatically verify the Logical Ordering tree. The proof is challenging due to future-dependent linearization points and complex structure overlays. It is the first formal proof of this data structure. Interestingly, our formalization revealed an unknown bug and an existing informal proof as erroneous.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {182},
numpages = {24},
keywords = {Hindsight, Linearizability, Logical Ordering Tree}
}

@software{10.5281/zenodo.7829982,
author = {Meyer, Roland and Wies, Thomas and Wolff, Sebastian},
title = {Artifact for "Embedding Hindsight Reasoning in Separation Logic"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7829982},
abstract = {
    <p>The artifact demonstrates that the implementation from the paper “Embedding Hindsight Reasoning in Separation Logic” [PLDI’23], an extension of the PLANKTON tool, (1) can automatically verify the Logical Ordering tree, and (2) compares the extension with the original version of PLANKTON in terms of performance and proof capabilities.</p>

},
keywords = {Automated reasoning, Hindsight, Hoare logic, Linearizability, Logical Ordering Tree, Program verification, Programming logic, Separation logic}
}

@article{10.1145/3591297,
author = {Lee, Sung-Hwan and Cho, Minki and Margalit, Roy and Hur, Chung-Kil and Lahav, Ori},
title = {Putting Weak Memory in Order via a Promising Intermediate Representation},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591297},
doi = {10.1145/3591297},
abstract = {We investigate the problem of developing an "in-order" shared-memory concurrency model for languages like C and C++, which executes instructions following their program order, and is thus more amenable to reasoning and verification compared to recent complex proposals with out-of-order execution. We demonstrate that it is possible to fully support non-atomic accesses in an in-order model in a way that validates all compiler optimizations that are performed in single-threaded code (including irrelevant load introduction). The key to doing so is to utilize the distinction between a source model (with catch-fire semantics) and an intermediate representation (IR) model (with undefined value for racy reads) and formally establish the soundness of mapping from source to IR. As for relaxed atomic accesses, an in-order model must forbid load-store reordering. We discuss the rather limited performance impact of this fact and present a pragmatic approach to this problem, which, in the long term, requires a new kind of hardware store instructions for implementing relaxed stores. The source and IR semantics proposed in this paper are based on recent versions of the promising semantics, and the correctness proofs of the mappings from the source to the IR and from the IR to Armv8 are mechanized in Coq. This work is the first to formally relate an in-order source model and an out-of-order IR model with the goal of having an in-order source semantics without any performance overhead for non-atomics.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {183},
numpages = {24},
keywords = {Compiler Optimizations, Intermediate Representation, Operational Semantics, Relaxed Memory Concurrency}
}

@article{10.1145/3591298,
author = {Wang, Jingbo and Gupta, Aarti and Wang, Chao},
title = {Synthesizing MILP Constraints for Efficient and Robust Optimization},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591298},
doi = {10.1145/3591298},
abstract = {While mixed integer linear programming (MILP) solvers are routinely used to solve a wide range of important science and engineering problems, it remains a challenging task for end users to write correct and efficient MILP constraints, especially for problems specified using the inherently non-linear Boolean logic operations. To overcome this challenge, we propose a syntax guided synthesis (SyGuS) method capable of generating high-quality MILP constraints from the specifications expressed using arbitrary combinations of Boolean logic operations. At the center of our method is an extensible domain specification language (DSL) whose expressiveness may be improved by adding new integer variables as decision variables, together with an iterative procedure for synthesizing linear constraints from non-linear Boolean logic operations using these integer variables. To make the synthesis method efficient, we also propose an over-approximation technique for soundly proving the correctness of the synthesized linear constraints, and an under-approximation technique for safely pruning away the incorrect constraints. We have implemented and evaluated the method on a wide range of benchmark specifications from statistics, machine learning, and data science applications. The experimental results show that the method is efficient in handling these benchmarks, and the quality of the synthesized MILP constraints is close to, or higher than, that of manually-written constraints in terms of both compactness and solving time.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {184},
numpages = {24},
keywords = {Data Science, Machine Learning, Statistics, Syntax Guided Synthesis}
}

@article{10.1145/3591299,
author = {Ugare, Shubham and Banerjee, Debangshu and Misailovic, Sasa and Singh, Gagandeep},
title = {Incremental Verification of Neural Networks},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591299},
doi = {10.1145/3591299},
abstract = {Complete verification of deep neural networks (DNNs) can exactly determine whether the DNN satisfies a desired trustworthy property (e.g., robustness, fairness) on an infinite set of inputs or not. Despite the tremendous progress to improve the scalability of complete verifiers over the years on individual DNNs, they are inherently inefficient when a deployed DNN is updated to improve its inference speed or accuracy. The inefficiency is because the expensive verifier needs to be run from scratch on the updated DNN. To improve efficiency, we propose a new, general framework for incremental and complete DNN verification based on the design of novel theory, data structure, and algorithms. Our contributions implemented in a tool named IVAN yield an overall geometric mean speedup of 2.4x for verifying challenging MNIST and CIFAR10 classifiers and a geometric mean speedup of 3.8x for the ACAS-XU classifiers over the state-of-the-art baselines.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {185},
numpages = {26},
keywords = {Deep Neural Networks, Robustness, Verification}
}

@software{10.5281/zenodo.7812282,
author = {Ugare, Shubham and Banerjee, Debangshu and Misailovic, Sasa and Singh, Gagandeep},
title = {Incremental Verification of Neural Networks},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7812282},
abstract = {
    <p>Complete verification of deep neural networks (DNNs) can exactly determine whether the DNN satisfies a desired trustworthy property (e.g., robustness, fairness) on an infinite set of inputs or not. Despite the tremendous progress to improve the scalability of complete verifiers over the years on individual DNNs, they are inherently inefficient when a deployed DNN is updated to improve its inference speed or accuracy. The inefficiency is because the expensive verifier needs to be run from scratch on the updated DNN. To improve efficiency, we propose a new, general framework for incremental and complete DNN verification based on the design of novel theory, data structure, and algorithms. Our contributions implemented in a tool named IVAN yield an overall geometric mean speedup of 2.4x for verifying challenging MNIST and CIFAR10 classifiers and a geometric mean speedup of 3.8x for the ACAS-XU classifiers over the state-of-the-art baselines.</p>

},
keywords = {Neural Networks, Verification}
}

@article{10.1145/3591300,
author = {Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin},
title = {Prompting Is Programming: A Query Language for Large Language Models},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591300},
doi = {10.1145/3591300},
abstract = {Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation.  
On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.  

Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics.  

To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model.  

We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85\% cost savings).},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {186},
numpages = {24},
keywords = {language model programming, prompt programming}
}

@software{10.5281/zenodo.7711823,
author = {Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin},
title = {LMQL as described in Prompting Is Programming: A Query Language for Large Language Models},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7711823},
abstract = {
    <p>LMQL is a query language for large language models (LLMs). It facilitates LLM interaction by combining the benefits of natural language prompting with the expressiveness of Python. With only a few lines of LMQL code, users can express advanced, multi-part and tool-augmented LM queries, which then are optimized by the LMQL runtime to run efficiently as part of the LM decoding loop.</p>
<p>An up to date version can be found at https://github.com/eth-sri/lmql</p>

},
keywords = {language model programming, prompt programming}
}

@article{10.1145/3591301,
author = {Yuviler, Tom and Drachsler-Cohen, Dana},
title = {One Pixel Adversarial Attacks via Sketched Programs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591301},
doi = {10.1145/3591301},
abstract = {Neural networks are successful in various tasks but are also susceptible to adversarial examples. An adversarial example is generated by adding a small perturbation to a correctly-classified input with the goal of causing a network classifier to misclassify. In one pixel attacks, an attacker aims to fool an image classifier by modifying a single pixel. This setting is challenging for two reasons: the perturbation region is very small and the perturbation is not differentiable. To cope, one pixel attacks iteratively generate candidate adversarial examples and submit them to the network until finding a successful candidate. However, existing works require a very large number of queries, which is infeasible in many practical settings, where the attacker is limited to a few thousand queries to the network. We propose a novel approach for computing one pixel attacks. The key idea is to leverage program synthesis and identify an expressive program sketch that enables to compute adversarial examples using significantly fewer queries. We introduce OPPSLA, a synthesizer that, given a classifier and a training set, instantiates the sketch with customized conditions over the input’s pixels and the classifier’s output. OPPSLA employs a stochastic search, inspired by the Metropolis-Hastings algorithm, that synthesizes typed expressions enabling minimization of the number of queries to the classifier. We further show how to extend OPPSLA to compute few pixel attacks minimizing the number of perturbed pixels. We evaluate OPPSLA on several deep networks for CIFAR-10 and ImageNet. We show that OPPSLA obtains a state-of-the-art success rate, often with an order of magnitude fewer queries than existing attacks. We further show that OPPSLA’s programs are transferable to other classifiers, unlike existing one pixel attacks, which run from scratch on every classifier and input.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {187},
numpages = {25},
keywords = {adversarial attack, computer vision, program synthesis}
}

@article{10.1145/3591302,
author = {Wilkinson, Lucas and Cheshmi, Kazem and Dehnavi, Maryam Mehri},
title = {Register Tiling for Unstructured Sparsity in Neural Network Inference},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591302},
doi = {10.1145/3591302},
abstract = {Unstructured sparse neural networks are an important class of machine learning (ML) models, as they compact model size and reduce floating point operations. The execution time of these models is frequently dominated by the sparse matrix multiplication (SpMM) kernel, C=A\texttimes{} B, where A is a sparse matrix, and B and C are dense matrices. The unstructured sparsity pattern of matrices in pruned machine learning models along with their sparsity ratio has rendered useless the large class of libraries and systems that optimize sparse matrix multiplications. Reusing registers is particularly difficult because accesses to memory locations should be known statically. This paper proposes Sparse Register Tiling, a new technique composed of an unroll-and-sparse-jam transformation followed by data compression that is specifically tailored to sparsity patterns in ML matrices. Unroll-and-sparse-jam uses sparsity information to jam the code while improving register reuse. Sparse register tiling is evaluated across 2396 weight matrices from transformer and convolutional models with a sparsity range of 60-95\% and provides an average speedup of 1.72\texttimes{} and 2.65\texttimes{} over MKL SpMM and dense matrix multiplication, respectively, on a multicore CPU processor. It also provides an end-to-end speedup of 2.12\texttimes{} for MobileNetV1 with 70\% sparsity on an ARM processor commonly used in edge devices.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {188},
numpages = {26},
keywords = {Loop Tiling, Pruned Neural Networks, Sparse Matrix}
}

@software{10.5281/zenodo.7832346,
author = {Wilkinson, Lucas and Cheshmi, Kazem and Dehnavi, Maryam Mehri},
title = {Register Tiling for Unstructured Sparsity in Neural Network Inference Artifact},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7832346},
abstract = {
    <p>This is the code for the “Register Tiling for Unstructured Sparsity in Neural Network Inference” paper in PLDI 2023, please see the README in size <code>artifact_src.tgz</code> for instructions on how to use the code. Please see https://github.com/SpRegTiling/sparse-register-tiling for the latest version of the code.</p>

},
keywords = {Matrix Multiplication, Pruned Neural Networks, Register Tiling, Sparse Matrix, SpMM}
}

