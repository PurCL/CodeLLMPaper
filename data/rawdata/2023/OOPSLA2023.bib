@article{10.1145/3622797,
author = {Yi, Pu (Luke) and Achour, Sara},
title = {Hardware-Aware Static Optimization of Hyperdimensional Computations},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622797},
doi = {10.1145/3622797},
abstract = {Binary spatter code (BSC)-based hyperdimensional computing (HDC) is a highly error-resilient approximate computational paradigm suited for error-prone, emerging hardware platforms. In BSC HDC, the basic datatype is a hypervector, a typically large binary vector, where the size of the hypervector has a significant impact on the fidelity and resource usage of the computation. Typically, the hypervector size is dynamically tuned to deliver the desired accuracy; this process is time-consuming and often produces hypervector sizes that lack accuracy guarantees and produce poor results when reused for very similar workloads. We present Heim, a hardware-aware static analysis and optimization framework for BSC HD computations. Heim analytically derives the minimum hypervector size that minimizes resource usage and meets the target accuracy requirement. Heim guarantees the optimized computation converges to the user-provided accuracy target on expectation, even in the presence of hardware error. Heim deploys a novel static analysis procedure that unifies theoretical results from the neuroscience community to systematically optimize HD computations. We evaluate Heim against dynamic tuning-based optimization on 25 benchmark data structures. Given a 99\% accuracy requirement, Heim-optimized computations achieve a 99.2\%-100.0\% median accuracy, up to 49.5\% higher than dynamic tuning-based optimization, while achieving 1.15x-7.14x reductions in hypervector size compared to HD computations that achieve comparable query accuracy and finding parametrizations 30.0x-100167.4x faster than dynamic tuning-based approaches. We also use Heim to systematically evaluate the performance benefits of using analog CAMs and multiple-bit-per-cell ReRAM over conventional hardware, while maintaining iso-accuracy – for both emerging technologies, we find usages where the emerging hardware imparts significant benefits.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {222},
numpages = {30},
keywords = {unconventional computing, program optimization, emerging hardware technologies}
}

@software{10.5281/zenodo.8329813,
author = {Yi, Pu (Luke) and Achour, Sara},
title = {Artifact for the OOPSLA 2023 Article "Hardware-Aware Static Optimization of Hyperdimensional Computations"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8329813},
abstract = {
    <p>This is the artifact accompanying our study of hardware-aware static optimization of hyperdimensional computation, accepted for presentation at the 38th ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA) 2023. The artifact contains the implementation of our system, scripts to reproduce the evaluation results shown in the paper, and detailed instructions for setting up the environment and step-by-step reproduction of the results.</p>

},
keywords = {emerging hardware technologies, program optimization, unconventional computing}
}

@article{10.1145/3622798,
author = {Hance, Travis and Howell, Jon and Padon, Oded and Parno, Bryan},
title = {Leaf: Modularity for Temporary Sharing in Separation Logic},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622798},
doi = {10.1145/3622798},
abstract = {In concurrent verification, separation logic provides a strong story for handling both resources that are owned exclusively and resources that are shared persistently (i.e., forever). However, the situation is more complicated for temporarily shared state, where state might be shared and then later reclaimed as exclusive. We believe that a framework for temporarily-shared state should meet two key goals not adequately met by existing techniques. One, it should allow and encourage users to verify new sharing strategies. Two, it should provide an abstraction where users manipulate shared state in a way agnostic to the means with which it is shared.  

We present Leaf, a library in the Iris separation logic which accomplishes both of these goals by introducing a novel operator, which we call guarding, that allows one proposition to represent a shared version of another. We demonstrate that Leaf meets these two goals through a modular case study: we verify a reader-writer lock that supports shared state, and a hash table built on top of it that uses shared state.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {223},
numpages = {28},
keywords = {verification, reader-writer lock, read-sharing, fractional permissions, counting permissions, concurrent separation logic}
}

@software{10.5281/zenodo.8327489,
author = {Hance, Travis and Howell, Jon and Padon, Oded and Parno, Bryan},
title = {Leaf: Modularity for Temporary Sharing in Separation Logic (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8327489},
abstract = {
    <p>This contains the Coq development validating the core technical contributions of the paper <em>Leaf: Modularity for Temporary Sharing in Separation Logic</em>. It contains:</p>
<ul>
<li>Definitions of Leaf concepts and proofs of Leaf inference rules</li>
<li>Instantiation of Leaf for a simple heap-based language with atomic heap operations</li>
<li>Derivation of fractional permissions and counting permissions within Leaf</li>
<li>The reader-writer lock example</li>
<li>The hash table example</li>
</ul>

},
keywords = {Coq, Iris, separation logic}
}

@article{10.1145/3622799,
author = {Gourdin, L\'{e}o and Bonneau, Benjamin and Boulm\'{e}, Sylvain and Monniaux, David and B\'{e}rard, Alexandre},
title = {Formally Verifying Optimizations with Block Simulations},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622799},
doi = {10.1145/3622799},
abstract = {CompCert (ACM Software System Award 2021) is the first industrial-strength compiler with a mechanically checked proof of correctness. Yet, CompCert remains a moderately optimizing C compiler. Indeed, some optimizations of “gcc ‍-O1” such as Lazy Code Motion (LCM) or Strength Reduction (SR) were still missing: developing these efficient optimizations together with their formal proofs remained a challenge. Cyril Six et al. have developed efficient formally verified translation validators for certifying the results of superblock schedulers and peephole optimizations. We revisit and generalize their approach into a framework (integrated into CompCert) able to validate many more optimizations: an enhanced superblock scheduler, but also Dead Code Elimination (DCE), Constant Propagation (CP), and more noticeably, LCM and SR. In contrast to other approaches to translation validation, we co-design our untrusted optimizations and their validators. Our optimizations provide hints, in the forms of invariants or CFG morphisms, that help keep the formally verified validators both simple and efficient. Such designs seem applicable beyond CompCert.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {224},
numpages = {30},
keywords = {the Coq proof assistant, Translation validation, Symbolic execution, Formal verification of compiler optimizations}
}

@software{10.5281/zenodo.8314677,
author = {Gourdin, L\'{e}o and Bonneau, Benjamin and Boulm\'{e}, Sylvain and Monniaux, David and B\'{e}rard, Alexandre},
title = {Formally Verifying Optimizations with Block Simulations},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8314677},
abstract = {
    <p>The goal of this artifact is to show that the optimizations presented in our paper are effectively applied, and proved thanks to our translation validation by symbolic execution mechanism. Moreover, we demonstrate that even on large or randomly generated tests, the validation does not produce any false alarm. The artifact also provides a means of reproducing runtime benchmarks (performance in number of cycles), although this requires specific hardware.</p>

},
keywords = {Formal verification of compiler optimizations, Symbolic Execution, The Coq proof assistant, Translation validation}
}

@article{10.1145/3622800,
author = {Sun, Yican and Peng, Xuanyu and Xiong, Yingfei},
title = {Synthesizing Efficient Memoization Algorithms},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622800},
doi = {10.1145/3622800},
abstract = {In this paper, we propose an automated approach to finding correct and efficient memoization algorithms from a given declarative specification. This problem has two major challenges: (i) a memoization algorithm is too large to be handled by conventional program synthesizers; (ii) we need to guarantee the efficiency of the memoization algorithm. To address this challenge, we structure the synthesis of memoization algorithms by introducing the local objective function and the memoization partition function and reduce the synthesis task to two smaller independent program synthesis tasks. Moreover, the number of distinct outputs of the function synthesized in the second synthesis task also decides the efficiency of the synthesized memoization algorithm, and we only need to minimize the number of different output values of the synthesized function. However, the generated synthesis task is still too complex for existing synthesizers. Thus, we propose a novel synthesis algorithm that combines the deductive and inductive methods to solve these tasks. To evaluate our algorithm, we collect 42 real-world benchmarks from Leetcode, the National Olympiad in Informatics in Provinces-Junior (a national-wide algorithmic programming contest in China), and previous approaches. Our approach successfully synhesizes 39/42 problems in a reasonable time, outperforming the baselines.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {225},
numpages = {27},
keywords = {Program Synthesis, Memoization Algorithms}
}

@software{10.5281/zenodo.8325410,
author = {Sun, Yican and Peng, Xuanyu and Xiong, Yingfei},
title = {Artifact for "Synthesizing Efficient Memoization Algorithms"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8325410},
abstract = {
    <p>This is the artifact for the OOPLSA2023 paper “Synthesizing Efficient Memoization Algorithms”, including: - the docker image of our tool, and the reproduction package. - the readme file.</p>

},
keywords = {docker file}
}

@article{10.1145/3622801,
author = {Paulino, Herv\'{e} and Almeida Matos, Ana and Cederquist, Jan and Giunti, Marco and Matos, Jo\~{a}o and Ravara, Ant\'{o}nio},
title = {AtomiS: Data-Centric Synchronization Made Practical},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622801},
doi = {10.1145/3622801},
abstract = {Data-Centric Synchronization (DCS) shifts the reasoning about concurrency restrictions from control structures to data declaration. It is a high-level declarative approach that abstracts away from the actual concurrency control mechanism(s) in use. Despite its advantages, the practical use of DCS is hindered by the fact that it may require many annotations and/or multiple implementations of the same method to cope with differently qualified parameters.  

To overcome these limitations, in this paper we present AtomiS, a new DCS approach that requires only qualifying types of parameters and return values in interface definitions, and of fields in class definitions. The latter may also be abstracted away in type parameters, rendering class implementations virtually annotation-free. From this high level specification, a static analysis infers the atomicity constraints that are local to each method, considering valid only the method variants that are consistent with the specification, and performs code generation for all valid variants of each method. The generated code is then the target for automatic injection of concurrency control primitives that are responsible for ensuring the absence of data-races,  
atomicity-violations and deadlocks.  

We provide a Java implementation and showcase the applicability of AtomiS in real-life code. For the  
benchmarks analysed, AtomiS requires fewer annotations than the original number of regions requiring locks, as well as fewer annotations than Atomic Sets (a reference DCS proposal).},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {226},
numpages = {30},
keywords = {Programming Model, Inference and Synthesis, Data-Centric, Concurrency}
}

@article{10.1145/3622802,
author = {Renaux, Thierry and Van den Vonder, Sam and De Meuter, Wolfgang},
title = {Secure RDTs: Enforcing Access Control Policies for Offline Available JSON Data},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622802},
doi = {10.1145/3622802},
abstract = {Replicated Data Types (RDTs) are a type of data structure that can be replicated over a network, where each replica can be kept (eventually) consistent with the other replicas. They are used in applications with intermittent network connectivity, since local (offline) edits can later be merged with the other replicas. Applications that want to use RDTs often have an inherent security component that restricts data access for certain clients. However, access control for RDTs is difficult to enforce for clients that are not running within a secure environment, e.g., web applications where the client-side software can be freely tampered with. In essence, an application cannot prevent a client from reading data which they are not supposed to read, and any malicious changes will also affect well-behaved clients.  

This paper proposes Secure RDTs (SRDTs), a data type that specifies role-based access control for offline-available JSON data. In brief, a trusted application server specifies a security policy based on roles with read and write privileges for certain fields of an SRDT. The server enforces read privileges by projecting the data and security policy to omit any non-readable fields for the user's given role, and it acts as an intermediary to enforce write privileges. The approach is presented as an operational semantics engineered in PLT Redex, which is validated by formal proofs and randomised testing in Redex to ensure that the formal specification is secure.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {227},
numpages = {27},
keywords = {security, role-based access control, replicated data types, conflict-free replicated data types}
}

@software{10.5281/zenodo.8310917,
author = {Renaux, Thierry and Van den Vonder, Sam and De Meuter, Wolfgang},
title = {Secure RDTs: Enforcing Access Control Policies for Offline Available JSON Data (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8310917},
abstract = {
    <p>This is an artifact associated with a paper called “Secure RDTs: Enforcing Access Control Policies for Offline Available JSON Data”. The paper associated with this artifact describes the SRDT, a secure replicated data type. To specify exactly how SRDTs work and to verify that they are secure, the paper uses a formal specification implemented in Redex, a library in the Racket language to specify executable formal semantics. The purpose of this artifact is to guide the reader on how to interact with the formal semantics, such that they can explore exactly how SRDTs work, are able to verify the claims of the paper, and are able to reproduce SRDTs for other systems.</p>

},
keywords = {conflict-free replicated data types, racket, redex, replicated data types, security}
}

@article{10.1145/3622803,
author = {Mururu, Girish and Khan, Sharjeel and Chatterjee, Bodhisatwa and Chen, Chao and Porter, Chris and Gavrilovska, Ada and Pande, Santosh},
title = {Beacons: An End-to-End Compiler Framework for Predicting and Utilizing Dynamic Loop Characteristics},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622803},
doi = {10.1145/3622803},
abstract = {Efficient management of shared resources is a critical problem in high-performance computing (HPC) environments. Existing workload management systems often promote non-sharing of resources among different co-executing applications to achieve performance isolation. Such schemes lead to poor resource utilization and suboptimal process throughput, adversely affecting user productivity. Tackling this problem in a scalable fashion is extremely challenging, since it requires the workload scheduler to possess an in-depth knowledge about various application resource requirements and runtime phases at fine granularities within individual applications. In this work, we show that applications’ resource requirements and execution phase behaviour can be captured in a scalable and lightweight manner at runtime by estimating important program artifacts termed as “dynamic loop characteristics”. Specifically, we propose a solution to the problem of efficient workload scheduling by designing a compiler and runtime cooperative framework that leverages novel loop-based compiler analysis for resource allocation. We present Beacons Framework, an end-to-end compiler and scheduling framework, that estimates dynamic loop characteristics, encapsulates them in compiler-instrumented beacons in an application, and broadcasts them during application runtime, for proactive workload scheduling. We focus on estimating four important loop characteristics: loop trip-count, loop timing, loop memory footprint, and loop data-reuse behaviour, through a combination of compiler analysis and machine learning. The novelty of the Beacons Framework also lies in its ability to tackle irregular loops that exhibit complex control flow with indeterminate loop bounds involving structure fields, aliased variables and function calls, which are highly prevalent in modern workloads. At the backend, Beacons Framework entails a proactive workload scheduler that leverages the runtime information to orchestrate aggressive process co-locations, for maximizing resource concurrency, without causing cache thrashing. Our results show that Beacons Framework can predict different loop characteristics with an accuracy of 85\% to 95\% on average, and the proactive scheduler obtains an average throughput improvement of 1.9x (up to 3.2x) over the state-of-the-art schedulers on an Amazon Graviton2 machine on consolidated workloads involving 1000-10000 co-executing processes, across 51 benchmarks.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {228},
numpages = {31},
keywords = {Throughput Scheduling, Proactive Scheduling, ML-based Static Analysis, Loop Trip Count Analysis, Loop Timing Analysis, Loop Memory Analysis, Compiler-Guided Scheduling}
}

@software{10.5281/zenodo.8153210,
author = {Mururu, Girish and Khan, Sharjeel and Chatterjee, Bodhisatwa and Chen, Chao and Porter, Chris and Gavrilovska, Ada and Pande, Santosh},
title = {Artifact for Paper "Beacons: An End-to-End Compiler Framework for Predicting and Utilizing Dynamic Loop Characteristics"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8153210},
abstract = {
    <p>This artifact captures the source code of our ``beacons framework,’’ along with the instructions and benchmarks for building the software and reproducing the main results of our paper.</p>
<p>The beacons framework is a collection of several components, including: 1. Compiler passes 2. Machine learning scripts 3. Runtime library 4. Scheduler The goal is to achieve efficient workload scheduling by designing a cooperative compiler-runtime framework which leverages novel loop-based compiler analysis and learned models for resource allocation.</p>

},
keywords = {Compiler-Guided Scheduling, Loop Memory Analysis, Loop Timing Analysis, Loop Trip Count Analysis, ML-based Static Analysis, Proactive Scheduling, Throughput Scheduling}
}

@article{10.1145/3622804,
author = {Ghorbani, Mahdi and Huot, Mathieu and Hashemian, Shideh and Shaikhha, Amir},
title = {Compiling Structured Tensor Algebra},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622804},
doi = {10.1145/3622804},
abstract = {Tensor algebra is essential for data-intensive workloads in various computational domains. Computational scientists face a trade-off between the specialization degree provided by dense tensor algebra and the algorithmic efficiency that leverages the structure provided by sparse tensors. This paper presents StructTensor, a framework that symbolically computes structure at compilation time. This is enabled by Structured Tensor Unified Representation (STUR), an intermediate language that can capture tensor computations as well as their sparsity and redundancy structures. Through a mathematical view of lossless tensor computations, we show that our symbolic structure computation and the related optimizations are sound. Finally, for different tensor computation workloads and structures, we experimentally show how capturing the symbolic structure can result in outperforming state-of-the-art frameworks for both dense and sparse tensor algebra.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {229},
numpages = {30},
keywords = {Tensor algebra, Structured tensors, Program synthesis, Program analysis, Code generation}
}

@article{10.1145/3622805,
author = {Chen, Qinlin and Zhang, Nairen and Wang, Jinpeng and Tan, Tian and Xu, Chang and Ma, Xiaoxing and Li, Yue},
title = {The Essence of Verilog: A Tractable and Tested Operational Semantics for Verilog},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622805},
doi = {10.1145/3622805},
abstract = {With the increasing need to apply modern software techniques to hardware design, Verilog, the most popular Hardware Description Language (HDL), plays an infrastructure role. However, Verilog has several semantic pitfalls that often confuse software and hardware developers. Although prior research on formal semantics for Verilog exists, it is not comprehensive and has not fully addressed these issues. In this work, we present a novel scheme inspired by previous work on defining core languages for software languages like JavaScript and Python. Specifically, we define the formal semantics of Verilog using a core language called λV, which captures the essence of Verilog using as few language structures as possible. λV not only covers the most complete set of language features to date, but also addresses the aforementioned pitfalls. We implemented λV with about 27,000 lines of Java code, and comprehensively tested its totality and conformance with Verilog. As a reliable reference semantics, λV can detect semantic bugs in real-world Verilog simulators and expose ambiguities in Verilog’s standard specification. Moreover, as a useful core language, λV has the potential to facilitate the development of tools such as a state-space explorer and a concolic execution tool for Verilog.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {230},
numpages = {30},
keywords = {Verilog, Semantics, Hardware Description Languages, Core Languages}
}

@software{10.5281/zenodo.8320642,
author = {Chen, Qinlin and Zhang, Nairen and Wang, Jinpeng and Tan, Tian and Xu, Chang and Ma, Xiaoxing and Li, Yue},
title = {The Essence of Verilog: A Tractable and Tested Operational Semantics for Verilog (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8320642},
abstract = {
    <p>This is the artifact of the paper “The Essence of Verilog: A Tractable and Tested Operational Semantics for Verilog”. It provides a Java implementation of <span class="math inline"><em>λ</em><sub><em>V</em></sub></span>, which is a core language of Verilog. The implementation includes an <span class="math inline"><em>λ</em><sub><em>V</em></sub></span> interpreter based on its formal semantics and a frontend that converts Verilog code into <span class="math inline"><em>λ</em><sub><em>V</em></sub></span>. Furthermore, this artifact offers an evaluation environment that allows for the reproduction of the results presented in our paper. For more detailed instructions on how to run our <span class="math inline"><em>λ</em><sub><em>V</em></sub></span> implementation and reproduce the results from our paper, please refer to the README.pdf included in the artifact. You can download the artifact from the following URL: https://doi.org/10.5281/zenodo.8320642.</p>

},
keywords = {Core Languages, Hardware Description Languages, Semantics, Verilog}
}

@article{10.1145/3622806,
author = {Wan, Chengcheng and Liu, Yuhan and Du, Kuntai and Hoffmann, Henry and Jiang, Junchen and Maire, Michael and Lu, Shan},
title = {Run-Time Prevention of Software Integration Failures of Machine Learning APIs},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622806},
doi = {10.1145/3622806},
abstract = {Due to the under-specified interfaces, developers face challenges in correctly integrating machine learning (ML) APIs in software. Even when the ML API and the software are well designed on their own, the resulting application misbehaves when the API output is incompatible with the software. It is desirable to have an adapter that converts ML API output at runtime to better fit the software need and prevent integration failures.  
In this paper, we conduct an empirical study to understand ML API integration problems in real-world applications. Guided by this study, we present SmartGear, a tool that automatically detects and converts mismatching or incorrect ML API output at run time, serving as a middle layer between ML API and software. Our evaluation on a variety of open-source applications shows that SmartGear detects 70\% incompatible API outputs and prevents 67\% potential integration failures, outperforming alternative solutions.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {231},
numpages = {28},
keywords = {software integration failure, run-time patching, machine learning API}
}

@article{10.1145/3622807,
author = {Conrado, Giovanna Kobus and Goharshady, Amir Kafshdar and Lam, Chun Kit},
title = {The Bounded Pathwidth of Control-Flow Graphs},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622807},
doi = {10.1145/3622807},
abstract = {Pathwidth and treewidth are standard and well-studied graph sparsity parameters which intuitively model the degree to which a given graph resembles a path or a tree, respectively. It is well-known that the control-flow graphs of structured goto-free programs have a tree-like shape and bounded treewidth. This fact has been exploited to design considerably more efficient algorithms for a wide variety of static analysis and compiler optimization problems, such as register allocation, µ-calculus model-checking and parity games, data-flow analysis, cache management, and liftetime-optimal redundancy elimination. However, there is no bound in the literature for the pathwidth of programs, except the general inequality that the pathwidth of a graph is at most O(lgn) times its treewidth, where n is the number of vertices of the graph. In this work, we prove that control-flow graphs of structured programs have bounded pathwidth and provide a linear-time algorithm to obtain a path decomposition of small width. Specifically, we establish a bound of 2 · d on the pathwidth of programs with nesting depth d. Since real-world programs have small nesting depth, they also have bounded pathwidth. This is significant for a number of reasons: (i) ‍pathwidth is a strictly stronger parameter than treewidth, i.e. ‍any graph family with bounded pathwidth has bounded treewidth, but the converse does not hold; (ii) ‍any algorithm that is designed with treewidth in mind can be applied to bounded-pathwidth graphs with no change; (iii) ‍there are problems that are fixed-parameter tractable with respect to pathwidth but not treewidth; (iv) ‍verification algorithms that are designed based on treewidth would become significantly faster when using pathwidth as the parameter; and (v) ‍it is easier to design algorithms based on bounded pathwidth since one does not have to consider the often-challenging case of merge nodes in treewidth-based dynamic programming. Thus, we invite the static analysis and compiler optimization communities to adopt pathwidth as their parameter of choice instead of, or in addition to, treewidth. Intuitively, control-flow graphs are not only tree-like, but also path-like and one can obtain simpler and more scalable algorithms by relying on path-likeness instead of tree-likeness. As a motivating example, we provide a simpler and more efficient algorithm for spill-free register allocation using bounded pathwidth instead of treewidth. Our algorithm reduces the runtime from O(n · r2 · tw · r + 2 · r) to O(n · pw · rpw· r + r + 1), where n is the number of lines of code, r is the number of registers, pw is the pathwidth of the control-flow graph and tw is its treewidth. We provide extensive experimental results showing that our approach is applicable to a wide variety of real-world embedded benchmarks from SDCC and obtains runtime improvements of 2-3 orders of magnitude. This is because the pathwidth is equal to the treewidth, or one more, in the overwhelming majority of real-world CFGs and thus our algorithm provides an exponential runtime improvement. As such, the benefits of using pathwidth are not limited to the theoretical side and simplicity in algorithm design, but are also apparent in practice.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {232},
numpages = {26},
keywords = {Treewidth, Pathwidth, Parameterized Algorithms, Control-flow Graphs}
}

@software{10.5281/zenodo.8312920,
author = {Conrado, Giovanna Kobus and Goharshady, Amir Kafshdar and Lam, Chun Kit},
title = {Artifact for The Bounded Pathwidth of Control-Flow Graphs},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8312920},
abstract = {
    <p>Artifact for the paper “The Bounded Pathwidth of Control-Flow Graphs”. It contains the tarball of sdcc, patched with path decomposition computation and code for running the benchmark in our paper. The README.pdf file contains instructions for running the benchmark and explanation of the algorithm.</p>

},
keywords = {Control-flow Graphs, Parameterized Algorithms, Pathwidth, Treewidth}
}

@article{10.1145/3622808,
author = {Larose, Octave and Kaleba, Sophie and Burchell, Humphrey and Marr, Stefan},
title = {AST vs. Bytecode: Interpreters in the Age of Meta-Compilation},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622808},
doi = {10.1145/3622808},
abstract = {Thanks to partial evaluation and meta-tracing, it became practical to build language implementations that reach state-of-the-art peak performance by implementing only an interpreter. Systems such as RPython and GraalVM provide components such as a garbage collector and just-in-time compiler in a language-agnostic manner, greatly reducing implementation effort.  

However, meta-compilation-based language implementations still need to improve further to reach the low memory use and fast warmup behavior that custom-built systems provide. A key element in this endeavor is interpreter performance. Folklore tells us that bytecode interpreters are superior to abstract-syntax-tree (AST) interpreters both in terms of memory use and run-time performance.  

This work assesses the trade-offs between AST and bytecode interpreters to verify common assumptions and whether they hold  
in the context of meta-compilation systems. We implemented four interpreters, each an AST and a bytecode one using RPython and GraalVM. We keep the difference between the interpreters as small as feasible to be able to evaluate interpreter performance, peak performance, warmup, memory use, and the impact of individual optimizations.  

Our results show that both systems indeed reach performance close to Node.js/V8. Looking at interpreter-only performance,  
our AST interpreters are on par with, or even slightly faster than their bytecode counterparts. After just-in-time compilation, the results are roughly on par. This means bytecode interpreters do not have their widely assumed performance advantage. However, we can confirm that bytecodes are more compact in memory than ASTs, which becomes relevant for larger applications. However, for smaller applications, we noticed that bytecode interpreters allocate more memory because boxing avoidance is not as applicable, and because the bytecode interpreter structure requires memory, e.g., for a reified stack.  

Our results show AST interpreters to be competitive on top of meta-compilation systems. Together with possible engineering benefits, they should thus not be discounted so easily in favor of bytecode interpreters.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {233},
numpages = {29},
keywords = {partial evaluation, meta-tracing, language implementation, just-in-time compilation, interpreters, comparison, case study, bytecode, abstract-syntax-tree}
}

@software{10.5281/zenodo.8333815,
author = {Larose, Octave and Kaleba, Sophie and Burchell, Humphrey and Marr, Stefan},
title = {AST vs. Bytecode: Interpreters in the Age of Meta-Compilation (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8333815},
abstract = {
    <p>This artifact accompanies our paper AST vs.&nbsp;Bytecode: Interpreters in the Age of Meta-Compilation to enable others to reuse our experimental setup and methodology, and verify our claims.</p>
<p>Specifically, the artifacts covers our three contributions:</p>
<pre><code>It contains the implementation of our methodology to identify run-time performance and memory usage tradeoffs between AST and bytecode interpreters. Thus, it contains all benchmarks and experiments for reproduction of results, and reuse for new experiments, as well as the data we collected to verify our analysis.
It contains PySOM and TruffleSOM, which both come with an AST and a bytecode interpreter to enable their comparison. It further contains all the variants of PySOM and TruffleSOM that assess the impact of specific optimizations.
It allows to verify the key claim of our paper, that bytecode interpreters cannot be assumed to be faster than AST interpreters in the context of metacompilation systems.</code></pre>

},
keywords = {abstract-syntax-tree, bytecode, case study, comparison, interpreters, just-in-time compilation, language implementation, meta-tracing, partial evaluation}
}

@article{10.1145/3622809,
author = {Rossberg, Andreas},
title = {Mutually Iso-Recursive Subtyping},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622809},
doi = {10.1145/3622809},
abstract = {Iso-recursive types are often taken as a type-theoretic model for type recursion as present in many programming languages, e.g., classes in object-oriented languages or algebraic datatypes in functional languages. Their main advantage over an equi-recursive semantics is that they are simpler and algorithmically less expensive, which is an important consideration when the cost of type checking matters, such as for intermediate or low-level code representations, virtual machines, or runtime casts. However, a closer look reveals that iso-recursion cannot, in its standard form, efficiently express essential type system features like mutual recursion or non-uniform recursion. While it has been folklore that mutual recursion and non-uniform type parameterisation can nicely be handled by generalising to higher kinds, this encoding breaks down when combined with subtyping: the classic “Amber” rule for subtyping iso-recursive types is too weak to express mutual recursion without falling back to encodings of quadratic size. We present a foundational core calculus of iso-recursive types with declared subtyping that can express both inter- and intra-recursion subtyping without such blowup, including subtyping between constructors of higher or mixed kind. In a second step, we identify a syntactic fragment of this general calculus that allows for more efficient type checking without “deep” substitutions, by observing that higher-kinded iso-recursive types can be inserted to “guard” against unwanted β-reductions. This fragment closely resembles the structure of typical nominal subtype systems, but without requiring nominal semantics. It has been used as the basis for a proposed extension of WebAssembly with recursive types.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {234},
numpages = {27},
keywords = {type systems, subtyping, recursive types, higher-order subtyping}
}

@article{10.1145/3622810,
author = {Sano, Chuta and Kavanagh, Ryan and Pientka, Brigitte},
title = {Mechanizing Session-Types using a Structural View: Enforcing Linearity without Linearity},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622810},
doi = {10.1145/3622810},
abstract = {Session types employ a linear type system that ensures that communication channels cannot be implicitly copied or discarded. As a result, many mechanizations of these systems require modeling channel contexts and carefully ensuring that they treat channels linearly. We demonstrate a technique that localizes linearity conditions as additional predicates embedded within type judgments, which allows us to use structural typing contexts instead of linear ones. This technique is especially relevant when leveraging (weak) higher-order abstract syntax to handle channel mobility and the intricate binding structures that arise in session-typed systems.  
Following this approach, we mechanize a session-typed system based on classical linear logic and its type preservation proof in the proof assistant Beluga, which uses the logical framework LF as its encoding language. We also prove adequacy for our encoding. This  
shows the tractability and effectiveness of our approach in modelling substructural systems such as session-typed languages.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {235},
numpages = {26},
keywords = {verification, session types, logical framework, linear logic, concurrency}
}

@software{10.5281/zenodo.8329645,
author = {Sano, Chuta and Kavanagh, Ryan and Pientka, Brigitte},
title = {Mechanization of SCP for article 'Mechanizing Session-Types using a Structural View: Enforcing Linearity without Linearity'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8329645},
abstract = {
    <p>Mechanization of Structural Classical Processes as introduced in the article ‘Mechanizing Session-Types using a Structural View: Enforcing Linearity without Linearity’ in the proof assistant Beluga using weak higher-order abstract syntax. The artifact includes the encoding of the language alongside a mechanized proof of type preservation.</p>

},
keywords = {concurrency, linear logic, mechanization, session types}
}

@article{10.1145/3622813,
author = {Bra\v{c}evac, Oliver and Wei, Guannan and Jia, Songlin and Abeysinghe, Supun and Jiang, Yuxuan and Bao, Yuyan and Rompf, Tiark},
title = {Graph IRs for Impure Higher-Order Languages: Making Aggressive Optimizations Affordable with Precise Effect Dependencies},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622813},
doi = {10.1145/3622813},
abstract = {Graph-based intermediate representations (IRs) are widely used for powerful compiler optimizations, either interprocedurally in pure functional languages, or intraprocedurally in imperative languages. Yet so far, no suitable graph IR exists for aggressive global optimizations in languages with both effects and higher-order functions: aliasing and indirect control transfers make it difficult to maintain sufficiently granular dependency information for optimizations to be effective. To close this long-standing gap, we propose a novel typed graph IR combining a notion of reachability types with an expressive effect system to compute precise and granular effect dependencies at an affordable cost while supporting local reasoning and separate compilation. Our high-level graph IR imposes lexical structure to represent structured control flow and nesting, enabling aggressive and yet inexpensive code motion and other optimizations for impure higher-order programs. We formalize the new graph IR based on a λ-calculus with a reachability type-and-effect system along with a specification of various optimizations. We present performance case studies for tensor loop fusion, CUDA kernel fusion, symbolic execution of LLVM IR, and SQL query compilation in the Scala LMS compiler framework using the new graph IR. We observe significant speedups of up to 21x.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {236},
numpages = {31},
keywords = {intermediate representations, higher-order languages, effects, compilers}
}

@article{10.1145/3622812,
author = {Bhanuka, Ishan and Parreaux, Lionel and Binder, David and Brachth\"{a}user, Jonathan Immanuel},
title = {Getting into the Flow: Towards Better Type Error Messages for Constraint-Based Type Inference},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622812},
doi = {10.1145/3622812},
abstract = {Creating good type error messages for constraint-based type inference systems is difficult. Typical type error messages reflect implementation details of the underlying constraint-solving algorithms rather than the specific factors leading to type mismatches. We propose using subtyping constraints that capture data flow to classify and explain type errors. Our algorithm explains type errors as faulty data flows, which programmers are already used to reasoning about, and illustrates these data flows as sequences of relevant program locations. We show that our ideas and algorithm are not limited to languages with subtyping, as they can be readily integrated with Hindley-Milner type inference. In addition to these core contributions, we present the results of a user study to evaluate the quality of our messages compared to other implementations. While the quantitative evaluation does not show that flow-based messages improve the localization or understanding of the causes of type errors, the qualitative evaluation suggests a real need and demand for flow-based messages.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {237},
numpages = {29},
keywords = {type inference, subtyping, error messages, data flow, constraint solving}
}

@software{10.5281/zenodo.8332129,
author = {Bhanuka, Ishan and Parreaux, Lionel and Binder, David and Brachth\"{a}user, Jonathan Immanuel},
title = {Reproduction Package for "Getting into the Flow: Towards Better Type Error Messages for Constraint-Based Type Inference"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8332129},
abstract = {
    <p>Source code for HMloc type system implementation described in the paper. It contains - * Source code * Instructions for running the code - oopsla23-artifact-overview.md * A guide explaining the code - hmloc-codebase-doc.md</p>

},
keywords = {front-end, functional programming, type systems, user-centered techniques}
}

@article{10.1145/3622814,
author = {Phipps-Costin, Luna and Rossberg, Andreas and Guha, Arjun and Leijen, Daan and Hillerstr\"{o}m, Daniel and Sivaramakrishnan, KC and Pretnar, Matija and Lindley, Sam},
title = {Continuing WebAssembly with Effect Handlers},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622814},
doi = {10.1145/3622814},
abstract = {WebAssembly (Wasm) is a low-level portable code format offering near  
native performance. It is intended as a compilation target for a wide  
variety of source languages. However, Wasm provides no direct support  
for non-local control flow features such as async/await,  
generators/iterators, lightweight threads, first-class continuations,  
etc. This means that compilers for source languages with such features  
must ceremoniously transform whole source programs in order to target  
Wasm.  

  
We present WasmFX an extension to Wasm which provides a universal  
target for non-local control features via effect handlers, enabling  
compilers to translate such features directly into Wasm. Our  
extension is minimal and only adds three main instructions for  
creating, suspending, and resuming continuations. Moreover, our  
primitive instructions are type-safe providing typed continuations  
which are well-aligned with the design principles of Wasm whose stacks  
are typed. We present a formal specification of WasmFX and show that  
the extension is sound. We have implemented WasmFX as an extension to  
the Wasm reference interpreter and also built a prototype WasmFX  
extension for Wasmtime, a production-grade Wasm engine, piggybacking  
on Wasmtime's existing fibers API. The preliminary performance  
results for our prototype are encouraging, and we outline future plans  
to realise a native implementation.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {238},
numpages = {26},
keywords = {stack switching, effect handlers, WebAssembly}
}

@software{10.5281/zenodo.8332962,
author = {Phipps-Costin, Luna and Rossberg, Andreas and Guha, Arjun and Leijen, Daan and Hillerstr\"{o}m, Daniel and Sivaramakrishnan, KC and Pretnar, Matija and Lindley, Sam},
title = {Artifact for Continuing WebAssembly with Effect Handlers},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8332962},
abstract = {
    <p>The artifact contains the software and instructions on how to reproduce the results of the associated paper.</p>

},
keywords = {effect handlers, experiments, WebAssembly}
}

@article{10.1145/3622815,
author = {Wang, Shangwen and Lin, Bo and Sun, Zhensu and Wen, Ming and Liu, Yepang and Lei, Yan and Mao, Xiaoguang},
title = {Two Birds with One Stone: Boosting Code Generation and Code Search via a Generative Adversarial Network},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622815},
doi = {10.1145/3622815},
abstract = {Automatically transforming developers' natural language descriptions into source code has been a longstanding goal in software engineering research.  
Two types of approaches have been proposed in the literature to achieve this: code generation, which involves generating a new code snippet, and code search, which involves reusing existing code.  
However, despite existing efforts, the effectiveness of the state-of-the-art techniques remains limited.  
To seek for further advancement, our insight is that code generation and code search can help overcome the limitation of each other:  
the code generator can benefit from feedback on the quality of its generated code, which can be provided by the code searcher, while the code searcher can benefit from the additional training data augmented by the code generator to better understand code semantics.  
Drawing on this insight, we propose a novel approach that combines code generation and code search techniques using a generative adversarial network (GAN), enabling mutual improvement through the adversarial training.  
Specifically, we treat code generation and code search as the generator and discriminator in the GAN framework, respectively, and incorporate several customized designs for our tasks.  
We evaluate our approach in eight different settings, and consistently observe significant performance improvements for both code generation and code search.  
For instance, when using NatGen, a state-of-the-art code generator, as the generator and GraphCodeBERT, a state-of-the-art code searcher, as the discriminator, we achieve a 32\% increase in CodeBLEU score for code generation, and a 12\% increase in mean reciprocal rank for code search on a large-scale Python dataset, compared to their original performances.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {239},
numpages = {30},
keywords = {Generative Adversarial Network, Code Search, Code Generation}
}

@software{10.5281/zenodo.7824776,
author = {Wang, Shangwen and Lin, Bo and Sun, Zhensu and Wen, Ming and Liu, Yepang and Lei, Yan and Mao, Xiaoguang},
title = {Reproduction Package of &nbsp;the paper "Two Birds with One Stone: Boosting Code Generation and Code Search via a Generative Adversarial Network"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7824776},
abstract = {
    <p>We have released the source code and dataset of our study.</p>

},
keywords = {Code Generation, Code Search, Generative Adversarial Network}
}

@article{10.1145/3622816,
author = {Madsen, Magnus and van de Pol, Jaco and Henriksen, Troels},
title = {Fast and Efficient Boolean Unification for Hindley-Milner-Style Type and Effect Systems},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622816},
doi = {10.1145/3622816},
abstract = {As type and effect systems become more expressive there is an increasing need for efficient type inference. We consider a polymorphic effect system based on Boolean formulas where inference requires Boolean unification. Since Boolean unification involves semantic equivalence, conventional syntax-driven unification is insufficient. At the same time, existing Boolean unification techniques are ill-suited for type inference.  
We propose a hybrid algorithm for solving Boolean unification queries based on Boole’s Successive Variable Elimination (SVE) algorithm. The proposed approach builds on several key observations regarding the Boolean unification queries encountered in practice, including: (i) most queries are simple, (ii) most queries involve a few flexible variables, (iii) queries are likely to repeat due similar programming patterns, and (iv) there is a long tail of complex queries. We exploit these observations to implement several strategies for formula minimization, including ones based on tabling and binary decision diagrams.  
We implement the new hybrid approach in the Flix programming language. Experimental results show that by reducing the overhead of Boolean unification, the compilation throughput increases from 8,580 lines/sec to 15,917 lines/sec corresponding to a 1.8x speed-up. Further, the overhead on type and effect inference time is only 16\% which corresponds to an overhead of less than 7\% on total compilation time. We study the hybrid approach and demonstrate that each design choice improves performance.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {240},
numpages = {28},
keywords = {type inference, Hindley-Milner type systems, Boolean unification}
}

@software{10.5281/zenodo.8318658,
author = {Madsen, Magnus and van de Pol, Jaco and Henriksen, Troels},
title = {Fast and Efficient Boolean Unification for Hindley-Milner-Style Type and Effect Systems},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8318658},
abstract = {
    <p>This artifact supports the paper Fast and Efficient Boolean Unification for Hindley-Milner-Style Type and Effect Systems. It reproduces the main quantifiable results of the paper, in particular the performance of six of the seven proposed strategies for Boolean unification (Strategy 1 is too slow to actually function). The artifact reproduces Figures 3-7 of the paper, as well as various other minor metrics referenced listed in the paper.</p>
<p>See documentation in artifact itself for more information.</p>

},
keywords = {flix, performance, type inference}
}

@article{10.1145/3622817,
author = {Greenman, Ben and Felleisen, Matthias and Dimoulas, Christos},
title = {How Profilers Can Help Navigate Type Migration},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622817},
doi = {10.1145/3622817},
abstract = {Sound migratory typing envisions a safe and smooth refactoring of untyped code bases to typed ones. However, the cost of enforcing safety with run-time checks is often prohibitively high, thus performance regressions are a likely occurrence. Additional types can often recover performance, but choosing the right components to type is difficult because of the exponential size of the migratory typing lattice. In principal though, migration could be guided by off-the-shelf profiling tools. To examine this hypothesis, this paper follows the rational programmer method and reports on the results of an experiment on tens of thousands of performance-debugging scenarios via seventeen strategies for turning profiler output into an actionable next step. The most effective strategy is the use of deep types to eliminate the most costly boundaries between typed and untyped components; this strategy succeeds in more than 50\% of scenarios if two performance degradations are tolerable along the way.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {241},
numpages = {30},
keywords = {rational programmer, profiling, migratory typing, gradual typing}
}

@software{10.5281/zenodo.8148784,
author = {Greenman, Ben and Felleisen, Matthias and Dimoulas, Christos},
title = {Artifact: How Profilers Can Help Navigate Type Migration},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8148784},
abstract = {
    <h4 id="contents">Contents:</h4>
<ul>
<li><code>artifact.tar.gz</code> code for reproducing our rational programmer experiment</li>
<li><code>benchmarks.tar.gz</code> GTP Benchmarks without and with modifications</li>
<li><code>cloudlab.tar.gz</code> for measuring performance on CloudLab</li>
<li><code>figure-data.tar.gz</code> figures and summarized data for the paper</li>
<li><code>rational-trails.tar.gz</code> output from the rational programmer</li>
<li><code>raw-data.tar.gz</code> running times, boundary profile output, and statistical profile output for all benchmarks</li>
</ul>

},
keywords = {gradual typing, migratory typing, profiling, rational programmer}
}

@article{10.1145/3622818,
author = {Flatt, Matthew and Allred, Taylor and Angle, Nia and De Gabrielle, Stephen and Findler, Robert Bruce and Firth, Jack and Gopinathan, Kiran and Greenman, Ben and Kasivajhula, Siddhartha and Knauth, Alex and McCarthy, Jay and Phillips, Sam and Porncharoenwase, Sorawee and S\o{}gaard, Jens Axel and Tobin-Hochstadt, Sam},
title = {Rhombus: A New Spin on Macros without All the Parentheses},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622818},
doi = {10.1145/3622818},
abstract = {Rhombus is a new language that is built on Racket. It offers the same  
 kind of language extensibility as Racket itself, but using traditional  
 (infix) notation. Although Rhombus is far from the first language to  
 support Lisp-style macros without Lisp-style parentheses, Rhombus offers  
 a novel synthesis of macro technology that is practical and expressive.  
 A key element is the use of multiple binding spaces  
 for context-specific sublanguages. For example, expressions and  
 pattern-matching forms can use the  
 same operators with different meanings and without creating conflicts.  
 Context-sensitive bindings, in turn, facilitate a language design that  
 reduces the notational distance between the core language and macro  
 facilities. For example, repetitions can be defined and used in binding  
 and expression contexts generally, which enables a smoother transition  
 from programming to metaprogramming.  
 Finally, since handling static information (such as types) is also a  
 necessary part of growing macros beyond Lisp, Rhombus includes support  
 in its expansion protocol for communicating static information among  
 bindings and expressions.  
 The Rhombus implementation  
 demonstrates that all of these pieces can work together in a coherent  
 and user-friendly language.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {242},
numpages = {30},
keywords = {macros, infix syntax, binding spaces}
}

@software{10.1145/3580417,
author = {Flatt, Matthew and Allred, Taylor and Angle, Nia and De Gabrielle, Stephen and Findler, Robert Bruce and Firth, Jack and Gopinathan, Kiran and Greenman, Ben and Kasivajhula, Siddhartha and Knauth, Alex and McCarthy, Jay and Phillips, Sam and Porncharoenwase, Sorawee and S\o{}gaard, Jens Axel and Tobin-Hochstadt, Sam},
title = {Artifact for "Rhombus: A New Spin on Macros without All the Parentheses"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3580417},
abstract = {
    <p>Rhombus is a programming language with conventional expression syntax that is built on Racket and that is macro-extensible in the same way as Racket. The paper aims to show that Rhombus is realizable via a novel synthesis of macro technology. This artifact provides a working implementation of Rhombus along with example programs in Rhombus that show this realization. The artifact is an OVA file which can be imported into VirtualBox (and perhaps other virtualization software).</p>

},
keywords = {binding spaces, infix syntax, macros}
}

@article{10.1145/3622819,
author = {Zhou, Chijin and Zhang, Quan and Guo, Lihua and Wang, Mingzhe and Jiang, Yu and Liao, Qing and Wu, Zhiyong and Li, Shanshan and Gu, Bin},
title = {Towards Better Semantics Exploration for Browser Fuzzing},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622819},
doi = {10.1145/3622819},
abstract = {Web browsers exhibit rich semantics that enable a plethora of web-based functionalities. However, these intricate semantics present significant challenges for the implementation and testing of browsers. For example, fuzzing, a widely adopted testing technique, typically relies on handwritten context-free grammars (CFGs) for automatically generating inputs. However, these CFGs fall short in adequately modeling the complex semantics of browsers, resulting in generated inputs that cover only a portion of the semantics and are prone to semantic errors. In this paper, we present SaGe, an automated method that enhances browser fuzzing through the use of production-context sensitive grammars (PCSGs) incorporating semantic information. Our approach begins by extracting a rudimentary CFG from W3C standards and iteratively enhancing it to create a PCSG. The resulting PCSG enables our fuzzer to generate inputs that explore a broader range of browser semantics with a higher proportion of semantically-correct inputs. To evaluate the efficacy of SaGe, we conducted 24-hour fuzzing campaigns on mainstream browsers, including Chrome, Safari, and Firefox. Our approach demonstrated better performance compared to existing browser fuzzers, with a 6.03\%-277.80\% improvement in edge coverage, a 3.56\%-161.71\% boost in semantic correctness rate, twice the number of bugs discovered. Moreover, we identified 62 bugs across the three browsers, with 40 confirmed and 10 assigned CVEs.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {243},
numpages = {28},
keywords = {Semantics-Aware Fuzzing, Context-Sensitive Grammar, Browser Security}
}

@software{10.5281/zenodo.8328742,
author = {Zhou, Chijin and Zhang, Quan and Guo, Lihua and Wang, Mingzhe and Jiang, Yu and Liao, Qing and Wu, Zhiyong and Li, Shanshan and Gu, Bin},
title = {Towards Better Semantics Exploration for Browser Fuzzing},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8328742},
abstract = {
    <p>This is the artifact for “Towards Better Semantics Exploration for Browser Fuzzing”, published in SPLASH/OOPSLA 2023. All instructions can be found in the readme.pdf file.</p>

},
keywords = {Browser Fuzzer, Browser Security, Context-Sensitive Grammar, Semantics-Aware Fuzzing}
}

@article{10.1145/3622820,
author = {Vindum, Simon Friis and Birkedal, Lars},
title = {Spirea: A Mechanized Concurrent Separation Logic for Weak Persistent Memory},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622820},
doi = {10.1145/3622820},
abstract = {Weak persistent memory (a.k.a. non-volatile memory) is an emerging technology that offers fast byte-addressable durable main memory.  
A wealth of algorithms and libraries has been developed to explore this exciting technology.  
As noted by others, this has led to a significant verification gap.  
Towards closing this gap, we present Spirea, the first concurrent separation logic for verification of programs under a weak persistent memory model.  
Spirea is based on the Iris and Perennial verification frameworks, and by combining features from these logics with novel techniques it supports high-level modular reasoning about crash-safe and thread-safe programs and libraries.  
Spirea is fully mechanized in the Coq proof assistant and allows for interactive development of proofs with the Iris Proof Mode.  
We use Spirea to verify several challenging examples with modular specifications.  
We show how our logic can verify thread-safety and crash-safety of non-blocking durable data structures with null-recovery,  
in particular the Treiber stack and the Michael-Scott queue adapted to persistent memory.  
This is the first time durable data structures have been verified with a program logic.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {244},
numpages = {26},
keywords = {weak memory, program verification, program logic, persistent memory, persistency, non-volatile memory, Iris}
}

@software{10.5281/zenodo.8314888,
author = {Vindum, Simon Friis and Birkedal, Lars},
title = {Artifact for the paper "Spirea: A Mechanized Concurrent Separation Logic for Weak Persistent Memory" in OOPSLA23},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8314888},
abstract = {
    <p>Artifact for the paper ‘Spirea: A Mechanized Concurrent Separation Logic for Weak Persistent Memory’. The artifact contains a full mechanization of the paper. The project is also available at: https://github.com/logsem/spirea</p>

},
keywords = {coq, iris, perennial, persistent memory}
}

@article{10.1145/3622821,
author = {Thy, Sewen and Costea, Andreea and Gopinathan, Kiran and Sergey, Ilya},
title = {Adventure of a Lifetime: Extract Method Refactoring for Rust},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622821},
doi = {10.1145/3622821},
abstract = {We present a design and implementation of the automated "Extract Method" refactoring for Rust programs. Even though Extract Method is one of the most well-studied and widely used in practice automated refactorings, featured in all major IDEs for all popular programming languages, implementing it soundly for Rust is surprisingly non-trivial due to the restrictions of the Rust's ownership and lifetime-based type system.  

In this work, we provide a systematic decomposition of the Extract Method refactoring for Rust programs into a series of program transformations, each concerned with satisfying a particular aspect of Rust type safety, eventually producing a well-typed Rust program. Our key discovery is the formulation of Extract Method as a composition of naive function hoisting and a series of automated program repair procedures that progressively make the resulting program "more well-typed" by relying on the corresponding repair oracles. Those oracles include a novel static intra-procedural ownership analysis that infers correct sharing annotations for the extracted function's parameters, and the lifetime checker of rustc, Rust's reference compiler.  

We implemented our approach in a tool called REM---an automated Extract Method refactoring built on top of IntelliJ IDEA plugin for Rust. Our extensive evaluation on a corpus of changes in five popular Rust projects shows that REM (a) can extract a larger class of feature-rich code fragments into semantically correct functions than other existing refactoring tools, (b) can reproduce method extractions performed manually by human developers in the past, and (c) is efficient enough to be used in interactive development.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {245},
numpages = {28},
keywords = {program repair, automated code refactoring, Rust}
}

@software{10.5281/zenodo.8124395,
author = {Thy, Sewen and Costea, Andreea and Gopinathan, Kiran and Sergey, Ilya},
title = {Reproduction Artefact for Article 'Adventure of a Lifetime: Extract Method Refactoring for Rust'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8124395},
abstract = {
    <p>This release snapshots the functionality of the submitted artefact for the tool Rusty Extraction Maestro (REM) described in the OOPSLA 23 paper “Adventure of a Lifetime: Extract Method Refactoring for Rust”:</p>
<ul>
<li><p>Docker file with reproducible build environment</p></li>
<li><p>Readme with getting started and step-by-step instructions</p></li>
<li><p>Source code and build files for REM</p></li>
<li><p>40 experiment programs</p></li>
</ul>

},
keywords = {Extract Method, IDE, Language Tooling, Program Repair, Refactoring, Rust}
}

@article{10.1145/3622822,
author = {Cao, Huanqi and Tang, Shizhi and Zhu, Qianchao and Yu, Bowen and Chen, Wenguang},
title = {Mat2Stencil: A Modular Matrix-Based DSL for Explicit and Implicit Matrix-Free PDE Solvers on Structured Grid},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622822},
doi = {10.1145/3622822},
abstract = {Partial differential equation (PDE) solvers are extensively utilized across numerous scientific and engineering fields. However, achieving high performance and scalability often necessitates intricate and low-level programming, particularly when leveraging deterministic sparsity patterns in structured grids. In this paper, we propose an innovative domain-specific language (DSL), Mat2Stencil, with its compiler, for PDE solvers on structured grids. Mat2Stencil introduces a structured sparse matrix abstraction, facilitating modular, flexible, and easy-to-use expression of solvers across a broad spectrum, encompassing components such as Jacobi or Gauss-Seidel preconditioners, incomplete LU or Cholesky decompositions, and multigrid methods built upon them. Our DSL compiler subsequently generates matrix-free code consisting of generalized stencils through multi-stage programming. The code allows spatial loop-carried dependence in the form of quasi-affine loops, in addition to the Jacobi-style stencil’s embarrassingly parallel on spatial dimensions. We further propose a novel automatic parallelization technique for the spatially dependent loops, which offers a compile-time deterministic task partitioning for threading, calculates necessary inter-thread synchronization automatically, and generates an efficient multi-threaded implementation with fine-grained synchronization. Implementing 4 benchmarking programs, 3 of them being the pseudo-applications in NAS Parallel Benchmarks with 6.3\% lines of code and 1 being matrix-free High Performance Conjugate Gradients with 16.4\% lines of code, we achieve up to 1.67\texttimes{} and on average 1.03\texttimes{} performance compared to manual implementations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {246},
numpages = {30},
keywords = {structured grid, stencil, polyhedral compilation, performance optimization, multi-stage programming, finite difference method, domain-specific language, compiler}
}

@software{10.5281/zenodo.8149701,
author = {Cao, Huanqi and Tang, Shizhi and Zhu, Qianchao and Yu, Bowen and Chen, Wenguang},
title = {Artifact of Mat2Stencil: A Modular Matrix-Based DSL for Explicit and Implicit Matrix-Free PDE Solvers on Structured Grid},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8149701},
abstract = {
    <p>The artifact submitted for evaluation of OOPSLA 23 conditionally accepted paper “Mat2Stencil: A Modular Matrix-Based DSL for Explicit and Implicit Matrix-Free PDE Solvers on Structured Grid”.</p>

},
keywords = {compiler, finite difference method, multi-stage programming, omain-specific language, performance optimization, polyhedral compilation, stencil, structured grid}
}

@article{10.1145/3622823,
author = {Gu\'{e}neau, Arma\"{e}l and Hostert, Johannes and Spies, Simon and Sammler, Michael and Birkedal, Lars and Dreyer, Derek},
title = {Melocoton: A Program Logic for Verified Interoperability Between OCaml and C},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622823},
doi = {10.1145/3622823},
abstract = {In recent years, there has been tremendous progress on developing program logics for verifying the correctness of programs in a rich and diverse array of languages. Thus far, however, such logics have assumed that programs are written entirely in a single programming language. In practice, this assumption rarely holds since programs are often composed of components written in different programming languages, which interact with one another via some kind of foreign function interface (FFI). In this paper, we take the first steps towards the goal of developing program logics for multi-language verification. Specifically, we present Melocoton, a multi-language program verification system for reasoning about OCaml, C, and their interactions through the OCaml FFI. Melocoton consists of the first formal semantics of (a large subset of) the OCaml FFI—previously only described in prose in the OCaml manual—as well as the first program logic to reason about the interactions of program components written in OCaml and C. Melocoton is fully mechanized in Coq on top of the Iris separation logic framework.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {247},
numpages = {29},
keywords = {transfinite step-indexing, separation logic, program logics, multi-language semantics, garbage collection, foreign-function interfaces, angelic non-determinism, OCaml, Iris, Coq, C}
}

@software{10.5281/zenodo.8331210,
author = {Gu\'{e}neau, Arma\"{e}l and Hostert, Johannes and Spies, Simon and Sammler, Michael and Birkedal, Lars and Dreyer, Derek},
title = {Artifact for "Melocoton: A Program Logic for Verified Interoperability Between OCaml and C"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8331210},
abstract = {
    <p>This is the artifact for the OOPSLA’23 paper “Melocoton: A Program Logic for Verified Interoperability Between OCaml and C”. It contains the Coq development for the paper.</p>

},
keywords = {angelic non-determinism, C, Coq, foreign-function interfaces, garbage collection, Iris, multi-language semantics, OCaml, program logics, separation logic, transfinite step-indexing}
}

@article{10.1145/3622824,
author = {Pacak, Andr\'{e} and Erdweg, Sebastian},
title = {Interactive Debugging of Datalog Programs},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622824},
doi = {10.1145/3622824},
abstract = {Datalog is used for complex programming tasks nowadays, consisting of numerous inter-dependent predicates. But Datalog lacks interactive debugging techniques that support the stepwise execution and inspection of the execution state. In this paper, we propose interactive debugging of Datalog programs following a top-down evaluation strategy called recursive query/subquery. While the recursive query/subquery approach is well-known in the literature, we are the first to provide a complete programming-language semantics based on it. Specifically, we develop the first small-step operational semantics for top-down Datalog, where subqueries occur as nested intermediate terms. The small-step semantics forms the basis of step-into interactions in the debugger. Moreover, we show how step-over interactions can be realized efficiently based on a hybrid Datalog semantics that adds a bottom-up database to our top-down operational semantics. We implemented a debugger for core Datalog following these semantics and explain how to adopt it for debugging the frontend languages of Souffl\'{e} and IncA. Our evaluation shows that our hybrid Datalog semantics can be used to debug real-world Datalog programs with realistic workloads.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {248},
numpages = {28},
keywords = {small-step operational semantics, debugging, QSQR, Datalog}
}

@article{10.1145/3622825,
author = {Ye, Fangke and Zhao, Jisheng and Shirako, Jun and Sarkar, Vivek},
title = {Concrete Type Inference for Code Optimization using Machine Learning with SMT Solving},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622825},
doi = {10.1145/3622825},
abstract = {Despite the widespread popularity of dynamically typed languages such as Python, it is well known that they pose significant challenges to code optimization due to the lack of concrete type information. To overcome this limitation, many ahead-of-time optimizing compiler approaches for Python rely on programmers to provide optional type information as a prerequisite for extensive code optimization. Since few programmers provide this information, a large majority of Python applications are executed without the benefit of code optimization, thereby contributing collectively to a significant worldwide wastage of compute and energy resources. In this paper, we introduce a new approach to concrete type inference that is shown to be effective in enabling code optimization for dynamically typed languages, without requiring the programmer to provide any type information. We explore three kinds of type inference algorithms in our approach based on: 1) machine learning models including GPT-4, 2) constraint-based inference based on SMT solving, and 3) a combination of 1) and 2). Our approach then uses the output from type inference to generate multi-version code for a bounded number of concrete type options, while also including a catch-all untyped version for the case when no match is found. The typed versions are then amenable to code optimization. Experimental results show that the combined algorithm in 3) delivers far superior precision and performance than the separate algorithms for 1) and 2). The performance improvement due to type inference, in terms of geometric mean speedup across all benchmarks compared to standard Python, when using 3) is 26.4\texttimes{} with Numba as an AOT optimizing back-end and 62.2\texttimes{} with the Intrepydd optimizing compiler as a back-end. These vast performance improvements can have a significant impact on programmers’ productivity, while also reducing their applications’ use of compute and energy resources.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {249},
numpages = {28},
keywords = {Type Inference, Python, Machine Learning, Code Optimization}
}

@software{10.5281/zenodo.8332121,
author = {Ye, Fangke and Zhao, Jisheng and Shirako, Jun and Sarkar, Vivek},
title = {Code for the Paper "Concrete Type Inference for Code Optimization using Machine Learning with SMT Solving"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8332121},
abstract = {
    <p>Code for the paper “Concrete Type Inference for Code Optimization using Machine Learning with SMT Solving”.</p>

},
keywords = {Code Optimization, Machine Learning, Python, Type Inference}
}

@article{10.1145/3622826,
author = {Wang, Yu and Wang, Ke and Wang, Linzhang},
title = {An Explanation Method for Models of Code},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622826},
doi = {10.1145/3622826},
abstract = {This paper introduces a novel method, called WheaCha, for explaining the predictions of code models. Similar to attribution methods, WheaCha seeks to identify input features that are responsible for a particular prediction that models make. On the other hand, it differs from attribution methods in crucial ways. Specifically, WheaCha separates an input program into "wheat" (i.e., defining features that are the reason for which models predict the label that they predict) and the rest "chaff" for any given prediction. We realize WheaCha in a tool, HuoYan, and use it to explain four prominent code models: code2vec, seq-GNN, GGNN, and CodeBERT. Results show that (1) HuoYan is efficient — taking on average under twenty seconds to compute wheat for an input program in an end-to-end fashion (i.e., including model prediction time); (2) the wheat that all models use to make predictions is predominantly comprised of simple syntactic or even lexical properties (i.e., identifier names); (3) neither the latest explainability methods for code models (i.e., SIVAND and CounterFactual Explanations) nor the most noteworthy attribution methods (i.e., Integrated Gradients and SHAP) can precisely capture wheat. Finally, we set out to demonstrate the usefulness of WheaCha, in particular, we assess if WheaCha’s explanations can help end users to identify defective code models (e.g., trained on mislabeled data or learned spurious correlations from biased data). We find that, with WheaCha, users achieve far higher accuracy in identifying faulty models than SIVAND, CounterFactual Explanations, Integrated Gradients and SHAP.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {250},
numpages = {27},
keywords = {Models of Code, Explainability Method, Defining Features}
}

@article{10.1145/3622827,
author = {Jung, Jaehwang and Lee, Janggun and Choi, Jaemin and Kim, Jaewoo and Park, Sunho and Kang, Jeehoon},
title = {Modular Verification of Safe Memory Reclamation in Concurrent Separation Logic},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622827},
doi = {10.1145/3622827},
abstract = {Formal verification is an effective method to address the challenge of designing correct and efficient concurrent data structures. But verification efforts often ignore memory reclamation, which involves nontrivial synchronization between concurrent accesses and reclamation. When incorrectly implemented, it may lead to critical safety errors such as use-after-free and the ABA problem. Semi-automatic safe memory reclamation schemes such as hazard pointers and RCU encapsulate the complexity of manual memory management in modular interfaces. However, this modularity has not been carried over to formal verification. We propose modular specifications of hazard pointers and RCU, and formally verify realistic implementations of them in concurrent separation logic. Specifically, we design abstract predicates for hazard pointers that capture the meaning of validating the protection of nodes, and those for RCU that support optimistic traversal to possibly retired nodes. We demonstrate that the specifications indeed facilitate modular verification in three criteria: compositional verification, general applicability, and easy integration. In doing so, we present the first formal verification of Harris’s list, the Harris-Michael list, the Chase-Lev deque, and RDCSS with reclamation. We report the Coq mechanization of all our results in the Iris separation logic framework.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {251},
numpages = {29},
keywords = {separation logic, safe memory reclamation, Iris}
}

@software{10.1145/3580418,
author = {Jung, Jaehwang and Lee, Janggun and Choi, Jaemin and Kim, Jaewoo and Park, Sunho and Kang, Jeehoon},
title = {Coq Formalization for the Article "Modular Verification of Safe Memory Reclamation in Concurrent Separation Logic"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3580418},
abstract = {
    <p>Coq Formalization for the Article “Modular Verification of Safe Memory Reclamation in Concurrent Separation Logic”.</p>

},
keywords = {concurrent algorithm, Coq, Iris, memory reclamation, separation logic}
}

@article{10.1145/3622828,
author = {Lee, Edward and Lhot\'{a}k, Ond\v{r}ej},
title = {Simple Reference Immutability for System F&lt;:},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622828},
doi = {10.1145/3622828},
abstract = {Reference immutability is a type based technique for taming mutation that has long been studied in the  
context of object-oriented languages, like Java. Recently, though, languages like Scala have blurred the lines  
between functional programming languages and object oriented programming languages. We explore how  
reference immutability interacts with features commonly found in these hybrid languages, in particular with  
higher-order functions – polymorphism – and subtyping. We construct a calculus System F&lt;:M which encodes  
a reference immutability system as a simple extension of System F&lt;: and prove that it satisfies the standard  
soundness and immutability safety properties.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {252},
numpages = {25},
keywords = {Type Systems, System F-sub, Reference Immutability}
}

@software{10.1145/3580414,
author = {Lee, Edward and Lhot\'{a}k, Ond\v{r}ej},
title = {Artifact for the OOPSLA 2023 paper 'Simple Reference Immutability for System F-Sub'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3580414},
abstract = {
    <p>This artifact The artifact consists of the Coq proofs for the paper ‘Simple Reference Immutability for System F-sub’. There are two calculi formalized in this artifact.</p>
<ul>
<li><p>System Lm, our untyped reference immutability calculus with the dynamic immutability safety results discussed in Section 3 of the paper.</p></li>
<li><p>System Fm, our typed calculi building on Lm and System F-sub with both the static soundness results discussed in Section 4 of our paper as well as the dynamic immutability safety results discussed in Section 4 of the paper.</p></li>
</ul>

},
keywords = {Coq, reference immutability, System F-sub, System Fm, System Lm}
}

@article{10.1145/3622829,
author = {Cai, Zhuo and Farokhnia, Soroush and Goharshady, Amir Kafshdar and Hitarth, S.},
title = {Asparagus: Automated Synthesis of Parametric Gas Upper-Bounds for Smart Contracts},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622829},
doi = {10.1145/3622829},
abstract = {Modern programmable blockchains have built-in support for smart contracts, i.e. ‍programs that are stored on the blockchain and whose state is subject to consensus. After a smart contract is deployed on the blockchain, anyone on the network can interact with it and call its functions by creating transactions. The blockchain protocol is then used to reach a consensus about the order of the transactions and, as a direct corollary, the state of every smart contract. Reaching such consensus necessarily requires every node on the network to execute all function calls. Thus, an attacker can perform DoS by creating expensive transactions and function calls that use considerable or even possibly infinite time and space. To avoid this, following Ethereum, virtually all programmable blockchains have introduced the concept of “gas”. A fixed hard-coded gas cost is assigned to every atomic operation and the user who calls a function has to pay for its total gas usage. This technique ensures that the protocol is not vulnerable to DoS attacks, but it has also had significant unintended consequences. Out-of-gas errors, i.e. ‍when a user misunderestimates the gas usage of their function call and does not allocate enough gas, are a major source of security vulnerabilities in Ethereum. We focus on the well-studied problem of automatically finding upper-bounds on the gas usage of a smart contract. This is a classical problem in the blockchain community and has also been extensively studied by researchers in programming languages and verification. In this work, we provide a novel approach using theorems from polyhedral geometry and real algebraic geometry, namely Farkas’ Lemma, Handelman’s Theorem, and Putinar’s Positivstellensatz, to automatically synthesize linear and polynomial parametric bounds for the gas usage of smart contracts. Our approach is the first to provide completeness guarantees for the synthesis of such parametric upper-bounds. Moreover, our theoretical results are independent of the underlying consensus protocol and can be applied to smart contracts written in any language and run on any blockchain. As a proof of concept, we also provide a tool, called “Asparagus” that implements our algorithms for Ethereum contracts written in Solidity. Finally, we provide extensive experimental results over 24,188 real-world smart contracts that are currently deployed on the Ethereum blockchain. We compare Asparagus against GASTAP, which is the only previous tool that could provide parametric bounds, and show that our method significantly outperforms it, both in terms of applicability and the tightness of the resulting bounds. More specifically, our approach can handle 80.56\% of the functions (126,269 out of 156,735) in comparison with GASTAP’s 58.62\%. Additionally, even on the benchmarks where both approaches successfully synthesize a bound, our bound is tighter in 97.85\% of the cases.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {253},
numpages = {30},
keywords = {Smart Contracts, Out-of-gas Vulnerabilities, Gas Bounds, Blockchain}
}

@software{10.5281/zenodo.8202373,
author = {Cai, Zhuo and Farokhnia, Soroush and Goharshady, Amir Kafshdar and Hitarth, S.},
title = {Artifact-Asparagus: Automated Synthesis of Parametric Gas Upper-bounds for Smart Contracts},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8202373},
abstract = {
    <p>The artifact offers code, data and a virtual machine that can reproduce the result of our paper. It can be used to estimate upper bound of gas cost for ethereum smart contract public functions.</p>
<p>Structure of the Repository The root directory of the repository is Aspragus/ present in Github https: //github.com/zhuocai/Asparagus/. The artifact repository consists of the following: Algorithm/Code. The code of the algorithm is present in the src directory. It is written in Python3. Shell Scripts Various shell scripts that we use to run the benchmarks are in the root folder of our repository. GASTAP Dataset. We have evaluated our tool on the GASTAP dataset that can be found in our GitHub Repository in the directory dataset/gastap dataset. Each subfolder contains a solidity source code .sol file, Rule-Based-Representation .rbr file compiled using the EthIR tool, and .meta file that stores auxiliary information about RBR and the variables.</p>

},
keywords = {Ethereum, gas}
}

@article{10.1145/3622830,
author = {Feser, Jack and Dillig, I\c{s}\i{}l and Solar-Lezama, Armando},
title = {Inductive Program Synthesis Guided by Observational Program Similarity},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622830},
doi = {10.1145/3622830},
abstract = {We present a new general-purpose synthesis technique for generating programs from input-output examples. Our method, called metric program synthesis, relaxes the observational equivalence idea (used widely in bottom-up enumerative synthesis) into a weaker notion of observational similarity, with the goal of reducing the search space that the synthesizer needs to explore. Our method clusters programs into equivalence classes based on an expert-provided distance metric and constructs a version space that compactly represents “approximately correct” programs. Then, given a “close enough” program sampled from this version space, our approach uses a distance-guided repair algorithm to find a program that exactly matches the given input-output examples. We have implemented our proposed metric program synthesis technique in a tool called SyMetric and evaluate it in three different domains considered in prior work. Our evaluation shows that SyMetric outperforms other domain-agnostic synthesizers that use observational equivalence and that it achieves results competitive with domain-specific synthesizers that are either designed for or trained on those domains.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {254},
numpages = {29},
keywords = {regular expression inference, program synthesis, inverse csg, distance metric}
}

@software{10.5281/zenodo.8200210,
author = {Feser, Jack and Dillig, I\c{s}\i{}l and Solar-Lezama, Armando},
title = {Inductive Program Synthesis Guided by Observational Program Similarity (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8200210},
abstract = {
    <p>Artifact for “Inductive Program Synthesis Guided by Observational Program Similarity”.</p>

},
keywords = {program synthesis}
}

@article{10.1145/3622831,
author = {M\"{u}ller, Marius and Schuster, Philipp and Starup, Jonathan Lindegaard and Ostermann, Klaus and Brachth\"{a}user, Jonathan Immanuel},
title = {From Capabilities to Regions: Enabling Efficient Compilation of Lexical Effect Handlers},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622831},
doi = {10.1145/3622831},
abstract = {Effect handlers are a high-level abstraction that enables programmers to use effects in a structured  
way. They have gained a lot of popularity within academia and subsequently also in industry. However,  
the abstraction often comes with a significant runtime cost and there has been intensive research  
recently on how to reduce this price.  

A promising approach in this regard is to implement effect handlers using a CPS translation and  
to provide sufficient information about the nesting of handlers. With this information the  
CPS translation can decide how effects have to be lifted through handlers, i.e., which handlers  
need to be skipped, in order to handle the effect at the correct place. A structured way to make  
this information available is to use a calculus with a region system and explicit subregion  
evidence. Such calculi, however, are quite verbose, which makes them impractical to use as a  
source-level language.  

We present a method to infer the lifting information for a calculus underlying a source-level language.  
This calculus uses second-class capabilities for the safe use of effects. To do so, we define a typed translation  
to a calculus with regions and evidence and we show that this lift-inference translation is typability-  
and semantics-preserving. On the one hand, this exposes the precise relation between the second-class  
property and the structure given by regions. On the other hand, it closes a gap in a compiler pipeline  
enabling efficient compilation of the source-level language. We have implemented lift inference in this  
compiler pipeline and conducted benchmarks which indicate that the approach is indeed working.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {255},
numpages = {30},
keywords = {region inference, lift inference, effect handlers}
}

@software{10.5281/zenodo.8315298,
author = {M\"{u}ller, Marius and Schuster, Philipp and Starup, Jonathan Lindegaard and Ostermann, Klaus and Brachth\"{a}user, Jonathan Immanuel},
title = {Artifact of the paper 'From Capabilities to Regions: Enabling Efficient Compilation of Lexical Effect Handlers'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8315298},
abstract = {
    <p>The artifact consists of the benchmarks conducted for the evaluation of the compilation approach presented in the paper. It contains a Dockerfile which can be used to build a Docker image for a container with all necessary languages installed. The benchmarks can hence be run inside this container.</p>

},
keywords = {effect handlers, lift inference, region inference}
}

@article{10.1145/3622832,
author = {He, Dongjie and Gui, Yujiang and Li, Wei and Tao, Yonggang and Zou, Changwei and Sui, Yulei and Xue, Jingling},
title = {A Container-Usage-Pattern-Based Context Debloating Approach for Object-Sensitive Pointer Analysis},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622832},
doi = {10.1145/3622832},
abstract = {In this paper, we introduce DebloaterX, a new approach for automatically identifying context-independent objects to debloat contexts in object-sensitive pointer analysis (kobj). Object sensitivity achieves high precision, but its context construction mechanism combines objects with their contexts indiscriminately. This leads to a combinatorial explosion of contexts in large programs, resulting in inefficiency. Previous research has proposed a context-debloating approach that inhibits a pre-selected set of context-independent objects from forming new contexts, improving the efficiency of kobj. However, this earlier context-debloating approach under-approximates the set of context-independent objects identified, limiting performance speedups. We introduce a novel context-debloating pre-analysis approach that identifies objects as context-dependent only when they are potentially precision-critical to kobj based on three general container-usage patterns. Our research finds that objects containing no fields of ”abstract” (i.e., open) types can be analyzed context-insensitively with negligible precision loss in real-world applications. We provide clear rules and efficient algorithms to recognize these patterns, selecting more context-independent objects for better debloating. We have implemented DebloaterX in the Qilin framework and will release it as an open-source tool. Our experimental results on 12 standard Java benchmarks and real-world programs show that DebloaterX selects 92.4\% of objects to be context-independent on average, enabling kobj to run significantly faster (an average of 19.3x when k = 2 and 150.2x when k = 3) and scale up to 8 more programs when k = 3, with only a negligible loss of precision (less than 0.2\%). Compared to state-of-the-art alternative pre-analyses in accelerating kobj, DebloaterX outperforms Zipper significantly in both precision and efficiency and outperforms Conch (the earlier context-debloating approach) in efficiency substantially while achieving nearly the same precision.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {256},
numpages = {30},
keywords = {pointer analysis, context debloating, container usage patterns}
}

@article{10.1145/3622833,
author = {Cai, Yuandao and Zhang, Charles},
title = {A Cocktail Approach to Practical Call Graph Construction},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622833},
doi = {10.1145/3622833},
abstract = {After decades of research, constructing call graphs for modern C-based software remains either imprecise or inefficient when scaling up to the ever-growing complexity. The main culprit is the difficulty of resolving function pointers, as precise pointer analyses are cubic in nature and become exponential when considering calling contexts. This paper takes a practical stance by first conducting a comprehensive empirical study of function pointer manipulations in the wild. By investigating 5355 indirect calls in five popular open-source systems, we conclude that, instead of the past uniform treatments for function pointers, a cocktail approach can be more effective in “squeezing” the number of difficult pointers to a minimum using a potpourri of cheap methods. In particular, we decompose the costs of constructing highly precise call graphs of big code by tailoring several increasingly precise algorithms and synergizing them into a concerted workflow. As a result, many indirect calls can be precisely resolved in an efficient and principled fashion, thereby reducing the final, expensive refinements. This is, in spirit, similar to the well-known cocktail medical therapy.  

The results are encouraging — our implemented prototype called Coral can achieve similar precision versus the previous field-, flow-, and context-sensitive Andersen-style call graph construction, yet scale up to millions of lines of code for the first time, to the best of our knowledge. Moreover, by evaluating the produced call graphs through the lens of downstream clients (i.e., use-after-free detection, thin slicing, and directed grey-box fuzzing), the results show that Coral can dramatically improve their effectiveness for better vulnerability hunting, understanding, and reproduction. More excitingly, we found twelve confirmed bugs (six impacted by indirect calls) in popular systems (e.g., MariaDB), spreading across multiple historical versions.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {257},
numpages = {33},
keywords = {pointer analyses, indirect calls, function pointers, Call graph construction}
}

@article{10.1145/3622834,
author = {Pal, Anjali and Saiki, Brett and Tjoa, Ryan and Richey, Cynthia and Zhu, Amy and Flatt, Oliver and Willsey, Max and Tatlock, Zachary and Nandi, Chandrakana},
title = {Equality Saturation Theory Exploration \`{a} la Carte},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622834},
doi = {10.1145/3622834},
abstract = {Rewrite rules are critical in equality saturation, an increasingly popular technique  
in optimizing compilers, synthesizers, and verifiers. Unfortunately,  
developing high-quality rulesets is difficult and error-prone. Recent  
work on automatically inferring rewrite rules does not scale to large  
terms or grammars, and existing rule inference tools are monolithic and  
opaque. Equality saturation users therefore struggle to guide inference and  
incrementally construct rulesets. As a result, most users still  
manually develop and maintain rulesets.  

This paper proposes Enumo, a new domain-specific language for  
programmable theory exploration. Enumo provides a small set of core  
operators that enable users to strategically guide rule inference and  
incrementally build rulesets. Short Enumo programs easily replicate  
results from state-of-the-art tools, but Enumo programs can also scale  
to infer deeper rules from larger grammars than prior approaches. Its  
composable operators even facilitate developing new strategies for  
ruleset inference. We introduce a new fast-forwarding strategy that does not require  
evaluating terms in the target language, and can thus support domains  
that were out of scope for prior work.  

We evaluate Enumo and fast-forwarding across a variety of domains. Compared to  
state-of-the-art techniques, enumo can synthesize better rulesets over a  
diverse set of domains, in some cases matching the effects of  
manually-developed rulesets in systems driven by equality saturation.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {258},
numpages = {29},
keywords = {program synthesis, equality saturation, Rewrite rules}
}

@software{10.5281/zenodo.8140951,
author = {Pal, Anjali and Saiki, Brett and Tjoa, Ryan and Richey, Cynthia and Zhu, Amy and Flatt, Oliver and Willsey, Max and Tatlock, Zachary and Nandi, Chandrakana},
title = {Reproduction package for "Equality Saturation Theory Exploration \`{a} la Carte"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8140951},
abstract = {
    <p>This is the software package that contains Enumo, a new domain-specific language presented in our paper “Equality Saturation Theory Exploration \`{a} la Carte.” It contains the code and tests for the Enumo DSL as well as automated scripts to replicate the five experiments presented in the paper.</p>

},
keywords = {equality saturation, program synthesis, Rewrite rules}
}

@article{10.1145/3622835,
author = {Murali, Adithya and Pe\~{n}a, Lucas and Jhala, Ranjit and Madhusudan, P.},
title = {Complete First-Order Reasoning for Properties of Functional Programs},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622835},
doi = {10.1145/3622835},
abstract = {Several practical tools for automatically verifying functional programs (e.g., Liquid Haskell and Leon for Scala programs) rely on a heuristic based on unrolling recursive function definitions followed by quantifier-free reasoning using SMT solvers. We uncover foundational theoretical properties of this heuristic, revealing that it can be generalized and formalized as a technique that is in fact complete for reasoning with combined First-Order theories of algebraic datatypes and background theories, where background theories support decidable quantifier-free reasoning. The theory developed in this paper explains the efficacy of these heuristics when they succeed, explain why they fail when they fail, and the precise role that user help plays in making proofs succeed.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {259},
numpages = {30},
keywords = {Thrifty Instantiation, Refinement Types, Natural Proofs, Liquid Haskell, First-Order Logic, Completeness, Algebraic Datatypes (ADTs)}
}

@article{10.1145/3622836,
author = {Tang, Wenhao and Hillerstr\"{o}m, Daniel and McKinna, James and Steuwer, Michel and Dardha, Ornela and Fu, Rongxiao and Lindley, Sam},
title = {Structural Subtyping as Parametric Polymorphism},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622836},
doi = {10.1145/3622836},
abstract = {Structural subtyping and parametric polymorphism provide similar flexibility and reusability to programmers. For example, both features enable the programmer to provide a wider record as an argument to a function that expects a narrower one. However, the means by which they do so differs substantially, and the precise details of the relationship between them exists, at best, as folklore in literature. In this paper, we systematically study the relative expressive power of structural subtyping and parametric polymorphism. We focus our investigation on establishing the extent to which parametric polymorphism, in the form of row and presence polymorphism, can encode structural subtyping for variant and record types. We base our study on various Church-style λ-calculi extended with records and variants, different forms of structural subtyping, and row and presence polymorphism. We characterise expressiveness by exhibiting compositional translations between calculi. For each translation we prove a type preservation and operational correspondence result. We also prove a number of non-existence results. By imposing restrictions on both source and target types, we reveal further subtleties in the expressiveness landscape, the restrictions enabling otherwise impossible translations to be defined. More specifically, we prove that full subtyping cannot be encoded via polymorphism, but we show that several restricted forms of subtyping can be encoded via particular forms of polymorphism.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {260},
numpages = {29},
keywords = {subtyping, row types, polymorphism, expressiveness}
}

@article{10.1145/3622837,
author = {Porncharoenwase, Sorawee and Pombrio, Justin and Torlak, Emina},
title = {A Pretty Expressive Printer},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622837},
doi = {10.1145/3622837},
abstract = {Pretty printers make trade-offs between the expressiveness of their pretty printing language, the optimality objective that they minimize when choosing between different ways to lay out a document, and the performance of their algorithm. This paper presents a new pretty printer, Πe, that is strictly more expressive than all pretty printers in the literature and provably minimizes an optimality objective. Furthermore, the time complexity of Πe is better than many existing pretty printers. When choosing among different ways to lay out a document, Πe consults a user-supplied cost factory, which determines the optimality objective, giving Πe a unique degree of flexibility. We use the Lean theorem prover to verify the correctness (validity and optimality) of Πe, and implement Πe concretely as a pretty printer that we call PrettyExpressive. To evaluate our pretty printer against others, we develop a formal framework for reasoning about the expressiveness of pretty printing languages, and survey pretty printers in the literature, comparing their expressiveness, optimality, worst-case time complexity, and practical running time. Our evaluation shows that PrettyExpressive is efficient and effective at producing optimal layouts. PrettyExpressive has also seen real-world adoption: it serves as a foundation of a code formatter for Racket.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {261},
numpages = {28},
keywords = {pretty printing}
}

@software{10.5281/zenodo.8332960,
author = {Porncharoenwase, Sorawee and Pombrio, Justin and Torlak, Emina},
title = {Artifact for A Pretty Expressive Printer},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8332960},
abstract = {
    <p>This artifact consists of - Lean proofs of PrettyExpressive’s functional correctness; - Rosette proofs related to cost factories; - An implementation of PrettyExpressive in OCaml and Racket, and their documentation; - A Racket code formatter that employs PrettyExpressive; and - Benchmarks to reproduce our evaluation to show that PrettyExpressive and practically efficient and optimal. See the README file for more details.</p>

},
keywords = {pretty printer}
}

@article{10.1145/3622838,
author = {Liu, Jiangyi and Zhu, Fengmin and He, Fei},
title = {Automated Ambiguity Detection in Layout-Sensitive Grammars},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622838},
doi = {10.1145/3622838},
abstract = {Layout-sensitive grammars have been adopted in many modern programming languages.  
In a serious language design phase, the specified syntax—typically a grammar—must be unambiguous.  
Although checking ambiguity is undecidable for context-free grammars and (trivially also) layout-sensitive grammars, ambiguity detection, on the other hand, is possible and can benefit language designers from exposing potential design flaws.  
  
In this paper, we tackle the ambiguity detection problem in layout-sensitive grammars.  
Inspired by a previous work on checking the bounded ambiguity of context-free grammars via SAT solving, we intensively extend their approach to support layout-sensitive grammars but via SMT solving to express the ordering and quantitative relations over line/column numbers.  
Our key novelty lies in a reachability condition, which takes the impact of layout constraints on ambiguity into careful account.  
With this condition in hand, we propose an equivalent ambiguity notion called local ambiguity for the convenience of SMT encoding.  
We translate local ambiguity into an SMT formula and developed a bounded ambiguity checker that automatically finds a shortest nonempty ambiguous sentence (if exists) for a user-input grammar.  
The soundness and completeness of our SMT encoding are mechanized in the Coq proof assistant.  
We conducted an evaluation on both grammar fragments and full grammars extracted from the language manuals of domain-specific languages like YAML as well as general-purpose languages like Python, which reveals the effectiveness of our approach.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {262},
numpages = {26},
keywords = {layout-sensitive grammar, ambiguity, SMT, Coq}
}

@software{10.5281/zenodo.8329981,
author = {Liu, Jiangyi and Zhu, Fengmin and He, Fei},
title = {Artifact of paper "Automated Ambiguity Detection in Layout-Sensitive Grammars"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8329981},
abstract = {
    <p>This is the artifact for the paper “Automated Ambiguity Detection in Layout-Sensitive Grammars” at OOPSLA’23. The main purpose of this artifact is to support our evaluation results in §7 (mostly Table 1) and the theoretical results in §3 – §5 (the main conclusions are Theorem 5.9 and Theorem 5.10).</p>
<p>This artifact consists of two parts (each is a directory):</p>
<ul>
<li><p>tool/: our prototype tool that implements the ambiguity detection approach (following §5), together with necessary data and scripts for reproducing the evaluation (§7);</p></li>
<li><p>proof/: our Coq mechanization (§6) of all the definitions and theorems mentioned in §3 – §5.</p></li>
</ul>

},
keywords = {ambiguity, Coq, layout-sensitive grammar, SMT}
}

@article{10.1145/3622839,
author = {Mehta, Meetesh Kalpesh and Krynski, Sebasti\'{a}n and Gualandi, Hugo Musso and Thakur, Manas and Vitek, Jan},
title = {Reusing Just-in-Time Compiled Code},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622839},
doi = {10.1145/3622839},
abstract = {Most code is executed more than once. If not entire programs then libraries remain unchanged from one run to the next. Just-in-time compilers expend considerable effort gathering insights about code they compiled many times, and often end up generating the same binary over and over again. We explore how to reuse compiled code across runs of different programs to reduce warm-up costs of dynamic languages. We propose to use speculative contextual dispatch to select versions of functions from an off-line curated code repository. That repository is a persistent database of previously compiled functions indexed by the context under which they were compiled. The repository is curated to remove redundant code and to optimize dispatch. We assess practicality by extending \v{R}, a compiler for the R language, and evaluating its performance. Our results suggest that the approach improves warmup times while preserving peak performance.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {263},
numpages = {22},
keywords = {Specialization, JIT compilation, Code reuse}
}

@software{10.5281/zenodo.8330884,
author = {Mehta, Meetesh Kalpesh and Krynski, Sebasti\'{a}n and Gualandi, Hugo Musso and Thakur, Manas and Vitek, Jan},
title = {Artifact of "Reusing Just-in-Time Compiled Code"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8330884},
abstract = {
    <p>This is the artifact to accompany our OOPSLA 2023 submission on “Reusing Just-in-Time Compiled Code”. The artifact consists of a virtual machine for the R language, called \v{R}, set of benchmarks for evaluation, and scripts to generate the plots. The instructions to run the artifact and reproduce the results are also attached (oopsla23aec-paper45-artifact_documentation.md).</p>

},
keywords = {Code reuse, JIT compilation, Specialization}
}

@article{10.1145/3622840,
author = {Sahebolamri, Arash and Barrett, Langston and Moore, Scott and Micinski, Kristopher},
title = {Bring Your Own Data Structures to Datalog},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622840},
doi = {10.1145/3622840},
abstract = {The restricted logic programming language Datalog has become a popular implementation target for deductive-analytic workloads including social-media analytics and program analysis. Modern Datalog engines compile Datalog rules to joins over explicit representations of relations—often B-trees or hash maps. While these modern engines have enabled high scalability in many application domains, they have a crucial weakness: achieving the desired algorithmic complexity may be impossible due to representation-imposed overhead of the engine’s data structures. In this paper, we present the "Bring Your Own Data Structures" (Byods) approach, in the form of a DSL embedded in Rust. Using Byods, an engineer writes logical rules which are implicitly parametric on the concrete data structure representation; our implementation provides an interface to enable "bringing their own" data structures to represent relations, which harmoniously interact with code generated by our compiler (implemented as Rust procedural macros). We formalize the semantics of Byods as an extension of Datalog’s; our formalization captures the key properties demanded of data structures compatible with Byods, including properties required for incrementalized (semi-na\"{\i}ve) evaluation. We detail many applications of the Byods approach, implementing analyses requiring specialized data structures for transitive and equivalence relations to scale, including an optimized version of the Rust borrow checker Polonius; highly-parallel PageRank made possible by lattices; and a large-scale analysis of LLVM utilizing index-sharing to scale. Our results show that Byods offers both improved algorithmic scalability (reduced time and/or space complexity) and runtimes competitive with state-of-the-art parallelizing Datalog solvers.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {264},
numpages = {26},
keywords = {Static Analysis, Program Analysis, Logic Programming, Datalog}
}

@software{10.5281/zenodo.8219447,
author = {Sahebolamri, Arash and Barrett, Langston and Moore, Scott and Micinski, Kristopher},
title = {Code and experiments for paper 'Bring Your Own Data Structures to Datalog'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8219447},
abstract = {
    <p>The artifact, packaged as a Docker image, contains the code for BYODS, a Datalog engine that allows custom relation-backing data structures in Datalog programs. In addition, the experiments presented in the paper ‘Bring Your Own Data Structures to Datalog’ are included in the artifact, allowing easy reproduction of experiments results. The file <code>README.MD</code> in the Docker image contains instructions for running the experiments.</p>

},
keywords = {data structures, Datalog, program analysis, union find}
}

@article{10.1145/3622841,
author = {Crichton, Will and Gray, Gavin and Krishnamurthi, Shriram},
title = {A Grounded Conceptual Model for Ownership Types in Rust},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622841},
doi = {10.1145/3622841},
abstract = {Programmers learning Rust struggle to understand ownership types, Rust’s core mechanism for ensuring memory safety without garbage collection. This paper describes our attempt to systematically design a pedagogy for ownership types. First, we studied Rust developers’ misconceptions of ownership to create the Ownership Inventory, a new instrument for measuring a person’s knowledge of ownership. We found that Rust learners could not connect Rust’s static and dynamic semantics, such as determining why an ill-typed program would (or would not) exhibit undefined behavior. Second, we created a conceptual model of Rust’s semantics that explains borrow checking in terms of flow-sensitive permissions on paths into memory. Third, we implemented a Rust compiler plugin that visualizes programs under the model. Fourth, we integrated the permissions model and visualizations into a broader pedagogy of ownership by writing a new ownership chapter for The Rust Programming Language, a popular Rust textbook. Fifth, we evaluated an initial deployment of our pedagogy against the original version, using reader responses to the Ownership Inventory as a point of comparison. Thus far, the new pedagogy has improved learner scores on the Ownership Inventory by an average of 9},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {265},
numpages = {29},
keywords = {program state visualization, ownership types, concept inventory, Rust}
}

@software{10.5281/zenodo.8317948,
author = {Crichton, Will and Gray, Gavin and Krishnamurthi, Shriram},
title = {Artifact for "A Grounded Conceptual Model for Ownership Types in Rust"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8317948},
abstract = {
    <p>These are Docker images that contain the codebase and evaluation scripts for our OOPSLA 2023 paper “A Grounded Conceptual Model for Ownership Types in Rust”.</p>

},
keywords = {concept inventory, ownership types, program state visualization, Rust}
}

@article{10.1145/3622842,
author = {Zhang, Quan and Zhou, Chijin and Xu, Yiwen and Yin, Zijing and Wang, Mingzhe and Su, Zhuo and Sun, Chengnian and Jiang, Yu and Sun, Jiaguang},
title = {Building Dynamic System Call Sandbox with Partial Order Analysis},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622842},
doi = {10.1145/3622842},
abstract = {Attack surface reduction is a security technique that secures the operating system by removing the unnecessary code or features of a program. By restricting the system calls that programs can use, the system call sandbox is able to reduce the exposed attack surface of the operating system and prevent attackers from damaging it through vulnerable programs. Ideally, programs should only retain access to system calls they require for normal execution. Many researchers focus on adopting static analysis to automatically restrict the system calls for each program. However, these methods do not adjust the restriction policy along with program execution. Thus, they need to permit all system calls required for program functionalities. We observe that some system calls, especially security-sensitive ones, are used a few times in certain stages of a program’s execution and then never used again. This motivates us to minimize the set of required system calls dynamically. In this paper, we propose , which gradually disables access to unnecessary system calls throughout the program’s execution. To accomplish this, we utilize partial order analysis to transform the program into a partially ordered graph, which enables efficient identification of the necessary system calls at any given point during program execution. Once a system call is no longer required by the program, can restrict it immediately. To evaluate , we applied it to seven widely-used programs with an average of 615 KLOC, including web servers and databases. With partial order analysis, restricts an average of 23.50, 16.86, and 15.89 more system calls than the state-of-the-art Chestnut, Temporal Specialization, and the configuration-aware sandbox, C2C, respectively. For mitigating malicious exploitations, on average, defeats 83.42\% of 1726 exploitation payloads with only a 5.07\% overhead.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {266},
numpages = {28},
keywords = {System Call Sandbox, Program Analysis, Attack Surface Reduction}
}

@software{10.5281/zenodo.8328524,
author = {Zhang, Quan and Zhou, Chijin and Xu, Yiwen and Yin, Zijing and Wang, Mingzhe and Su, Zhuo and Sun, Chengnian and Jiang, Yu and Sun, Jiaguang},
title = {Artifact for "Building Dynamic System Call Sandbox with Partial Order Analysis"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8328524},
abstract = {
    <p>This is the artifact for the OOPSLA 2023 paper, titled “Building Dynamic System Call Sandbox with Partial Order Analysis”. It contains the DynBox prototype as well as the relevant data needed to replicate the results.</p>

},
keywords = {Attack Surface Reduction, Program Analysis, System Call Sandbox}
}

@article{10.1145/3622843,
author = {Bianchini, Riccardo and Dagnino, Francesco and Giannini, Paola and Zucca, Elena},
title = {Resource-Aware Soundness for Big-Step Semantics},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622843},
doi = {10.1145/3622843},
abstract = {We extend the semantics and type system of a lambda calculus equipped with common constructs to be resource-aware. That is, reduction is instrumented to keep track of the usage of resources, and the type system guarantees, besides standard soundness, that for well-typed programs there is a computation where no needed resource gets exhausted. The resource-aware extension is parametric on an arbitrary grade algebra, and does not require ad-hoc changes to the underlying language. To this end, the semantics needs to be formalized in big-step style; as a consequence, expressing and proving (resource-aware) soundness is challenging, and is achieved by applying recent techniques based on coinductive reasoning.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {267},
numpages = {29},
keywords = {generalized inference systems, Graded modal types}
}

@article{10.1145/3622844,
author = {Liu, Fengyun and Lhot\'{a}k, Ond\v{r}ej and Hua, David and Xing, Enze},
title = {Initializing Global Objects: Time and Order},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622844},
doi = {10.1145/3622844},
abstract = {Object-oriented programming has been bothered by an awkward feature for a long time: static members. Static members not only compromise the conceptual integrity of object-oriented programming, but also give rise to subtle initialization errors, such as reading non-initialized fields and deadlocks. The Scala programming language eliminated static members from the language, replacing them with global objects that present a unified object-oriented programming model. However, the problem of global object initialization remains open, and programmers still suffer from initialization errors. We propose partial ordering and initialization-time irrelevance as two fundamental principles for initializing global objects. Based on these principles, we put forward an effective static analysis to ensure safe initialization of global objects, which eliminates initialization errors at compile time. The analysis also enables static scheduling of global object initialization to avoid runtime overhead. The analysis is modular at the granularity of objects and it avoids whole-program analysis. To make the analysis explainable and tunable, we introduce the concept of regions to make context-sensitivity understandable and customizable by programmers.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {268},
numpages = {28},
keywords = {region context, initialization-time irrelevance, initialization safety}
}

@software{10.5281/zenodo.8329922,
author = {Liu, Fengyun and Lhot\'{a}k, Ond\v{r}ej and Hua, David and Xing, Enze},
title = {Initializing Global Objects: Time and Order},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8329922},
abstract = {
    <p>An artifact for the OOPSLA 2023 paper Initializing Global Objects: Time and Order.</p>
<p>The interested audience can check out the latest Dotty, which already contains the checker as part of its source code.</p>

},
keywords = {Dotty, global objects, initialization safety, initialization-time irrelevance}
}

@article{10.1145/3622845,
author = {Mohan, Anshuman and Liu, Yunhe and Foster, Nate and Kapp\'{e}, Tobias and Kozen, Dexter},
title = {Formal Abstractions for Packet Scheduling},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622845},
doi = {10.1145/3622845},
abstract = {Early programming models for software-defined networking (SDN) focused on basic features for controlling network-wide forwarding paths, but more recent work has considered richer features, such as packet scheduling and queueing, that affect performance. In particular, PIFO trees, proposed by Sivaraman et al., offer a flexible and efficient primitive for programmable packet scheduling. Prior work has shown that PIFO trees can express a wide range of practical algorithms including strict priority, weighted fair queueing, and hierarchical schemes. However, the semantic properties of PIFO trees are not well understood. This paper studies PIFO trees from a programming language perspective. We formalize the syntax and semantics of PIFO trees in an operational model that decouples the scheduling policy running on a tree from the topology of the tree. Building on this formalization, we develop compilation algorithms that allow the behavior of a PIFO tree written against one topology to be realized using a tree with a different topology. Such a compiler could be used to optimize an implementation of PIFO trees, or realize a logical PIFO tree on a target with a fixed topology baked into the hardware. To support experimentation, we develop a software simulator for PIFO trees, and we present case studies illustrating its behavior on standard and custom algorithms.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {269},
numpages = {25},
keywords = {programmable scheduling, packet scheduling, formal semantics}
}

@software{10.5281/zenodo.8329703,
author = {Mohan, Anshuman and Liu, Yunhe and Foster, Nate and Kapp\'{e}, Tobias and Kozen, Dexter},
title = {Reproduction Package for 'Formal Abstractions for Packet Scheduling'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8329703},
abstract = {
    <p>This artifact contains an implementation of PIFO trees as described in the paper, along with several key definitions and concepts. The implementation obeys the semantics we describe formally in the paper. Further, the artifact contains an implementation of the embedding algorithm that we describe in our paper, along with a simulator that allows a PCAP of packets to be “run” through a PIFO tree scheduler. There is small tool to generate your own synthetic PCAPs, and also a visualization tool that generates the graphs that we show in our paper.</p>

},
keywords = {formal semantics, packet scheduling, programmable scheduling}
}

@article{10.1145/3622846,
author = {Arvidsson, Ellen and Castegren, Elias and Clebsch, Sylvan and Drossopoulou, Sophia and Noble, James and Parkinson, Matthew J. and Wrigstad, Tobias},
title = {Reference Capabilities for Flexible Memory Management},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622846},
doi = {10.1145/3622846},
abstract = {Verona is a concurrent object-oriented programming language that organises all the objects in a program into a forest of isolated regions. Memory is managed locally for each region, so programmers can control a program's memory use by adjusting objects' partition into regions, and by setting each region's memory management strategy. A thread can only mutate (allocate, deallocate) objects within one active region---its "window of mutability". Memory management costs are localised to the active region, ensuring overheads can be predicted and controlled. Moving the mutability window between regions is explicit, so code can be executed wherever it is required, yet programs remain in control of memory use. An ownership type system based on reference capabilities enforces region isolation, controlling aliasing within and between regions, yet supporting objects moving between regions and threads. Data accesses never need expensive atomic operations, and are always thread-safe.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {270},
numpages = {31},
keywords = {type systems, ownership, memory management, isolation}
}

@article{10.1145/3622847,
author = {Thakkar, Aalok and Sands, Nathaniel and Petrou, George and Alur, Rajeev and Naik, Mayur and Raghothaman, Mukund},
title = {Mobius: Synthesizing Relational Queries with Recursive and Invented Predicates},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622847},
doi = {10.1145/3622847},
abstract = {Synthesizing relational queries from data is challenging in the presence of recursion and invented predicates. We propose a fully automated approach to synthesize such queries. Our approach comprises of two steps: it first synthesizes a non-recursive query consistent with the given data, and then identifies recursion schemes in it and thereby generalizes to arbitrary data. This generalization is achieved by an iterative predicate unification procedure which exploits the notion of data provenance to accelerate convergence. In each iteration of the procedure, a constraint solver proposes a candidate query, and a query evaluator checks if the proposed program is consistent with the given data. The data provenance for a failed query allows us to construct additional constraints for the constraint solver and refine the search. We have implemented our approach in a tool named Mobius. On a suite of 21 challenging recursive query synthesis tasks, Mobius outperforms three state-of-the-art baselines Gensynth, ILASP, and Popper, both in terms of runtime and accuracy. We also demonstrate that the synthesized queries generalize well to unseen data.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {271},
numpages = {24},
keywords = {recursive program synthesis, example-guided synthesis, Programming by example}
}

@article{10.1145/3622848,
author = {Zhou, Jin and Silvestro, Sam and Tang, Steven (Jiaxun) and Yang, Hanmei and Liu, Hongyu and Zeng, Guangming and Wu, Bo and Liu, Cong and Liu, Tongping},
title = {MemPerf: Profiling Allocator-Induced Performance Slowdowns},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622848},
doi = {10.1145/3622848},
abstract = {The memory allocator plays a key role in the performance of applications, but none of the existing profilers can pinpoint performance slowdowns caused by a memory allocator. Consequently, programmers may spend time improving application code incorrectly or unnecessarily, achieving low or no performance improvement. This paper designs the first profiler—MemPerf—to identify allocator-induced performance slowdowns without comparing against another allocator. Based on the key observation that an allocator may impact the whole life-cycle of heap objects, including the accesses (or uses) of these objects, MemPerf proposes a life-cycle based detection to identify slowdowns caused by slow memory management operations and slow accesses separately. For the prior one, MemPerf proposes a thread-aware and type-aware performance modeling to identify slow management operations. For slow memory accesses, MemPerf utilizes a top-down approach to identify all possible reasons for slow memory accesses introduced by the allocator, mainly due to cache and TLB misses, and further proposes a unified method to identify them correctly and efficiently. Based on our extensive evaluation, MemPerf reports 98\% medium and large allocator-reduced slowdowns (larger than 5\%) correctly without reporting any false positives. MemPerf also pinpoints multiple known and unknown design issues in widely-used allocators.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {272},
numpages = {24},
keywords = {Performance Slowdowns, Memory Allocator}
}

@article{10.1145/3622849,
author = {Linvill, Kirby and Kaki, Gowtham and Wustrow, Eric},
title = {Verifying Indistinguishability of Privacy-Preserving Protocols},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622849},
doi = {10.1145/3622849},
abstract = {Internet users rely on the protocols they use to protect their private  
information including their identity and the websites they visit. Formal  
verification of these protocols can detect subtle bugs that compromise these  
protections at design time, but is a challenging task as it involves  
probabilistic reasoning about random sampling, cryptographic primitives, and  
concurrent execution. Existing approaches either reason about symbolic  
models of the protocols that sacrifice precision for automation, or reason  
about more precise computational models that are harder to automate and  
require cryptographic expertise. In this paper we propose a novel approach  
to verifying privacy-preserving protocols that is more precise than symbolic  
models yet more accessible than computational models. Our approach permits  
direct-style proofs of privacy, as opposed to indirect game-based  
proofs in computational models, by formalizing privacy as  
indistinguishability of possible network traces induced by a  
protocol. We ease automation by leveraging insights from the distributed  
systems verification community to create sound synchronous models of  
concurrent protocols. Our verification framework is implemented in F* as a  
library we call Waldo. We describe two large case studies of using Waldo  
to verify indistinguishability; one on the Encrypted Client Hello  
(ECH) extension of the TLS protocol and another on a Private  
Information Retrieval (PIR) protocol. We uncover subtle flaws in the TLS  
ECH specification that were missed by other models.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {273},
numpages = {28},
keywords = {Synchronization, Protocol Verification, Privacy, Indistinguishability, Concurrency}
}

@article{10.1145/3622850,
author = {Ma, Cong and Wu, Dinghao and Tan, Gang and Kandemir, Mahmut Taylan and Zhang, Danfeng},
title = {Quantifying and Mitigating Cache Side Channel Leakage with Differential Set},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622850},
doi = {10.1145/3622850},
abstract = {Cache side-channel attacks leverage secret-dependent footprints in CPU cache to steal confidential information, such as encryption keys. Due to the lack of a proper abstraction for reasoning about cache side channels, existing static program analysis tools that can quantify or mitigate cache side channels are built on very different kinds of abstractions. As a consequence, it is hard to bridge advances in quantification and mitigation research. Moreover, existing abstractions lead to imprecise results. In this paper, we present a novel abstraction, called differential set, for analyzing cache side channels at compile time. A distinguishing feature of differential sets is that it allows compositional and precise reasoning about cache side channels. Moreover, it is the first abstraction that carries sufficient information for both side channel quantification and mitigation. Based on this new abstraction, we develop a static analysis tool DSA that automatically quantifies and mitigates cache side channel leakage at the same time. Experimental evaluation on a set of commonly used benchmarks shows that DSA can produce more precise leakage bound as well as mitigated code with fewer memory footprints, when compared with state-of-the-art tools that only quantify or mitigate cache side channel leakage.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {274},
numpages = {29},
keywords = {side channels, information flow, differential set}
}

@software{10.5281/zenodo.8418984,
author = {Ma, Cong and Wu, Dinghao and Tan, Gang and Kandemir, Mahmut Taylan and Zhang, Danfeng},
title = {Artiract for "Quantifying and Mitigating Cache Side Channel Leakage with Differential Set"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8418984},
abstract = {
    <p>DSA is a tool that quantifies and mitigate the cache side channel leakage of a program. Part of the implementation is a modified model-checker CBMC, which takes in C code and outputs the constraints of the program in either DIMACS and SMT-LIB v.2 format depending on the task. The constraints are analyzed and transformed through a series of Python scripts. For quantification task, we use approxMC to count the number of solutions of the constraints. For mitigation task, we use Z3 to enumerate each differential set. For more information please check the paper</p>

},
keywords = {differential set, information flow, side channels}
}

@article{10.1145/3622851,
author = {Rennels, Lisa and Chasins, Sarah E.},
title = {How Domain Experts Use an Embedded DSL},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622851},
doi = {10.1145/3622851},
abstract = {Programming tools are increasingly integral to research and analysis in myriad domains, including specialized areas with no formal relation to computer science. Embedded domain-specific languages (eDSLs) have the potential to serve these programmers while placing relatively light implementation burdens on language designers. However, barriers to eDSL use reduce their practical value and adoption. In this paper, we aim to deepen our understanding of how programmers use eDSLs and identify user needs to inform future eDSL designs. We performed a contextual inquiry (9 participants) with domain experts using Mimi, an eDSL for climate change economics modeling. A thematic analysis identified five key themes, including: the interaction between the eDSL and the host language has significant and sometimes unexpected impacts on eDSL user experience, and users preferentially engage with domain-specific communities and code templates rather than host language resources. The needs uncovered in our study offer design considerations for future eDSLs and suggest directions for future DSL usability research.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {275},
numpages = {32},
keywords = {user experience, usability, need finding, embedded domain-specific languages, contextual inquiry}
}

@article{10.1145/3622852,
author = {Cheeseman, Luke and Parkinson, Matthew J. and Clebsch, Sylvan and Kogias, Marios and Drossopoulou, Sophia and Chisnall, David and Wrigstad, Tobias and Li\'{e}tar, Paul},
title = {When Concurrency Matters: Behaviour-Oriented Concurrency},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622852},
doi = {10.1145/3622852},
abstract = {Expressing parallelism and coordination is central for modern concurrent programming. Many mechanisms  
exist for expressing both parallelism and coordination. However, the design decisions for these two mechanisms  
are tightly intertwined. We believe that the interdependence of these two mechanisms should be recognised  
and achieved through a single, powerful primitive. We are not the first to realise this: the prime example  
is actor model programming, where parallelism arises through fine-grained decomposition of a program’s  
state into actors that are able to execute independently in parallel. However, actor model programming has a  
serious pain point: updating multiple actors as a single atomic operation is a challenging task.  
We address this pain point by introducing a new concurrency paradigm: Behaviour-Oriented Concurrency  
(BoC). In BoC, we are revisiting the fundamental concept of a behaviour to provide a more transactional  
concurrency model. BoC enables asynchronously creating atomic and ordered units of work with exclusive  
access to a collection of independent resources.  
In this paper, we describe BoC informally in terms of examples, which demonstrate the advantages of  
exclusive access to several independent resources, as well as the need for ordering. We define it through a  
formal model. We demonstrate its practicality by implementing a C++ runtime. We argue its applicability  
through the Savina benchmark suite: benchmarks in this suite can be more compactly represented using BoC  
in place of Actors, and we observe comparable, if not better, performance.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {276},
numpages = {30},
keywords = {actors}
}

@software{10.5281/zenodo.8320212,
author = {Cheeseman, Luke and Parkinson, Matthew J. and Clebsch, Sylvan and Kogias, Marios and Drossopoulou, Sophia and Chisnall, David and Wrigstad, Tobias and Li\'{e}tar, Paul},
title = {Artifact for "When Concurrency Matters: Behaviour Oriented Concurrency"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8320212},
abstract = {
    <p>This contains the docker image to support the OOPSLA 2023 paper “When Concurrency Matters: Behavioural Oriented Concurrency”.</p>

},
keywords = {actors, concurrent programming, parallel programming, threads and locks}
}

@article{10.1145/3622853,
author = {D'Souza, Matt and You, James and Lhot\'{a}k, Ond\v{r}ej and Prokopec, Aleksandar},
title = {TASTyTruffle: Just-in-Time Specialization of Parametric Polymorphism},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622853},
doi = {10.1145/3622853},
abstract = {Parametric polymorphism enables programmers to express algorithms independently of the types of values that they operate on. The approach used to implement parametric polymorphism can have important performance implications. One popular approach, erasure, uses a uniform representation for generic data, which entails primitive boxing and other indirections that harm performance. Erasure destroys type information that could be used by language implementations to optimize generic code.  

We present TASTyTruffle, an implementation for a subset of the Scala programming language. Instead of JVM bytecode, TASTyTruffle interprets Scala's TASTy intermediate representation, a typed representation wherein generic types are not erased. TASTy's precise type information empowers TASTyTruffle to implement generic code more effectively. In particular, it allows TASTyTruffle to reify types as run-time objects that can be passed around. Using reified types, TASTyTruffle supports heterogeneous box-free representations for generic values. TASTyTruffle also uses reified types to specialize generic code, producing monomorphic copies of generic code that can be easily and reliably optimized by its just-in-time (JIT) compiler.  

Empirically, TASTyTruffle is competitive with standard JVM implementations on a small set of benchmark programs; when generic code is used with multiple types, TASTyTruffle consistently outperforms the JVM. The precise type information in TASTy enables TASTyTruffle to find additional optimization opportunities that could not be uncovered with erased JVM bytecode.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {277},
numpages = {28},
keywords = {specialization, reified types, parametric polymorphism, just-in-time compiler, Truffle, Scala}
}

@software{10.5281/zenodo.8332577,
author = {D'Souza, Matt and You, James and Lhot\'{a}k, Ond\v{r}ej and Prokopec, Aleksandar},
title = {Artifact for paper "TASTyTruffle: Just-in-time Specialization of Parametric Polymorphism"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8332577},
abstract = {
    <p>This is the accompanying artifact for the paper “TASTyTruffle: Just-in-time Specialization of Parametric Polymorphism”. It contains the source code for TASTyTruffle and the accompanying benchmark scripts required to execute the benchmarks included in the evaluation.</p>

},
keywords = {just-in-time compiler, parametric polymorphism, reified types, Scala, specialization, Truffle}
}

@article{10.1145/3622854,
author = {Iraci, Grant and Chuang, Cheng-En and Hu, Raymond and Ziarek, Lukasz},
title = {Validating IoT Devices with Rate-Based Session Types},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622854},
doi = {10.1145/3622854},
abstract = {We develop a session types based framework for implementing and validating rate-based message passing systems in Internet of Things (IoT) domains. To model the indefinite repetition present in many embedded and IoT systems, we introduce a timed process calculus with a periodic recursion primitive. This allows us to model rate-based computations and communications inherent to these application domains. We introduce a definition of rate based session types in a binary session types setting and a new compatibility relationship, which we call rate compatibility. Programs which type check enjoy the standard session types guarantees as well as rate error freedom --- meaning processes which exchanges messages do so at the same rate. Rate compatibility is defined through a new notion of type expansion, a relation that allows communication between processes of differing periods by synthesizing and checking a common superperiod type. We prove type preservation and rate error freedom for our system, and show a decidable method for type checking based on computing superperiods for a collection of processes. We implement a prototype of our type system including rate compatibility via an embedding into the native type system of Rust. We apply this framework to a range of examples from our target domain such as Android software sensors, wearable devices, and sound processing.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {278},
numpages = {29},
keywords = {type systems, session types, rate-based systems}
}

@software{10.1145/3580415,
author = {Iraci, Grant and Chuang, Cheng-En and Hu, Raymond and Ziarek, Lukasz},
title = {Rate Based Session Types: Rust Implementation},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3580415},
abstract = {
    <p>This artifact contains a prototype implementation of rate-based session types as a Rust crate. The overall artifact is split into two parts: the Rust standard library based crate implementation in rust_pst and a version specialized to FreeRTOS on the STM32F407 in STM32_FreeRTOS.</p>

},
keywords = {rate-based systems, session types, type systems}
}

@article{10.1145/3622855,
author = {Haas, Thomas and Maseli, Ren\'{e} and Meyer, Roland and Ponce de Le\'{o}n, Hern\'{a}n},
title = {Static Analysis of Memory Models for SMT Encodings},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622855},
doi = {10.1145/3622855},
abstract = {The goal of this work is to improve the efficiency of bounded model checkers that are modular in the memory model.  
Our first contribution is a static analysis for the given memory model that is performed as a preprocessing step and helps us significantly reduce the encoding size.  
Memory model make use of relations to judge whether an execution is consistent. The analysis computes bounds on these relations: which pairs of events may or must be related.  
What is new is that the bounds are relativized to the execution of events.  
This makes it possible to derive, for the first time, not only upper but also meaningful lower bounds.  
Another important feature is that the analysis can import information about the verification instance from external sources to improve its precision.  
Our second contribution are new optimizations for the SMT encoding.  
Notably, the lower bounds allow us to simplify the encoding of acyclicity constraints.  
We implemented our analysis and optimizations within a bounded model checker and evaluated it on challenging benchmarks.  
The evaluation shows up-to 40\% reduction in verification time (including the analysis) over previous encodings.  
Our optimizations allow us to efficiently check safety, liveness, and data race freedom in Linux kernel code.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {279},
numpages = {30},
keywords = {weak memory models, bounded model checking, Abstract interpretation}
}

@software{10.5281/zenodo.8313104,
author = {Haas, Thomas and Maseli, Ren\'{e} and Meyer, Roland and Ponce de Le\'{o}n, Hern\'{a}n},
title = {Static Analysis of Memory Models for SMT Encodings (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8313104},
abstract = {
    <p>This artifact allows for reproducing the results of Section 6 of the paper Static Analysis of Memory Models for SMT Encodings published OOPSLA2023.</p>

},
keywords = {Abstract interpretation, axiomatic semantics, verification, weak memory models}
}

@article{10.1145/3622856,
author = {Renda, Alex and Ding, Yi and Carbin, Michael},
title = {Turaco: Complexity-Guided Data Sampling for Training Neural Surrogates of Programs},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622856},
doi = {10.1145/3622856},
abstract = {Programmers and researchers are increasingly developing surrogates of programs, models of a subset of the observable behavior of a given program, to solve a variety of software development challenges. Programmers train surrogates from measurements of the behavior of a program on a dataset of input examples. A key challenge of surrogate construction is determining what training data to use to train a surrogate of a given program.  

We present a methodology for sampling datasets to train neural-network-based surrogates of programs. We first characterize the proportion of data to sample from each region of a program's input space (corresponding to different execution paths of the program) based on the complexity of learning a surrogate of the corresponding execution path. We next provide a program analysis to determine the complexity of different paths in a program. We evaluate these results on a range of real-world programs, demonstrating that complexity-guided sampling results in empirical improvements in accuracy.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {280},
numpages = {29},
keywords = {surrogate models, programming languages, neural networks}
}

@software{10.5281/zenodo.8353069,
author = {Renda, Alex and Ding, Yi and Carbin, Michael},
title = {Artifact for OOPSLA 2023 Paper "Turaco: Complexity-Guided Data Sampling for Training Neural Surrogates of Programs"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8353069},
abstract = {
    <p>This repository contains the implementation of the Turaco programming language and its analysis, and the experiments in the paper “Turaco: Complexity-Guided Data Sampling for Training Neural Surrogates of Programs”.</p>

},
keywords = {neural networks, programming languages, surrogate models}
}

@article{10.1145/3622857,
author = {Cho, Minki and Song, Youngju and Lee, Dongjae and G\"{a}her, Lennard and Dreyer, Derek},
title = {Stuttering for Free},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622857},
doi = {10.1145/3622857},
abstract = {One of the most common tools for proving behavioral refinements between transition systems is the method of simulation proofs, which has been explored extensively over the past several decades. Stuttering simulations are an extension of traditional simulations—used, for example, in CompCert—in which either the source or target of the simulation is permitted to “stutter” (stay in place) while the other side steps forward. In the interest of ensuring soundness, however, existing stuttering simulations restrict proofs to only perform a finite number of stuttering steps before making synchronous progress—a step of reasoning in which both sides of the simulation progress forward together. This restriction guarantees that a terminating program cannot be proven to simulate a non-terminating one. In this paper, we observe that the requirement to eventually achieve synchronous progress is burdensome and, what’s more, unnecessary: it is possible to ensure soundness of stuttering simulations while only requiring asynchronous progress (progress on both sides of the simulation that may be achieved with only stuttering steps). Building on this observation, we develop a new simulation technique we call FreeSim (short for “freely-stuttering simulations”), mechanized in Coq, and we demonstrate its effectiveness on a range of interesting case studies. These include a simplification of the meta-theory of CompCert, as well as the DTrees library, which enriches the ITrees (Interaction Trees) library with dual non-determinism.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {281},
numpages = {28},
keywords = {verification, stuttering simulation, Coq}
}

@software{10.5281/zenodo.8331740,
author = {Cho, Minki and Song, Youngju and Lee, Dongjae and G\"{a}her, Lennard and Dreyer, Derek},
title = {Stuttering For Free},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8331740},
abstract = {
    <p>This is the artifact for the paper “Stuttering For Free”.</p>

},
keywords = {Coq, stuttering simulation, verification}
}

@article{10.1145/3622858,
author = {Shadab, Narges and Gharat, Pritam and Tiwari, Shrey and Ernst, Michael D. and Kellogg, Martin and Lahiri, Shuvendu K. and Lal, Akash and Sridharan, Manu},
title = {Inference of Resource Management Specifications},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622858},
doi = {10.1145/3622858},
abstract = {A resource leak occurs when a program fails to free some finite resource after it is no longer needed. Such leaks are a significant cause of real-world crashes and performance problems. Recent work proposed an approach to prevent resource leaks based on checking resource management specifications. A resource management specification expresses how the program allocates resources, passes them around, and releases them; it also tracks the ownership relationship between objects and resources, and aliasing relationships between objects. While this specify-and-verify approach has several advantages compared to prior techniques, the need to manually write annotations presents a significant barrier to its practical adoption.  

This paper presents a novel technique to automatically infer a resource management specification for a program, broadening the applicability of specify-and-check verification for resource leaks. Inference in this domain is challenging because resource management specifications differ significantly in nature from the types that most inference techniques target. Further, for practical effectiveness, we desire a technique that can infer the resource management specification intended by the developer, even in cases when the code does not fully adhere to that specification. We address these challenges through a set of inference rules carefully designed to capture real-world coding patterns, yielding an effective fixed-point-based inference algorithm.  

We have implemented our inference algorithm in two different systems, targeting programs written in Java and C#. In an experimental evaluation, our technique inferred 85.5\% of the annotations that programmers had written manually for the benchmarks. Further, the verifier issued nearly the same rate of false alarms with the manually-written and automatically-inferred annotations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {282},
numpages = {24},
keywords = {typestate analysis, static analysis, specify-and-verify, specify-and-check, resource leaks, accumulation analysis, Pluggable type systems}
}

@software{10.5281/zenodo.8333055,
author = {Shadab, Narges and Gharat, Pritam and Tiwari, Shrey and Ernst, Michael D. and Kellogg, Martin and Lahiri, Shuvendu K. and Lal, Akash and Sridharan, Manu},
title = {Inference of Resource Management Specifications},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8333055},
abstract = {
    <p>This artifact contains the data and analysis supporting the evaluation section in the ‘Inference of Resource Management Specifications’ paper. It includes the implementation of the inference algorithm for resource management specifications in both Java and C#, along with instructions for running the tools on the benchmarks we used to evaluate our tools. The goal of these tools is to automatically infer specifications for the Resource Leak Checker and RLC#.</p>

},
keywords = {accumulation analysis, Pluggable type systems, resource leaks, specify-and-check, specify-and-verify, static analysis, typestate analysis}
}

@article{10.1145/3622859,
author = {Chitre, Khushboo and Kedia, Piyus and Purandare, Rahul},
title = {Rapid: Region-Based Pointer Disambiguation},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622859},
doi = {10.1145/3622859},
abstract = {Interprocedural alias analyses often sacrifice precision for scalability. Thus, modern compilers such as GCC and LLVM implement more scalable but less precise intraprocedural alias analyses. This compromise makes the compilers miss out on potential optimization opportunities, affecting the performance of the application. Modern compilers implement loop-versioning with dynamic checks for pointer disambiguation to enable the missed optimizations. Polyhedral access range analysis and symbolic range analysis enable 𝑂 (1) range checks for non-overlapping of memory accesses inside loops. However, these approaches work only for the loops in which the loop bounds are loop invariants. To address this limitation, researchers proposed a technique that requires 𝑂 (𝑙𝑜𝑔 𝑛) memory accesses for pointer disambiguation. Others improved the performance of dynamic checks to single memory access by constraining the object size and alignment. However, the former approach incurs noticeable overhead due to its dynamic checks, whereas the latter has a noticeable allocator overhead. Thus, scalability remains a challenge.  

In this work, we present a tool, Rapid, that further reduces the overheads of the allocator and dynamic checks proposed in the existing approaches. The key idea is to identify objects that need disambiguation checks using a profiler and allocate them in different regions, which are disjoint memory areas. The disambiguation checks simply compare the regions corresponding to the objects. The regions are aligned such that the top 32 bits in the addresses of any two objects allocated in different regions are always different. As a consequence, the dynamic checks do not require any memory access to ensure that the objects belong to different regions, making them efficient.  

Rapid achieved a maximum performance benefit of around 52.94\% for Polybench and 1.88\% for CPU SPEC 2017 benchmarks. The maximum CPU overhead of our allocator is 0.57\% with a geometric mean of -0.2\% for CPU SPEC 2017 benchmarks. Due to the low overhead of the allocator and dynamic checks, Rapid could improve the performance of 12 out of 16 CPU SPEC 2017 benchmarks. In contrast, a state-of-the-art approach used in the comparison could improve only five CPU SPEC 2017 benchmarks.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {283},
numpages = {29},
keywords = {regions, optimizations, memory allocation, dynamic checks, allocation site, alias analysis, LLVM}
}

@software{10.5281/zenodo.8321488,
author = {Chitre, Khushboo and Kedia, Piyus and Purandare, Rahul},
title = {Rapid: Region-based Pointer Disambiguation},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8321488},
abstract = {
    <p>This includes the source code of Rapid and all the required artifacts.</p>

},
keywords = {CPU SPEC 2017, LLVM, Mimalloc, Polybench, Scout}
}

@article{10.1145/3622860,
author = {New, Max S. and Giovannini, Eric and Licata, Daniel R.},
title = {Gradual Typing for Effect Handlers},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622860},
doi = {10.1145/3622860},
abstract = {We present a gradually typed language, GrEff, with effects and handlers that supports migration from unchecked to checked effect typing. This serves as a simple model of the integration of an effect typing discipline with an existing effectful typed language that does not track fine-grained effect information. Our language supports a simple module system to model the programming model of gradual migration from unchecked to checked effect typing in the style of Typed Racket.  

The surface language GrEff is given semantics by elaboration to a core language Core GrEff. We equip Core GrEff with an inequational theory for reasoning about the semantic error ordering and desired program equivalences for programming with effects and handlers. We derive an operational semantics for the language from the equations provable in the theory. We then show that the theory is sound by constructing an operational logical relations model to prove the graduality theorem. This extends prior work on embedding-projection pair models of gradual typing to handle effect typing and subtyping.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {284},
numpages = {29},
keywords = {operational semantics, logical relation, graduality, gradual typing, effect handlers}
}

@article{10.1145/3622861,
author = {Park, Kanghee and D'Antoni, Loris and Reps, Thomas},
title = {Synthesizing Specifications},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622861},
doi = {10.1145/3622861},
abstract = {Every program should be accompanied by a specification that describes important aspects of the code's behavior, but writing good specifications is often harder than writing the code itself. This paper addresses the problem of synthesizing specifications automatically, guided by user-supplied inputs of two kinds: i) a query posed about a set of function definitions, and ii) a domain-specific language L in which the extracted property is to be expressed (we call properties in the language L-properties). Each of the property is a best L-property for the query: there is no other L-property that is strictly more precise. Furthermore, the set of synthesized L-properties is exhaustive: no more L-properties can be added to it to make the conjunction more precise.  
We implemented our method in a tool, Spyro. The ability to modify both the query and L provides a Spyro user with ways to customize the kind of specification to be synthesized. We use this ability to show that Spyro can be used in a variety of applications, such as mining program specifications, performing abstract-domain operations, and synthesizing algebraic properties of program modules.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {285},
numpages = {30},
keywords = {Program Synthesis, Program Specifications}
}

@software{10.5281/zenodo.8327699,
author = {Park, Kanghee and D'Antoni, Loris and Reps, Thomas},
title = {Docker Image for Article 'Synthesizing Specifications'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8327699},
abstract = {
    <p>This is the artifact for paper “Synthesizing Specifications”. Following are the contents of the artifact.</p>
<ul>
<li>spyro_oopsla23.tar.gz: A Docker image containing the source code and the dependencies to run Spyro[SMT] and Spyro[Sketch].</li>
<li>README.md: A readme containing all the step-by-step instructions to reproduce the results shown in the paper.</li>
</ul>

},
keywords = {Program Specifications, Program Synthesis}
}

@article{10.1145/3622862,
author = {Gao, Pengfei and Zhang, Yedi and Song, Fu and Chen, Taolue and Standaert, Francois-Xavier},
title = {Compositional Verification of Efficient Masking Countermeasures against Side-Channel Attacks},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622862},
doi = {10.1145/3622862},
abstract = {Masking is one of the most effective countermeasures for securely implementing cryptographic algorithms against power side-channel attacks, the design of which however turns out to be intricate and error-prone. While techniques have been proposed to rigorously verify implementations of cryptographic algorithms, currently they are limited in scalability. To address this issue, compositional approaches have been investigated, but insofar they fail to prove the security of recent efficient implementations. To fill this gap, we propose a novel compositional verification approach. In particular, we introduce two new language-level security notions based on which we propose composition strategies and verification algorithms. Our approach is able to prove efficient implementations, which cannot be done by prior compositional approaches. We implement our approach as a tool CONVINCE and conduct extensive experiments to confirm its efficacy. We also use CONVINCE to further explore the design space of the AES Sbox with least refreshing by replacing its implementation for finite-field multiplication with more efficient counterparts. We automatically prove leakage-freeness of these new versions. As a result, we can effectively reduce 1,600 randomness and 3,200 XOR-operations of the state-of-the-art AES implementation.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {286},
numpages = {31},
keywords = {power side-channel attacks, cryptographic implementations, countermeasures, compositional reasoning, Formal verification}
}

@software{10.5281/zenodo.8416208,
author = {Gao, Pengfei and Zhang, Yedi and Song, Fu and Chen, Taolue and Standaert, Francois-Xavier},
title = {Peproduction Package for Article "Compositional Verification of Efficient Masking Countermeasures against Side-Channel Attacks"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8416208},
abstract = {
    <p>CONVINCE is the artifact of paper “Compositional Verification of Efficient Masking Countermeasures against Side-Channel Attacks”.</p>

},
keywords = {compositional reasoning, countermeasures, cryptographic implementations, Formal verification, power side-channel attacks}
}

@article{10.1145/3622863,
author = {Chen, Qiaochu and Banerjee, Arko and Demiralp, \c{C}a\u{g}atay and Durrett, Greg and Dillig, I\c{s}\i{}l},
title = {Data Extraction via Semantic Regular Expression Synthesis},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622863},
doi = {10.1145/3622863},
abstract = {Many data extraction tasks of practical relevance require not only syntactic pattern matching but also semantic reasoning about the content of the underlying text. While regular expressions are very well suited for tasks that require only syntactic pattern matching, they fall short for data extraction tasks that involve both a syntactic and semantic component. To address this issue, we introduce semantic regexes, a generalization of regular expressions that facilitates combined syntactic and semantic reasoning about textual data. We also propose a novel learning algorithm that can synthesize semantic regexes from a small number of positive and negative examples. Our proposed learning algorithm uses a combination of neural sketch generation and compositional type-directed synthesis for fast and effective generalization from a small number of examples.  We have implemented these ideas in a new tool called Smore and evaluated it on representative data extraction tasks involving several textual datasets. Our evaluation shows that semantic regexes can better support complex data extraction tasks than standard regular expressions and that our learning algorithm significantly outperforms existing tools, including state-of-the-art neural networks and program synthesis tools.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {287},
numpages = {30},
keywords = {Regular Expression, Program Synthesis}
}

@software{10.5281/zenodo.8144182,
author = {Chen, Qiaochu and Banerjee, Arko and Demiralp, \c{C}a\u{g}atay and Durrett, Greg and Dillig, I\c{s}\i{}l},
title = {Reproduction Package for Article 'Data Extraction via Semantic Regular Expression Synthesis'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8144182},
abstract = {
    <p>This repository contains the code artifact for evaluation section of the paper “Data Extraction via Semantic Regular Expression Synthesis”. The included README file contains further instructions on how to reproduce the evaluations.</p>

},
keywords = {Programming by examples, regular expressions}
}

@article{10.1145/3622864,
author = {Tamir, Orr and Taube, Marcelo and McMillan, Kenneth L. and Shoham, Sharon and Howell, Jon and Gueta, Guy and Sagiv, Mooly},
title = {Counterexample Driven Quantifier Instantiations with Applications to Distributed Protocols},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622864},
doi = {10.1145/3622864},
abstract = {Formally verifying infinite-state systems can be a daunting task, especially when it comes to reasoning about quantifiers. In particular, quantifier alternations in conjunction with function symbols can create function cycles that result in infinitely many ground terms, making it difficult for solvers to instantiate quantifiers and causing them to diverge. This can  
leave users with no useful information on how to proceed.  
		  
To address this issue, we propose an interactive verification methodology that uses a relational abstraction technique to mitigate solver divergence in the presence of quantifiers. This technique abstracts functions in the verification conditions (VCs) as one-to-one relations, which avoids the creation of function cycles and the resulting proliferation of ground terms.  
		  
Relational abstraction is sound and guarantees correctness if the solver cannot find counter-models. However, it may also lead to false counterexamples, which can be addressed by refining the abstraction and requiring the existence of corresponding elements.  
In the domain of distributed protocols, we can refine the abstraction by diagnosing counterexamples and manually instantiating elements in the range of the original function. If the verification conditions are correct, there always exist finitely many refinement steps that eliminate all spurious counter-models, making the approach complete.  
		  
We applied this approach in Ivy to verify the safety properties of consensus protocols and found that: (1) most verification goals can be automatically verified using relational abstraction, while SMT solvers often diverge when given the original VC, (2) only a few manual instantiations were needed, and the counterexamples provided valuable guidance for the user  
compared to timeouts produced by the traditional approach, and (3) the technique can be used to derive efficient low-level implementations of tricky algorithms.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {288},
numpages = {27},
keywords = {SMT, Ivy, Formal verification, Abstraction-refinement}
}

@article{10.1145/3622865,
author = {Meier, Shawn and Mover, Sergio and Kaki, Gowtham and Chang, Bor-Yuh Evan},
title = {Historia: Refuting Callback Reachability with Message-History Logics},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622865},
doi = {10.1145/3622865},
abstract = {This paper considers the callback reachability problem --- determining if a callback can be called by an event-driven framework in an unexpected state.  
Event-driven programming frameworks are pervasive for creating user-interactive applications (apps) on just about every modern platform.  
Control flow between callbacks is determined by the framework and largely opaque to the programmer.  
This opacity of the callback control flow not only causes difficulty for the programmer but is also difficult for those developing static analysis.  
Previous static analysis techniques address this opacity either by assuming an arbitrary framework implementation or attempting to eagerly specify all possible callback control flow,  
but this is either too coarse to prove properties requiring callback-ordering constraints or too burdensome and tricky to get right.  
Instead, we present a middle way where the callback control flow can be gradually refined in a targeted manner to prove assertions of interest.  
The key insight to get this middle way is by reasoning about the history of method invocations at the boundary between app and framework code --- enabling a decoupling of the specification of callback control flow from the analysis of app code.  
We call the sequence of such boundary-method invocations message histories and develop message-history logics to do this reasoning.  
In particular, we define the notion of an application-only transition system with boundary transitions, a message-history program logic for programs with such transitions, and a temporal specification logic for capturing callback control flow in a targeted and compositional manner.  
Then to utilize the logics in a goal-directed verifier, we define a way to combine after-the-fact an assertion about message histories with a specification of callback control flow.  
We implemented a prototype message history-based verifier called Historia and provide evidence that our approach is uniquely capable of distinguishing between buggy and fixed versions on challenging examples drawn from real-world issues and that our targeted specification approach enables proving the absence of multi-callback bug patterns in real-world open-source Android apps.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {289},
numpages = {30},
keywords = {temporal logics, refuting callback reachability, ordered linear logics, message history logics, goal-directed verification, framework modeling, event-driven frameworks, callback control flow, backwards abstract interpretation}
}

@software{10.5281/zenodo.8331516,
author = {Meier, Shawn and Mover, Sergio and Kaki, Gowtham and Chang, Bor-Yuh Evan},
title = {Historia: Refuting Callback Reachability with Message-History Logics (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8331516},
abstract = {
    <p>This is the artifact for the paper Historia: Refuting Callback Reachability with Message-History Logics and contains the implementation and open source Android applications used for evaluation. This artifact implements an application-only message-history based program analysis for Android. It handles safety properties relating to event and callback order including null pointer exceptions and other runtime exceptions in the Android framework.</p>

},
keywords = {event-driven applications, program analysis, separation logic, software safety, static analysis, temporal logics}
}

@article{10.1145/3622866,
author = {Larsen, Jens Kanstrup and Guanciale, Roberto and Haller, Philipp and Scalas, Alceste},
title = {P4R-Type: A Verified API for P4 Control Plane Programs},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622866},
doi = {10.1145/3622866},
abstract = {Software-Defined Networking (SDN) significantly simplifies programming, reconfiguring, and optimizing network devices, such as switches and routers. The de facto standard for programming SDN devices is the P4 language. However, the flexibility and power of P4, and SDN more generally, gives rise to important risks. As a number of incidents at major cloud providers have shown, errors in SDN programs can compromise the availability of networks, leaving them in a non-functional state. The focus of this paper are errors in control-plane programs that interact with P4-enabled network devices via the standardized P4Runtime API. For clients of the P4Runtime API it is easy to make mistakes that may lead to catastrophic failures, despite the use of Google’s Protocol Buffers as an interface definition language. This paper proposes P4R-Type, a novel verified P4Runtime API for Scala that performs static checks for P4 control plane operations, ruling out mismatches between P4 tables, allowed actions, and action parameters. As a formal foundation of P4R-Type, we present the FP4R calculus and its typing system, which ensure that well-typed programs never get stuck by issuing invalid P4Runtime operations. We evaluate the safety and flexibility of P4R-Type with 3 case studies. To the best of our knowledge, this is the first work that formalises P4Runtime control plane applications, and a typing discipline ensuring the correctness of P4Runtime operations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {290},
numpages = {29},
keywords = {Type systems, Software-defined networking, Semantics, P4Runtime, P4}
}

@software{10.1145/3580420,
author = {Larsen, Jens Kanstrup and Guanciale, Roberto and Haller, Philipp and Scalas, Alceste},
title = {P4R-Type},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3580420},
abstract = {
    <p>This artifact consists mainly of the code for the P4R-Type API and the associated type generator, as well examples that use the P4R-Type API. It contains also a virtual machine that can be used to set up a simulation network using mininet, which is used for testing the API. The virtual machine comes both in the form of a ready-to-use VM image as well as Vagrant configuration files for building the image yourself.</p>

},
keywords = {Match types, P4, P4Runtime, Protobuf, Scala 3, Software-defined networking}
}

@article{10.1145/3622867,
author = {Laurel, Jacob and Qian, Siyuan Brant and Singh, Gagandeep and Misailovic, Sasa},
title = {Synthesizing Precise Static Analyzers for Automatic Differentiation},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622867},
doi = {10.1145/3622867},
abstract = {We present Pasado, a technique for synthesizing precise static analyzers for Automatic Differentiation. Our technique allows one to automatically construct a static analyzer specialized for the Chain Rule, Product Rule, and Quotient Rule computations for Automatic Differentiation in a way that abstracts all of the nonlinear operations of each respective rule simultaneously. By directly synthesizing an abstract transformer for the composite expressions of these 3 most common rules of AD, we are able to obtain significant precision improvement compared to prior works which compose standard abstract transformers together suboptimally. We prove our synthesized static analyzers sound and additionally demonstrate the generality of our approach by instantiating these AD static analyzers with different nonlinear functions, different abstract domains (both intervals and zonotopes) and both forward-mode and reverse-mode AD.  

We evaluate Pasado on multiple case studies, namely soundly computing bounds on a neural network’s  
local Lipschitz constant, soundly bounding the sensitivities of financial models, certifying monotonicity, and lastly, bounding sensitivities of the solutions of differential equations from climate science and chemistry for verified ranges of initial conditions and parameters. The local Lipschitz constants computed by Pasado on our largest CNN are up to 2750\texttimes{} more precise compared to the existing state-of-the-art zonotope analysis. The bounds obtained on the sensitivities of the climate, chemical, and financial differential equation solutions are between 1.31 − 2.81\texttimes{} more precise (on average) compared to a state-of-the-art zonotope analysis.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {291},
numpages = {29},
keywords = {Differentiable Programming, Abstract Interpretation}
}

@software{10.5281/zenodo.8332724,
author = {Laurel, Jacob and Qian, Siyuan Brant and Singh, Gagandeep and Misailovic, Sasa},
title = {Reproduction Artifact for "Synthesizing Precise Static Analyzers for Automatic Differentiation"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8332724},
abstract = {
    <p>This artifact contains all of the source code for Pasado and all of the experimental scripts needed to reproduce the evaluation from our paper “Synthesizing Precise Static Analyzers for Automatic Differentiation”. This artifact is hosted on both Zenodo, as well as on github at the following repository: “https://github.com/uiuc-arc/Pasado”</p>

},
keywords = {Abstract Interpretation, Automatic Differentiation, Differentiable Programming, Static Analysis}
}

@article{10.1145/3622868,
author = {Conrado, Giovanna Kobus and Goharshady, Amir Kafshdar and Kochekov, Kerim and Tsai, Yun Chen and Zaher, Ahmed Khaled},
title = {Exploiting the Sparseness of Control-Flow and Call Graphs for Efficient and On-Demand Algebraic Program Analysis},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622868},
doi = {10.1145/3622868},
abstract = {Algebraic Program Analysis (APA) is a ubiquitous framework that has been employed as a unifying model for various problems in data-flow analysis, termination analysis, invariant generation, predicate abstraction and a wide variety of other standard static analysis tasks. APA models program summaries as elements of a regular algebra . Suppose that a summary in A is assigned to every transition of the program and that we aim to compute the effect of running the program starting at line s and ending at line t. APA first computes a regular expression  capturing all program paths of interest. In case of intraprocedural analysis,  models all paths from s to t, whereas in the interprocedural case it models all interprocedurally-valid paths, i.e. ‍paths that go back to the right caller function when a callee returns. This regular expression  is then interpreted over the algebra  to obtain the desired result. Suppose the program has n lines of code and each evaluation of an operation in the regular algebra takes O(k) time. It is well-known that a single APA query, or a set of queries with the same starting point s, can be answered in O(n · α(n) · k), where α is the inverse Ackermann function. 		 		In this work, we consider an on-demand setting for APA: the program is given in the input and can be preprocessed. The analysis has to then answer a large number of on-line queries, each providing a pair (s, t) of program lines which are the start and end point of the query, respectively. The goal is to avoid the significant cost of running a fresh APA instance for each query. Our main contribution is a series of algorithms that, after a lightweight preprocessing of O(n · lgn · k), answer each query in O(k) time. In other words, our preprocessing has almost the same asymptotic complexity as a single APA query, except for a sub-logarithmic factor, and then every future query is answered instantly, i.e. ‍by a constant number of operations in the algebra. We achieve this remarkable speedup by relying on certain structural sparsity properties of control-flow and call graphs (CFGs and CGs). Specifically, we exploit the fact that control-flow graphs of real-world programs have a tree-like structure and bounded treewidth and nesting depth and that their call graphs have small treedepth in comparison to the size of the program. Finally, we provide experimental results demonstrating the effectiveness and efficiency of our approach and showing that it beats the runtime of classical APA by several orders of magnitude.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {292},
numpages = {30},
keywords = {Treewidth, Treedepth, Parameterized Algorithms, Graph Sparsity, Data-flow Analysis, Algebaric Program Analysis}
}

@software{10.5281/zenodo.8320671,
author = {Conrado, Giovanna Kobus and Goharshady, Amir Kafshdar and Kochekov, Kerim and Tsai, Yun Chen and Zaher, Ahmed Khaled},
title = {Artifact for Exploiting the Sparseness of Control-flow and Call Graphs for Efficient and On-demand Algebraic Program Analysis},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8320671},
abstract = {
    <p>This artifact compares the performance of our algorithm presented in the paper vs.&nbsp;the classical approach of using Tarjan’s algorithm. It uses programs from DaCapo benchmarks as well as programs generated by Windows device drivers for the purpose of running the algorithms on them. After putting these programs into an appropriate form and finding the associated tree and depth decompositions, it feeds them to both our algorithm and Tarjan’s, and compares the performance of the two algorithms. Detailed instructions can be found inside the artifact in artifact-documentation.pdf</p>

},
keywords = {Algebaric Program Analysis, Data-flow Analysis, Graph Sparsity, Parameterized Algorithms, Parameterized complexity and exact algorithms, Program analysis, Program reasoning, Program verification, Treedepth, Treewidth}
}

@article{10.1145/3622869,
author = {Miltner, Anders and Loehr, Devon and Mong, Arnold and Fisher, Kathleen and Walker, David},
title = {Saggitarius: A DSL for Specifying Grammatical Domains},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622869},
doi = {10.1145/3622869},
abstract = {Common data types like dates, addresses, phone numbers and tables can have multiple textual representations, and many heavily-used languages, such as SQL, come in several dialects. These variations can cause data to be misinterpreted, leading to silent data corruption, failure of data processing systems, or even security vulnerabilities. Saggitarius is a new language and system designed to help programmers reason about the format of data, by describing grammatical domains---that is, sets of context-free grammars that describe the many possible representations of a datatype. We describe the design of Saggitarius via example and provide a relational semantics. We show how Saggitarius may be used to analyze a data set: given example data, it uses an algorithm based on semi-ring parsing and MaxSAT to infer which grammar in a given domain best matches that data. We evaluate the effectiveness of the algorithm on a benchmark suite of 110 example problems, and we demonstrate that our system typically returns a satisfying grammar within a few seconds with only a small number of examples. We also delve deeper into a more extensive case study on using Saggitarius for CSV dialect detection. Despite being general-purpose, we find that Saggitarius offers comparable results to hand-tuned, specialized tools; in the case of CSV, it infers grammars for 84\% of benchmarks within 60 seconds, and has comparable accuracy to custom-built dialect detection tools.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {293},
numpages = {29},
keywords = {Syntax-Guided Synthesis, Semiring Parsing, Grammar Induction}
}

@article{10.1145/3622870,
author = {Schr\"{o}er, Philipp and Batz, Kevin and Kaminski, Benjamin Lucien and Katoen, Joost-Pieter and Matheja, Christoph},
title = {A Deductive Verification Infrastructure for Probabilistic Programs},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622870},
doi = {10.1145/3622870},
abstract = {This paper presents a quantitative program verification infrastructure for discrete  
probabilistic programs.  
Our infrastructure can be viewed as the probabilistic analogue of Boogie:  
its central components are an intermediate verification language (IVL) together with a real-valued logic.  
Our IVL provides a programming-language-style for expressing verification  
conditions whose validity implies the correctness of a program under investigation.  
As our  
focus is on verifying quantitative properties such as bounds on  
expected outcomes, expected run-times, or termination  
probabilities, off-the-shelf IVLs based on Boolean first-order logic do not  
suffice.  
Instead, a paradigm shift from the standard Boolean to a real-valued domain is required.  

Our IVL features quantitative generalizations of standard  
verification constructs such as assume- and assert-statements. Verification conditions are generated by a  
weakest-precondition-style semantics, based on our real-valued logic.  
We show that  
our verification infrastructure supports natural encodings of numerous verification  
techniques from the literature. With our SMT-based implementation, we automatically verify a variety of benchmarks. To the best of our knowledge, this establishes the first deductive verification  
infrastructure for expectation-based reasoning about probabilistic programs.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {294},
numpages = {31},
keywords = {weakest preexpectations, real-valued logics, quantitative verification, probabilistic programs, deductive verification, automated reasoning}
}

@software{10.5281/zenodo.8146987,
author = {Schr\"{o}er, Philipp and Batz, Kevin and Kaminski, Benjamin Lucien and Katoen, Joost-Pieter and Matheja, Christoph},
title = {Reproduction Package for Article 'A Deductive Verification Infrastructure for Probabilistic Programs'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8146987},
abstract = {
    <p>Contained within the artifact: * Our tool <em>Caesar</em>, which parses HeyVL programs and tries to verify them. Caesar constitutes our main implementation contribution and is the focus of this artifact. * A script to reproduce our benchmarks (Table 2). * We also include our prototypical tool <em>pgcl2heyvl</em>, which takes pGCL programs with annotations and produces a HeyVL file that encodes the required proof obligations. * Our full source code is contained within the artifact as well.</p>

},
keywords = {automated reasoning, deductive verification, probabilistic programs, quantitative verification, real-valued logics, weakest preexpectations}
}

@article{10.1145/3622871,
author = {Cui, Chen and Jiang, Shengyi and Oliveira, Bruno C. d. S.},
title = {Greedy Implicit Bounded Quantification},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622871},
doi = {10.1145/3622871},
abstract = {Mainstream object-oriented programming languages such as Java, Scala, C#, or TypeScript have polymorphic type systems with subtyping and bounded quantification. Bounded quantification, despite being a pervasive and widely used feature, has attracted little research work on type-inference algorithms to support it. A notable exception is local type inference, which is the basis of most current implementations of type inference for mainstream languages. However, support for bounded quantification in local type inference has important restrictions, and its non-algorithmic specification is complex. In this paper, we present a variant of kernel F≤, which is the canonical calculus with bounded quantification, with implicit polymorphism. Our variant, called F≤b, comes with a declarative and an algorithmic formulation of the type system. The declarative type system is based on previous work on bidirectional typing for predicative higher-rank polymorphism and a greedy approach to implicit instantiation. This allows for a clear declarative specification where programs require few type annotations and enables implicit polymorphism where applications omit type parameters. Just as local type inference, explicit type applications are also available in F≤b if desired. This is useful to deal with impredicative instantiations, which would not be allowed otherwise in F≤b. Due to the support for impredicative instantiations, we can obtain a completeness result with respect to kernel F≤, showing that all the well-typed kernel F≤ programs can type-check in F≤b. The corresponding algorithmic version of the type system is shown to be sound, complete, and decidable. All the results have been mechanically formalized in the Abella theorem prover.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {295},
numpages = {29},
keywords = {Type Inference, Mechanical Formalization, Bounded Quantification}
}

@software{10.5281/zenodo.8336774,
author = {Cui, Chen and Jiang, Shengyi and Oliveira, Bruno C. d. S.},
title = {Greedy Implicit Bounded Quantification (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8336774},
abstract = {
    <p>The artifact includes the implementation, proofs, and the extended version of the paper “Greedy Implicit Bounded Quantification”.</p>

},
keywords = {Abella, Bounded Quantification, Mechanical Formalization, Type Inference}
}

@article{10.1145/3622872,
author = {Chen, Yu-Fang and Chocholat\'{y}, David and Havlena, Vojt\v{e}ch and Hol\'{\i}k, Luk\'{a}\v{s} and Leng\'{a}l, Ond\v{r}ej and S\'{\i}\v{c}, Juraj},
title = {Solving String Constraints with Lengths by Stabilization},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622872},
doi = {10.1145/3622872},
abstract = {We present a new algorithm for solving string constraints. The algorithm builds upon a recent method for solving word equations and regular constraints that interprets string variables as languages rather than strings and, consequently, mitigates the combinatorial explosion that plagues other approaches. We extend the approach to handle linear integer arithmetic length constraints by combination with a known principle of equation alignment and splitting, and by extension to other common types of string constraints, yielding a fully-fledged string solver. The ability of the framework to handle unrestricted disequalities even extends one of the largest decidable classes of string constraints, the chain-free fragment. We integrate our algorithm into a DPLL-based SMT solver. The performance of our implementation is competitive and even significantly better than state-of-the-art string solvers on several established benchmarks obtained from applications in verification of string programs.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {296},
numpages = {30},
keywords = {word equations, string constraints, stabilization, regular languages, length constraints, SMT solving}
}

@software{10.5281/zenodo.8289595,
author = {Chen, Yu-Fang and Chocholat\'{y}, David and Havlena, Vojt\v{e}ch and Hol\'{\i}k, Luk\'{a}\v{s} and Leng\'{a}l, Ond\v{r}ej and S\'{\i}\v{c}, Juraj},
title = {Artifact for the OOPSLA'23 paper "Solving String Constraints with Lengths by Stabilization"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8289595},
abstract = {
    <p>This is an artifact for the OOPSLA’23 paper “Solving String Constraints with Lengths by Stabilization”. It contains a virtual machine with Ubuntu GNU/Linux and all solvers, benchmarks, and supporting scripts to reproduce all experiments in the paper.</p>

},
keywords = {length constraints, regular languages, SMT solving, stabilization, string constraints, word equations}
}

@article{10.1145/3622873,
author = {Zakhour, George and Weisenburger, Pascal and Salvaneschi, Guido},
title = {Type-Safe Dynamic Placement with First-Class Placed Values},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622873},
doi = {10.1145/3622873},
abstract = {Several distributed programming language solutions have been proposed to reason about the placement of data, computations, and peers interaction. Such solutions include, among the others, multitier programming, choreographic programming and various approaches based on behavioral types. These methods statically ensure safety properties thanks to a complete knowledge about placement of data and computation at compile time. In distributed systems, however, dynamic placement of computation and data is crucial to enable performance optimizations, e.g., driven by data locality or in presence of a number of other constraints such as security and compliance regarding data storage location. Unfortunately, in existing programming languages, dynamic placement conflicts with static reasoning about distributed programs: the flexibility required by dynamic placement hinders statically tracking the location of data and computation. 

In this paper we present Dyno, a programming language that enables static reasoning about dynamic placement. Dyno features a type system where values are explicitly placed, but in contrast to existing approaches, placed values are also first class, ensuring that they can be passed around and referred to from other locations. Building on top of this mechanism, we provide a novel interpretation of dynamic placement as unions of placement types. We formalize type soundness, placement correctness (as part of type soundness) and architecture conformance. In case studies and benchmarks, our evaluation shows that Dyno enables static reasoning about programs even in presence of dynamic placement, ensuring type safety and placement correctness of programs at negligible performance cost. We reimplement an Android app with ∼ 7 K LOC in Dyno, find a bug in the existing implementation, and show that the app's approach is representative of a common way to implement dynamic placement found in over 100 apps in a large open-source app store.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {297},
numpages = {29},
keywords = {Union Types, Scala, Placement Types, Multitier Programming, Dynamic Placement, Distributed Programming}
}

@software{10.5281/zenodo.8329679,
author = {Zakhour, George and Weisenburger, Pascal and Salvaneschi, Guido},
title = {Type-Safe Dynamic Placement with First-Class Placed Values},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8329679},
abstract = {
    <h2 id="dyno">Dyno</h2>
<h3 id="artifact-for-the-paper-type-safe-dynamic-placement-with-first-class-placed-values">Artifact for the paper “Type-Safe Dynamic Placement with First-Class Placed Values”</h3>
<p>The artifact is available at: https://doi.org/10.5281/zenodo.8329679</p>
<h2 id="getting-started">GETTING STARTED</h2>
<h3 id="building-and-loading-the-docker-image">BUILDING AND LOADING THE DOCKER IMAGE</h3>
<p>We provide you with <code>dyno.tar.xz</code>, which is a pre-built container image that contains all necessary programs. To load, run the following command:</p>
<pre><code>docker load &lt; dyno.tar.xz</code></pre>
<p>Running the image may not work on Apple M1/M2 machines, or any machine with Apple’s ARM-based chips, because of incomplete emulation of system calls (specifically the inotify kernel subsystem). Hence, we recommend running the image on a platform fully supported by Docker, like x86-64 systems.</p>
<h2 id="step-by-step-instructions">STEP-BY-STEP INSTRUCTIONS</h2>
<h3 id="compiling-dyno">COMPILING DYNO</h3>
<p>The provided container already contains the pre-compiled jar files of Dyno.</p>
<p>To compile Dyno yourself, run the following command:</p>
<pre><code>docker run -it --rm dyno bash -c 'cd /dyno; sbt clean publishLocal'</code></pre>
<p>Compiling Propel may not work inside the Docker container on Apple M1/M2 machines for the reasons mentioned earlier.</p>
<p>The resulting jar files are in <code>~/.ivy2/local/io.github.dyno/</code>. You can run this command to see all of them <code>find ~/.ivy2/local/io.github.dyno/ -name "*.jar"</code></p>
<h3 id="testing-dyno">TESTING DYNO</h3>
<p>To run the tests in Dyno, execute:</p>
<pre><code>docker run -it --rm dyno bash -c 'cd /dyno &amp;\&amp; SBT_OPTS="-Xmx4G" sbt lociJVM/test'</code></pre>
<p>Running the Dyno tests may not work inside the Docker container on Apple M1/M2.</p>
<p>Running the tests may take up to five minutes.</p>
<h3 id="executing-paper-evaluations">EXECUTING PAPER EVALUATIONS</h3>
<h4 id="variants-analysis-section-8.1">VARIANTS ANALYSIS (Section 8.1)</h4>
<h5 id="verification-of-the-numbers-in-table-1">Verification of the numbers in Table 1</h5>
<p>Each variant is implemented under <code>/evaluation/casestudies</code>. In each variant’s folder there are three folders for each implementation: <code>dyno/</code>, <code>rmi/</code>, and <code>akka/</code>.</p>
<p>Reference creation, acquisition, and access are all labeled in the source code in each implementation. Creations are labeled with a comment <code>/*ref-creation*/</code> that follows immediately every expression that creates (or simulates a creation) of a reference. Similarly, an acquisition and an access are labeled with the comments <code>/*ref-acquire*/</code> and <code>/*ref-use*/</code> respectively and they follow immediately every expression that is relevant.</p>
<p>Counting each comment should match with the numbers we provide in Table 1 in the paper.</p>
<h5 id="running-each-variant">Running each variant</h5>
<h6 id="resources-variant">Resources Variant</h6>
<p>In the resources variant, the client asks the user to input their identifier (it can be any string), the supervisor takes over in case the identifier is new or untrusted and asks whether the identifier should be trusted or not. If the supervisor says yes then all subsequent calls will result in the same resource generated for that identifier, otherwise no resource will be given.</p>
<h6 id="dyno-implementation">Dyno Implementation</h6>
<p>First start by running a new container image with <code>docker run -it --rm dyno bash</code> in a console.</p>
<p>To run the Dyno Resources evaluation, you must execute in six different consoles the following commands in order:</p>
<ol type="1">
<li>On console 1, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/dyno; sbt "runMain loci.resources.TrustedKeyDb"'</code></li>
<li>On console 2, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/dyno; sbt "runMain loci.resources.PublicKeyDb"'</code></li>
<li>On console 3, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/dyno; sbt "runMain loci.resources.KeyManager"'</code></li>
<li>On console 4, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/dyno; sbt "runMain loci.resources.ResourceManager"'</code></li>
<li>On console 5, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/dyno; sbt "runMain loci.resources.Supervisor"'</code></li>
<li>On console 6, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/dyno; sbt "runMain loci.resources.Clienct"'</code></li>
</ol>
<p>The <code>CONTAINER_ID</code> variable can be found by executing: <code>docker container ls | grep dyno | head -n 1 | cut -f1 -d' '</code>.</p>
<p>Please allow each command to complete compilation and to begin running before you execute the next one.</p>
<h6 id="rmi-implementation">RMI Implementation</h6>
<p>First start by running a new container image with <code>docker run -it --rm dyno bash</code> in a console.</p>
<p>To run the RMI Resources evaluation, you must execute in six different consoles the following commands in order:</p>
<ol type="1">
<li>On console 1, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/rmi; sbt "runMain loci.resources.TrustedKeyDbMain"'</code></li>
<li>On console 2, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/rmi; sbt "runMain loci.resources.PublicKeyDbMain"'</code></li>
<li>On console 3, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/rmi; sbt "runMain loci.resources.KeyManagerMain"'</code></li>
<li>On console 4, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/rmi; sbt "runMain loci.resources.ResourceManagerMain"'</code></li>
<li>On console 5, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/rmi; sbt "runMain loci.resources.SupervisorMain"'</code></li>
<li>On console 6, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/resources/rmi; sbt "runMain loci.resources.ClienctMain"'</code></li>
</ol>
<p>Please allow each command to complete compilation and to begin running before you execute the next one.</p>
<h6 id="akka-implementation">Akka Implementation</h6>
<p>To run the Akka Resources evaluation, you must execute the following command:</p>
<pre><code>docker run -it --rm dyno bash -c 'cd /evaluation/casestudies/resources/akka; sbt run'</code></pre>
<p>This will spawn, for each peer, different actors that run in the same process.</p>
<h6 id="sessions-variant">Sessions Variant</h6>
<p>In the sessions variant, the client navigates an application that consists of different components. To login in some components you can use the username <code>admin</code> and the password <code>adminPassword</code>.</p>
<h6 id="dyno-implementation-1">Dyno Implementation</h6>
<p>First start by running a new container image with <code>docker run -it --rm dyno bash</code> in a console.</p>
<p>To run the Dyno Sessions evaluation, you must execute in four different consoles the following commands in order:</p>
<ol type="1">
<li>On console 1, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/session/dyno; sbt "runMain loci.sessions.Auth"'</code></li>
<li>On console 2, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/session/dyno; sbt "runMain loci.sessions.AdminPanel"'</code></li>
<li>On console 3, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/session/dyno; sbt "runMain loci.sessions.Server"'</code></li>
<li>On console 4, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/session/dyno; sbt "runMain loci.sessions.Client"'</code></li>
</ol>
<p>Please allow each command to complete compilation and to begin running before you execute the next one.</p>
<h6 id="rmi-implementation-1">RMI Implementation</h6>
<p>First start by running a new container image with <code>docker run -it --rm dyno bash</code> in a console.</p>
<p>To run the RMI Sessions evaluation, you must execute in four different consoles the following commands in order:</p>
<ol type="1">
<li>On console 1, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/session/rmi; PEER_TYPE=Server sbt "runMain loci.sessions.ServerMain"'</code></li>
<li>On console 2, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/session/rmi; PEER_TYPE=Auth sbt "runMain loci.sessions.AuthMain"'</code></li>
<li>On console 3, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/session/rmi; PEER_TYPE=AdminPanel sbt "runMain loci.sessions.AdminPanelMain"'</code></li>
<li>On console 4, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/session/rmi; PEER_TYPE=Client sbt "runMain loci.sessions.ClientMain"'</code></li>
</ol>
<p>Please allow each command to complete compilation and to begin running before you execute the next one.</p>
<h6 id="akka-implementation-1">Akka Implementation</h6>
<p>To run the Akka Sessions evaluation, you must execute the following command:</p>
<pre><code>docker run -it --rm dyno bash -c 'cd /evaluation/casestudies/session/akka; sbt run'</code></pre>
<h6 id="unifeed-variant">Unifeed Variant</h6>
<p>In the unifeed variant the unifier takes two streams of tweets and toots and merges them in a single feed chronologically. Tweets come from Twitter and require you to have an API key. You can obtain an API key by following the instructions on: https://developer.twitter.com/en/docs/authentication/oauth-1-0a/api-key-and-secret Toots come from Mastodon. If you are on mastodon.social you can create an API key on this URL https://mastodon.social/settings/applications/new</p>
<p>The Mastodon key is called “Your access token” and the Twitter key is called “Bearer Token”. We refer to them in the following commands using <code>&lt;MASTODON_KEY&gt;</code> and <code>&lt;TWITTER_KEY&gt;</code> respectively.</p>
<p>Twitter’s API support has been getting more and more restrictive for developers. If you are developer with a Basic or Pro Twitter subscription then please apply the patch suggested at the end of this section to unify a twitter stream with a mastodon stream. If you do not have a Basic or Pro twitter subscription then you can not use the API endpoints that this application requires. The provided example instead unified two Mastodon streams.</p>
<h6 id="dyno-implementation-2">Dyno Implementation</h6>
<p>First start by running a new container image with <code>docker run -it --rm dyno bash</code> in a console.</p>
<p>To run the Dyno Unifeed evaluation, you must execute in four different consoles the following commands in order:</p>
<ol type="1">
<li>On console 1, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/unifeed/dyno; ACCESS_TOKEN=&lt;TWITTER_KEY&gt; sbt "runMain loci.unifeed.Twitter"'</code></li>
<li>On console 2, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/unifeed/dyno; ACCESS_TOKEN=&lt;MASTODON_KEY&gt; sbt "runMain loci.unifeed.Mastodon"'</code></li>
<li>On console 3, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/unifeed/dyno; sbt "runMain loci.unifeed.Unifier"'</code></li>
<li>On console 4, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/unifeed/dyno; sbt "runMain loci.unifeed.Client"'</code></li>
</ol>
<p>Please allow each command to complete compilation and to begin running before you execute the next one.</p>
<h6 id="rmi-implementation-2">RMI Implementation</h6>
<p>First start by running a new container image with <code>docker run -it --rm dyno bash</code> in a console.</p>
<p>To run the RMI Unifeed evaluation, you must execute in three different consoles the following commands in order:</p>
<ol type="1">
<li>On console 1, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/unifeed/rmi; ACCESS_TOKEN=&lt;TWITTER_KEY&gt; sbt "runMain loci.unifeed.TwitterMain"'</code></li>
<li>On console 2, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/unifeed/rmi; ACCESS_TOKEN=&lt;MASTODON_KEY&gt; sbt "runMain loci.unifeed.MastodonMain"'</code></li>
<li>On console 3, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/casestudies/unifeed/rmi; sbt "runMain loci.sessions.ClientMain"'</code></li>
</ol>
<p>Please allow each command to complete compilation and to begin running before you execute the next one.</p>
<h6 id="akka-implementation-2">Akka Implementation</h6>
<p>To run the Akka Unifeed evaluation, you must execute the following command:</p>
<pre><code>docker run -it --rm dyno bash -c 'cd /evaluation/casestudies/unifeed/akka; TWITTER_KEY=&lt;TWITTER_KEY&gt; MASTODON_KEY=&lt;MASTODON_KEY&gt; sbt run'</code></pre>
<h6 id="patch-to-use-twitter-api">Patch to use Twitter API</h6>
<p>If you wish to use the twitter API you can go use the object that’s commented out in the source code and preceded with the string <code>TWITTER_API:</code></p>
<h4 id="performance-section-8.2">PERFORMANCE (Section 8.2)</h4>
<h5 id="running-the-dyno-version">Running the Dyno version</h5>
<p>First start by running a new container image with <code>docker run -it --rm dyno bash -c 'bash -c "service mariadb start" 2&gt;/dev/null; redis-server'</code> in a console.</p>
<p>To run the Dyno performance evaluation, you must execute in three different consoles the following commands, in order:</p>
<ol type="1">
<li>On console 1, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/performance; sbt "runMain loci.dbcachedyno.Database"'</code></li>
<li>On console 2, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/performance; sbt "runMain loci.dbcachedyno.Cache"'</code></li>
<li>On console 3, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/performance; sbt "runMain loci.dbcachedyno.Client"'</code></li>
</ol>
<h5 id="running-the-rmi-version">Running the RMI version</h5>
<p>First start by running a new container image with <code>docker run -it --rm dyno bash -c 'bash -c "service mariadb start" 2&gt;/dev/null; redis-server'</code> in a console.</p>
<p>To run the Dyno performance evaluation, you must execute in three different consoles the following commands, in order:</p>
<ol type="1">
<li>On console 1, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/performance; sbt "runMain loci.dbcachermi.DatabaseMain"'</code></li>
<li>On console 2, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/performance; sbt "runMain loci.dbcachermi.CacheMain"'</code></li>
<li>On console 3, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /evaluation/performance; sbt "runMain loci.dbcachermi.ClientMain"'</code></li>
</ol>
<h5 id="reading-the-output">Reading the Output</h5>
<p>The output will be visible in console 3.</p>
<p>The client first does a warmup trial that may take up to two minutes. Thus no output will be visible during the warmup. The whole experiment is not expected to take more than fifteen minutes.</p>
<p>The output is in the CSV format with 3 columns. The first column is the Hit percentage, from 0\% to 100\% in steps of 10\%. The second column is the time in milliseconds that N=5K queries took. The third column is the number of trials.</p>
<h5 id="changing-the-parameters">Changing the parameters</h5>
<p>If you wish to modify the number of trials, you must edit the value of the <code>N</code> value defined in <code>/evaluation/performance/src/main/scala/DbCache.scala</code> at line 77. The database already contains 200,001 unique keys. Therefore to preserve the correctness of the evaluation we recommend to keep N &lt; 200,000.</p>
<h4 id="antenna-pod-case-study-section-8.3">ANTENNA-POD CASE STUDY (Section 8.3)</h4>
<p>For the sake of completeness we describe in high-level the evaluation of Section 8.3 that we do not expect reviewers to repeat in full-details.</p>
<p>To demonstrate the bug we have found in AntennaPod we provide the following minimal RSS feed that you can host online:</p>
<pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;rss version="2.0"
    xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"&gt;
  &lt;channel&gt;
    &lt;title&gt;A Bad Feed&lt;/title&gt;
    &lt;itunes:owner&gt;
        &lt;itunes:email&gt;anon@anon.non&lt;/itunes:email&gt;
    &lt;/itunes:owner&gt;
    &lt;itunes:author&gt;Anon&lt;/itunes:author&gt;
    &lt;description&gt;A Really Bad Feed&lt;/description&gt;
    &lt;link&gt;https://bad.feed/&lt;/link&gt;
    &lt;item&gt;
      &lt;title&gt;A Bad Episode&lt;/title&gt;
      &lt;description&gt;A Really Bad Episode&lt;/description&gt;
      &lt;pubDate&gt;Tue, 14 Mar 2017 12:00:00 GMT&lt;/pubDate&gt;
      &lt;enclosure url=/do/10.5281/zenodo.8329679/export-citation-abs/"/idonotexist.mp3" type="audio/mpeg" length="34216300"/&gt;
      &lt;itunes:duration&gt;30:00&lt;/itunes:duration&gt;
    &lt;/item&gt;
  &lt;/channel&gt;
&lt;/rss&gt;</code></pre>
<p>After installing AntennaPod on your Android device from the Google Play store or F-Droid you can do the following steps:</p>
<ol type="1">
<li>Open the hamburger menu.</li>
<li>Click on the “+ Add Podcast” menu.</li>
<li>In the Advanced section, click on “Add Podcast by RSS address”.</li>
<li>In the popup window you can enter the URL of the bad RSS feed.</li>
<li>After pressing confirm a new popup should show up. Click on “A Bad Episode”.</li>
<li>A “Preview” button should appear. Click on it.</li>
<li>The player will attempt to play the file for a few seconds then it hides the “Preview” button.</li>
<li>Press the “back” button on your device and you will see the following Java error message:</li>
</ol>
<blockquote>
<p>com.google.android.exoplayer2.upstream.FileDataSource$FileDataSourceException: java.io.FileNotFoundException: /idonotexist.mp3: open failed: ENOENT (No such file or directory)</p>
</blockquote>
<h4 id="f-droid-case-study-section-8.4">F-DROID CASE STUDY (Section 8.4)</h4>
<p>For the sake of completeness we describe in high-level the evaluation of Section 8.4 that we do not expect reviewers to repeat in full-details.</p>
<ul>
<li>The F-Droid repository is available at https://f-droid.org/repo/index-v2.json which we also provide in <code>/fdroid_index.json</code></li>
<li>After cloning all git repositories in the index we kept those whose source code matched the following regular expression: <code>startsWith("file|equals("file|equalsIgnoreCase("file|startsWith("content|equals("content|equalsIgnoreCase("content</code></li>
<li>The result were 133 projects whose IDs we provide in the <code>/fdroid_matches.txt</code> file</li>
<li>The code snippets we provide in Section 8.4 illustrating the treatment of URLs are given in full-context in the <code>/fdroid_snippets/</code> folder</li>
</ul>
<h2 id="a-small-starting-example">A SMALL STARTING EXAMPLE</h2>
<p>Dyno is a general library that can be reused in other applications. We invite you to start with <code>/examples/src/main/scala/simple_example</code> which contains the source code for a small and simple example. It is made of a single file <code>SimpleExample.scala</code> that is annotated with comments explaining the details of the program.</p>
<p>The program consists of three peers. The first, <code>FirstProvider</code> contains an integer. Running this peer will prompt (repeatedly) the user to input a number that it will store inside its integer. The second, <code>SecondProvider</code> behaves exactly the same. The third, <code>Selector</code> starts by prompting the user to choose the peer from which it will retrieve a reference to its integer. If the user inputs <code>1</code> then they have chosen to use the value from <code>FirstProvider</code>, if the user inputs <code>2</code> they have chosen to use the value from <code>SecondProvider</code>. Any other input will re-prompt the user. Once a reference is obtained the user will be repeatedly prompted to press enter to dereference the integer reference. If the value has changed in between dereferences then the selector must observe these changes.</p>
<p>First start by running a new container image with <code>docker run -it --rm dyno bash</code> in a console.</p>
<p>To run these examples, you must execute in three different consoles the following commands, in order:</p>
<ol type="1">
<li>On console 1, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /examples; sbt "runMain loci.simple_example.FirstProvider"'</code></li>
<li>On console 2, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /examples; sbt "runMain loci.simple_example.SecondProvider"'</code></li>
<li>On console 3, execute <code>docker exec -it $CONTAINER_ID bash -c 'cd /examples; sbt "runMain loci.simple_example.Selector"'</code></li>
</ol>
<p>The <code>CONTAINER_ID</code> variable can be found by executing: <code>docker container ls | grep dyno | head -n 1 | cut -f1 -d' '</code>.</p>
<p>Please allow each command to complete compilation and to begin running before you execute the next one.</p>
<p>Now you are ready to interact with the peers.</p>
<p>sudo docker exec -it $(sudo docker container ls | grep dyno | head -n1 | cut -f1 -d’ ‘) bash -c ’bash -c “service mariadb start” 2&gt;/dev/null; cd /evaluation/performance; sbt “runMain loci.dbcachedyno.Database”’</p>

},
keywords = {Distributed Programming, Dynamic Placement, Multitier Programming, Placement Types, Scala, Union Types}
}

@article{10.1145/3622874,
author = {Nazari, Amirmohammad and Huang, Yifei and Samanta, Roopsha and Radhakrishna, Arjun and Raghothaman, Mukund},
title = {Explainable Program Synthesis by Localizing Specifications},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622874},
doi = {10.1145/3622874},
abstract = {The traditional formulation of the program synthesis problem is to find a program that meets a logical correctness specification. When synthesis is successful, there is a guarantee that the implementation satisfies the specification. Unfortunately, synthesis engines are typically monolithic algorithms, and obscure the correspondence between the specification, implementation and user intent. In contrast, humans often include comments in their code to guide future developers towards the purpose and design of different parts of the codebase. In this paper, we introduce subspecifications as a mechanism to augment the synthesized implementation with explanatory notes of this form. In this model, the user may ask for explanations of different parts of the implementation; the subspecification generated in response is a logical formula that describes the constraints induced on that subexpression by the global specification and surrounding implementation. We develop algorithms to construct and verify subspecifications and investigate their theoretical properties. We perform an experimental evaluation of the subspecification generation procedure, and measure its effectiveness and running time. Finally, we conduct a user study to determine whether subspecifications are useful: we find that subspecifications greatly aid in understanding the global specification, in identifying alternative implementations, and in debugging faulty implementations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {298},
numpages = {25},
keywords = {program comprehension, explainability, Program synthesis}
}

@software{10.5281/zenodo.8331495,
author = {Nazari, Amirmohammad and Huang, Yifei and Samanta, Roopsha and Radhakrishna, Arjun and Raghothaman, Mukund},
title = {Reproduction Package for Article "Explainable Program Synthesis by Localizing Specifications"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8331495},
abstract = {
    <p>This is the artifact package accompanying our OOPSLA 2023 submission titled Explainable Program Synthesis By Localizing Specifications. Our paper presents a new approach to explain the programs produced by program synthesis tools. We call this concept the sub-specification. Our paper presents examples of how subspecs can be useful and an algorithm to synthesize subspecifications. We have implemented this algorithm, which we call S3, for two program synthesis settings, SyGuS and DreamCoder. Our paper includes a user study and an experimental evaluation of the subspec synthesis procedure.</p>
<p>This artifact contains all the tools (S3, CVC5, EUSolver), benchmark files, and scripts to reproduce the experiments described in the paper. In this document, we will describe the outline of these experiments, how to run them, and also describe how one may use S3 to calculate sub-specifications on SyGuS solver and DreamCoder’s results of their own.</p>

},
keywords = {explainability, program comprehension, Program synthesis}
}

@article{10.1145/3622875,
author = {Astorga, Angello and Hsieh, Chiao and Madhusudan, P. and Mitra, Sayan},
title = {Perception Contracts for Safety of ML-Enabled Systems},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622875},
doi = {10.1145/3622875},
abstract = {We introduce a novel notion of perception contracts to reason about the safety of controllers that interact with an environment using neural perception. Perception contracts capture errors in ground-truth estimations that preserve invariants when systems act upon them. We develop a theory of perception contracts and design symbolic learning algorithms for synthesizing them from a finite set of images. We implement our algorithms and evaluate synthesized perception contracts for two realistic vision-based control systems, a lane tracking system for an electric vehicle and an agricultural robot that follows crop rows. Our evaluation shows that our approach is effective in synthesizing perception contracts and generalizes well when evaluated over test images obtained during runtime monitoring of the systems.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {299},
numpages = {28},
keywords = {safety, perception contracts, neural perception}
}

@article{10.1145/3622876,
author = {Mora, Federico and Desai, Ankush and Polgreen, Elizabeth and Seshia, Sanjit A.},
title = {Message Chains for Distributed System Verification},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622876},
doi = {10.1145/3622876},
abstract = {Verification of asynchronous distributed programs is challenging due to the need to reason about numerous control paths resulting from the myriad interleaving of messages and failures. In this paper, we propose an automated bookkeeping method based on message chains. Message chains reveal structure in asynchronous distributed system executions and can help programmers verify their systems at the message passing level of abstraction. To evaluate our contributions empirically we build a verification prototype for the P programming language that integrates message chains. We use it to verify 16 benchmarks from related work, one new benchmark that exemplifies the kinds of systems our method focuses on, and two industrial benchmarks. We find that message chains are able to simplify existing proofs and our prototype performs comparably to existing work in terms of runtime. We extend our work with support for specification mining and find that message chains provide enough structure to allow existing learning and program synthesis tools to automatically infer meaningful specifications using only execution examples.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {300},
numpages = {27},
keywords = {message passing, distributed systems, Formal verification}
}

@article{10.1145/3586027,
author = {Li, Shaohua and Su, Zhendong},
title = {Accelerating Fuzzing through Prefix-Guided Execution},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586027},
doi = {10.1145/3586027},
abstract = {Coverage-guided fuzzing is one of the most effective approaches for discovering software defects and vulnerabilities. It executes all mutated tests from seed inputs to expose coverage-increasing tests. However, executing all mutated tests incurs significant performance penalties---most of the mutated tests are discarded because they do not increase code coverage. Thus, determining if a test increases code coverage without actually executing it is beneficial, but a paradoxical challenge. In this paper, we introduce the notion of prefix-guided execution (PGE) to tackle this challenge. PGE leverages two key observations: (1) Only a tiny fraction of the mutated tests increase coverage, thus requiring full execution; and (2) whether a test increases coverage may be accurately inferred from its partial execution. PGE monitors the execution of a test and applies early termination when the execution prefix indicates that the test is unlikely to increase coverage.  

To demonstrate the potential of PGE, we implement a prototype on top of AFL++, which we call AFL++-PGE. We evaluate AFL++-PGE on MAGMA, a ground-truth benchmark set that consists of 21 programs from nine popular real-world projects. Our results show that, after 48 hours of fuzzing, AFL++-PGE finds more bugs, discovers bugs faster, and achieves higher coverage.  
Prefix-guided execution is general and can benefit the AFL-based family of fuzzers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {75},
numpages = {27},
keywords = {software testing, fuzzing, code coverage}
}

@software{10.5281/zenodo.7727577,
author = {Li, Shaohua and Su, Zhendong},
title = {Accelerating Fuzzing through Prefix-Guided Execution},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7727577},
abstract = {
    <p>This is the artifact for “Accelerating Fuzzing through Prefix-Guided Execution”, published in SPLASH/OOPSLA 2023. All instructions can be found in the zip file.</p>

},
keywords = {code coverage, fuzzing, software testing}
}

@article{10.1145/3586028,
author = {Wang, Chenglin and Lin, Fangzhen},
title = {Solving Conditional Linear Recurrences for Program Verification: The Periodic Case},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586028},
doi = {10.1145/3586028},
abstract = {In program verification, one method for reasoning about loops is to convert them into sets of recurrences, and  
 then try to solve these recurrences by computing their closed-form solutions.  
 While there are solvers for computing closed-form solutions to these recurrences,  
 their capabilities are limited when the recurrences have  
 conditional expressions, which arise when the body of a loop contains  
 conditional statements.  
 In this paper, we take a step towards solving these recurrences. Specifically, we consider what we call  
 conditional linear recurrences and show that given such a recurrence and an initial value, if the  
 index sequence generated by the recurrence on the initial value is what we call ultimately periodic,  
 then it has a closed-form solution. However, checking whether such a sequence is  
 ultimately periodic is undecidable so we propose a heuristic "generate and verify" algorithm for  
 checking the ultimate periodicity of the sequence and computing closed-form solutions at the same time.  
 We implemented a solver based on this algorithm, and  
 our experiments show that a straightforward program verifier based on our solver and using the SMT solver Z3  
 is effective in verifying properties of many benchmark programs that contain conditional statements in their loops,  
 and compares favorably to other recurrence-based verification tools. Finally, we also consider extending  
 our results to computing closed-form solutions of recurrences with unknown initial values.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {76},
numpages = {28},
keywords = {loop summarization, invariant generation, Recurrence solving}
}

@software{10.1145/3554354,
author = {Wang, Chenglin and Lin, Fangzhen},
title = {Reproduction package for paper "Solving Conditional Linear Recurrences for Program Verification: The Periodic Case"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3554354},
abstract = {
    <p>This repo contains a conditional recurrence solver which tries to solve a conditional linear recurrence. As claimed by the Theorem 5.2 in the paper, if the index sequence of the input recurrence is ultimately periodic and the periodic constraints (formula (14) in the paper) lie in language <span class="math inline">ℒ</span>, then a closed-form solution to the recurrence will be computed successfully.</p>
<p>Solving parameterized conditional linear recurrences (Sec 7.1 of the paper), in which the initial values are unknown, is also be implemented.</p>
<p>To show the effectiveness of our recurrence solver for program verification, a program verifier that tries to verify the correctness of an assertion in a C program is also implemented.</p>

},
keywords = {invariant generation, loop summarization, Recurrence solving}
}

@article{10.1145/3586029,
author = {Lin, Zhengyao and Chen, Xiaohong and Trinh, Minh-Thai and Wang, John and Ro\c{s}u, Grigore},
title = {Generating Proof Certificates for a Language-Agnostic Deductive Program Verifier},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586029},
doi = {10.1145/3586029},
abstract = {Previous work on rewriting and reachability logic establishes a vision for a language-agnostic program verifier, which takes three inputs: a program, its formal specification, and the formal semantics of the programming language in which the program is written. The verifier then uses a language-agnostic verification algorithm to prove the program correct with respect to the specification and the formal language semantics. Such a complex verifier can easily have bugs. This paper proposes a method to certify the correctness of each successful verification run by generating a proof certificate. The proof certificate can be checked by a small proof checker. The preliminary experiments apply the method to generate proof certificates for program verification in an imperative language, a functional language, and an assembly language, showing that the proposed method is language-agnostic.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {77},
numpages = {29},
keywords = {Reachability Logic, Program Verification, Matching Logic}
}

@software{10.5281/zenodo.7503088,
author = {Lin, Zhengyao and Chen, Xiaohong and Trinh, Minh-Thai and Wang, John and Ro\c{s}u, Grigore},
title = {Reproduction Docker Image for `Generating Proof Certificates for a Language-Agnostic Deductive Program Verifier'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7503088},
abstract = {
    <p>This artifact is an Docker image containing code and experiment setup for our paper.</p>

},
keywords = {Matching Logic, Program Verification, Reachability Logic}
}

@article{10.1145/3586030,
author = {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
title = {Grounded Copilot: How Programmers Interact with Code-Generating Models},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586030},
doi = {10.1145/3586030},
abstract = {Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants—with a range of prior experience using the assistant—as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {78},
numpages = {27},
keywords = {Program Synthesis, Grounded Theory, AI Assistants}
}

@software{10.5281/zenodo.7713789,
author = {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
title = {Replication Package for Article: "Grounded Copilot: How Programmers Interact with Code-Generating Models"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7713789},
abstract = {
    <p>This artifact contains: - The scripts to generate our plots - Detailed study information to re-run our user study - Livestreams that we observed and included in our dataset - Our codebook</p>

},
keywords = {AI Assistants, Grounded Theory, Program Synthesis}
}

@article{10.1145/3586031,
author = {Gheri, Lorenzo and Yoshida, Nobuko},
title = {Hybrid Multiparty Session Types: Compositionality for Protocol Specification through Endpoint Projection},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586031},
doi = {10.1145/3586031},
abstract = {Multiparty session types (MPST) are a specification and verification framework for distributed message-passing systems. The communication protocol of the system is specified as a global type, from which a collection of local types (local process implementations) is obtained by endpoint projection. A global type is a single disciplining entity for the whole system, specified by one designer that has full knowledge of the communication protocol. On the other hand, distributed systems are often described in terms of their components: a different designer is in charge of providing a subprotocol for each component. The problem of modular specification of global protocols has been addressed in the literature, but the state of the art focuses only on dual input/output compatibility. Our work overcomes this limitation. We propose the first MPST theory of multiparty compositionality for distributed protocol specification that is semantics-preserving, allows the composition of two or more components, and retains full MPST expressiveness. We introduce hybrid types for describing subprotocols interacting with each other, define a novel compatibility relation, explicitly describe an algorithm for composing multiple subprotocols into a well-formed global type, and prove that compositionality preserves projection, thus retaining semantic guarantees, such as liveness and deadlock freedom. Finally, we test our work against real-world case studies and we smoothly extend our novel compatibility to MPST with delegation and explicit connections.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {79},
numpages = {31},
keywords = {protocol design, multiparty session types, concurrency, compositionality}
}

@article{10.1145/3586032,
author = {Krogmeier, Paul and Madhusudan, P.},
title = {Languages with Decidable Learning: A Meta-theorem},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586032},
doi = {10.1145/3586032},
abstract = {We study expression learning problems with syntactic restrictions and introduce the class of finite-aspect checkable languages to characterize symbolic languages that admit decidable learning. The semantics of such languages can be defined using a bounded amount of auxiliary information that is independent of expression size but depends on a fixed structure over which evaluation occurs. We introduce a generic programming language for writing programs that evaluate expression syntax trees, and we give a meta-theorem that connects such programs for finite-aspect checkable languages to finite tree automata, which allows us to derive new decidable learning results and decision procedures for several expression learning problems by writing programs in the programming language.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {80},
numpages = {29},
keywords = {version space algebra, tree automata, program synthesis, learning symbolic languages, interpretable learning, exact learning}
}

@article{10.1145/3586033,
author = {Wagner, Christopher and Jaber, Nouraldin and Samanta, Roopsha},
title = {Enabling Bounded Verification of Doubly-Unbounded Distributed Agreement-Based Systems via Bounded Regions},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586033},
doi = {10.1145/3586033},
abstract = {The ubiquity of distributed agreement protocols, such as consensus, has galvanized interest in verification of such protocols as well as applications built on top of them. The complexity and unboundedness of such systems, however, makes their verification onerous in general, and, particularly prohibitive for full automation. An exciting, recent breakthrough reveals that, through careful modeling, it becomes possible to reduce verification of interesting distributed agreement-based (DAB) systems, that are unbounded in the number of processes, to model checking of small, finite-state systems. It is an open question if such reductions are also possible for DAB systems that are doubly-unbounded, in particular, DAB systems that additionally have unbounded data domains. We answer this question in the affirmative in this work thereby broadening the class of DAB systems which can be automatically and efficiently verified. We present a novel reduction which leverages value symmetry and a new notion of data saturation to reduce verification of doubly-unbounded DAB systems to model checking of small, finite-state systems. We develop a tool, Venus, that can efficiently verify sophisticated DAB system models such as the arbitration mechanism for a consortium blockchain, a distributed register, and a simple key-value store.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {81},
numpages = {29},
keywords = {Reduction, Layered Verification, Data Saturation}
}

@software{10.5281/zenodo.7574712,
author = {Wagner, Christopher and Jaber, Nouraldin and Samanta, Roopsha},
title = {Enabling Bounded Verification of Doubly-Unbounded Distributed Agreement-Based Systems via Bounded Regions (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7574712},
abstract = {
    <p>Venus is a tool for formal verification of doubly-unbounded distributed agreement-based (DAB) systems that combines a variable domain reduction with a recent tool, QuickSilver (ref: https://zenodo.org/record/5501650), for parameterized verification of DAB systems with finite-state processes.</p>

},
keywords = {Data Saturation, Layered Verification, Reduction}
}

@article{10.1145/3586034,
author = {Wang, Bo and Kolluri, Aashish and Nikoli\'{c}, Ivica and Baluta, Teodora and Saxena, Prateek},
title = {User-Customizable Transpilation of Scripting Languages},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586034},
doi = {10.1145/3586034},
abstract = {A transpiler converts code from one programming language to another. Many practical uses of transpilers require the user to be able to guide or customize the program produced from a given input program. This customizability is important for satisfying many application-specific goals for the produced code such as ensuring performance, readability, ease of exposition or maintainability, compatibility with external environment or analysis tools, and so on. Conventional transpilers are deterministic rule-driven systems often written without offering customizability per user and per program. Recent advances in transpilers based on neural networks offer some customizability to users, e.g. through interactive prompts, but they are still difficult to precisely control the production of a desired output. Both conventional and neural transpilation also suffer from the "last mile" problem: they produce correct code on average, i.e., on most parts of a given program, but not necessarily for all parts of it. We propose a new transpilation approach that offers fine-grained customizability and reusability of transpilation rules created by others, without burdening the user to understand the global semantics of the given source program. Our approach is mostly automatic and incremental, i.e., constructs translation rules needed to transpile the given program as per the user's guidance piece-by-piece. Users can rely on existing transpilation rules to translate most of the program correctly while focusing their effort locally, only on parts that are incorrect or need customization. This improves the correctness of the end result. We implement the transpiler as a tool called DuoGlot, which translates Python to Javascript programs, and evaluate it on the popular GeeksForGeeks benchmarks. DuoGlot achieves 90\% translation accuracy and so it outperforms all existing translators (both handcrafted and neural-based), while it produces readable code. We evaluate DuoGlot on two additional benchmarks, containing more challenging and longer programs, and similarly observe improved accuracy compared to the other transpilers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {82},
numpages = {29},
keywords = {Program Translation, Program Synthesis}
}

@software{10.5281/zenodo.7709003,
author = {Wang, Bo and Kolluri, Aashish and Nikoli\'{c}, Ivica and Baluta, Teodora and Saxena, Prateek},
title = {DuoGlot: User-Customizable Transpilation of Scripting Languages (Artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7709003},
abstract = {
    <p>This is the artifact named DuoGlot from the paper User-Customizable Transpilation of Scripting Languages accepted by the conference OOPSLA 2023. DuoGlot is a customizable code translator. Translation rules in DuoGlot are synthesized from user-provided code snippets. DuoGlot iteratively explores possible translations given the provided translation rules until finding a translation that passes tests. It currently supports customized translation from Python to JavaScript for single-file standalone programs. This artifact includes code and datasets to reproduce all the main results in the paper.</p>

},
keywords = {Program Synthesis, Program Translation}
}

@article{10.1145/3586035,
author = {Zhang, Xing and Guo, Guanchen and He, Xiao and Hu, Zhenjiang},
title = {Bidirectional Object-Oriented Programming: Towards Programmatic and Direct Manipulation of Objects},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586035},
doi = {10.1145/3586035},
abstract = {Many bidirectional programming languages, which are mainly functional and relational, have been designed to support writing programs that run in both forward and backward directions. Nevertheless, there is little study on the bidirectionalization of object-oriented languages that are more popular in practice. This paper presents the first bidirectional object-oriented language that supports programmatic and direct manipulation of objects. Specifically, we carefully extend a core object-oriented language, which has a standard forward evaluation semantics, with backward updating semantics for class inheritance hierarchies and references. We formally prove that the bidirectional evaluation semantics satisfies the round-tripping properties if the output is altered consistently. To validate the utility of our approach, we have developed a tool called BiOOP for generating HTML documents through bidirectional GUI design. We evaluate the expressiveness and effectiveness of BiOOP for HTML webpage development by reproducing ten classic object-oriented applications from a Java Swing tutorial and one large project from GitHub. The experimental results show the response time of direct manipulation programming on object-oriented programs that produce HTML webpages is acceptable for developers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {83},
numpages = {26},
keywords = {Object-Oriented Programming, Language Design and Implementation, Direct Manipulation, Bidirectional Transformation}
}

@software{10.5281/zenodo.7698353,
author = {Zhang, Xing and Guo, Guanchen and He, Xiao and Hu, Zhenjiang},
title = {Reproduction Package for Article 'Bidirectional Object-Oriented Programming: Towards Programmatic and Direct Manipulation of Objects'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7698353},
abstract = {
    <p>Our paper presents a bidirectional object-oriented language BiFJ (a Featherweight-Java-like language) that supports programmatic and direct manipulation of objects. The artifact BiOOP is a programming environment to support BiFJ in single web-page GUI design. As shown below, BiOOP supports developers not only to write object-oriented programs in the left editor and get the output (e.g., a web page) in the right Output window, but also to directly manipulate the output on the right, and automatically synchronize the manipulated output with the object-oriented program.</p>

},
keywords = {Bidirectional Transformation, Direct Manipulation Programming, Object-oriented Programming}
}

@article{10.1145/3586036,
author = {Ye, Wenjia and Toro, Mat\'{\i}as and Olmedo, Federico},
title = {A Gradual Probabilistic Lambda Calculus},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586036},
doi = {10.1145/3586036},
abstract = {Probabilistic programming languages have recently gained a lot of attention, in particular due to their applications in domains such as machine learning and differential privacy. To establish invariants of interest, many such languages include some form of static checking in the form of type systems. However, adopting such a type discipline can be cumbersome or overly conservative.  
Gradual typing addresses this problem by supporting a smooth transition between static and dynamic checking, and has been successfully applied for languages with different constructs and type abstractions. Nevertheless, its benefits have never been explored in the context of probabilistic languages.  
In this work, we present and formalize GPLC, a gradual source probabilistic lambda calculus. GPLC includes a binary probabilistic choice operator and allows programmers to gradually introduce/remove static type–and probability–annotations. The static semantics of GPLC heavily relies on the notion of probabilistic couplings, as required for defining several relations, such as consistency, precision, and consistent transitivity. The dynamic semantics of GPLC is given via elaboration to the target language TPLC, which features a distribution-based semantics interpreting programs as probability distributions over final values. Regarding the language metatheory, we establish that TPLC–and therefore also GPLC–is type safe and satisfies two of the so-called refined criteria for gradual languages, namely, that it is a conservative extension of a fully static variant and that it satisfies the gradual guarantee, behaving smoothly with respect to type precision.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {84},
numpages = {30},
keywords = {Type Systems, Probabilistic Lambda Calculus, Gradual Typing}
}

@article{10.1145/3586037,
author = {Lattuada, Andrea and Hance, Travis and Cho, Chanhee and Brun, Matthias and Subasinghe, Isitha and Zhou, Yi and Howell, Jon and Parno, Bryan and Hawblitzel, Chris},
title = {Verus: Verifying Rust Programs using Linear Ghost Types},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586037},
doi = {10.1145/3586037},
abstract = {The Rust programming language provides a powerful type system that checks linearity and borrowing, allowing code to safely manipulate memory without garbage collection and making Rust ideal for developing low-level, high-assurance systems. For such systems, formal verification can be useful to prove functional correctness properties beyond type safety. This paper presents Verus, an SMT-based tool for formally verifying Rust programs.  
With Verus, programmers express proofs and specifications using the Rust language, allowing proofs to take advantage of Rust's linear types and borrow checking. We show how this allows proofs to manipulate linearly typed permissions that let Rust code safely manipulate memory, pointers, and concurrent resources. Verus organizes proofs and specifications using a novel mode system that distinguishes specifications, which are not checked for linearity and borrowing, from executable code and proofs, which are checked for linearity and borrowing.  
We formalize Verus' linearity, borrowing, and modes in a small lambda calculus, for which we prove type safety and termination of specifications and proofs. We demonstrate Verus on a series of examples, including pointer-manipulating code (an xor-based doubly linked list), code with interior mutability, and concurrent code.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {85},
numpages = {30},
keywords = {systems verification, linear types, Rust}
}

@software{10.5281/zenodo.7511039,
author = {Lattuada, Andrea and Hance, Travis and Cho, Chanhee and Brun, Matthias and Subasinghe, Isitha and Zhou, Yi and Howell, Jon and Parno, Bryan and Hawblitzel, Chris},
title = {Software Artifact (virtual machine, pre-built distributions) for "Verus: Verifying Rust Programs using Linear Ghost Types"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7511039},
abstract = {
    <p>This is the software artifact accompanying the OOPSLA 2023 Paper “Verus: Verifying Rust Programs using Linear Ghost Types”. Verus is an SMT-based tool for formally verifying Rust programs. With Verus, programmers express proofs and specifications using the Rust language, allowing proofs to take advantage of Rust’s linear types and borrow checking. We show how this allows proofs to manipulate linearly typed permissions that let Rust code safely manipulate memory, pointers, and concurrent resources. We demonstrate Verus on a series of examples, including pointer-manipulating code (an xor-based doubly linked list), code with interior mutability, and concurrent code.</p>
<p>The artifact contains a virtual machine image and pre-built distributions of Verus, and the examples and scripts used for evaluation in the paper. The artifact demonstrates that (i) Verus runs correctly, (ii) it successfully verifies example code that exercises the paper’s claims and (iii) the examples verify quickly. More detail is available on the artifact page on Zenodo.</p>

},
keywords = {linear types, Rust, systems verification}
}

@article{10.1145/3586038,
author = {Zhou, Jie and Criswell, John and Hicks, Michael},
title = {Fat Pointers for Temporal Memory Safety of C},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586038},
doi = {10.1145/3586038},
abstract = {Temporal memory safety bugs, especially use-after-free and double free bugs, pose a major security threat to C programs. Real-world exploits utilizing these bugs enable attackers to read and write arbitrary memory locations, causing disastrous violations of confidentiality, integrity, and availability. Many previous solutions retrofit temporal memory safety to C, but they all either incur high performance overhead and/or miss detecting certain types of temporal memory safety bugs.  

In this paper, we propose a temporal memory safety solution that is both efficient and comprehensive. Specifically, we extend Checked C, a spatially-safe extension to C, with temporally-safe pointers. These are implemented by combining two techniques: fat pointers and dynamic key-lock checks. We show that the fat-pointer solution significantly improves running time and memory overhead compared to the disjoint-metadata approach that provides the same level of protection. With empirical program data and hands-on experience porting real-world applications, we also show that our solution is practical in terms of backward compatibility---one of the major complaints about fat pointers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {86},
numpages = {32},
keywords = {Temporal Memory Safety, Fat Pointers, Checked C}
}

@software{10.5281/zenodo.7719431,
author = {Zhou, Jie and Criswell, John and Hicks, Michael},
title = {Artifact of the `Fat Pointers for Temporal Memory Safety of C Paper` of OOPSLA23},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7719431},
abstract = {
    <p>This artifact provides a VM and a docker image that contain all the source code to perform experiments of the Fat Pointers for Temporal Memory Safety of C Paper of OOPSLA’23.</p>

},
keywords = {Checked C, Fat Pointers, LLVM, Temporal Memory Safety}
}

@article{10.1145/3586039,
author = {Kang, Chan Gu and Oh, Hakjoo},
title = {Modular Component-Based Quantum Circuit Synthesis},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586039},
doi = {10.1145/3586039},
abstract = {In this article, we present a novel method for synthesizing quantum circuits from user-supplied components. Given input-output state vectors and component quantum gates, our synthesizer aims to construct a quantum circuit that implements the provided functionality in terms of the supplied component gates.&nbsp;To achieve this, we basically use an enumerative search with pruning. To accelerate the procedure, however, we perform the search and pruning at the module level; instead of simply enumerating candidate circuits by appending component gates in sequence, we stack modules, which are groups of gate operations.  
With this modular approach, we can effectively reduce the search space by directing the search in a way that bridges the gap between the current circuit and the input-output specification.  
Evaluation on 17 benchmark problems shows that our technique is highly effective at synthesizing quantum circuits. Our method successfully synthesized 16 out of 17 benchmark circuits in 96.6 seconds on average. On the other hand, the conventional, gate-level synthesis algorithm succeeded in 10 problems with an average time of 639.1 seconds. Our algorithm increased the speed of the baseline by 20.3x for the 10 problems commonly solved by both approaches.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {87},
numpages = {28},
keywords = {Quantum programming, Quantum circuit synthesis}
}

@software{10.5281/zenodo.7710436,
author = {Kang, Chan Gu and Oh, Hakjoo},
title = {Artifact for paper "Modular Component-Based Quantum Circuit Synthesis"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7710436},
abstract = {
    <p>This artifact manual aims to reproduce results in the paper “Modular Component-based Quantum Circuit Synthesis” submitted to OOPSLA 2023. Our algorithm produces a quantum circuit, given user-provided in/output spec and component gates for circuit synthesis. Following the artifact manual (attached artifiact_manual.pdf) will give:</p>
<ul>
<li>Reproduction of Table 3 in our paper, which is main result of our synthesis experiment</li>
<li>Explanation on how to give new input (i.e, new synthesis problem) to our program</li>
<li>Reproduction of qiskit’s transpiled circuit appeared in Figure 2, 7, 8 of our paper</li>
</ul>
<p>Our project also can be found in Github Repository (https://github.com/kupl/qsyn).</p>

},
keywords = {Quantum circuit synthesis, Quantum programming}
}

@article{10.1145/3586040,
author = {Fox, Anthony C. J. and Stockwell, Gareth and Xiong, Shale and Becker, Hanno and Mulligan, Dominic P. and Petri, Gustavo and Chong, Nathan},
title = {A Verification Methodology for the Arm® Confidential Computing Architecture: From a Secure Specification to Safe Implementations},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586040},
doi = {10.1145/3586040},
abstract = {We present Arm's efforts in verifying the specification and prototype reference implementation of the Realm Management Monitor (RMM), an essential firmware component of Arm Confidential Computing Architecture (Arm CCA), the recently-announced Confidential Computing technologies incorporated in the Armv9-A architecture. Arm CCA introduced the Realm Management Extension (RME), an architectural extension for Armv9-A, and a technology that will eventually be deployed in hundreds of millions of devices. Given the security-critical nature of the RMM, and its taxing threat model, we use a combination of interactive theorem proving, model checking, and concurrency-aware testing to validate and verify security and safety properties of both the specification and a prototype implementation of the RMM. Crucially, our verification efforts were, and are still being, developed and refined contemporaneously with active development of both specification and implementation, and have been adopted by Arm's product teams.  

We describe our major achievements, realized through the application of formal techniques, as well as challenges that remain for future work. We believe that the work reported in this paper is the most thorough application of formal techniques to the design and implementation of any current commercially-viable Confidential Computing implementation, setting a new high-water mark for work in this area.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {88},
numpages = {30},
keywords = {separation kernel, operating system verification, formal methods, Confidential Computing, Arm Confidential Computing Architecture (Arm CCA)}
}

@article{10.1145/3586041,
author = {Menz, Jan and Hirsch, Andrew K. and Li, Peixuan and Garg, Deepak},
title = {Compositional Security Definitions for Higher-Order Where Declassification},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586041},
doi = {10.1145/3586041},
abstract = {To ensure programs do not leak private data, we often want to be able to provide formal guarantees ensuring such data is handled correctly. Often, we cannot keep such data secret entirely; instead programmers specify how private data may be declassified. While security definitions for declassification exist, they mostly do not handle higher-order programs. In fact, in the higher-order setting no compositional security definition exists for intensional information-flow properties such as where declassification, which allows declassification in specific parts of a program. We use logical relations to build a model (and thus security definition) of where declassification. The key insight required for our model is that we must stop enforcing indistinguishability once a relevant declassification has occurred. We show that the resulting security definition provides more security than the most related previous definition, which is for the lower-order setting.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {89},
numpages = {28},
keywords = {where declassification, relevant declassification, logical relations}
}

@article{10.1145/3586042,
author = {Shapira, Yuval and Avneri, Eran and Drachsler-Cohen, Dana},
title = {Deep Learning Robustness Verification for Few-Pixel Attacks},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586042},
doi = {10.1145/3586042},
abstract = {While successful, neural networks have been shown to be vulnerable to adversarial example attacks. In L0 adversarial attacks, also known as few-pixel attacks, the attacker picks t pixels from the image and arbitrarily perturbs them. To understand the robustness level of a network to these attacks, it is required to check the robustness of the network to perturbations of every set of t pixels. Since the number of sets is exponentially large, existing robustness verifiers, which can reason about a single set of pixels at a time, are impractical for L0 robustness verification. We introduce Calzone, an L0 robustness verifier for neural networks. To the best of our knowledge, Calzone is the first to provide a sound and complete analysis for L0 adversarial attacks. Calzone builds on the following observation: if a classifier is robust to any perturbation of a set of k pixels, for k&gt;t, then it is robust to any perturbation of its subsets of size t. Thus, to reduce the verification time, Calzone predicts the largest k that can be proven robust, via dynamic programming and sampling. It then relies on covering designs to compute a covering of the image with sets of size k. For each set in the covering, Calzone submits its corresponding box neighborhood to an existing L∞ robustness verifier. If a set’s neighborhood is not robust, Calzone repeats this process and covers this set with sets of size k′&lt;k. We evaluate Calzone on several datasets and networks, for t≤ 5. Typically, Calzone verifies L0 robustness within few minutes. On our most challenging instances (e.g., t=5), Calzone completes within few hours. We compare to a MILP baseline and show that it does not scale already for t=3.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {90},
numpages = {28},
keywords = {Neural network verification, L0 adversarial example attacks}
}

@article{10.1145/3586043,
author = {Mulder, Ike and Krebbers, Robbert},
title = {Proof Automation for Linearizability in Separation Logic},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586043},
doi = {10.1145/3586043},
abstract = {Recent advances in concurrent separation logic enabled the formal verification of increasingly sophisticated fine-grained (i.e., lock-free) concurrent programs. For such programs, the golden standard of correctness is linearizability, which expresses that concurrent executions always behave as some valid sequence of sequential executions. Compositional approaches to linearizability (such as contextual refinement and logical atomicity) make it possible to prove linearizability of whole programs or compound data structures (e.g., a ticket lock) using proofs of linearizability of their individual components (e.g., a counter). While powerful, these approaches are also laborious—state-of-the-art tools such as Iris, FCSL, and Voila all require a form of interactive proof. This paper develops proof automation for contextual refinement and logical atomicity in Iris. The key ingredient of our proof automation is a collection of proof rules whose application is directed by both the program and the logical state. This gives rise to effective proof search strategies that can prove linearizability of simple examples fully automatically. For more complex examples, we ensure the proof automation cooperates well with interactive proof tactics by minimizing the use of backtracking. We implement our proof automation in Coq by extending and generalizing Diaframe, a proof automation extension for Iris. While the old version (Diaframe 1.0) was limited to ordinary Hoare triples, the new version (Diaframe 2.0) is extensible in its support for program verification styles: our proof search strategies for contextual refinement and logical atomicity are implemented as modules for Diaframe 2.0. We evaluate our proof automation on a set of existing benchmarks and novel proofs, showing that it provides significant reduction of proof work for both approaches to linearizability.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {91},
numpages = {30},
keywords = {refinement, proof automation, logical atomicity, linearizability, fine-grained concurrency, Separation logic, Iris, Coq}
}

@software{10.5281/zenodo.7712620,
author = {Mulder, Ike and Krebbers, Robbert},
title = {Artifact of 'Proof Automation for Linearizability in Separation Logic'},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7712620},
abstract = {
    <p>This is the artifact for the OOPSLA ‘23 paper ’Proof Automation for Linearizability in Separation Logic’. It contains the Diaframe 2.0 source code, a VM containing a compiled version of this source code, and instructions for evaluation.</p>
<p>Diaframe 2.0’s current development can be found at https://gitlab.mpi-sws.org/iris/diaframe .</p>

},
keywords = {automated reasoning, Coq, Iris, program verification, Separation logic}
}

@article{10.1145/3586044,
author = {Le Glaunec, Alexis and Kong, Lingkun and Mamouras, Konstantinos},
title = {Regular Expression Matching using Bit Vector Automata},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586044},
doi = {10.1145/3586044},
abstract = {Regular expressions (regexes) are ubiquitous in modern software. There is a variety of implementation techniques for regex matching, which can be roughly categorized as (1) relying on backtracking search, or (2) being based on finite-state automata. The implementations that use backtracking are often chosen due to their ability to support advanced pattern-matching constructs. Unfortunately, they are known to suffer from severe performance problems. For some regular expressions, the running time for matching can be exponential in the size of the input text. In order to provide stronger guarantees of matching efficiency, automata-based regex matching is the preferred choice. However, even these regex engines may exhibit severe performance degradation for some patterns. The main reason for this is that regexes used in practice are not exclusively built from the classical regular constructs, i.e., concatenation, nondeterministic choice and Kleene's star. They involve additional constructs that provide succinctness and convenience of expression. The most common such construct is bounded repetition (also called counting), which describes the repetition of the pattern a fixed number of times.  

In this paper, we propose a new algorithm for the efficient matching of regular expressions that involve bounded repetition. Our algorithms are based on a new model of automata, which we call nondeterministic bit vector automata (NBVA). This model is chosen to be expressively equivalent to nondeterministic counter automata with bounded counters, a very natural model for expressing patterns with bounded repetition. We show that there is a class of regular expressions with bounded repetition that can be matched in time that is independent from the repetition bounds. Our algorithms are general enough to cover the vast majority of challenging bounded repetitions that arise in practice. We provide an implementation of our approach in a regex engine, which we call BVA-Scan. We compare BVA-Scan against state-of-the-art regex engines on several real datasets.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {92},
numpages = {30},
keywords = {regex, counter automata, bounded repetition, automata theory}
}

@article{10.1145/3586045,
author = {Zilberstein, Noam and Dreyer, Derek and Silva, Alexandra},
title = {Outcome Logic: A Unifying Foundation for Correctness and Incorrectness Reasoning},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586045},
doi = {10.1145/3586045},
abstract = {Program logics for bug-finding (such as the recently introduced Incorrectness Logic) have framed correctness and incorrectness as dual concepts requiring different logical foundations. In this paper, we argue that a single unified theory can be used for both correctness and incorrectness reasoning. We present Outcome Logic (OL), a novel generalization of Hoare Logic that is both monadic (to capture computational effects) and monoidal (to reason about outcomes and reachability). OL expresses true positive bugs, while retaining correctness reasoning abilities as well. To formalize the applicability of OL to both correctness and incorrectness, we prove that any false OL specification can be disproven in OL itself. We also use our framework to reason about new types of incorrectness in nondeterministic and probabilistic programs. Given these advances, we advocate for OL as a new foundational theory of correctness and incorrectness.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {93},
numpages = {29},
keywords = {Program Logics, Incorrectness Reasoning, Hoare Logic}
}

@article{10.1145/3586046,
author = {Emre, Mehmet and Boyland, Peter and Parekh, Aesha and Schroeder, Ryan and Dewey, Kyle and Hardekopf, Ben},
title = {Aliasing Limits on Translating C to Safe Rust},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586046},
doi = {10.1145/3586046},
abstract = {The Rust language was created to provide safe low-level systems programming. There is both industrial and academic interest in the problem of (semi-)automatically translating C code to Rust in order to exploit Rust's safety guarantees. We study the effectiveness and limitations of existing techniques for automatically translating unsafe raw pointers (in Rust programs translated from C) into safe Rust references via ownership and lifetime inference. Our novel evaluation methodology enables our study to extend beyond prior studies, and to discover new information contradicting the conclusions of prior studies. We find that existing translation methods are severely limited by a lack of precision in the Rust compiler's safety checker, causing many safe pointer manipulations to be labeled as potentially unsafe. Leveraging this information, we propose methods for improving translation, based on encoding the results of a more precise analysis in a manner that is understandable to an unmodified Rust compiler. We implement one of our proposed methods, increasing the number of pointers that can be translated to safe Rust references by 75\% over the baseline (from 12\% to 21\% of all pointers).},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {94},
numpages = {29},
keywords = {Translation, Rust, Memory Safety, Empirical Study, C}
}

@software{10.5281/zenodo.7714175,
author = {Emre, Mehmet and Boyland, Peter and Parekh, Aesha and Schroeder, Ryan and Dewey, Kyle and Hardekopf, Ben},
title = {Artifact for "Aliasing Limits on Translating C to Safe Rust"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7714175},
abstract = {
    <p>The artifact is a Docker image that contains (1) the prototype of our pseudo-safety transformations, (2) the affected pointer set analysis we implemented on top of SVF, (3) Laertes with our extensions for directionality, and (4) data analysis scripts to produce the tables and the figures in the paper. It also contains C2Rust, SVF, and Laertes themselves to run these prototypes, as well as a corpus of programs we use to evaluate our method.</p>

},
keywords = {C, Empirical Study, Memory Safety, Rust, Translation}
}

@article{10.1145/3586047,
author = {Zhang, Guoqiang and Mariano, Benjamin and Shen, Xipeng and Dillig, I\c{s}\i{}l},
title = {Automated Translation of Functional Big Data Queries to SQL},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586047},
doi = {10.1145/3586047},
abstract = {Big data analytics frameworks like Apache Spark and Flink enable users to implement queries over large, distributed databases using functional APIs. In recent years, these APIs have grown in popularity because their functional interfaces abstract away much of the minutiae of distributed programming required by traditional query languages like SQL. However, the convenience of these APIs comes at a cost because functional queries are often less efficient than their SQL counterparts. Motivated by this observation, we present a new technique for automatically transpiling functional queries to SQL. While our approach is based on the standard paradigm of counterexample-guided inductive synthesis, it uses a novel column-wise decomposition technique to split the synthesis task into smaller subquery synthesis problems. We have implemented this approach as a new tool called RDD2SQL for translating Spark RDD queries to SQL and empirically evaluate the effectiveness of RDD2SQL on a set of real-world RDD queries. Our results show that (1) most RDD queries can be translated to SQL, (2) our tool is very effective at automating this translation, and (3) performing this translation offers significant performance benefits.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {95},
numpages = {29},
keywords = {source-to-source compiler, query optimization, program synthesis}
}

@article{10.1145/3586048,
author = {Yuan, Yongwei and Guest, Scott and Griffis, Eric and Potter, Hannah and Moon, David and Omar, Cyrus},
title = {Live Pattern Matching with Typed Holes},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586048},
doi = {10.1145/3586048},
abstract = {Several modern programming systems, including GHC Haskell, Agda, Idris, and Hazel, support typed holes. Assigning static and, to varying degree, dynamic meaning to programs with holes allows program editors and other tools to offer meaningful feedback and assistance throughout editing, i.e. in a live manner. Prior work, however, has considered only holes appearing in expressions and types. This paper considers, from type theoretic and logical first principles, the problem of typed pattern holes. We confront two main difficulties, (1) statically reasoning about exhaustiveness and irredundancy when patterns are not fully known, and (2) live evaluation of expressions containing both pattern and expression holes. In both cases, this requires reasoning conservatively about all possible hole fillings. We develop a typed lambda calculus, Peanut, where reasoning about exhaustiveness and redundancy is mapped to the problem of deriving first order entailments. We equip Peanut with an operational semantics in the style of Hazelnut Live that allows us to evaluate around holes in both expressions and patterns. We mechanize the metatheory of Peanut in Agda and formalize a procedure capable of deciding the necessary entailments. Finally, we scale up and implement these mechanisms within Hazel, a programming environment for a dialect of Elm that automatically inserts holes during editing to provide static and dynamic feedback to the programmer in a maximally live manner, i.e. for every possible editor state. Hazel is the first maximally live environment for a general-purpose functional language.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {96},
numpages = {27},
keywords = {typed holes, pattern matching}
}

@software{10.5281/zenodo.7713722,
author = {Yuan, Yongwei and Guest, Scott and Griffis, Eric and Potter, Hannah and Moon, David and Omar, Cyrus},
title = {Artifact for "Live Pattern Matching with Typed Holes"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7713722},
abstract = {
    <p>The artifact includes a proof mechanization that concludes the type safety of the system presented in the paper, and a minimal solver-based implementation of exhaustiveness and redundancy checker.</p>

},
keywords = {Functional Programming, Pattern Matching, Typed Holes}
}

@article{10.1145/3586049,
author = {Xu, Zhenyang and Tian, Yongqiang and Zhang, Mengxiao and Zhao, Gaosen and Jiang, Yu and Sun, Chengnian},
title = {Pushing the Limit of 1-Minimality of Language-Agnostic Program Reduction},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586049},
doi = {10.1145/3586049},
abstract = {Program reduction has demonstrated its usefulness in facilitating debugging language implementations in practice, by minimizing bug-triggering programs. There are two categories of program reducers: language-agnostic program reducers (AGRs) and language-specific program reducers (SPRs). AGRs, such as HDD and Perses, are generally applicable to various languages; SPRs are specifically designed for one language with meticulous thoughts and significant engineering efforts, e.g., C-Reduce for reducing C/C++ programs.  

Program reduction is an NP-complete problem: finding the globally minimal program is usually infeasible. Thus all existing program reducers resort to producing 1-minimal results, a special type of local minima. However, 1-minimality can still be large and contain excessive bug-irrelevant program elements. This is especially the case for AGR-produced results because of the generic reduction algorithms used in AGRs. An SPR often yields smaller results than AGRs for the language for which the SPR has customized reduction algorithms. But SPRs are not language-agnostic, and implementing a new SPR for a different language requires significant engineering efforts.  

This paper proposes Vulcan, a language-agnostic framework to further minimize AGRs-produced results by exploiting the formal syntax of the language to perform aggressive program transformations, in hope of creating reduction opportunities for other reduction algorithms to progress or even directly deleting bugirrelevant elements from the results. Our key insights are two-fold. First, the program transformations in all existing program reducers including SPRs are not diverse enough, which traps these program reducers early in 1-minimality. Second, compared with the original program, the results of AGRs are much smaller, and time-wise it is affordable to perform diverse program transformations that change programs but do not necessarily reduce the sizes of the programs directly. Within the Vulcan framework, we proposed three simple examples of fine-grained program transformations to demonstrate that Vulcan can indeed further push the 1-minimality of AGRs. By performing these program transformations, a 1-minimal program might become a non-1-minimal one that can be further reduced later.  

Our extensive evaluations on multilingual benchmarks including C, Rust and SMT-LIBv2 programs strongly demonstrate the effectiveness and generality of Vulcan. Vulcan outperforms the state-of-the-art language-agnostic program reducer Perses in size in all benchmarks: On average, the result of Vulcan contains 33.55\%, 21.61\%, and 31.34\% fewer tokens than that of Perses on C, Rust, and SMT-LIBv2 subjects respectively. Vulcan can produce even smaller results if more reduction time is allocated. Moreover, for the C programs that are reduced by C-Reduce, Vulcan is even able to further minimize them by 10.07\%.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {97},
numpages = {29},
keywords = {Test Input Minimization, Program Reduction, Automated Debugging}
}

@software{10.5281/zenodo.8197652,
author = {Xu, Zhenyang and Tian, Yongqiang and Zhang, Mengxiao and Zhao, Gaosen and Jiang, Yu and Sun, Chengnian},
title = {Artifact for "Pushing the Limit of 1-Minimality of Language-Agnostic Program Reduction"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.8197652},
abstract = {
    <p>This artifact contains the source code, benchmarks, scripts, and documentation for reproduce the evaluation results described in the paper “Pushing the Limit of 1-Minimality of Language-Agnostic Program Reduction” accepted at OOPSLA 2023.</p>

},
keywords = {Automated Debugging, Program Reduction, Test Input Minimization}
}

@article{10.1145/3586050,
author = {Chiang, David and McDonald, Colin and Shan, Chung-chieh},
title = {Exact Recursive Probabilistic Programming},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586050},
doi = {10.1145/3586050},
abstract = {Recursive calls over recursive data are useful for generating probability distributions, and probabilistic programming allows computations over these distributions to be expressed in a modular and intuitive way. Exact inference is also useful, but unfortunately, existing probabilistic programming languages do not perform exact inference on recursive calls over recursive data, forcing programmers to code many applications manually. We introduce a probabilistic language in which a wide variety of recursion can be expressed naturally, and inference carried out exactly. For instance, probabilistic pushdown automata and their generalizations are easy to express, and polynomial-time parsing algorithms for them are derived automatically. We eliminate recursive data types using program transformations related to defunctionalization and refunctionalization. These transformations are assured correct by a linear type system, and a successful choice of transformations, if there is one, is guaranteed to be found by a greedy algorithm.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {98},
numpages = {31},
keywords = {recursive types, probabilistic programming, linear types}
}

@software{10.5281/zenodo.7720410,
author = {Chiang, David and McDonald, Colin and Shan, Chung-chieh},
title = {Reproduction Package for Article "Exact Recursive Probabilistic Programming"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7720410},
abstract = {
    <p>This Docker image contains everything needed to reproduce the experiments in the paper “Exact Recursive Probabilistic Programming.”</p>

},
keywords = {linear types, probabilistic programming, recursive types}
}

@article{10.1145/3586051,
author = {Feng, Shenghua and Chen, Mingshuai and Su, Han and Kaminski, Benjamin Lucien and Katoen, Joost-Pieter and Zhan, Naijun},
title = {Lower Bounds for Possibly Divergent Probabilistic Programs},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586051},
doi = {10.1145/3586051},
abstract = {We present a new proof rule for verifying lower bounds on quantities of probabilistic programs. Our proof rule is not confined to almost-surely terminating programs -- as is the case for existing rules -- and can be used to establish non-trivial lower bounds on, e.g., termination probabilities and expected values, for possibly divergent probabilistic loops, e.g., the well-known three-dimensional random walk on a lattice.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {99},
numpages = {31},
keywords = {weakest preexpectations, uniform integrability, quantitative verification, probabilistic programs, lower bounds, almost-sure termination}
}

@article{10.1145/3586052,
author = {Goharshady, Amir Kafshdar and Hitarth, S. and Mohammadi, Fatemeh and Motwani, Harshit Jitendra},
title = {Algebro-geometric Algorithms for Template-Based Synthesis of Polynomial Programs},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586052},
doi = {10.1145/3586052},
abstract = {Template-based synthesis, also known as sketching, is a localized approach to program synthesis in which the programmer provides not only a specification, but also a high-level "sketch" of the program. The sketch is basically a partial program that models the general intuition of the programmer, while leaving the low-level details as unimplemented "holes". The role of the synthesis engine is then to fill in these holes such that the completed program satisfies the desired specification. In this work, we focus on template-based synthesis of polynomial imperative programs with real variables, i.e. imperative programs in which all expressions appearing in assignments, conditions and guards are polynomials over program variables. While this problem can be solved in a sound and complete manner by a reduction to the first-order theory of the reals, the resulting formulas will contain a quantifier alternation and are extremely hard for modern SMT solvers, even when considering toy programs with a handful of lines. Moreover, the classical algorithms for quantifier elimination are notoriously unscalable and not at all applicable to this use-case.  

In contrast, our main contribution is an algorithm, based on several well-known theorems in polyhedral and real algebraic geometry, namely Putinar's Positivstellensatz, the Real Nullstellensatz, Handelman's Theorem and Farkas' Lemma, which sidesteps the quantifier elimination difficulty and reduces the problem directly to Quadratic Programming (QP). Alternatively, one can view our algorithm as an efficient way of eliminating quantifiers in the particular formulas that appear in the synthesis problem. The resulting QP instances can then be handled quite easily by SMT solvers. Notably, our reduction to QP is sound and semi-complete, i.e. it is complete if polynomials of a sufficiently high degree are used in the templates. Thus, we provide the first method for sketching-based synthesis of polynomial programs that does not sacrifice completeness, while being scalable enough to handle meaningful programs. Finally, we provide experimental results over a variety of examples from the literature.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {100},
numpages = {30},
keywords = {syntax-guided synthesis, sketching, program synthesis}
}

@software{10.5281/zenodo.7697453,
author = {Goharshady, Amir Kafshdar and Hitarth, S. and Mohammadi, Fatemeh and Motwani, Harshit Jitendra},
title = {PolySynth},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7697453},
abstract = {
    <p>PolySynth is a tool for synthesis of polynomial programs in a C-like imperative programming language where all assignments, guards, and assertions are restricted to be polynomial expression over the program variables. Our tool is implemented in Python3 is open source. Our algorithm, as described in our paper, is based on Farkas’ Lemma, Handelman Theorem, and Putinar’s Positivstellensatz.</p>
<p>The artifact code consists of the following main directories.</p>
<ol type="1">
<li><p>The directory <code>benchmarks-polysynth</code> contains all the benchmarks in theformat that can be passed to our tool PolySynth.</p></li>
<li><p>The directory <code>benchmarks-rosette</code> contains all the benchmarks that can bepassed to Rosette.</p></li>
<li><p>The directory <code>benchmarks-sketch</code> contains all the benchmarks that can bepassed to Sketch.</p></li>
<li><p>The directory <code>Code</code> contains the code for our main algorithms.</p></li>
<li><p>The directory <code>polysynth-outputs</code> contains the output programs and other intermediate files our tool Polysynth creates for all of the benchmarks.</p></li>
</ol>
<p>Follow the following steps to run the synthesizer on a given benchmark:</p>
<p>Open the terminal and change the directory to <code>Code/</code></p>
<p>All the examples/benchmarks are stored in the folder <code>benchmarks-polysynth</code></p>
<p>The easiest way to run an example is to type the following command: sh run_polysynth_all_benchmarks.sh from the root directory of the repository.</p>
<p>Suppose you want to run the synthesizer for the example Closest_cube_root, then you would run the following command:</p>
<p><code>python3 synthesizer.py --filename Examples/Closest_cube_root/closest_cube_root.c</code></p>

},
keywords = {algebro-geometric algorithm, polynomial programs, program synthesis, template-based synthesis}
}

@article{10.1145/3586053,
author = {Winter, Levin N. and Buse, Florena and de Graaf, Daan and von Gleissenthall, Klaus and Kulahcioglu Ozkan, Burcu},
title = {Randomized Testing of Byzantine Fault Tolerant Algorithms},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586053},
doi = {10.1145/3586053},
abstract = {Byzantine fault-tolerant algorithms promise agreement on a correct value, even if a subset of processes can deviate from the algorithm arbitrarily. While these algorithms provide strong guarantees in theory, in practice, protocol bugs and implementation mistakes may still cause them to go wrong. This paper introduces ByzzFuzz, a simple yet effective method for automatically finding errors in implementations of Byzantine fault-tolerant algorithms through randomized testing. ByzzFuzz detects fault-tolerance bugs by injecting randomly generated network and process faults into their executions. To navigate the space of possible process faults, ByzzFuzz introduces small-scope message mutations which mutate the contents of the protocol messages by applying small changes to the original message either in value (e.g., by incrementing the round number) or in time (e.g., by repeating a proposal value from a previous message). We find that small-scope mutations, combined with insights from the testing and fuzzing literature, are effective at uncovering protocol logic and implementation bugs in real-world fault-tolerant systems.  

We implemented ByzzFuzz and applied it to test the production implementations of two popular blockchain systems, Tendermint and Ripple, and an implementation of the seminal PBFT protocol. ByzzFuzz detected several bugs in the implementation of PBFT, a potential liveness violation in Tendermint, and materialized two theoretically described vulnerabilities in Ripple’s XRP Ledger Consensus Algorithm. Moreover, we discovered a previously unknown fault-tolerance bug in the production implementation of Ripple, which is confirmed by the developers and fixed.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {101},
numpages = {32},
keywords = {Random testing, Distributed consensus, Byzantine fault-tolerance}
}

@software{10.5281/zenodo.7510752,
author = {Winter, Levin N. and Buse, Florena and de Graaf, Daan and von Gleissenthall, Klaus and Kulahcioglu Ozkan, Burcu},
title = {Artifact for "Randomized Testing of Byzantine Fault Tolerant Consensus Algorithms"},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7510752},
abstract = {
    <p>This upload is a virtual machine (VM) containing the artifact accompanying our paper “Randomized Testing of Byzantine Fault Tolerant Algorithms”. The VirtualBox VM image contains the source code for our testing algorithm and the systems under test. The VM has all the dependencies resolved.</p>

},
keywords = {Byzantine fault-tolerance, Distributed algorithms, Distributed consensus, Random testing, Software testing and debugging}
}

@article{10.1145/3586054,
author = {Dardinier, Thibault and Parthasarathy, Gaurav and M\"{u}ller, Peter},
title = {Verification-Preserving Inlining in Automatic Separation Logic Verifiers},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586054},
doi = {10.1145/3586054},
abstract = {Bounded verification has proved useful to detect bugs and to increase confidence in the correctness of a program. In contrast to unbounded verification, reasoning about calls via (bounded) inlining and about loops via (bounded) unrolling does not require method specifications and loop invariants and, therefore, reduces the annotation overhead to the bare minimum, namely specifications of the properties to be verified. For verifiers based on traditional program logics, verification is preserved by inlining (and unrolling): successful unbounded verification of a program w.r.t. some annotation implies successful verification of the inlined program. That is, any error detected in the inlined program reveals a true error in the original program. However, this essential property might not hold for automatic separation logic verifiers such as Caper, GRASShopper, RefinedC, Steel, VeriFast, and verifiers based on Viper. In this setting, inlining generally changes the resources owned by method executions, which may affect automatic proof search algorithms and introduce spurious errors.  
In this paper, we present the first technique for verification-preserving inlining in automatic separation logic verifiers. We identify a semantic condition on programs and prove in Isabelle/HOL that it ensures verification-preserving inlining for state-of-the-art automatic separation logic verifiers. We also prove a dual result: successful verification of the inlined program ensures that there are method and loop annotations that enable the verification of the original program for bounded executions. To check our semantic condition automatically, we present two approximations that can be checked syntactically and with a program verifier, respectively. We implement these checks in Viper and demonstrate that they are effective for non-trivial examples from different verifiers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {102},
numpages = {30},
keywords = {Modular Verification, Loop Unrolling, Inlining, Bounded Verification}
}

@software{10.5281/zenodo.7711788,
author = {Dardinier, Thibault and Parthasarathy, Gaurav and M\"{u}ller, Peter},
title = {Verification-Preserving Inlining in Automatic Separation Logic Verifiers (artifact)},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7711788},
abstract = {
    <p>This artifact contains: 1. An Isabelle/HOL mechanization that fully supports the technical claims from the paper. 2. A reproducible analysis of the test suites of VeriFast, GRASShopper, RSL-Viper, and Nagini, corresponding to the results shown in table 1. 3. An inlining tool for Viper, which inlines calls and unrolls loops, while also checking the structural condition. 4. A test framework that runs the inlining tool on the examples in table 2 (main paper) and table 3 (appendix in the extended version).</p>

},
keywords = {Bounded Verification, GRASShopper, Isabelle, Isabelle/HOL, Loop Unrolling, Method Inlining, Modular Verification, Nagini, RSL-Viper, VeriFast, Viper}
}

@article{10.1145/3586055,
author = {Ji, Ruyi and Kong, Chaozhe and Xiong, Yingfei and Hu, Zhenjiang},
title = {Improving Oracle-Guided Inductive Synthesis by Efficient Question Selection},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586055},
doi = {10.1145/3586055},
abstract = {Oracle-guided inductive synthesis (OGIS) is a widely-used framework to apply program synthesis techniques in practice. The question selection problem aims at reducing the number of iterations in OGIS by selecting a proper input for each OGIS iteration. Theoretically, a question selector can generally improve the performance of OGIS solvers on both interactive and non-interactive tasks if it is not only effective for reducing iterations but also efficient. However, all existing effective question selectors fail in satisfying the requirement of efficiency. To ensure effectiveness, they convert the question selection problem into an optimization one, which is difficult to solve within a short time. In this paper, we propose a novel question selector, named LearnSy. LearnSy is both efficient and effective and thus achieves general improvement for OGIS solvers for the first time. Since we notice that the optimization tasks in previous studies are difficult because of the complex behavior of operators, we estimate these behaviors in LearnSy as simple random events. Subsequently, we provide theoretical results for the precision of this estimation and design an efficient algorithm for its calculation. According to our evaluation, when dealing with interactive tasks, LearnSy can offer competitive performance compared to existing selectors while being more efficient and more general. Moreover, when working on non-interactive tasks, LearnSy can generally reduce the time cost of existing CEGIS solvers by up to 43.0\%.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {103},
numpages = {29},
keywords = {Question Selection Problem, Oracle-Guided Inductive Synthesis}
}

@software{10.5281/zenodo.7722241,
author = {Ji, Ruyi and Kong, Chaozhe and Xiong, Yingfei and Hu, Zhenjiang},
title = {Artifact for OOPSLA'23: Improving Oracle-Guided Inductive Synthesis by Efficient Question Selection},
year = {2023},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.7722241},
abstract = {
    <p>Artifact for OOPSLA’23: Improving Oracle-Guided Inductive Synthesis by Efficient Question Selection</p>
<p>This project will be maintained at https://github.com/jiry17/LearnSy, and details about this artifact can be found in README.md.</p>

},
keywords = {Oracle-Guided Inductive Synthesis, Question Selection Problem}
}

@article{10.1145/3586056,
author = {M\"{u}ller, Marius and Schuster, Philipp and Brachth\"{a}user, Jonathan Immanuel and Ostermann, Klaus},
title = {Back to Direct Style: Typed and Tight},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586056},
doi = {10.1145/3586056},
abstract = {Translating programs into continuation-passing style is a well-studied  
tool to explicitly deal with the control structure of programs. This is  
useful, for example, for compilation.  
In a typed setting, there also is a logical interpretation of such a translation  
as an embedding of classical logic into intuitionistic logic.  
A naturally arising question is whether there is an inverse translation  
back to direct style. The answer to this question depends on how the  
continuation-passing translation is defined and on the domain of the inverse translation.  
In general, translating programs from continuation-passing style back to direct  
style requires the use of control operators to account for the use of  
continuations in non-trivial ways.  

We present two languages, one in direct style and one in continuation-passing  
style. Both languages are typed and equipped with an abstract machine semantics.  
Moreover, both languages allow for non-trivial control flow.  
We further present a translation to continuation-passing style and a translation  
back to direct style. We show that both translations are type-preserving and  
also preserve semantics in a very precise way giving an operational  
correspondence between the two languages.  
Moreover, we show that the compositions of the translations are well-behaved.  
In particular, they are syntactic one-sided inverses on the full language and full  
syntactic inverses when restricted to trivial control flow.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {104},
numpages = {28},
keywords = {direct-style translation, continuation-passing style}
}

@article{10.1145/3586057,
author = {Roth, Ori and Gil, Yossi},
title = {Fluent APIs in Functional Languages},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586057},
doi = {10.1145/3586057},
abstract = {Fluent API is an object-oriented pattern for elegant APIs and embedded DSLs.  
A smart fluent API can enforce the API protocol or DSL syntax at compile time.  
Since fluent API implementations typically rely on overloading function names, they are hard to realize in functional programming languages.  
This work shows how functional fluent APIs can be implemented in the absence of name overloading, by relying on parametric polymorphism and Hindley-Milner type inference.  
The implementation supports fluent API protocols in the regular- and deterministic context-free language classes, and even beyond.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {105},
numpages = {26},
keywords = {fluent API, embedded DSLs, API protocols}
}