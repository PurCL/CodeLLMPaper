# NeurIPS2023

Number of papers: 6

## [Language agents as hackers: Evaluating cybersecurity skills with capture the flag](paper_1.md)
- **Authors**: Yang, John and Prabhakar, Akshara and Yao, Shunyu and Pei, Kexin and Narasimhan, Karthik R
- **Abstract**: Amidst the advent of language models (LMs) and their wide-ranging capabilities, concerns have been raised about their implications with regards to privacy and security. In particular, the emergence of language agents as a promising aid for automating and augmenting digital work poses immediate quest...
- **Link**: [Read Paper](https://openreview.net/forum?id=KOZwk7BFc3&noteId=OIANITRY6R)
- **Labels**: program testing, vulnerability exploitation, benchmark

## [Self-evaluation guided beam search for reasoning](paper_2.md)
- **Authors**: Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael
- **Abstract**: Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge ...
- **Link**: [Read Paper](No Link Available)
- **Labels**: hallucination in reasoning, prompt strategy

## [Tree of thoughts: Deliberate problem solving with large language models](paper_3.md)
- **Authors**: Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik
- **Abstract**: Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in  tasks that require exploration, strategic lookahead, or where ...
- **Link**: [Read Paper](No Link Available)
- **Labels**: hallucination in reasoning, prompt strategy

## [Reflexion: Language agents with verbal reinforcement learning](paper_4.md)
- **Authors**: Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu
- **Abstract**: Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning ...
- **Link**: [Read Paper](No Link Available)
- **Labels**: hallucination in reasoning, prompt strategy

## [Satlm: Satisfiability-aided language models using declarative prompting](paper_5.md)
- **Authors**: Ye, Xi and Chen, Qiaochu and Dillig, Isil and Durrett, Greg
- **Abstract**: Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effectiv...
- **Link**: [Read Paper](No Link Available)
- **Labels**: hallucination in reasoning, prompt strategy

## [Leandojo: Theorem proving with retrieval-augmented language models](paper_6.md)
- **Authors**: Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan J and Anandkumar, Animashree
- **Abstract**: Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine le...
- **Link**: [Read Paper](No Link Available)
- **Labels**: prompt strategy, RAG

