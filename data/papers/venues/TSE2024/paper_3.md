# Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models

**Authors**: Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Zhuo, Terry Yue and Chen, Taolue

**Abstract**:

Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models (&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;ℓ&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="zhou-ieq1-3440503.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;ℓ&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="zhou-ieq2-3440503.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach &lt;monospace&gt;COTTON&lt;/monospace&gt; which can leverage &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;ℓ&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="zhou-ieq3-3440503.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by &lt;monospace&gt;COTTON&lt;/monospace&gt; outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by &lt;monospace&gt;COTTON&lt;/monospace&gt; boost various &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;ℓ&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="zhou-ieq4-3440503.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that &lt;monospace&gt;COTTON&lt;/monospace&gt; not only improves the performance of &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;ℓ&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="zhou-ieq5-3440503.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs, but also enhances the performance of LLMs. Our study showcases the potential of &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;ℓ&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="zhou-ieq6-3440503.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs in software engineering applications.

**Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3440503)

**Labels**: [code generation](../../labels/code_generation.md), [program synthesis](../../labels/program_synthesis.md), [empirical study](../../labels/empirical_study.md)
