# TSE2024

Number of papers: 14

## [Towards Efficient Fine-Tuning of Language Models With Organizational Data for Automated Software Review](paper_1.md)
- **Authors**: Nashaat, Mona and Miller, James
- **Abstract**: Large language models like BERT and GPT possess significant capabilities and potential impacts across various applications. Software engineers often use these models for code-related tasks, including generating, debugging, and summarizing code. Nevertheless, large language models still have several ...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3428324)
- **Labels**: code generation, program repair, code model, code model training, source code model

## [LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation](paper_2.md)
- **Authors**: Fakhoury, Sarah and Naik, Aaditya and Sakkas, Georgios and Chakraborty, Saikat and Lahiri, Shuvendu K.
- **Abstract**: Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3428972)
- **Labels**: code generation, program synthesis, empirical study

## [Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models](paper_3.md)
- **Authors**: Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Zhuo, Terry Yue and Chen, Taolue
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding th...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3440503)
- **Labels**: code generation, program synthesis, empirical study

## [Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models](paper_4.md)
- **Authors**: Tu, Haoxin and Zhou, Zhide and Jiang, He and Yusuf, Imam Nur Bani and Li, Yuxian and Jiang, Lingxiao
- **Abstract**: Compiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task....
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3397822)
- **Labels**: program testing, debugging, code model, code model training, source code model

## [ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation](paper_5.md)
- **Authors**: Tang, Yutian and Liu, Zhijie and Zhou, Zhichao and Luo, Xiapu
- **Abstract**: Recent advancements in large language models (LLMs) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, LLMs have shown potential in various software engineering applications. In this study, we present a syst...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3382365)
- **Labels**: program testing, unit testing, empirical study

## [No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT](paper_6.md)
- **Authors**: Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng
- **Abstract**: Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks, such as machine translation, question answering, summarization, and so on. Additionally, LLMs are also highly valuable in supporting software engineering tasks, particularly...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3392499)
- **Labels**: code generation, program synthesis, empirical study

## [Automatic Commit Message Generation: A Critical Review and Directions for Future Work](paper_7.md)
- **Authors**: Zhang, Yuxia and Qiu, Zhiqing and Stol, Klaas-Jan and Zhu, Wenhui and Zhu, Jiaxin and Tian, Yingchen and Liu, Hui
- **Abstract**: Commit messages are critical for code comprehension and software maintenance. Writing a high-quality message requires skill and effort. To support developers and reduce their effort on this task, several approaches have been proposed to automatically generate commit messages. Despite the promising p...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3364675)
- **Labels**: software maintenance and deployment, commit message generation, empirical study

## [Software Testing With Large Language Models: Survey, Landscape, and Vision](paper_8.md)
- **Authors**: Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing
- **Abstract**: Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a cr...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3368208)
- **Labels**: program testing, survey

## [Code Review Automation: Strengths and Weaknesses of the State of the Art](paper_9.md)
- **Authors**: Tufano, Rosalia and Dabi\'{c}, Ozren and Mastropaolo, Antonio and Ciniselli, Matteo and Bavota, Gabriele
- **Abstract**: The automation of code review has been tackled by several researchers with the goal of reducing its cost. The adoption of deep learning in software engineering pushed the automation to new boundaries, with techniques &lt;italic&gt;imitating&lt;/italic&gt; developers in generative tasks, such as comm...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2023.3348172)
- **Labels**: code review, empirical study

## [An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation](paper_10.md)
- **Authors**: Sch\"{a}fer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank
- **Abstract**: Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for aut...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2023.3334955)
- **Labels**: program testing, unit testing, empirical study

## [Learning to Generate Structured Code Summaries From Hybrid Code Context](paper_11.md)
- **Authors**: Zhou, Ziyi and Li, Mingchen and Yu, Huiqun and Fan, Guisheng and Yang, Penghui and Huang, Zijie
- **Abstract**: Code summarization aims to automatically generate natural language descriptions for code, and has become a rapidly expanding research area in the past decades. Unfortunately, existing approaches mainly focus on the “one-to-one” mapping from methods to short descriptions, which hinders them from beco...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3439562)
- **Labels**: static analysis, code summarization, benchmark

## [Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration](paper_12.md)
- **Authors**: Paltenghi, Matteo and Pandita, Rahul and Henley, Austin Z. and Ziegler, Albert
- **Abstract**: Recent neural models of code, such as OpenAI Codex and AlphaCode, have demonstrated remarkable proficiency at code generation due to the underlying attention mechanism. However, it often remains unclear how the models actually process code, and to what extent their reasoning and the way their attent...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3445338)
- **Labels**: code generation, code completion, code model, code model training, source code model, benchmark

## [Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction](paper_13.md)
- **Authors**: Kang, Sungmin and Yoon, Juyeon and Askarbekkyzy, Nargiz and Yoo, Shin
- **Abstract**: Bug reproduction is a critical developer activity that is also challenging to automate, as bug reports are often in natural language and thus can be difficult to transform to test cases consistently. As a result, existing techniques mostly focused on crash bugs, which are easier to automatically det...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3450837)
- **Labels**: program testing, bug reproduction, empirical study

## [BinCola: Diversity-Sensitive Contrastive Learning for Binary Code Similarity Detection](paper_14.md)
- **Authors**: Jiang, Shuai and Fu, Cai and He, Shuai and Lv, Jianqiang and Han, Lansheng and Hu, Hong
- **Abstract**: Binary Code Similarity Detection (BCSD) is a fundamental binary analysis technique in the area of software security. Recently, advanced deep learning algorithms are integrated into BCSD platforms to achieve superior performance on well-known benchmarks. However, real-world large programs embed more ...
- **Link**: [Read Paper](https://doi.org/10.1109/TSE.2024.3411072)
- **Labels**: static analysis, code similarity analysis, code model, code model training, binary code model

