# ICLR2021

Number of papers: 1

## [Graphcodebert: Pre-training code representations with data flow](paper_1.md)
- **Authors**: Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others
- **Abstract**: Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent ...
- **Link**: [Read Paper](https://arxiv.org/pdf/2302.05319)
- **Labels**: general coding task, code model, code model training, source code model

