# NeurIPS2024

Number of papers: 9

## [Can LLMs Implicitly Learn Numeric Parameter Constraints in Data Science APIs?](paper_8.md)
- **Authors**: Yinlin Deng, Chunqiu Steven Xia, Zhezhen Cao, Meiziniu Li, Lingming Zhang
- **Abstract**: Data science (DS) programs, typically built on popular DS libraries (such as PyTorch and NumPy) with thousands of APIs, serve as the cornerstone for various mission-critical domains such as financial systems, autonomous driving software, and coding assistants. Recently, large language models (LLMs) have been widely applied to generate DS programs across diverse scenarios, such as assisting users for DS programming or detecting critical vulnerabilities in DS frameworks. Such applications have all...
- **Link**: [Read Paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/617ffb01ea5b57769b0d63d5e9fefd3f-Paper-Conference.pdf)
- **Labels**: [static analysis](../../labels/static_analysis.md), [specification inference](../../labels/specification_inference.md)


## [Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search](paper_5.md)
- **Authors**: Nicola Dainese and Matteo Merler and Minttu Alakuijala and Pekka Marttinen
- **Abstract**: In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL). Calling code instead of LLMs for planning has the advantages of being precise, reliable, interpretable, and extremely efficient. However, writing appropriate Code World Models requires the ability to understand complex instructions, to generate exact code with non-trivial logic and to self-debug a long program with feedback fro...
- **Link**: [Read Paper](https://doi.org/10.48550/arXiv.2405.15383)
- **Labels**: [code generation](../../labels/code_generation.md), [program synthesis](../../labels/program_synthesis.md)


## [Hysynth: Context-free llm approximation for guiding program synthesis](paper_9.md)
- **Authors**: Barke, Shraddha and Anaya Gonzalez, Emmanuel and Kasibatla, Saketh Ram and Berg-Kirkpatrick, Taylor and Polikarpova, Nadia
- **Abstract**: Many structured prediction and reasoning tasks can be framed as program synthesis problems, where the goal is to generate a program in a domain-specific language (DSL) that transforms input data into the desired output. Unfortunately, purely neural approaches, such as large language models (LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while purely symbolic methods based on combinatorial search scale poorly to complex problems. Motivated by these limitations, we introdu...
- **Link**: [Read Paper](https://openreview.net/forum?id=5jt0ZSA6Co)
- **Labels**: [program synthesis](../../labels/program_synthesis.md)


## [LLMDFA: Analyzing Dataflow in Code with Large Language Model](paper_6.md)
- **Authors**: Chengpeng Wang and Wuqi Zhang and Zian Su and Xiangzhe Xu and Xiaoheng Xie and Xiangyu Zhang
- **Abstract**: Dataflow analysis is a fundamental code analysis technique that identifies dependencies between program values. Traditional approaches typically necessitate successful compilation and expert customization, hindering their applicability and usability for analyzing uncompilable programs with evolving analysis needs in realworld scenarios. This paper presents LLMDFA, an LLM-powered compilation-free and customizable dataflow analysis framework. To address hallucinations for reliable results, we deco...
- **Link**: [Read Paper](https://chengpeng-wang.github.io/publications/LLMDFA_NeurIPS2024.pdf)
- **Labels**: [static analysis](../../labels/static_analysis.md), [bug detection](../../labels/bug_detection.md)


## [SemCoder: Training Code Language Models with Comprehensive Semantics](paper_1.md)
- **Authors**: Ding, Yangruibo and Peng, Jinjun and Min, Marcus J and Kaiser, Gail and Yang, Junfeng and Ray, Baishakhi
- **Abstract**: Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states. This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for semantic understanding for complex tasks like debugging and program repair. We introduce a novel strategy, monologue reasoning, to train Code LLMs to reason comprehensive semantics, encompassing high-level functional descriptions, local exe...
- **Link**: [Read Paper](https://arxiv.org/pdf/2406.01006)
- **Labels**: [general coding task](../../labels/general_coding_task.md), [code model](../../labels/code_model.md), [code model training](../../labels/code_model_training.md), [source code model](../../labels/source_code_model.md)


## [Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases](paper_2.md)
- **Authors**: Su, Zian and Xu, Xiangzhe and Huang, Ziyang and Zhang, Kaiyuan and Zhang, Xiangyu
- **Abstract**: Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely hea...
- **Link**: [Read Paper](https://openreview.net/pdf?id=qPpVDzPhSL)
- **Labels**: [code model](../../labels/code_model.md), [code model training](../../labels/code_model_training.md), [binary code model](../../labels/binary_code_model.md)


## [Towards General Loop Invariant Generation: A Benchmark of Programs with Memory Manipulation](paper_7.md)
- **Authors**: Liu, Chang and Wu, Xiwei and Feng, Yuan and Cao, Qinxiang and Yan, Junchi
- **Abstract**: Program verification is vital for ensuring software reliability, especially in the context of increasingly complex systems. Loop invariants, remaining true before and after each iteration of loops, are crucial for this verification process. Traditional provers and machine learning based methods for generating loop invariants often require expert intervention or extensive labeled data, and typically only handle numerical property verification. These methods struggle with programs involving comple...
- **Link**: [Read Paper](https://arxiv.org/pdf/2311.10483)
- **Labels**: [static analysis](../../labels/static_analysis.md), [program verification](../../labels/program_verification.md), [benchmark](../../labels/benchmark.md)


## [Verified Code Transpilation with LLMs](paper_4.md)
- **Authors**: Bhatia, Sahil and Qiu, Jie and Hasabnis, Niranjan and Seshia, Sanjit A and Cheung, Alvin
- **Abstract**: Domain-specific languages (DSLs) are integral to various software workflows. Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability. However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API. While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the transpiled code. Another approach f...
- **Link**: [Read Paper](https://arxiv.org/pdf/2406.03003)
- **Labels**: [code generation](../../labels/code_generation.md), [program synthesis](../../labels/program_synthesis.md), [static analysis](../../labels/static_analysis.md), [program verification](../../labels/program_verification.md)


## [Verified multi-step synthesis using large language models and monte carlo tree search](paper_3.md)
- **Authors**: Brandfonbrener, David and Raja, Sibi and Prasad, Tarun and Loughridge, Chloe and Yang, Jianang and Henniger, Simon and Byrd, William E and Zinkov, Robert and Amin, Nada
- **Abstract**: We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq. Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step. In combination with the LLM prior, the verifier feedback raises the synthesis capabilities of open source models. On a set of five verified programming problems, we find that in four problems where the base model cannot s...
- **Link**: [Read Paper](https://openreview.net/pdf?id=HmB9uZTzaD)
- **Labels**: [code generation](../../labels/code_generation.md), [program synthesis](../../labels/program_synthesis.md)
