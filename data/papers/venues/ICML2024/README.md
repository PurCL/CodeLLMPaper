# ICML2024

Number of papers: 3

## [Instruction tuning for secure code generation](paper_1.md)
- **Authors**: He, Jingxuan and Vero, Mark and Krasnopolska, Gabriela and Vechev, Martin
- **Abstract**: Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and...
- **Link**: [Read Paper](https://arxiv.org/pdf/2405.00218)
- **Labels**: code generation, code model, code model security, defense

## [Magicoder: Empowering Code Generation with OSS-Instruct](paper_2.md)
- **Authors**: Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang
- **Abstract**: We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a n...
- **Link**: [Read Paper](https://arxiv.org/abs/2312.02120)
- **Labels**: code generation, code model, code model training, source code model

## [Symmetry-Preserving Program Representations for Learning Code Semantics](paper_3.md)
- **Authors**: Pei, Kexin and Li, Weichen and Jin, Qirui and Liu, Shuyang and Geng, Scott and Cavallaro, Lorenzo and Yang, Junfeng and Jana, Suman
- **Abstract**: Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to ...
- **Link**: [Read Paper](https://arxiv.org/pdf/2308.03312)
- **Labels**: general coding task, code model, code model training, source code model

