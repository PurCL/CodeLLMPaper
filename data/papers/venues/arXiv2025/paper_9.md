# Is Your Benchmark (Still) Useful? Dynamic Benchmarking for Code Language Models

**Authors**: Batu Guan, Xiao Wu, Yuanyuan Yuan, Shaohua Li

**Abstract**:

In this paper, we tackle a critical challenge in model evaluation: how to keep code benchmarks useful when models might have already seen them during training. We introduce a novel solution, dynamic benchmarking framework, to address this challenge. Given a code understanding or reasoning benchmark, our framework dynamically transforms each input, i.e., programs, with various semantic-preserving mutations to build a syntactically new while semantically identical benchmark. We evaluated ten popular language models on our dynamic benchmarks. Our evaluation reveals several interesting or surprising findings: (1) all models perform significantly worse than before, (2) the ranking between some models shifts dramatically, and (3) our dynamic benchmarks can resist against the data contamination problem.

**Link**: [Read Paper](https://arxiv.org/abs/2503.06643)

**Labels**: [benchmark](../../labels/benchmark.md)
