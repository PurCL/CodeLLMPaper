# Meta2024

Number of papers: 2

- **Labels**: [code generation](../../labels/code_generation.md), [compiler optimization](../../labels/compiler_optimization.md), [code model](../../labels/code_model.md), [code model training](../../labels/code_model_training.md), [IR code model](../../labels/IR_code_model.md)

- **Labels**: [prompt strategy](../../labels/prompt_strategy.md), [reason with code](../../labels/reason_with_code.md)

## [Don't Transform the Code, Code the Transforms: Towards Precise Code Rewriting using LLMs](paper_2.md)
- **Authors**: Cummins, Chris and Seeker, Volker and Armengol-Estap{\'e}, Jordi and Markosyan, Aram H and Synnaeve, Gabriel and Leather, Hugh
- **Abstract**: Tools for rewriting, refactoring and optimizing code should be fast and correct. Large language models (LLMs), by their nature, possess neither of these qualities. Yet, there remains tremendous opportunity in using LLMs to improve code. We explore the use of LLMs not to transform code, but to code transforms. We propose a chain-of-thought approach to synthesizing code transformations from a small number of input/output code examples that incorporates execution and feedback. Unlike the direct rew...
- **Link**: [Read Paper](https://arxiv.org/pdf/2410.08806)


## [Meta large language model compiler: Foundation models of compiler optimization](paper_1.md)
- **Authors**: Cummins, Chris and Seeker, Volker and Grubisic, Dejan and Roziere, Baptiste and Gehring, Jonas and Synnaeve, Gabriel and Leather, Hugh
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models speci...
- **Link**: [Read Paper](https://arxiv.org/pdf/2302.05319)
