# CCS2023

Number of papers: 3

## [Large language models for code: Security hardening and adversarial testing](paper_1.md)
- **Authors**: He, Jingxuan and Vechev, Martin
- **Abstract**: Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to en...
- **Link**: [Read Paper](https://arxiv.org/abs/2302.05319)
- **Labels**: code generation, code model, code model security, defense, attack

## [Prompt Fuzzing for Fuzz Driver Generation](paper_2.md)
- **Authors**: Lyu, Yunlong and Xie, Yuxuan and Chen, Peng and Chen, Hao
- **Abstract**: Writing high-quality fuzz drivers is time-consuming and requires a deep understanding of the library. However, the performance of the state-of-the-art automatic fuzz driver generation techniques leaves a lot to be desired. Fuzz drivers, which are learned from consumer code, can reach deep states but...
- **Link**: [Read Paper](https://arxiv.org/pdf/2312.17677.pdf)
- **Labels**: program testing, fuzzing

## [Poster: Boosting Adversarial Robustness by Adversarial Pre-training](paper_3.md)
- **Authors**: Xu, Xiaoyun and Picek, Stjepan
- **Abstract**: Vision Transformer (ViT) shows superior performance on various tasks, but, similar to other deep learning techniques, it is vulnerable to adversarial attacks. Due to the differences between ViT and traditional CNNs, previous works designed new adversarial training methods as defenses according to th...
- **Link**: [Read Paper](https://doi.org/10.1145/3576915.3624370)
- **Labels**: code model, code model security, defense

