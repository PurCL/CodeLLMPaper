# USENIXSec2023

Number of papers: 3

## [Lost at C: a user study on the security implications of large language model code assistants](paper_1.md)
- **Authors**: Sandoval, Gustavo and Pearce, Hammond and Nys, Teo and Karri, Ramesh and Garg, Siddharth and Dolan-Gavitt, Brendan
- **Abstract**: Large Language Models (LLMs) such as OpenAI Codex are increasingly being used as AI-based coding assistants. Understanding the impact of these tools on developers' code is paramount, especially as recent work showed that LLMs may suggest cybersecurity vulnerabilities. We conduct a security-driven us...
- **Link**: [Read Paper](No Link Available)
- **Labels**: code generation, program synthesis, empirical study

## [Continuous learning for android malware detection](paper_2.md)
- **Authors**: Chen, Yizheng and Ding, Zhoujie and Wagner, David
- **Abstract**: Machine learning methods can detect Android malware with very high accuracy. However, these classifiers have an Achilles heel, concept drift: they rapidly become out of date and ineffective, due to the evolution of malware apps and benign apps. Our research finds that, after training an Android malw...
- **Link**: [Read Paper](No Link Available)
- **Labels**: static analysis, bug detection, empirical study

## [PELICAN: exploiting backdoors of naturally trained deep learning models in binary code analysis](paper_3.md)
- **Authors**: Zhang, Zhuo and Tao, Guanhong and Shen, Guangyu and An, Shengwei and Xu, Qiuling and Liu, Yingqi and Ye, Yapeng and Wu, Yaoxuan and Zhang, Xiangyu
- **Abstract**: Deep Learning (DL) models are increasingly used in many cyber-security applications and achieve superior performance compared to traditional solutions. In this paper, we study backdoor vulnerabilities in naturally trained models used in binary analysis. These backdoors are not injected by attackers ...
- **Link**: [Read Paper](No Link Available)
- **Labels**: code model, security, attack, code model, training, binary code model

