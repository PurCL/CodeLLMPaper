{
    "ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "han-etal-2024-archcode",
        "author": "Han, Hojae and Kim, Jaejin and Yoo, Jaeseok and Lee, Youngwon and Hwang, Seung-won",
        "booktitle": "ACL2024",
        "title": "ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (i.e. achieving expected behavior for inputs) and non-functional (e.g., time/space performance, robustness, maintainability) requirements. However, textual descriptions can either express requirements verbosely or may even omit some of them. We introduce ARCHCODE, a novel framework that leverages in-context learning to organize requirements observed in descriptions and to extrapolate unexpressed requirements from them. ARCHCODE generates requirements from given descriptions, conditioning them to produce code snippets and test cases. Each test case is tailored to one of the requirements, allowing for the ranking of code snippets based on the compliance of their execution results with the requirements. Public benchmarks show that ARCHCODE enhances to satisfy functional requirements, significantly improving Pass@k scores.Furthermore, we introduce HumanEval-NFR, the first evaluation of LLMs\u2019 non-functional requirements in code generation, demonstrating ARCHCODE\u2019s superiority over baseline methods. The implementation of ARCHCODE and the HumanEval-NFR benchmark are both publicly accessible.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.730",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-hirope",
        "author": "Zhang, Kechi and Li, Ge and Zhang, Huangzhao and Jin, Zhi",
        "booktitle": "ACL2024",
        "title": "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Addressing the limitation of context length in large language models for code-related tasks is the primary focus of this paper. Existing LLMs are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences. Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code. HiRoPE offers easy integration into existing LLMs without extra training costs. Our method is extensively evaluated with various LLMs, demonstrating stable performance in tasks such as language modeling and long code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. Theoretically and experimentally, we find that HiRoPE also addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length.",
        "labels": [
            "code generation",
            "code completion",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.735",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-codeagent",
        "author": "Zhang, Kechi and Li, Jia and Li, Ge and Shi, Xianjie and Jin, Zhi",
        "booktitle": "ACL2024",
        "title": "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. However, real-world software development often involves complex code repositories with complex dependencies and extensive documentation. To enable LLMs to handle these realworld repo-level code generation, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code implementation, and code testing. We implement four agent strategies to optimize these tools\u2019 usage. To the best of our knowledge, CodeAgent is the first agent tool framework specifically for repo-level code generation. In order to measure the effectiveness of our method at the repository level, we have introduced a benchmark dataset CodAgentBench. The performance on this dataset shows a significant improvement brought by our method, with improvements of pass rate ranging from 2.0 to 15.8. Further tests on the HumanEval benchmark confirm CodeAgent\u2019s adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent\u2019s robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.",
        "labels": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.737",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models": {
        "type": "INPROCEEDINGS",
        "key": "riddell-etal-2024-quantifying",
        "author": "Riddell, Martin and Ni, Ansong and Cohan, Arman",
        "booktitle": "ACL2024",
        "title": "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. While recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of LLMs in programming contexts. In this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform significantly better on the subset of the benchmarks where similar solutions are seen during training. We also conduct extensive analysis on the factors that affect model memorization and generalization, such as model size, problem difficulty, and question length. We release all resulting files from our matching pipeline for future research.",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.761",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "ChatDev: Communicative Agents for Software Development": {
        "type": "INPROCEEDINGS",
        "key": "qian-etal-2024-chatdev",
        "author": "Qian, Chen and Liu, Wei and Liu, Hongzhang and Chen, Nuo and Dang, Yufan and Li, Jiahao and Yang, Cheng and Chen, Weize and Su, Yusheng and Cong, Xin and Xu, Juyuan and Li, Dahai and Liu, Zhiyuan and Sun, Maosong",
        "booktitle": "ACL2024",
        "title": "ChatDev: Communicative Agents for Software Development",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev.",
        "labels": [
            "general coding task"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.810",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents": {
        "type": "INPROCEEDINGS",
        "key": "trivedi-etal-2024-appworld",
        "author": "Trivedi, Harsh and Khot, Tushar and Hartmann, Mareike and Manku, Ruskin and Dong, Vinty and Li, Edward and Gupta, Shashank and Sabharwal, Ashish and Balasubramanian, Niranjan",
        "booktitle": "ACL2024",
        "title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only ~49% of our \u2018normal\u2019 tasks and ~30% of \u2018challenge\u2019 tasks, while other models solve at least 16% fewer. This highlights the benchmark\u2019s difficulty and AppWorld\u2019s potential to push the frontiers of interactive coding agents.",
        "labels": [
            "benchmark",
            "agent design"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.850",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "On Improving Repository-Level Code QA for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "strich-etal-2024-improving",
        "author": "Strich, Jan and Schneider, Florian and Nikishina, Irina and Biemann, Chris",
        "booktitle": "ACL2024",
        "title": "On Improving Repository-Level Code QA for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) such as ChatGPT, GitHub Copilot, Llama, or Mistral assist programmers as copilots and knowledge sources to make the coding process faster and more efficient. This paper aims to improve the copilot performance by implementing different self-alignment processes and retrieval-augmented generation (RAG) pipelines, as well as their combination. To test the effectiveness of all approaches, we create a dataset and apply a model-based evaluation, using LLM as a judge. It is designed to check the model\u2019s abilities to understand the source code semantics, the dependency between files, and the overall meta-information about the repository. We also compare our approach with other existing solutions, e.g. ChatGPT-3.5, and evaluate on the existing benchmarks. Code and dataset are available online (https://anonymous.4open.science/r/ma_llm-382D).",
        "labels": [
            "general coding task",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-srw.28",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks": {
        "type": "INPROCEEDINGS",
        "key": "10646663",
        "author": "Ullah, Saad and Han, Mingji and Pujar, Saurabh and Pearce, Hammond and Coskun, Ayse and Stringhini, Gianluca",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "862-880",
        "abstract": "Large Language Models (LLMs) have been suggested for use in automated vulnerability repair, but benchmarks showing they can consistently identify security-related bugs are lacking. We thus develop SecLLMHolmes, a fully automated evaluation framework that performs the most detailed investigation to date on whether LLMs can reliably identify and reason about security-related bugs. We construct a set of 228 code scenarios and analyze eight of the most capable LLMs across eight different investigative dimensions using our framework. Our evaluation shows LLMs provide non-deterministic responses, incorrect and unfaithful reasoning, and perform poorly in real-world scenarios. Most importantly, our findings reveal significant non-robustness in even the most advanced models like \u2018PaLM2\u2019 and \u2018GPT-4\u2019: by merely changing function or variable names, or by the addition of library functions in the source code, these models can yield incorrect answers in 26% and 17% of cases, respectively. These findings demonstrate that further LLM advances are needed before LLMs can be used as general purpose security assistants.",
        "labels": [
            "static analysis",
            "bug detection",
            "code generation",
            "program repair",
            "empirical study"
        ],
        "doi": "10.1109/SP54263.2024.00210",
        "ISSN": "2375-1207",
        "month": "May",
        "venue": "S&P2024",
        "url": "https://arxiv.org/pdf/2312.12575"
    },
    "LLMIF: Augmented Large Language Model for Fuzzing IoT Devices": {
        "type": "INPROCEEDINGS",
        "key": "10646659",
        "author": "Wang, Jincheng and Yu, Le and Luo, Xiapu",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "LLMIF: Augmented Large Language Model for Fuzzing IoT Devices",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "881-896",
        "abstract": "Despite the efficacy of fuzzing in verifying the implementation correctness of network protocols, existing IoT protocol fuzzing approaches grapple with several limitations, including obfuscated message formats, unresolved message dependencies, and a lack of evaluations on the testing cases. These limitations significantly curtail the capabilities of IoT fuzzers in vulnerability identification. In this work, we show that the protocol specification contains fruitful descriptions of protocol messages, which can be used to overcome the above limitations and guide IoT protocol fuzzing. To automate the specification analysis, we augment the large language model with the specification contents, and drive it to perform two tasks (i.e., protocol information extraction, and device response reasoning). We further design and implement a fuzzing algorithm, LLMIF, which incorporates the LLM into IoT fuzzing. Finally, we select Zigbee as the target protocol and initiate comprehensive evaluations. The evaluation result shows that LLMIF successfully addressed the above limitations. Compared with the existing Zigbee fuzzers, it increases the protocol message coverage and code coverage by 55.2% and 53.9%, respectively. Besides the enhanced coverage, LLMIF unearthed 11 vulnerabilities on real-world Zigbee devices, which include eight previously unknown vulnerabilities. Seven of them are not covered by the existing Zigbee fuzzers.",
        "labels": [
            "program testing",
            "fuzzing"
        ],
        "doi": "10.1109/SP54263.2024.00211",
        "ISSN": "2375-1207",
        "month": "May",
        "venue": "S&P2024",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646659"
    },
    "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models": {
        "type": "INPROCEEDINGS",
        "key": "10646865",
        "author": "Aghakhani, Hojjat and Dai, Wei and Manoel, Andre and Fernandes, Xavier and Kharkar, Anant and Kruegel, Christopher and Vigna, Giovanni and Evans, David and Zorn, Ben and Sim, Robert",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "1122-1140",
        "abstract": "With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model\u2019s training by injecting malicious data. Poisoning attacks could be designed to influence the model\u2019s suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, Covert and TrojanPuzzle, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TrojanPuzzle, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TrojanPuzzle robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both Covert and TrojanPuzzle have significant implications for practitioners when selecting code used to train or tune code-suggestion models.",
        "labels": [
            "code model",
            "code model security",
            "code generation",
            "code completion"
        ],
        "doi": "10.1109/SP54263.2024.00140",
        "ISSN": "2375-1207",
        "month": "May",
        "venue": "S&P2024",
        "url": "https://arxiv.org/pdf/2301.02344"
    },
    "Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694987",
        "author": "Gao, Xinyu and Xiong, Yun and Wang, Deze and Guan, Zhenhan and Shi, Zejian and Wang, Haofen and Li, Shanshan",
        "title": "Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694987",
        "doi": "10.1145/3691620.3694987",
        "abstract": "Retrieval-augmented code generation utilizes Large Language Models as the generator and significantly expands their code generation capabilities by providing relevant code, documentation, and more via the retriever. The current approach suffers from two primary limitations: 1) information redundancy. The indiscriminate inclusion of redundant information can result in resource wastage and may misguide generators, affecting their effectiveness and efficiency. 2) preference gap. Due to different optimization objectives, the retriever strives to procure code with higher ground truth similarity, yet this effort does not substantially benefit the generator. The retriever and the generator may prefer different golden code, and this gap in preference results in a suboptimal design. Additionally, differences in parameterization knowledge acquired during pre-training result in varying preferences among different generators.To address these limitations, in this paper, we propose RRG (Retrieve, Refactor, Generate), a novel framework for effective and efficient code generation. This framework introduces a code refactorer module between the retriever and the generator to bridge them. The refactoring process transforms the raw retrieved code into a more concise, efficient, and model-friendly version. It eliminates redundant information and noise, reducing the input length. Consequently, the generator receives higher-quality context, enabling it to produce more accurate results with lower inference costs. We conducted comprehensive experiments on multiple datasets. In the experiments, we confirmed the existence of a preference gap between the retriever and the generator, and RRG effectively bridges this gap. Specifically, RRG achieved significant performance improvements, with increases of up to 28\\% on EM, 13\\% on BLEU, and 6.8\\% on CodeBLEU.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "65\u201377",
        "numpages": "13",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Instructive Code Retriever: Learn from Large Language Model's Feedback for Code Intelligence Tasks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694997",
        "author": "Lu, Jiawei and Wang, Haoye and Liu, Zhongxin and Liang, Keyu and Bao, Lingfeng and Yang, Xiaohu",
        "title": "Instructive Code Retriever: Learn from Large Language Model's Feedback for Code Intelligence Tasks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694997",
        "doi": "10.1145/3691620.3694997",
        "abstract": "Recent studies proposed to leverage large language models (LLMs) with In-Context Learning (ICL) to handle code intelligence tasks without fine-tuning. ICL employs task instructions and a set of examples as demonstrations to guide the model in generating accurate answers without updating its parameters. While ICL has proven effective for code intelligence tasks, its performance heavily relies on the selected examples. Previous work has achieved some success in using BM25 to retrieve examples for code intelligence tasks. However, existing approaches lack the ability to understand the semantic and structural information of queries, resulting in less helpful demonstrations. Moreover, they do not adapt well to the complex and dynamic nature of user queries in diverse domains. In this paper, we introduce a novel approach named Instructive Code Retriever (ICR), which is designed to retrieve examples that enhance model inference across various code intelligence tasks and datasets. We enable ICR to learn the semantic and structural information of the corpus by a tree-based loss function. To better understand the correlation between queries and examples, we incorporate the feedback from LLMs to guide the training of the retriever. Experimental results demonstrate that our retriever significantly outperforms state-of-the-art approaches. We evaluate our model's effectiveness on various tasks, i.e., code summarization, program synthesis, and bug fixing. Compared to previous state-of-the-art algorithms, our method achieved improvements of 50.0\\% and 90.0\\% in terms of BLEU-4 for two code summarization datasets, 74.6\\% CodeBLEU on program synthesis dataset, and increases of 3.6 and 3.2 BLEU-4 on two bug fixing datasets.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "191\u2013203",
        "numpages": "13",
        "labels": [
            "agent design",
            "prompt strategy",
            "retrieval-augmented generation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Understanding Code Changes Practically with Small-Scale Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694999",
        "author": "Li, Cong and Xu, Zhaogui and Di, Peng and Wang, Dongxia and Li, Zheng and Zheng, Qian",
        "title": "Understanding Code Changes Practically with Small-Scale Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694999",
        "doi": "10.1145/3691620.3694999",
        "abstract": "Recent studies indicate that traditional techniques for understanding code changes are not as effective as techniques that directly prompt language models (LMs). However, current LM-based techniques heavily rely on expensive, large LMs (LLMs) such as GPT-4 and Llama-13b, which are either commercial or prohibitively costly to deploy on a wide scale, thereby restricting their practical applicability. This paper explores the feasibility of deploying small LMs (SLMs) while maintaining comparable or superior performance to LLMs in code change understanding. To achieve this, we created a small yet high-quality dataset called HQCM which was meticulously reviewed, revised, and validated by five human experts. We fine-tuned state-of-the-art 7b and 220m SLMs using HQCM and compared them with traditional techniques and LLMs with \u226570b parameters. Our evaluation confirmed HQCM's benefits and demonstrated that SLMs, after finetuning by HQCM, can achieve superior performance in three change understanding tasks: change summarization, change classification, and code refinement. This study supports the use of SLMs in environments with security, computational, and financial constraints, such as in industry scenarios and on edge devices, distinguishing our work from the others.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "216\u2013228",
        "numpages": "13",
        "labels": [
            "static analysis",
            "code summarization",
            "code model",
            "code model training",
            "source code model"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695000",
        "author": "Sun, Zhihong and Wan, Yao and Li, Jia and Zhang, Hongyu and Jin, Zhi and Li, Ge and Lyu, Chen",
        "title": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695000",
        "doi": "10.1145/3691620.3695000",
        "abstract": "Large Language Models (LLMs), such as GPT-4, StarCoder, and Code Llama, are transforming the way developers approach programming by automatically generating code based on given contexts, such as natural language descriptions or incomplete surrounding code. Despite advancements, generating syntactically and semantically correct code remains challenging, especially for complex programming tasks. Existing approaches typically generate multiple candidate solutions using LLMs to increase the likelihood of producing correct code. However, selecting the correct code from these candidates --- a process known as code ranking --- remains a major challenge. Current research on code ranking can be categorized into execution-based and non-execution-based methods. Execution-based methods, although effective, encounter notable limitations, such as scarcity of quality unit tests and security risks. Non-execution-based methods like CodeRanker, which rely solely on classification labels to train a code ranker, struggle to capture subtle errors and provide detailed error insights. Recognizing the strengths and limitations of both approaches, we propose a new method that integrates the advantages of execution-based and non-execution-based techniques. The key insight of our work is that an effective code ranker is expected to truly comprehend the underlying causes of erroneous code, as relying solely on classification labels is insufficient. Inspired by this, this paper puts forward RankEF, an innovative approach for code ranking that leverages execution feedback. RankEF employs multi-task learning to integrate code classification with execution feedback generation. This approach enables the model to understand the reasons behind incorrect code, distinguishing between correct and incorrect solutions without the need to execute the code during the ranking phase. Experiments on three code generation benchmarks---APPS, MBPP, and HumanEval---demonstrate that RankEF significantly outperforms the state-of-the-art CodeRanker, achieving relative improvements of +30.97\\%, +31.43\\%, and +19.51\\% in Pass@1, Pass@2, and Pass@5 on APPS test, respectively.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "229\u2013241",
        "numpages": "13",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695010",
        "author": "Zhang, Yichi and Liu, Zixi and Feng, Yang and Xu, Baowen",
        "title": "Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695010",
        "doi": "10.1145/3691620.3695010",
        "abstract": "Rust is renowned for its robust memory safety capabilities, yet its distinctive memory management model poses substantial challenges in both writing and understanding programs. Within Rust source code, comments are employed to clearly delineate conditions that might cause panic behavior, thereby warning developers about potential hazards associated with specific operations. Therefore, comments are particularly crucial for documenting Rust's program logic and design. Nevertheless, as modern software frequently undergoes updates and modifications, maintaining the accuracy and relevance of these comments becomes a labor-intensive endeavor.In this paper, inspired by the remarkable capabilities of Large Language Models (LLMs) in understanding software programs, we propose a code-comment inconsistency detection tool, namely RustC4, that combines program analysis and LLM-driven techniques to identify inconsistencies in code comments. RustC4 leverages LLMs' ability to interpret natural language descriptions within code comments, facilitating the extraction of design constraints. Program analysis techniques are then employed to accurately verify the implementation of these constraints. To evaluate the effectiveness of RustC4, we construct a dataset from 12 large-scale real-world Rust projects. The experiment results demonstrate that RustC4 is effective in detecting 176 real inconsistent cases and 23 of them have been confirmed and fixed by developers by the time this paper was submitted.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "356\u2013366",
        "numpages": "11",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695013",
        "author": "Wu, Yulun and Wen, Ming and Yu, Zeliang and Guo, Xiaochen and Jin, Hai",
        "title": "Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695013",
        "doi": "10.1145/3691620.3695013",
        "abstract": "Open-source software (OSS) has profoundly transformed the software development paradigm by facilitating effortless code reuse. However, in recent years, there has been an alarming increase in disclosed vulnerabilities within OSS, posing significant security risks to downstream users. Therefore, analyzing existing vulnerabilities and precisely assessing their threats to downstream applications become pivotal. Plenty of efforts have been made recently towards this problem, such as vulnerability reachability analysis and vulnerability reproduction. The key to these tasks is identifying the vulnerable function (i.e., the function where the root cause of a vulnerability resides). However, public vulnerability datasets (e.g., NVD) rarely include this information as pinpointing the exact vulnerable functions remains to be a longstanding challenge.Existing methods mainly detect vulnerable functions based on vulnerability patches or Proof-of-Concept (PoC). However, such methods face significant limitations due to data availability and the requirement for extensive manual efforts, thus hindering scalability. To address this issue, we propose a novel approach VFFinder that localizes vulnerable functions based on Common Vulnerabilities and Exposures (CVE) descriptions and the corresponding source code utilizing Large Language Models (LLMs). Specifically, VFFinder adopts a customized in-context learning (ICL) approach based on CVE description patterns to enable LLM to extract key entities. It then performs priority matching with the source code to localize vulnerable functions. We assess the performance of VFFinder on 75 large open-source projects. The results demonstrate that VFFinder surpasses existing baselines significantly. Notably, the Top-1 and MRR metrics have been improved substantially, averaging 4.25X and 2.37X respectively. We also integrate VFFinder with Software Composition Analysis (SCA) tools, and the results show that our tool can reduce the false positive rates of existing SCA tools significantly.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "393\u2013405",
        "numpages": "13",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695014",
        "author": "Wu, Guangyuan and Cao, Weining and Yao, Yuan and Wei, Hengfeng and Chen, Taolue and Ma, Xiaoxing",
        "title": "LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695014",
        "doi": "10.1145/3691620.3695014",
        "abstract": "Loop invariant inference, a key component in program verification, is a challenging task due to the inherent undecidability and complex loop behaviors in practice. Recently, machine learning based techniques have demonstrated impressive performance in generating loop invariants automatically. However, these methods highly rely on the labeled training data, and are intrinsically random and uncertain, leading to unstable performance. In this paper, we investigate a synergy of large language models (LLMs) and bounded model checking (BMC) to address these issues. The key observation is that, although LLMs may not be able to return the correct loop invariant in one response, they usually can provide all individual predicates of the correct loop invariant in multiple responses. To this end, we propose a \"query-filter-reassemble\" strategy, namely, we first leverage the language generation power of LLMs to produce a set of candidate invariants, where training data is not needed. Then, we employ BMC to identify valid predicates from these candidate invariants, which are assembled to produce new candidate invariants and checked by off-the-shelf SMT solvers. The feedback is incorporated into the prompt for the next round of LLM querying. We expand the existing benchmark of 133 programs to 316 programs, providing a more comprehensive testing ground. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art techniques, successfully generating 309 loop invariants out of 316 cases, whereas the existing baseline methods are only able to tackle 219 programs at best. The code is publicly available at https://github.com/SoftWiser-group/LaM4Inv.git.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "406\u2013417",
        "numpages": "12",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Semantic-Enhanced Indirect Call Analysis with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695016",
        "author": "Cheng, Baijun and Zhang, Cen and Wang, Kailong and Shi, Ling and Liu, Yang and Wang, Haoyu and Guo, Yao and Li, Ding and Chen, Xiangqun",
        "title": "Semantic-Enhanced Indirect Call Analysis with Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695016",
        "doi": "10.1145/3691620.3695016",
        "abstract": "In contemporary software development, the widespread use of indirect calls to achieve dynamic features poses challenges in constructing precise control flow graphs (CFGs), which further impacts the performance of downstream static analysis tasks. To tackle this issue, various types of indirect call analyzers have been proposed. However, they do not fully leverage the semantic information of the program, limiting their effectiveness in real-world scenarios.To address these issues, this paper proposes Semantic-Enhanced Analysis (SEA), a new approach to enhance the effectiveness of indirect call analysis. Our fundamental insight is that for common programming practices, indirect calls often exhibit semantic similarity with their invoked targets. This semantic alignment serves as a supportive mechanism for static analysis techniques in filtering out false targets. Notably, contemporary large language models (LLMs) are trained on extensive code corpora, encompassing tasks such as code summarization, making them well-suited for semantic analysis. Specifically, SEA leverages LLMs to generate natural language summaries of both indirect calls and target functions from multiple perspectives. Through further analysis of these summaries, SEA can determine their suitability as caller-callee pairs. Experimental results demonstrate that SEA can significantly enhance existing static analysis methods by producing more precise target sets for indirect calls.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "430\u2013442",
        "numpages": "13",
        "labels": [
            "static analysis",
            "call graph analysis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "WaDec: Decompiling WebAssembly Using Large Language Model": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695020",
        "author": "She, Xinyu and Zhao, Yanjie and Wang, Haoyu",
        "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695020",
        "doi": "10.1145/3691620.3695020",
        "abstract": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34\\%, a dramatic 97\\% reduction compared to the state-of-the-art's 116.94\\%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11\\%, a re-execution rate of 43.55\\%, and an output consistency of 27.15\\%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185\\%, cyclomatic complexity by 8\\%, and cosine similarity by 41\\%, achieving an average code similarity above 50\\%. In summary, WaDec enhances understanding of the code's structure and execution flow, facilitating automated code analysis, optimization, and security auditing.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "481\u2013492",
        "numpages": "12",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "labels": [
            "static analysis",
            "program decompilation",
            "code model",
            "code model training",
            "binary code model"
        ]
    },
    "GraphCoder: Enhancing Repository-Level Code Completion via Coarse-to-fine Retrieval Based on Code Context Graph": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695054",
        "author": "Liu, Wei and Yu, Ailun and Zan, Daoguang and Shen, Bo and Zhang, Wei and Zhao, Haiyan and Jin, Zhi and Wang, Qianxiang",
        "title": "GraphCoder: Enhancing Repository-Level Code Completion via Coarse-to-fine Retrieval Based on Code Context Graph",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695054",
        "doi": "10.1145/3691620.3695054",
        "abstract": "The performance of repository-level code completion depends upon the effective leverage of both general and repository-specific knowledge. Despite the impressive capability of code LLMs in general code completion tasks, they often exhibit less satisfactory performance on repository-level completion due to the lack of repository-specific knowledge in these LLMs. To address this problem, we propose GraphCoder, a retrieval-augmented code completion framework that leverages LLMs' general code knowledge and the repository-specific knowledge via a graph-based retrieval-generation process. In particular, GraphCoder captures the context of completion target more accurately through code context graph (CCG) that consists of control-flow, data- and control-dependence between code statements, a more structured way to capture the completion target context than the sequence-based context used in existing retrieval-augmented approaches; based on CCG, GraphCoder further employs a coarse-to-fine retrieval process to locate context-similar code snippets with the completion target from the current repository. Experimental results demonstrate both the effectiveness and efficiency of GraphCoder: Compared to baseline retrieval-augmented methods, GraphCoder achieves higher exact match (EM) on average, with increases of +6.06 in code match and +6.23 in identifier match, while using less time and space.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "570\u2013581",
        "numpages": "12",
        "labels": [
            "code generation",
            "code completion"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695055",
        "author": "Wu, Cong and Chen, Jing and Wang, Ziwei and Liang, Ruichao and Du, Ruiying",
        "title": "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695055",
        "doi": "10.1145/3691620.3695055",
        "abstract": "Smart contracts, self-executing agreements directly encoded in code, are fundamental to blockchain technology, especially in decentralized finance (DeFi) and Web3. However, the rise of Ponzi schemes in smart contracts poses significant risks, leading to substantial financial losses and eroding trust in blockchain systems. Existing detection methods, such as PonziGuard, depend on large amounts of labeled data and struggle to identify unseen Ponzi schemes, limiting their reliability and generalizability. In contrast, we introduce PonziSleuth, the first LLM-driven approach for detecting Ponzi smart contracts, which requires no labeled training data. PonziSleuth utilizes advanced language understanding capabilities of LLMs to analyze smart contract source code through a novel two-step zero-shot chain-of-thought prompting technique. Our extensive evaluation on benchmark datasets and real-world contracts demonstrates that PonziSleuth delivers comparable, and often superior, performance without the extensive data requirements, achieving a balanced detection accuracy of 96.06\\% with GPT-3.5-turbo, 93.91\\% with LLAMA3, and 94.27\\% with Mistral. In real-world detection, PonziSleuth successfully identified 15 new Ponzi schemes from 4,597 contracts verified by Etherscan in March 2024, with a false negative rate of 0\\% and a false positive rate of 0.29\\%. These results highlight PonziSleuth's capability to detect diverse and novel Ponzi schemes, marking a significant advancement in leveraging LLMs for enhancing blockchain security and mitigating financial scams.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "582\u2013593",
        "numpages": "12",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695062",
        "author": "Liu, Fang and Liu, Zhenwei and Zhao, Qianhui and Jiang, Jing and Zhang, Li and Sun, Zian and Li, Ge and Li, Zhongqi and Ma, Yuchi",
        "title": "FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695062",
        "doi": "10.1145/3691620.3695062",
        "abstract": "Providing personalized and timely feedback for student's programming assignments is useful for programming education. Automated program repair (APR) techniques have been used to fix the bugs in programming assignments, where the Large Language Models (LLMs) based approaches have shown promising results. Given the growing complexity of identifying and fixing bugs in advanced programming assignments, current fine-tuning strategies for APR are inadequate in guiding the LLM to identify bugs and make accurate edits during the generative repair process. Furthermore, the autoregressive decoding approach employed by the LLM could potentially impede the efficiency of the repair, thereby hindering the ability to provide timely feedback. To tackle these challenges, we propose FastFixer, an efficient and effective approach for programming assignment repair. To assist the LLM in accurately identifying and repairing bugs, we first propose a novel repair-oriented fine-tuning strategy, aiming to enhance the LLM's attention towards learning how to generate the necessary patch and its associated context. Furthermore, to speed up the patch generation, we propose an inference acceleration approach that is specifically tailored for the program repair task. The evaluation results demonstrate that FastFixer obtains an overall improvement of 20.46\\% in assignment fixing when compared to the state-of-the-art baseline. Considering the repair efficiency, FastFixer achieves a remarkable inference speedup of 16.67\\texttimes{} compared to the autoregressive decoding algorithm.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "669\u2013680",
        "numpages": "12",
        "labels": [
            "code generation",
            "program repair"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "DroidCoder: Enhanced Android Code Completion with Context-Enriched Retrieval-Augmented Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695063",
        "author": "Yu, Xinran and Li, Chun and Pan, Minxue and Li, Xuandong",
        "title": "DroidCoder: Enhanced Android Code Completion with Context-Enriched Retrieval-Augmented Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695063",
        "doi": "10.1145/3691620.3695063",
        "abstract": "Android is the most popular mobile operating system. However, Android development requires extensive coding, especially for unique features such as lifecycle callbacks and UI widgets. Existing code completion methods typically utilize Retrieval-Augmented Generation (RAG) to provide contextual information for pre-trained code large language models (Code LLMs) to perform completion. Despite considerable progress in these methods, their effectiveness in Android development remains limited. This is because the features of Android development make it challenging for existing retrieval mechanisms to extract sufficient context effectively. In response, we propose DroidCoder, a novel Android code completion framework that employs Android development features and contextual information of code snippets to enrich RAG. It also incorporates a specifically designed loss function to fine-tune the model, enabling it to better utilize context-enhanced RAG for Android code completion. We evaluated our method on three base models and different types of applications, comparing it with two state-of-the-art code completion methods. The experimental results demonstrate that our method significantly outperforms the baselines at line-level and multi-line-level code completion and improves the quality of the completed code.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "681\u2013693",
        "numpages": "13",
        "labels": [
            "code generation",
            "code completion",
            "agent design",
            "prompt strategy",
            "retrieval-augmented generation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Closing the Gap: A User Study on the Real-world Usefulness of AI-powered Vulnerability Detection & Repair in the IDE": {
        "type": "inproceedings",
        "key": "lewei_ICSE2025",
        "author": "Benjamin Steenhoek, Kalpathy Sivaraman, Renata Saldivar Gonzalez, Yevhen Mohylevskyy, Roshanak Zilouchian Moghaddam, Wei Le",
        "title": "Closing the Gap: A User Study on the Real-world Usefulness of AI-powered Vulnerability Detection & Repair in the IDE",
        "year": "2025",
        "url": "https://www.arxiv.org/abs/2412.14306",
        "abstract": "This paper presents the first empirical study of a vulnerability detection and fix tool with professional software developers on real projects that they own. We implemented DeepVulGuard, an IDE-integrated tool based on state-of-the-art detection and fix models, and show that it has promising performance on benchmarks of historic vulnerability data. DeepVulGuard scans code for vulnerabilities (including identifying the vulnerability type and vulnerable region of code), suggests fixes, provides natural-language explanations for alerts and fixes, leveraging chat interfaces. We recruited 17 professional software developers at Microsoft, observed their usage of the tool on their code, and conducted interviews to assess the tool's usefulness, speed, trust, relevance, and workflow integration. We also gathered detailed qualitative feedback on users' perceptions and their desired features. Study participants scanned a total of 24 projects, 6.9k files, and over 1.7 million lines of source code, and generated 170 alerts and 50 fix suggestions. We find that although state-of-the-art AI-powered detection and fix tools show promise, they are not yet practical for real-world use due to a high rate of false positives and non-applicable fixes. User feedback reveals several actionable pain points, ranging from incomplete context to lack of customization for the user's codebase. Additionally, we explore how AI features, including confidence scores, explanations, and chat interaction, can apply to vulnerability detection and fixing. Based on these insights, we offer practical recommendations for evaluating and deploying AI detection and fix models. Our code and data are available at this https URL.",
        "labels": [
            "static analysis",
            "bug detection",
            "code generation",
            "program repair",
            "empirical study"
        ],
        "series": "ICSE' 25",
        "venue": "ICSE2025"
    },
    "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695066",
        "author": "Li, Guochang and Zhi, Chen and Chen, Jialiang and Han, Junxiao and Deng, Shuiguang",
        "title": "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695066",
        "doi": "10.1145/3691620.3695066",
        "abstract": "Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that \"pre-training and fine-tuning\" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-Instruction, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-Instruction. The best fine-tuned model fixes 58\\% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that (IA)3 improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-Instruction, PEFT weights, and the fine-tuning code are publicly available as open-source resources.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "719\u2013731",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair",
            "code model",
            "code model training",
            "source code model"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695068",
        "author": "Cui, Di and Wang, Jiaqi and Wang, Qiangqiang and Ji, Peng and Qiao, Minglang and Zhao, Yutong and Hu, Jingzhao and Wang, Luqiao and Li, Qingshan",
        "title": "Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695068",
        "doi": "10.1145/3691620.3695068",
        "abstract": "Methods implemented in incorrect classes will cause excessive reliance on other classes than their own, known as a typical code smell symptom: feature envy, which makes it difficult to maintain increased coupling between classes. Addressing this issue, several Move Method refactoring tools have been proposed, employing a two-phase process: identifying misplaced methods to move and appropriate classes to receive, and implementing the mechanics of refactoring. These tools traditionally use hard-coded metrics to measure correlations between movable methods and target classes and apply heuristic thresholds or trained classifiers to unearth refactoring opportunities. Yet, these approaches predominantly illuminate pairwise correlations between methods and classes while overlooking the complex and complicated dependencies binding multiple code entities within these methods/classes that are prevalent in real-world cases. This narrow focus can lead to refactoring suggestions that may diverge from developers' actual needs. To bridge this gap, our paper leverages the concept of inter-class code entity dependency hypergraph to model complicated dependency relationships involving multiple code entities within various methods/classes and proposes a hypergraph learning-based approach to suggest Move Method refactoring opportunities named HMove. We first construct inter-class code entity dependency hypergraphs from training samples and assign attributes to entities with a pre-trained code model. All the attributed hypergraphs are fed into a hybrid hypergraph neural network for training. Utilizing this trained neural network alongside a large language model, we construct a refactoring suggestion system. We trained HMove on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 27.8\\% in precision, 2.5\\% in recall, and 18.5\\% in f1-measure compared to 9 state-of-the-art refactoring tools, which is more useful for 68\\% of participants. The results also unveil practical suggestions and new insights that benefit existing feature envy-related refactoring techniques.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "745\u2013757",
        "numpages": "13",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "An Empirical Study to Evaluate AIGC Detectors on Code Content": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695468",
        "author": "JianWang and Liu, Shangqing and Xie, Xiaofei and Li, Yi",
        "title": "An Empirical Study to Evaluate AIGC Detectors on Code Content",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695468",
        "doi": "10.1145/3691620.3695468",
        "abstract": "Artificial Intelligence Generated Content (AIGC) has garnered considerable attention for its impressive performance, with Large Language Models (LLMs), like ChatGPT, emerging as a leading AIGC model that produces high-quality responses across various applications, including software development and maintenance. Despite its potential, the misuse of LLMs, especially in security and safety-critical domains, such as academic integrity and answering questions on Stack Overflow, poses significant concerns. Numerous AIGC detectors have been developed and evaluated on natural language data. However, their performance on code-related content generated by LLMs remains unexplored.To fill this gap, in this paper, we present an empirical study evaluating existing AIGC detectors in the software domain. We select three state-of-the-art LLMs, i.e., GPT-3.5, WizardCoder and CodeLlama, for machine-content generation. We further created a comprehensive dataset including 2.23M samples comprising code-related content for each model, encompassing popular software activities like Q&amp;A (150K), code summarization (1M), and code generation (1.1M). We evaluated thirteen AIGC detectors, comprising six commercial and seven open-source solutions, assessing their performance on this dataset. Our results indicate that AIGC detectors perform less on code-related data than natural language data. Fine-tuning can enhance detector performance, especially for content within the same domain; but generalization remains a challenge.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "844\u2013856",
        "numpages": "13",
        "labels": [
            "general coding task",
            "benchmark"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695470",
        "author": "Cao, Jialun and Chen, Zhiyong and Wu, Jiarong and Cheung, Shing-Chi and Xu, Chang",
        "title": "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695470",
        "doi": "10.1145/3691620.3695470",
        "abstract": "Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8\\% of benchmarks involve Python, while only 5 benchmarks involve Java, resulting in an insufficient understanding of LLMs' capability to generate Java code. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3\\% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills (e.g., variables, operators, and control structures), while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism). Considering the prevalence of these advanced features in real-world Java project development, constructing benchmarks to test LLMs on handling OOP features is necessary.To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92\\%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 48.24\\% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench. We also release a leaderboard and invite model developers to participate and test their models against JavaBench at https://java-bench.github.io/leaderboard.html.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "870\u2013882",
        "numpages": "13",
        "labels": [
            "benchmark",
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695480",
        "author": "Chen, Jiachi and Zhong, Qingyuan and Wang, Yanlin and Ning, Kaiwen and Liu, Yongkun and Xu, Zenan and Zhao, Zhe and Chen, Ting and Zheng, Zibin",
        "title": "RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695480",
        "doi": "10.1145/3691620.3695480",
        "abstract": "Warning: Please note that this article contains potential harmful or offensive content. This content is only for the evaluating and analysis of LLMs and does not imply any intention to promote criminal activities.The emergence of Large Language Models (LLMs) has significantly influenced various aspects of software development activities. Despite their benefits, LLMs also pose notable risks, including the potential to generate harmful content and being abused by malicious developers to create malicious code. Several previous studies have focused on the ability of LLMs to resist the generation of harmful content that violates human ethical standards, such as biased or offensive content. However, there is no research evaluating the ability of LLMs to resist malicious code generation. To fill this gap, we propose RMCBench, the first benchmark comprising 473 prompts designed to assess the ability of LLMs to resist malicious code generation. This benchmark employs two scenarios: a text-to-code scenario, where LLMs are prompted with descriptions to generate code, and a code-to-code scenario, where LLMs translate or complete existing malicious code. Based on RMCBench, we conduct an empirical study on the 11 representative LLMs to assess their ability to resist malicious code generation. Our findings indicate that current LLMs have a limited ability to resist malicious code generation with an average refusal rate of 40.36\\% in text-to-code scenario and 11.52\\% in code-to-code scenario. The average refusal rate of all LLMs in RMCBench is only 28.71\\%; ChatGPT-4 has a refusal rate of only 35.73\\%. We also analyze the factors that affect LLM's ability to resist malicious code generation and provide implications for developers to enhance model robustness.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "995\u20131006",
        "numpages": "12",
        "labels": [
            "code generation",
            "benchmark",
            "code model",
            "code model robustness"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Contextualized Data-Wrangling Code Generation in Computational Notebooks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695503",
        "author": "Huang, Junjie and Guo, Daya and Wang, Chenglong and Gu, Jiazhen and Lu, Shuai and Inala, Jeevana Priya and Yan, Cong and Gao, Jianfeng and Duan, Nan and Lyu, Michael R.",
        "title": "Contextualized Data-Wrangling Code Generation in Computational Notebooks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695503",
        "doi": "10.1145/3691620.3695503",
        "abstract": "Data wrangling, the process of preparing raw data for further analysis in computational notebooks, is a crucial yet time-consuming step in data science. Code generation has the potential to automate the data wrangling process to reduce analysts' overhead by translating user intents into executable code. Precisely generating data wrangling code necessitates a comprehensive consideration of the rich context present in notebooks, including textual context, code context and data context. However, notebooks often interleave multiple non-linear analysis tasks into linear sequence of code blocks, where the contextual dependencies are not clearly reflected. Directly training models with source code blocks fails to fully exploit the contexts for accurate wrangling code generation.To bridge the gap, we aim to construct a high quality datasets with clear and rich contexts to help training models for data wrangling code generation tasks. In this work, we first propose an automated approach, CoCoMine to mine data-wrangling code generation examples with clear multi-modal contextual dependency. It first adopts data flow analysis to identify the code blocks containing data wrangling codes. Then, CoCoMine extracts the contextualized data-wrangling code examples through tracing and replaying notebooks. With CoCoMine, we construct CoCoNote, a dataset containing 58,221 examples for Contextualized Data-wrangling Code generation in Notebooks. To demonstrate the effectiveness of our dataset, we finetune a range of pretrained code models and prompt various large language models on our task. Furthermore, we also propose Data-Coder, which encodes data context and code&amp;textual contexts separately to enhance code generation. Experiment results demonstrate the significance of incorporating data context in data-wrangling code generation and the effectiveness of our model. We release code and data at https://github.com/Jun-jie-Huang/CoCoNote.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1282\u20131294",
        "numpages": "13",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Dynamic Scoring Code Token Tree: A Novel Decoding Strategy for Generating High-Performance Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695505",
        "author": "Qu, Muzi and Liu, Jie and Kang, Liangyi and Wang, Shuai and Ye, Dan and Huang, Tao",
        "title": "Dynamic Scoring Code Token Tree: A Novel Decoding Strategy for Generating High-Performance Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695505",
        "doi": "10.1145/3691620.3695505",
        "abstract": "Within the realms of scientific computing, large-scale data processing, and artificial intelligence-powered computation, disparities in performance, which originate from differing code implementations, directly influence the practicality of the code. Although existing works tried to utilize code knowledge to enhance the execution performance of codes generated by large language models, they neglect code evaluation outcomes which directly refer to the code execution details, resulting in inefficient computation. To address this issue, we propose DSCT-Decode, an innovative adaptive decoding strategy for large language models, that employs a data structure named 'Code Token Tree' (CTT), which guides token selection based on code evaluation outcomes. DSCT-Decode assesses generated code across three dimensions---correctness, performance, and similarity---and utilizes a dynamic penalty-based boundary intersection method to compute multi-objective scores, which are then used to adjust the scores of nodes in the CTT during backpropagation. By maintaining a balance between exploration, through token selection probabilities, and exploitation, through multi-objective scoring, DSCT-Decode effectively navigates the code space to swiftly identify high-performance code solutions. To substantiate our framework, we developed a new benchmark, big-DS-1000, which is an extension of DS-1000. This benchmark is the first of its kind to specifically evaluate code generation methods based on execution performance. Comparative evaluations with leading large language models, such as CodeLlama and GPT-4, show that our framework achieves an average performance enhancement of nearly 30\\%. Furthermore, 30\\% of the codes exhibited a performance improvement of more than 20\\%, underscoring the effectiveness and potential of our framework for practical applications.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1308\u20131318",
        "numpages": "11",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model training",
            "source code model"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695506",
        "author": "Zhang, Huan and Cheng, Wei and Wu, Yuhan and Hu, Wei",
        "title": "A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695506",
        "doi": "10.1145/3691620.3695506",
        "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation. The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback. The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement. This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers. We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks. Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00\\%--162.43\\% compared to prompting LLMs directly.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1319\u20131331",
        "numpages": "13",
        "labels": [
            "code generation",
            "program synthesis",
            "agent design",
            "planning"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "iSMELL: Assembling LLMs with Expert Toolsets for Code Smell Detection and Refactoring": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695508",
        "author": "Wu, Di and Mu, Fangwen and Shi, Lin and Guo, Zhaoqiang and Liu, Kui and Zhuang, Weiguang and Zhong, Yuqi and Zhang, Li",
        "title": "iSMELL: Assembling LLMs with Expert Toolsets for Code Smell Detection and Refactoring",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695508",
        "doi": "10.1145/3691620.3695508",
        "abstract": "Detecting and refactoring code smells is challenging, laborious, and sustaining. Although large language models have demonstrated potential in identifying various types of code smells, they also have limitations such as input-output token restrictions, difficulty in accessing repository-level knowledge, and performing dynamic source code analysis. Existing learning-based methods or commercial expert toolsets have advantages in handling complex smells. They can analyze project structures and contextual information in-depth, access global code repositories, and utilize advanced code analysis techniques. However, these toolsets are often designed for specific types and patterns of code smells and can only address fixed smells, lacking flexibility and scalability. To resolve that problem, we propose iSMELL, an ensemble approach that employs various code smell detection toolsets via Mixture of Experts (MoE) architecture for comprehensive code smell detection, and enhances the LLMs with the detection results from expert toolsets for refactoring those identified code smells. First, we train a MoE model that, based on input code vectors, outputs the most suitable expert tool for identifying each type of smell. Then, we select the recommended toolsets for code smell detection and obtain their results. Finally, we equip the prompts with the detection results from the expert toolsets, thereby enhancing the refactoring capability of LLMs for code with existing smells, enabling them to provide different solutions based on the type of smell. We evaluate our approach on detecting and refactoring three classical and complex code smells, i.e., Refused Bequest, God Class, and Feature Envy. The results show that, by adopting seven expert code smell toolsets, iSMELL achieved an average F1 score of 75.17\\% on code smell detection, outperforming LLMs baselines by an increase of 35.05\\% in F1 score. We further evaluate the code refactored by the enhanced LLM. The quantitative and human evaluation results show that iSMELL could improve code quality metrics and conduct satisfactory refactoring toward the identified code smells. We believe that our proposed solution could provide new insights into better leveraging LLMs and existing approaches to resolving complex software tasks.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1345\u20131357",
        "numpages": "13",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "labels": [
            "code generation",
            "program repair",
            "static analysis",
            "bug detection"
        ]
    },
    "LLM-Generated Invariants for Bounded Model Checking Without Loop Unrolling": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695512",
        "author": "Pirzada, Muhammad A. A. and Reger, Giles and Bhayat, Ahmed and Cordeiro, Lucas C.",
        "title": "LLM-Generated Invariants for Bounded Model Checking Without Loop Unrolling",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695512",
        "doi": "10.1145/3691620.3695512",
        "abstract": "We investigate a modification of the classical Bounded Model Checking (BMC) procedure that does not handle loops through unrolling but via modifications to the control flow graph (CFG). A portion of the CFG representing a loop is replaced by a node asserting invariants of the loop. We generate these invariants using Large Language Models (LLMs) and use a first-order theorem prover to ensure the correctness of the generated statements. We thus transform programs to loop-free variants in a sound manner. Our experimental results show that the resulting tool, ESBMC ibmc, is competitive with state-of-the-art formal verifiers for programs with unbounded loops, significantly improving the number of programs verified by the industrial-strength software verifier ESBMC and verifying programs that state-of-the-art software verifiers such as SeaHorn and VeriAbs could not.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1395\u20131407",
        "numpages": "13",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Semi-Supervised Code Translation Overcoming the Scarcity of Parallel Code Data": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695524",
        "author": "Zhu, Ming and Karim, Mohimenul and Lourentzou, Ismini and Yao, Daphne",
        "title": "Semi-Supervised Code Translation Overcoming the Scarcity of Parallel Code Data",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695524",
        "doi": "10.1145/3691620.3695524",
        "abstract": "Neural code translation is the task of converting source code from one programming language to another. One of the main challenges is the scarcity of parallel code data, which hinders the ability of translation models to learn accurate cross-language alignments. In this paper, we introduce MIRACLE, a semi-supervised approach that improves code translation through synthesizing high-quality parallel code data and curriculum learning on code data with ascending alignment levels. MIRACLE leverages static analysis and compilation to generate synthetic parallel code datasets with enhanced quality and alignment to address the challenge of data scarcity. We evaluate the proposed method along with strong baselines including instruction-tuned Large Language Models (LLMs) for code. Our analysis reveals that LLMs pre-trained on open-source code data, regardless of their size, suffer from the \"shallow translation\" problem. This issue arises when translated code copies labels, statements, and even code blocks from the source language, leading to compilation and runtime errors. Extensive experiments demonstrate that our method significantly mitigates this issue, enhancing code translation performance across multiple models in C++, Java, Python, and C. Remarkably, MIRACLE outperforms code LLMs that are ten times larger in size. MIRACLE also achieves up to a 43\\% improvement in C code translation with fewer than 150 annotated examples.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1545\u20131556",
        "numpages": "12",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Test-Driven Development and LLM-based Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695527",
        "author": "Mathews, Noble Saji and Nagappan, Meiyappan",
        "title": "Test-Driven Development and LLM-based Code Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695527",
        "doi": "10.1145/3691620.3695527",
        "abstract": "Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1583\u20131594",
        "numpages": "12",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "On the Evaluation of Large Language Models in Unit Test Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695529",
        "author": "Yang, Lin and Yang, Chen and Gao, Shutao and Wang, Weijing and Wang, Bo and Zhu, Qihao and Chu, Xiao and Zhou, Jianyi and Liang, Guangtai and Wang, Qianxiang and Chen, Junjie",
        "title": "On the Evaluation of Large Language Models in Unit Test Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695529",
        "doi": "10.1145/3691620.3695529",
        "abstract": "Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs' capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1607\u20131619",
        "numpages": "13",
        "labels": [
            "program testing",
            "unit testing",
            "empirical study"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695536",
        "author": "Chen, Mouxiang and Liu, Zhongxin and Tao, He and Hong, Yusu and Lo, David and Xia, Xin and Sun, Jianling",
        "title": "B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695536",
        "doi": "10.1145/3691620.3695536",
        "abstract": "Selecting the best code solution from multiple generated ones is an essential task in code generation, which can be achieved by using some reliable validators (e.g., developer-written test cases) for assistance. Since reliable test cases are not always available and can be expensive to build in practice, researchers propose to automatically generate test cases to assess code solutions. However, when both code solutions and test cases are plausible and not reliable, selecting the best solution becomes challenging. Although some heuristic strategies have been proposed to tackle this problem, they lack a strong theoretical guarantee and it is still an open question whether an optimal selection strategy exists. Our work contributes in two ways. First, we show that within a Bayesian framework, the optimal selection strategy can be defined based on the posterior probability of the observed passing states between solutions and tests. The problem of identifying the best solution is then framed as an integer programming problem. Second, we propose an efficient approach for approximating this optimal (yet uncomputable) strategy, where the approximation error is bounded by the correctness of prior knowledge. We then incorporate effective prior knowledge to tailor code generation tasks. Both theoretical and empirical studies confirm that existing heuristics are limited in selecting the best solutions with plausible test cases. Our proposed approximated optimal strategy \u212c4 significantly surpasses existing heuristics in selecting code solutions generated by large language models (LLMs) with LLM-generated tests, achieving a relative performance improvement by up to 50\\% over the strongest heuristic and 246\\% over the random selection in the most challenging scenarios. Our code is publicly available at https://github.com/ZJU-CTAG/B4.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1693\u20131705",
        "numpages": "13",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695552",
        "author": "Feng, Jia and Liu, Jiachen and Gao, Cuiyun and Chong, Chun Yong and Wang, Chaozheng and Gao, Shan and Xia, Xin",
        "title": "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695552",
        "doi": "10.1145/3691620.3695552",
        "abstract": "In recent years, with the widespread attention of academia and industry on the application of large language models (LLMs) to code-related tasks, an increasing number of large code models (LCMs) have been proposed and corresponding evaluation benchmarks have continually emerged. Although existing evaluation benchmarks are helpful for comparing different LCMs, they may not reflect the performance of LCMs in various development scenarios. Specifically, they might evaluate model performance in only one type of scenario (e.g., code generation or code completion), whereas real development contexts are diverse and may involve multiple tasks such as code generation, code completion, API recommendation, and test function generation. Additionally, the questions may not originate from actual development practices, failing to capture the programming challenges faced by developers during the development process.To address the aforementioned issues, we propose Complex-CodeEval, a new benchmark for evaluating the performance of LCMs in various development scenarios. ComplexCodeEval includes 3,897 Java samples from 1,055 high-star GitHub repositories and 7,184 Python samples from 2,107 high-star repositories. Each function sample in ComplexCodeEval contains multiple annotations (e.g., function signatures, docstrings and reference APIs) to accommodate various downstream tasks. Furthermore, to better reflect diverse development scenarios, each function sample is required to originate from a repository that depends on at least one selected library (based on popularity), and each function sample must invoke at least one API from the selected library. Additionally, each function sample has multiple timestamps to avoid data leakage. Based on ComplexCodeEval, we evaluate the performance of ten LCMs across four tasks (i.e., code generation, code completion, API recommendation, and test case generation) to explore their performance in complex development environments. Furthermore, we conduct an in-depth analysis of the impact of context and data leakage on model performance. Our experimental results reveal several key findings. For instance, LCMs exhibit varying performance across different coding tasks. Additionally, rich contextual information can greatly enhance the performance of LCMs. Moreover, using leaked data for evaluation may lead to an overestimation of model performance, resulting in inaccurate evaluation outcomes that deviate from the performance in practice.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1895\u20131906",
        "numpages": "12",
        "labels": [
            "general coding task",
            "benchmark"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Understanding Developer-Analyzer Interactions in Code Reviews": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695257",
        "author": "Schaef, Martin and Cirisci, Berk and Luo, Linghui and Mansur, Muhammad Numair and Tripp, Omer and Sanchez, Daniel and Zhou, Qiang and Zafar, Muhammad Bilal",
        "title": "Understanding Developer-Analyzer Interactions in Code Reviews",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695257",
        "doi": "10.1145/3691620.3695257",
        "abstract": "Static code analyzers are now a common part of the codereview process. These automated tools integrate into the code review process by commenting on code changes and suggesting improvements, in the same way as human reviewers. The comments made by static analyzers often trigger a conversation between developers to align on if and how the issue should be fixed. Because developers rarely give feedback directly to the tool, understanding the sentiment and intent in the conversation triggered by the tool comments can be used to measure the usefulness of the static analyzer.In this paper, we report on an experiment where we use large language models to automatically label and categorize the sentiment and intent of such conversations triggered by static analyzer comments. Our experiment demonstrates that LLMs not only classify and interpret complex developer-analyzer conversations, but can be more accurate than human experts.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1945\u20131955",
        "numpages": "11",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "labels": [
            "software maintenance and deployment",
            "code review",
            "empirical study"
        ]
    },
    "Ansible Lightspeed: A Code Generation Service for IT Automation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695277",
        "author": "Sahoo, Priyam and Pujar, Saurabh and Nalawade, Ganesh and Genhardt, Richard and Mandel, Louis and Buratti, Luca",
        "title": "Ansible Lightspeed: A Code Generation Service for IT Automation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695277",
        "doi": "10.1145/3691620.3695277",
        "abstract": "The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for Information Technology (IT) automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Ansible Lightspeed is an LLM-based service designed explicitly to generate Ansible YAML, given natural language prompt.In this paper, we present the design and implementation of the Ansible Lightspeed service. We then evaluate its utility to developers using diverse indicators, including extended utilization, analysis of user edited suggestions, as well as user sentiments analysis. The evaluation is based on data collected for 10,696 real users including 3,910 returning users. The code for Ansible Lightspeed service and the analysis framework is made available for others to use.To our knowledge, our study is the first to involve thousands of users of code assistants for domain-specific languages. We are also the first code completion tool to present N-Day user retention figures, which is 13.66\\% on Day 30. We propose an improved version of user acceptance rate, called Strong Acceptance rate, where a suggestion is considered accepted only if less than 50\\% of it is edited and these edits do not change critical parts of the suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong acceptance rate of 49.08\\% for multi-line Ansible task suggestions. With our findings we provide insights into the effectiveness of small, dedicated models in a domain-specific context. We hope this work serves as a reference for software engineering and machine learning researchers exploring code completion.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2148\u20132158",
        "numpages": "11",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "labels": [
            "code generation",
            "program synthesis"
        ]
    },
    "Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695290",
        "author": "Zhang, Beiqi and Liang, Peng and Feng, Qiong and Fu, Yujia and Li, Zengyang",
        "title": "Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695290",
        "doi": "10.1145/3691620.3695290",
        "abstract": "As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released in September 2023, functions as an interactive tool aimed at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot Chat's ability to fix the code smells. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot Chat in fixing these code smells employing different prompts. The results show that 8 out of 10 types of code smells can be detected in Copilot-generated Python code, among which Multiply-Nested Container is the most common one. For these code smells, Copilot Chat achieves a highest fixing rate of 87.1\\%, showing promise in fixing Python code smells generated by Copilot itself. In addition, the effectiveness of Copilot Chat in fixing these smells can be improved by providing more detailed prompts.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2230\u20132234",
        "numpages": "5",
        "labels": [
            "code generation",
            "program repair"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Attacks and Defenses for Large Language Models on Coding Tasks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695297",
        "author": "Zhang, Chi and Wang, Zifan and Zhao, Ruoshi and Mangal, Ravi and Fredrikson, Matt and Jia, Limin and Pasareanu, Corina",
        "title": "Attacks and Defenses for Large Language Models on Coding Tasks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695297",
        "doi": "10.1145/3691620.3695297",
        "abstract": "Modern large language models (LLMs), such as ChatGPT, have demonstrated impressive capabilities for coding tasks, including writing and reasoning about code. They improve upon previous neural network models of code, such as code2seq or seq2seq, that already demonstrated competitive results when performing tasks such as code summarization and identifying code vulnerabilities. However, these previous code models were shown vulnerable to adversarial examples, i.e., small syntactic perturbations designed to \"fool\" the models. In this paper, we first aim to study the transferability of adversarial examples, generated through white-box attacks on smaller code models, to LLMs. We also propose a new attack using an LLM to generate the perturbations. Further, we propose novel cost-effective techniques to defend LLMs against such adversaries via prompting, without incurring the cost of retraining. These prompt-based defenses involve modifying the prompt to include additional information, such as examples of adversarially perturbed code and explicit instructions for reversing adversarial perturbations. Our preliminary experiments show the effectiveness of the attacks and the proposed defenses on popular LLMs such as GPT-3.5 and GPT-4.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2268\u20132272",
        "numpages": "5",
        "labels": [
            "code model",
            "code model security"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "RepoSim: Evaluating Prompt Strategies for Code Completion via User Behavior Simulation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695299",
        "author": "Peng, Chao and Wu, Qinyun and Liu, Jiangchao and Liu, Jierui and Jiang, Bo and Xu, Mengqian and Wang, Yinghao and Liu, Xia and Yang, Ping",
        "title": "RepoSim: Evaluating Prompt Strategies for Code Completion via User Behavior Simulation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695299",
        "doi": "10.1145/3691620.3695299",
        "abstract": "Large language models (LLMs) have revolutionized code completion tasks. IDE plugins such as MarsCode can generate code recommendations, saving developers significant time and effort. However, current evaluation methods for code completion are limited by their reliance on static code benchmarks, which do not consider human interactions and evolving repositories. This paper proposes RepoSim, a novel benchmark designed to evaluate code completion tasks by simulating the evolving process of repositories and incorporating user behaviors. RepoSim leverages data from an IDE plugin, by recording and replaying user behaviors to provide a realistic programming context for evaluation. This allows for the assessment of more complex prompt strategies, such as utilizing recently visited files and incorporating user editing history. Additionally, RepoSim proposes a new metric based on users' acceptance or rejection of predictions, offering a user-centric evaluation criterion. Our preliminary evaluation demonstrates that incorporating users' recent edit history into prompts significantly improves the quality of LLM-generated code, highlighting the importance of temporal context in code completion. RepoSim represents a significant advancement in benchmarking tools, offering a realistic and user-focused framework for evaluating code completion performance.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2279\u20132283",
        "numpages": "5",
        "labels": [
            "code generation",
            "code completion"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "PACGBI: A Pipeline for Automated Code Generation from Backlog Items": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695346",
        "author": "Sarschar, Mahja and Zhang, Gefei and Nowak, Annika",
        "title": "PACGBI: A Pipeline for Automated Code Generation from Backlog Items",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695346",
        "doi": "10.1145/3691620.3695346",
        "abstract": "While there exist several tools to leverage Large Language Models (LLMs) for code generation, their capabilities are limited to the source code editor and are disconnected from the overall software development process. These tools typically generate standalone code snippets that still require manual integration into the codebase. There is still a lack of integrated solutions that seamlessly automate the entire development cycle, from backlog items to code generation and merge requests. We present the Pipeline for Automated Code Generation from Backlog Items (PACGBI), an LLM-assisted pipeline integrated into GitLab CI. PACGBI reads backlog items in the code repository, automatically generates the corresponding code, and creates merge requests for the generated changes. Our case study demonstrates the potential of PACGBI in automating agile software development processes, allowing parallelization of development and reduction of development costs. PACGBI can be utilized by software developers and enables nontechnical stakeholders and designers by providing a holistic solution for using LLMs in software development. A screencast of this tool is available at https://youtu.be/TI53m-fIoyc, its source code at https://github.com/Masa-99/pacgbi.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2338\u20132341",
        "numpages": "4",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "labels": [
            "code generation",
            "program synthesis"
        ]
    },
    "ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695349",
        "author": "Wang, Che and Zhang, Jiashuo and Gao, Jianbo and Xia, Libin and Guan, Zhi and Chen, Zhong",
        "title": "ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695349",
        "doi": "10.1145/3691620.3695349",
        "abstract": "Smart contracts are susceptible to being exploited by attackers, especially when facing real-world vulnerabilities. To mitigate this risk, developers often rely on third-party audit services to identify potential vulnerabilities before project deployment. Nevertheless, repairing the identified vulnerabilities is still complex and laborintensive, particularly for developers lacking security expertise. Moreover, existing pattern-based repair tools mostly fail to address real-world vulnerabilities due to their lack of high-level semantic understanding. To fill this gap, we propose ContractTinker, a Large Language Models (LLMs)-empowered tool for real-world vulnerability repair. The key insight is our adoption of the Chain-of-Thought approach to break down the entire generation task into subtasks. Additionally, to reduce hallucination, we integrate program static analysis to guide the LLM. We evaluate ContractTinker on 48 high-risk vulnerabilities. The experimental results show that among the patches generated by ContractTinker, 23 (48\\%) are valid patches that fix the vulnerabilities, while 10 (21\\%) require only minor modifications. A video of ContractTinker is available at https://youtu.be/HWFVi-YHcPE.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2350\u20132353",
        "numpages": "4",
        "labels": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "CoqPilot, a plugin for LLM-based generation of proofs": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695357",
        "author": "Kozyrev, Andrei and Solovev, Gleb and Khramov, Nikita and Podkopaev, Anton",
        "title": "CoqPilot, a plugin for LLM-based generation of proofs",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695357",
        "doi": "10.1145/3691620.3695357",
        "abstract": "We present CoqPilot, a VS Code extension designed to help automate writing of Coq proofs. The plugin collects the parts of proofs marked with the admit tactic in a Coq file, i.e., proof holes, and combines LLMs along with non-machine-learning methods to generate proof candidates for the holes. Then, CoqPilot checks if each proof candidate solves the given subgoal and, if successful, replaces the hole with it. The focus of CoqPilot is twofold. Firstly, we want to allow users to seamlessly combine multiple Coq generation approaches and provide a zero-setup experience for our tool. Secondly, we want to deliver a platform for LLM-based experiments on Coq proof generation. We developed a benchmarking system for Coq generation methods, available in the plugin, and conducted an experiment using it, showcasing the framework's possibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo. Code at: https://github.com/JetBrains-Research/coqpilot",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2382\u20132385",
        "numpages": "4",
        "labels": [
            "code generation",
            "program synthesis",
            "static analysis",
            "program verification"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "LLM-Based Java Concurrent Program to ArkTS Converter": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695362",
        "author": "Liu, Runlin and Lin, Yuhang and Hu, Yunge and Zhang, Zhe and Gao, Xiang",
        "title": "LLM-Based Java Concurrent Program to ArkTS Converter",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695362",
        "doi": "10.1145/3691620.3695362",
        "abstract": "HarmonyOS NEXT is a distributed operating system developed to support HarmonyOS native apps. To support the new and independent Harmony ecosystem, developers are required to migrate their applications from Android to HarmonyOS. However, HarmonyOS utilizes ArkTS, a superset of TypeScript, as the programming language for application development. Hence, migrating applications to HarmonyOS requires translating programs across different program languages, e.g., Java, which is known to be very challenging, especially for concurrency programs. Java utilizes shared memory to implement concurrency programs, while ArkTS relies on message passing (i.e., Actor model). This paper presents an LLM-based concurrent Java program to ArkTS converter.Our converter utilizes large language models (LLMs) for efficient code translation, integrating ArkTS's SharedArrayBuffer API to create ThreadBridge, a library that replicates Java's shared memory model. Using LLM's Chain-of-Thought mechanism, the translation process is divided into specialized chains: the TS chain, concurrency chain, and synchronization chain, each handling TypeScript syntax, concurrency patterns, and synchronization logic with precision.This study offers solutions to bridge concurrency model differences between Java and ArkTS, reducing manual code rewriting and speeding up adaptation for HarmonyOS NEXT. Experiments show the converter successfully compiles 66\\% of 53 test samples, with 69\\% accuracy for compiled results. Overall, the approach shows promise in converting concurrent Java programs to ArkTS, laying the foundation for future improvements.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2403\u20132406",
        "numpages": "4",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Automated Validation of COBOL to Java Transformation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695365",
        "author": "Kumar, Atul and Saha, Diptikalyan and Yasue, Toshiaki and Ono, Kohichi and Krishnan, Saravanan and Hans, Sandeep and Satoh, Fumiko and Mitchell, Gerald and Kumar, Sachin",
        "title": "Automated Validation of COBOL to Java Transformation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695365",
        "doi": "10.1145/3691620.3695365",
        "abstract": "Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterpriselevel code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code. We propose a framework and a tool to help validate the equivalence of COBOL and translated Java. The results can also help repair the code if there are some issues and provide feedback to the AI model to improve. We have developed a symbolic-execution-based test generation to automatically generate unit tests for the source COBOL programs which also mocks the external resource calls. We generate equivalent JUnit test cases with equivalent mocking as COBOL and run them to check semantic equivalence between original and translated programs. Demo Video: https://youtu.be/aqF_agNP-lU",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2415\u20132418",
        "numpages": "4",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Can Large Language Models Comprehend Code Stylometry?": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695370",
        "author": "Dipongkor, Atish",
        "title": "Can Large Language Models Comprehend Code Stylometry?",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695370",
        "doi": "10.1145/3691620.3695370",
        "abstract": "Code Authorship Attribution (CAA) has several applications such as copyright disputes, plagiarism detection and criminal prosecution. Existing studies mainly focused on CAA by proposing machine learning (ML) and Deep Learning (DL) based techniques. The main limitations of ML-based techniques are (a) manual feature engineering is required to train these models and (b) they are vulnerable to adversarial attack. In this study, we initially fine-tune five Large Language Models (LLMs) for CAA and evaluate their performance. Our results show that LLMs are robust and less vulnerable compared to existing techniques in CAA task.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2429\u20132431",
        "numpages": "3",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "labels": [
            "static analysis",
            "software composition analysis"
        ]
    },
    "Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695322",
        "author": "Luo, Yang and Yu, Richard and Zhang, Fajun and Liang, Ling and Xiong, Yongqiang",
        "title": "Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695322",
        "doi": "10.1145/3691620.3695322",
        "abstract": "When using large language models (LLMs) for code translation of complex software, numerous compilation and runtime errors can occur due to insufficient context awareness. To address this issue, this paper presents a code translation method based on call graphs and bridged debuggers: TransGraph. TransGraph first obtains the call graph of the entire code project using the Language Server Protocol, which provides a detailed description of the function call relationships in the program. Through this structured view of the code, LLMs can more effectively handle large-scale and complex codebases, significantly reducing compilation errors. Furthermore, TransGraph, combined with bridged debuggers and dynamic test case generation, significantly reduces runtime errors, overcoming the limitations of insufficient test case coverage in traditional methods. In experiments on six datasets including CodeNet and Avatar, TransGraph outperformed existing code translation methods and LLMs in terms of translation accuracy, with improvements of up to 10.2\\%.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2448\u20132449",
        "numpages": "2",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Cross-lingual Code Clone Detection: When LLMs Fail Short Against Embedding-based Classifier": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695335",
        "author": "Moumoula, Micheline Benedicte and Kabore, Abdoul Kader and Klein, Jacques and Bissyande, Tegawende F.",
        "title": "Cross-lingual Code Clone Detection: When LLMs Fail Short Against Embedding-based Classifier",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695335",
        "doi": "10.1145/3691620.3695335",
        "abstract": "Cross-lingual code clone detection has gained attention in software development due to the use of multiple programming languages. Recent advances in machine learning, particularly Large Language Models (LLMs), have motivated a reexamination of this problem.This paper evaluates the performance of four LLMs and eight prompts for detecting cross-lingual code clones, as well as a pretrained embedding model for classifying clone pairs. Both approaches are tested on the XLCoST and CodeNet datasets.Our findings show that while LLMs achieve high F1 scores (up to 0.98) on straightforward programming examples, they struggle with complex cases and cross-lingual understanding. In contrast, embedding models, which map code fragments from different languages into a common representation space, allow for the training of a basic classifier that outperforms LLMs by approximately 2 and 24 percentage points on the XLCoST and CodeNet datasets, respectively. This suggests that embedding models provide more robust representations, enabling state-of-the-art performance in cross-lingual code clone detection.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2474\u20132475",
        "numpages": "2",
        "labels": [
            "static analysis",
            "code similarity analysis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Who Judges the Judge: An Empirical Study on Online Judge Tests": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598060",
        "author": "Liu, Kaibo and Han, Yudong and Zhang, Jie M. and Chen, Zhenpeng and Sarro, Federica and Harman, Mark and Huang, Gang and Ma, Yun",
        "title": "Who Judges the Judge: An Empirical Study on Online Judge Tests",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598060",
        "doi": "10.1145/3597926.3598060",
        "abstract": "Online Judge platforms play a pivotal role in education, competitive programming, recruitment, career training, and large language model training. They rely on predefined test suites to judge the correctness of submitted solutions. It is therefore important that the solution judgement is reliable and free from potentially misleading false positives (i.e., incorrect solutions that are judged as correct). In this paper, we conduct an empirical study of 939 coding problems with 541,552 solutions, all of which are judged to be correct according to the test suites used by the platform, finding that 43.4\\% of the problems include false positive solutions (3,440 bugs are revealed in total). We also find that test suites are, nevertheless, of high quality according to widely-studied test effectiveness measurements: 88.2\\% of false positives have perfect (100\\%) line coverage, 78.9\\% have perfect branch coverage, and 32.5\\% have a perfect mutation score. Our findings indicate that more work is required to weed out false positive solutions and to further improve test suite effectiveness. We have released the detected false positive solutions and the generated test inputs to facilitate future research.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "334\u2013346",
        "numpages": "13",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023",
        "venue": "ISSTA2023"
    },
    "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598067",
        "author": "Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming",
        "title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598067",
        "doi": "10.1145/3597926.3598067",
        "abstract": "Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations.  To address these limitations, we propose TitanFuzz \u2013 the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38\\%/50.84\\% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs.  This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "423\u2013435",
        "numpages": "13",
        "labels": [
            "program testing",
            "fuzzing",
            "library testing"
        ],
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023",
        "venue": "ISSTA2023"
    },
    "How Effective Are Neural Networks for Fixing Security Vulnerabilities": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598135",
        "author": "Wu, Yi and Jiang, Nan and Pham, Hung Viet and Lutellier, Thibaud and Davis, Jordan and Tan, Lin and Babkin, Petr and Shah, Sameena",
        "title": "How Effective Are Neural Networks for Fixing Security Vulnerabilities",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598135",
        "doi": "10.1145/3597926.3598135",
        "abstract": "Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans, to better evaluate LLMs and APR techniques, and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4\\%), the most number of vulnerabilities. Many of the generated patches are uncompilable patches. (2) Fine-tuning with general APR data improves LLMs\u2019 vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes 8.7 transformed vulnerabilities, outperforming all the other LLMs and APR models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning LLMs with such data, and applying code simplification transformation to facilitate vulnerability repair.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1282\u20131294",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair",
            "benchmark"
        ],
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023",
        "venue": "ISSTA2023"
    },
    "CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00085",
        "author": "Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K. and Sen, Siddhartha",
        "title": "CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00085",
        "doi": "10.1109/ICSE48619.2023.00085",
        "abstract": "Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "919\u2013931",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "labels": [
            "program testing",
            "fuzzing"
        ]
    },
    "CCTest: Testing and Repairing Code Completion Systems": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00110",
        "author": "Li, Zongjie and Wang, Chaozheng and Liu, Zhibo and Wang, Haoxuan and Chen, Dong and Wang, Shuai and Gao, Cuiyun",
        "title": "CCTest: Testing and Repairing Code Completion Systems",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00110",
        "doi": "10.1109/ICSE48619.2023.00110",
        "abstract": "Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems.In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in black-box settings. CCTest features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTest repairs the code completion outputs by selecting the output that mostly reflects the \"average\" appearance of all output cases, as the final output of the code completion systems. With around 18K test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86\\%) from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40\\% and 67\\% with respect to BLEU score and Levenshtein edit similarity.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1238\u20131250",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "labels": [
            "code generation",
            "code completion"
        ]
    },
    "Automated Repair of Programs from Large Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00128",
        "author": "Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei",
        "title": "Automated Repair of Programs from Large Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00128",
        "doi": "10.1109/ICSE48619.2023.00128",
        "abstract": "Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1469\u20131481",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "labels": [
            "code generation",
            "program repair"
        ]
    },
    "Automated Program Repair in the Era of Large Pre-Trained Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00129",
        "author": "Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming",
        "title": "Automated Program Repair in the Era of Large Pre-Trained Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00129",
        "doi": "10.1109/ICSE48619.2023.00129",
        "abstract": "Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1482\u20131494",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "labels": [
            "code generation",
            "program repair"
        ]
    },
    "Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00194",
        "author": "Kang, Sungmin and Yoon, Juyeon and Yoo, Shin",
        "title": "Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00194",
        "doi": "10.1109/ICSE48619.2023.00194",
        "abstract": "Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28\\% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose LIBRO, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of LIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate failure reproducing test cases for 33\\% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate LIBRO against 31 bug reports submitted after the collection of the LLM training data terminated: LIBRO produces bug reproducing tests for 32\\% of the studied bug reports. Overall, our results show LIBRO has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "2312\u20132323",
        "numpages": "12",
        "labels": [
            "program testing",
            "bug reproduction"
        ],
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023"
    },
    "COMEX: A Tool for Generating Customized Source Code Representations": {
        "type": "INPROCEEDINGS",
        "key": "10298568",
        "author": "Das, Debeshee and Mathews, Noble Saji and Mathai, Alex and Tamilselvam, Srikanth and Sedamaki, Kranthi and Chimalakonda, Sridhar and Kumar, Atul",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "COMEX: A Tool for Generating Customized Source Code Representations",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "2054-2057",
        "abstract": "Learning effective representations of source code is critical for any Machine Learning for Software Engineering (ML4SE) system. Inspired by natural language processing, large language models (LLMs) like Codex and CodeGen treat code as generic sequences of text and are trained on huge corpora of code data, achieving state of the art performance on several software engineering (SE) tasks. However, valid source code, unlike natural language, follows a strict structure and pattern governed by the underlying grammar of the programming language. Current LLMs do not exploit this property of the source code as they treat code like a sequence of tokens and overlook key structural and semantic properties of code that can be extracted from code-views like the Control Flow Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc. Unfortunately, the process of generating and integrating code-views for every programming language is cumbersome and time consuming. To overcome this barrier, we propose our tool COMEX - a framework that allows researchers and developers to create and combine multiple code-views which can be used by machine learning (ML) models for various SE tasks. Some salient features of our tool are: (i) it works directly on source code (which need not be compilable), (ii) it currently supports Java and C#, (iii) it can analyze both method-level snippets and program-level snippets by using both intra-procedural and inter-procedural analysis, and (iv) it is easily extendable to other languages as it is built on tree-sitter - a widely used incremental parser that supports over 40 languages. We believe this easy-to-use code-view generation and customization tool will give impetus to research in source code representation learning methods and ML4SE. The source code and demonstration of our tool can be found at https://github.com/IBM/tree-sitter-codeviews and https://youtu.be/GER6U87FVbU, respectively.",
        "labels": [
            "code generation",
            "source code model"
        ],
        "doi": "10.1109/ASE56229.2023.00010",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://arxiv.org/pdf/2307.04693"
    },
    "An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair": {
        "type": "INPROCEEDINGS",
        "key": "10298532",
        "author": "Huang, Kai and Meng, Xiangxin and Zhang, Jian and Liu, Yang and Wang, Wenjie and Li, Shuhao and Zhang, Yuqing",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1162-1174",
        "abstract": "The advent of large language models (LLMs) has opened up new opportunities for automated program repair (APR). In particular, some recent studies have explored how to leverage large language models of code (LLMCs) for program repair tasks and show promising results. However, most of them adopt the zero/few-shot learning paradigm for APR, which directly use LLMCs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMCs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMCs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in this work, we conduct a comprehensive study on the program repair capability of LLMCs in the fine-tuning paradigm. We select 5 popular LLMCs with representative pre-training architectures, including CodeBERT, GraphCode-BERT, PLBART, CodeT5, and UniX coder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, $\\mathrm{C}/\\mathrm{C}++$, and JavaScript). Notably, we take both single-hunk and multi-hunk bugs/vulnerabilities into account. We then fine-tune them on widely-used datasets and compare them with existing state-of-the-art APR tools. We also investigate the impact of different design choices, which include code abstractions, code representations, and model evaluation metrics. Our experimental results show that LLMCs in the fine-tuning paradigm can significantly outperform previous state-of-the-art APR tools. Through in-depth analysis, we provide insights into choosing appropriate strategies to guide LLMCs for better performance. Lastly, we reveal several limitations of LLMCs for APR and make suggestions for future research on LLMC-based APR.",
        "labels": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "doi": "10.1109/ASE56229.2023.00181",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10298532"
    },
    "SMT Solver Validation Empowered by Large Pre-Trained Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10298442",
        "author": "Sun, Maolin and Yang, Yibiao and Wang, Yang and Wen, Ming and Jia, Haoxiang and Zhou, Yuming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "SMT Solver Validation Empowered by Large Pre-Trained Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1288-1300",
        "abstract": "SMT solvers are utilized to check the satisfiability of logic formulas and have been applied in various crucial domains, including software verification, test case generation, and program synthesis. However, bugs hidden in SMT solvers can lead to severe consequences, causing erroneous results in these domains. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Despite several testing approaches proposed for SMT solvers, generating effective test formulas to comprehensively test SMT solvers remains a challenge. To address this challenge, in this study, we propose to port large language models (LLMs) to generate SMT formulas for fuzzing solvers. Specifically, the study presents a novel retrain-finetune pipeline to unleash the potential of language models to generate effective SMT formulas and improve their generation performance through data augmentation. We implemented our approach as a practical fuzzing tool, named LasT,and then extensively tested the state-of-the-art SMT solvers, namely Z3, cvc5, and Bitwuzla. To date, Last has successfully uncovered 65 genuine bugs for the solvers, of which 45 have been fixed by the developers.",
        "labels": [
            "program testing",
            "fuzzing"
        ],
        "doi": "10.1109/ASE56229.2023.00180",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10298442"
    },
    "Improving Code Extraction from Coding Screencasts Using a Code-Aware Encoder-Decoder Model": {
        "type": "INPROCEEDINGS",
        "key": "10298433",
        "author": "Malkadi, Abdulkarim and Tayeb, Ahmad and Haiduc, Sonia",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Improving Code Extraction from Coding Screencasts Using a Code-Aware Encoder-Decoder Model",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1492-1504",
        "abstract": "Accurate automatic code extraction from tutorial videos is crucial for software developers seeking to reuse the code contained in these videos. Current methods using optical character recognition (OCR) often yield inaccurate results due to code complexity and variations in screencast formats. To address this issue, we introduce CodeT5-OCRfix, an approach that leverages the pre-trained code-aware large language model CodeT5 to enhance code extraction accuracy by post-processing OCRed code. We first collect a large and diverse dataset of source code screenshots captured from more than 10K Java projects from GitHub. We then apply the most widely used OCR engine for the task of code extraction from videos, Tesseract, on these screenshots and collect the OCRed code along with the ground truth code extracted from the Java files. We built a training dataset of more than 585K pairs of OCRed and ground truth code pairs, which we then used to fine-tune CodeT5, obtaining our model CodeT5-OCRfix. An empirical evaluation on both screenshots and screencast frames shows that CodeT5-OCRfix outperforms baseline code extraction models and is also more time-efficient. Our approach therefore improves the state-of-the-art in code extraction techniques from screencasts and images.",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model training",
            "source code model"
        ],
        "doi": "10.1109/ASE56229.2023.00184",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10298433"
    },
    "Better Patching Using LLM Prompting, via Self-Consistency": {
        "type": "INPROCEEDINGS",
        "key": "10298561",
        "author": "Ahmed, Toufique and Devanbu, Premkumar",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Better Patching Using LLM Prompting, via Self-Consistency",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1742-1746",
        "abstract": "Large Language models (LLMs) can be induced to solve non-trivial problems with \u201cfew-shot\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \u201cchain of thought\u201d ($\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \u201cexplained\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\mathcal{S}-C$ (or even $\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.",
        "labels": [
            "code generation",
            "program repair",
            "agent design",
            "prompt strategy"
        ],
        "doi": "10.1109/ASE56229.2023.00065",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://arxiv.org/pdf/2306.00108"
    },
    "A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT": {
        "type": "INPROCEEDINGS",
        "key": "10298505",
        "author": "Yan, Dapeng and Gao, Zhipeng and Liu, Zhiming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1887-1898",
        "abstract": "Code generation aims to generate source code implementing human requirements illustrated with natural language specifications. With the rapid development of intelligent software engineering, automated code generation has become a hot research topic in both artificial intelligence and software engineering, and researchers have made significant achievements on code generation. More recently, large language models (LLMs) have demonstrated outstanding performance on code generation tasks, such as ChatGPT released by OpenAI presents the fantastic potential on automated code generation. However, the existing studies are limited to exploring LLMs' ability for generating code snippets to solve simple programming problems, the task of competition-level code generation has never been investigated. The specifications of the programming competition are always complicated and require the specific input/output format as well as the high-level algorithmic reasoning ability. In this study, we conduct the first large empirical study to investigate the zero-shot learning ability of ChatGPT for solving competition programming problems. Specifically, we warm up the design of prompts by using the Human-Eval dataset. Then, we apply the well-designed prompt to the competition-level code generation dataset, namely APPS, to further explore the effectiveness of using ChatGPT for solving competition problems. We collect ChatGPT's outputs on 5,000 code competition problems, the evaluation results show that it can successfully pass 25.4% test cases. By further feeding extra information (e.g, test failed information) to ChatGPT, we observe that ChatGPT has the potential to fix partial pass into a fully pass program. Moreover, we investigate the solutions generated by LLMs and the existing solutions, we find that it prefers to directly copy the code instead of re-write when facing more difficult problems. Finally, we evaluate the code quality generated by ChatGPT in terms of \u201ccode cleanness\u201d, we observe that the generated codes are with small functions and file sizes, which are in line with the standard of clean code.",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "doi": "10.1109/ASE56229.2023.00096",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10298505"
    },
    "Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting": {
        "type": "INPROCEEDINGS",
        "key": "10298538",
        "author": "Li, Tsz-On and Zong, Wenxi and Wang, Yibo and Tian, Haoye and Wang, Ying and Cheung, Shing-Chi and Kramer, Jeff",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "14-26",
        "abstract": "Automated detection of software failures is an important but challenging software engineering task. It involves finding in a vast search space the failure-inducing test cases that contain an input triggering the software fault and an oracle asserting the incorrect execution. We are motivated to study how far this outstanding challenge can be solved by recent advances in large language models (LLMs) such as ChatGPT. However, our study reveals that ChatGPT has a relatively low success rate (28.8%) in finding correct failure-inducing test cases for buggy programs. A possible conjecture is that finding failure-inducing test cases requires analyzing the subtle differences (nuances) between the tokens of a program's correct version and those for its buggy version. When these two versions have similar sets of tokens and attentions, ChatGPT is weak in distinguishing their differences. We find that ChatGPT can successfully generate failure-inducing test cases when it is guided to focus on the nuances. Our solution is inspired by an interesting observation that ChatGPT could infer the intended functionality of buggy code if it is similar to the correct version. Driven by the inspiration, we develop a novel technique, called Differential Prompting, to effectively find failure-inducing test cases with the help of the compilable code synthesized by the inferred intention. Prompts are constructed based on the nuances between the given version and the synthesized code. We evaluate Differential Prompting on Quixbugs (a popular benchmark of buggy programs) and recent programs published at Codeforces (a popular programming contest portal, which is also an official benchmark of ChatGPT). We compare Differential Prompting with two baselines constructed using conventional ChatGPT prompting and Pynguin (the state-of-the-art unit test generation tool for Python programs). Our evaluation results show that for programs of Quixbugs, Differential Prompting can achieve a success rate of 75.0% in finding failure-inducing test cases, outperforming the best baseline by 2.6X. For programs of Codeforces, Differential Prompting's success rate is 66.7%, outperforming the best baseline by 4.0X.",
        "labels": [
            "program testing",
            "differential testing"
        ],
        "doi": "10.1109/ASE56229.2023.00089",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10298538"
    },
    "SCPatcher: Mining Crowd Security Discussions to Enrich Secure Coding Practices": {
        "type": "INPROCEEDINGS",
        "key": "10298463",
        "author": "Jiang, Ziyou and Shi, Lin and Yang, Guowei and Wang, Qing",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "SCPatcher: Mining Crowd Security Discussions to Enrich Secure Coding Practices",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "358-370",
        "abstract": "Secure coding practices (SCPs) have been proposed to guide software developers to write code securely to prevent potential security vulnerabilities. Yet, they are typically one-sentence principles without detailed specifications, e.g., \u201cProperly free allocated memory upon the completion of functions and at all exit points.\u201d, which makes them difficult to follow in practice, especially for software developers who are not yet experienced in secure programming. To address this problem, this paper proposes SCPatcher, an automated approach to enrich secure coding practices by mining crowd security discussions on online knowledge-sharing platforms, such as Stack Overflow. In particular, for each security post, SCPatcher first extracts the area of coding examples and coding explanations with a fix-prompt tuned Large Language Model (LLM) via Prompt Learning. Then, it hierarchically slices the lengthy code into coding examples and summarizes the coding explanations with the areas. Finally, SCPatcher matches the CWE and Public SCP, integrating them with extracted coding examples and explanations to form the SCP specifications, which are the wild SCPs with details, proposed by the developers. To evaluate the performance of SCPatcher, we conduct experiments on 3,907 security posts from Stack Overflow. The experimental results show that SCPatcher outperforms all baselines in extracting the coding examples with 2.73 % MLine on average, as well as coding explanations with 3.97 % F1 on average. Moreover, we apply SCPatcher on 447 new security posts to further evaluate its practicality, and the extracted SCP specifications enrich the public SCPs with 3,074 lines of code and 1,967 sentences.",
        "labels": [
            "software maintenance and deployment",
            "code review"
        ],
        "doi": "10.1109/ASE56229.2023.00040",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10298463"
    },
    "The Plastic Surgery Hypothesis in the Era of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10298499",
        "author": "Xia, Chunqiu Steven and Ding, Yifeng and Zhang, Lingming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "The Plastic Surgery Hypothesis in the Era of Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "522-534",
        "abstract": "Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names. The plastic surgery hypothesis is a well-known insight for APR, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional APR tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based approaches to exploit such existing code ingredients. However, as recent APR research starts focusing on LLM-based approaches, the plastic surgery hypothesis has been largely ignored. In this paper, we ask the following question: How useful is the plastic surgery hypothesis in the era of LLMs? Interestingly, LLM-based APR presents a unique opportunity to fully automate the plastic surgery hypothesis via fine-tuning (training on the buggy project) and prompting (directly providing valuable code ingredients as hints to the LLM). To this end, we propose FitRepair, which combines the direct usage of LLMs with two domain-specific fine-tuning strategies and one prompting strategy (via information retrieval and static analysis) for more powerful APR. While traditional APR techniques require intensive manual efforts in both generating patches based on the plastic surgery hypothesis and guaranteeing patch validity, our approach is fully automated and general. Moreover, while it is very challenging to manually design heuristics/patterns for effectively leveraging the hypothesis, due to the power of LLMs in code vectorization/understanding, even partial/imprecise project-specific information can still guide LLMs in generating correct patches! Our experiments on the widely studied Defects4j 1.2 and 2.0 datasets show that FitRepair fixes 89 and 44 bugs (substantially outperforming baseline techniques by 15 and 8), respectively, demonstrating a promising future of the plastic surgery hypothesis in the era of LLMs.",
        "labels": [
            "code generation",
            "program repair"
        ],
        "doi": "10.1109/ASE56229.2023.00047",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://dl.acm.org/doi/pdf/10.1109/ASE56229.2023.00047"
    },
    "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?": {
        "type": "INPROCEEDINGS",
        "key": "10298329",
        "author": "Gao, Shuzheng and Wen, Xin-Cheng and Gao, Cuiyun and Wang, Wenxuan and Zhang, Hongyu and Lyu, Michael R.",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "761-773",
        "abstract": "Pre-trained models of source code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning (ICL). ICL employs task instructions and a few examples as demonstrations, and then inputs the demonstrations to the language models for making predictions. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of ICL heavily relies on the quality of demonstrations, e.g., the selected examples. It is important to systematically investigate how to construct a good demonstration for code-related tasks. In this paper, we empirically explore the impact of three key factors on the performance of ICL in code intelligence tasks: the selection, order, and number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including code summarization, bug fixing, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of ICL in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We also show that a carefully-designed demonstration based on our findings can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90%, 175.96%, and 50.81% on code summarization, bug fixing, and program synthesis, respectively.",
        "labels": [
            "general coding task"
        ],
        "doi": "10.1109/ASE56229.2023.00109",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://arxiv.org/pdf/2304.07575"
    },
    "From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining": {
        "type": "INPROCEEDINGS",
        "key": "10298349",
        "author": "Ren, Xiaoxue and Ye, Xinyuan and Zhao, Dehai and Xing, Zhenchang and Yang, Xiaohu",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "976-987",
        "abstract": "Large Language Models (LLMs) have shown promising results in automatic code generation by improving coding efficiency to a certain extent. However, generating high-quality and reliable code remains a formidable task because of LLMs' lack of good programming practice, especially in exception handling. In this paper, we first conduct an empirical study and summarize three crucial challenges of LLMs in exception handling, i.e., incomplete exception handling, incorrect exception handling and abuse of try-catch. We then try prompts with different granularities to address such challenges, finding fine-grained knowledge-driven prompts works best. Based on our empirical study, we propose a novel Knowledge-driven Prompt Chaining-based code generation approach, name KPC, which decomposes code generation into an AI chain with iterative check-rewrite steps and chains fine-grained knowledge-driven prompts to assist LLMs in considering exception-handling specifications. We evaluate our KPC-based approach with 3,079 code generation tasks extracted from the Java official API documentation. Extensive experimental results demonstrate that the KPC-based approach has considerable potential to ameliorate the quality of code generated by LLMs. It achieves this through proficiently managing exceptions and obtaining remarkable enhancements of 109.86% and 578.57% with static evaluation methods, as well as a reduction of 18 runtime bugs in the sampled dataset with dynamic validation.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "doi": "10.1109/ASE56229.2023.00143",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://arxiv.org/pdf/2309.15606"
    },
    "Generative Type Inference for Python": {
        "type": "INPROCEEDINGS",
        "key": "10298512",
        "author": "Peng, Yun and Wang, Chaozheng and Wang, Wenxuan and Gao, Cuiyun and Lyu, Michael R.",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Generative Type Inference for Python",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "988-999",
        "abstract": "Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub. However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs. Existing type inference approaches can be generally grouped into three categories, i.e., rule-based, supervised, and cloze- style approaches. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems caused by dynamic features and external calls. Supervised type inference approaches, while feature-agnostic and able to mitigate the low coverage problem, require large, high- quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem by leveraging the general knowledge in powerful pre-trained code models. However, their performance is limited since they ignore the domain knowledge from static typing rules which reflect the inference logic. What is more, their predictions are not interpretable, hindering developers' understanding and verification of the results. This paper introduces Typegen, a few-shot generative type inference approach that incorporates static domain knowledge from static analysis. Typegen creates chain-of-thought (COT) prompts by translating the type inference steps of static analysis into prompts based on the type dependency graphs (TDGs), enabling language models to learn from how static analysis infers types. By combining COT prompts with code slices and type hints, TypegEnconstructs example prompts from human annotations. Typeg Enonly requires very few annotated examples to teach language models to generate similar COT prompts via in-context learning. Moreover, Typeg Enenhances the interpretability of results through the use of the input- explanation-output strategy, which generates both explanations and type predictions in COT prompts. Experiments show that Typegen outperforms the best baseline Type4Py by 10.0% for argument type prediction and 22.5 % in return value type prediction in terms of top-l Exact Match by using only five examples. Furthermore, Typeg Enachieves substantial improvements of 27 % to 84 % compared to the zero-shot performance of large language models with parameter sizes ranging from 1.3B to 175B in terms of top-I Exact Match.",
        "labels": [
            "static analysis",
            "type inference"
        ],
        "doi": "10.1109/ASE56229.2023.00031",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://arxiv.org/pdf/2307.09163"
    },
    "Lost at C: a user study on the security implications of large language model code assistants": {
        "type": "inproceedings",
        "key": "10.5555/3620237.3620361",
        "author": "Sandoval, Gustavo and Pearce, Hammond and Nys, Teo and Karri, Ramesh and Garg, Siddharth and Dolan-Gavitt, Brendan",
        "title": "Lost at C: a user study on the security implications of large language model code assistants",
        "year": "2023",
        "isbn": "978-1-939133-37-3",
        "publisher": "USENIX Association",
        "address": "USA",
        "abstract": "Large Language Models (LLMs) such as OpenAI Codex are increasingly being used as AI-based coding assistants. Understanding the impact of these tools on developers' code is paramount, especially as recent work showed that LLMs may suggest cybersecurity vulnerabilities. We conduct a security-driven user study (N=58) to assess code written by student programmers when assisted by LLMs. Given the potential severity of low-level bugs as well as their relative frequency in real-world projects, we tasked participants with implementing a singly-linked 'shopping list' structure in C. Our results indicate that the security impact in this setting (low-level C with pointer and array manipulations) is small: AI-assisted users produce critical security bugs at a rate no greater than 10\\% more than the control, indicating the use of LLMs does not introduce new security risks.",
        "booktitle": "Proceedings of the 32nd USENIX Conference on Security Symposium",
        "articleno": "124",
        "numpages": "18",
        "location": "Anaheim, CA, USA",
        "series": "SEC '23",
        "venue": "USENIXSec2023",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://arxiv.org/pdf/2208.09727"
    },
    "Examining Zero-Shot Vulnerability Repair with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10179420",
        "author": "Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan",
        "booktitle": "2023 IEEE Symposium on Security and Privacy (SP)",
        "title": "Examining Zero-Shot Vulnerability Repair with Large Language Models",
        "year": "2023",
        "volume": "",
        "ISSN": "",
        "pages": "2339-2356",
        "abstract": "Human developers can produce code with cybersecurity bugs. Can emerging \u2018smart\u2019 code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI\u2019s Codex and AI21\u2019s Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information\u2014 both semantically and syntactically\u2014with natural languages. We perform a large scale study of five commercially available, black-box, \"off-the-shelf\" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model\u2019s performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.",
        "labels": [
            "static analysis",
            "bug detection",
            "empirical study"
        ],
        "doi": "10.1109/SP46215.2023.10179420",
        "url": "https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.10179420",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "month": "May",
        "venue": "S&P2023"
    },
    "CodeT5+: Open Code Large Language Models for Code Understanding and Generation": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-codet5",
        "author": "Wang, Yue and Le, Hung and Gotmare, Akhilesh and Bui, Nghi and Li, Junnan and Hoi, Steven",
        "booktitle": "EMNLP2023",
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks, lacking the flexibility to operate in the optimal architecture for a specific task. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some tasks and hence result in substantial performance degrade. To address these limitations, we propose \u201cCodeT5+\u201d, a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives, which cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) performance on various code-related tasks, and our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model.",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.68",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2023-personalized",
        "author": "Chen, Hailin and Saha, Amrita and Hoi, Steven and Joty, Shafiq",
        "booktitle": "EMNLP2023",
        "title": "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher\u2019s prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K personalised examples that incur a data-collection cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to achieve 45.8% pass@1 on HumanEval.",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.417",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Benchmarking and Improving Text-to-SQL Generation under Ambiguity": {
        "type": "INPROCEEDINGS",
        "key": "bhaskar-etal-2023-benchmarking",
        "author": "Bhaskar, Adithya and Tomar, Tushar and Sathe, Ashutosh and Sarawagi, Sunita",
        "booktitle": "EMNLP2023",
        "title": "Benchmarking and Improving Text-to-SQL Generation under Ambiguity",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or structural ambiguity. When faced with ambiguity, an ideal top-k decoder should generate all valid interpretations for possible disambiguation by the user. We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal. The primary reason is that the prevalent beam search algorithm and its variants, treat SQL queries as a string and produce unhelpful token-level diversity in the top-k. We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic space using a blend of plan-based template generation and constrained infilling. Counterfactually generated plans diversify templates while in-filling with a beam-search that branches solely on schema names provides value diversity. LogicalBeam is up to 2.5 times more effective than state-of-the-art models at generating all candidate SQLs in the top-k ranked outputs. It also enhances the top-5 Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA.",
        "labels": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.436",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Symbolic Planning and Code Generation for Grounded Dialogue": {
        "type": "INPROCEEDINGS",
        "key": "chiu-etal-2023-symbolic-planning",
        "author": "Chiu, Justin and Zhao, Wenting and Chen, Derek and Vaduguru, Saujas and Rush, Alexander and Fried, Daniel",
        "booktitle": "EMNLP2023",
        "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) excel at processing and generating text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system, consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code\u2019s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system\u2019s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.",
        "labels": [
            "code generation",
            "program synthesis",
            "agent design",
            "planning"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.460",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Generating Data for Symbolic Language with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ye-etal-2023-generating",
        "author": "Ye, Jiacheng and Li, Chengzu and Kong, Lingpeng and Yu, Tao",
        "booktitle": "EMNLP2023",
        "title": "Generating Data for Symbolic Language with Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks, and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort. SymGen takes a step toward data generation for annotation-expensive complex tasks, and we release the code at URL.",
        "labels": [
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.523",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Let\u2019s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs": {
        "type": "INPROCEEDINGS",
        "key": "aggarwal-etal-2023-lets",
        "author": "Aggarwal, Pranjal and Madaan, Aman and Yang, Yiming and Mausam",
        "booktitle": "EMNLP2023",
        "title": "Let\u2019s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%",
        "labels": [
            "agent design",
            "prompt strategy",
            "sampling and ranking"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.761",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Question Answering as Programming for Solving Time-Sensitive Questions": {
        "type": "INPROCEEDINGS",
        "key": "zhu-etal-2023-question",
        "author": "Zhu, Xinyu and Yang, Cheng and Chen, Bei and Li, Siheng and Lou, Jian-Guang and Yang, Yujiu",
        "booktitle": "EMNLP2023",
        "title": "Question Answering as Programming for Solving Time-Sensitive Questions",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs\u2019 inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the Question Answering task as Programming (QAaP). Concretely, by leveraging modern LLMs\u2019 superior capability in understanding both natural language and programming language, we endeavor to harness LLMs to represent diversely expressed text as well-structured code and select the best matching answer from multiple candidates through programming. We evaluate our QAaP framework on several time-sensitive question answering datasets and achieve decent improvement, up to 14.5% over strong baselines.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.787",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL": {
        "type": "INPROCEEDINGS",
        "key": "kothyari-etal-2023-crush4sql",
        "author": "Kothyari, Mayank and Dhingra, Dhruva and Sarawagi, Sunita and Chakrabarti, Soumen",
        "booktitle": "EMNLP2023",
        "title": "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Existing Text-to-SQL generators require the entire schema to be encoded with the user text. This is expensive or impractical for large databases with tens of thousands of columns. Standard dense retrieval techniques are inadequate for schema subsetting of a large structured database, where the correct semantics of retrieval demands that we rank sets of schema elements rather than individual documents. In response, we propose a two-stage process for effective coverage during retrieval. First, we use an LLM to hallucinate a minimal DB schema that it deems adequate to answer the query. We use the hallucinated schema to retrieve a subset of the actual schema, by composing the results from multiple dense retrievals. Remarkably, hallucination \u2014 generally considered a nuisance \u2014 turns out to be actually useful as a bridging mechanism. Since no existing benchmarks exist for schema subsetting on large databases, we introduce two benchmarks: (1) A semi-synthetic dataset of 4502 schema elements, by taking a union of schema on the well-known SPIDER dataset, and (2) A real-life benchmark called SocialDB sourced from an actual large data warehouse comprising of 17844 schema elements. We show that our method leads to significantly higher recall than SOTA retrieval-based augmentation methods.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.868",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "API-Assisted Code Generation for Question Answering on Varied Table Structures": {
        "type": "INPROCEEDINGS",
        "key": "cao-etal-2023-api",
        "author": "Cao, Yihan and Chen, Shuyi and Liu, Ryan and Wang, Zhiruo and Fried, Daniel",
        "booktitle": "EMNLP2023",
        "title": "API-Assisted Code Generation for Question Answering on Varied Table Structures",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures \u2014 relational, multi-table, and hierarchical matrix shapes \u2014 and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.897",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Prompting with Pseudo-Code Instructions": {
        "type": "INPROCEEDINGS",
        "key": "mishra-etal-2023-prompting",
        "author": "Mishra, Mayank and Kumar, Prince and Bhat, Riyaz and Murthy, Rudra and Contractor, Danish and Tamilselvam, Srikanth",
        "booktitle": "EMNLP2023",
        "title": "Prompting with Pseudo-Code Instructions",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models (LLM). Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, like pseudo-code. In this paper, we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models. We manually create a dataset of pseudo-code prompts for 132 different tasks spanning classification, QA, and generative language tasks, sourced from the Super-NaturalInstructions dataset. Using these prompts along with their counterparts in natural language, we study their performance on two LLM families - BLOOM, CodeGen. Our experiments show that using pseudo-code instructions leads to better results, with an average increase (absolute) of 7-16 points in F1 scores for classification tasks and an improvement (relative) of 12-38% in aggregate ROUGE-L scores across all tasks. We include detailed ablation studies which indicate that code comments, docstrings, and the structural clues encoded in pseudo-code all contribute towards the improvement in performance. To the best of our knowledge, our work is the first to demonstrate how pseudo-code prompts can be helpful in improving the performance of pre-trained LMs.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.939",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Exploring Distributional Shifts in Large Language Models for Code Analysis": {
        "type": "INPROCEEDINGS",
        "key": "arakelyan-etal-2023-exploring",
        "author": "Arakelyan, Shushan and Das, Rocktim and Mao, Yi and Ren, Xiang",
        "booktitle": "EMNLP2023",
        "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. We establish that samples from each new domain present all the models with a significant challenge of distribution shift. We study how established methods adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. Moreover, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain.",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.1013",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "MiniChain: A Small Library for Coding with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "rush-2023-minichain",
        "author": "Rush, Alexander",
        "booktitle": "EMNLP2023",
        "title": "MiniChain: A Small Library for Coding with Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Programming augmented by large language models (LLMs) opens up many new application areas, but also requires care. LLMs are accurate enough, on average, to replace core functionality, yet make basic mistakes that demonstrate a lack of robustness. An ecosystem of prompting tools, from intelligent agents to new programming languages, have emerged with different solutions for patching LLMs with other tools. In this work, we introduce MiniChain, an opinionated tool for LLM augmented programming, with the design goals of ease-of-use of prototyping, transparency through automatic visualization, and a minimalistic approach to advanced features. The MiniChain library provides core primitives for coding LLM calls, separating out prompt templates, and capturing program structure. The library includes demo implementations of the main applications papers in the area, including chat-bots, code generation, retrieval-based question answering, and complex information extraction. The library is open-source and available at https://github.com/srush/MiniChain, with code demos available at https://srush-minichain.hf.space/, and video demo at https://www.youtube.com/watch?v=VszZ1VnO7sk.",
        "labels": [
            "general coding task"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-demo.27",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Self-Edit: Fault-Aware Code Editor for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-self",
        "author": "Zhang, Kechi and Li, Zhuo and Li, Jia and Li, Ge and Jin, Zhi",
        "booktitle": "ACL2023",
        "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.",
        "labels": [
            "code generation",
            "program repair",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.45",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Making Language Models Better Reasoners with Step-Aware Verifier": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-making",
        "author": "Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu",
        "booktitle": "ACL2023",
        "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",
        "labels": [
            "agent design",
            "prompt strategy",
            "sampling and ranking"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.291",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Fact-Checking Complex Claims with Program-Guided Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "pan-etal-2023-fact",
        "author": "Pan, Liangming and Wu, Xiaobao and Lu, Xinyuan and Luu, Anh Tuan and Wang, William Yang and Kan, Min-Yen and Nakov, Preslav",
        "booktitle": "ACL2023",
        "title": "Fact-Checking Complex Claims with Program-Guided Reasoning",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/ProgramFC.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.386",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Large Language Models Meet NL2Code: A Survey": {
        "type": "INPROCEEDINGS",
        "key": "zan-etal-2023-large",
        "author": "Zan, Daoguang and Chen, Bei and Zhang, Fengji and Lu, Dianjie and Wu, Bingchao and Guan, Bei and Yongji, Wang and Lou, Jian-Guang",
        "booktitle": "ACL2023",
        "title": "Large Language Models Meet NL2Code: A Survey",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \u201cLarge Size, Premium Data, Expert Tuning\u201d. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.",
        "labels": [
            "survey",
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.411",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-codeie",
        "author": "Li, Peng and Sun, Tianxiang and Tang, Qiong and Yan, Hang and Wu, Yuanbin and Huang, Xuanjing and Qiu, Xipeng",
        "booktitle": "ACL2023",
        "title": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.855",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-xsemplr",
        "author": "Zhang, Yusen and Wang, Jun and Wang, Zhiguo and Zhang, Rui",
        "booktitle": "ACL2023",
        "title": "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (full dataset, few-shot, and zero-shot). Our experiments show that encoder-decoder models (mT5) achieve the highest performance compared with other popular models, and multilingual training can further improve the average performance. Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks. We also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training. Our dataset and code are available at https://github.com/psunlpgroup/XSemPLR.",
        "labels": [
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.887",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616271",
        "author": "Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming",
        "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616271",
        "doi": "10.1145/3611643.3616271",
        "abstract": "During Automated Program Repair (APR), it can be challenging&nbsp;to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models&nbsp;(LLMs) have been shown to be helpful \u201ccopilots\u201d in assisting developers with various coding tasks, and have also been directly&nbsp;applied for patch synthesis. However, most LLMs treat programs as&nbsp;sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This&nbsp;results in plenty of statically invalid generated patches, impeding&nbsp;the practicality of the technique. Therefore, we propose Repilot,&nbsp;a framework to further copilot the AI \u201ccopilots\u201d (i.e., LLMs) by&nbsp;synthesizing more valid patches during the repair process. Our key&nbsp;insight is that many LLMs produce outputs autoregressively (i.e.,&nbsp;token by token), resembling human writing programs, which can&nbsp;be significantly boosted and guided through a Completion Engine.&nbsp;Repilot synergistically synthesizes a candidate patch through the&nbsp;interaction between an LLM and a Completion Engine, which 1)&nbsp;prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the&nbsp;Completion Engine. Our evaluation on a subset of the widely-used&nbsp;Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50&nbsp;bugs, respectively, surpassing the best-performing baseline by 14&nbsp;and 16 bugs fixed. More&nbsp;importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given&nbsp;the same generation budget.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "172\u2013184",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Multilingual Code Co-evolution using Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616350",
        "author": "Zhang, Jiyang and Nie, Pengyu and Li, Junyi Jessy and Gligoric, Milos",
        "title": "Multilingual Code Co-evolution using Large Language Models",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616350",
        "doi": "10.1145/3611643.3616350",
        "abstract": "Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#). Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics. Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "695\u2013707",
        "numpages": "13",
        "labels": [
            "code generation",
            "program transformation",
            "code model",
            "code model training",
            "source code model"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Baldur: Whole-Proof Generation and Repair with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616243",
        "author": "First, Emily and Rabe, Markus N. and Ringer, Talia and Brun, Yuriy",
        "title": "Baldur: Whole-Proof Generation and Repair with Large Language Models",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616243",
        "doi": "10.1145/3611643.3616243",
        "abstract": "Formally verifying software is a highly desirable but labor-intensive task.  Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time and using that model to search through the space of possible proofs.  This paper introduces a new method to automate formal verification: We use large language models, trained on natural language and code and fine-tuned on proofs, to generate whole proofs at once.  We then demonstrate that a model fine-tuned to repair generated proofs further increasing proving power.  This paper:  (1) Demonstrates that whole-proof generation using transformers is possible and is as effective but more efficient than search-based techniques.  (2) Demonstrates that giving the learned model additional context, such as a prior failed proof attempt and the ensuing error message, results in proof repair that further improves automated proof generation.  (3) Establishes, together with prior work, a new state of the art for fully automated proof synthesis.  We reify our method in a prototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL theorems and their proofs,  empirically showing the effectiveness of whole-proof generation, repair, and added context. We also show that Baldur complements the state-of-the-art tool, Thor, by automatically generating proofs for an additional 8.7\\% of the theorems. Together, Baldur and Thor can prove 65.7\\% of the theorems fully automatically. This paper paves the way for new research into using large language models for automating formal verification.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1229\u20131241",
        "numpages": "13",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Grace: Language Models Meet Code Edits": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616253",
        "author": "Gupta, Priyanshu and Khare, Avishree and Bajpai, Yasharth and Chakraborty, Saikat and Gulwani, Sumit and Kanade, Aditya and Radhakrishna, Arjun and Soares, Gustavo and Tiwari, Ashish",
        "title": "Grace: Language Models Meet Code Edits",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616253",
        "doi": "10.1145/3611643.3616253",
        "abstract": "Developers spend a significant amount of time in editing code for a variety of reasons such as bug fixing or adding new features. Designing effective methods to predict code edits has been an active yet challenging area of research due to the diversity of code edits and the difficulty of capturing the developer intent. In this work, we address these challenges by endowing pre-trained large language models (LLMs) with the knowledge of relevant prior associated edits, which we call the Grace (Generation conditioned on Associated Code Edits) method. The generative capability of the LLMs helps address the diversity in code changes and conditioning code generation on prior edits helps capture the latent developer intent. We evaluate two well-known LLMs, codex and CodeT5, in zero-shot and fine-tuning settings respectively. In our experiments with two datasets, Grace boosts the performance of the LLMs significantly, enabling them to generate 29\\% and 54\\% more correctly edited code in top-1 suggestions relative to the current state-of-the-art symbolic and neural approaches, respectively.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1483\u20131495",
        "numpages": "13",
        "labels": [
            "code generation",
            "code completion",
            "code model",
            "code model training",
            "source code model"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "InferFix: End-to-End Program Repair with LLMs": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3613892",
        "author": "Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey",
        "title": "InferFix: End-to-End Program Repair with LLMs",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3613892",
        "doi": "10.1145/3611643.3613892",
        "abstract": "Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose\u202f: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever \u2013 transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator \u2013 an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6\\% for generating fixes in C# and 76.8\\% in Java. We discuss the deployment of  alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1646\u20131656",
        "numpages": "11",
        "labels": [
            "code generation",
            "program repair"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Assisting Static Analysis with Large Language Models: A ChatGPT Experiment": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3613078",
        "author": "Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun",
        "title": "Assisting Static Analysis with Large Language Models: A ChatGPT Experiment",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3613078",
        "doi": "10.1145/3611643.3613078",
        "abstract": "Recent advances of Large Language Models (LLMs), e.g., ChatGPT, exhibited strong capabilities of comprehending and responding to questions across a variety of domains. Surprisingly, ChatGPT even possesses a strong understanding of program code. In this paper, we investigate where and how LLMs can assist static analysis by asking appropriate questions. In particular, we target a specific bug-finding tool, which produces many false positives from the static analysis. In our evaluation, we find that these false positives can be effectively pruned by asking carefully constructed questions about function-level behaviors or function summaries. Specifically, with a pilot study of 20 false positives, we can successfully prune 8 out of 20 based on GPT-3.5, whereas GPT-4 had a near-perfect result of 16 out of 20, where the four failed ones are not currently considered/supported by our questions, e.g., involving concurrency. Additionally, it also identified one false negative case (a missed bug). We find LLMs a promising tool that can enable a more effective and efficient program analysis.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "2107\u20132111",
        "numpages": "5",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "LLM-Based Code Generation Method for Golang Compiler Testing": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3617850",
        "author": "Gu, Qiuhan",
        "title": "LLM-Based Code Generation Method for Golang Compiler Testing",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3617850",
        "doi": "10.1145/3611643.3617850",
        "abstract": "Modern optimizing compilers are among the most complex software systems humans build. One way to identify subtle compiler bugs is fuzzing. Both the quantity and the quality of testcases are crucial to the performance of fuzzing. Traditional testcase-generation methods, such as Csmith and YARPGen, have been proven successful at discovering compiler bugs. However, such generated testcases have limited coverage and quantity. In this paper, we present a code generation method for compiler testing based on LLM to maximize the quality and quantity of the generated code. In particular, to avoid undefined behavior and syntax errors in generated testcases, we design a filter strategy to clean the source code, preparing a high-quality dataset for the model training. Besides, we present a seed schedule strategy to improve code generation. We apply the method to test the Golang compiler and the result shows that our pipeline outperforms previous methods both qualitatively and quantitatively. It produces testcases with an average coverage of 3.38\\%, in contrast to the testcases generated by GoFuzz, which have an average coverage of 0.44\\%. Moreover, among all the generated testcases, only 2.79\\% exhibited syntax errors, and none displayed undefined behavior.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "2201\u20132203",
        "numpages": "3",
        "labels": [
            "program testing",
            "fuzzing",
            "compiler testing"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities": {
        "type": "article",
        "key": "10.1145/3664606",
        "author": "Ma, Wei and Liu, Shangqing and Zhao, Mengjie and Xie, Xiaofei and Wang, Wenhang and Hu, Qiang and Zhang, Jie and Liu, Yang",
        "title": "Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664606",
        "doi": "10.1145/3664606",
        "abstract": "Code models have made significant advancements in code intelligence by encoding knowledge about programming languages. While previous studies have explored the capabilities of these models in learning code syntax, there has been limited investigation on their ability to understand code semantics. Additionally, existing analyses assume that the number of edges between nodes at the abstract syntax tree&nbsp;(AST) is related to syntax distance, and also often require transforming the high-dimensional space of deep learning models to a low-dimensional one, which may introduce inaccuracies. To study how code models represent code syntax and semantics, we conduct a comprehensive analysis of seven code models, including four representative code pre-trained models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) and three large language models (LLMs) (StarCoder, CodeLlama and CodeT5+). We design four probing tasks to assess the models\u2019 capacities in learning both code syntax and semantics. These probing tasks reconstruct code syntax and semantics structures (AST, control dependence graph (CDG), data dependence graph (DDG), and control flow graph (CFG)) in the representation space. These structures are core concepts for code understanding. We also investigate the syntax token role in each token representation and the long dependency between the code tokens. Additionally, we analyze the distribution of attention weights related to code semantic structures. Through extensive analysis, our findings highlight the strengths and limitations of different code models in learning code syntax and semantics. The results demonstrate that these models excel in learning code syntax, successfully capturing the syntax relationships between tokens and the syntax roles of individual tokens. However, their performance in encoding code semantics varies. CodeT5 and CodeBERT demonstrate proficiency in capturing control and data dependencies, whereas UnixCoder shows weaker performance in this aspect. We do not observe LLMs generally performing much better than pre-trained models. The shallow layers of LLMs perform better than their deep layers. The investigation of attention weights reveals that different attention heads play distinct roles in encoding code semantics. Our research findings emphasize the need for further enhancements in code models to better learn code semantics. This study contributes to the understanding of code models\u2019 abilities in syntax and semantics analysis. Our findings provide guidance for future improvements in code models, facilitating their effective application in various code-related tasks.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "169",
        "numpages": "29",
        "labels": [
            "static analysis",
            "pointer analysis",
            "data-flow analysis",
            "empirical study"
        ],
        "venue": "TOSEM2024"
    },
    "Self-Planning Code Generation with Large Language Models": {
        "type": "article",
        "key": "10.1145/3672456",
        "author": "Jiang, Xue and Dong, Yihong and Wang, Lecheng and Fang, Zheng and Shang, Qiwei and Li, Ge and Jin, Zhi and Jiao, Wenpin",
        "title": "Self-Planning Code Generation with Large Language Models",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672456",
        "doi": "10.1145/3672456",
        "abstract": "Although large language models (LLMs) have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule solution steps prior to implementation. To this end, we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem-solving. This paper proposes a self-planning code generation approach with large language models, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, LLM plans out concise solution steps from the intent combined with few-shot prompting. Subsequently, in the implementation phase, the model generates code step by step, guided by the preceding solution steps. We conduct extensive experiments on various code-generation benchmarks across multiple programming languages. Experimental results show that self-planning code generation achieves a relative improvement of up to 25.4\\% in Pass@1 compared to direct code generation, and up to 11.9\\% compared to Chain-of-Thought of code generation. Moreover, our self-planning approach also enhances the quality of the generated code with respect to correctness, readability, and robustness, as assessed by humans.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "182",
        "numpages": "30",
        "labels": [
            "code generation",
            "program synthesis",
            "agent design",
            "planning",
            "empirical study"
        ],
        "venue": "TOSEM2024"
    },
    "Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Dataflows to Generate Dataflow Graphs in Dynamically Typed Code": {
        "type": "article",
        "key": "10.1145/3672458",
        "author": "Huang, Qing and Luo, Zhiwen and Xing, Zhenchang and Zeng, Jinshan and Chen, Jieshan and Xu, Xiwei and Chen, Yong",
        "title": "Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Dataflows to Generate Dataflow Graphs in Dynamically Typed Code",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672458",
        "doi": "10.1145/3672458",
        "abstract": "Dataflow graphs (DFGs) capture definitions (defs) and uses across program blocks, which is a fundamental program representation for program analysis, testing and maintenance. However, dynamically typed programming languages like Python present implicit dataflow issues that make it challenging to determine def-use flow information at compile time. Static analysis methods like Soot and WALA are inadequate for handling these issues, and manually enumerating comprehensive heuristic rules is impractical. Large pre-trained language models (LLMs) offer a potential solution, as they have powerful language understanding and pattern matching abilities, allowing them to predict implicit dataflow by analyzing code context and relationships between variables, functions, and statements in code. We propose leveraging LLMs\u2019 in-context learning ability to learn implicit rules and patterns from code representation and contextual information to solve implicit dataflow problems. To further enhance the accuracy of LLMs, we design a five-step chain of thought (CoT) and break it down into an Artificial Intelligence (AI) chain, with each step corresponding to a separate AI unit to generate accurate DFGs for Python code. Our approach\u2019s performance is thoroughly assessed, demonstrating the effectiveness of each AI unit in the AI Chain. Compared to static analysis, our method achieves 82\\% higher def coverage and 58\\% higher use coverage in DFG generation on implicit dataflow. We also prove the indispensability of each unit in the AI Chain. Overall, our approach offers a promising direction for building software engineering tools by utilizing foundation models, eliminating significant engineering and maintenance effort, but focusing on identifying problems for AI to solve.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "183",
        "numpages": "29",
        "labels": [
            "static analysis",
            "data-flow analysis"
        ],
        "venue": "TOSEM2024"
    },
    "LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models": {
        "type": "article",
        "key": "10.1145/3664812",
        "author": "Feng, Xiaoning and Han, Xiaohong and Chen, Simin and Yang, Wei",
        "title": "LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664812",
        "doi": "10.1145/3664812",
        "abstract": "Large Language Models (LLMs) have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of LLMs, which is of paramount importance due to often vast generation demands and real-time requirements, has surprisingly received little attention. In this article, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art LLMs. By analyzing the working mechanism and implementation of 20,543 public-accessible LLMs, we observe a fundamental property in LLMs that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of LLMs instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime-generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that LLMs would have to go through enough iterations to satisfy the pre-configured threshold. We present LLMEffiChecker, which can work under both white-box setting and black-box setting. In the white-box scenario, LLMEffiChecker develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level. In the black-box scenario, LLMEffiChecker employs a causal inference-based approach to find critical tokens and similarly applies three levels of imperceptible perturbation to them. Both the white-box and black-box settings effectively delay the appearance of EOS, compelling these inputs to reach the naturally unreachable threshold. To demonstrate the effectiveness of LLMEffiChecker, we conduct a systematic evaluation on nine publicly available LLMs: Google T5, AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT, and Salesforce CodeGen. Experimental results show that LLMEffiChecker can increase on average LLMs\u2019 response latency and energy consumption by 325\\% to 3,244\\% and 344\\% to 3,616\\%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by LLMEffiChecker significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "186",
        "numpages": "38",
        "labels": [
            "code model",
            "code model robustness"
        ],
        "venue": "TOSEM2024"
    },
    "Self-Collaboration Code Generation via ChatGPT": {
        "type": "article",
        "key": "10.1145/3672459",
        "author": "Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge",
        "title": "Self-Collaboration Code Generation via ChatGPT",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672459",
        "doi": "10.1145/3672459",
        "abstract": "Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct \u201cexperts,\u201d each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other\u2019s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development\u2019s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9\u201347.1\\% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "189",
        "numpages": "38",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "venue": "TOSEM2024"
    },
    "Risky Dynamic Typing-related Practices in Python: An Empirical Study": {
        "type": "article",
        "key": "10.1145/3649593",
        "author": "Chen, Zhifei and Chen, Lin and Yang, Yibiao and Feng, Qiong and Li, Xuansong and Song, Wei",
        "title": "Risky Dynamic Typing-related Practices in Python: An Empirical Study",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649593",
        "doi": "10.1145/3649593",
        "abstract": "Python\u2019s dynamic typing nature provides developers with powerful programming abstractions. However, many type-related bugs are accumulated in code bases of Python due to the misuse of dynamic typing. The goal of this article is to aid in the understanding of developers\u2019 high-risk practices toward dynamic typing and the early detection of type-related bugs. We first formulate the rules of six types of risky dynamic typing-related practices (type smells for short) in Python. We then develop a rule-based tool named RUPOR, which builds an accurate type base to detect type smells. Our evaluation shows that RUPOR outperforms the existing type smell detection techniques (including the Large Language Models\u2013based approaches, Mypy, and PYDYPE) on a benchmark of 900 Python methods. Based on RUPOR, we conduct an empirical study on 25 real-world projects. We find that type smells are significantly related to the occurrence of post-release faults. The fault-proneness prediction model built with type smell features slightly outperforms the model built without them. We also summarize the common patterns, including inserting type check to fix type smell bugs. These findings provide valuable insights for preventing and fixing type-related bugs in the programs written in dynamic-typed languages.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "140",
        "numpages": "35",
        "labels": [
            "static analysis",
            "type inference",
            "bug detection",
            "empirical study"
        ],
        "venue": "TOSEM2024"
    },
    "Statically Contextualizing Large Language Models with Typed Holes": {
        "type": "article",
        "key": "10.1145/3689728",
        "author": "Blinn, Andrew and Li, Xiang and Kim, June Hyung and Omar, Cyrus",
        "title": "Statically Contextualizing Large Language Models with Typed Holes",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689728",
        "doi": "10.1145/3689728",
        "abstract": "Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate code context, particularly when working with definitions that are neither in the training data nor near the cursor. This paper demonstrates that tighter integration with the type and binding structure of the programming language in use, as exposed by its language server, can help address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server is able to identify the type and typing context of the hole that the programmer is filling, with Hazel's total syntax and type error correction ensuring that a meaningful program sketch is available whenever the developer requests a completion. This allows the system to prompt the LLM with codebase-wide contextual information that is not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server, which provides error localization and error messages. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications with accompanying unit tests that have been written from scratch to avoid data contamination, and that can easily be ported to new languages because they do not have large external library dependencies. These applications serve as challenge problems due to their extensive reliance on application-specific data structures. Through an ablation study, we examine the impact of contextualization with type definitions, function headers, and errors messages, individually and in combination. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel, a low-resource language, we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource mainstream languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "288",
        "numpages": "31",
        "labels": [
            "code generation",
            "program synthesis",
            "benchmark",
            "empirical study"
        ],
        "venue": "OOPSLA2024"
    },
    "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs": {
        "type": "article",
        "key": "10.1145/3689735",
        "author": "Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Freeman, Anders and Anderson, Carolyn Jane and Feldman, Molly Q and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun",
        "title": "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689735",
        "doi": "10.1145/3689735",
        "abstract": "Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as building blocks for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming language. Code LLMs produce impressive results on high-resource programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages that have limited training data available (e.g., OCaml, Racket, and several others).                                                                                                                                                                                                                                                              This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach, called MultiPL-T, generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. MultiPL-T translates training data from high-resource languages into training data for low-resource languages in the following way. 1) We use a Code LLM to synthesize unit tests for commented code from a high-resource source language, filtering out faulty tests and code with low test coverage. 2) We use a Code LLM to translate the code from the high-resource source language to a target low-resource language. This gives us a corpus of candidate training data in the target language, but many of these translations are wrong. 3) We use a lightweight compiler to compile the test cases generated in (1) from the source language to the target language, which allows us to filter our obviously wrong translations. The result is a training corpus in the target low-resource language where all items have been validated with test cases. We apply this approach to generate tens of thousands of new, validated training items for five low-resource languages: Julia, Lua, OCaml, R, and Racket, using Python as the source high-resource language. Furthermore, we use an open Code LLM (StarCoderBase) with open training data (The Stack), which allows us to decontaminate benchmarks, train models without violating licenses, and run experiments that could not otherwise be done.                                                                                                                                                                                                                                                              Using datasets generated with MultiPL-T, we present fine-tuned versions of StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket that outperform other fine-tunes of these base models on the natural language to code task. We also present Racket fine-tunes for two very recent models, DeepSeek Coder and StarCoder2, to show that MultiPL-T continues to outperform other fine-tuning approaches for low-resource languages. The MultiPL-T approach is easy to apply to new languages, and is significantly more efficient and effective than alternatives such as training longer.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "295",
        "numpages": "32",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model training",
            "source code model"
        ],
        "venue": "OOPSLA2024"
    },
    "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models": {
        "type": "article",
        "key": "10.1145/3689736",
        "author": "Yang, Chenyuan and Deng, Yinlin and Lu, Runyu and Yao, Jiayi and Liu, Jiawei and Jabbarvand, Reyhaneh and Zhang, Lingming",
        "title": "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689736",
        "doi": "10.1145/3689736",
        "abstract": "Compiler correctness is crucial, as miscompilation can falsify program behaviors, leading to serious consequences over the software supply chain. In the literature, fuzzing has been extensively studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates test programs without sufficient understanding of internal compiler behaviors. As such, they often fail to construct test programs to exercise intricate optimizations. Meanwhile, traditional white-box techniques, such as symbolic execution, are computationally inapplicable to the giant codebase of compiler systems. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks and even have achieved state-of-the-art performance in black-box fuzzing. Nonetheless, guiding LLMs with compiler source-code information remains a missing piece of research in compiler testing. To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization, with a spotlight on detecting deep logic bugs in the emerging deep learning (DL) compilers. WhiteFox adopts a multi-agent framework: (i) an LLM-based analysis agent examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; (ii) an LLM-based generation agent produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are also used as feedback to further enhance the test generation prompt on the fly. Our evaluation on the three most popular DL compilers (i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows that WhiteFox can generate high-quality test programs to exercise deep optimizations requiring intricate conditions, practicing up to 8 times more optimizations than state-of-the-art fuzzers. To date, WhiteFox has found in total 101 bugs for the compilers under test, with 92 confirmed as previously unknown and 70 already fixed. Notably, WhiteFox has been recently acknowledged by the PyTorch team, and is in the process of being incorporated into its development workflow. Finally, beyond DL compilers, WhiteFox can also be adapted for compilers in different domains, such as LLVM, where WhiteFox has already found multiple bugs.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "296",
        "numpages": "27",
        "labels": [
            "program testing",
            "fuzzing",
            "compiler testing"
        ],
        "venue": "OOPSLA2024"
    },
    "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models": {
        "type": "article",
        "key": "10.1145/3689776",
        "author": "Li, Ningke and Li, Yuekang and Liu, Yi and Shi, Ling and Wang, Kailong and Wang, Haoyu",
        "title": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689776",
        "doi": "10.1145/3689776",
        "abstract": "Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations \u2014 coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges: 1) Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases. 2) Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations.    To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth.    Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7\\% to 59.8\\%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations.    To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "336",
        "numpages": "30",
        "labels": [
            "program testing",
            "fuzzing"
        ],
        "venue": "OOPSLA2024"
    },
    "Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach": {
        "type": "article",
        "key": "10.1145/3649828",
        "author": "Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun",
        "title": "Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA1",
        "url": "https://doi.org/10.1145/3649828",
        "doi": "10.1145/3649828",
        "abstract": "While static analysis is instrumental in uncovering software bugs, its precision in analyzing large and intricate codebases remains challenging. The emerging prowess of Large Language Models (LLMs) offers a promising avenue to address these complexities. In this paper, we present LLift, a pioneering framework that synergizes static analysis and LLMs, with a spotlight on identifying use-before-initialization (UBI) bugs within the Linux kernel. Drawing from our insights into variable usage conventions in Linux, we enhance path analysis using post-constraint guidance. This approach, combined with our methodically crafted procedures, empowers LLift to adeptly handle the challenges of bug-specific modeling, extensive codebases, and the unpredictable nature of LLMs. Our real-world evaluations identified four previously undiscovered UBI bugs in the mainstream Linux kernel, which the Linux community has acknowledged. This study reaffirms the potential of marrying static analysis with LLMs, setting a compelling direction for future research in this area.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "apr",
        "articleno": "111",
        "numpages": "26",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "venue": "OOPSLA2024"
    },
    "PyDex: Repairing Bugs in Introductory Python Assignments using LLMs": {
        "type": "article",
        "key": "10.1145/3649850",
        "author": "Zhang, Jialu and Cambronero, Jos\\'{e} Pablo and Gulwani, Sumit and Le, Vu and Piskac, Ruzica and Soares, Gustavo and Verbruggen, Gust",
        "title": "PyDex: Repairing Bugs in Introductory Python Assignments using LLMs",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA1",
        "url": "https://doi.org/10.1145/3649850",
        "doi": "10.1145/3649850",
        "abstract": "Students often make mistakes in their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex (a version of GPT), to build an APR system -- PyDex -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate PyDex on 286 real student programs and compare to three baselines, including one that combines a state-of-the-art Python syntax repair engine, BIFI, and a state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that PyDex can fix more programs and produce smaller patches on average.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "apr",
        "articleno": "133",
        "numpages": "25",
        "labels": [
            "code generation",
            "program repair"
        ],
        "venue": "OOPSLA2024"
    },
    "TransLLaMa: LLM-based Simultaneous Translation System": {
        "type": "INPROCEEDINGS",
        "key": "koshkin-etal-2024-transllama",
        "author": "Koshkin, Roman and Sudoh, Katsuhito and Nakamura, Satoshi",
        "booktitle": "EMNLP2024",
        "title": "TransLLaMa: LLM-based Simultaneous Translation System",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers. This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special \u201cwait\u201d token. This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems.",
        "labels": [
            "code generation",
            "program transformation",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.27",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-introducing",
        "author": "Zhang, Shuoming and Zhao, Jiacheng and Xia, Chunwei and Wang, Zheng and Chen, Yunji and Cui, Huimin",
        "booktitle": "EMNLP2024",
        "title": "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Compilers are complex software containing millions of lines of code, taking years to develop. This paper investigates to what extent Large Language Models (LLMs) can replace hand-crafted compilers in translating high-level programming languages to machine instructions, using C to x86 assembly as a case study. We identify two challenges of using LLMs for code translation and introduce two novel data pre-processing techniques to address the challenges: numerical value conversion and training data resampling. While only using a 13B model, our approach achieves a behavioral accuracy of over 91%, outperforming the much larger GPT-4 Turbo model by over 50%. Our results are encouraging, showing that LLMs have the potential to transform how compilation tools are constructed.",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.55",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "EvoR: Evolving Retrieval for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "su-etal-2024-evor",
        "author": "Su, Hongjin and Jiang, Shuyang and Lai, Yuhang and Wu, Haoyuan and Shi, Boao and Liu, Che and Liu, Qian and Yu, Tao",
        "booktitle": "EMNLP2024",
        "title": "EvoR: Evolving Retrieval for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently the retrieval-augmented generation (RAG) has been successfully applied in code generation. However, existing pipelines for retrieval-augmented code generation (RACG) employ static knowledge bases with a single source, limiting the adaptation capabilities of Large Language Models (LLMs) to domains they have insufficient knowledge of. In this work, we develop a novel pipeline, EVOR, that employs the synchronous evolution of both queries and diverse knowledge bases. On two realistic settings where the external knowledge is required to solve code generation tasks, we compile four new datasets associated with frequently updated libraries and long-tail programming languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR achieves two to four times of execution accuracy compared to other methods such as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We demonstrate that EVOR is flexible and can be easily combined with them to achieve further improvement. Further analysis reveals that EVOR benefits from the synchronous evolution of queries and documents and the diverse information sources in the knowledge base. We hope that our studies will inspire more insights into the design of advanced RACG pipelines in future research.",
        "labels": [
            "code generation",
            "code completion",
            "source code model",
            "agent design",
            "prompt strategy",
            "retrieval-augmented generation"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.143",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Sanitizing Large Language Models in Bug Detection with Data-Flow": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-sanitizing",
        "author": "Wang, Chengpeng and Zhang, Wuqi and Su, Zian and Xu, Xiangzhe and Zhang, Xiangyu",
        "booktitle": "EMNLP2024",
        "title": "Sanitizing Large Language Models in Bug Detection with Data-Flow",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) show potential in code reasoning tasks, facilitating the customization of detecting bugs in software development. However, the hallucination effect can significantly compromise the reliability of bug reports. This work formulates a new schema of bug detection and presents a novel sanitization technique that detects false positives for hallucination mitigation. Our key idea is to enforce LLMs to emit data-flow paths in few-shot chain-of-thought prompting and validate them via the program-property decomposition. Specifically, we dissect data-flow paths into basic properties upon concise code snippets and leverage parsing-based analysis and LLMs for validation. Our approach averagely achieves 91.03% precision and 74.00% recall upon synthetic benchmarks and boosts the precision by 21.99% with the sanitization. The evaluation upon real-world Android malware applications also demonstrates the superiority over an industrial analyzer, surpassing the precision and recall by 15.36% and 3.61%, respectively.",
        "labels": [
            "static analysis",
            "bug detection",
            "data-flow analysis"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.217",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Stanceformer: Target-Aware Transformer for Stance Detection": {
        "type": "INPROCEEDINGS",
        "key": "garg-caragea-2024-stanceformer",
        "author": "Garg, Krishna and Caragea, Cornelia",
        "booktitle": "EMNLP2024",
        "title": "Stanceformer: Target-Aware Transformer for Stance Detection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The task of Stance Detection involves discerning the stance expressed in a text towards a specific subject or target. Prior works have relied on existing transformer models that lack the capability to prioritize targets effectively. Consequently, these models yield similar performance regardless of whether we utilize or disregard target information, undermining the task\u2019s significance. To address this challenge, we introduce Stanceformer, a target-aware transformer model that incorporates enhanced attention towards the targets during both training and inference. Specifically, we design a Target Awareness matrix that increases the self-attention scores assigned to the targets. We demonstrate the efficacy of the Stanceformer with various BERT-based models, including state-of-the-art models and Large Language Models (LLMs), and evaluate its performance across three stance detection datasets, alongside a zero-shot dataset. Our approach Stanceformer not only provides superior performance but also generalizes even to other domains, such as Aspect-based Sentiment Analysis. We make the code publicly available.",
        "labels": [
            "static analysis",
            "bug detection",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.286",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-defending-large",
        "author": "Zhao, Wei and Li, Zhe and Li, Yige and Zhang, Ye and Sun, Jun",
        "booktitle": "EMNLP2024",
        "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs respond to harmful prompts and propose a novel defense method termed Layer-specific Editing (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical safety layers exist among the early layers of LLMs. We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from identified toxic layers can significantly improve the alignment of LLMs against jailbreak attacks. Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. Our code is available at https://github.com/ledllm/ledllm.",
        "labels": [
            "code model",
            "code model security"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.293",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement": {
        "type": "INPROCEEDINGS",
        "key": "feng-etal-2024-self",
        "author": "Feng, Yunlong and Teng, Dechuan and Xu, Yang and Mu, Honglin and Xu, Xiao and Qin, Libo and Zhu, Qingfu and Che, Wanxiang",
        "booktitle": "EMNLP2024",
        "title": "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decompilation transforms compiled code back into a high-level programming language for analysis when source code is unavailable. Previous work has primarily focused on enhancing decompilation performance by increasing the scale of model parameters or training data for pre-training. Based on the characteristics of the decompilation task, we propose two methods: (1) Without fine-tuning, the Self-Constructed Context Decompilation (sc\u00b2dec) method recompiles the LLM\u2019s decompilation results to construct pairs for in-context learning, helping the model improve decompilation performance. (2) Fine-grained Alignment Enhancement (FAE), which meticulously aligns assembly code with source code at the statement level by leveraging debugging information, is employed during the fine-tuning phase to achieve further improvements in decompilation. By integrating these two methods, we achieved a Re-Executability performance improvement of approximately 3.90% on the Decompile-Eval benchmark, establishing a new state-of-the-art performance of 52.41%. The code, data, and models are available at https://github.com/AlongWY/sccdec.",
        "labels": [
            "static analysis",
            "program decompilation",
            "code model",
            "code model training",
            "binary code model",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.385",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "cheng-etal-2024-autodetect",
        "author": "Cheng, Jiale and Lu, Yida and Gu, Xiaotao and Ke, Pei and Liu, Xiao and Dong, Yuxiao and Wang, Hongning and Tang, Jie and Huang, Minlie",
        "booktitle": "EMNLP2024",
        "title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction-following or coding tasks.As these unexpected errors could lead to severe consequences in practical deployments, it is crucial to investigate the limitations within LLMs systematically.Traditional benchmarking approaches cannot thoroughly pinpoint specific model deficiencies, while manual inspections are costly and not scalable. In this paper, we introduce a unified framework, AutoDetect, to automatically expose weaknesses in LLMs across various tasks. Inspired by the educational assessment process that measures students\u2019 learning outcomes, AutoDetect consists of three LLM-powered agents: Examiner, Questioner, and Assessor.The collaboration among these three agents is designed to realize comprehensive and in-depth weakness identification. Our framework demonstrates significant success in uncovering flaws, with an identification success rate exceeding 30% in prominent models such as ChatGPT and Claude.More importantly, these identified weaknesses can guide specific model improvements, proving more effective than untargeted data augmentation methods like Self-Instruct. Our approach has led to substantial enhancements in popular LLMs, including the Llama series and Mistral-7b, boosting their performance by over 10% across several benchmarks.Code and data are publicly available at https://github.com/thu-coai/AutoDetect.",
        "labels": [
            "code model",
            "code model security",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.397",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code": {
        "type": "INPROCEEDINGS",
        "key": "guan-etal-2024-codeip",
        "author": "Guan, Batu and Wan, Yao and Bi, Zhangqian and Wang, Zheng and Zhang, Hongyu and Zhou, Pan and Sun, Lichao",
        "booktitle": "EMNLP2024",
        "title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress in code generation. It now becomes crucial to identify whether the code is AI-generated and to determine the specific model used, particularly for purposes such as protecting Intellectual Property (IP) in industry and preventing cheating in programming exercises. To this end, several attempts have been made to insert watermarks into machine-generated code. However, existing approaches are limited to inserting only a single bit of information. In this paper, we introduce CodeIP, a novel multi-bit watermarking technique that embeds additional information to preserve crucial provenance details, such as the vendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation. Furthermore, to ensure the syntactical correctness of the generated code, we propose constraining the sampling process for predicting the next token by training a type predictor. Experiments conducted on a real-world dataset across five programming languages demonstrate the effectiveness of CodeIP in watermarking LLMs for code generation while maintaining the syntactical correctness of code.",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model security"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.541",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging": {
        "type": "INPROCEEDINGS",
        "key": "kargupta-etal-2024-instruct",
        "author": "Kargupta, Priyanka and Agarwal, Ishika and Tur, Dilek Hakkani and Han, Jiawei",
        "booktitle": "EMNLP2024",
        "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student\u2019s conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes\u2013 all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct\u2019s state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct\u2019s ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning.",
        "labels": [
            "program testing",
            "debugging",
            "agent design",
            "planning"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.553",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Can LLMs Reason in the Wild with Programs?": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2024-llms",
        "author": "Yang, Yuan and Xiong, Siheng and Payani, Ali and Shareghi, Ehsan and Fekri, Faramarz",
        "booktitle": "EMNLP2024",
        "title": "Can LLMs Reason in the Wild with Programs?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown superior capability to solve reasoning problems with programs. While being a promising direction, most of such frameworks are trained and evaluated in settings with a prior knowledge of task requirements. However, as LLMs become more capable, it is necessary to assess their reasoning abilities in more realistic scenarios where many real-world problems are open-ended with ambiguous scope, and often require multiple formalisms to solve. To investigate this, we introduce the task of reasoning in the wild, where an LLM is tasked to solve a reasoning problem of unknown type by identifying the sub-problems and their corresponding formalisms, and writing a program to solve each sub-problem, guided by a tactic. We create a large tactic-guided trajectory dataset containing detailed solutions to a diverse set of reasoning problems, ranging from well-defined single-form reasoning (e.g., math, logic), to ambiguous and hybrid ones (e.g., commonsense, combined math and logic). This allows us to test various aspects of LLMs reasoning at the fine-grained level such as the selection and execution of tactics, and the tendency to take undesired shortcuts. In experiments, we highlight that existing LLMs fail significantly on problems with ambiguous and mixed scope, revealing critical limitations and overfitting issues (e.g. accuracy on GSM8K drops by at least 50%). We further show the potential of finetuning a local LLM on the tactic-guided trajectories in achieving better performance. Project repo is available at https://github.com/gblackout/Reason-in-the-Wild.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.573",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Rethinking Code Refinement: Learning to Judge Code Efficiency": {
        "type": "INPROCEEDINGS",
        "key": "seo-etal-2024-rethinking",
        "author": "Seo, Minju and Baek, Jinheon and Hwang, Sung Ju",
        "booktitle": "EMNLP2024",
        "title": "Rethinking Code Refinement: Learning to Judge Code Efficiency",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating codes. Due to these capabilities, many recent methods are proposed to automatically refine the codes with LLMs. However, we should rethink that the refined codes (from LLMs and even humans) are not always more efficient than their original versions. On the other hand, running two different versions of codes and comparing them every time is not ideal and time-consuming. Therefore, in this work, we propose a novel method based on the code language model that is trained to judge the efficiency between two different codes (generated across humans and machines) by either classifying the superior one or predicting the relative improvement. We validate our method on multiple programming languages with multiple refinement steps, demonstrating that the proposed method can effectively distinguish between more and less efficient versions of code.",
        "labels": [
            "code generation",
            "program transformation",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.645",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Revisiting the Impact of Pursuing Modularity for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "kang-etal-2024-revisiting",
        "author": "Kang, Deokyeong and Seo, KiJung and Kim, Taeuk",
        "booktitle": "EMNLP2024",
        "title": "Revisiting the Impact of Pursuing Modularity for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Modular programming, which aims to construct the final program by integrating smaller, independent building blocks, has been regarded as a desirable practice in software development. However, with the rise of recent code generation agents built upon large language models (LLMs), a question emerges: is this traditional practice equally effective for these new tools? In this work, we assess the impact of modularity in code generation by introducing a novel metric for its quantitative measurement. Surprisingly, unlike conventional wisdom on the topic, we find that modularity is not a core factor for improving the performance of code generation models. We also explore potential explanations for why LLMs do not exhibit a preference for modular code compared to non-modular code.",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.676",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation": {
        "type": "INPROCEEDINGS",
        "key": "shen-etal-2024-enhancing",
        "author": "Shen, Zizhuo and Shao, Yanqiu and Li, Wei",
        "booktitle": "EMNLP2024",
        "title": "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Due to the high complexity of Discourse Dependency Parsing (DDP) tasks, their existing annotation resources are relatively scarce compared to other NLP tasks, and different DDP tasks also have significant differences in annotation schema. These issues have led to the dilemma of low resources for DDP tasks. Thanks to the powerful capabilities of Large Language Models (LLMs) in cross-task learning, we can use LLMs to model dependency parsing under different annotation schema in an unified manner, in order to alleviate the dilemma of low resources for DDP tasks. However, enabling LLMs to deeply comprehend dependency parsing tasks is a challenge that remains underexplored. Inspired by the application of code-based methods in complex tasks, we propose a code-based unified dependency parsing method. We treat the process of dependency parsing as a search process of dependency paths and use code to represent this search process. Furthermore, we use a curriculum-learning based instruction tuning strategy for joint training of multiple dependency parsing tasks. The experimental results show that our proposed code-based DDP system has achieved good performance on two Chinese DDP tasks (especially significant improvement on the DDP task with relatively less training data).",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.729",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "On Leakage of Code Generation Evaluation Datasets": {
        "type": "INPROCEEDINGS",
        "key": "matton-etal-2024-leakage",
        "author": "Matton, Alexandre and Sherborne, Tom and Aumiller, Dennis and Tommasone, Elena and Alizadeh, Milad and He, Jingyi and Ma, Raymond and Voisin, Maxime and Gilsenan-McMahon, Ellen and Gall\u00e9, Matthias",
        "booktitle": "EMNLP2024",
        "title": "On Leakage of Code Generation Evaluation Datasets",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we consider contamination by code generation test sets, in particular in their use in modern large language models.We discuss three possible sources of such contamination and show findings supporting each of them: (i) direct data leakage, (ii) indirect data leakage through the use of synthetic data and (iii) overfitting to evaluation sets during model selection.To address this, we release Less Basic Python Problems (LBPP): an uncontaminated new benchmark of 161 prompts with their associated Python solutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp",
        "labels": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.772",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "AI-Assisted Code Authoring at Scale: Fine-Tuning, Deploying, and Mixed Methods Evaluation": {
        "type": "article",
        "key": "10.1145/3643774",
        "author": "Murali, Vijayaraghavan and Maddila, Chandra and Ahmad, Imad and Bolin, Michael and Cheng, Daniel and Ghorbani, Negar and Fernandez, Renuka and Nagappan, Nachiappan and Rigby, Peter C.",
        "title": "AI-Assisted Code Authoring at Scale: Fine-Tuning, Deploying, and Mixed Methods Evaluation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643774",
        "doi": "10.1145/3643774",
        "abstract": "Generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 9 programming languages and several coding surfaces. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges.        To release a LLM model at this scale, we needed to first ensure that it is sufficiently accurate. In a random sample of 20K source code files, depending on the language, we are able to reproduce hidden lines between 40\\% and 58\\% of the time, an improvement of 1.4\\texttimes{} and 4.1\\texttimes{} over a model trained only on public data.        We gradually rolled CodeCompose out to developers. At the time of this writing, 16K developers have used it with 8\\% of their code coming directly from CodeCompose.        To triangulate our numerical findings, we conduct a thematic analysis on the feedback from 70 developers. We find that 91.5\\% of the feedback is positive, with the most common themes being discovering APIs, dealing with boilerplate code, and accelerating coding. Meta continues to integrate this feedback into CodeCompose.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "48",
        "numpages": "20",
        "labels": [
            "code generation",
            "code model",
            "code model training",
            "source code model"
        ],
        "venue": "FSE2024"
    },
    "Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models": {
        "type": "article",
        "key": "10.1145/3643776",
        "author": "Zhang, Zejun and Xing, Zhenchang and Ren, Xiaoxue and Lu, Qinghua and Xu, Xiwei",
        "title": "Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643776",
        "doi": "10.1145/3643776",
        "abstract": "Pythonic idioms are highly valued and widely used in the Python programming community. However, many  Python users find it challenging to use Pythonic idioms. Adopting rule-based approach or LLM-only approach is not sufficient to overcome three persistent challenges of code idiomatization including code miss, wrong detection and wrong refactoring. Motivated by the determinism of rules and adaptability of LLMs, we propose a hybrid approach consisting of three modules. We not only write prompts to instruct LLMs to complete tasks, but we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks. The ARIs are Python code generated by prompting LLMs to generate code. We first construct a knowledge module with three elements including ASTscenario, ASTcomponent and Condition, and prompt LLMs to generate Python code for incorporation into an ARI library for subsequent use. After that, for any syntax-error-free Python code, we invoke ARIs from the ARI library to extract ASTcomponent from the ASTscenario, and then filter out ASTcomponent that does not meet the condition. Finally, we design prompts to instruct LLMs to abstract and idiomatize code, and then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code. Next, we conduct a comprehensive evaluation of our approach, RIdiom, and Prompt-LLM on nine established Pythonic idioms in RIdiom. Our approach exhibits superior accuracy, F1 score, and recall, while maintaining precision levels comparable to RIdiom, all of which consistently exceed or come close to 90\\% for each metric of each idiom. Lastly, we extend our evaluation to encompass four new Pythonic idioms. Our approach consistently outperforms Prompt-LLM, achieving metrics with values consistently exceeding 90\\% for accuracy, F1-score, precision, and recall.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "50",
        "numpages": "22",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "venue": "FSE2024"
    },
    "Can GPT-4 Replicate Empirical Software Engineering Research?": {
        "type": "article",
        "key": "10.1145/3660767",
        "author": "Liang, Jenny T. and Badea, Carmen and Bird, Christian and DeLine, Robert and Ford, Denae and Forsgren, Nicole and Zimmermann, Thomas",
        "title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660767",
        "doi": "10.1145/3660767",
        "abstract": "Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners could benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help replicate and thus democratize empirical software engineering research.  In this paper, we examine GPT-4\u2019s abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodologies, as well as their ability to plan and generate code for analysis pipelines on seven empirical software engineering papers. We perform a user study with 14 participants with software engineering research expertise, who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module specifications) from the papers. We find that GPT-4 is able to surface correct assumptions, but struggles to generate ones that apply common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering knowledge. Our findings have implications for leveraging LLMs for software engineering research as well as practitioner data scientists in software teams.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "60",
        "numpages": "24",
        "labels": [
            "empirical study",
            "agent design"
        ],
        "venue": "FSE2024"
    },
    "SimLLM: Calculating Semantic Similarity in Code Summaries using a Large Language Model-Based Approach": {
        "type": "article",
        "key": "10.1145/3660769",
        "author": "Jin, Xin and Lin, Zhiqiang",
        "title": "SimLLM: Calculating Semantic Similarity in Code Summaries using a Large Language Model-Based Approach",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660769",
        "doi": "10.1145/3660769",
        "abstract": "Code summaries are pivotal in software engineering, serving to improve code readability, maintainability, and collaboration. While recent advancements in Large Language Models (LLMs) have opened new avenues for automatic code summarization, existing metrics for evaluating summary quality, such as BLEU and BERTScore, have notable limitations. Specifically, these existing metrics either fail to capture the nuances of semantic meaning in summaries or are further limited in understanding domain-specific terminologies and expressions prevalent in code summaries. In this paper, we present SimLLM, a novel LLM-based approach designed to more precisely evaluate the semantic similarity of code summaries. Built upon an autoregressive LLM using a specialized pretraining task on permutated inputs and a pooling-based pairwise similarity measure, SimLLM overcomes the shortcomings of existing metrics. Our empirical evaluations demonstrate that SimLLM not only outperforms existing metrics but also shows a significantly high correlation with human ratings.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "62",
        "numpages": "24",
        "labels": [
            "static analysis",
            "code summarization"
        ],
        "venue": "FSE2024"
    },
    "A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization": {
        "type": "article",
        "key": "10.1145/3660771",
        "author": "Kang, Sungmin and An, Gabin and Yoo, Shin",
        "title": "A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660771",
        "doi": "10.1145/3660771",
        "abstract": "Fault Localization (FL), in which a developer seeks to identify which part of the code is malfunctioning and needs to be fixed, is a recurring challenge in debugging. To reduce developer burden, many automated FL techniques have been proposed. However, prior work has noted that existing techniques fail to provide rationales for the suggested locations, hindering developer adoption of these techniques. With this in mind, we propose AutoFL, a Large Language Model (LLM)-based FL technique that generates an explanation of the bug along with a suggested fault location. AutoFL prompts an LLM to use function calls to navigate a repository, so that it can effectively localize faults over a large software repository and overcome the limit of the LLM context length. Extensive experiments on 798 real-world bugs in Java and Python reveal AutoFL improves method-level acc@1 by up to 233.3\\% over baselines. Furthermore, developers were interviewed on their impression of AutoFL-generated explanations, showing that developers generally liked the natural language explanations of AutoFL, and that they preferred reading a few, high-quality explanations instead of many.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "64",
        "numpages": "23",
        "labels": [
            "program testing",
            "debugging"
        ],
        "venue": "FSE2024"
    },
    "Exploring and Unleashing the Power of Large Language Models in Automated Code Translation": {
        "type": "article",
        "key": "10.1145/3660778",
        "author": "Yang, Zhen and Liu, Fang and Yu, Zhongxing and Keung, Jacky Wai and Li, Jia and Liu, Shuo and Hong, Yifan and Ma, Xiaoxue and Jin, Zhi and Li, Ge",
        "title": "Exploring and Unleashing the Power of Large Language Models in Automated Code Translation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660778",
        "doi": "10.1145/3660778",
        "abstract": "Code translation tools, namely transpilers, are developed for automatic source-to-source translation. Latest learning-based transpilers have shown impressive enhancement against rule-based counterparts in both translation accuracy and readability, owing to their task-specific pre-training on extensive monolingual corpora. Nevertheless, their current performance still remains unsatisfactory for practical deployment, and the associated training resources are also prohibitively expensive. Large Language Models (LLMs), pre-trained on huge amounts of human-written code/text, have shown remarkable performance in many code intelligence tasks due to their powerful generality, even without task-specific re-training/fine-tuning. Thus, LLMs can potentially circumvent the above limitations, but they have not been exhaustively explored yet. This paper investigates diverse LLMs and learning-based transpilers for automated code translation tasks, finding that: although certain LLMs have outperformed current transpilers, they still have some accuracy issues, where most of the failures are induced by a lack of comprehension of source programs (38.51\\%), missing clear instructions on I/O types in translation (14.94\\%), and ignoring discrepancies between source and target programs (41.38\\%).  Enlightened by the above findings, we further propose UniTrans, a Unified code Translation framework, applicable to various LLMs, for unleashing their power in this field. Specifically, UniTrans first crafts a series of test cases for target programs with the assistance of source programs. Next, it harnesses the above auto-generated test cases to augment the code translation and then evaluate their correctness via execution. Afterward, UniTrans further (iteratively) repairs incorrectly translated programs prompted by test case execution results. Extensive experiments are conducted on six settings of translation datasets between Python, Java, and C++. Three recent LLMs of diverse sizes, including GPT-3.5 and LLaMA-13B/7B, are tested with UniTrans, and all achieve substantial improvements in terms of computational accuracy and exact match accuracy among almost all translation settings, showing the universal effectiveness of UniTrans in practice.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "71",
        "numpages": "24",
        "labels": [
            "code generation",
            "program transformation",
            "code model",
            "code model training",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Evaluating and Improving ChatGPT for Unit Test Generation": {
        "type": "article",
        "key": "10.1145/3660783",
        "author": "Yuan, Zhiqiang and Liu, Mingwei and Ding, Shiji and Wang, Kaixin and Chen, Yixuan and Peng, Xin and Lou, Yiling",
        "title": "Evaluating and Improving ChatGPT for Unit Test Generation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660783",
        "doi": "10.1145/3660783",
        "abstract": "Unit testing plays an essential role in detecting bugs in functionally-discrete program units (e.g., methods). Manually writing high-quality unit tests is time-consuming and laborious. Although the traditional techniques are able to generate tests with reasonable coverage, they are shown to exhibit low readability and still cannot be directly adopted by developers in practice. Recent work has shown the large potential of large language models (LLMs) in unit test generation. By being pre-trained on a massive developer-written code corpus, the models are capable of generating more human-like and meaningful test code.                                                                                                In this work, we perform the first empirical study to evaluate the capability of ChatGPT (i.e., one of the most representative LLMs with outstanding performance in code generation and comprehension) in unit test generation. In particular, we conduct both a quantitative analysis and a user study to systematically investigate the quality of its generated tests in terms of correctness, sufficiency, readability, and usability. We find that the tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures (mostly caused by incorrect assertions); but the passing tests generated by ChatGPT almost resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved.                                                                                                Inspired by our findings above, we further propose ChatTester, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTester incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTester by generating 34.3\\% more compilable tests and 18.7\\% more tests with correct assertions than the default ChatGPT. In addition to ChatGPT, we further investigate the generalization capabilities of ChatTester by applying it to two recent open-source LLMs (i.e., CodeLLama-Instruct and CodeFuse) and our results show that ChatTester can also improve the quality of tests generated by these LLMs.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "76",
        "numpages": "24",
        "labels": [
            "program testing",
            "unit testing",
            "empirical study",
            "code generation"
        ],
        "venue": "FSE2024"
    },
    "Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice": {
        "type": "article",
        "key": "10.1145/3660788",
        "author": "Khojah, Ranim and Mohamad, Mazen and Leitner, Philipp and de Oliveira Neto, Francisco Gomes",
        "title": "Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660788",
        "doi": "10.1145/3660788",
        "abstract": "Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering. Currently, there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry. We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey). We find that rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms. We also propose a theoretical framework for how the (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust). We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "81",
        "numpages": "22",
        "labels": [
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?": {
        "type": "article",
        "key": "10.1145/3660791",
        "author": "Endres, Madeline and Fakhoury, Sarah and Chakraborty, Saikat and Lahiri, Shuvendu K.",
        "title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660791",
        "doi": "10.1145/3660791",
        "abstract": "Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a program\u2019s intent. However, there is typically no guarantee that a program\u2019s implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language, which makes natural language intent challenging to check programmatically. The \u201cemergent abilities\u201d of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice.     In this paper, we describe nl2postcondition, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions.   We introduce and validate metrics to measure and compare different nl2postcondition approaches, using the correctness and discriminative power of generated postconditions.   We then use qualitative and quantitative methods to assess the quality of nl2postcondition postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that  via LLMs has the potential to be helpful in practice;  generated postconditions were able to catch 64 real-world historical bugs from Defects4J.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "84",
        "numpages": "24",
        "labels": [
            "static analysis",
            "specification inference",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "An Empirical Study on Code Review Activity Prediction and Its Impact in Practice": {
        "type": "article",
        "key": "10.1145/3660806",
        "author": "Olewicki, Doriane and Habchi, Sarra and Adams, Bram",
        "title": "An Empirical Study on Code Review Activity Prediction and Its Impact in Practice",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660806",
        "doi": "10.1145/3660806",
        "abstract": "During code reviews, an essential step in software quality assurance, reviewers have the difficult task of understanding and evaluating code changes to validate their quality and prevent introducing faults to the codebase. This is a tedious process where the effort needed is highly dependent on the code submitted, as well as the author\u2019s and the reviewer\u2019s experience, leading to median wait times for review feedback of 15-64 hours. Through an initial user study carried with 29 experts, we found that re-ordering the files changed by a patch within the review environment has potential to improve review quality, as more comments are written (+23\\%), and participants\u2019 file-level hot-spot precision and recall increases to 53\\% (+13\\%) and 28\\% (+8\\%), respectively, compared to the alphanumeric ordering. Hence, this paper aims to help code reviewers by predicting which files in a submitted patch need to be (1) commented, (2) revised, or (3) are hot-spots (commented or revised). To predict these tasks, we evaluate two different types of text embeddings (i.e., Bag-of-Words and Large Language Models encoding) and review process features (i.e., code size-based and history-based features). Our empirical study on three open-source and two industrial datasets shows that combining the code embedding and review process features leads to better results than the state-of-the-art approach. For all tasks, F1-scores (median of 40-62\\%) are significantly better than the state-of-the-art (from +1 to +9\\%).",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "99",
        "numpages": "23",
        "labels": [
            "software maintenance and deployment",
            "code review",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?": {
        "type": "article",
        "key": "10.1145/3660807",
        "author": "Kou, Bonan and Chen, Shengmai and Wang, Zhijie and Ma, Lei and Zhang, Tianyi",
        "title": "Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660807",
        "doi": "10.1145/3660807",
        "abstract": "Large Language Models (LLMs) have recently been widely used for code generation. Due to the complexity and opacity of LLMs, little is known about how these models generate code. We made the first attempt to bridge this knowledge gap by investigating whether LLMs attend to the same parts of a task description as human programmers during code generation. An analysis of six LLMs, including GPT-4, on two popular code generation benchmarks revealed a consistent misalignment between LLMs' and programmers' attention. We manually analyzed 211 incorrect code snippets and found five attention patterns that can be used to explain many code generation errors. Finally, a user study showed that model attention computed by a perturbation-based method is often favored by human programmers. Our findings highlight the need for human-aligned LLMs for better interpretability and programmer trust.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "100",
        "numpages": "24",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Mining Action Rules for Defect Reduction Planning": {
        "type": "article",
        "key": "10.1145/3660809",
        "author": "Oueslati, Khouloud and Laberge, Gabriel and Lamothe, Maxime and Khomh, Foutse",
        "title": "Mining Action Rules for Defect Reduction Planning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660809",
        "doi": "10.1145/3660809",
        "abstract": "Defect reduction planning plays a vital role in enhancing software quality and minimizing software maintenance costs. By training a black box machine learning model and \u201cexplaining\u201d its predictions, explainable AI for software engineering aims to identify the code characteristics that impact maintenance risks. However, post-hoc explanations do not always faithfully reflect what the original model computes. In this paper, we introduce CounterACT, a Counterfactual ACTion rule mining approach that can generate defect reduction plans without black-box models. By leveraging action rules, CounterACT provides a course of action that can be considered as a counterfactual explanation for the class (e.g., buggy or not buggy) assigned to a piece of code. We compare the effectiveness of CounterACT with the original action rule mining algorithm and six established defect reduction approaches on 9 software projects. Our evaluation is based on (a) overlap scores between proposed code changes and actual developer modifications; (b) improvement scores in future releases; and (c) the precision, recall, and F1-score of the plans. Our results show that, compared to competing approaches, CounterACT\u2019s explainable plans achieve higher overlap scores at the release level (median 95\\%) and commit level (median 85.97\\%), and they offer better trade-off between precision and recall (median F1-score 88.12\\%). Finally, we venture beyond planning and explore leveraging Large Language models (LLM) for generating code edits from our generated plans. Our results show that suggested LLM code edits supported by our plans are actionable and are more likely to pass relevant test cases than vanilla LLM code recommendations.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "102",
        "numpages": "23",
        "labels": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification": {
        "type": "article",
        "key": "10.1145/3660810",
        "author": "Mu, Fangwen and Shi, Lin and Wang, Song and Yu, Zhuohao and Zhang, Binquan and Wang, ChenXue and Liu, Shichao and Wang, Qing",
        "title": "ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660810",
        "doi": "10.1145/3660810",
        "abstract": "Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96\\% to 80.80\\% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43\\% to 69.60\\% and from 54.32\\% to 62.37\\%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "103",
        "numpages": "23",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "venue": "FSE2024"
    },
    "Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning": {
        "type": "article",
        "key": "10.1145/3660811",
        "author": "Mai, Yubo and Gao, Zhipeng and Hu, Xing and Bao, Lingfeng and Liu, Yu and Sun, JianLing",
        "title": "Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660811",
        "doi": "10.1145/3660811",
        "abstract": "Inspired by the great potential of Large Language Models (LLMs) for solving complex coding tasks, in this paper, we propose a novel approach, named Code2API, to automatically perform APIzation for Stack Overflow code snippets. Code2API does not require additional model training or any manual crafting rules and can be easily deployed on personal computers without relying on other external tools. Specifically, Code2API guides the LLMs through well-designed prompts to generate well-formed APIs for given code snippets. To elicit knowledge and logical reasoning from LLMs, we used chain-of-thought (CoT) reasoning and few-shot in-context learning, which can help the LLMs fully understand the APIzation task and solve it step by step in a manner similar to a developer. Our evaluations show that Code2API achieves a remarkable accuracy in identifying method parameters (65\\%) and return statements (66\\%) equivalent to human-generated ones, surpassing the current state-of-the-art approach, APIzator, by 15.0\\% and 16.5\\% respectively. Moreover, compared with APIzator, our user study demonstrates that Code2API exhibits superior performance in generating meaningful method names, even surpassing the human-level performance, and developers are more willing to use APIs generated by our approach, highlighting the applicability of our tool in practice. Finally, we successfully extend our framework to the Python dataset, achieving a comparable performance with Java, which verifies the generalizability of our tool.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "104",
        "numpages": "23",
        "labels": [
            "code generation",
            "program synthesis",
            "source code model"
        ],
        "venue": "FSE2024"
    },
    "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-unleashing",
        "author": "Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng",
        "booktitle": "NAACL2024",
        "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds\u2019 strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
        "labels": [
            "agent design",
            "planning"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.15",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "Program-Aided Reasoners (Better) Know What They Know": {
        "type": "INPROCEEDINGS",
        "key": "kabra-etal-2024-program",
        "author": "Kabra, Anubha and Rangreji, Sanketh and Mathur, Yash and Madaan, Aman and Liu, Emmy and Neubig, Graham",
        "booktitle": "NAACL2024",
        "title": "Program-Aided Reasoners (Better) Know What They Know",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to \u201cknow what they know\u201d, which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types - LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.125",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning": {
        "type": "INPROCEEDINGS",
        "key": "zhu-etal-2024-pad",
        "author": "Zhu, Xuekai and Qi, Biqing and Zhang, Kaiyan and Long, Xinwei and Lin, Zhouhan and Zhou, Bowen",
        "booktitle": "NAACL2024",
        "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.142",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "Evaluating In-Context Learning of Libraries for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "patel-etal-2024-evaluating",
        "author": "Patel, Arkil and Reddy, Siva and Bahdanau, Dzmitry and Dasigi, Pradeep",
        "booktitle": "NAACL2024",
        "title": "Evaluating In-Context Learning of Libraries for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability. A particularly promising area is their ability to interpret code modules from unfamiliar libraries for solving user-instructed tasks. Recent work has shown that large proprietary LLMs can learn novel library usage in-context from demonstrations. These results raise several open questions: whether demonstrations of library usage is required, whether smaller (and more open) models also possess such capabilities, etc. In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context. Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context. Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions, which are often cheaper to obtain than demonstrations. Overall, our results pave the way for harnessing LLMs in more adaptable and dynamic coding environments.",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.161",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback": {
        "type": "INPROCEEDINGS",
        "key": "wu-2024-uicoder",
        "author": "Wu, Jason and Schoop, Eldon and Leung, Alan and Barik, Titus and Bigham, Jeffrey and Nichols, Jeffrey",
        "booktitle": "NAACL2024",
        "title": "UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Many large language models (LLMs) struggle to consistently generate UI code that compiles and produces visually relevant designs. Existing approaches to improve generation rely either on expensive human feedback or distilling a proprietary model. In this paper, we explore the use of automated feedback (compilers and multi-modal models) to guide LLMs to generate high-quality UI code. Our method starts with an existing LLM and iteratively produces improved models by self-generating a large synthetic dataset using an original model, applying automated tools to aggressively filter, score, and de-duplicate the data into a refined higher quality dataset, and producing a new LLM by finetuning the original on the refined dataset.We applied our approach to several open-source LLMs and compared the resulting performance to baseline models with both automated metrics and human preferences.Our results show the resulting models outperform all other downloadable baselines and approach the performance of larger proprietary models.",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.417",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation": {
        "type": "INPROCEEDINGS",
        "key": "salim-etal-2024-impeding",
        "author": "Salim, Saiful Islam and Yang, Rubin Yuchan and Cooper, Alexander and Ray, Suryashree and Debray, Saumya and Rahaman, Sazzadur",
        "booktitle": "EMNLP2024",
        "title": "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at measuring the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77%, ii) the drop in correctness caused by these perturbations was affected based on their detectability.",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.27",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-amr",
        "author": "Luo, Ziyang and Li, Xin and Lin, Hongzhan and Ma, Jing and Bing, Lidong",
        "booktitle": "EMNLP2024",
        "title": "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The impressive performance of proprietary LLMs like GPT4 in code generation has led to a trend to replicate these capabilities in open-source models through knowledge distillation (e.g. Code Evol-Instruct). However, these efforts often neglect the crucial aspect of response quality, relying heavily on teacher models for direct response distillation. This paradigm, especially for complex instructions, can degrade the quality of synthesized data, compromising the knowledge distillation process. To this end, our study introduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which employs a two-stage process to refine response distillation. The first stage, modular decomposition, breaks down the direct response into more manageable sub-modules. The second stage, adaptive response evolution, automatically evolves the response with the related function modules. Our experiments with three popular code benchmarks\u2014HumanEval, MBPP, and EvalPlus\u2014attests to the superiority of the AMR-Evol framework over baseline response distillation methods. By comparing with the open-source Code LLMs trained on a similar scale of data, we observed performance enhancements: more than +3.0 points on HumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the effectiveness of our framework. Our codes are available at https://github.com/ChiYeungLaw/AMR-Evol.",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.66",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "LLM4Decompile: Decompiling Binary Code with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "tan-etal-2024-llm4decompile",
        "author": "Tan, Hanzhuo and Luo, Qi and Li, Jing and Zhang, Yuqun",
        "booktitle": "EMNLP2024",
        "title": "LLM4Decompile: Decompiling Binary Code with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results.",
        "labels": [
            "static analysis",
            "program decompilation",
            "code model",
            "code model training",
            "binary code model"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.203",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-ptd",
        "author": "Luo, Ruilin and Wang, Liyuan and Lin, Binghuai and Lin, Zicheng and Yang, Yujiu",
        "booktitle": "EMNLP2024",
        "title": "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problem and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-the-art (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress. Code is available at https://github.com/lrlbbzl/PTD-SQL.",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.221",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "How Do Humans Write Code? Large Models Do It the Same Way Too": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-humans",
        "author": "Li, Long and He, Xuzheng and Wang, Haozhe and Wang, Linlin and He, Liang",
        "booktitle": "EMNLP2024",
        "title": "How Do Humans Write Code? Large Models Do It the Same Way Too",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model\u2019s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.267",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "chidambaram-etal-2024-socratic",
        "author": "Chidambaram, Subramanian and Li, Li Erran and Bai, Min and Li, Xiaopeng and Lin, Kaixiang and Zhou, Xiong and Williams, Alex C.",
        "booktitle": "EMNLP2024",
        "title": "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) are increasingly used for generating code solutions, empowered by features like self-debugging and self-reflection. However, LLMs often struggle with complex programming problems without human guidance. This paper investigates the strategies employed by expert programmers to steer code-generating LLMs toward successful outcomes. Through a study involving experts using natural language to guide GPT-4, Gemini Ultra, and, Claude 3.5 Sonnet on highly difficult programming challenges, we frame our analysis using the \u201cSocratic Feedback\u201d paradigm for understanding effective steering strategies. By analyzing 30 conversational transcripts across all three models, we map observed feedback strategies to five stages of Socratic Questioning: Definition, Elenhus, Maieutic, Dialectic, and Counter-factual reasoning. We find evidence that by employing a combination of different Socratic feedback strategies across multiple turns, programmers successfully guided the models to solve 74% of the problems that the models initially failed to solve on their own.",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.908",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs": {
        "type": "INPROCEEDINGS",
        "key": "yadav-etal-2024-pythonsaga",
        "author": "Yadav, Ankit and Beniwal, Himanshu and Singh, Mayank",
        "booktitle": "EMNLP2024",
        "title": "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of *HumanEval* and *MBPP*, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks that can inflate model performance estimations. To address these limitations, we propose a novel benchmark, *PythonSaga*, featuring 185 hand-crafted prompts in a balanced representation of 38 programming concepts across diverse difficulty levels. The robustness of our benchmark is demonstrated by the poor performance of existing Code-LLMs. The code and data set are openly available to the NLP community at this [URL](https://github.com/PythonSaga/PythonSaga).",
        "labels": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.996",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Towards Efficient Fine-Tuning of Language Models With Organizational Data for Automated Software Review": {
        "type": "article",
        "key": "10.1109/TSE.2024.3428324",
        "author": "Nashaat, Mona and Miller, James",
        "title": "Towards Efficient Fine-Tuning of Language Models With Organizational Data for Automated Software Review",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3428324",
        "doi": "10.1109/TSE.2024.3428324",
        "abstract": "Large language models like BERT and GPT possess significant capabilities and potential impacts across various applications. Software engineers often use these models for code-related tasks, including generating, debugging, and summarizing code. Nevertheless, large language models still have several flaws, including model hallucination. (e.g., generating erroneous code and producing outdated and inaccurate programs) and the substantial computational resources and energy required for training and fine-tuning. To tackle these challenges, we propose CodeMentor, a framework for few-shot learning to train large language models with the data available within the organization. We employ the framework to train a language model for code review activities, such as code refinement and review generation. The framework utilizes heuristic rules and weak supervision techniques to leverage available data, such as previous review comments, issue reports, and related code updates. Then, the framework employs the constructed dataset to fine-tune LLMs for code review tasks. Additionally, the framework integrates domain expertise by employing reinforcement learning with human feedback. This allows domain experts to assess the generated code and enhance the model performance. Also, to assess the performance of the proposed model, we evaluate it with four state-of-the-art techniques in various code review tasks. The experimental results attest that CodeMentor enhances the performance in all tasks compared to the state-of-the-art approaches, with an improvement of up to 22.3%, 43.4%, and 24.3% in code quality estimation, review generation, and bug report summarization tasks, respectively.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2240\u20132253",
        "numpages": "14",
        "venue": "TSE2024",
        "labels": [
            "code generation",
            "program repair",
            "code model",
            "code model training",
            "source code model"
        ]
    },
    "LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3428972",
        "author": "Fakhoury, Sarah and Naik, Aaditya and Sakkas, Georgios and Chakraborty, Saikat and Lahiri, Shuvendu K.",
        "title": "LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3428972",
        "doi": "10.1109/TSE.2024.3428972",
        "abstract": "Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow &lt;sc&gt;TiCoder&lt;/sc&gt; for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97\\% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2254\u20132268",
        "numpages": "15",
        "venue": "TSE2024",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ]
    },
    "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3440503",
        "author": "Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Zhuo, Terry Yue and Chen, Taolue",
        "title": "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3440503",
        "doi": "10.1109/TSE.2024.3440503",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models (&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq1-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq2-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach &lt;monospace&gt;COTTON&lt;/monospace&gt; which can leverage &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq3-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by &lt;monospace&gt;COTTON&lt;/monospace&gt; outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by &lt;monospace&gt;COTTON&lt;/monospace&gt; boost various &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq4-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that &lt;monospace&gt;COTTON&lt;/monospace&gt; not only improves the performance of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq5-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs, but also enhances the performance of LLMs. Our study showcases the potential of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq6-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs in software engineering applications.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2437\u20132457",
        "numpages": "21",
        "venue": "TSE2024",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ]
    },
    "Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3397822",
        "author": "Tu, Haoxin and Zhou, Zhide and Jiang, He and Yusuf, Imam Nur Bani and Li, Yuxian and Jiang, Lingxiao",
        "title": "Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3397822",
        "doi": "10.1109/TSE.2024.3397822",
        "abstract": "Compiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task. Existing compiler bug isolation approaches convert the problem into a test program mutation problem, but they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named &lt;sc&gt;LLM4CBI&lt;/sc&gt; to utilize LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation may not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts. To overcome the challenges, three new components are designed in &lt;sc&gt;LLM4CBI&lt;/sc&gt;. First, &lt;sc&gt;LLM4CBI&lt;/sc&gt; utilizes a program complexity-guided prompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in programs for mutation. Second, &lt;sc&gt;LLM4CBI&lt;/sc&gt; employs a memorized prompt selection component, which adopts reinforcement learning to select specialized prompts for mutating test programs continuously. Third, a test program validation component is proposed to select specialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with the state-of-the-art approaches (DiWi and RecBi) over 120 real bugs from the two most popular compilers, namely GCC and LLVM, our evaluation demonstrates the advantages of &lt;sc&gt;LLM4CBI&lt;/sc&gt;: It can isolate 69.70\\%/21.74\\% and 24.44\\%/8.92\\% more bugs than DiWi and RecBi within Top-1/Top-5 ranked results. Additionally, we demonstrate that the LLMs component (i.e., GPT-3.5) used in &lt;sc&gt;LLM4CBI&lt;/sc&gt; can be easily replaced by other LLMs while still achieving reasonable results in comparison to related studies.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1768\u20131788",
        "numpages": "21",
        "venue": "TSE2024",
        "labels": [
            "program testing",
            "debugging",
            "code model",
            "code model training",
            "source code model"
        ]
    },
    "ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3382365",
        "author": "Tang, Yutian and Liu, Zhijie and Zhou, Zhichao and Luo, Xiapu",
        "title": "ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3382365",
        "doi": "10.1109/TSE.2024.3382365",
        "abstract": "Recent advancements in large language models (LLMs) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, LLMs have shown potential in various software engineering applications. In this study, we present a systematic comparison of test suites generated by the ChatGPT LLM and the state-of-the-art SBST tool EvoSuite. Our comparison is based on several critical factors, including correctness, readability, code coverage, and bug detection capability. By highlighting the strengths and weaknesses of LLMs (specifically ChatGPT) in generating unit test cases compared to EvoSuite, this work provides valuable insights into the performance of LLMs in solving software engineering problems. Overall, our findings underscore the potential of LLMs in software engineering and pave the way for further research in this area.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1340\u20131359",
        "numpages": "20",
        "venue": "TSE2024",
        "labels": [
            "program testing",
            "unit testing",
            "empirical study"
        ]
    },
    "No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT": {
        "type": "article",
        "key": "10.1109/TSE.2024.3392499",
        "author": "Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng",
        "title": "No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3392499",
        "doi": "10.1109/TSE.2024.3392499",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks, such as machine translation, question answering, summarization, and so on. Additionally, LLMs are also highly valuable in supporting software engineering tasks, particularly in the field of code generation. Automatic code generation is a process of automatically generating source code or executable code based on given specifications or requirements, improving developer productivity. In this study, we perform a systematic empirical assessment to the quality of code generation using &lt;italic&gt;ChatGPT&lt;/italic&gt;, a recent state-of-the-art product LLM. We leverage 728 algorithm problems in five languages (i.e., C, C++, Java, Python, and JavaScript) and 18 CWEs with 54 code scenarios for the code generation task. Our evaluation encompasses a comprehensive analysis of code snippets generated by &lt;italic&gt;ChatGPT&lt;/italic&gt;, focusing on three critical aspects: correctness, complexity, and security. We also specifically investigate &lt;italic&gt;ChatGPT&lt;/italic&gt;'s ability to engage in multi-round fixing process (i.e., &lt;italic&gt;ChatGPT&lt;/italic&gt;'s dialog ability, chatting between users and &lt;italic&gt;ChatGPT&lt;/italic&gt; for fixing generated buggy code) of facilitating code generation. By delving into the generated code and examining the experimental results, this work provides valuable insights into the performance of &lt;italic&gt;ChatGPT&lt;/italic&gt; in tackling code generation tasks over the three critical aspects. The experimental results demonstrate that (1) &lt;italic&gt;ChatGPT&lt;/italic&gt; is better at generating functionally correct code for problems before 2021 in different languages than problems after 2021 with &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$48.14\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;48.14&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"tang-ieq1-3392499.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; advantage in &lt;italic&gt;Accepted&lt;/italic&gt; rate on judgment platform, but &lt;italic&gt;ChatGPT&lt;/italic&gt;'s ability to directly fix erroneous code with multi-round fixing process to achieve correct functionality is relatively weak; (2) the distribution of cyclomatic and cognitive complexity levels for code snippets in different languages varies. Furthermore, the multi-round fixing process with &lt;italic&gt;ChatGPT &lt;/italic&gt; generally preserves or increases the complexity levels of code snippets; (3) in algorithm scenarios with languages of C, C++, and Java, and CWE scenarios with languages of C and Python3, the code generated by &lt;italic&gt;ChatGPT &lt;/italic&gt; has relevant vulnerabilities. However, the multi-round fixing process for vulnerable code snippets demonstrates promising results, with more than &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$89\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;89&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"tang-ieq2-3392499.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; of vulnerabilities successfully addressed; and (4) code generation may be affected by &lt;italic&gt;ChatGPT&lt;/italic&gt;'s non-determinism factor, resulting in variations of code snippets in functional correctness, complexity, and security. Overall, our findings uncover potential issues and limitations that arise in the &lt;italic&gt;ChatGPT&lt;/italic&gt;-based code generation and lay the groundwork for improving AI and LLM-based code generation techniques.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1548\u20131584",
        "numpages": "37",
        "venue": "TSE2024",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ]
    },
    "Automatic Commit Message Generation: A Critical Review and Directions for Future Work": {
        "type": "article",
        "key": "10.1109/TSE.2024.3364675",
        "author": "Zhang, Yuxia and Qiu, Zhiqing and Stol, Klaas-Jan and Zhu, Wenhui and Zhu, Jiaxin and Tian, Yingchen and Liu, Hui",
        "title": "Automatic Commit Message Generation: A Critical Review and Directions for Future Work",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3364675",
        "doi": "10.1109/TSE.2024.3364675",
        "abstract": "Commit messages are critical for code comprehension and software maintenance. Writing a high-quality message requires skill and effort. To support developers and reduce their effort on this task, several approaches have been proposed to automatically generate commit messages. Despite the promising performance reported, we have identified three significant and prevalent threats in these automated approaches: 1) the datasets used to train and evaluate these approaches contain a considerable amount of \u2018noise\u2019; 2) current approaches only consider commits of a limited diff size; and 3) current approaches can only generate the subject of a commit message, not the message body. The first limitation may let the models \u2018learn\u2019 inappropriate messages in the training stage, and also lead to inflated performance results in their evaluation. The other two threats can considerably weaken the practical usability of these approaches. Further, with the rapid emergence of large language models (LLMs) that show superior performance in many software engineering tasks, it is worth asking: can LLMs address the challenge of long diffs and whole message generation? This article first reports the results of an empirical study to assess the impact of these three threats on the performance of the state-of-the-art auto generators of commit messages. We collected commit data of the Top 1,000 most-starred Java projects in GitHub and systematically removed noisy commits with bot-submitted and meaningless messages. We then compared the performance of four approaches representative of the state-of-the-art before and after the removal of noisy messages, or with different lengths of commit diffs. We also conducted a qualitative survey with developers to investigate their perspectives on simply generating message subjects. Finally, we evaluate the performance of two representative LLMs, namely UniXcoder and ChatGPT, in generating more practical commit messages. The results demonstrate that generating commit messages is of great practical value, considerable work is needed to mature the current state-of-the-art, and LLMs can be an avenue worth trying to address the current limitations. Our analyses provide insights for future work to achieve better performance in practice.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "816\u2013835",
        "numpages": "20",
        "venue": "TSE2024",
        "labels": [
            "software maintenance and deployment",
            "commit message generation",
            "empirical study"
        ]
    },
    "Software Testing With Large Language Models: Survey, Landscape, and Vision": {
        "type": "article",
        "key": "10.1109/TSE.2024.3368208",
        "author": "Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing",
        "title": "Software Testing With Large Language Models: Survey, Landscape, and Vision",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3368208",
        "doi": "10.1109/TSE.2024.3368208",
        "abstract": "Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "911\u2013936",
        "numpages": "26",
        "venue": "TSE2024",
        "labels": [
            "program testing",
            "survey"
        ]
    },
    "Code Review Automation: Strengths and Weaknesses of the State of the Art": {
        "type": "article",
        "key": "10.1109/TSE.2023.3348172",
        "author": "Tufano, Rosalia and Dabi\\'{c}, Ozren and Mastropaolo, Antonio and Ciniselli, Matteo and Bavota, Gabriele",
        "title": "Code Review Automation: Strengths and Weaknesses of the State of the Art",
        "year": "2024",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3348172",
        "doi": "10.1109/TSE.2023.3348172",
        "abstract": "The automation of code review has been tackled by several researchers with the goal of reducing its cost. The adoption of deep learning in software engineering pushed the automation to new boundaries, with techniques &lt;italic&gt;imitating&lt;/italic&gt; developers in generative tasks, such as commenting on a code change as a reviewer would do or addressing a reviewer's comment by modifying code. The performance of these techniques is usually assessed through quantitative metrics, &lt;italic&gt;e.g.,&lt;/italic&gt; the percentage of instances in the test set for which correct predictions are generated, leaving many open questions on the techniques\u2019 capabilities. For example, knowing that an approach is able to correctly address a reviewer's comment in 10\\% of cases is of little value without knowing what was asked by the reviewer: What if in all successful cases the code change required to address the comment was just the removal of an empty line? In this paper we aim at characterizing the cases in which three code review automation techniques tend to succeed or fail in the two above-described tasks. The study has a strong qualitative focus, with &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$sim$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u223c&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"bavota-ieq1-3348172.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;105 man-hours of manual inspection invested in manually analyzing correct and wrong predictions generated by the three techniques, for a total of 2,291 inspected predictions. The output of this analysis are two taxonomies reporting, for each of the two tasks, the types of code changes on which the experimented techniques tend to succeed or to fail, pointing to areas for future work. A result of our manual analysis was also the identification of several issues in the datasets used to train and test the experimented techniques. Finally, we assess the importance of researching in techniques specialized for code review automation by comparing their performance with ChatGPT, a general purpose large language model, finding that ChatGPT struggles in commenting code as a human reviewer would do.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "338\u2013353",
        "numpages": "16",
        "venue": "TSE2024",
        "labels": [
            "code review",
            "empirical study"
        ]
    },
    "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3334955",
        "author": "Sch\\\"{a}fer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank",
        "title": "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation",
        "year": "2023",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3334955",
        "doi": "10.1109/TSE.2023.3334955",
        "abstract": "Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in &lt;sc&gt;TestPilot&lt;/sc&gt;, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate &lt;sc&gt;TestPilot&lt;/sc&gt; using OpenAI's &lt;italic&gt;gpt3.5-turbo&lt;/italic&gt; LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2\\% and branch coverage of 52.8\\%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3\\% statement coverage and 25.6\\% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8\\% of &lt;sc&gt;TestPilot&lt;/sc&gt;'s generated tests have &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$leq$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u2264&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"schaefer-ieq1-3334955.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; 50\\% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run &lt;sc&gt;TestPilot&lt;/sc&gt; with two additional LLMs, OpenAI's older &lt;italic&gt;code-cushman-002&lt;/italic&gt; LLM and &lt;italic&gt;StarCoder&lt;/italic&gt;, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2\\% median statement coverage), and somewhat worse results with the latter (54.0\\% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "85\u2013105",
        "numpages": "21",
        "venue": "TSE2024",
        "labels": [
            "program testing",
            "unit testing",
            "empirical study"
        ]
    },
    "Learning to Generate Structured Code Summaries From Hybrid Code Context": {
        "type": "article",
        "key": "10.1109/TSE.2024.3439562",
        "author": "Zhou, Ziyi and Li, Mingchen and Yu, Huiqun and Fan, Guisheng and Yang, Penghui and Huang, Zijie",
        "title": "Learning to Generate Structured Code Summaries From Hybrid Code Context",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3439562",
        "doi": "10.1109/TSE.2024.3439562",
        "abstract": "Code summarization aims to automatically generate natural language descriptions for code, and has become a rapidly expanding research area in the past decades. Unfortunately, existing approaches mainly focus on the \u201cone-to-one\u201d mapping from methods to short descriptions, which hinders them from becoming practical tools: 1) The program context is ignored, so they have difficulty in predicting labels outside the target method; 2) They are typically trained to generate brief function descriptions with only one sentence in length, and therefore have difficulty in providing specific information. These drawbacks are partially due to the limitations of public code summarization datasets. In this paper, we first build a large code summarization dataset including different code contexts and summary content annotations, and then propose a deep learning framework that learns to generate structured code summaries from hybrid program context, named StructCodeSum. It provides both an LLM-based approach and a lightweight approach which are suitable for different scenarios. Given a target method, StructCodeSum predicts its function description, return description, parameter description, and usage description through hybrid code context, and ultimately builds a Javadoc-style code summary. The hybrid code context consists of path context, class context, documentation context and call context of the target method. Extensive experimental results demonstrate: 1) The hybrid context covers more than 70\\% of the summary tokens in average and significantly boosts the model performance; 2) When generating function descriptions, StructCodeSum outperforms the state-of-the-art approaches by a large margin; 3) According to human evaluation, the quality of the structured summaries generated by our approach is better than the documentation generated by Code Llama.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2512\u20132528",
        "numpages": "17",
        "venue": "TSE2024",
        "labels": [
            "static analysis",
            "code summarization",
            "benchmark"
        ]
    },
    "Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration": {
        "type": "article",
        "key": "10.1109/TSE.2024.3445338",
        "author": "Paltenghi, Matteo and Pandita, Rahul and Henley, Austin Z. and Ziegler, Albert",
        "title": "Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3445338",
        "doi": "10.1109/TSE.2024.3445338",
        "abstract": "Recent neural models of code, such as OpenAI Codex and AlphaCode, have demonstrated remarkable proficiency at code generation due to the underlying attention mechanism. However, it often remains unclear how the models actually process code, and to what extent their reasoning and the way their attention mechanism scans the code matches the patterns of developers. A poor understanding of the model reasoning process limits the way in which current neural models are leveraged today, so far mostly for their raw prediction. To fill this gap, this work studies how the processed attention signal of three open large language models - CodeGen, InCoder and GPT-J - agrees with how developers look at and explore code when each answers the same sensemaking questions about code. Furthermore, we contribute an open-source eye-tracking dataset comprising 92 manually-labeled sessions from 25 developers engaged in sensemaking tasks. We empirically evaluate five heuristics that do not use the attention and ten attention-based post-processing approaches of the attention signal of CodeGen against our ground truth of developers exploring code, including the novel concept of &lt;italic&gt;follow-up attention&lt;/italic&gt; which exhibits the highest agreement between model and human attention. Our follow-up attention method can predict the next line a developer will look at with 47\\% accuracy. This outperforms the baseline prediction accuracy of 42.3\\%, which uses the session history of other developers to recommend the next line. These results demonstrate the potential of leveraging the attention signal of pre-trained models for effective code exploration.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2568\u20132582",
        "numpages": "15",
        "venue": "TSE2024",
        "labels": [
            "code generation",
            "code completion",
            "code model",
            "code model training",
            "source code model",
            "benchmark"
        ]
    },
    "Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction": {
        "type": "article",
        "key": "10.1109/TSE.2024.3450837",
        "author": "Kang, Sungmin and Yoon, Juyeon and Askarbekkyzy, Nargiz and Yoo, Shin",
        "title": "Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3450837",
        "doi": "10.1109/TSE.2024.3450837",
        "abstract": "Bug reproduction is a critical developer activity that is also challenging to automate, as bug reports are often in natural language and thus can be difficult to transform to test cases consistently. As a result, existing techniques mostly focused on crash bugs, which are easier to automatically detect and verify. In this work, we overcome this limitation by using large language models (LLMs), which have been demonstrated to be adept at natural language processing and code generation. By prompting LLMs to generate bug-reproducing tests, and via a post-processing pipeline to automatically identify promising generated tests, our proposed technique &lt;sc&gt;Libro&lt;/sc&gt; could successfully reproduce about one-third of all bugs in the widely used Defects4J benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11 open-source LLMs, suggests that open-source LLMs also demonstrate substantial potential, with the StarCoder LLM achieving 70\\% of the reproduction performance of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J benchmark, and 90\\% of performance on a held-out bug dataset likely not part of any LLM's training data. In addition, our experiments on LLMs of different sizes show that bug reproduction using &lt;sc&gt;Libro&lt;/sc&gt; improves as LLM size increases, providing information as to which LLMs can be used with the &lt;sc&gt;Libro&lt;/sc&gt; pipeline.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "2677\u20132694",
        "numpages": "18",
        "venue": "TSE2024",
        "labels": [
            "program testing",
            "bug reproduction",
            "empirical study"
        ]
    },
    "DeGPT: Optimizing Decompiler Output with LLM": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Peiwei Hu and Chinese Academy of Sciences and Beijing and China) and Ruigang Liang and Chinese Academy of Sciences and Beijing and China) and Kai Chen and Chinese Academy of Sciences and China)",
        "booktitle": "NDSS2024",
        "title": "DeGPT: Optimizing Decompiler Output with LLM",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reverse engineering is essential in malware analysis, vulnerability discovery, etc. Decompilers assist the reverse engineers by lifting the assembly to the high-level programming language, which highly boosts binary comprehension. However, decompilers suffer from problems such as meaningless variable names, redundant variables, and lacking comments describing the purpose of the code. Previous studies have shown promising performance in refining the decompiler output by training the models with huge datasets containing various decompiler outputs. However, even datasets that take much time to construct cover limited binaries in the real world. The performance degrades severely facing the binary migration.In this paper, we present DeGPT, an end-to-end framework aiming to optimize the decompiler output to improve its readability and simplicity and further assist the reverse engineers in understanding the binaries better. The Large Language Model (LLM) can mitigate performance degradation with its extraordinary ability endowed by large model size and training set containing rich multi-modal data. However, its potential is difficult to unlock through one-shot use. Thus, we propose the three-role mechanism, which includes referee (R_ref), advisor (R_adv), and operator (R_ope), to adapt the LLM to our optimization tasks. Specifically, R_ref provides the optimization scheme for the target decompiler output, while R_adv gives the rectification measures based on the scheme, and R_ope inspects whether the optimization changes the original function semantics and concludes the final verdict about whether to accept the optimizations. We evaluate DeGPT on the datasets containing decompiler outputs of various software, such as the practical command line tools, malware, a library for audio processing, and implementations of algorithms. The experimental results show that even on the output of the current top-level decompiler (Ghidra), DeGPT can achieve 24.4% reduction in the cognitive burden of understanding the decompiler outputs and provide comments of which 62.9% can provide practical semantics for the reverse engineers to help the understanding of binaries. Our user surveys also show that the optimizations can significantly simplify the code and add helpful semantic information (variable names and comments), facilitating a quick and accurate understanding of the binary.",
        "labels": [
            "static analysis",
            "program decompilation"
        ],
        "url": "https://www.ndss-symposium.org/ndss-paper/degpt-optimizing-decompiler-output-with-llm",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2024"
    },
    "Large Language Model guided Protocol Fuzzing": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Ruijie Meng and Singapore) and Martin Mirchev and Marcel B\u00f6hme and Germany and Monash University and Australia) and Abhik Roychoudhury",
        "booktitle": "NDSS2024",
        "title": "Large Language Model guided Protocol Fuzzing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "How to find security flaws in a protocol implementation without a machine-readable specification of the protocol? Facing the internet, protocol implementations are particularly security-critical software systems where inputs must adhere to a specific structure and order that is often informally specified in hundreds of pages in natural language (RFC). Without some machine-readable version of that protocol, it is difficult to automatically generate valid test inputs for its implementation that follow the required structure and order. It is possible to partially alleviate this challenge using mutational fuzzing on a set of recorded message sequences as seed inputs. However, the set of available seeds is often quite limited and will hardly cover the great diversity of protocol states and input structures.In this paper, we explore the opportunities of systematic interaction with a pre-trained large language models (LLM) which has ingested millions of pages of human-readable protocol specifications, to draw out machine-readable information about the protocol that can be used during protocol fuzzing.  We use the knowledge of the LLMs about protocol message types for well-known protocols. We also checked the LLM's capability in detecting ``states\" for stateful protocol implementations by generating sequences of messages and predicting response codes. Based on these observations, we have developed an LLM-guided protocol implementation fuzzing engine. Our protocol fuzzer ChatAFL constructs grammars for each message type in a protocol, and then mutates messages or predicts the next messages in a message sequence via interactions with LLMs. Experiments on a wide range of real-world protocols from ProFuzzbench show significant efficacy in state and code coverage. Our LLM-guided stateful fuzzer was compared with state-of-the-art fuzzers AFLNet and NSFuzz. ChatAFL covers 47.6% and 42.7% more state transitions, 29.6% and 25.8% more states, and 5.8% and 6.7% more code, respectively. Apart from enhanced coverage, ChatAFL discovered nine distinct and previously unknown vulnerabilities in widely-used and extensively-tested protocol implementations while AFLNet and NSFuzz only discover three and four of them, respectively.",
        "labels": [
            "program testing",
            "fuzzing",
            "protocol fuzzing"
        ],
        "url": "https://www.ndss-symposium.org/ndss-paper/large-language-model-guided-protocol-fuzzing",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2024"
    },
    "Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652106",
        "author": "Shan, Shiwen and Huo, Yintong and Su, Yuxin and Li, Yichen and Li, Dan and Zheng, Zibin",
        "title": "Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652106",
        "doi": "10.1145/3650212.3652106",
        "abstract": "Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis. To the best of our knowledge, this is the first work to localize the root-cause configuration properties for end-users based on Large Language Models (LLMs) and logs. We evaluate the proposed strategy on Hadoop by LogConfigLocalizer and prove its efficiency with an average accuracy as high as 99.91\\%. Additionally, we also demonstrate the effectiveness and necessity of different phases of the methodology by comparing it with two other variants and a baseline tool. Moreover, we validate the proposed methodology through a practical case study to demonstrate its effectiveness and feasibility.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "13\u201325",
        "numpages": "13",
        "labels": [
            "software maintenance and deployment",
            "software configuration"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Automated Program Repair via Conversation:Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT": {
        "type": "inproceedings",
        "key": "issta24fixlingming",
        "author": "Xia, Chunqiu Steven and Zhang, Lingming",
        "title": "Automated Program Repair via Conversation:Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://dl.acm.org/doi/pdf/10.1145/3650212.3680323",
        "doi": "10.1145/3650212.3680323",
        "abstract": "Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Traditional APR techniques suffer from a lack of patch variety as they rely heavily on handcrafted or mined bug fixing patterns and cannot easily generalize to other bug/fix types. To address this limitation, recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then querying the LLM to either fill-in (cloze-style APR) the correct code at the bug location or to produce a completely new code snippet as the patch. While the LLM-based APR tools are able to achieve state-of-the-art results, they still follow the classic Generate and Validate (GV) repair paradigm of first generating lots of patches by sampling from the same initial prompt and then validating each one afterwards. This not only leads to many repeated patches that are incorrect, but also misses the crucial and yet previously ignored information in test failures as well as in plausible patches. To address these aforementioned limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same mistakes. For earlier patches that passed all the tests (i.e., plausible patches), we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM – ChatGPT. Our evaluation on the widely studied Defects4j dataset shows that ChatRepair is able to achieve the new state-of-the-art in repair performance, achieving 114 and 48 correct fixes on Defects4j 1.2 and 2.0 respectively. By calculating the cost of accessing ChatGPT, we can fix 162 out of 337 bugs for $0.42 each!",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "labels": [
            "code generation",
            "program repair"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652115",
        "author": "Zeng, Zhengran and Wang, Yidong and Xie, Rui and Ye, Wei and Zhang, Shikun",
        "title": "CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652115",
        "doi": "10.1145/3650212.3652115",
        "abstract": "In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "124\u2013136",
        "numpages": "13",
        "labels": [
            "code generation",
            "program testing",
            "bug detection",
            "benchmark"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652124",
        "author": "Wen, Xin-Cheng and Gao, Cuiyun and Gao, Shuzheng and Xiao, Yang and Lyu, Michael R.",
        "title": "SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652124",
        "doi": "10.1145/3650212.3652124",
        "abstract": "Recently, there has been a growing interest in automatic software vulnerability detection.     Pre-trained model-based approaches have demonstrated superior performance than other Deep Learning (DL)-based approaches in detecting vulnerabilities.     However, the existing pre-trained model-based approaches generally employ code sequences as input during prediction, and may ignore vulnerability-related structural information, as reflected in the following two aspects.     First, they tend to fail to infer the semantics of the code statements with complex logic such as those containing multiple operators and pointers.     Second, they are hard to comprehend various code execution sequences, which is essential for precise vulnerability detection.         To mitigate the challenges, we propose a Structured Natural Language Comment tree-based vulnerAbiLity dEtection framework based on the pre-trained models, named . The proposed Structured Natural Language Comment Tree (SCT) integrates the semantics of code statements with code execution sequences based on the Abstract Syntax Trees (ASTs).Specifically, comprises three main modules:     (1) Comment Tree Construction, which aims at enhancing the model\u2019s ability to infer the semantics of code statements by first incorporating Large Language Models (LLMs) for comment generation and then adding the comment node to ASTs.     (2) Structured Natural Language Comment Tree Construction, which aims at explicitly involving code execution sequence by combining the code syntax templates with the comment tree.     (3) SCT-Enhanced Representation, which finally incorporates the constructed SCTs for well capturing vulnerability patterns.     Experimental results demonstrate that outperforms the best-performing baseline, including the pre-trained model and LLMs, with improvements of 2.96\\%, 13.47\\%, and 3.75\\% in terms of F1 score on the FFMPeg+Qemu, Reveal, and SVulD datasets, respectively. Furthermore, can be applied to different pre-trained models, such as CodeBERT and UniXcoder, yielding the F1 score performance enhancements ranging from 1.37\\% to 10.87\\%.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "235\u2013247",
        "numpages": "13",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "LPR: Large Language Models-Aided Program Reduction": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652126",
        "author": "Zhang, Mengxiao and Tian, Yongqiang and Xu, Zhenyang and Dong, Yiwen and Tan, Shin Hwei and Sun, Chengnian",
        "title": "LPR: Large Language Models-Aided Program Reduction",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652126",
        "doi": "10.1145/3650212.3652126",
        "abstract": "Program reduction is a widely used technique to facilitate debugging                compilers by automatically minimizing programs that trigger                compiler bugs. Existing program reduction techniques are either                generic to a wide range of languages (such as Perses and Vulcan)                or specifically optimized for one certain language by exploiting                language-specific knowledge (e.g., C-Reduce). However, synergistically                combining both generality across languages and optimality                to a specific language in program reduction is yet to be explored.                This paper proposes LPR, the first LLMs-aided technique leveraging                LLMs to perform language-specific program reduction for                multiple languages. The key insight is to utilize both the language                generality of program reducers such as Perses and the languagespecific                semantics learned by LLMs. Concretely, language-generic                program reducers can efficiently reduce programs into a small size                that is suitable for LLMs to process; LLMs can effectively transform                programs via the learned semantics to create new reduction opportunities                for the language-generic program reducers to further                reduce the programs.                Our thorough evaluation on 50 benchmarks across three programming                languages (i.e., C, Rust and JavaScript) has demonstrated                LPR\u2019s practicality and superiority over Vulcan, the state-of-the-art                language-generic program reducer. For effectiveness, LPR surpasses                Vulcan by producing 24.93\\%, 4.47\\%, and 11.71\\% smaller programs                on benchmarks in C, Rust and JavaScript, separately. Moreover, LPR                and Vulcan have the potential to complement each other. For the C                language for which C-Reduce is optimized, by applying Vulcan to                the output produced by LPR, we can attain program sizes that are                on par with those achieved by C-Reduce. For efficiency perceived                by users, LPR is more efficient when reducing large and complex                programs, taking 10.77\\%, 34.88\\%, 36.96\\% less time than Vulcan to                finish all the benchmarks in C, Rust and JavaScript, separately.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "261\u2013273",
        "numpages": "13",
        "labels": [
            "code generation",
            "program transformation",
            "program testing",
            "debugging"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Automating Zero-Shot Patch Porting for Hard Forks": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652134",
        "author": "Pan, Shengyi and Wang, You and Liu, Zhongxin and Hu, Xing and Xia, Xin and Li, Shanping",
        "title": "Automating Zero-Shot Patch Porting for Hard Forks",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652134",
        "doi": "10.1145/3650212.3652134",
        "abstract": "Forking is a typical way of code reuse, which provides a simple way for developers to create a variant software (denoted as hard fork) by copying and modifying an existing codebase. Despite of the benefits, forking also leads to duplicate efforts in software maintenance. Developers need to port patches across the hard forks to address similar bugs or implement similar features. Due to the divergence between the source project and the hard fork, patch porting is complicated, which requires an adaption regarding different implementations of the same functionality. In this work, we take the first step to automate patch porting for hard forks under a zero-shot setting. We first conduct an empirical study of the patches ported from Vim to Neovim over the last ten years to investigate the necessities of patch porting and the potential flaws in the current practice. We then propose a large language model (LLM) based approach (namely PPatHF) to automatically port patches for hard forks on a function-wise basis. Specifically, PPatHF is composed of a reduction module and a porting module. Given the pre- and post-patch versions of a function from the reference project and the corresponding function from the target project, the reduction module first slims the input functions by removing code snippets less relevant to the patch. Then, the porting module leverages a LLM to apply the patch to the function from the target project. To better elicit the power of the LLM on patch porting, we design a prompt template to enable efficient in-context learning. We further propose an instruction-tuning based training task to better guide the LLM to port the patch and inject task-specific knowledge. We evaluate PPatHF on 310 Neovim patches ported from Vim. The experimental results show that PPatHF outperforms the baselines significantly. Specifically, PPatHF can correctly port 131 (42.3\\%) patches and automate 57\\% of the manual edits required for the developer to port the patch.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "363\u2013375",
        "numpages": "13",
        "labels": [
            "software maintenance and deployment",
            "empirical study"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Benchmarking Automated Program Repair: An Extensive Study on Both Real-World and Artificial Bugs": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652140",
        "author": "Ouyang, Yicheng and Yang, Jun and Zhang, Lingming",
        "title": "Benchmarking Automated Program Repair: An Extensive Study on Both Real-World and Artificial Bugs",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652140",
        "doi": "10.1145/3650212.3652140",
        "abstract": "As bugs are inevitable and prevalent in real-world programs, many Automated Program Repair (APR) techniques have been proposed to generate patches for them. However, due to the lack of a standard for evaluating APR techniques, prior works tend to use different settings and benchmarks in evaluation, threatening the trustworthiness of the evaluation results. Additionally, they typically only adopt plausibility and genuineness as evaluation metrics, which may potentially mask some underlying issues in APR techniques. To overcome these issues, in this paper, we conduct an extensive and multi-dimensional evaluation of nine learning-based and three traditional state-of-the-art APR techniques under the same environment and settings. We employ the widely studied Defects4J V2.0.0 benchmark and a newly constructed large-scale mutation-based benchmark named MuBench, derived from Defects4J and including 1,700 artificial bugs generated by various mutators, to uncover potential limitations in these APR techniques. We also apply multi-dimensional metrics, including compilability/plausibility/genuineness metrics, as well as SYE (SYntactic Equivalence) and TCE (Trivial Compiler Equivalence) metrics, to thoroughly analyze the 1,814,652 generated patches. This paper presents noteworthy findings from the extensive evaluation: Firstly, Large Language Model (LLM) based APR demonstrates less susceptibility to overfitting on the Defects4J V1.2.0 dataset and fixes the most number of bugs. Secondly, the study suggests a promising future for combining traditional and learning-based APR techniques, as they exhibit complementary advantages in fixing different types of bugs. Additionally, this work highlights the necessity for further enhancing patch compilability of learning-based APR techniques, despite the presence of various existing strategies attempting to improve it. The study also reveals other guidelines for enhancing APR techniques, including the need for handling unresolvable symbol compilability issues and reducing duplicate/no-op patch generation. Finally, our study uncovers seven implementation issues in the studied techniques, with five of them confirmed and fixed by the corresponding authors.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "440\u2013452",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair",
            "benchmark"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652142",
        "author": "Liu, Chenyan and Cai, Yufan and Lin, Yun and Huang, Yuhuan and Pei, Yunrui and Jiang, Bo and Yang, Ping and Dong, Jin Song and Mei, Hong",
        "title": "CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652142",
        "doi": "10.1145/3650212.3652142",
        "abstract": "Recent years have seen the development of LLM-based code generation. Compared to generating code in a software project, incremental code edits are empirically observed to be more frequent. The emerging code editing approaches usually formulate the problem as generating an edit based on known relevant prior edits and context. However, practical code edits can be more complicated. First, an editing session can include multiple (ir)relevant edits to the code under edit. Second, the inference of the subsequent edits is non-trivial as the scope of its ripple effect can be the whole project.        In this work, we propose CoEdPilot, an LLM-driven solution to recommend code edits by discriminating the relevant edits, exploring their interactive natures, and estimating its ripple effect in the project. Specifically, CoEdPilot orchestrates multiple neural transformers to identify what and how to edit in the project regarding both edit location and edit content. When a user accomplishes an edit with an optional editing description, an Subsequent Edit Analysis first reports the most relevant files in the project with what types of edits (e.g., keep, insert, and replace) can happen for each line of their code. Next, an Edit-content Generator generates concrete edit options for the lines of code, regarding its relevant prior changes reported by an Edit-dependency Analyzer. Last, both the Subsequent Edit Analysis and the Edit-content Generator capture relevant prior edits as feedback to readjust their recommendations. We train our models by collecting over 180K commits from 471 open-source projects in 5 programming languages. Our extensive experiments show that (1) CoEdPilot can well predict the edits (i.e., predicting edit location with accuracy of 70.8\\%-85.3\\%, and the edit content with exact match rate of 41.8\\% and BLEU4 score of 60.7); (2) CoEdPilot can well boost existing edit generators such as GRACE and CCT5 on exact match rate by 8.57\\% points and BLEU4 score by 18.08. Last, our user study on 18 participants with 3 editing tasks (1) shows that CoEdPilot can be effective in assisting users to edit code in comparison with Copilot, and (2) sheds light on the future improvement of the tool design. The video demonstration of our tool is available at https://sites.google.com/view/coedpilot/home.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "466\u2013478",
        "numpages": "13",
        "labels": [
            "code generation",
            "code completion",
            "code model",
            "code model training",
            "source code model"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Oracle-Guided Program Selection from Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680308",
        "author": "Fan, Zhiyu and Ruan, Haifeng and Mechtaev, Sergey and Roychoudhury, Abhik",
        "title": "Oracle-Guided Program Selection from Large Language Models",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680308",
        "doi": "10.1145/3650212.3680308",
        "abstract": "While large language models (LLMs) have shown significant advancements in code generation, their susceptibility to producing incorrect code poses a significant challenge to the adoption of LLM-generated programs. This issue largely stems from the reliance on natural language descriptions as informal oracles in code generation. Current strategies to mitigate this involve selecting the best program from multiple LLM-generated alternatives, judged by criteria like the consistency of their execution results on an LLM-generated test suite. However, this approach has crucial limitations: (1) LLMs often generate redundant tests or tests that cannot distinguish between correct and incorrect solutions, (2) the used consistency criteria, such as the majority vote, fail to foster developer trust due to the absence of transparent rationale behind the made choices. In this work, we propose a new perspective on increasing the quality of LLM-generated code via program selection using the LLM as a test oracle. Our method is based on our experimentally confirmed observation that LLMs serve more effectively as oracles when tasked with selecting the correct output from multiple choices. Leveraging this insight, we first generate distinguishing inputs that capture semantic discrepancies of programs sampled from an LLM, and record outputs produced by the programs on these inputs. An LLM then selects the most likely to be correct output from these, guided by the natural language problem description. We implemented this idea in a tool LLMCodeChoice and evaluated its accuracy in generating and selecting standalone programs. Our experiments demonstrated its effectiveness in improving pass@1 by 3.6-7\\% on HumanEval and MBPP benchmarks compared to the state-of-art CodeT. Most interestingly, the selected input-output specifications helped us to uncover incompleteness and ambiguities in task descriptions and also identify incorrect ground-truth implementations in the benchmarks.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "628\u2013640",
        "numpages": "13",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680323",
        "author": "Xia, Chunqiu Steven and Zhang, Lingming",
        "title": "Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680323",
        "doi": "10.1145/3650212.3680323",
        "abstract": "Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Traditional APR techniques suffer from a lack of patch variety as they rely heavily on handcrafted or mined bug fixing patterns and cannot easily generalize to other bug/fix types. To address this limitation, recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then querying the LLM to either fill-in (cloze-style APR) the correct code at the bug location or to produce a completely new code snippet as the patch. While the LLM-based APR tools are able to achieve state-of-the-art results, they still follow the classic Generate and Validate (GV) repair paradigm of first generating lots of patches by sampling from the same initial prompt and then validating each one afterwards. This not only leads to many repeated patches that are incorrect, but also misses the crucial and yet previously ignored information in test failures as well as in plausible patches.        To address these aforementioned limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same    mistakes. For earlier patches that passed all the tests (i.e., plausible patches), we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM \u2013 ChatGPT. Our evaluation on the widely studied Defects4j dataset shows that ChatRepair is able to achieve the new state-of-the-art in repair performance, achieving 114 and 48 correct fixes on Defects4j 1.2 and 2.0 respectively. By calculating the cost    of accessing ChatGPT, we can fix 162 out of 337 bugs for $0.42 each!",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "819\u2013831",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "CREF: An LLM-Based Conversational Software Repair Framework for Programming Tutors": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680328",
        "author": "Yang, Boyang and Tian, Haoye and Pian, Weiguo and Yu, Haoran and Wang, Haitao and Klein, Jacques and Bissyand\\'{e}, Tegawend\\'{e} F. and Jin, Shunfu",
        "title": "CREF: An LLM-Based Conversational Software Repair Framework for Programming Tutors",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680328",
        "doi": "10.1145/3650212.3680328",
        "abstract": "With the proven effectiveness of Large Language Models (LLMs) in code-related tasks, researchers have explored their potential for program repair. However, existing repair benchmarks might have influenced LLM training data, potentially causing data leakage. To evaluate LLMs\u2019 realistic repair capabilities, (i) we introduce an extensive, non-crawled benchmark TutorCode, comprising 1,239 C++ defect codes and associated information such as tutor guidance, solution description, failing test cases, and the corrected code. Our work assesses LLM\u2019s repair performance on TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision (RPSR). (ii) We then provide a comprehensive investigation into which types of extra information can help LLMs improve their repair performance. Among these types, tutor guidance was the most effective information. To fully harness LLMs\u2019 conversational capabilities and the benefits of augmented information, (iii) we introduce a novel conversational semi-automatic repair framework CREF assisting human programming tutors. It demonstrates a remarkable AVG-5 improvement of 17.2\\%-24.6\\% compared to the baseline, achieving an impressive AVG-5 of 76.6\\% when utilizing GPT-4. These results highlight the potential for enhancing LLMs\u2019 repair capabilities through tutor interactions and historical conversations. The successful application of CREF in a real-world educational setting demonstrates its effectiveness in reducing tutors\u2019 workload and improving students\u2019 learning experience, showing promise for code review and other software engineering tasks.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "882\u2013894",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680343",
        "author": "Guo, Lianghong and Wang, Yanlin and Shi, Ensheng and Zhong, Wanjun and Zhang, Hongyu and Chen, Jiachi and Zhang, Ruikai and Ma, Yuchi and Zheng, Zibin",
        "title": "When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680343",
        "doi": "10.1145/3650212.3680343",
        "abstract": "Code generation aims to automatically generate code snippets that meet given natural language requirements and plays an important role in software development. Although Code LLMs have shown excellent performance in this domain, their long generation time poses a signification limitation in practice use. In this paper, we first conduct an in-depth preliminary study with different Code LLMs on code generation task and identify a significant efficiency issue, i.e., continual generation of excess tokens. It harms the developer productivity and leads to huge computational wastes. To address it, we introduce CodeFast, an inference acceleration approach for Code LLMs on code generation. The key idea of CodeFast is to terminate the inference process in time when unnecessary excess tokens are detected. First, we propose an automatic data construction framework to obtain training data. Then, we train a unified lightweight model GenGuard applicable to multiple programming languages to predict whether to terminate inference at the current step. Finally, we enhance Code LLM with GenGuard to accelerate its inference in code generation task. We conduct extensive experiments with CodeFast on five representative Code LLMs across four widely used code generation datasets. Experimental results show that (1) CodeFast can significantly improve the inference speed of various Code LLMs in code generation, ranging form 34\\% to 452\\%, without compromising the quality of generated code. (2) CodeFast is stable across different parameter settings and can generalize to untrained datasets. Our code and data are available at https://github.com/DeepSoftwareAnalytics/CodeFast.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1073\u20131085",
        "numpages": "13",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "AI Coders Are among Us: Rethinking Programming Language Grammar towards Efficient Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680347",
        "author": "Sun, Zhensu and Du, Xiaoning and Yang, Zhou and Li, Li and Lo, David",
        "title": "AI Coders Are among Us: Rethinking Programming Language Grammar towards Efficient Code Generation",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680347",
        "doi": "10.1145/3650212.3680347",
        "abstract": "Artificial Intelligence (AI) models have emerged as another important audience for programming languages alongside humans and machines, as we enter the era of large language models (LLMs). LLMs can now perform well in coding competitions and even write programs like developers to solve various tasks, including mathematical problems. However, the grammar and layout of current programs are designed to cater the needs of human developers -- with many grammar tokens and formatting tokens being used to make the code easier for humans to read. While this is helpful, such a design adds unnecessary computational work for LLMs, as each token they either use or produce consumes computational resources.               To improve inference efficiency and reduce computational costs, we propose the concept of AI-oriented grammar.This aims to represent code in a way that better suits the working mechanism of AI models. Code written with AI-oriented grammar discards formats and uses a minimum number of tokens to convey code semantics effectively. To demonstrate the feasibility of this concept, we explore and implement the first AI-oriented grammar for Python, named Simple Python (SimPy). SimPy is crafted by revising the original Python grammar through a series of heuristic rules. Programs written in SimPy maintain identical Abstract Syntax Tree (AST) structures to those in standard Python. This allows for not only execution via a modified AST parser, but also seamless transformation between programs written in Python and SimPy, enabling human developers and LLMs to use Python and SimPy, respectively, when they need to collaborate. We also look into methods to help existing LLMs understand and use SimPy effectively. In the experiments, compared with Python, SimPy enables a reduction in token usage by 13.5\\% and 10.4\\% for CodeLlama and GPT-4, respectively, when completing the same set of code-related tasks. Additionally, these models can maintain or even improve their performance when using SimPy instead of Python for these tasks. With these promising results, we call for further contributions to the development of AI-oriented program grammar within our community.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1124\u20131136",
        "numpages": "13",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680355",
        "author": "Zhang, Cen and Zheng, Yaowen and Bai, Mingqiang and Li, Yeting and Ma, Wei and Xie, Xiaofei and Li, Yuekang and Sun, Limin and Liu, Yang",
        "title": "How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680355",
        "doi": "10.1145/3650212.3680355",
        "abstract": "Fuzz drivers are essential for library API fuzzing. However, automatically generating fuzz drivers is a complex task, as it demands the creation of high-quality, correct, and robust API usage code. An LLM-based (Large Language Model) approach for generating fuzz drivers is a promising area of research. Unlike traditional program analysis-based generators, this text-based approach is more generalized and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers. However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges.  To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers. Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects. Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings. In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs ($8,000+ charged tokens). Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year). Our study uncovered that:  1) While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications; 2) LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics. Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process; 3) While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bug detection.  Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1223\u20131235",
        "numpages": "13",
        "labels": [
            "program testing",
            "fuzzing"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "ThinkRepair: Self-Directed Automated Program Repair": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680359",
        "author": "Yin, Xin and Ni, Chao and Wang, Shaohua and Li, Zhenhao and Zeng, Limin and Yang, Xiaohu",
        "title": "ThinkRepair: Self-Directed Automated Program Repair",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680359",
        "doi": "10.1145/3650212.3680359",
        "abstract": "Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor.   To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase. The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt. The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information.   Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs. Notably, ThinkRepair fixes 98 bugs and improves baselines by 27\\%\u223c344.4\\% on Defects4J V1.2. On Defects4J V2.0, ThinkRepair fixes 12\u223c65 more bugs than the SOTA APRs. Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most).",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1274\u20131286",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "SelfPiCo: Self-Guided Partial Code Execution with LLMs": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680368",
        "author": "Xue, Zhipeng and Gao, Zhipeng and Wang, Shaohua and Hu, Xing and Xia, Xin and Li, Shanping",
        "title": "SelfPiCo: Self-Guided Partial Code Execution with LLMs",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680368",
        "doi": "10.1145/3650212.3680368",
        "abstract": "Code executability plays a vital role in software debugging and testing (e.g., detecting runtime exceptions or assertion violations). However, code execution, especially partial or arbitrary code execution, is a non-trivial task due to missing definitions and complex third-party dependencies. To make partial code (such as code snippets posted on the web or code fragments deep inside complex software projects) executable, the existing study has proposed a machine learning model to predict the undefined element types and inject the pre-defined dummy values into execution. However, the performance of their tool is limited due to its simply designed dummy values and the inability to continue learning. In this paper, we design and implement a novel framework, named SelfPiCo (Self-Guided Partial Code Executor), to dynamically guide partial code execution by incorporating the open-source LLM (i.e., Code Llama) within an interactive loop. Particularly, SelfPiCo leverages few-shot in-context learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning based on fine-tuning the Code Llama model. SelfPiCo continuously learns from code execution results and refines its predictions step after step. Our evaluations demonstrate that SelfPiCo can execute 72.7\\% and 83.3\\% of all lines in the open-source code and Stack Overflow snippets, outperforming the most recent state-of-the-art Lexecutor by 37.9\\% and 33.5\\%, respectively. Moreover, SelfPiCo successfully detected 18 and 33 runtime type error issues by executing the partial code from eight GitHub software projects and 43 Stack Overflow posts, demonstrating the practical usage and potential application of our framework in practice.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1389\u20131401",
        "numpages": "13",
        "labels": [
            "program testing",
            "debugging"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Neurosymbolic Repair of Test Flakiness": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680369",
        "author": "Chen, Yang and Jabbarvand, Reyhaneh",
        "title": "Neurosymbolic Repair of Test Flakiness",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680369",
        "doi": "10.1145/3650212.3680369",
        "abstract": "Test flakiness, a non-deterministic behavior of builds irrelevant to code changes, is a major and continuing impediment to deliver- ing reliable software. The very few techniques for the automated repair of test flakiness are specifically crafted to repair either Order- Dependent (OD) or Implementation-Dependent (ID) flakiness. They are also all symbolic approaches, i.e., they leverage program analy- sis to detect and repair known test flakiness patterns and root causes, failing to generalize. To bridge the gap, we propose FlakyDoctor, a neuro-symbolic technique that combines the power of LLMs\u2014 generalizability\u2014and program analysis\u2014soundness\u2014to fix different types of test flakiness. Our extensive evaluation using 873 confirmed flaky tests (332 OD and 541 ID) from 243 real-world projects demonstrates the ability of FlakyDoctor in repairing flakiness, achieving 57\\% (OD) and 59\\% (ID) success rate. Comparing to three alternative flakiness repair approaches, FlakyDoctor can repair 8\\% more ID tests than DexFix, 12\\% more OD flaky tests than ODRepair, and 17\\% more OD flaky tests than iFixFlakies. Regardless of underlying LLM, the non-LLM components of FlakyDoctor contribute to 12\u201331 \\% of the overall performance, i.e., while part of the FlakyDoctor power is from using LLMs, they are not good enough to repair flaky tests in real-world projects alone. What makes the proposed technique superior to related research on test flakiness mitigation specifically and program repair, in general, is repairing 79 previously unfixed flaky tests in real-world projects. We opened pull requests for all cases with corresponding patches; 19 of them were accepted and merged at the time of submission.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1402\u20131414",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "CoSec: On-the-Fly Security Hardening of Code LLMs via Supervised Co-decoding": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680371",
        "author": "Li, Dong and Yan, Meng and Zhang, Yaosheng and Liu, Zhongxin and Liu, Chao and Zhang, Xiaohong and Chen, Ting and Lo, David",
        "title": "CoSec: On-the-Fly Security Hardening of Code LLMs via Supervised Co-decoding",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680371",
        "doi": "10.1145/3650212.3680371",
        "abstract": "Large Language Models (LLMs) specialized in code have shown exceptional proficiency across various programming-related tasks, particularly code generation. Nonetheless, due to its nature of pretraining on massive uncritically filtered data, prior studies have shown that code LLMs are prone to generate code with potential vulnerabilities. Existing approaches to mitigate this risk involve crafting data without vulnerability and subsequently retraining or fine-tuning the model. As the number of parameters exceeds a billion, the computation and data demands of the above approaches will be enormous. Moreover, an increasing number of code LLMs tend to be distributed as services, where the internal representation is not accessible, and the API is the only way to reach the LLM, making the prior mitigation strategies non-applicable.    To cope with this, we propose CoSec, an on-the-fly Security hardening method of code LLMs based on security model-guided Co-decoding, to reduce the likelihood of code LLMs to generate code containing vulnerabilities. Our key idea is to train a separate but much smaller security model to co-decode with a target code LLM. Since the trained secure model has higher confidence for secure tokens, it guides the generation of the target base model towards more secure code generation. By adjusting the probability distributions of tokens during each step of the decoding process, our approach effectively influences the tendencies of generation without accessing the internal parameters of the target code LLM. We have conducted extensive experiments across various parameters in multiple code LLMs (i.e., CodeGen, StarCoder, and DeepSeek-Coder), and the results show that our approach is effective in security hardening. Specifically, our approach improves the average security ratio of six base models by 5.02\\%-37.14\\%, while maintaining the functional correctness of the target model.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1428\u20131439",
        "numpages": "12",
        "labels": [
            "code model",
            "code model security"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "One-to-One or One-to-Many? Suggesting Extract Class Refactoring Opportunities with Intra-class Dependency Hypergraph Neural Network": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680379",
        "author": "Cui, Di and Wang, Qiangqiang and Zhao, Yutong and Wang, Jiaqi and Wei, Minjie and Hu, Jingzhao and Wang, Luqiao and Li, Qingshan",
        "title": "One-to-One or One-to-Many? Suggesting Extract Class Refactoring Opportunities with Intra-class Dependency Hypergraph Neural Network",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680379",
        "doi": "10.1145/3650212.3680379",
        "abstract": "Excessively large classes that encapsulate multiple responsibilities are challenging to comprehend and maintain. Addressing this issue, several Extract Class refactoring tools have been proposed, employing a two-phase process: identifying suitable fields or methods for extraction, and implementing the mechanics of refactoring. These tools traditionally generate an intra-class dependency graph to analyze the class structure, applying hard-coded rules based on this graph to unearth refactoring opportunities. Yet, the graph-based approach predominantly illuminates direct, \u201cone-to-one\u201d relationship between pairwise entities. Such a perspective is restrictive as it overlooks the complex, \u201cone-to-many\u201d dependencies among multiple entities that are prevalent in real-world classes. This narrow focus can lead to refactoring suggestions that may diverge from developers\u2019 actual needs, given their multifaceted nature. To bridge this gap, our paper leverages the concept of intra-class dependency hypergraph to model one-to-many dependency relationship and proposes a hypergraph learning-based approach to suggest Extract Class refactoring opportunities named HECS. For each target class, we first construct its intra-class dependency hypergraph and assign attributes to nodes with a pre-trained code model. All the attributed hypergraphs are fed into an enhanced hypergraph neural network for training. Utilizing this trained neural network alongside a large language model (LLM), we construct a refactoring suggestion system. We trained HECS on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 38.5\\% in precision, 9.7\\% in recall, and 44.4\\% in f1-measure compared to 3 state-of-the-art refactoring tools including JDeodorant, SSECS, and LLMRefactor, which is more useful for 64\\% of participants. The results also unveil practical suggestions and new insights that benefit existing extract-related refactoring techniques.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1529\u20131540",
        "numpages": "12",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "AutoCodeRover: Autonomous Program Improvement": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680384",
        "author": "Zhang, Yuntong and Ruan, Haifeng and Fan, Zhiyu and Roychoudhury, Abhik",
        "title": "AutoCodeRover: Autonomous Program Improvement",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680384",
        "doi": "10.1145/3650212.3680384",
        "abstract": "Researchers have made significant progress in automating the software development process in the past decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM\u2019s understanding of the issue\u2019s root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on the recently proposed SWE-bench-lite (300 real-life Github issues) show increased efficacy in solving Github issues (19\\% on SWE-bench-lite), which is higher than the efficacy of the recently reported Swe-agent. Interestingly, our approach resolved 57 GitHub issues in about 4 minutes each (pass@1), whereas developers spent more than 2.68 days on average. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1592\u20131604",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680388",
        "author": "Xue, Zhiyi and Li, Liangguo and Tian, Senyue and Chen, Xiaohong and Li, Pingping and Chen, Liangyu and Jiang, Tingting and Zhang, Min",
        "title": "LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680388",
        "doi": "10.1145/3650212.3680388",
        "abstract": "FinTech software, crucial for both safety and timely market deployment, presents a compelling case for automated acceptance testing against regulatory business rules. However, the inherent challenges of comprehending unstructured natural language descriptions of these rules and crafting comprehensive test cases demand human intelligence. The emergence of Large Language Models (LLMs) holds promise for automated test case generation, leveraging their natural language processing capabilities. Yet, their dependence on human intervention for effective prompting hampers efficiency.    In response, we introduce a groundbreaking, fully automated approach for generating high-coverage test cases from natural language business rules. Our methodology seamlessly integrates the versatility of LLMs with the predictability of algorithmic methods. We fine-tune pre-trained LLMs for improved information extraction accuracy and algorithmically generate comprehensive testable scenarios for the extracted business rules.\tOur prototype, LLM4Fin, is designed for testing real-world stock-trading software. Experimental results demonstrate LLM4Fin\u2019s superiority over both state-of-the-art LLM, such as ChatGPT, and skilled testing engineers. We achieve remarkable performance, with up to 98.18\\% and an average of 20\\%\u2212110\\% improvement on business scenario coverage, and up to 93.72\\% on code coverage, while reducing the time cost from 20 minutes to a mere 7 seconds. These results provide robust evidence of the framework\u2019s practical applicability and efficiency, marking a significant advancement in FinTech software testing.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1643\u20131655",
        "numpages": "13",
        "labels": [
            "program testing"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Fuzzing JavaScript Interpreters with Coverage-Guided Reinforcement Learning for LLM-Based Mutation": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680389",
        "author": "Eom, Jueon and Jeong, Seyeon and Kwon, Taekyoung",
        "title": "Fuzzing JavaScript Interpreters with Coverage-Guided Reinforcement Learning for LLM-Based Mutation",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680389",
        "doi": "10.1145/3650212.3680389",
        "abstract": "JavaScript interpreters, crucial for modern web browsers, require an effective fuzzing method to identify security-related bugs. However, the strict grammatical requirements for input present significant challenges. Recent efforts to integrate language models for context- aware mutation in fuzzing are promising but lack the necessary coverage guidance to be fully effective. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with Reinforcement Learning (RL) from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the generation of test cases that are more likely to discover new coverage areas, thus improving bug detection while minimizing syntax and semantic errors, all without needing extra post-processing. Our evaluation results show that CovRL-Fuzz outperforms the state-of-the-art fuzzers in enhancing code coverage and identifying bugs in JavaScript interpreters: CovRL-Fuzz identified 58 real-world security-related bugs in the latest JavaScript interpreters, including 50 previously unknown bugs and 15 CVEs.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1656\u20131668",
        "numpages": "13",
        "labels": [
            "program testing",
            "fuzzing",
            "compiler testing"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Large Language Models for Equivalent Mutant Detection: How Far Are We?": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680395",
        "author": "Tian, Zhao and Shu, Honglin and Wang, Dong and Cao, Xuejie and Kamei, Yasutaka and Chen, Junjie",
        "title": "Large Language Models for Equivalent Mutant Detection: How Far Are We?",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680395",
        "doi": "10.1145/3650212.3680395",
        "abstract": "Mutation testing is vital for ensuring software quality. However, the presence of equivalent mutants is known to introduce redundant cost and bias issues, hindering the effectiveness of mutation testing in practical use. Although numerous equivalent mutant detection (EMD) techniques have been proposed, they exhibit limitations due to the scarcity of training data and challenges in generalizing to unseen mutants. Recently, large language models (LLMs) have been extensively adopted in various code-related tasks and have shown superior performance by more accurately capturing program semantics. Yet the performance of LLMs in equivalent mutant detection remains largely unclear. In this paper, we conduct an empirical study on 3,302 method-level Java mutant pairs to comprehensively investigate the effectiveness and efficiency of LLMs for equivalent mutant detection. Specifically, we assess the performance of LLMs compared to existing EMD techniques, examine the various strategies of LLMs, evaluate the orthogonality between EMD techniques, and measure the time overhead of training and inference. Our findings demonstrate that LLM-based techniques significantly outperform existing techniques (i.e., the average improvement of 35.69\\% in terms of F1-score), with the fine-tuned code embedding strategy being the most effective. Moreover, LLM-based techniques offer an excellent balance between cost (relatively low training and inference time) and effectiveness. Based on our findings, we further discuss the impact of model size and embedding quality, and provide several promising directions for future research. This work is the first to examine LLMs in equivalent mutant detection, affirming their effectiveness and efficiency.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1733\u20131745",
        "numpages": "13",
        "labels": [
            "program testing",
            "mutation testing",
            "empirical study"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Maltracker: A Fine-Grained NPM Malware Tracker Copiloted by LLM-Enhanced Dataset": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680397",
        "author": "Yu, Zeliang and Wen, Ming and Guo, Xiaochen and Jin, Hai",
        "title": "Maltracker: A Fine-Grained NPM Malware Tracker Copiloted by LLM-Enhanced Dataset",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680397",
        "doi": "10.1145/3650212.3680397",
        "abstract": "As the largest package registry, Node Package Manager (NPM) has become the prime target for various supply chain attacks recently and has been flooded with numerous malicious packages, posing significant security risks to end-users. Learning-based methods have demonstrated promising performance with good adaptability to various types of attacks. However, they suffer from two main limitations. First, they often utilize metadata features or coarse-grained code features extracted at the package level while overlooking complex code semantics. Second, the dataset used to train the model often suffers from a lack of variety both in quantity and diversity, and thus cannot detect significant types of attacks.      To address these problems, we introduce Maltracker, a learningbased NPM malware tracker based on fine-grained features empowered by LLM-enhanced dataset. First, Maltracker constructs precise call graphs to extract suspicious functions that are reachable to a pre-defined set of sensitive APIs, and then utilizes community detection algorithm to identify suspicious code gadgets based on program dependency graph, from which fine-grained features are then extracted. To address the second limitation, we extend the dataset using advanced large language models (LLM) to translate malicious functions from other languages (e.g., C/C++, Python, and Go) into JavaScript. Evaluations shows that Maltracker can achieve an improvement of about 12.6\\% in terms of F1-score at the package level and 31.0\\% at the function level compared with the SOTA learning-based methods. Moreover, the key components of \ud835\udc40\ud835\udc4e\ud835\udc59\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc5f all contribute to the effectiveness of its performance. Finally, Maltracker has also detected 230 new malicious packages in NPM and received 61 thanks letters, among which some contain new malicious behaviors that cannot be detected by existing tools.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1759\u20131771",
        "numpages": "13",
        "labels": [
            "static analysis",
            "software composition analysis"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "HECS: A Hypergraph Learning-Based System for Detecting Extract Class Refactoring Opportunities": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3685307",
        "author": "Wang, Luqiao and Wang, Qiangqiang and Wang, Jiaqi and Zhao, Yutong and Wei, Minjie and Quan, Zhou and Cui, Di and Li, Qingshan",
        "title": "HECS: A Hypergraph Learning-Based System for Detecting Extract Class Refactoring Opportunities",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3685307",
        "doi": "10.1145/3650212.3685307",
        "abstract": "HECS is an advanced tool designed for Extract Class refactoring by leveraging hypergraph learning to model complex dependencies within large classes. Unlike traditional tools that rely on direct one-to-one dependency graphs, HECS uses intra-class dependency hypergraphs to capture one-to-many relationships. This allows HECS to provide more accurate and relevant refactoring suggestions. The tool constructs hypergraphs for each target class, attributes nodes using a pre-trained code model, and trains an enhanced hypergraph neural network. Coupled with a large language model, HECS delivers practical refactoring suggestions. In evaluations on large-scale and real-world datasets, HECS achieved a 38.5\\% increase in precision, 9.7\\% in recall, and 44.4\\% in f1-measure compared to JDeodorant, SSECS, and LLMRefactor. These improvements make HECS a valuable tool for developers, offering practical insights and enhancing existing refactoring techniques.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1851\u20131855",
        "numpages": "5",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Collaboration to Repository-Level Vulnerability Detection": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3685562",
        "author": "Wen, Xin-Cheng",
        "title": "Collaboration to Repository-Level Vulnerability Detection",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3685562",
        "doi": "10.1145/3650212.3685562",
        "abstract": "Large Language Model (LLM)-based methods have proven to be effective for many software engineering domains, with a potential for substantial productivity effective for software vulnerability detection.    However, due to the limitation of the length of input contexts of LLM, the existing LLM-based methods mainly focus on detecting function-level and leveraging the in-file context information for vulnerability detection (i.e., intra-procedural vulnerabilities), ignoring the more complex inter-procedural vulnerability detection scenarios in practice.    For instance, in real-world scenarios, developers routinely engage with program analysis to detect vulnerabilities that span multiple cross-file information within repositories.       Since complex processes tend to have redundancy dependencies from spanning multiple files in the repository level and invoking multiple static analysis tools, the ideal goal of vulnerability detection is to extract the vulnerability-related information from the repository and provide potential possible explanations for vulnerability triggers.   However, such a goal is hard to achieve, and thus in this work, we design three works through multi-agent collaboration to approach the goal of repository-level vulnerability detection.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1926\u20131928",
        "numpages": "3",
        "labels": [
            "code generation",
            "bug detection"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "UniLog: Automatic Logging via LLM and In-Context Learning": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623326",
        "author": "Xu, Junjielong and Cui, Ziang and Zhao, Yuan and Zhang, Xu and He, Shilin and He, Pinjia and Li, Liqun and Kang, Yu and Lin, Qingwei and Dang, Yingnong and Rajmohan, Saravan and Zhang, Dongmei",
        "title": "UniLog: Automatic Logging via LLM and In-Context Learning",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623326",
        "doi": "10.1145/3597503.3623326",
        "abstract": "Logging, which aims to determine the position of logging statements, the verbosity levels, and the log messages, is a crucial process for software reliability enhancement. In recent years, numerous automatic logging tools have been designed to assist developers in one of the logging tasks (e.g., providing suggestions on whether to log in try-catch blocks). These tools are useful in certain situations yet cannot provide a comprehensive logging solution in general. Moreover, although recent research has started to explore end-to-end logging, it is still largely constrained by the high cost of fine-tuning, hindering its practical usefulness in software development. To address these problems, this paper proposes UniLog, an automatic logging framework based on the in-context learning (ICL) paradigm of large language models (LLMs). Specifically, UniLog can generate an appropriate logging statement with only a prompt containing five demonstration examples without any model tuning. In addition, UniLog can further enhance its logging ability after warmup with only a few hundred random samples. We evaluated UniLog on a large dataset containing 12,012 code snippets extracted from 1,465 GitHub repositories. The results show that UniLog achieved the state-of-the-art performance in automatic logging: (1) 76.9\\% accuracy in selecting logging positions, (2) 72.3\\% accuracy in predicting verbosity levels, and (3) 27.1 BLEU-4 score in generating log messages. Meanwhile, UniLog requires less than 4\\% of the parameter tuning time needed by fine-tuning the same LLM.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "14",
        "numpages": "12",
        "labels": [
            "software maintenance and deployment",
            "system log analysis"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623345",
        "author": "Steenhoek, Benjamin and Gao, Hongyang and Le, Wei",
        "title": "Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623345",
        "doi": "10.1145/3597503.3623345",
        "abstract": "Deep learning-based vulnerability detection has shown great performance and, in some studies, outperformed static analysis tools. However, the highest-performing approaches use token-based transformer models, which are not the most efficient to capture code semantics required for vulnerability detection. Classical program analysis techniques such as dataflow analysis can detect many types of bugs based on their root causes. In this paper, we propose to combine such causal-based vulnerability detection algorithms with deep learning, aiming to achieve more efficient and effective vulnerability detection. Specifically, we designed DeepDFA, a dataflow analysis-inspired graph learning framework and an embedding technique that enables graph learning to simulate dataflow computation. We show that DeepDFA is both performant and efficient. DeepDFA outperformed all non-transformer baselines. It was trained in 9 minutes, 75x faster than the highest-performing baseline model. When using only 50+ vulnerable and several hundreds of total examples as training data, the model retained the same performance as 100\\% of the dataset. DeepDFA also generalized to real-world vulnerabilities in DbgBench; it detected 8.7 out of 17 vulnerabilities on average across folds and was able to distinguish between patched and buggy versions, while the highest-performing baseline models did not detect any vulnerabilities. By combining DeepDFA with a large language model, we surpassed the state-of-the-art vulnerability detection performance on the Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Our replication package is located at https://doi.org/10.6084/m9.figshare.21225413.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "16",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "labels": [
            "static analysis",
            "bug detection",
            "code model",
            "code model training",
            "source code model"
        ]
    },
    "Large Language Models for Test-Free Fault Localization": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623342",
        "author": "Yang, Aidan Z. H. and Le Goues, Claire and Martins, Ruben and Hellendoorn, Vincent",
        "title": "Large Language Models for Test-Free Fault Localization",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623342",
        "doi": "10.1145/3597503.3623342",
        "abstract": "Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3\\%--54.4\\%, and Top-5 results by 14.4\\%-35.6\\%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "17",
        "numpages": "12",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "labels": [
            "program testing",
            "debugging",
            "code model",
            "code model training",
            "source code model"
        ]
    },
    "Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3608134",
        "author": "Geng, Mingyang and Wang, Shangwen and Dong, Dezun and Wang, Haotian and Li, Ge and Jin, Zhi and Mao, Xiaoguang and Liao, Xiangke",
        "title": "Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3608134",
        "doi": "10.1145/3597503.3608134",
        "abstract": "Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers' program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers' diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers' intents. We thus postulate that the LLMs can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the LLM (e.g., providing it with ten or more examples), the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM's performances, which shed light on future research directions for using LLMs to achieve comment generation.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "39",
        "numpages": "13",
        "labels": [
            "software maintenance and deployment",
            "commit message generation"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623343",
        "author": "Deng, Yinlin and Xia, Chunqiu Steven and Yang, Chenyuan and Zhang, Shizhuo Dylan and Yang, Shujing and Zhang, Lingming",
        "title": "Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623343",
        "doi": "10.1145/3597503.3623343",
        "abstract": "Bugs in Deep Learning (DL) libraries may affect almost all downstream DL applications, and it is crucial to ensure the quality of such systems. It is challenging to generate valid input programs for fuzzing DL libraries, since the input programs need to satisfy both the syntax/semantics of the supported languages (e.g., Python) and the tensor/operator constraints for constructing valid computational graphs. Recently, the TitanFuzz work demonstrates that modern Large Language Models (LLMs) can be directly leveraged to implicitly learn all the language and DL computation constraints to generate valid programs for fuzzing DL libraries (and beyond). However, LLMs tend to generate ordinary programs following similar patterns/tokens with typical programs seen in their massive pre-training corpora (e.g., GitHub), while fuzzing favors unusual inputs that cover edge cases or are unlikely to be manually produced.To fill this gap, this paper proposes FuzzGPT, the first approach to priming LLMs to synthesize unusual programs for fuzzing. FuzzGPT is mainly built on the well-known hypothesis that historical bug-triggering programs may include rare/valuable code ingredients important for bug finding. Meanwhile, while traditional techniques leveraging such historical information require intensive human efforts to both design dedicated generators and ensure the syntactic/semantic validity of generated programs, FuzzGPT demonstrates that this process can be fully automated via the intrinsic capabilities of LLMs (including fine-tuning and in-context learning), while being generalizable and applicable to challenging domains. While FuzzGPT can be applied with different LLMs, this paper focuses on the powerful GPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potential of directly leveraging the instruction-following capability of the recent ChatGPT for effective fuzzing. The experimental study on two popular DL libraries (PyTorch and TensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz, detecting 76 bugs, with 49 already confirmed as previously unknown bugs, including 11 high-priority bugs or security vulnerabilities.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "70",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "labels": [
            "program testing",
            "fuzzing"
        ]
    },
    "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639091",
        "author": "Li, Zongjie and Wang, Chaozheng and Ma, Pingchuan and Liu, Chaowei and Wang, Shuai and Wu, Daoyuan and Gao, Cuiyun and Liu, Yang",
        "title": "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639091",
        "doi": "10.1145/3597503.3639091",
        "abstract": "Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models.In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as \"code synthesis\" and \"code translation.\" We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "74",
        "numpages": "13",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639120",
        "author": "Sun, Zhensu and Du, Xiaoning and Song, Fu and Wang, Shangwen and Li, Li",
        "title": "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639120",
        "doi": "10.1145/3597503.3639120",
        "abstract": "Leveraging recent advancements in large language models, modern neural code completion models have demonstrated the capability to generate highly accurate code suggestions. However, their massive size poses challenges in terms of computational costs and environmental impact, hindering their widespread adoption in practical scenarios. Dynamic inference emerges as a promising solution, as it allocates minimal computation during inference while maintaining the model's performance. In this research, we explore dynamic inference within the context of code completion. Initially, we conducted an empirical investigation on GPT-2, focusing on the inference capabilities of intermediate layers for code completion. We found that 54.4\\% of tokens can be accurately generated using just the first layer, signifying significant computational savings potential. Moreover, despite using all layers, the model still fails to predict 14.5\\% of tokens correctly, and the subsequent completions continued from them are rarely considered helpful, with only a 4.2\\% Acceptance Rate. These findings motivate our exploration of dynamic inference in code completion and inspire us to enhance it with a decision-making mechanism that stops the generation of incorrect code. We thus propose a novel dynamic inference method specifically tailored for code completion models. This method aims not only to produce correct predictions with largely reduced computation but also to prevent incorrect predictions proactively. Our extensive evaluation shows that it can averagely skip 1.7 layers out of 16 layers in the models, leading to an 11.2\\% speedup with only a marginal 1.1\\% reduction in ROUGE-L.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "75",
        "numpages": "12",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "labels": [
            "code generation",
            "code completion",
            "empirical study"
        ]
    },
    "Traces of Memorisation in Large Language Models for Code": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639133",
        "author": "Al-Kaswan, Ali and Izadi, Maliheh and van Deursen, Arie",
        "title": "Traces of Memorisation in Large Language Models for Code",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639133",
        "doi": "10.1145/3597503.3639133",
        "abstract": "Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentially extractable we were able to extract 47\\% from a CodeGen-Mono-16B code completion model. We also observe that models memorise more, as their parameter count grows, and that their pre-training data are also vulnerable to attack. We also find that data carriers are memorised at a higher rate than regular code or documentation and that different model architectures memorise different samples. Data leakage has severe outcomes, so we urge the research community to further investigate the extent of this phenomenon using a wider range of models and extraction techniques in order to build safeguards to mitigate this issue.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "78",
        "numpages": "12",
        "labels": [
            "code model",
            "code model security",
            "benchmark"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Evaluating Large Language Models in Class-Level Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639219",
        "author": "Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling",
        "title": "Evaluating Large Language Models in Class-Level Code Generation",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639219",
        "doi": "10.1145/3597503.3639219",
        "abstract": "Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a simple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios.To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "81",
        "numpages": "13",
        "labels": [
            "code generation",
            "benchmark"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639226",
        "author": "Pan, Rangeet and Ibrahimzada, Ali Reza and Krishna, Rahul and Sankar, Divya and Wassi, Lambert Pouguem and Merler, Michele and Sobolev, Boris and Pavuluri, Raju and Sinha, Saurabh and Jabbarvand, Reyhaneh",
        "title": "Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639226",
        "doi": "10.1145/3597503.3639226",
        "abstract": "Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation---with correct translations ranging from 2.1\\% to 47.3\\% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5\\% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset---consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs---can help drive research in this area.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "82",
        "numpages": "13",
        "labels": [
            "code generation",
            "program transformation",
            "empirical study"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Rust-lancet: Automated Ownership-Rule-Violation Fixing with Behavior Preservation": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639103",
        "author": "Yang, Wenzhang and Song, Linhai and Xue, Yinxing",
        "title": "Rust-lancet: Automated Ownership-Rule-Violation Fixing with Behavior Preservation",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639103",
        "doi": "10.1145/3597503.3639103",
        "abstract": "As a relatively new programming language, Rust is designed to provide both memory safety and runtime performance. To achieve this goal, Rust conducts rigorous static checks against its safety rules during compilation, effectively eliminating memory safety issues that plague C/C++ programs. Although useful, the safety rules pose programming challenges to Rust programmers, since programmers can easily violate safety rules when coding in Rust, leading their code to be rejected by the Rust compiler, a fact underscored by a recent user study. There exists a desire to automate the process of fixing safety-rule violations to enhance Rust's programmability.In this paper, we concentrate on Rust's ownership rules and develop rust-lancet to automatically fix their violations. We devise three strategies for altering code, each intended to modify a Rust program and make it pass Rust's compiler checks. Additionally, we introduce mental semantics to model the behaviors of Rust programs that cannot be compiled due to ownership-rule violations. We design an approach to verify whether modified programs preserve their original behaviors before patches are applied. We apply rust-lancet to 160 safety-rule violations from two sources, successfully fixing 102 violations under the optimal configuration --- more than rustc and six LLM-based techniques. Notably, rust-lancet avoids generating any incorrect patches, a distinction from all other baseline techniques. We also verify the effectiveness of each fixing strategy and behavior preservation validation and affirm the rationale behind these components.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "85",
        "numpages": "13",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "PyTy: Repairing Static Type Errors in Python": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639184",
        "author": "Chow, Yiu Wai and Di Grazia, Luca and Pradel, Michael",
        "title": "PyTy: Repairing Static Type Errors in Python",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639184",
        "doi": "10.1145/3597503.3639184",
        "abstract": "Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4\\% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "87",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair",
            "static analysis",
            "type inference"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Programming Assistant for Exception Handling with CodeBERT": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639188",
        "author": "Cai, Yuchen and Yadavally, Aashish and Mishra, Abhishek and Montejo, Genesis and Nguyen, Tien",
        "title": "Programming Assistant for Exception Handling with CodeBERT",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639188",
        "doi": "10.1145/3597503.3639188",
        "abstract": "With practical code reuse, the code fragments from developers' forums often migrate to applications. Owing to the incomplete nature of such fragments, they often lack the details on exception handling. The adaptation for exception handling to the codebase is not trivial as developers must learn and memorize what API methods could cause exceptions and what exceptions need to be handled. We propose Neurex, an exception handling recommender that learns from complete code, and accepts a given Java code snippet and recommends 1) if a try-catch block is needed, 2) what statements need to be placed in a try block, and 3) what exception types need to be caught in the catch clause. Inspired by the sequence chunking techniques in natural language processing, we design Neurex via a multi-tasking model with the fine-tuning of the large language model CodeBERT for these three exception handling recommendation tasks. Via the large language model, Neurex can learn the surrounding context, leading to better learning the dependencies among the API elements, and the relations between the statements and the corresponding exception types needed to be handled.Our empirical evaluation shows that Neurex correctly performs all three exception handling recommendation tasks in 71.5\\% of the cases with a F1-score of 70.2\\%, which is a relative improvement of 166\\% over the baseline. It achieves high F1-score from 98.2\\%-99.7\\% in try-catch block necessity checking (a relative improvement of up to 55.9\\% over the baselines). It also correctly decides both the need for try-catch block(s) and the statements to be placed in try blocks with the F1-scores of 74.7\\% and 87.1\\% at the instance and statement levels, an improvement of 129.1\\% and 44.9\\% over the baseline, respectively. Our extrinsic evaluation shows that Neurex relatively improves over the baseline by 56.5\\% in F1-score for detecting exception-related bugs in incomplete Android code snippets.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "94",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Using an LLM to Help With Code Understanding": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639187",
        "author": "Nam, Daye and Macvean, Andrew and Hellendoorn, Vincent and Vasilescu, Bogdan and Myers, Brad",
        "title": "Using an LLM to Help With Code Understanding",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639187",
        "doi": "10.1145/3597503.3639187",
        "abstract": "Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5-turbo model with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "97",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "labels": [
            "software maintenance and deployment"
        ]
    },
    "Fuzz4All: Universal Fuzzing with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639121",
        "author": "Xia, Chunqiu Steven and Paltenghi, Matteo and Le Tian, Jia and Pradel, Michael and Zhang, Lingming",
        "title": "Fuzz4All: Universal Fuzzing with Large Language Models",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639121",
        "doi": "10.1145/3597503.3639121",
        "abstract": "Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are well-suited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java, and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 64 bugs already confirmed by developers as previously unknown.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "126",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "labels": [
            "program testing",
            "fuzzing"
        ]
    },
    "Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639210",
        "author": "Fu, Jingzhou and Liang, Jie and Wu, Zhiyong and Jiang, Yu",
        "title": "Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639210",
        "doi": "10.1145/3597503.3639210",
        "abstract": "Effective DBMS fuzzing relies on high-quality initial seeds, which serve as the starting point for mutation. These initial seeds should incorporate various DBMS features to explore the state space thoroughly. While built-in test cases are typically used as initial seeds, many DBMSs lack comprehensive test cases, making it difficult to apply state-of-the-art fuzzing techniques directly.To address this, we propose Sedar which produces initial seeds for a target DBMS by transferring test cases from other DBMSs. The underlying insight is that many DBMSs share similar functionalities, allowing seeds that cover deep execution paths in one DBMS to be adapted for other DBMSs. The challenge lies in converting these seeds to a format supported by the grammar of the target database. Sedar follows a three-step process to generate seeds. First, it executes existing SQL test cases within the DBMS they were designed for and captures the schema information during execution. Second, it utilizes large language models (LLMs) along with the captured schema information to guide the generation of new test cases based on the responses from the LLM. Lastly, to ensure that the test cases can be properly parsed and mutated by fuzzers, Sedar temporarily comments out unparsable sections for the fuzzers and uncomments them after mutation. We integrate Sedar into the DBMS fuzzers Sqirrel and Griffin, targeting DBMSs such as Virtuoso, MonetDB, DuckDB, and ClickHouse. Evaluation results demonstrate significant improvements in both fuzzers. Specifically, compared to Sqirrel and Griffin with non-transferred seeds, Sedar enhances code coverage by 72.46\\%-214.84\\% and 21.40\\%-194.46\\%; compared to Sqirrel and Griffin with native test cases of these DBMSs as initial seeds, incorporating the transferred seeds of Sedar results in an improvement in code coverage by 4.90\\%-16.20\\% and 9.73\\%-28.41\\%. Moreover, Sedar discovered 70 new vulnerabilities, with 60 out of them being uniquely found by Sedar with transferred seeds, and 19 of them have been assigned with CVEs.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "146",
        "numpages": "12",
        "labels": [
            "program testing",
            "fuzzing",
            "DBMS testing"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639117",
        "author": "Sun, Yuqiang and Wu, Daoyuan and Xue, Yue and Liu, Han and Wang, Haijun and Xu, Zhengzi and Xie, Xiaofei and Liu, Yang",
        "title": "GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639117",
        "doi": "10.1145/3597503.3639117",
        "abstract": "Smart contracts are prone to various vulnerabilities, leading to substantial financial losses over time. Current analysis tools mainly target vulnerabilities with fixed control- or data-flow patterns, such as re-entrancy and integer overflow. However, a recent study on Web3 security bugs revealed that about 80\\% of these bugs cannot be audited by existing tools due to the lack of domain-specific property description and checking. Given recent advances in Large Language Models (LLMs), it is worth exploring how Generative Pre-training Transformer (GPT) could aid in detecting logic vulnerabilities.In this paper, we propose GPTScan, the first tool combining GPT with static analysis for smart contract logic vulnerability detection. Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge, we utilize GPT as a versatile code understanding tool. By breaking down each logic vulnerability type into scenarios and properties, GPTScan matches candidate vulnerabilities with GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation. Evaluation on diverse datasets with around 400 contract projects and 3K Solidity files shows that GPTScan achieves high precision (over 90\\%) for token contracts and acceptable precision (57.14\\%) for large projects like Web3Bugs. It effectively detects ground-truth logic vulnerabilities with a recall of over 70\\%, including 9 new vulnerabilities missed by human auditors. GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01 USD to scan per thousand lines of Solidity code. Moreover, static confirmation helps GPTScan reduce two-thirds of false positives.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "166",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "labels": [
            "static analysis",
            "bug detection"
        ]
    },
    "Where is it? Tracing the Vulnerability-relevant Files from Vulnerability Reports": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639202",
        "author": "Sun, Jiamou and Chen, Jieshan and Xing, Zhenchang and Lu, Qinghua and Xu, Xiwei and Zhu, Liming",
        "title": "Where is it? Tracing the Vulnerability-relevant Files from Vulnerability Reports",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639202",
        "doi": "10.1145/3597503.3639202",
        "abstract": "With the widely usage of open-source software, supply-chain-based vulnerability attacks, including SolarWind and Log4Shell, have posed significant risks to software security. Currently, people rely on vulnerability advisory databases or commercial software bill of materials (SBOM) to defend against potential risks. Unfortunately, these datasets do not provide finer-grained file-level vulnerability information, compromising their effectiveness. Previous works have not adequately addressed this issue, and mainstream vulnerability detection methods have their drawbacks that hinder resolving this gap. Driven by the real needs, we propose a framework that can trace the vulnerability-relevant file for each disclosed vulnerability. Our approach uses NVD descriptions with metadata as the inputs, and employs a series of strategies with a LLM model, search engine, heuristic-based text matching method and a deep learning classifier to recommend the most likely vulnerability-relevant file, effectively enhancing the completeness of existing NVD data. Our experiments confirm that the efficiency of the proposed framework, with CodeBERT achieving 0.92 AUC and 0.85 MAP, and our user study proves our approach can help with vulnerability-relevant file detection effectively. To the best of our knowledge, our work is the first one focusing on tracing vulnerability-relevant files, laying the groundwork of building finer-grained vulnerability-aware software bill of materials.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "200",
        "numpages": "13",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639183",
        "author": "Ahmed, Toufique and Pai, Kunal Suresh and Devanbu, Premkumar and Barr, Earl",
        "title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639183",
        "doi": "10.1145/3597503.3639183",
        "abstract": "Large Language Models (LLM) are a new class of computation engines, \"programmed\" via prompt engineering. Researchers are still learning how to best \"program\" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of \"code analysis\" and extracting such information, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps.Prior work shows that LLM performance on code summarization benefits from embedding a few code \\&amp; summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization.We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU1. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs' line completion performance.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "220",
        "numpages": "13",
        "labels": [
            "static analysis",
            "code summarization",
            "agent design",
            "prompt strategy",
            "retrieval-augmented generation"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "LILAC: Log Parsing using LLMs with Adaptive Parsing Cache": {
        "type": "article",
        "key": "10.1145/3643733",
        "author": "Jiang, Zhihan and Liu, Jinyang and Chen, Zhuangbin and Li, Yichen and Huang, Junjie and Huo, Yintong and He, Pinjia and Gu, Jiazhen and Lyu, Michael R.",
        "title": "LILAC: Log Parsing using LLMs with Adaptive Parsing Cache",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643733",
        "doi": "10.1145/3643733",
        "abstract": "Log parsing transforms log messages into structured formats, serving as the prerequisite step for various log analysis tasks. Although a variety of log parsing approaches have been proposed, their performance on complicated log data remains compromised due to the use of human-crafted rules or learning-based models with limited training data. The recent emergence of powerful large language models (LLMs) demonstrates their vast pre-trained knowledge related to code and logging, making it promising to apply LLMs for log parsing. However, their lack of specialized log parsing capabilities currently hinders their parsing accuracy. Moreover, the inherent inconsistent answers, as well as the substantial overhead, prevent the practical adoption of LLM-based log parsing.   To address these challenges, we propose LILAC, the first practical Log parsIng framework using LLMs with Adaptive parsing Cache. To facilitate accurate and robust log parsing, LILAC leverages the in-context learning (ICL) capability of the LLM by performing a hierarchical candidate sampling algorithm and selecting high-quality demonstrations. Furthermore, LILAC incorporates a novel component, an adaptive parsing cache, to store and refine the templates generated by the LLM. It helps mitigate LLM's inefficiency issue by enabling rapid retrieval of previously processed log templates. In this process, LILAC adaptively updates the templates within the parsing cache to ensure the consistency of parsed results. The extensive evaluation on public large-scale datasets shows that LILAC outperforms state-of-the-art methods by 69.5\\% in terms of the average F1 score of template accuracy. In addition, LILAC reduces the query times to LLMs by several orders of magnitude, achieving a comparable efficiency to the fastest baseline.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "7",
        "numpages": "24",
        "labels": [
            "software maintenance and deployment",
            "system log analysis"
        ],
        "venue": "FSE2024"
    },
    "DiffCoder: Enhancing Large Language Model on API Invocation via Analogical Code Exercises": {
        "type": "article",
        "key": "10.1145/3643745",
        "author": "Zan, Daoguang and Yu, Ailun and Shen, Bo and Chen, Bei and Li, Wei and Gong, Yongshun and Chen, Xiaolin and Yao, Yafen and Luo, Weihua and Guan, Bei and Liu, Yan and Wang, Yongji and Wang, Qianxiang and Cui, Lizhen",
        "title": "DiffCoder: Enhancing Large Language Model on API Invocation via Analogical Code Exercises",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643745",
        "doi": "10.1145/3643745",
        "abstract": "The task of code generation aims to generate code solutions based on given programming problems. Recently, code large language models (code LLMs) have shed new light on this task, owing to their formidable code generation capabilities. While these models are powerful, they seldom focus on further improving the accuracy of library-oriented API invocation. Nonetheless, programmers frequently invoke APIs in routine coding tasks. In this paper, we aim to enhance the proficiency of existing code LLMs regarding API invocation by mimicking analogical learning, which is a critical learning strategy for humans to learn through differences among multiple instances. Motivated by this, we propose a simple yet effective approach, namely DiffCoder, which excels in API invocation by effectively training on the differences (diffs) between analogical code exercises. To assess the API invocation capabilities of code LLMs, we conduct experiments on seven existing benchmarks that focus on mono-library API invocation. Additionally, we construct a new benchmark, namely PanNumEval, to evaluate the performance of multi-library API invocation. Extensive experiments on eight benchmarks demonstrate the impressive performance of DiffCoder. Furthermore, we develop a VSCode plugin for DiffCoder, and the results from twelve invited participants further verify the practicality of DiffCoder.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "19",
        "numpages": "21",
        "labels": [
            "code generation",
            "code completion"
        ],
        "venue": "FSE2024"
    },
    "Natural Is the Best: Model-Agnostic Code Simplification for Pre-trained Large Language Models": {
        "type": "article",
        "key": "10.1145/3643753",
        "author": "Wang, Yan and Li, Xiaoning and Nguyen, Tien N. and Wang, Shaohua and Ni, Chao and Ding, Ling",
        "title": "Natural Is the Best: Model-Agnostic Code Simplification for Pre-trained Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643753",
        "doi": "10.1145/3643753",
        "abstract": "Pre-trained Large Language Models (LLM) have achieved remarkable successes in several domains. However, code-oriented LLMs are often heavy in computational complexity, and quadratically with the length of the input code sequence. Toward simplifying the input program of an LLM, the state-of-the-art approach has the strategies to filter the input code tokens based on the attention scores given by the LLM. The decision to simplify the input program should not rely on the attention patterns of an LLM, as these patterns are influenced by both the model architecture and the pre-training dataset. Since the model and dataset are part of the solution domain, not the problem domain where the input program belongs, the outcome may differ when the model is pre-trained on a different dataset. We propose SlimCode, a model-agnostic code simplification solution for LLMs that depends on the nature of input code tokens. As an empirical study on the LLMs including CodeBERT, CodeT5, and GPT-4 for two main tasks: code search and summarization, we reported that 1) the removal ratio of code has a linear-like relation with the saving ratio on training time, 2) the impact of categorized tokens on code simplification can vary significantly, 3) the impact of categorized tokens on code simplification is task-specific but model-agnostic, and 4) the above findings hold for the paradigm\u2013prompt engineering and interactive in-context learning. The empirical results showed that SlimCode can improve the state-of-the-art technique by 9.46\\% and 5.15\\% in terms of MRR and BLEU score on code search and summarization, respectively. More importantly, SlimCode is 133 times faster than the state-of-the-art approach. Additionally, SlimCode can reduce the cost of invoking GPT-4 by up to 24\\% per API query, while still producing comparable results to those with the original code. With this result, we call for a new direction on code-based, model-agnostic code simplification solutions to further empower LLMs.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "27",
        "numpages": "23",
        "labels": [
            "static analysis",
            "code search",
            "code summarization",
            "code model",
            "code model training",
            "source code model"
        ],
        "venue": "FSE2024"
    },
    "Go Static: Contextualized Logging Statement Generation": {
        "type": "article",
        "key": "10.1145/3643754",
        "author": "Li, Yichen and Huo, Yintong and Zhong, Renyi and Jiang, Zhihan and Liu, Jinyang and Huang, Junjie and Gu, Jiazhen and He, Pinjia and Lyu, Michael R.",
        "title": "Go Static: Contextualized Logging Statement Generation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643754",
        "doi": "10.1145/3643754",
        "abstract": "Logging practices have been extensively investigated to assist developers in writing appropriate logging statements for documenting software behaviors. Although numerous automatic logging approaches have been proposed, their performance remains unsatisfactory due to the constraint of the single-method input, without informative programming context outside the method. Specifically, we identify three inherent limitations with single-method context: limited static scope of logging statements, inconsistent logging styles, and missing type information of logging variables.                                To tackle these limitations, we propose SCLogger, the first contextualized logging statement generation approach with inter-method static contexts.First, SCLogger extracts inter-method contexts with static analysis to construct the contextualized prompt for language models to generate a tentative logging statement. The contextualized prompt consists of an extended static scope and sampled similar methods, ordered by the chain-of-thought (COT) strategy. Second, SCLogger refines the access of logging variables by formulating a new refinement prompt for language models, which incorporates detailed type information of variables in the tentative logging statement.                                The evaluation results show that SCLogger surpasses the state-of-the-art approach by 8.7\\% in logging position accuracy, 32.1\\% in level accuracy, 19.6\\% in variable precision, and 138.4\\% in text BLEU-4 score. Furthermore, SCLogger consistently boosts the performance of logging statement generation across a range of large language models, thereby showcasing the generalizability of this approach.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "28",
        "numpages": "22",
        "labels": [
            "software maintenance and deployment",
            "system log analysis"
        ],
        "venue": "FSE2024"
    },
    "Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example": {
        "type": "article",
        "key": "10.1145/3643755",
        "author": "Dilhara, Malinda and Bellur, Abhiram and Bryksin, Timofey and Dig, Danny",
        "title": "Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643755",
        "doi": "10.1145/3643755",
        "abstract": "Software developers often repeat the same code changes within a project or across different projects. These repetitive changes are known as \u201ccode change patterns\u201d (CPATs). Automating CPATs is crucial to expedite the software development process. While current Transformation by Example (TBE) techniques can automate CPATs, they are limited by the quality and quantity of the provided input examples. Thus, they miss transforming code variations that do not have the exact syntax, data-, or control-flow of the provided input examples, despite being semantically similar. Large Language Models (LLMs), pre-trained on extensive source code datasets, offer a potential solution. Harnessing the capability of LLMs to generate semantically equivalent, yet previously unseen variants of the original CPAT could significantly increase the effectiveness of TBE systems.                In this paper, we first discover best practices for harnessing LLMs to generate code variants that meet three criteria: correctness (semantic equivalence to the original CPAT), usefulness (reflecting what developers typically write), and applicability (aligning with the primary intent of the original CPAT). We then implement these practices in our tool PyCraft, which synergistically combines static code analysis, dynamic analysis, and LLM capabilities. By employing chain-of-thought reasoning, PyCraft generates variations of input examples and comprehensive test cases that identify correct variations with an F-measure of 96.6\\%. Our algorithm uses feedback iteration to expand the original input examples by an average factor of 58x. Using these richly generated examples, we inferred transformation rules and then automated these changes, resulting in an increase of up to 39x, with an average increase of 14x in target codes compared to a previous state-of-the-art tool that relies solely on static analysis. We submitted patches generated by PyCraft to a range of projects, notably esteemed ones like microsoft/DeepSpeed and IBM/inFairness. Their developers accepted and merged 83\\% the 86 CPAT instances submitted through 44 pull requests. This confirms the usefulness of these changes.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "29",
        "numpages": "23",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "venue": "FSE2024"
    },
    "CodePlan: Repository-Level Coding using LLMs and Planning": {
        "type": "article",
        "key": "10.1145/3643757",
        "author": "Bairi, Ramakrishna and Sonwane, Atharv and Kanade, Aditya and C., Vageesh D. and Iyer, Arun and Parthasarathy, Suresh and Rajamani, Sriram and Ashok, B. and Shet, Shashank",
        "title": "CodePlan: Repository-Level Coding using LLMs and Planning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643757",
        "doi": "10.1145/3643757",
        "abstract": "Software engineering activities such as package migration, fixing error reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code.     We formulate these activities as repository-level coding tasks.         Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems.     Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt.     We frame repository-level coding as a planning problem and present a task-agnostic, neuro-symbolic framework called CodePlan to solve it.     CodePlan synthesizes a multi-step chain-of-edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions.     CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm (symbolic components) with the neural LLMs.         We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2\u201397 files).     Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines.     CodePlan is able to get 5/7 repositories to pass the validity checks (i.e., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them.     We provide our (non-proprietary) data, evaluation scripts and supplementary material at https://github.com/microsoft/codeplan.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "31",
        "numpages": "24",
        "labels": [
            "code generation",
            "program synthesis",
            "agent design",
            "planning"
        ],
        "venue": "FSE2024"
    },
    "Only diff Is Not Enough: Generating Commit Messages Leveraging Reasoning and Action of Large Language Model": {
        "type": "article",
        "key": "10.1145/3643760",
        "author": "Li, Jiawei and Farag\\'{o}, David and Petrov, Christian and Ahmed, Iftekhar",
        "title": "Only diff Is Not Enough: Generating Commit Messages Leveraging Reasoning and Action of Large Language Model",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643760",
        "doi": "10.1145/3643760",
        "abstract": "Commit messages play a vital role in software development and maintenance. While previous research has introduced various Commit Message Generation (CMG) approaches, they often suffer from a lack of consideration for the broader software context associated with code changes. This limitation resulted in generated commit messages that contained insufficient information and were poorly readable. To address these shortcomings, we approached CMG as a knowledge-intensive reasoning task. We employed ReAct prompting with a cutting-edge Large Language Model (LLM) to generate high-quality commit messages. Our tool retrieves a wide range of software context information, enabling the LLM to create commit messages that are factually grounded and comprehensive. Additionally, we gathered commit message quality expectations from software practitioners, incorporating them into our approach to further enhance message quality. Human evaluation demonstrates the overall effectiveness of our CMG approach, which we named Omniscient Message Generator (OMG). It achieved an average improvement of 30.2\\% over human-written messages and a 71.6\\% improvement over state-of-the-art CMG methods.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "34",
        "numpages": "22",
        "labels": [
            "software maintenance and deployment",
            "commit message generation"
        ],
        "venue": "FSE2024"
    },
    "CORE: Resolving Code Quality Issues using LLMs": {
        "type": "article",
        "key": "10.1145/3643762",
        "author": "Wadhwa, Nalin and Pradhan, Jui and Sonwane, Atharv and Sahu, Surya Prakash and Natarajan, Nagarajan and Kanade, Aditya and Parthasarathy, Suresh and Rajamani, Sriram",
        "title": "CORE: Resolving Code Quality Issues using LLMs",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643762",
        "doi": "10.1145/3643762",
        "abstract": "As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues.    We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The proposer LLM of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, the LLM may introduce subtle, unintended functionality changes which may go un-detected by the static analysis. The ranker LLM evaluates the changes made by the proposer using a rubric that closely follows the acceptance criteria that a developer would enforce. CORE uses the scores assigned by the ranker LLM to rank the candidate revisions before presenting them to the developer.    We conduct a variety of experiments on two public benchmarks to show the ability of CORE:  (1) to generate code revisions acceptable to both static analysis tools and human reviewers (the latter evaluated with user study on a subset of the Python benchmark),  (2) to reduce human review efforts by detecting and eliminating revisions with unintended changes,  (3) to readily work across multiple languages (Python and Java), static analysis tools (CodeQL and SonarQube) and quality checks (52 and 10 checks, respectively),  and  (4) to achieve fix rate comparable to a rule-based automated program repair tool but with much smaller engineering efforts (on the Java benchmark).  CORE could revise 59.2\\% Python files (across 52 quality checks) so that they pass scrutiny by both a tool and a human reviewer. The ranker LLM reduced false positives by 25.8\\% in these cases. CORE produced revisions that passed the static analysis tool in 76.8\\% Java files (across 10 quality checks) comparable to 78.3\\% of a specialized program repair tool, with significantly much less engineering efforts. We release code, data, and supplementary material publicly at http://aka.ms/COREMSRI.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "36",
        "numpages": "23",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "venue": "FSE2024"
    },
    "Towards AI-Assisted Synthesis of Verified Dafny Methods": {
        "type": "article",
        "key": "10.1145/3643763",
        "author": "Misu, Md Rakib Hossain and Lopes, Cristina V. and Ma, Iris and Noble, James",
        "title": "Towards AI-Assisted Synthesis of Verified Dafny Methods",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643763",
        "doi": "10.1145/3643763",
        "abstract": "Large language models show great promise in many domains, including programming. A promise is easy to make but hard to keep, and language models often fail to keep their promises, generating erroneous code. A promising avenue to keep models honest is to incorporate formal verification: generating programs\u2019 specifications as well as code so that the code can be proved correct with respect to the specifications. Unfortunately, existing large language models show a severe lack of proficiency in verified programming.        In this paper, we demonstrate how to improve two pretrained models\u2019 proficiency in the Dafny verification-aware language. Using 178 problems from the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to synthesize Dafny methods. We use three different types of prompts: a direct Contextless prompt; a Signature prompt that includes a method signature and test cases, and a Chain of Thought (CoT) prompt that decomposes the problem into steps and includes retrieval augmentation generated example problems and solutions. Our results show that GPT-4 performs better than PaLM-2 on these tasks and that both models perform best with the retrieval augmentation generated CoT prompt. GPT-4 was able to generate verified, human-evaluated, Dafny methods for 58\\% of the problems, however, GPT-4 managed only 19\\% of the problems with the Contextless prompt, and even fewer (10\\%) for the Signature prompt. We are thus able to contribute 153 verified Dafny solutions to MBPP problems, 50 that we wrote manually, and 103 synthesized by GPT-4.        Our results demonstrate that the benefits of formal program verification are now within reach of code generating large language models. Likewise, program verification systems can benefit from large language models, whether to synthesize code wholesale, to generate specifications, or to act as a \"programmer\u2019s verification apprentice\", to construct annotations such as loop invariants which are hard for programmers to write or verification tools to find. Finally, we expect that the approach we have pioneered here \u2014 generating candidate solutions that are subsequently formally checked for correctness \u2014 should transfer to other domains (e.g., legal arguments, transport signaling, structural engineering) where solutions must be correct, where that correctness must be demonstrated, explained and understood by designers and end-users.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "37",
        "numpages": "24",
        "labels": [
            "code generation",
            "program synthesis",
            "static analysis",
            "program verification"
        ],
        "venue": "FSE2024"
    },
    "COSTELLO: Contrastive Testing for Embedding-Based Large Language Model as a Service Embeddings": {
        "type": "article",
        "key": "10.1145/3643767",
        "author": "Jiang, Weipeng and Zhai, Juan and Ma, Shiqing and Zhang, Xiaoyu and Shen, Chao",
        "title": "COSTELLO: Contrastive Testing for Embedding-Based Large Language Model as a Service Embeddings",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643767",
        "doi": "10.1145/3643767",
        "abstract": "Large language models have gained significant popularity and are often provided as a service (i.e., LLMaaS).  Companies like OpenAI and Google provide online APIs of LLMs to allow downstream users to create innovative applications.  Despite its popularity, LLM safety and quality assurance is a well-recognized concern in the real world, requiring extra efforts for testing these LLMs.  Unfortunately, while end-to-end services like ChatGPT have garnered rising attention in terms of testing, the LLMaaS embeddings have comparatively received less scrutiny.  We state the importance of testing and uncovering problematic individual embeddings without considering downstream applications.  The abstraction and non-interpretability of embedded vectors, combined with the black-box inaccessibility of LLMaaS, make testing a challenging puzzle.  This paper proposes COSTELLO, a black-box approach to reveal potential defects in abstract embedding vectors from LLMaaS by contrastive testing.  Our intuition is that high-quality LLMs can adequately capture the semantic relationships of the input texts and properly represent their relationships in the high-dimensional space.  For the given interface of LLMaaS and seed inputs, COSTELLO can automatically generate test suites and output words with potential problematic embeddings.  The idea is to synthesize contrastive samples with guidance, including positive and negative samples, by mutating seed inputs.  Our synthesis guide will leverage task-specific properties to control the mutation procedure and generate samples with known partial relationships in the high-dimensional space.  Thus, we can compare the expected relationship (oracle) and embedding distance (output of LLMs) to locate potential buggy cases.  We evaluate COSTELLO on 42 open-source (encoder-based) language models and two real-world commercial LLMaaS.  Experimental results show that COSTELLO can effectively detect semantic violations, where more than 62\\% of violations on average result in erroneous behaviors (e.g., unfairness) of downstream applications.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "41",
        "numpages": "23",
        "labels": [
            "code model",
            "code model training",
            "source code model"
        ],
        "venue": "FSE2024"
    },
    "Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM": {
        "type": "article",
        "key": "10.1145/3643769",
        "author": "Ryan, Gabriel and Jain, Siddhartha and Shang, Mingyue and Wang, Shiqi and Ma, Xiaofei and Ramanathan, Murali Krishna and Ray, Baishakhi",
        "title": "Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643769",
        "doi": "10.1145/3643769",
        "abstract": "Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt\u2019s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26\\% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "43",
        "numpages": "21",
        "labels": [
            "program testing",
            "fuzzing"
        ],
        "venue": "FSE2024"
    },
    "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-rewriting",
        "author": "Li, Haochen and Zhou, Xin and Shen, Zhiqi",
        "booktitle": "ACL2024",
        "title": "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%), and fine-tuned dense (up to 23.6%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances. The source code and data are available at https://github.com/Alex-HaochenLi/ReCo.",
        "labels": [
            "code generation",
            "code completion",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.75",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-enhancing",
        "author": "Huang, Baizhou and Lu, Shuai and Wan, Xiaojun and Duan, Nan",
        "booktitle": "ACL2024",
        "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs\u2019 reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph.MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.78",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "UniCoder: Scaling Code Large Language Model via Universal Code": {
        "type": "INPROCEEDINGS",
        "key": "sun-etal-2024-unicoder",
        "author": "Sun, Tao and Chai, Linzheng and Yang, Jian and Yin, Yuwei and Guo, Hongcheng and Liu, Jiaheng and Wang, Bing and Yang, Liqun and Li, Zhoujun",
        "booktitle": "ACL2024",
        "title": "UniCoder: Scaling Code Large Language Model via Universal Code",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks.When applying LLMs for code generation, recent works mainly focus on directing the models to articulate intermediate natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and then output code with the natural language or other structured intermediate steps. However, such output is not suitable for code translation or generation tasks since the standard CoT has different logical structures and forms of expression with the code. In this work, we introduce the universal code (UniCode) as the intermediate representation. It is a description of algorithm steps using a mix of conventions of programming languages, such as assignment operator, conditional operator, and loop. Hence, we collect an instruction dataset UniCoder-Instruct to train our model UniCoder on multi-task learning objectives. UniCoder-Instruct comprises natural-language questions, code solutions, and the corresponding universal code. The alignment between the intermediate universal code representation and the final code solution significantly improves the quality of the generated code. The experimental results demonstrate that UniCoder with the universal code significantly outperforms the previous prompting methods by a large margin, showcasing the effectiveness of the structural clues in pseudo-code.",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model training",
            "IR code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.100",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Virtual Compiler Is All You Need For Assembly Code Search": {
        "type": "INPROCEEDINGS",
        "key": "gao-etal-2024-virtual",
        "author": "Gao, Zeyu and Wang, Hao and Wang, Yuanda and Zhang, Chao",
        "booktitle": "ACL2024",
        "title": "Virtual Compiler Is All You Need For Assembly Code Search",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Assembly code search is vital for reducing the burden on reverse engineers, allowing them to quickly identify specific functions using natural language within vast binary programs.Despite its significance, this critical task is impeded by the complexities involved in building high-quality datasets. This paper explores training a Large Language Model (LLM) to emulate a general compiler. By leveraging Ubuntu packages to compile a dataset of 20 billion tokens, we further continue pre-train CodeLlama as a Virtual Compiler (ViC), capable of compiling any source code to assembly code. This approach allows for \u201cvirtual\u201d compilation across a wide range of programming languages without the need for a real compiler, preserving semantic equivalency and expanding the possibilities for assembly code dataset construction. Furthermore, we use ViC to construct a sufficiently large dataset for assembly code search. Employing this extensive dataset, we achieve a substantial improvement in assembly code search performance, with our model surpassing the leading baseline by 26%.",
        "labels": [
            "code generation",
            "program transformation",
            "static analysis",
            "code search",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.167",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning": {
        "type": "INPROCEEDINGS",
        "key": "dai-etal-2024-mpcoder",
        "author": "Dai, Zhenlong and Yao, Chang and Han, WenKang and Yuanying, Yuanying and Gao, Zhipeng and Chen, Jingyuan",
        "booktitle": "ACL2024",
        "title": "MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated great potential for assisting developers in their daily development. However, most research focuses on generating correct code, how to use LLMs to generate personalized code has seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user Personalized Code Generator) to generate personalized code for multiple users. To better learn coding style features, we utilize explicit coding style residual learning to capture the syntax code style standards and implicit style learning to capture the semantic code style conventions. We train a multi-user style adapter to better differentiate the implicit feature representations of different users through contrastive learning, ultimately enabling personalized code generation for multiple users. We further propose a novel evaluation metric for estimating similarities between codes of different coding styles. The experimental results show the effectiveness of our approach for this novel task.",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.207",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback": {
        "type": "INPROCEEDINGS",
        "key": "dou-etal-2024-stepcoder",
        "author": "Dou, Shihan and Liu, Yan and Jia, Haoxiang and Zhou, Enyu and Xiong, Limao and Shan, Junjie and Huang, Caishuang and Wang, Xiao and Fan, Xiaoran and Xi, Zhiheng and Zhou, Yuhao and Ji, Tao and Zheng, Rui and Zhang, Qi and Gui, Tao and Huang, Xuanjing",
        "booktitle": "ACL2024",
        "title": "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. The code and dataset will be made available upon publication.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.251",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-dolphcoder",
        "author": "Wang, Yejie and He, Keqing and Dong, Guanting and Wang, Pei and Zeng, Weihao and Diao, Muxi and Xu, Weiran and Wang, Jingang and Zhang, Mengdi and Cai, Xunliang",
        "booktitle": "ACL2024",
        "title": "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Various instruction finetuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model DolphCoder with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with more distinct reasoning paths increases the code capability of LLMs. (2) Improving one\u2019s ability to evaluate the correctness of code also enhances their ability to create it.",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.259",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Who Wrote this Code? Watermarking for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "lee-etal-2024-wrote",
        "author": "Lee, Taehyun and Hong, Seokhee and Ahn, Jaewoo and Hong, Ilgee and Lee, Hwaran and Yun, Sangdoo and Shin, Jamin and Kim, Gunhee",
        "booktitle": "ACL2024",
        "title": "Who Wrote this Code? Watermarking for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to detect machine-generated text by embedding watermarks are being developed.However, we discover that the existing works fail to function appropriately in code generation tasks due to the task\u2019s nature of having low entropy.Extending a logit-modifying watermark method, we propose Selective WatErmarking via Entropy Thresholding (SWEET), which enhances detection ability and mitigates code quality degeneration by removing low-entropy segments at generating and detecting watermarks.Our experiments show that SWEET significantly improves code quality preservation while outperforming all baselines, including post-hoc detection methods, in detecting machine-generated code text.Our code is available inhttps://github.com/hongcheki/sweet-watermark.",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model security"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.268",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving": {
        "type": "INPROCEEDINGS",
        "key": "islam-etal-2024-mapcoder",
        "author": "Islam, Md. Ashraful and Ali, Mohammed Eunus and Parvez, Md Rizwan",
        "booktitle": "ACL2024",
        "title": "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code synthesis, which requires a deep understanding of complex natural language (NL) problem descriptions, generation of code instructions for complex algorithms and data structures, and the successful execution of comprehensive unit tests, presents a significant challenge. Thus, while large language models (LLMs) demonstrate impressive proficiency in natural language processing (NLP), their performance in code generation tasks remains limited. In this paper, we introduce a new approach to code generation tasks leveraging the multi-agent prompting that uniquely replicates the full cycle of program synthesis as observed in human developers. Our framework, MapCoder, consists of four LLM agents specifically designed to emulate the stages of this cycle: recalling relevant examples, planning, code generation, and debugging. After conducting thorough experiments, with multiple LLMs ablations and analyses across eight challenging competitive problem-solving and program synthesis benchmarks\u2014MapCoder showcases remarkable code generation capabilities, achieving their new state-of-the-art (pass@1) results\u2014(HumanEval 93.9%, MBPP 83.1%, APPS 22.0%, CodeContests 28.5%, and xCodeEval 45.3%). Moreover, our method consistently delivers superior performance across various programming languages and varying problem difficulties. We open-source our framework at https://github.com/Md-Ashraful-Pramanik/MapCoder.",
        "labels": [
            "code generation",
            "program synthesis",
            "agent design"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.269",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning": {
        "type": "INPROCEEDINGS",
        "key": "yu-etal-2024-wavecoder",
        "author": "Yu, Zhaojian and Zhang, Xin and Shang, Ning and Huang, Yangyu and Xu, Can and Zhao, Yishujie and Hu, Wenxiang and Yin, Qiufeng",
        "booktitle": "ACL2024",
        "title": "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeOcean, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks.",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.280",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation": {
        "type": "INPROCEEDINGS",
        "key": "yan-etal-2024-codescope",
        "author": "Yan, Weixiang and Liu, Haitian and Wang, Yunkun and Li, Yunzhe and Chen, Qian and Wang, Wen and Lin, Tingyu and Zhao, Weishan and Zhu, Li and Sundaram, Hari and Deng, Shuiguang",
        "booktitle": "ACL2024",
        "title": "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce **CodeScope**, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers **43 programming languages** and **eight coding tasks**. It evaluates the coding performance of LLMs from three dimensions (perspectives): **length**, **difficulty**, and **efficiency**. To facilitate execution-based evaluations of code generation, we develop **MultiCodeEngine**, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at https://github.com/WeixiangYAN/CodeScope.",
        "labels": [
            "general coding task",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.301",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Experiential Co-Learning of Software-Developing Agents": {
        "type": "INPROCEEDINGS",
        "key": "qian-etal-2024-experiential",
        "author": "Qian, Chen and Dang, Yufan and Li, Jiahao and Liu, Wei and Xie, Zihao and Wang, YiFei and Chen, Weize and Yang, Cheng and Cong, Xin and Che, Xiaoyin and Liu, Zhiyuan and Sun, Maosong",
        "booktitle": "ACL2024",
        "title": "Experiential Co-Learning of Software-Developing Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce Experiential Co-Learning, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at https://github.com/OpenBMB/ChatDev.",
        "labels": [
            "general coding task",
            "agent design",
            "planning"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.305",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval": {
        "type": "INPROCEEDINGS",
        "key": "khan-etal-2024-xcodeeval",
        "author": "Khan, Mohammad Abdullah Matin and Bari, M. Saiful and Long, Do and Wang, Weishi and Parvez, Md Rizwan and Joty, Shafiq",
        "booktitle": "ACL2024",
        "title": "XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce *xCodeEval*, the largest executable multilingual multitask benchmark to date consisting of 25 M document-level coding examples (16.5 B tokens) from about 7.5 K unique problems covering up to 11 programming languages with execution-level parallelism. It features a total of 7 tasks involving code understanding, generation, translation and retrieval. *xCodeEval* adopts an execution-based evaluation and offers a multilingual code execution engine, *ExecEval* that supports unit test based execution in all the 11 languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI\u2019s LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate to be quite challenging as per the current advancements in language models.",
        "labels": [
            "general coding task",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.367",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Lightweight reranking for language model generations": {
        "type": "INPROCEEDINGS",
        "key": "jain-etal-2024-lightweight",
        "author": "Jain, Siddhartha and Ma, Xiaofei and Deoras, Anoop and Xiang, Bing",
        "booktitle": "ACL2024",
        "title": "Lightweight reranking for language model generations",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) can exhibit considerable variation in the quality of their sampled outputs. Reranking and selecting the best generation from the sampled set is a popular way of obtaining strong gains in generation quality. In this paper, we present a novel approach for reranking LLM generations. Unlike other techniques that might involve additional inferences or training a specialized reranker, our approach relies on easy to compute pairwise statistics between the generations that have minimal compute overhead. We show that our approach can be formalized as an extension of self-consistency and analyze its performance in that framework, theoretically as well as via simulations. We show strong improvements for selecting the best k generations for code generation tasks as well as robust improvements for the best generation for the tasks of autoformalization, summarization, and translation. While our approach only assumes black-box access to LLMs, we show that additional access to token probabilities can improve performance even further.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.376",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-integrate",
        "author": "Wang, Xinglin and Li, Yiwei and Feng, Shaoxiong and Yuan, Peiwen and Pan, Boyuan and Wang, Heda and Hu, Yao and Li, Kan",
        "booktitle": "ACL2024",
        "title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample selection or voting mechanisms to improve output quality. These methods, however, face limitations due to their inability to fully utilize the nuanced consensus knowledge present within multiple candidate samples, often resulting in suboptimal outputs. We propose Fine-Grained Self-Consistency (FSC) to addresses these limitations by extracting and integrating segment-level commonalities from candidate samples, enhancing the performance of LLMs both in open-ended and reasoning tasks. Based on this, we present two additional strategies: candidate filtering, which enhances overall quality by identifying highly similar candidate sets, and merging, which reduces input token requirements by combining similar samples. The effectiveness of FSC is demonstrated through extensive experiments on various tasks, including summarization, code generation, and mathematical reasoning, using GPT-3.5-turbo and GPT-4. The results indicate significant improvements over baseline methods, showcasing the potential of FSC to optimize output quality by effectively synthesizing fine-grained consensus knowledge from multiple samples.",
        "labels": [
            "general coding task",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.634",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Language-to-Code Translation with a Single Labeled Example": {
        "type": "INPROCEEDINGS",
        "key": "bostrom-etal-2024-language",
        "author": "Bostrom, Kaj and Jhamtani, Harsh and Fang, Hao and Thomson, Sam and Shin, Richard and Xia, Patrick and Van Durme, Benjamin and Eisner, Jason and Andreas, Jacob",
        "booktitle": "EMNLP2024",
        "title": "Language-to-Code Translation with a Single Labeled Example",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Tools for translating natural language into code promise natural, open-ended interaction with databases, web APIs, and other software systems. However, this promise is complicated by the diversity and continual development of these systems, each with its own interface and distinct set of features. Building a new language-to-code translator, even starting with a large language model (LM), typically requires annotating a large set of natural language commands with their associated programs. In this paper, we describe ICIP (In-Context Inverse Programming), a method for bootstrapping a language-to-code system using mostly (or entirely) unlabeled programs written using a potentially unfamiliar (but human-readable) library or API. ICIP uses a pre-trained LM to assign candidate natural language descriptions to these programs, then iteratively refines the descriptions to ensure global consistency. Across nine different application domains from the Overnight and Spider benchmarks and text-davinci-003 and CodeLlama-7b-Instruct models, ICIP outperforms a number of prompting baselines. Indeed, in a \u201cnearly unsupervised\u201d setting with only a single annotated program and 100 unlabeled examples, it achieves up to 85% of the performance of a fully supervised system.",
        "labels": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.462",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "RealVul: Can We Detect Vulnerabilities in Web Applications with LLM?": {
        "type": "INPROCEEDINGS",
        "key": "cao-etal-2024-realvul",
        "author": "Cao, Di and Liao, Yong and Shang, Xiuwei",
        "booktitle": "EMNLP2024",
        "title": "RealVul: Can We Detect Vulnerabilities in Web Applications with LLM?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The latest advancements in large language models (LLMs) have sparked interest in their potential for software vulnerability detection. However, there is currently a lack of research specifically focused on vulnerabilities in the PHP language, and challenges in data sampling and processing persist, hindering the model\u2019s ability to effectively capture the characteristics of specific vulnerabilities. In this paper, we present RealVul, the first LLM-based framework designed for PHP vulnerability detection, addressing these issues. By improving code sampling methods and employing normalization techniques, we can isolate potential vulnerability triggers while streamlining the code and eliminating unnecessary semantic information, enabling the model to better understand and learn from the generated vulnerability samples. We also address the issue of insufficient PHP vulnerability samples by improving data synthesis methods. To evaluate RealVul\u2019s performance, we conduct an extensive analysis using five distinct code LLMs on vulnerability data from 180 PHP projects. The results demonstrate a significant improvement in both effectiveness and generalization compared to existing methods, effectively boosting the vulnerability detection capabilities of these models.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.472",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs": {
        "type": "INPROCEEDINGS",
        "key": "puerto-etal-2024-code",
        "author": "Puerto, Haritz and Tutek, Martin and Aditya, Somak and Zhu, Xiaodan and Gurevych, Iryna",
        "booktitle": "EMNLP2024",
        "title": "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reasoning is a fundamental component of language understanding. Recent prompting techniques, such as chain of thought, have consistently improved LLMs\u2019 performance on various reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs in the inference stage. In this paper, we investigate the effect of the input representation on the reasoning abilities of LLMs. We hypothesize that representing natural language tasks as code can enhance specific reasoning abilities such as entity tracking or logical reasoning. To study this, we propose code prompting, a methodology we operationalize as a chain of prompts that transforms a natural language problem into code and directly prompts the LLM using the generated code without resorting to external code execution. We find that code prompting exhibits a high-performance boost for multiple LLMs (up to 22.52 percentage points on GPT 3.5, 7.75 on Mixtral, and 16.78 on Mistral) across multiple conditional reasoning datasets. We then conduct comprehensive experiments to understand how the code representation triggers reasoning abilities and which capabilities are elicited in the underlying models. Our analysis on GPT 3.5 reveals that the code formatting of the input problem is essential for performance improvement. Furthermore, the code representation improves sample efficiency of in-context learning and facilitates state tracking of entities.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.629",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "CodeAgent: Autonomous Communicative Agents for Code Review": {
        "type": "INPROCEEDINGS",
        "key": "tang-etal-2024-codeagent",
        "author": "Tang, Xunzhu and Kim, Kisub and Song, Yewei and Lothritz, Cedric and Li, Bei and Ezzini, Saad and Tian, Haoye and Klein, Jacques and Bissyand\u00e9, Tegawend\u00e9 F.",
        "booktitle": "EMNLP2024",
        "title": "CodeAgent: Autonomous Communicative Agents for Code Review",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code review, which aims at ensuring the overall quality and reliability of software, is a cornerstone of software development. Unfortunately, while crucial, Code review is a labor-intensive process that the research community is looking to automate. Existing automated methods rely on single input-output generative models and thus generally struggle to emulate the collaborative nature of code review. This work introduces CodeAgent, a novel multi-agent Large Language Model (LLM) system for code review automation. CodeAgent incorporates a supervisory agent, QA-Checker, to ensure that all the agents\u2019 contributions address the initial review question. We evaluated CodeAgent on critical code review tasks: (1) detect inconsistencies between code changes and commit messages, (2) identify vulnerability introductions, (3) validate code style adherence, and (4) suggest code revisions. The results demonstrate CodeAgent\u2019s effectiveness, contributing to a new state-of-the-art in code review automation. Our data and code are publicly available (https://github.com/Daniel4SE/codeagent).",
        "labels": [
            "software maintenance and deployment",
            "code review",
            "agent design"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.632",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ranaldi-etal-2024-empowering-multi",
        "author": "Ranaldi, Leonardo and Pucci, Giulia and Haddow, Barry and Birch, Alexandra",
        "booktitle": "EMNLP2024",
        "title": "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In-context learning methods are popular inference strategies where Large Language Models (LLMs) are elicited to solve a task using provided demonstrations without parameter updates. Among these approaches are the reasoning methods, best exemplified by Chain-of-Thought (CoT) and Program-Aided Language Models (PAL), which elicit LLMs to generate reasoning paths, thus promoting accuracy and attracting increasing attention. However, despite the success of these methods, the ability to deliver multi-step reasoning remains limited to a single language, making it challenging to generalize to other languages and hindering global development.In this work, we propose Cross-lingual Program-Aided Language Models (CrossPAL), a method for aligning reasoning programs across languages. In particular, our method delivers programs as intermediate reasoning steps in different languages through a double-step cross-lingual prompting mechanism inspired by the Program-Aided approach. In addition, we introduce Self-consistent CrossPAL (SCrossPAL) to ensemble different reasoning paths across languages. Our experimental evaluations show that our method significantly outperforms existing prompting methods, reducing the number of interactions and achieving state-of-the-art performance.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.678",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-da",
        "author": "Huang, Yiming and Luo, Jianwen and Yu, Yan and Zhang, Yitong and Lei, Fangyu and Wei, Yifan and He, Shizhu and Huang, Lifu and Liu, Xiao and Zhao, Jun and Liu, Kang",
        "booktitle": "EMNLP2024",
        "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, including Python and SQL, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously designed the evaluation suite to ensure the accuracy and robustness of the evaluation. We developed the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at [link](https://github.com/yiyihum/dabench)",
        "labels": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.748",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Leveraging Context-Aware Prompting for Commit Message Generation": {
        "type": "INPROCEEDINGS",
        "key": "jiang-etal-2024-leveraging-context",
        "author": "Jiang, Zhihua and Chen, Jianwei and Rao, Dongning and Ye, Guanghui",
        "booktitle": "EMNLP2024",
        "title": "Leveraging Context-Aware Prompting for Commit Message Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Writing comprehensive commit messages is tedious yet important, because these messages describe changes of code, such as fixing bugs or adding new features. However, most existing methods focus on either only the changed lines or nearest context lines, without considering the effectiveness of selecting useful contexts. On the other hand, it is possible that introducing excessive contexts can lead to noise. To this end, we propose a code model COMMIT (Context-aware prOMpting based comMIt-message generaTion) in conjunction with a code dataset CODEC (COntext and metaData Enhanced Code dataset). Leveraging program slicing, CODEC consolidates code changes along with related contexts via property graph analysis. Further, utilizing CodeT5+ as the backbone model, we train COMMIT via context-aware prompt on CODEC. Experiments show that COMMIT can surpass all compared models including pre-trained language models for code (code-PLMs) such as CommitBART and large language models for code (code-LLMs) such as Code-LlaMa. Besides, we investigate several research questions (RQs), further verifying the effectiveness of our approach. We release the data and code at: https://github.com/Jnunlplab/COMMIT.git.",
        "labels": [
            "software maintenance and deployment",
            "commit message generation"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.749",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "How Do Your Code LLMs perform? Empowering Code Instruction Tuning with Really Good Data": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-code",
        "author": "Wang, Yejie and He, Keqing and Fu, Dayuan and GongQue, Zhuoma and Xu, Heyang and Chen, Yanxu and Wang, Zhexu and Fu, Yujia and Dong, Guanting and Diao, Muxi and Wang, Jingang and Zhang, Mengdi and Cai, Xunliang and Xu, Weiran",
        "booktitle": "EMNLP2024",
        "title": "How Do Your Code LLMs perform? Empowering Code Instruction Tuning with Really Good Data",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we observe Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we find that many datasets suffer from severe data leakage. After cleaning up most of the leaked data, some well-known high-quality datasets perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data pruning strategy for selecting good samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Our experiments show Xcoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs.",
        "labels": [
            "code generation",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.777",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?": {
        "type": "INPROCEEDINGS",
        "key": "waghjale-etal-2024-ecco",
        "author": "Waghjale, Siddhant and Veerendranath, Vishruth and Wang, Zhiruo and Fried, Daniel",
        "booktitle": "EMNLP2024",
        "title": "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although large language models (LLMs) have been largely successful in generating functionally correct programs, conditioning models to produce efficient solutions while ensuring correctness remains a challenge. Further, unreliability in benchmarking code efficiency is a hurdle across varying hardware specifications for popular interpreted languages such as Python. In this paper, we present ECCO, a reproducible benchmark for evaluating program efficiency via two paradigms: natural language (NL) based code generation and history-based code editing. On ECCO, we adapt and thoroughly investigate the three most promising existing LLM-based approaches: in-context learning, iterative refinement with execution or NL feedback, and fine-tuning conditioned on execution and editing history. While most methods degrade functional correctness and moderately increase program efficiency, we find that adding execution information often helps maintain functional correctness, and NL feedback enhances more on efficiency. We release our benchmark to support future work on LLM-based generation of efficient code.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.859",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Re-Reading Improves Reasoning in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-reading",
        "author": "Xu, Xiaohan and Tao, Chongyang and Shen, Tao and Xu, Can and Xu, Hongbo and Long, Guodong and Lou, Jian-Guang and Ma, Shuai",
        "booktitle": "EMNLP2024",
        "title": "Re-Reading Improves Reasoning in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, RE2, i.e., Re-Reading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, RE2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, RE2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, RE2 facilitates a \u201cbidirectional\u201d encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of RE2, illustrating its potential to enable \u201cbidirectional\u201d attention mechanisms. We then evaluate RE2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, RE2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal RE2\u2019s adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies.",
        "labels": [
            "agent design",
            "prompt strategy"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.871",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "DocCGen: Document-based Controlled Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "pimparkhede-etal-2024-doccgen",
        "author": "Pimparkhede, Sameer and Kammakomati, Mehant and Tamilselvam, Srikanth G. and Kumar, Prince and Kumar, Ashok Pon and Bhattacharyya, Pushpak",
        "booktitle": "EMNLP2024",
        "title": "DocCGen: Document-based Controlled Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent developments show that Large Language Models (LLMs) produce state-of-the-art performance on natural language (NL) to code generation for resource-rich general-purpose languages like C++, Java, and Python. However, their practical usage for structured domain-specific languages (DSLs) such as YAML, JSON is limited due to domain-specific schema, grammar, and customizations generally unseen by LLMs during pre-training. Efforts have been made to mitigate this challenge via in-context learning through relevant examples or by fine-tuning. However, it suffers from problems, such as limited DSL samples and prompt sensitivity but enterprises maintain good documentation of the DSLs. Therefore, we propose DocCGen, a framework that can leverage such rich knowledge by breaking the NL-to-Code generation task for structured code languages into a two-step process. First, it detects the correct libraries using the library documentation that best matches the NL query. Then, it utilizes schema rules extracted from the documentation of these libraries to constrain the decoding. We evaluate our framework for two complex structured languages, Ansible YAML and Bash command, consisting of two settings: Out-of-domain (OOD) and In domain (ID). Our extensive experiments show that DocCGen consistently improves different sized language models across all six evaluation metrics, reducing syntactic and semantic errors in structured code.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1040",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing": {
        "type": "INPROCEEDINGS",
        "key": "he-etal-2024-cocost",
        "author": "He, Xinyi and Zou, Jiaru and Lin, Yun and Zhou, Mengyu and Han, Shi and Yuan, Zejian and Zhang, Dongmei",
        "booktitle": "EMNLP2024",
        "title": "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CoCoST framework, which enhances complex code generation by online searching for more information with planned queries and correctness testing for code refinement. Moreover, CoCoST serializes the complex inputs and outputs to improve comprehension and generates test cases to ensure the adaptability for real-world applications. CoCoST is validated through rigorous experiments on the DS-1000 and ClassEval datasets. Experimental results show that CoCoST substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality of LLMs in generating complex code.",
        "labels": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1082",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "CodeJudge: Evaluating Code Generation with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "tong-zhang-2024-codejudge",
        "author": "Tong, Weixi and Zhang, Tianyi",
        "booktitle": "EMNLP2024",
        "title": "CodeJudge: Evaluating Code Generation with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown promising performance in code generation. However, how to reliably evaluate code generated by LLMs remains an unresolved problem. This paper presents CodeJudge, a code evaluation framework that leverages LLMs to evaluate the semantic correctness of generated code without the need for test cases. We investigate different ways to guide the LLM in performing \u201cslow thinking\u201d to arrive at an in-depth and reliable evaluation. We experimented with four LLMs as evaluators on four code generation datasets and five programming languages. The results show that CodeJudge significantly outperformed existing methods in most settings. Furthermore, compared with a SOTA GPT-3.5-based code evaluation method, CodeJudge achieved better results even when using a much smaller model, Llama-3-8B-Instruct. Our code and datasets are available on GitHub https://github.com/VichyTong/CodeJudge.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1118",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records": {
        "type": "INPROCEEDINGS",
        "key": "shi-etal-2024-ehragent",
        "author": "Shi, Wenqi and Xu, Ran and Zhuang, Yuchen and Yu, Yue and Zhang, Jieyu and Wu, Hang and Zhu, Yuanda and Ho, Joyce C. and Yang, Carl and Wang, May Dongmei",
        "booktitle": "EMNLP2024",
        "title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Clinicians often rely on data engineers to retrieve complex patient information from electronic health record (EHR) systems, a process that is both inefficient and time-consuming. We propose EHRAgent, a large language model (LLM) agent empowered with accumulative domain knowledge and robust coding capability. EHRAgent enables autonomous code generation and execution to facilitate clinicians in directly interacting with EHRs using natural language. Specifically, we formulate a multi-tabular reasoning task based on EHRs as a tool-use planning process, efficiently decomposing a complex task into a sequence of manageable actions with external toolsets. We first inject relevant medical information to enable EHRAgent to effectively reason about the given query, identifying and extracting the required records from the appropriate tables. By integrating interactive coding and execution feedback, EHRAgent then effectively learns from error messages and iteratively improves its originally generated code. Experiments on three real-world EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate, verifying its strong capacity to tackle complex clinical tasks with minimal demonstrations.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1245",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chae-etal-2024-language",
        "author": "Chae, Hyungjoo and Kim, Yeonghyeon and Kim, Seungone and Ong, Kai Tzu-iunn and Kwak, Beong-woo and Kim, Moohyeon and Kim, Sunghwan and Kwon, Taeyoon and Chung, Jiwan and Yu, Youngjae and Yeo, Jinyoung",
        "booktitle": "EMNLP2024",
        "title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Algorithmic reasoning tasks that involve complex logical patterns, such as completing Dyck language, pose challenges for large language models (LLMs), despite their recent success. Prior work has used LLMs to generate programming language and applied external compilers for such tasks. Yet, when on the fly, it is hard to generate an executable code with the correct logic for the solution. Even so, code for one instance cannot be reused for others, although they might require the same logic to solve. We present Think-and-Execute, a novel framework that improves LLMs\u2019 algorithmic reasoning: (1) In Think, we discover task-level logic shared across all instances, and express such logic with pseudocode; (2) In Execute, we tailor the task-level pseudocode to each instance and simulate the execution of it. Think-and-Execute outperforms several strong baselines (including CoT and PoT) in diverse algorithmic reasoning tasks. We manifest the advantage of using task-level pseudocode over generating instance-specific solutions one by one. Also, we show that pseudocode can better improve LMs\u2019 reasoning than natural language (NL) guidance, even though they are trained with NL instructions.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1253",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code": {
        "type": "INPROCEEDINGS",
        "key": "chae-etal-2024-coffee",
        "author": "Chae, Hyungjoo and Kwon, Taeyoon and Moon, Seungjun and Song, Yongho and Kang, Dongjin and Ong, Kai Tzu-iunn and Kwak, Beong-woo and Bae, Seonghyeon and Hwang, Seung-won and Yeo, Jinyoung",
        "booktitle": "EMNLP2024",
        "title": "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper presents Coffee-Gym, a comprehensive RL environment for training models that provide feedback on code editing. Coffee-Gym includes two major components: (1) Coffee, a dataset containing humans\u2019 code edit traces for coding questions and human-written feedback for editing erroneous code; (2) CoffeeEval, a reward function that faithfully reflects the helpfulness of feedback by assessing the performance of the revised code in unit tests. With them, Coffee-Gym addresses the unavailability of high-quality datasets for training feedback models with RL, and provides more accurate rewards than the SOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback models that outperform baselines in enhancing open-source code LLMs\u2019 code editing, making them comparable with closed-source LLMs. We make the dataset and the model checkpoint publicly available in https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym.",
        "labels": [
            "code generation",
            "program repair",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1254",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP2024"
    },
    "Security of Language Models for Code: A Systematic Literature Review": {
        "type": "article",
        "key": "chen2024security",
        "title": "Security of Language Models for Code: A Systematic Literature Review",
        "author": "Chen, Yuchen and Sun, Weisong and Fang, Chunrong and Chen, Zhenpeng and Ge, Yifei and Han, Tingxu and Zhang, Quanjun and Liu, Yang and Chen, Zhenyu and Xu, Baowen",
        "journal": "arXiv preprint arXiv:2410.15631",
        "year": "2024",
        "volume": "",
        "number": "",
        "venue": "arXiv2024",
        "abstract": "Language models for code (CodeLMs) have emerged as powerful tools for code-related tasks, outperforming traditional methods and standard machine learning approaches. However, these models are susceptible to security vulnerabilities, drawing increasing research attention from domains such as software engineering, artificial intelligence, and cybersecurity. Despite the growing body of research focused on the security of CodeLMs, a comprehensive survey in this area remains absent. To address this gap, we systematically review 67 relevant papers, organizing them based on attack and defense strategies. Furthermore, we provide an overview of commonly used language models, datasets, and evaluation metrics, and highlight open-source tools and promising directions for future research in securing CodeLMs.",
        "labels": [
            "code model",
            "code model security",
            "survey"
        ],
        "url": "https://arxiv.org/pdf/2410.15631"
    },
    "The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by LLMs": {
        "type": "article",
        "key": "Haonan_arXiv_2025",
        "title": "The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by LLMs",
        "author": "Haonan Li, Hang Zhang, Kexin Pei, Zhiyun Qian",
        "journal": "arXiv:2504.11711 ",
        "year": "2025",
        "volume": "",
        "number": "",
        "venue": "arXiv2025",
        "abstract": "Static analysis is a cornerstone for software vulnerability detection, yet it often struggles with the classic precision-scalability trade-off. In practice, such tools often produce high false positive rates, particularly in large codebases like the Linux kernel. This imprecision can arise from simplified vulnerability modeling and over-approximation of path and data constraints. While large language models (LLMs) show promise in code understanding, their naive application to program analysis yields unreliable results due to inherent reasoning limitations. We introduce BugLens, a post-refinement framework that significantly improves static analysis precision. BugLens guides an LLM to follow traditional analysis steps by assessing buggy code patterns for security impact and validating the constraints associated with static warnings. Evaluated on real-world Linux kernel bugs, BugLens raises precision from 0.10 (raw) and 0.50 (semi-automated refinement) to 0.72, substantially reducing false positives and revealing four previously unreported vulnerabilities. Our results suggest that a structured LLM-based workflow can meaningfully enhance the effectiveness of static analysis tools.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/abs/2504.11711"
    },
    "Show Me Why It’s Correct: Saving 1/3 of Debugging Time in Program Repair with Interactive Runtime Comparison": {
        "type": "article",
        "key": "Haonan_arXiv_2025",
        "title": "Show Me Why It’s Correct: Saving 1/3 of Debugging Time in Program Repair with Interactive Runtime Comparison",
        "author": "Ruixin Wang, Zhongkai Zhao, Le Fang, Nan Jiang, Yiling Lou, Lin Tan, Tianyi Zhang",
        "journal": "OOPSLA 2025",
        "year": "2025",
        "volume": "",
        "number": "",
        "venue": "OOPSLA2025",
        "abstract": "Automated Program Repair (APR) holds the promise of alleviating the burden of debugging and fixing software bugs. Despite this, developers still need to manually inspect each patch to confirm its correctness, which is tedious and time-consuming. This challenge is exacerbated in the presence of plausible patches, which accidentally pass test cases but may not correctly fix the bug. To address this challenge, we propose an interactive approach called iFix to facilitate patch understanding and comparison based on their runtime difference. iFix performs static analysis to identify runtime variables related to the buggy statement and captures their runtime values during execution for each patch. These values are then aligned across different patch candidates, allowing users to compare and contrast their runtime behavior. To evaluate iFix, we conducted a within-subjects user study with 28 participants. Compared with manual inspection and a state-of-the-art interactive patch filtering technique, iFix reduced participants’ task completion time by 36% and 33% while also improving their confidence by 50% and 20%, respectively. Besides, quantitative experiments demonstrate that iFix improves the ranking of correct patches by at least 39% compared with other patch ranking methods and is generalizable to different APR tools.",
        "labels": [
            "code generation",
            "program repair",
            "program testing",
            "debugging"
        ],
        "url": "https://dl.acm.org/doi/10.1145/3720510"
    },
    "LAMD: Context-driven Android Malware Detection and Classification with LLMs": {
        "type": "article",
        "key": "qian2025arXiv",
        "title": "LAMD: Context-driven Android Malware Detection and Classification with LLMs",
        "author": "Xingzhi Qian, Xinran Zheng, Yiling He, Shuo Yang, Lorenzo Cavallaro",
        "journal": "arXiv preprint arXiv:2502.13055v1",
        "year": "2025",
        "volume": "",
        "number": "",
        "venue": "arXiv2025",
        "abstract": "The rapid growth of mobile applications has escalated Android malware threats. Although there are numerous detection methods, they often struggle with evolving attacks, dataset biases, and limited explainability. Large Language Models (LLMs) offer a promising alternative with their zero-shot inference and reasoning capabilities. However, applying LLMs to Android malware detection presents two key challenges: (1)the extensive support code in Android applications, often spanning thousands of classes, exceeds LLMs' context limits and obscures malicious behavior within benign functionality; (2)the structural complexity and interdependencies of Android applications surpass LLMs' sequence-based reasoning, fragmenting code analysis and hindering malicious intent inference. To address these challenges, we propose LAMD, a practical context-driven framework to enable LLM-based Android malware detection. LAMD integrates key context extraction to isolate security-critical code regions and construct program structures, then applies tier-wise code reasoning to analyze application behavior progressively, from low-level instructions to high-level semantics, providing final prediction and explanation. A well-designed factual consistency verification mechanism is equipped to mitigate LLM hallucinations from the first tier. Evaluation in real-world settings demonstrates LAMD's effectiveness over conventional detectors, establishing a feasible basis for LLM-driven malware analysis in dynamic threat landscapes.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://www.arxiv.org/pdf/2502.13055"
    },
    "SpecRover: Code Intent Extraction via LLMs": {
        "type": "article",
        "key": "ruan2025ICSE",
        "title": "SpecRover: Code Intent Extraction via LLMs",
        "author": "Haifeng Ruan, Yuntong Zhang, Abhik Roychoudhury",
        "journal": "arXiv preprint arXiv:2408.02232v4",
        "year": "2025",
        "volume": "",
        "number": "",
        "venue": "ICSE2025",
        "abstract": "Autonomous program improvement typically involves automatically producing bug fixes and feature additions. Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches. In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent. Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior. The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches. Our approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent AutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub issues, it shows more than 50% improvement in efficacy over AutoCodeRover. Compared to the open-source agents available, our work shows modest cost ($0.65 per issue) in resolving an average GitHub issue in SWE-Bench lite. The production of explanation by SpecRover allows for a better \"signal\" to be given to the developer, on when the suggested patches can be accepted with confidence. SpecRover also seeks to demonstrate the continued importance of specification inference in automated program repair, even as program repair technologies enter the LLM era.",
        "labels": [
            "static analysis",
            "specification inference",
            "code generation",
            "program repair"
        ],
        "url": "https://arxiv.org/pdf/2408.02232"
    },
    "Large Language Models for Validating Network Protocol Parsers": {
        "type": "article",
        "key": "ParVal",
        "title": "Large Language Models for Validating Network Protocol Parsers",
        "author": "Mingwei Zheng, Danning Xie, Xiangyu Zhang",
        "journal": "LangSec2025",
        "year": "2025",
        "volume": "",
        "number": "",
        "venue": "LangSec2025",
        "abstract": "Network protocol parsers are essential for enabling correct and secure communication between devices. Bugs in these parsers can introduce critical vulnerabilities, including memory corruption, information leakage, and denial-of-service attacks. An intuitive way to assess parser correctness is to compare the implementation with its official protocol standard. However, this comparison is challenging because protocol standards are typically written in natural language, whereas implementations are in source code. Existing methods like model checking, fuzzing, and differential testing have been used to find parsing bugs, but they either require significant manual effort or ignore the protocol standards, limiting their ability to detect semantic violations. To enable more automated validation of parser implementations against protocol standards, we propose PARVAL, a multi-agent framework built on large language models (LLMs). PARVAL leverages the capabilities of LLMs to understand both natural language and code. It transforms both protocol standards and their implementations into a unified intermediate representation, referred to as format specifications, and performs a differential comparison to uncover inconsistencies. We evaluate PARVAL on the Bidirectional Forwarding Detection (BFD) protocol. Our experiments demonstrate that PARVAL successfully identifies inconsistencies between the implementation and its RFC standard, achieving a low false positive rate of 5.6%. PARVAL uncovers seven unique bugs, including five previously unknown issues.",
        "labels": [
            "static analysis",
            "specification inference",
            "bug detection"
        ],
        "url": "https://arxiv.org/abs/2504.13515"
    },
    "Language Models for Code Optimization: Survey, Challenges and Future Directions": {
        "type": "article",
        "key": "arxiv2025codeoptimization",
        "title": "Language Models for Code Optimization: Survey, Challenges and Future Directions",
        "author": "Jingzhi Gong, Vardan Voskanyan, Paul Brookes, Fan Wu, Wei Jie, Jie Xu, Rafail Giavrimis, Mike Basios, Leslie Kanthan, Zheng Wang",
        "journal": "arXiv preprint arXiv:2501.01277",
        "year": "2025",
        "volume": "",
        "number": "",
        "venue": "arXiv2025",
        "abstract": "Language models (LMs) built upon deep neural networks (DNNs) have recently demonstrated breakthrough effectiveness in software engineering tasks such as code generation, completion, and repair. This has paved the way for the emergence of LM-based code optimization techniques, which are crucial for enhancing the performance of existing programs, such as accelerating program execution time. However, a comprehensive survey dedicated to this specific application has been lacking. To fill this gap, we present a systematic literature review of over 50 primary studies, identifying emerging trends and addressing 11 specialized questions. Our findings reveal five critical open challenges, such as balancing model complexity with practical usability, cross-language/performance generalizability, and building trust in AI-driven solutions. Furthermore, we provide eight future research directions to facilitate more efficient, robust, and reliable LM-based code optimization. Thereby, this study aims to provide actionable insights and foundational references for both researchers and practitioners in this rapidly evolving field.",
        "labels": [
            "static analysis",
            "program optimization",
            "survey"
        ],
        "url": "https://arxiv.org/abs/2501.01277"
    },
    "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation": {
        "type": "article",
        "key": "crust_bench",
        "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
        "author": "Anirudh Khatry, Robert Zhang, Jia Pan, Ziteng Wang, Qiaochu Chen, Greg Durrett, Isil Dillig",
        "journal": "arXiv preprint arXiv:2504.15254",
        "year": "2025",
        "volume": "",
        "number": "",
        "venue": "arXiv2025",
        "abstract": "C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at this https URL.",
        "labels": [
            "benchmark",
            "code generation",
            "program transformation"
        ],
        "url": "https://arxiv.org/abs/2504.15254"
    },
    "DR.FIX: Automatically Fixing Data Races at Industry Scale": {
        "type": "article",
        "key": "DR_Fix",
        "title": "DR.FIX: Automatically Fixing Data Races at Industry Scale",
        "author": "Farnaz Behrang, Zhizhou Zhang, Georgian-Vlad Saioc, Peng Liu, Milind Chabbi",
        "journal": "arXiv preprint arXiv:2504.15637",
        "year": "2025",
        "volume": "",
        "number": "",
        "venue": "PLDI2025",
        "abstract": "Data races are a prevalent class of concurrency bugs in shared-memory parallel programs, posing significant challenges to software reliability and reproducibility. While there is an extensive body of research on detecting data races and a wealth of practical detection tools across various programming languages, considerably less effort has been directed toward automatically fixing data races at an industrial scale. In large codebases, data races are continuously introduced and exhibit myriad patterns, making automated fixing particularly challenging. In this paper, we tackle the problem of automatically fixing data races at an industrial scale. We present this http URL, a tool that combines large language models (LLMs) with program analysis to generate fixes for data races in real-world settings, effectively addressing a broad spectrum of racy patterns in complex code contexts. Implemented for Go--the programming language widely used in modern microservice architectures where concurrency is pervasive and data races are this http URL seamlessly integrates into existing development workflows. We detail the design of this http URL and examine how individual design choices influence the quality of the fixes produced. Over the past 18 months, this http URL has been integrated into developer workflows at Uber demonstrating its practical utility. During this period, this http URL produced patches for 224 (55%) from a corpus of 404 data races spanning various categories; 193 of these patches (86%) were accepted by more than a hundred developers via code reviews and integrated into the codebase.",
        "labels": [
            "code generation",
            "program repair"
        ],
        "url": "https://arxiv.org/abs/2504.15637"
    },
    "SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI": {
        "type": "article",
        "key": "secdoeplt",
        "title": "SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI",
        "author": "Yu Yang, Yuzhou Nie, Zhun Wang, Yuheng Tang, Wenbo Guo, Bo Li, Dawn Song",
        "journal": "arXiv preprint arXiv:2410.11096",
        "year": "2024",
        "volume": "",
        "number": "",
        "venue": "arXiv2024",
        "abstract": "Language models for code (CodeLMs) have emerged as powerful tools for code-related tasks, outperforming traditional methods and standard machine learning approaches. However, these models are susceptible to security vulnerabilities, drawing increasing research attention from domains such as software engineering, artificial intelligence, and cybersecurity. Despite the growing body of research focused on the security of CodeLMs, a comprehensive survey in this area remains absent. To address this gap, we systematically review 67 relevant papers, organizing them based on attack and defense strategies. Furthermore, we provide an overview of commonly used language models, datasets, and evaluation metrics, and highlight open-source tools and promising directions for future research in securing CodeLMs.",
        "labels": [
            "code generation",
            "program synthesis",
            "code model",
            "code model security",
            "benchmark"
        ],
        "url": "https://arxiv.org/abs/2410.11096"
    },
    "METAMON: Finding Inconsistencies between Program Documentation and Behavior using Metamorphic LLM Queries": {
        "type": "article",
        "key": "METAMON",
        "title": "METAMON: Finding Inconsistencies between Program Documentation and Behavior using Metamorphic LLM Queries",
        "author": "Hyeonseok Lee, Gabin An, Shin Yoo",
        "journal": "LLM4Code2025",
        "year": "2025",
        "volume": "",
        "number": "",
        "venue": "LLM4Code2025",
        "abstract": "Code documentation can, if written precisely, help developers better understand the code they accompany. However, unlike code, code documentation cannot be automatically verified via execution, potentially leading to inconsistencies between documentation and the actual behavior. While such inconsistencies can harmful for developer’s understanding of the code, checking and finding them remains a costly task due to the involvement of human engineers. This paper proposes METAMON, which uses an existing search-based test generation technique to capture the current program behavior in the form of test cases, and subsequently uses LLM-based code reasoning to identify the generated regression test oracles that are not consistent with the program specifications in the documentation. METAMON is supported in this task by metamorphic testing and self-consistency. An empirical evaluation against 9,482 pairs of code documentation and code snippets, generated using five opensource projects from Defects4J v2.0.1, shows that METAMON can classify the code-and-documentation inconsistencies with the precision of 0.72 and the recall of 0.48.",
        "labels": [
            "program testing",
            "differential testing"
        ],
        "url": "https://coinse.github.io/publications/pdfs/Lee2025aa.pdf"
    },
    "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent": {
        "type": "article",
        "key": "redagent",
        "title": "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent",
        "author": "Huiyu Xu, Wenhui Zhang, Zhibo Wang, Feng Xiao, Rui Zheng, Yunhe Feng, Zhongjie Ba, Kui Ren",
        "journal": "arXiv preprint arXiv:2407.16667",
        "year": "2024",
        "volume": "",
        "number": "",
        "venue": "arXiv2024",
        "abstract": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats. Among them, jailbreak attacks that induce toxic responses through jailbreak prompts have raised critical safety concerns. To identify these threats, a growing number of red teaming approaches simulate potential adversarial scenarios by crafting jailbreak prompts to test the target LLM. However, existing red teaming methods do not consider the unique vulnerabilities of LLM in different scenarios, making it difficult to adjust the jailbreak prompts to find context-specific vulnerabilities. Meanwhile, these methods are limited to refining jailbreak templates using a few mutation operations, lacking the automation and scalability to adapt to different scenarios. To enable context-aware and efficient red teaming, we abstract and model existing attacks into a coherent concept called \"jailbreak strategy\" and propose a multi-agent LLM system named RedAgent that leverages these strategies to generate context-aware jailbreak prompts. By self-reflecting on contextual feedback in an additional memory buffer, RedAgent continuously learns how to leverage these strategies to achieve effective jailbreaks in specific contexts. Extensive experiments demonstrate that our system can jailbreak most black-box LLMs in just five queries, improving the efficiency of existing red teaming methods by two times. Additionally, RedAgent can jailbreak customized LLM applications more efficiently. By generating context-aware jailbreak prompts towards applications on GPTs, we discover 60 severe vulnerabilities of these real-world applications with only two queries per vulnerability. We have reported all found issues and communicated with OpenAI and Meta for bug fixes.",
        "labels": [
            "code model",
            "code model security",
            "benchmark"
        ],
        "url": "https://arxiv.org/abs/2407.16667"
    },
    "Large Language Model assisted Hybrid Fuzzing": {
        "type": "article",
        "key": "llmfuzz",
        "title": "Large Language Model assisted Hybrid Fuzzing",
        "author": "Ruijie Meng, Gregory J. Duck, Abhik Roychoudhury",
        "journal": "arXiv:2412.15931v1",
        "year": "2024",
        "volume": "",
        "number": "",
        "venue": "arXiv2024",
        "abstract": "Greybox fuzzing is one of the most popular methods for detecting software vulnerabilities, which conducts a biased random search within the program input space. To enhance its effectiveness in achieving deep coverage of program behaviors, greybox fuzzing is often combined with concolic execution, which performs a path-sensitive search over the domain of program inputs. In hybrid fuzzing, conventional greybox fuzzing is followed by concolic execution in an iterative loop, where reachability roadblocks encountered by greybox fuzzing are tackled by concolic execution. However, such hybrid fuzzing still suffers from difficulties conventionally faced by symbolic execution, such as the need for environment modeling and system call support. In this work, we show how to achieve the effect of concolic execution without having to compute and solve symbolic path constraints. When coverage-based greybox fuzzing reaches a roadblock in terms of reaching certain branches, we conduct a slicing on the execution trace and suggest modifications of the input to reach the relevant branches. A Large Language Model (LLM) is used as a solver to generate the modified input for reaching the desired branches. Compared with both the vanilla greybox fuzzer AFL and hybrid fuzzers Intriguer and Qsym, our LLM-based hybrid fuzzer HyLLfuzz (pronounced \"hill fuzz\") demonstrates superior coverage. Furthermore, the LLM-based concolic execution in HyLLfuzz takes a time that is 4-19 times faster than the concolic execution running in existing hybrid fuzzing tools. This experience shows that LLMs can be effectively inserted into the iterative loop of hybrid fuzzers, to efficiently expose more program behaviors.",
        "labels": [
            "program testing",
            "fuzzing"
        ],
        "url": "https://doi.org/10.48550/arXiv.2412.15931"
    },
    "OpenAI’s Approach to External Red Teaming for AI Models and Systems": {
        "type": "article",
        "key": "openairedteaming",
        "title": "OpenAI’s Approach to External Red Teaming for AI Models and Systems",
        "author": "Lama Ahmad, Sandhini Agarwal, Michael Lampe, Pamela Mishkin",
        "journal": "technical report",
        "year": "2024",
        "volume": "",
        "number": "",
        "venue": "OpenAI2024",
        "abstract": "Red teaming has emerged as a critical practice in assessing the possible risks of AI models and systems. It aids in the discovery of novel risks, stress testing possible gaps in existing mitigations, enriching existing quantitative safety metrics, facilitating the creation of new safety measurements, and enhancing public trust and the legitimacy of AI risk assessments. This white paper describes OpenAI’s work to date in external red teaming and draws some more general conclusions from this work. We describe the design considerations underpinning external red teaming, which include: selecting composition of red team, deciding on access levels, and providing guidance required to conduct red teaming. Additionally, we show outcomes red teaming can enable such as input into risk assessment and automated evaluations. We also describe the limitations of external red teaming, and how it can fit into a broader range of AI model and system evaluations. Through these contributions, we hope that AI developers and deployers,evaluation creators, and policymakers will be able to better design red teaming campaigns and get a deeper look into how external red teaming can fit into model deployment and evaluation processes. These methods are evolving and the value of different methods continues to shift as the ecosystem around red teaming matures and models themselves improve as tools for red teaming.",
        "labels": [
            "code model",
            "code model security",
            "benchmark"
        ],
        "url": "https://cdn.openai.com/papers/openais-approach-to-external-red-teaming.pdf"
    },
    "LLMs: Understanding Code Syntax and Semantics for Code Analysis": {
        "type": "article",
        "key": "ma2023lms",
        "title": "LLMs: Understanding Code Syntax and Semantics for Code Analysis",
        "author": "Ma, Wei and Liu, Shangqing and Lin, Zhihao and Wang, Wenhan and Hu, Qiang and Liu, Ye and Zhang, Cen and Nie, Liming and Li, Li and Liu, Yang",
        "journal": "arXiv preprint arXiv:2305.12138",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "Large language models~(LLMs) demonstrate significant potential to revolutionize software engineering (SE) by exhibiting outstanding performance in SE tasks such as code and document generation. However, the high reliability and risk control requirements in software engineering raise concerns about the lack of interpretability of LLMs. To address this concern, we conducted a study to evaluate the capabilities of LLMs and their limitations for code analysis in SE. We break down the abilities needed for artificial intelligence~(AI) models to address SE tasks related to code analysis into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on the ability of LLMs to comprehend code syntax and semantic structures, which include abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We employed four state-of-the-art foundational models, GPT4, GPT3.5, StarCoder and CodeLlama-13b-instruct. We assessed the performance of LLMs on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while LLMs have a talent for understanding code syntax, they struggle with comprehending code semantics, particularly dynamic semantics. We conclude that LLMs possess capabilities similar to an Abstract Syntax Tree (AST) parser, demonstrating initial competencies in static code analysis. Furthermore, our study highlights that LLMs are susceptible to hallucinations when interpreting code semantic structures and fabricating nonexistent facts. These results indicate the need to explore methods to verify the correctness of LLM output to ensure its dependability in SE. More importantly, our study provides an initial answer to why the codes generated by LLM are usually syntax-correct but vulnerable.",
        "labels": [
            "static analysis",
            "data-flow analysis",
            "call graph analysis",
            "data-flow analysis",
            "code model",
            "code model training",
            "source code model",
            "empirical study"
        ],
        "url": "https://arxiv.org/abs/2305.12138"
    },
    "Codemind: A framework to challenge large language models for code reasoning": {
        "type": "article",
        "key": "liu2024codemind",
        "title": "Codemind: A framework to challenge large language models for code reasoning",
        "author": "Liu, Changshu and Zhang, Shizhuo Dylan and Ibrahimzada, Ali Reza and Jabbarvand, Reyhaneh",
        "journal": "arXiv preprint arXiv:2402.09664",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly follow control flow constructs and, in general, explain how inputs evolve to output, specifically for simple programs and the ones they can correctly synthesize. However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls. Furthermore, we observe that, while correlated, specification reasoning (essential for code synthesis) does not imply execution reasoning (essential for broader programming tasks such as testing and debugging): ranking LLMs based on test passing can be different compared to code reasoning.",
        "labels": [
            "general coding task",
            "empirical study"
        ],
        "url": "https://arxiv.org/pdf/2402.09664"
    },
    "Which Syntactic Capabilities Are Statistically Learned by Masked Language Models for Code?": {
        "type": "inproceedings",
        "key": "velasco2024syntactic",
        "title": "Which Syntactic Capabilities Are Statistically Learned by Masked Language Models for Code?",
        "author": "Velasco, Alejandro and Palacio, David N and Rodriguez-Cardenas, Daniel and Poshyvanyk, Denys",
        "booktitle": "Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results",
        "pages": "72--76",
        "year": "2024",
        "abstract": "This paper discusses the limitations of evaluating Masked Language Models (MLMs) in code completion tasks. We highlight that relying on accuracy-based measurements may lead to an overestimation of models' capabilities by neglecting the syntax rules of programming languages. To address these issues, we introduce a technique called SyntaxEval in which Syntactic Capabilities are used to enhance the evaluation of MLMs. SyntaxEval automates the process of masking elements in the model input based on their Abstract Syntax Trees (ASTs). We conducted a case study on two popular MLMs using data from GitHub repositories. Our results showed negative causal effects between the node types and MLMs' accuracy. We conclude that MLMs under study fail to predict some syntactic capabilities.",
        "venue": "ICSE2024",
        "labels": [
            "static analysis",
            "syntactic analysis",
            "empirical study"
        ],
        "url": "https://arxiv.org/pdf/2401.01512"
    },
    "Grounded Copilot: How Programmers Interact with Code-Generating Models": {
        "type": "article",
        "key": "10.1145/3586030",
        "author": "Barke, Shraddha and James, Michael B. and Polikarpova, Nadia",
        "title": "Grounded Copilot: How Programmers Interact with Code-Generating Models",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "7",
        "number": "OOPSLA1",
        "url": "https://doi.org/10.1145/3586030",
        "doi": "10.1145/3586030",
        "abstract": "Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants\u2014with a range of prior experience using the assistant\u2014as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "apr",
        "articleno": "78",
        "numpages": "27",
        "labels": [
            "code generation",
            "code completion",
            "empirical study"
        ],
        "venue": "OOPLSA2023"
    },
    "SemCoder: Training Code Language Models with Comprehensive Semantics": {
        "type": "article",
        "key": "ding2024semcoder",
        "title": "SemCoder: Training Code Language Models with Comprehensive Semantics",
        "author": "Ding, Yangruibo and Peng, Jinjun and Min, Marcus J and Kaiser, Gail and Yang, Junfeng and Ray, Baishakhi",
        "journal": "arXiv preprint arXiv:2406.01006",
        "year": "2024",
        "abstract": "Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states. This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for semantic understanding for complex tasks like debugging and program repair. We introduce a novel strategy, monologue reasoning, to train Code LLMs to reason comprehensive semantics, encompassing high-level functional descriptions, local execution effects of individual statements, and overall input/output behavior, thereby linking static code text with dynamic execution states. We begin by collecting PyX, a clean Python corpus of fully executable code samples with functional descriptions and test cases. We propose training Code LLMs not only to write code but also to understand code semantics by reasoning about key properties, constraints, and execution behaviors using natural language, mimicking human verbal debugging, i.e., rubber-duck debugging. This approach led to the development of SemCoder, a Code LLM with only 6.7B parameters, which shows competitive performance with GPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder achieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I (GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5-turbo: 59.0%). We also study the effectiveness of SemCoder's monologue-style execution reasoning compared to concrete scratchpad reasoning, showing that our approach integrates semantics from multiple dimensions more smoothly. Finally, we demonstrate the potential of applying learned semantics to improve Code LLMs' debugging and self-refining capabilities.",
        "venue": "NeurIPS2024",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://arxiv.org/pdf/2406.01006"
    },
    "CodeFort: Robust Training for Code Generation Models": {
        "type": "article",
        "key": "zhang2024codefort",
        "title": "CodeFort: Robust Training for Code Generation Models",
        "author": "Zhang, Yuhao and Wang, Shiqi and Qian, Haifeng and Wang, Zijian and Shang, Mingyue and Liu, Linbo and Gouda, Sanjay Krishna and Ray, Baishakhi and Ramanathan, Murali Krishna and Ma, Xiaofei and others",
        "journal": "arXiv preprint arXiv:2405.01567",
        "year": "2024",
        "abstract": "Code generation models are not robust to small perturbations, which often lead to inconsistent and incorrect generations and significantly degrade the performance of these models. Improving the robustness of code generation models is crucial to better user experience when these models are deployed in real-world applications. However, existing efforts have not addressed this issue for code generation models. To fill this gap, we propose CodeFort, a framework to improve the robustness of code generation models, generalizing a large variety of code perturbations to enrich the training data and enabling various robust training strategies, mixing data augmentation, batch augmentation, adversarial logits pairing, and contrastive learning, all carefully designed to support high-throughput training. Extensive evaluations show that we improve the average robust pass rates of baseline CodeGen models from 14.79 to 21.74. Notably, the improvement in robustness against code-syntax perturbations is evidenced by a significant decrease in pass rate drop from 95.04% to 53.35%",
        "venue": "EMNLP2024",
        "labels": [
            "code generation",
            "code model",
            "code model training",
            "code model",
            "code model robustness"
        ],
        "url": "https://arxiv.org/pdf/2405.01567"
    },
    "Constrained Decoding for Secure Code Generation": {
        "type": "article",
        "key": "fu2024constrained",
        "title": "Constrained Decoding for Secure Code Generation",
        "author": "Fu, Yanjun and Baker, Ethan and Ding, Yu and Chen, Yizheng",
        "journal": "arXiv preprint arXiv:2405.00218",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Code Large Language Models (Code LLMs) have been increasingly used by developers to boost productivity, but they often generate vulnerable code. Thus, there is an urgent need to ensure that code generated by Code LLMs is correct and secure. Previous research has primarily focused on generating secure code, overlooking the fact that secure code also needs to be correct. This oversight can lead to a false sense of security. Currently, the community lacks a method to measure actual progress in this area, and we need solutions that address both security and correctness of code generation. This paper introduces a new benchmark, CodeGuard+, along with two new metrics, to measure Code LLMs' ability to generate both secure and correct code. Using our new evaluation methods, we show that the state-of-the-art defense technique, prefix tuning, may not be as strong as previously believed, since it generates secure code but sacrifices functional correctness. We also demonstrate that different decoding methods significantly affect the security of Code LLMs. Furthermore, we explore a new defense direction: constrained decoding for secure code generation. We propose new constrained decoding techniques to generate secure code. Our results reveal that constrained decoding is more effective than prefix tuning to improve the security of Code LLMs, without requiring a specialized training dataset. Moreover, our evaluations over eight state-of-the-art Code LLMs show that constrained decoding has strong performance to improve the security of Code LLMs, and our technique outperforms GPT-4.",
        "labels": [
            "code generation",
            "code model",
            "code model security"
        ],
        "url": "https://arxiv.org/pdf/2405.00218"
    },
    "Instruction tuning for secure code generation": {
        "type": "article",
        "key": "he2024instruction",
        "title": "Instruction tuning for secure code generation",
        "author": "He, Jingxuan and Vero, Mark and Krasnopolska, Gabriela and Vechev, Martin",
        "journal": "arXiv preprint arXiv:2402.09497",
        "year": "2024",
        "abstract": "Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets. It is able to drastically improve security (by about 30%), while preserving utility.",
        "venue": "ICML2024",
        "labels": [
            "code generation",
            "code model",
            "code model security"
        ],
        "url": "https://arxiv.org/pdf/2405.00218"
    },
    "Large language models for code: Security hardening and adversarial testing": {
        "type": "inproceedings",
        "key": "he2023large",
        "title": "Large language models for code: Security hardening and adversarial testing",
        "author": "He, Jingxuan and Vechev, Martin",
        "booktitle": "Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "1865--1879",
        "year": "2023",
        "abstract": "Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN closely matches the original LMs in functional correctness.",
        "venue": "CCS2023",
        "labels": [
            "code generation",
            "code model",
            "code model security"
        ],
        "url": "https://arxiv.org/abs/2302.05319"
    },
    "Graphcodebert: Pre-training code representations with data flow": {
        "type": "article",
        "key": "guo2020graphcodebert",
        "title": "Graphcodebert: Pre-training code representations with data flow",
        "author": "Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others",
        "journal": "arXiv preprint arXiv:2009.08366",
        "year": "2020",
        "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.",
        "venue": "ICLR2021",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://arxiv.org/pdf/2302.05319"
    },
    "Codebert: A pre-trained model for programming and natural languages": {
        "type": "article",
        "key": "feng2020codebert",
        "title": "Codebert: A pre-trained model for programming and natural languages",
        "author": "Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others",
        "journal": "arXiv preprint arXiv:2002.08155",
        "year": "2020",
        "abstract": "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and nat-ural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language codesearch, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both bimodal data of NL-PL pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation tasks. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing.",
        "venue": "EMNLP2020",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://arxiv.org/pdf/2208.09727"
    },
    "Neural code comprehension: A learnable representation of code semantics": {
        "type": "article",
        "key": "ben2018neural",
        "title": "Neural code comprehension: A learnable representation of code semantics",
        "author": "Ben-Nun, Tal and Jakobovits, Alice Shoshana and Hoefler, Torsten",
        "journal": "Advances in neural information processing systems",
        "volume": "31",
        "year": "2018",
        "abstract": "With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human-and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data-and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.",
        "venue": "NeurIPS2018",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.5555/3327144.3327276"
    },
    "LLM Compiler: Foundation Language Models for Compiler Optimization": {
        "type": "article",
        "key": "cummins2024meta",
        "title": "LLM Compiler: Foundation Language Models for Compiler Optimization",
        "author": "Cummins, Chris and Seeker, Volker and Grubisic, Dejan and Roziere, Baptiste and Gehring, Jonas and Synnaeve, Gabriel and Leather, Hugh",
        "journal": "CC",
        "year": "2025",
        "venue": "CC2025",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce LLM Compiler, a suite of robust, openly available, pre-trained models specifically designed for compiler tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The models have been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and have undergone instruction fine-tuning to interpret compiler behavior. To demonstrate the utility of these research tools, we also present fine-tuned versions of the models with enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77 % of the optimising potential of an autotuning search, and 45 % disassembly round trip (14 % exact match). LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. Our aim is to provide scalable, cost-effective foundational models for further research and development in compiler optimization by both academic researchers and industry practitioners. Since we released LLM Compiler the community has quantized, repackaged, and downloaded the models over 250k times.",
        "labels": [
            "static analysis",
            "program optimization",
            "code model",
            "code model training",
            "IR code model"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3708493.3712691"
    },
    "CompilerGym: robust, performant compiler optimization environments for AI research": {
        "type": "article",
        "key": "CGO53902.2022.9741258",
        "title": "CompilerGym: robust, performant compiler optimization environments for AI research",
        "author": "Cummins, Chris and Wasti, Bram and Guo, Jiadong and Cui, Brandon and Ansel, Jason and Gomez, Sahir and Jain, Somya and Liu, Jia and Teytaud, Olivier and Steiner, Benoit and Tian, Yuandong and Leather, Hugh",
        "journal": "Proceedings of the 20th IEEE/ACM International Symposium on Code Generation and Optimization",
        "year": "2022",
        "venue": "CGO2022",
        "abstract": "Interest in applying Artificial Intelligence (AI) techniques to compiler optimizations is increasing rapidly, but compiler research has a high entry barrier. Unlike in other domains, compiler and AI researchers do not have access to the datasets and frameworks that enable fast iteration and development of ideas, and getting started requires a significant engineering investment. What is needed is an easy, reusable experimental infrastructure for real world compiler optimization tasks that can serve as a common benchmark for comparing techniques, and as a platform to accelerate progress in the field.We introduce CompilerGym, a set of environments for real world compiler optimization tasks, and a toolkit for exposing new optimization tasks to compiler researchers. CompilerGym enables anyone to experiment on production compiler optimization problems through an easy-to-use package, regardless of their experience with compilers. We build upon the popular OpenAI Gym interface enabling researchers to interact with compilers using Python and a familiar API.We describe the CompilerGym architecture and implementation, characterize the optimization spaces and computational efficiencies of three included compiler environments, and provide extensive empirical evaluations. Compared to prior works, CompilerGym offers larger datasets and optimization spaces, is 27X more computationally efficient, is fault-tolerant, and capable of detecting reproducibility bugs in the underlying compilers.In making it easy for anyone to experiment with compilers - irrespective of their background - we aim to accelerate progress in the AI and compiler research domains.",
        "labels": [
            "static analysis",
            "program optimization",
            "benchmark"
        ],
        "url": "https://doi.org/10.1109/CGO53902.2022.9741258"
    },
    "Magicoder: Empowering Code Generation with OSS-Instruct": {
        "type": "article",
        "key": "magicoder",
        "title": "Magicoder: Empowering Code Generation with OSS-Instruct",
        "author": "Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang",
        "journal": "arXiv preprint arXiv:2312.02120",
        "year": "2024",
        "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.",
        "venue": "ICML2024",
        "labels": [
            "code generation",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://arxiv.org/abs/2312.02120"
    },
    "Symmetry-Preserving Program Representations for Learning Code Semantics": {
        "type": "article",
        "key": "pei2023symmetry",
        "title": "Symmetry-Preserving Program Representations for Learning Code Semantics",
        "author": "Pei, Kexin and Li, Weichen and Jin, Qirui and Liu, Shuyang and Geng, Scott and Cavallaro, Lorenzo and Yang, Junfeng and Jana, Suman",
        "journal": "arXiv preprint arXiv:2308.03312",
        "year": "2023",
        "venue": "ICML2024",
        "abstract": "Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures. Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiveness in generalization and robustness through detailed experimental evaluations across different binary and source code analysis tasks. Overall, our code symmetry framework offers rigorous and powerful reasoning techniques that can guide the future development of specialized LLMs for code and advance LLM-guided program reasoning tasks.",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://arxiv.org/pdf/2308.03312"
    },
    "Fair: Flow type-aware pre-training of compiler intermediate representations": {
        "type": "inproceedings",
        "key": "niu2024fair",
        "title": "Fair: Flow type-aware pre-training of compiler intermediate representations",
        "author": "Niu, Changan and Li, Chuanyi and Ng, Vincent and Lo, David and Luo, Bin",
        "booktitle": "Proceedings of the 46th IEEE/ACM International Conference on Software Engineering",
        "pages": "1--12",
        "year": "2024",
        "abstract": "While the majority of existing pre-trained models from code learn source code features such as code tokens and abstract syntax trees, there are some other works that focus on learning from compiler intermediate representations (IRs). Existing IR-based models typically utilize IR features such as instructions, control and data flow graphs (CDFGs), call graphs, etc. However, these methods confuse variable nodes and instruction nodes in a CDFG and fail to distinguish different types of flows, and the neural networks they use fail to capture long-distance dependencies and have over-smoothing and over-squashing problems. To address these weaknesses, we propose FAIR, a Flow type-Aware pre-trained model for IR that involves employing (1) a novel input representation of IR programs; (2) Graph Transformer to address over-smoothing, over-squashing and long-dependencies problems; and (3) five pre-training tasks that we specifically propose to enable FAIR to learn the semantics of IR tokens, flow type information, and the overall representation of IR. Experimental results show that FAIR can achieve state-of-the-art results on four code-related downstream tasks.",
        "venue": "ICSE2024",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "IR code model"
        ],
        "url": "https://arxiv.org/pdf/2308.03312"
    },
    "How could neural networks understand programs?": {
        "type": "inproceedings",
        "key": "peng2021could",
        "title": "How could neural networks understand programs?",
        "author": "Peng, Dinglan and Zheng, Shuxin and Li, Yatao and Ke, Guolin and He, Di and Liu, Tie-Yan",
        "booktitle": "International Conference on Machine Learning",
        "pages": "8476--8486",
        "year": "2021",
        "organization": "PMLR",
        "venue": "ICML2021",
        "abstract": "Semantic understanding of programs is a fundamental problem for programming language processing (PLP). Recent works that learn representations of code based on pre-training techniques in NLP have pushed the frontiers in this direction. However, the semantics of PL and NL have essential differences. These being ignored, we believe it is difficult to build a model to better understand programs, by either directly applying off-the-shelf NLP pre-training techniques to the source code, or adding features to the model by the heuristic. In fact, the semantics of a program can be rigorously defined by formal semantics in PL theory. For example, the operational semantics, describes the meaning of a valid program as updating the environment (ie, the memory address-value function) through fundamental operations, such as memory I/O and conditional branching. Inspired by this, we propose a novel program semantics learning paradigm, that the model should learn from information composed of (1) the representations which align well with the fundamental operations in operational semantics, and (2) the information of environment transition, which is indispensable for program understanding. To validate our proposal, we present a hierarchical Transformer-based pre-training model called OSCAR to better facilitate the understanding of programs. OSCAR learns from intermediate representation (IR) and an encoded representation derived from static analysis, which are used for representing the fundamental operations and approximating the environment transitions respectively. OSCAR empirically shows the outstanding capability of program semantics understanding on many practical software engineering tasks. Code and models are released at:\\url {https://github. com/pdlan/OSCAR}.",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "IR code model"
        ],
        "url": "https://arxiv.org/abs/2105.04297"
    },
    "QEDCartographer: Automating Formal Verification Using Reward-Free Reinforcement Learning": {
        "type": "inproceedings",
        "key": "QEDCartographer",
        "title": "QEDCartographer: Automating Formal Verification Using Reward-Free Reinforcement Learning",
        "author": "Alex Sanchez-Stern, Abhishek Varghese, Zhanna Kaufman, Dylan Zhang, Talia Ringer, Yuriy Brun",
        "booktitle": "ICSE",
        "year": "2025",
        "organization": "ICSE",
        "venue": "ICSE2025",
        "abstract": "Formal verification is a promising method for producing reliable software, but the difficulty of manually writing verification proofs severely limits its utility in practice. Recent methods have automated some proof synthesis by guiding a search through the proof space using a theorem prover. Unfortunately, the theorem prover provides only the crudest estimate of progress, resulting in effectively undirected search. To address this problem, we create QEDCartographer, an automated proof-synthesis tool that combines supervised and reinforcement learning to more effectively explore the proof space. QEDCartographer incorporates the proofs' branching structure, enabling reward-free search and overcoming the sparse reward problem inherent to formal verification. We evaluate QEDCartographer using the CoqGym benchmark of 68.5K theorems from 124 open-source Coq projects. QEDCartographer fully automatically proves 21.4% of the test-set theorems. Previous search-based proof-synthesis tools Tok, Tac, ASTactic, Passport, and Proverbot9001, which rely only on supervised learning, prove 9.6%, 9.8%, 10.9%, 12.5%, and 19.8%, respectively. Diva, which combines 62 tools, proves 19.2%. Comparing to the most effective prior tool, Proverbot9001, QEDCartographer produces 34% shorter proofs 29% faster, on average over the theorems both tools prove. Together, QEDCartographer and non-learning-based CoqHammer prove 30.3% of the theorems, while CoqHammer alone proves 26.6%. Our work demonstrates that reinforcement learning is a fruitful research direction for improving proof-synthesis tools' search mechanisms.",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "url": "https://arxiv.org/abs/2408.09237"
    },
    "Programl: A graph-based program representation for data flow analysis and compiler optimizations": {
        "type": "inproceedings",
        "key": "cummins2021programl",
        "title": "Programl: A graph-based program representation for data flow analysis and compiler optimizations",
        "author": "Cummins, Chris and Fisches, Zacharias V and Ben-Nun, Tal and Hoefler, Torsten and O\u2019Boyle, Michael FP and Leather, Hugh",
        "booktitle": "International Conference on Machine Learning",
        "pages": "2244--2253",
        "year": "2021",
        "organization": "PMLR",
        "venue": "ICML2021",
        "abstract": "Machine learning (ML) is increasingly seen as a viable approach for building compiler optimization heuristics, but many ML methods cannot replicate even the simplest of the data flow analyses that are critical to making good optimization decisions. We posit that if ML cannot do that, then it is insufficiently able to reason about programs. We formulate data flow analyses as supervised learning tasks and introduce a large open dataset of programs and their corresponding labels from several analyses. We use this dataset to benchmark ML methods and show that they struggle on these fundamental program reasoning tasks. We propose ProGraML-Program Graphs for Machine Learning-a language-independent, portable representation of program semantics. ProGraML overcomes the limitations of prior works and yields improved performance on downstream optimization tasks.",
        "labels": [
            "static analysis",
            "data-flow analysis",
            "program optimization",
            "code model",
            "code model training",
            "IR code model"
        ],
        "url": "https://arxiv.org/abs/2105.04297"
    },
    "ReSym: Harnessing LLMs to Recover Variable and Data Structure Symbols from Stripped Binaries": {
        "type": "inproceedings",
        "key": "xie2024resym",
        "title": "ReSym: Harnessing LLMs to Recover Variable and Data Structure Symbols from Stripped Binaries",
        "author": "Xie, Danning and Zhang, Zhuo and Jiang, Nan and Xu, Xiangzhe and Tan, Lin and Zhang, Xiangyu",
        "booktitle": "Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security (CCS)",
        "year": "2024",
        "venue": "CCS2024",
        "abstract": "Decompilation aims to recover a binary executable to the source code form and hence has a wide range of applications in cyber security, such as malware analysis and legacy code hardening. A prominent challenge is to recover variable symbols, including both primitive and complex types such as user-defined data structures, along with their symbol information such as names and types. Existing efforts focus on solving parts of the problem, eg, recovering only types (without names) or only local variables (without user-defined structures). In this paper, we propose ReSym, a novel hybrid technique that combines Large Language Models (LLMs) and program analysis to recover both names and types for local variables and user-defined data structures. Our method encompasses fine-tuning two LLMs to handle local variables and structures, respectively. To overcome the inherent token limitations in current LLMs, we devise a novel Prolog-based algorithm to aggregate and cross-check results from multiple LLM queries, suppressing uncertainty and hallucinations. Our experiments show that ReSym is effective in recovering variable information and user-defined data structures, substantially outperforming the state-of-the-art methods.",
        "labels": [
            "code model",
            "code model training",
            "binary code model",
            "static analysis",
            "program decompilation"
        ],
        "url": "https://www.cs.purdue.edu/homes/lintan/publications/resym-ccs24.pdf"
    },
    "Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases": {
        "type": "article",
        "key": "su2024source",
        "title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases",
        "author": "Su, Zian and Xu, Xiangzhe and Huang, Ziyang and Zhang, Kaiyuan and Zhang, Xiangyu",
        "journal": "arXiv preprint arXiv:2405.19581",
        "year": "2024",
        "venue": "NeurIPS2024",
        "abstract": "Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a GPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.",
        "labels": [
            "code model",
            "code model training",
            "binary code model"
        ],
        "url": "https://openreview.net/pdf?id=qPpVDzPhSL"
    },
    "Codeart: Better code models by attention regularization when symbols are lacking": {
        "type": "article",
        "key": "su2024codeart",
        "title": "Codeart: Better code models by attention regularization when symbols are lacking",
        "author": "Su, Zian and Xu, Xiangzhe and Huang, Ziyang and Zhang, Zhuo and Ye, Yapeng and Huang, Jianjun and Zhang, Xiangyu",
        "journal": "Proceedings of the ACM on Software Engineering",
        "volume": "1",
        "number": "FSE",
        "pages": "562--585",
        "year": "2024",
        "publisher": "ACM New York, NY, USA",
        "venue": "FSE2024",
        "abstract": "Transformer based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more important compared to others. To realize the idea, we enhance the vanilla tokenization and model architecture of a BERT model, construct and utilize attention masks, and introduce a new pre-training algorithm. We pre-train this BERT-like model from scratch, using a dataset of 26 million stripped binary functions with explicit program dependence information extracted by our tool. We apply the model in three downstream tasks: binary similarity, type inference, and malware family classification. Our pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49% to 60%, and 74% to 94%, respectively. It also substantially outperforms other general pre-training techniques of code understanding models.",
        "labels": [
            "code model",
            "code model training",
            "binary code model"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3643752"
    },
    "Lmpa: Improving decompilation by synergy of large language model and program analysis": {
        "type": "article",
        "key": "xu2023lmpa",
        "title": "Lmpa: Improving decompilation by synergy of large language model and program analysis",
        "author": "Xu, Xiangzhe and Zhang, Zhuo and Feng, Shiwei and Ye, Yapeng and Su, Zian and Jiang, Nan and Cheng, Siyuan and Tan, Lin and Zhang, Xiangyu",
        "journal": "arXiv preprint arXiv:2306.02546",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "Decompilation aims to recover the source code form of a binary executable. It has many applications in security and software engineering such as malware analysis, vulnerability detection and code reuse. A prominent challenge in decompilation is to recover variable names. We propose a novel method that leverages the synergy of large language model (LLM) and program analysis. Language models encode rich multi-modal knowledge, but its limited input size prevents providing sufficient global context for name recovery. We propose to divide the task to many LLM queries and use program analysis to correlate and propagate the query results, which in turn improves the performance of LLM by providing additional contextual information. Our results show that 75% of the recovered names are considered good by users and our technique outperforms the state-of-the-art technique by 16.5% and 20.23% in precision and recall, respectively.",
        "labels": [
            "static analysis",
            "program decompilation",
            "code model",
            "code model training",
            "binary code model"
        ],
        "url": "https://arxiv.org/pdf/2306.02546v1"
    },
    "Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning": {
        "type": "article",
        "key": "nan2023nova",
        "title": "Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning",
        "author": "Nan Jiang, Chengxiao Wang, Kevin Liu, Xiangzhe Xu, Lin Tan, Xiangyu Zhang, and Petr Babkin",
        "journal": "arXiv preprint arXiv:2311.13721",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code due to the unique challenges of assembly: (1) the low information density of assembly and (2) the diverse optimizations in assembly code. To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively and designs contrastive learning objectives to train LLMs to learn assembly optimization. Equipped with these techniques, this work develops Nova, a generative LLM for assembly code. Nova outperforms existing techniques on binary code decompilation by up to 14.84 -- 21.58% (absolute percentage point improvement) higher Pass@1 and Pass@10, and outperforms the latest binary code similarity detection techniques by up to 6.17% Recall@1, showing promising abilities on both assembly generation and understanding tasks.",
        "labels": [
            "static analysis",
            "program decompilation",
            "static analysis",
            "code similarity analysis",
            "code model",
            "code model training",
            "binary code model"
        ],
        "url": "https://arxiv.org/pdf/2311.13721"
    },
    "Is Your Benchmark (Still) Useful? Dynamic Benchmarking for Code Language Models": {
        "type": "article",
        "key": "shaohua_arxiv25",
        "title": "Is Your Benchmark (Still) Useful? Dynamic Benchmarking for Code Language Models",
        "author": "Batu Guan, Xiao Wu, Yuanyuan Yuan, Shaohua Li",
        "journal": "arXiv preprint arXiv:2503.06643",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "In this paper, we tackle a critical challenge in model evaluation: how to keep code benchmarks useful when models might have already seen them during training. We introduce a novel solution, dynamic benchmarking framework, to address this challenge. Given a code understanding or reasoning benchmark, our framework dynamically transforms each input, i.e., programs, with various semantic-preserving mutations to build a syntactically new while semantically identical benchmark. We evaluated ten popular language models on our dynamic benchmarks. Our evaluation reveals several interesting or surprising findings: (1) all models perform significantly worse than before, (2) the ranking between some models shifts dramatically, and (3) our dynamic benchmarks can resist against the data contamination problem.",
        "labels": [
            "benchmark"
        ],
        "url": "https://arxiv.org/abs/2503.06643"
    },
    "KNighter: Transforming Static Analysis with LLM-Synthesized Checkers": {
        "type": "article",
        "key": "knighter_2025",
        "title": "KNighter: Transforming Static Analysis with LLM-Synthesized Checkers",
        "author": "Chenyuan Yang, Zijie Zhao, Zichen Xie, Haoyu Li, Lingming Zhang",
        "journal": "arXiv preprint arXiv:2503.09002v1",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "Static analysis is a powerful technique for bug detection in critical systems like operating system kernels. However, designing and implementing static analyzers is challenging, timeconsuming, and typically limited to predefined bug patterns. While large language models (LLMs) have shown promise for static analysis, directly applying them to scan large codebases remains impractical due to computational constraints and contextual limitations. We present KNighter, the first approach that unlocks practical LLM-based static analysis by automatically synthesizing static analyzers from historical bug patterns. Rather than using LLMs to directly analyze massive codebases, our key insight is leveraging LLMs to generate specialized static analyzers guided by historical patch knowledge. KNighter implements this vision through a multi-stage synthesis pipeline that validates checker correctness against original patches and employs an automated refinement process to iteratively reduce false positives. Our evaluation on the Linux kernel demonstrates that KNighter generates high-precision checkers capable of detecting diverse bug patterns overlooked by existing human-written analyzers. To date, KNighter-synthesized checkers have discovered 70 new bugs/vulnerabilities in the Linux kernel, with 56 confirmed and 41 already fixed. 11 of these findings have been assigned CVE numbers. This work establishes an entirely new paradigm for scalable, reliable, and traceable LLM-based static analysis for real-world systems via checker synthesis.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/pdf/2503.09002"
    },
    "Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of LLMs on Formal Specification Inference": {
        "type": "article",
        "key": "knighter_2025",
        "title": "Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of LLMs on Formal Specification Inference",
        "author": "Thanh Le-Cong, Bach Le, Toby Murray",
        "journal": "arXiv preprint arXiv:2503.04779",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "Large Language Models (LLMs) are increasingly being used to automate programming tasks. Yet, LLMs' capabilities in reasoning about program semantics are still inadequately studied, leaving significant potential for further exploration. This paper introduces FormalBench, a comprehensive benchmark designed to evaluate LLMs' reasoning abilities on program semantics, particularly via the task of synthesizing formal program specifications to assist verifying program correctness. This task requires both comprehensive reasoning over all possible program executions and the generation of precise, syntactically correct expressions that adhere to formal syntax and semantics. Using this benchmark, we evaluated the ability of LLMs in synthesizing consistent and complete specifications. Our findings show that LLMs perform well with simple control flows but struggle with more complex structures, especially loops, even with advanced prompting. Additionally, LLMs exhibit limited robustness against semantic-preserving transformations. We also highlight common failure patterns and design self-repair prompts, improving success rates by 25%.",
        "labels": [
            "static analysis",
            "specification inference",
            "benchmark",
            "empirical study"
        ],
        "url": "https://arxiv.org/abs/2503.04779"
    },
    "Type-Aware Constraining for Code LLMs": {
        "type": "article",
        "key": "knighter_2025",
        "title": "Type-Aware Constraining for Code LLMs",
        "author": "Niels Mündler, Jingxuan He, Hao Wang, Koushik Sen, Dawn Song, Martin Vechev",
        "journal": "ICLR 2025 workshop",
        "year": "2025",
        "venue": "ICLR2025",
        "abstract": "Large Language Models (LLMs) have achieved notable success in code generation. However, they still frequently produce invalid code, as they do not precisely model formal aspects of programming languages. Constrained decoding is a promising approach to alleviate this issue and has been successfully applied to domain-specific languages and syntactic features, but is not able to enforce more semantic features, such as well-typedness. To address this issue, we introduce type-aware constrained decoding. We develop a novel prefix automata formalism and introduce a sound approach to guarantee existence of a type-safe completion of a partial program based on type inference and a search over inhabitable types. We implement type-aware constraining first for a foundational simply-typed language, then extend it to TypeScript. In our evaluation across state-of-the-art open-weight LLMs of up to 34B parameters and various model families, type-aware constraining reduces compilation errors by on average 70.9% and increases functional correctness by 16.2% in code synthesis, translation, and repair tasks.",
        "labels": [
            "code generation",
            "code completion",
            "code model"
        ],
        "url": "https://openreview.net/forum?id=DNAapYMXkc"
    },
    "Evaluating the Effectiveness of Small Language Models in Detecting Refactoring Bugs": {
        "type": "article",
        "key": "Rohit2025arXiv",
        "title": "Evaluating the Effectiveness of Small Language Models in Detecting Refactoring Bugs",
        "author": "Rohit Gheyi, Marcio Ribeiro, Jonhnanthan Oliveira",
        "journal": "arXiv preprint arXiv:2502.18454",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "Popular IDEs frequently contain bugs in their refactoring implementations. Ensuring that a transformation preserves a program's behavior is a complex task. Traditional detection methods rely on predefined preconditions for each refactoring type, limiting their scalability and adaptability to new transformations. These methods often require extensive static and dynamic analyses, which are computationally expensive, time-consuming, and may still fail to detect certain refactoring bugs. This study evaluates the effectiveness of Small Language Models (SLMs) in detecting two types of refactoring bugs in Java and Python: (i) transformations that introduce errors or behavioral changes (Type I) and (ii) transformations unnecessarily blocked by IDEs despite being valid (Type II). We assess whether Llama 3.2 3B, Mistral 7B, Gemma 2 9B, DeepSeek-R1 14B, Phi-4 14B, o1-mini, and o3-mini-high can accurately detect 100 refactoring bugs reported in widely used Java and Python IDEs, such as Eclipse and NetBeans. The study covers 16 refactoring types and employs zero-shot prompting on consumer-grade hardware to evaluate the models' ability to reason about refactoring correctness without explicit prior training. The proprietary o3-mini-high model achieved the highest detection rate, identifying 84.3% of Type I bugs. The open-source Phi-4 14B performed comparably well, demonstrating strong effectiveness across both bug types. However, o3-mini-high struggled with Type II bugs, correctly identifying and applying valid but blocked transformations in only 40% of cases. The findings highlight the potential of SLMs for efficiently detecting refactoring bugs, particularly in verifying behavioral changes. Additionally, SLMs offer a more adaptable solution capable of generalizing across different refactoring types and programming languages, addressing key limitations of traditional approaches.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/abs/2502.18454"
    },
    "Combining Large Language Models with Static Analyzers for Code Review Generation": {
        "type": "article",
        "key": "Rohit2025arXiv",
        "title": "Evaluating the Effectiveness of Small Language Models in Detecting Refactoring Bugs",
        "author": "Rohit Gheyi, Marcio Ribeiro, Jonhnanthan Oliveira",
        "journal": "arXiv preprint arXiv:2502.06633",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "Code review is a crucial but often complex, subjective, and time-consuming activity in software development. Over the past decades, significant efforts have been made to automate this process. Early approaches focused on knowledge-based systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases. More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision. In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews. Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO). We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a real-world dataset. Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/pdf/2502.06633"
    },
    "ClassInvGen: Class Invariant Synthesis using Large Language Models": {
        "type": "article",
        "key": "chunyue2025arXiv",
        "title": "ClassInvGen: Class Invariant Synthesis using Large Language Models",
        "author": "Chuyue Sun, Viraj Agashe, Saikat Chakraborty, Jubi Taneja, Clark Barrett, David Dill, Xiaokang Qiu, Shuvendu K. Lahiri",
        "journal": "arXiv preprint arXiv:2502.18917",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "Formal program specifications in the form of preconditions, postconditions, and class invariants have several benefits for the construction and maintenance of programs. They not only aid in program understanding due to their unambiguous semantics but can also be enforced dynamically (or even statically when the language supports a formal verifier). However, synthesizing high-quality specifications in an underlying programming language is limited by the expressivity of the specifications or the need to express them in a declarative manner. Prior work has demonstrated the potential of large language models (LLMs) for synthesizing high-quality method pre/postconditions for Python and Java, but does not consider class invariants.\n In this work, we describe ClassInvGen, a method for co-generating executable class invariants and test inputs to produce high-quality class invariants for a mainstream language such as C++, leveraging LLMs' ability to synthesize pure functions. We show that ClassInvGen outperforms a pure LLM-based technique to generate specifications (from code) as well as prior data-driven invariant inference techniques such as Daikon. We contribute a benchmark of standard C++ data structures along with a harness that can help measure both the correctness and completeness of generated specifications using tests and mutants. We also demonstrate its applicability to real-world code by performing a case study on several classes within a widely used and high-integrity C++ codebase.",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "url": "https://www.arxiv.org/abs/2502.18917"
    },
    "Refining Decompiled C Code with Large Language Models": {
        "type": "article",
        "key": "ken2023",
        "title": "Refining Decompiled C Code with Large Language Models",
        "author": "Wai Kin Wong, Huaijin Wang, Zongjie Li, Zhibo Liu, Shuai Wang, Qiyi Tang, Sen Nie, and Shi Wu",
        "journal": "arXiv preprint arXiv:2310.06530",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "A C decompiler converts an executable into source code. The recovered C source code, once re-compiled, is expected to produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications. Despite the prosperous development of C decompilers, it is widely acknowledged that decompiler outputs are mainly used for human consumption, and are not suitable for automatic recompilation. Often, a substantial amount of manual effort is required to fix the decompiler outputs before they can be recompiled and executed properly. This paper is motived by the recent success of large language models (LLMs) in comprehending dense corpus of natural language. To alleviate the tedious, costly and often error-prone manual effort in fixing decompiler outputs, we investigate the feasibility of using LLMs to augment decompiler outputs, thus delivering recompilable decompilation. Note that different from previous efforts that focus on augmenting decompiler outputs with higher readability (e.g., recovering type/variable names), we focus on augmenting decompiler outputs with recompilability, meaning to generate code that can be recompiled into an executable with the same functionality as the original executable. We conduct a pilot study to characterize the obstacles in recompiling the outputs of the de facto commercial C decompiler -- IDA-Pro. We then propose a two-step, hybrid approach to augmenting decompiler outputs with LLMs. We evaluate our approach on a set of popular C test cases, and show that our approach can deliver a high recompilation success rate to over 75% with moderate effort, whereas none of the IDA-Pro's original outputs can be recompiled. We conclude with a discussion on the limitations of our approach and promising future research directions.",
        "labels": [
            "static analysis",
            "program decompilation"
        ],
        "url": "https://arxiv.org/abs/2310.06530"
    },
    "Jtrans: Jump-aware transformer for binary code similarity detection": {
        "type": "inproceedings",
        "key": "wang2022jtrans",
        "title": "Jtrans: Jump-aware transformer for binary code similarity detection",
        "author": "Wang, Hao and Qu, Wenjie and Katz, Gilad and Zhu, Wenyu and Gao, Zeyu and Qiu, Han and Zhuge, Jianwei and Zhang, Chao",
        "booktitle": "Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1--13",
        "year": "2022",
        "venue": "ISSTA2022",
        "abstract": "Binary code similarity detection (BCSD) has important applications in various fields such as vulnerabilities detection, software component analysis, and reverse engineering. Recent studies have shown that deep neural networks (DNNs) can comprehend instructions or control-flow graphs (CFG) of binary code and support BCSD. In this study, we propose a novel Transformer-based approach, namely jTrans, to learn representations of binary code. It is the first solution that embeds control flow information of binary code into Transformer-based language models, by using a novel jump-aware representation of the analyzed binaries and a newly-designed pre-training task. Additionally, we release to the community a newly-created large dataset of binaries, BinaryCorp, which is the most diverse to date. Evaluation results show that jTrans outperforms state-of-the-art (SOTA) approaches on this more challenging dataset by 30.5% (i.e., from 32.0% to 62.5%). In a real-world task of known vulnerability searching, jTrans achieves a recall that is 2X higher than existing SOTA baselines.",
        "labels": [
            "static analysis",
            "bug detection",
            "code model",
            "code model training",
            "binary code model"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3533767.3534367"
    },
    "Swe-bench: Can language models resolve real-world github issues?": {
        "type": "article",
        "key": "jimenez2023swe",
        "title": "Swe-bench: Can language models resolve real-world github issues?",
        "author": "Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik",
        "journal": "arXiv preprint arXiv:2310.06770",
        "year": "2024",
        "venue": "ICLR2024",
        "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",
        "labels": [
            "benchmark",
            "code generation",
            "program repair"
        ],
        "url": "https://arxiv.org/pdf/2310.06770"
    },
    "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories": {
        "type": "article",
        "key": "li2024evocodebench",
        "title": "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories",
        "author": "Li, Jia and Li, Ge and Zhang, Xuanming and Dong, Yihong and Jin, Zhi",
        "journal": "arXiv preprint arXiv:2404.00599",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repository-level code generation and evaluate 10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa, Gemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 only is 20.73% in our experiments. We also analyze failed cases and summarize the shortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.",
        "labels": [
            "benchmark",
            "code generation"
        ],
        "url": "https://arxiv.org/pdf/2404.00599"
    },
    "CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks": {
        "type": "article",
        "key": "xie2024codebenchgen",
        "title": "CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks",
        "author": "Xie, Yiqing and Xie, Alex and Sheth, Divyanshu and Liu, Pengfei and Fried, Daniel and Rose, Carolyn",
        "journal": "arXiv preprint arXiv:2404.00566",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "To facilitate evaluation of code generation systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based benchmarks that only requires light guidance from humans. Specifically, we leverage a large language model (LLM) to convert an arbitrary piece of code into an evaluation example, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries revised from code in 367 GitHub repositories taken from the CodeSearchNet dataset. To demonstrate the complexity and solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as ``requires effort to solve''. We conduct code generation experiments on open-source and proprietary models and analyze the performance of both humans and models. We will release the code of both the framework and the dataset upon acceptance.",
        "labels": [
            "code generation",
            "benchmark"
        ],
        "url": "https://arxiv.org/pdf/2404.00566"
    },
    "A Survey on Large Language Models for Code Generation": {
        "type": "article",
        "key": "jiang2024survey",
        "title": "A Survey on Large Language Models for Code Generation",
        "author": "Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun",
        "journal": "arXiv preprint arXiv:2406.00515",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels of difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (this https URL) to continuously document and disseminate the most recent advances in the field.",
        "labels": [
            "survey",
            "code generation"
        ],
        "url": "https://arxiv.org/pdf/2406.00515"
    },
    "Is Self-Repair a Silver Bullet for Code Generation?": {
        "type": "inproceedings",
        "key": "olausson2023self",
        "title": "Is Self-Repair a Silver Bullet for Code Generation?",
        "author": "Olausson, Theo X and Inala, Jeevana Priya and Wang, Chenglong and Gao, Jianfeng and Solar-Lezama, Armando",
        "booktitle": "The Twelfth International Conference on Learning Representations",
        "year": "2023",
        "venue": "ICLR2023",
        "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair---in which the model debugs and repairs its own code---has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.",
        "labels": [
            "code generation",
            "program repair"
        ],
        "url": "https://openreview.net/forum?id=y0GJXRungR"
    },
    "Repairagent: An autonomous, llm-based agent for program repair": {
        "type": "article",
        "key": "bouzenia2024repairagent",
        "title": "Repairagent: An autonomous, llm-based agent for program repair",
        "author": "Bouzenia, Islem and Devanbu, Premkumar and Pradel, Michael",
        "journal": "arXiv preprint arXiv:2403.17134",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.",
        "labels": [
            "code generation",
            "program repair",
            "agent design",
            "planning"
        ],
        "url": "https://arxiv.org/pdf/2403.17134"
    },
    "Natural Language Commanding via Program Synthesis": {
        "type": "article",
        "key": "gandhi2023natural",
        "title": "Natural Language Commanding via Program Synthesis",
        "author": "Gandhi, Apurva and Nguyen, Thong Q and Jiao, Huitian and Steen, Robert and Bhatawdekar, Ameya",
        "journal": "arXiv preprint arXiv:2306.03460",
        "year": "2023",
        "venue": "Microsoft2023",
        "abstract": "We present Semantic Interpreter, a natural language-friendly AI system for productivity software such as Microsoft Office that leverages large language models (LLMs) to execute user intent across application features. While LLMs are excellent at understanding user intent expressed as natural language, they are not sufficient for fulfilling application-specific user intent that requires more than text-to-text transformations. We therefore introduce the Office Domain Specific Language (ODSL), a concise, high-level language specialized for performing actions in and interacting with entities in Office applications. Semantic Interpreter leverages an Analysis-Retrieval prompt construction method with LLMs for program synthesis, translating natural language user utterances to ODSL programs that can be transpiled to application APIs and then executed. We focus our discussion primarily on a research exploration for Microsoft PowerPoint.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://arxiv.org/pdf/2306.03460.pdf"
    },
    "Effective Large Language Model Debugging with Best-first Tree Search": {
        "type": "article",
        "key": "song2024effective",
        "title": "Effective Large Language Model Debugging with Best-first Tree Search",
        "author": "Song, Jialin and Raiman, Jonathan and Catanzaro, Bryan",
        "journal": "arXiv preprint arXiv:2407.19055",
        "year": "2024",
        "venue": "NVDIA2024",
        "abstract": "Large Language Models (LLMs) show promise in code generation tasks. However, their code-writing abilities are often limited in scope: while they can successfully implement simple functions, they struggle with more complex tasks. A fundamental difference with how an LLM writes code, compared to a human programmer, is that it cannot consistently spot and fix bugs. Debugging is a crucial skill for programmers and it enables iterative code refinement towards a correct implementation. In this work, we propose a novel algorithm to enable LLMs to debug their code via self-reflection and search where a model attempts to identify its previous mistakes. Our key contributions are 1) a best-first tree search algorithm with self-reflections (BESTER) that achieves state-of-the-art Pass@1 in three code generation benchmarks. BESTER maintains its superiority when we measure pass rates taking into account additional inference costs incurred by tree search. 2) A novel interpretability study on what self-reflections attend to in buggy programs and how they impact bug fixes, which provides a deeper understanding of the debugging process. 3) An extensive study on when self-reflections are effective in finding bugs.",
        "labels": [
            "code generation",
            "debugging"
        ],
        "url": "https://arxiv.org/pdf/2407.19055"
    },
    "Automatic Programming: Large Language Models and Beyond": {
        "type": "article",
        "key": "lyu2024automatic",
        "title": "Automatic Programming: Large Language Models and Beyond",
        "author": "Lyu, Michael R and Ray, Baishakhi and Roychoudhury, Abhik and Tan, Shin Hwei and Thongtanunam, Patanamon",
        "journal": "arXiv preprint arXiv:2405.02213",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs, can help produce higher assurance code from LLMs, along with evidence of assurance.",
        "labels": [
            "general coding task",
            "empirical study"
        ],
        "url": "https://arxiv.org/pdf/2405.02213"
    },
    "Verified multi-step synthesis using large language models and monte carlo tree search": {
        "type": "article",
        "key": "brandfonbrener2024verified",
        "title": "Verified multi-step synthesis using large language models and monte carlo tree search",
        "author": "Brandfonbrener, David and Raja, Sibi and Prasad, Tarun and Loughridge, Chloe and Yang, Jianang and Henniger, Simon and Byrd, William E and Zinkov, Robert and Amin, Nada",
        "journal": "arXiv preprint arXiv:2402.08147",
        "year": "2024",
        "venue": "NeurIPS2024",
        "abstract": "We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq. Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step. In combination with the LLM prior, the verifier feedback raises the synthesis capabilities of open source models. On a set of five verified programming problems, we find that in four problems where the base model cannot solve the question even when re-sampling solutions for one hour, VMCTS can solve the problems within 6 minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented with plugins and multiple re-tries on these problems. Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://openreview.net/pdf?id=HmB9uZTzaD"
    },
    "Hypothesis search: Inductive reasoning with language models": {
        "type": "article",
        "key": "wang2023hypothesis",
        "title": "Hypothesis search: Inductive reasoning with language models",
        "author": "Wang, Ruocheng and Zelikman, Eric and Poesia, Gabriel and Pu, Yewen and Haber, Nick and Goodman, Noah D",
        "journal": "arXiv preprint arXiv:2309.05660",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which can then be robustly generalized to novel scenarios. Recent work has evaluated large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding \"in context learning.\" This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be directly verified by running on the observed examples and generalized to novel inputs. Because of the prohibitive cost of generation with state-of-the-art LLMs, we consider a middle step to filter the set of hypotheses that will be implemented into programs: we either ask the LLM to summarize into a smaller set of hypotheses, or ask human annotators to select a subset of the hypotheses. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. On a random 40-problem subset of ARC, our automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted to 37.5%. (And we argue this is a lower bound on the performance of our approach without filtering.) Our ablation studies show that abstract hypothesis generation and concrete program representations are both beneficial for LLMs to perform inductive reasoning tasks.",
        "labels": [
            "code generation",
            "program synthesis",
            "static analysis",
            "program verification"
        ],
        "url": "https://openreview.net/forum?id=G7UtIGQmjm"
    },
    "Guess \\& Sketch: Language Model Guided Transpilation": {
        "type": "article",
        "key": "lee2023guess",
        "title": "Guess \\& Sketch: Language Model Guided Transpilation",
        "author": "Lee, Celine and Mahmoud, Abdulrahman and Kurek, Michal and Campanoni, Simone and Brooks, David and Chong, Stephen and Wei, Gu-Yeon and Rush, Alexander M",
        "journal": "arXiv preprint arXiv:2309.14396",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods. Guess & Sketch extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test Guess & Sketch on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler. We also share a training and evaluation dataset for this task.",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "url": "https://openreview.net/forum?id=qPFsIbF3V6"
    },
    "Verified Code Transpilation with LLMs": {
        "type": "article",
        "key": "bhatia2024verified",
        "title": "Verified Code Transpilation with LLMs",
        "author": "Bhatia, Sahil and Qiu, Jie and Hasabnis, Niranjan and Seshia, Sanjit A and Cheung, Alvin",
        "journal": "arXiv preprint arXiv:2406.03003",
        "year": "2024",
        "venue": "NeurIPS2024",
        "abstract": "Domain-specific languages (DSLs) are integral to various software workflows. Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability. However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API. While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the transpiled code. Another approach for automating this task is verified lifting, which relies on program synthesis to find programs in the target language that are functionally equivalent to the source language program. While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient. In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools. We use the LLM's capabilities to reason about programs to translate a given program into its corresponding equivalent in the target language. Additionally, we use LLMs to generate proofs for functional equivalence. We develop lifting-based compilers for {\\em four different} DSLs targeting different application domains. Our approach not only outperforms previous symbolic-based tools in both the number of benchmarks transpiled and transpilation time, but also requires significantly less effort to build.",
        "labels": [
            "code generation",
            "program synthesis",
            "static analysis",
            "program verification"
        ],
        "url": "https://arxiv.org/pdf/2406.03003"
    },
    "Rectifier: Code translation with corrector via llms": {
        "type": "article",
        "key": "yin2024rectifier",
        "title": "Rectifier: Code translation with corrector via llms",
        "author": "Yin, Xin and Ni, Chao and Nguyen, Tien N and Wang, Shaohua and Yang, Xiaohu",
        "journal": "arXiv preprint arXiv:2407.07472",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Software migration is garnering increasing attention with the evolution of software and society. Early studies mainly relied on handcrafted translation rules to translate between two languages, the translation process is error-prone and time-consuming. In recent years, researchers have begun to explore the use of pre-trained large language models (LLMs) in code translation. However, code translation is a complex task that LLMs would generate mistakes during code translation, they all produce certain types of errors when performing code translation tasks, which include (1) compilation error, (2) runtime error, (3) functional error, and (4) non-terminating execution. We found that the root causes of these errors are very similar (e.g. failure to import packages, errors in loop boundaries, operator errors, and more). In this paper, we propose a general corrector, namely Rectifier, which is a micro and universal model for repairing translation errors. It learns from errors generated by existing LLMs and can be widely applied to correct errors generated by any LLM. The experimental results on translation tasks between C++, Java, and Python show that our model has effective repair ability, and cross experiments also demonstrate the robustness of our method.",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "url": "https://arxiv.org/pdf/2407.07472"
    },
    "Learning performance-improving code edits": {
        "type": "article",
        "key": "shypula2023learning",
        "title": "Learning performance-improving code edits",
        "author": "Shypula, Alexander and Madaan, Aman and Zeng, Yimeng and Alon, Uri and Gardner, Jacob and Hashemi, Milad and Neubig, Graham and Ranganathan, Parthasarathy and Bastani, Osbert and Yazdanbakhsh, Amir",
        "journal": "arXiv preprint arXiv:2302.07867",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "With the waning of Moore's law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code. Simultaneously, pretrained large language models (LLMs) have demonstrated strong capabilities at solving a wide range of programming tasks. To that end, we introduce a framework for adapting LLMs to high-level program optimization. First, we curate a dataset of performance-improving edits made by human programmers of over 77K competitive C++ programming submission pairs, accompanied by extensive unit tests. A major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious \"improvements\". To isolate and reliably evaluate the impact of program optimizations, we design an environment based on the gem5 full system simulator, the de facto simulator used in academia and industry. Next, we propose a broad range of adaptation strategies for code optimization; for prompting, these include retrieval-based few-shot prompting and chain-of-thought, and for finetuning, these include performance-conditioned generation and synthetic data augmentation based on self-play. A combination of these techniques achieves an average speedup of 5.65X on CodeLlama-13B and 6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our proposed performance-conditioned generation is particularly effective at improving performance as well as increasing the fraction of optimized programs.",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "url": "https://arxiv.org/pdf/2302.07867"
    },
    "Enabling Memory Safety of C Programs using LLMs": {
        "type": "article",
        "key": "mohammed2024enabling",
        "title": "Enabling Memory Safety of C Programs using LLMs",
        "author": "Mohammed, Nausheen and Lal, Akash and Rastogi, Aseem and Roy, Subhajit and Sharma, Rahul",
        "journal": "arXiv preprint arXiv:2404.01096",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Memory safety violations in low-level code, written in languages like C, continues to remain one of the major sources of software vulnerabilities. One method of removing such violations by construction is to port C code to a safe C dialect. Such dialects rely on programmer-supplied annotations to guarantee safety with minimal runtime overhead. This porting, however, is a manual process that imposes significant burden on the programmer and, hence, there has been limited adoption of this technique. The task of porting not only requires inferring annotations, but may also need refactoring/rewriting of the code to make it amenable to such annotations. In this paper, we use Large Language Models (LLMs) towards addressing both these concerns. We show how to harness LLM capabilities to do complex code reasoning as well as rewriting of large codebases. We also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an LLM. We implement our ideas in a tool called MSA that targets the CheckedC dialect. We evaluate MSA on several micro-benchmarks, as well as real-world code ranging up to 20K lines of code. We showcase superior performance compared to a vanilla LLM baseline, as well as demonstrate improvement over a state-of-the-art symbolic (non-LLM) technique.",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "url": "https://arxiv.org/pdf/2404.01096.pdf"
    },
    "Refactoring programs using large language models with few-shot examples": {
        "type": "inproceedings",
        "key": "shirafuji2023refactoring",
        "title": "Refactoring programs using large language models with few-shot examples",
        "author": "Shirafuji, Atsushi and Oda, Yusuke and Suzuki, Jun and Morishita, Makoto and Watanobe, Yutaka",
        "booktitle": "2023 30th Asia-Pacific Software Engineering Conference (APSEC)",
        "pages": "151--160",
        "year": "2023",
        "organization": "IEEE",
        "abstract": "A less complex and more straightforward program is a crucial factor that enhances its maintainability and makes writing secure and bug-free programs easier. However, due to its heavy workload and the risks of breaking the working programs, programmers are reluctant to do code refactoring, and thus, it also causes the loss of potential learning experiences. To mitigate this, we demonstrate the application of using a large language model (LLM), GPT-3.5, to suggest less complex versions of the user-written Python program, aiming to encourage users to learn how to write better programs. We propose a method to leverage the prompting with few-shot examples of the LLM by selecting the best-suited code refactoring examples for each target programming problem based on the prior evaluation of prompting with the one-shot example. The quantitative evaluation shows that 95.68% of programs can be refactored by generating 10 candidates each, resulting in a 17.35% reduction in the average cyclomatic complexity and a 25.84% decrease in the average number of lines after filtering only generated programs that are semantically correct. Further-more, the qualitative evaluation shows outstanding capability in code formatting, while unnecessary behaviors such as deleting or translating comments are also observed.",
        "venue": "APSEC2023",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "url": "https://arxiv.org/pdf/2311.11690.pdf"
    },
    "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2405-15383",
        "author": "Nicola Dainese and Matteo Merler and Minttu Alakuijala and Pekka Marttinen",
        "title": "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search",
        "journal": "CoRR",
        "volume": "abs/2405.15383",
        "year": "2024",
        "url": "https://doi.org/10.48550/arXiv.2405.15383",
        "doi": "10.48550/ARXIV.2405.15383",
        "eprinttype": "arXiv",
        "eprint": "2405.15383",
        "timestamp": "Wed, 19 Jun 2024 08:52:55 +0200",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2405-15383.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "NeurIPS2024",
        "abstract": "In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL). Calling code instead of LLMs for planning has the advantages of being precise, reliable, interpretable, and extremely efficient. However, writing appropriate Code World Models requires the ability to understand complex instructions, to generate exact code with non-trivial logic and to self-debug a long program with feedback from unit tests and environment trajectories. To address these challenges, we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for LLMs. To test our approach, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories. GIF-MCTS surpasses all baselines on the CWMB and two other benchmarks, and we show that the Code World Models synthesized with it can be successfully used for planning, resulting in model-based RL agents with greatly improved sample efficiency and inference speed.",
        "labels": [
            "code generation",
            "program synthesis"
        ]
    },
    "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules": {
        "type": "inproceedings",
        "key": "DBLP:conf/iclr/LeCSGSJ24",
        "author": "Hung Le and Hailin Chen and Amrita Saha and Akash Gokul and Doyen Sahoo and Shafiq Joty",
        "title": "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules",
        "booktitle": "The Twelfth International Conference on Learning Representations",
        "publisher": "OpenReview.net",
        "year": "2024",
        "url": "https://openreview.net/forum?id=vYhglxSj8j",
        "timestamp": "Wed, 07 Aug 2024 17:11:53 +0200",
        "biburl": "https://dblp.org/rec/conf/iclr/LeCSGSJ24.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "ICLR2024",
        "abstract": "Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extracting and clustering the generated sub-modules and selecting the cluster representatives as the more generic and re-usable implementations, and 2) augmenting the original chain-of-thought prompt with these selected module-implementations and instructing the LLM to re-generate new modularized solutions. We find that by naturally encouraging the LLM to reuse the previously developed and verified sub-modules, CodeChain can significantly boost both modularity as well as correctness of the generated solutions, achieving relative pass@1 improvements of 35% on APPS and 76% on CodeContests. It is shown to be effective on both OpenAI LLMs as well as open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation studies with different methods of prompting, number of clusters, model sizes, program qualities, etc., to provide useful insights that underpin CodeChain's success.",
        "labels": [
            "code generation",
            "program synthesis"
        ]
    },
    "LongCoder: {A} Long-Range Pre-trained Language Model for Code Completion": {
        "type": "inproceedings",
        "key": "DBLP:conf/icml/GuoXD0M23",
        "author": "Daya Guo and Canwen Xu and Nan Duan and Jian Yin and Julian J. McAuley",
        "title": "LongCoder: {A} Long-Range Pre-trained Language Model for Code Completion",
        "booktitle": "International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA",
        "series": "Proceedings of Machine Learning Research",
        "volume": "202",
        "pages": "12098--12107",
        "publisher": "PMLR",
        "year": "2023",
        "url": "https://proceedings.mlr.press/v202/guo23j.html",
        "timestamp": "Mon, 28 Aug 2023 17:23:08 +0200",
        "biburl": "https://dblp.org/rec/conf/icml/GuoXD0M23.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "ICML2023",
        "abstract": "In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens-bridge tokens and memory tokens-to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference.",
        "labels": [
            "code generation",
            "code completion",
            "code model",
            "code model training",
            "source code model"
        ]
    },
    "Repository-Level Prompt Generation for Large Language Models of Code": {
        "type": "inproceedings",
        "key": "DBLP:conf/icml/ShrivastavaLT23",
        "author": "Disha Shrivastava and Hugo Larochelle and Daniel Tarlow",
        "title": "Repository-Level Prompt Generation for Large Language Models of Code",
        "booktitle": "International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA",
        "series": "Proceedings of Machine Learning Research",
        "volume": "202",
        "pages": "31693--31715",
        "publisher": "PMLR",
        "year": "2023",
        "url": "https://proceedings.mlr.press/v202/shrivastava23a.html",
        "timestamp": "Mon, 28 Aug 2023 17:23:09 +0200",
        "biburl": "https://dblp.org/rec/conf/icml/ShrivastavaLT23.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "ICML2023",
        "abstract": "With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex used in GitHub Copilot), techniques for introducing domain-specific knowledge in the prompt design process become important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn't require any access to the weights of the LLM, making it applicable in cases where we only have black-box access to the LLM. We conduct experiments on the task of single-line code-autocompletion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our prompt proposals gives a remarkably high relative improvement of 36% over Codex, showing the quality of these proposals. Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines.",
        "labels": [
            "code generation",
            "code completion",
            "agent design",
            "prompt strategy",
            "retrieval-augmented generation"
        ]
    },
    "Vulnerability Detection with Code Language Models: How Far Are We?": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2403-18624",
        "author": "Yangruibo Ding and Yanjun Fu and Omniyyah Ibrahim and Chawin Sitawarin and Xinyun Chen and Basel Alomair and David A. Wagner and Baishakhi Ray and Yizheng Chen",
        "title": "Vulnerability Detection with Code Language Models: How Far Are We?",
        "journal": "CoRR",
        "volume": "abs/2403.18624",
        "year": "2024",
        "url": "https://doi.org/10.48550/arXiv.2403.18624",
        "doi": "10.48550/ARXIV.2403.18624",
        "eprinttype": "arXiv",
        "eprint": "2403.18624",
        "timestamp": "Wed, 10 Apr 2024 17:37:45 +0200",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2403-18624.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "ICSE2025",
        "abstract": "In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection.To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings. This comprehensive approach aims to provide a more accurate assessment of code LMs' performance in real-world conditions.Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models. For instance, a state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on PrimeVul. Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings. These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain.",
        "labels": [
            "static analysis",
            "bug detection",
            "benchmark"
        ]
    },
    "VulEval: Towards Repository-Level Evaluation of Software Vulnerability Detection": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2404-15596",
        "author": "Xin{-}Cheng Wen and Xinchen Wang and Yujia Chen and Ruida Hu and David Lo and Cuiyun Gao",
        "title": "VulEval: Towards Repository-Level Evaluation of Software Vulnerability Detection",
        "journal": "CoRR",
        "volume": "abs/2404.15596",
        "year": "2024",
        "url": "https://doi.org/10.48550/arXiv.2404.15596",
        "doi": "10.48550/ARXIV.2404.15596",
        "eprinttype": "arXiv",
        "eprint": "2404.15596",
        "timestamp": "Mon, 03 Jun 2024 20:47:53 +0200",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2404-15596.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "arXiv2024",
        "abstract": "Deep Learning (DL)-based methods have proven to be effective for software vulnerability detection, with a potential for substantial productivity enhancements for detecting vulnerabilities. Current methods mainly focus on detecting single functions (i.e., intra-procedural vulnerabilities), ignoring the more complex inter-procedural vulnerability detection scenarios in practice. For example, developers routinely engage with program analysis to detect vulnerabilities that span multiple functions within repositories. In addition, the widely-used benchmark datasets generally contain only intra-procedural vulnerabilities, leaving the assessment of inter-procedural vulnerability detection capabilities unexplored.To mitigate the issues, we propose a repository-level evaluation system, named \\textbf{VulEval}, aiming at evaluating the detection performance of inter- and intra-procedural vulnerabilities simultaneously. Specifically, VulEval consists of three interconnected evaluation tasks: \\textbf{(1) Function-Level Vulnerability Detection}, aiming at detecting intra-procedural vulnerability given a code snippet; \\textbf{(2) Vulnerability-Related Dependency Prediction}, aiming at retrieving the most relevant dependencies from call graphs for providing developers with explanations about the vulnerabilities; and \\textbf{(3) Repository-Level Vulnerability Detection}, aiming at detecting inter-procedural vulnerabilities by combining with the dependencies identified in the second task. VulEval also consists of a large-scale dataset, with a total of 4,196 CVE entries, 232,239 functions, and corresponding 4,699 repository-level source code in C/C++ programming languages. Our analysis highlights the current progress and future directions for software vulnerability detection.",
        "labels": [
            "static analysis",
            "bug detection",
            "benchmark"
        ]
    },
    "A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2403-17218",
        "author": "Benjamin Steenhoek and Md Mahbubur Rahman and Monoshi Kumar Roy and Mirza Sanjida Alam and Earl T. Barr and Wei Le",
        "title": "A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection",
        "journal": "CoRR",
        "volume": "abs/2403.17218",
        "year": "2024",
        "url": "https://doi.org/10.48550/arXiv.2403.17218",
        "doi": "10.48550/ARXIV.2403.17218",
        "eprinttype": "arXiv",
        "eprint": "2403.17218",
        "timestamp": "Wed, 10 Apr 2024 17:37:45 +0200",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2403-17218.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "arXiv2024",
        "abstract": "Large Language Models (LLMs) have demonstrated great potential for code generation and other software engineering tasks. Vulnerability detection is of crucial importance to maintaining the security, integrity, and trustworthiness of software systems. Precise vulnerability detection requires reasoning about the code, making it a good case study for exploring the limits of LLMs' reasoning capabilities. Although recent work has applied LLMs to vulnerability detection using generic prompting techniques, their full capabilities for this task and the types of errors they make when explaining identified vulnerabilities remain unclear.In this paper, we surveyed eleven LLMs that are state-of-the-art in code generation and commonly used as coding assistants, and evaluated their capabilities for vulnerability detection. We systematically searched for the best-performing prompts, incorporating techniques such as in-context learning and chain-of-thought, and proposed three of our own prompting methods. Our results show that while our prompting methods improved the models' performance, LLMs generally struggled with vulnerability detection. They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average. By comprehensively analyzing and categorizing 287 instances of model reasoning, we found that 57% of LLM responses contained errors, and the models frequently predicted incorrect locations of buggy code and misidentified bug types. LLMs only correctly localized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted correctly by 70-100% of human participants. These findings suggest that despite their potential for other tasks, LLMs may fail to properly comprehend critical code structures and security-related concepts.",
        "labels": [
            "static analysis",
            "bug detection",
            "empirical study"
        ]
    },
    "Source Code Vulnerability Detection: Combining Code Language Models and Code Property Graphs": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2404-14719",
        "author": "Ruitong Liu and Yanbin Wang and Haitao Xu and Bin Liu and Jianguo Sun and Zhenhao Guo and Wenrui Ma",
        "title": "Source Code Vulnerability Detection: Combining Code Language Models and Code Property Graphs",
        "journal": "CoRR",
        "volume": "abs/2404.14719",
        "year": "2024",
        "url": "https://doi.org/10.48550/arXiv.2404.14719",
        "doi": "10.48550/ARXIV.2404.14719",
        "eprinttype": "arXiv",
        "eprint": "2404.14719",
        "timestamp": "Sat, 25 May 2024 18:35:25 +0200",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2404-14719.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "arXiv2024",
        "abstract": "Currently, deep learning successfully applies to code vulnerability detection by learning from code sequences or property graphs. However, sequence-based methods often overlook essential code attributes such as syntax, control flow, and data dependencies, whereas graph-based approaches might underestimate the semantics of code and face challenges in capturing long-distance contextual information.To address this gap, we propose Vul-LMGNN, a unified model that combines pre-trained code language models with code property graphs for code vulnerability detection. Vul-LMGNN constructs a code property graph that integrates various code attributes (including syntax, flow control, and data dependencies) into a unified graph structure, thereafter leveraging pre-trained code model to extract local semantic features as node embeddings in the code property graph. Furthermore, to effectively retain dependency information among various attributes, we introduce a gated code Graph Neural Network (GNN). By jointly training the code language model and the gated code GNN modules in Vul-LMGNN, our proposed method efficiently leverages the strengths of both mechanisms. Finally, we utilize a pre-trained CodeBERT as an auxiliary classifier, with the final detection results derived by learning the linear interpolation of Vul-LMGNN and CodeBERT. The proposed method, evaluated across four real-world vulnerability datasets, demonstrated superior performance compared to six state-of-the-art approaches.",
        "labels": [
            "static analysis",
            "bug detection",
            "code model",
            "code model training",
            "source code model"
        ]
    },
    "Your Instructions Are Not Always Helpful: Assessing the Efficacy of Instruction Fine-tuning for Software Vulnerability Detection": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2401-07466",
        "author": "Imam Nur Bani Yusuf and Lingxiao Jiang",
        "title": "Your Instructions Are Not Always Helpful: Assessing the Efficacy of Instruction Fine-tuning for Software Vulnerability Detection",
        "journal": "CoRR",
        "volume": "abs/2401.07466",
        "year": "2024",
        "url": "https://doi.org/10.48550/arXiv.2401.07466",
        "doi": "10.48550/ARXIV.2401.07466",
        "eprinttype": "arXiv",
        "eprint": "2401.07466",
        "timestamp": "Thu, 01 Feb 2024 15:35:36 +0100",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2401-07466.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "arXiv2024",
        "abstract": "Software, while beneficial, poses potential cybersecurity risks due to inherent vulnerabilities. Detecting these vulnerabilities is crucial, and deep learning has shown promise as an effective tool for this task due to its ability to perform well without extensive feature engineering. However, a challenge in deploying deep learning for vulnerability detection is the limited availability of training data. Recent research highlights the deep learning efficacy in diverse tasks. This success is attributed to instruction fine-tuning, a technique that remains under-explored in the context of vulnerability detection. This paper investigates the capability of models, specifically a recent language model, to generalize beyond the programming languages used in their training data. It also examines the role of natural language instructions in enhancing this generalization. Our study evaluates the model performance on a real-world dataset to predict vulnerable code. We present key insights and lessons learned, contributing to understanding the deep learning application in software vulnerability detection.",
        "labels": [
            "static analysis",
            "bug detection",
            "code model",
            "code model training",
            "source code model"
        ]
    },
    "LLM4Vuln: {A} Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2401-16185",
        "author": "Yuqiang Sun and Daoyuan Wu and Yue Xue and Han Liu and Wei Ma and Lyuye Zhang and Miaolei Shi and Yang Liu",
        "title": "LLM4Vuln: {A} Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning",
        "journal": "CoRR",
        "volume": "abs/2401.16185",
        "year": "2024",
        "url": "https://doi.org/10.48550/arXiv.2401.16185",
        "doi": "10.48550/ARXIV.2401.16185",
        "eprinttype": "arXiv",
        "eprint": "2401.16185",
        "timestamp": "Fri, 20 Sep 2024 11:00:56 +0200",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2401-16185.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "arXiv2024",
        "abstract": "Large language models (LLMs) have demonstrated significant potential in various tasks, including vulnerability detection. However, current efforts in this area are preliminary, lacking clarity on whether LLMs' vulnerability reasoning capabilities stem from the models themselves or external aids such as knowledge retrieval and tooling support.This paper aims to isolate LLMs' vulnerability reasoning from other capabilities, such as vulnerability knowledge adoption, context information retrieval, and structured output generation. We introduce LLM4Vuln, a unified evaluation framework that separates and assesses LLMs' vulnerability reasoning capabilities and examines improvements when combined with other enhancements.We conducted controlled experiments with 97 ground-truth vulnerabilities and 97 non-vulnerable cases in Solidity and Java, testing them in a total of 9,312 scenarios across four LLMs (GPT-4, GPT-3.5, Mixtral, and Llama 3). Our findings reveal the varying impacts of knowledge enhancement, context supplementation, prompt schemes, and models. Additionally, we identified 14 zero-day vulnerabilities in four pilot bug bounty programs, resulting in $3,576 in bounties.",
        "labels": [
            "static analysis",
            "bug detection",
            "benchmark"
        ]
    },
    "Top Score on the Wrong Exam: On Benchmarking in Machine Learning for Vulnerability Detection": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2408-12986",
        "author": "Niklas Risse and Marcel B{\\\"{o}hme",
        "title": "Top Score on the Wrong Exam: On Benchmarking in Machine Learning for Vulnerability Detection",
        "journal": "CoRR",
        "volume": "abs/2408.12986",
        "year": "2024",
        "url": "https://doi.org/10.48550/arXiv.2408.12986",
        "doi": "10.48550/ARXIV.2408.12986",
        "eprinttype": "arXiv",
        "eprint": "2408.12986",
        "timestamp": "Sat, 28 Sep 2024 18:01:43 +0200",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2408-12986.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "arXiv2024",
        "abstract": "According to our survey of the machine learning for vulnerability detection (ML4VD) literature published in the top Software Engineering conferences, every paper in the past 5 years defines ML4VD as a binary classification problem: Given a function, does it contain a security flaw?In this paper, we ask whether this decision can really be made without further context and study both vulnerable and non-vulnerable functions in the most popular ML4VD datasets. A function is vulnerable if it was involved in a patch of an actual security flaw and confirmed to cause the vulnerability. It is non-vulnerable otherwise. We find that in almost all cases this decision cannot be made without further context. Vulnerable functions are often vulnerable only because a corresponding vulnerability-inducing calling context exists while non-vulnerable functions would often be vulnerable if a corresponding context existed.But why do ML4VD techniques perform so well even though there is demonstrably not enough information in these samples? Spurious correlations: We find that high accuracy can be achieved even when only word counts are available. This shows that these datasets can be exploited to achieve high accuracy without actually detecting any security vulnerabilities.We conclude that the current problem statement of ML4VD is ill-defined and call into question the internal validity of this growing body of work. Constructively, we call for more effective benchmarking methodologies to evaluate the true capabilities of ML4VD, propose alternative problem statements, and examine broader implications for the evaluation of machine learning and programming analysis research.",
        "labels": [
            "static analysis",
            "bug detection",
            "empirical study"
        ]
    },
    "How Far Have We Gone in Vulnerability Detection Using Large Language Models": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2311-12420",
        "author": "Zeyu Gao and Hao Wang and Yuchen Zhou and Wenyu Zhu and Chao Zhang",
        "title": "How Far Have We Gone in Vulnerability Detection Using Large Language Models",
        "journal": "CoRR",
        "volume": "abs/2311.12420",
        "year": "2023",
        "url": "https://doi.org/10.48550/arXiv.2311.12420",
        "doi": "10.48550/ARXIV.2311.12420",
        "eprinttype": "arXiv",
        "eprint": "2311.12420",
        "timestamp": "Thu, 26 Sep 2024 15:34:35 +0200",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2311-12420.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "arXiv2023",
        "abstract": "As software becomes increasingly complex and prone to vulnerabilities, automated vulnerability detection is critically important, yet challenging. Given the significant successes of large language models (LLMs) in various tasks, there is growing anticipation of their efficacy in vulnerability detection. However, a quantitative understanding of their potential in vulnerability detection is still missing. To bridge this gap, we introduce a comprehensive vulnerability benchmark VulBench. This benchmark aggregates high-quality data from a wide range of CTF (Capture-the-Flag) challenges and real-world applications, with annotations for each vulnerable function detailing the vulnerability type and its root cause. Through our experiments encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models and static analyzers, we find that several LLMs outperform traditional deep learning approaches in vulnerability detection, revealing an untapped potential in LLMs. This work contributes to the understanding and utilization of LLMs for enhanced software security.",
        "labels": [
            "static analysis",
            "bug detection",
            "benchmark"
        ]
    },
    "DiverseVul: {A} New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection": {
        "type": "inproceedings",
        "key": "DBLP:conf/raid/0001DACW23",
        "author": "Yizheng Chen and Zhoujie Ding and Lamya Alowain and Xinyun Chen and David A. Wagner",
        "title": "DiverseVul: {A} New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection",
        "booktitle": "Proceedings of the 26th International Symposium on Research in Attacks",
        "pages": "654--668",
        "publisher": "ACM",
        "year": "2023",
        "url": "https://doi.org/10.1145/3607199.3607242",
        "doi": "10.1145/3607199.3607242",
        "timestamp": "Fri, 27 Oct 2023 20:40:34 +0200",
        "biburl": "https://dblp.org/rec/conf/raid/0001DACW23.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "RAID2023",
        "abstract": "We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 18,945 vulnerable functions spanning 150 CWEs and 330,492 non-vulnerable functions extracted from 7,514 commits. Our dataset covers 295 more projects than all previous datasets combined.Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonstrate an important generalization challenge for the deployment of deep learning-based models. We show that increasing the volume of training data may not further improve the performance of deep learning models for vulnerability detection, but might be useful to improve the generalization ability to unseen projects.We also identify hopeful future research directions. We demonstrate that large language models (LLMs) are a promising research direction for ML-based vulnerability detection, outperforming Graph Neural Networks (GNNs) with code-structure features in our experiments. Moreover, developing source code specific pre-training objectives is a promising research direction to improve the vulnerability detection performance.",
        "labels": [
            "static analysis",
            "bug detection",
            "benchmark"
        ]
    },
    "Large Language Models for Code Analysis: Do LLMs Really Do Their Job?": {
        "type": "inproceedings",
        "key": "DBLP:conf/uss/FangMS0ZFATNWH24",
        "author": "Chongzhou Fang and Ning Miao and Shaurya Srivastav and Jialin Liu and Ruoyu Zhang and Ruijie Fang and Asmita and Ryan Tsang and Najmeh Nazari and Han Wang and Houman Homayoun",
        "editor": "Davide Balzarotti and Wenyuan Xu",
        "title": "Large Language Models for Code Analysis: Do LLMs Really Do Their Job?",
        "booktitle": "33rd {USENIX} Security Symposium, {USENIX} Security 2024, Philadelphia",
        "publisher": "USENIX} Association",
        "year": "2024",
        "url": "https://www.usenix.org/conference/usenixsecurity24/presentation/fang",
        "timestamp": "Mon, 22 Jul 2024 17:10:49 +0200",
        "biburl": "https://dblp.org/rec/conf/uss/FangMS0ZFATNWH24.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "USENIXSec2024",
        "abstract": "Large language models (LLMs) have demonstrated significant potential in the realm of natural language understanding and programming code processing tasks. Their capacity to comprehend and generate human-like code has spurred research into harnessing LLMs for code analysis purposes. However, the existing body of literature falls short in delivering a systematic evaluation and assessment of LLMs' effectiveness in code analysis, particularly in the context of obfuscated code.This paper seeks to bridge this gap by offering a comprehensive evaluation of LLMs' capabilities in performing code analysis tasks. Additionally, it presents real-world case studies that employ LLMs for code analysis. Our findings indicate that LLMs can indeed serve as valuable tools for automating code analysis, albeit with certain limitations. Through meticulous exploration, this research contributes to a deeper understanding of the potential and constraints associated with utilizing LLMs in code analysis, paving the way for enhanced applications in this critical domain.",
        "labels": [
            "static analysis",
            "bug detection",
            "empirical study"
        ]
    },
    "Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2311-16169",
        "author": "Avishree Khare and Saikat Dutta and Ziyang Li and Alaia Solko{-}Breslin and Rajeev Alur and Mayur Naik",
        "title": "Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities",
        "journal": "CoRR",
        "volume": "abs/2311.16169",
        "year": "2023",
        "url": "https://doi.org/10.48550/arXiv.2311.16169",
        "doi": "10.48550/ARXIV.2311.16169",
        "eprinttype": "arXiv",
        "eprint": "2311.16169",
        "timestamp": "Mon, 04 Dec 2023 10:53:08 +0100",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2311-16169.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "arXiv2023",
        "abstract": "While automated vulnerability detection techniques have made promising progress in detecting security vulnerabilities, their scalability and applicability remain challenging. The remarkable performance of Large Language Models (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted recent works to explore if LLMs can be used to detect vulnerabilities. In this paper, we perform a more comprehensive study by concurrently examining a higher number of datasets, languages and LLMs, and qualitatively evaluating performance across prompts and vulnerability classes while addressing the shortcomings of existing tools. Concretely, we evaluate the effectiveness of 16 pre-trained LLMs on 5,000 code samples from five diverse security datasets. These balanced datasets encompass both synthetic and real-world projects in Java and C/C++ and cover 25 distinct vulnerability classes.Overall, LLMs across all scales and families show modest effectiveness in detecting vulnerabilities, obtaining an average accuracy of 62.8% and F1 score of 0.71 across datasets. They are significantly better at detecting vulnerabilities only requiring intra-procedural analysis, such as OS Command Injection and NULL Pointer Dereference. Moreover, they report higher accuracies on these vulnerabilities than popular static analysis tools, such as CodeQL.We find that advanced prompting strategies that involve step-by-step analysis significantly improve performance of LLMs on real-world datasets in terms of F1 score (by upto 0.18 on average). Interestingly, we observe that LLMs show promising abilities at performing parts of the analysis correctly, such as identifying vulnerability-related specifications and leveraging natural language information to understand code behavior (e.g., to check if code is sanitized). We expect our insights to guide future work on LLM-augmented vulnerability detection systems.",
        "labels": [
            "static analysis",
            "bug detection",
            "empirical study"
        ]
    },
    "Do Language Models Learn Semantics of Code? {A} Case Study in Vulnerability Detection": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2311-04109",
        "author": "Benjamin Steenhoek and Md Mahbubur Rahman and Shaila Sharmin and Wei Le",
        "title": "Do Language Models Learn Semantics of Code? {A} Case Study in Vulnerability Detection",
        "journal": "CoRR",
        "volume": "abs/2311.04109",
        "year": "2023",
        "url": "https://doi.org/10.48550/arXiv.2311.04109",
        "doi": "10.48550/ARXIV.2311.04109",
        "eprinttype": "arXiv",
        "eprint": "2311.04109",
        "timestamp": "Tue, 14 Nov 2023 14:47:55 +0100",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2311-04109.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "arXiv2023",
        "abstract": "Recently, pretrained language models have shown state-of-the-art performance on the vulnerability detection task. These models are pretrained on a large corpus of source code, then fine-tuned on a smaller supervised vulnerability dataset. Due to the different training objectives and the performance of the models, it is interesting to consider whether the models have learned the semantics of code relevant to vulnerability detection, namely bug semantics, and if so, how the alignment to bug semantics relates to model performance. In this paper, we analyze the models using three distinct methods: interpretability tools, attention analysis, and interaction matrix analysis. We compare the models' influential feature sets with the bug semantic features which define the causes of bugs, including buggy paths and Potentially Vulnerable Statements (PVS). We find that (1) better-performing models also aligned better with PVS, (2) the models failed to align strongly to PVS, and (3) the models failed to align at all to buggy paths. Based on our analysis, we developed two annotation methods which highlight the bug semantics inside the model's inputs. We evaluated our approach on four distinct transformer models and four vulnerability datasets and found that our annotations improved the models' performance in the majority of settings - 11 out of 16, with up to 9.57 points improvement in F1 score compared to conventional fine-tuning. We further found that with our annotations, the models aligned up to 232% better to potentially vulnerable statements. Our findings indicate that it is helpful to provide the model with information of the bug semantics, that the model can attend to it, and motivate future work in learning more complex path-based bug semantics.",
        "labels": [
            "static analysis",
            "bug detection",
            "empirical study"
        ]
    },
    "SkipAnalyzer: An Embodied Agent for Code Analysis with Large Language Models": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2310-18532",
        "author": "Mohammad Mahdi Mohajer and Reem Aleithan and Nima Shiri Harzevili and Moshi Wei and Alvine Boaye Belle and Hung Viet Pham and Song Wang",
        "title": "SkipAnalyzer: An Embodied Agent for Code Analysis with Large Language Models",
        "journal": "CoRR",
        "volume": "abs/2310.18532",
        "year": "2023",
        "url": "https://doi.org/10.48550/arXiv.2310.18532",
        "doi": "10.48550/ARXIV.2310.18532",
        "eprinttype": "arXiv",
        "eprint": "2310.18532",
        "timestamp": "Thu, 02 Nov 2023 17:30:29 +0100",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2310-18532.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "arXiv2023",
        "abstract": "We introduce SkipAnalyzer, a large language model (LLM)-powered tool for static code analysis. SkipAnalyzer has three components: 1) an LLM-based static bug detector that scans source code and reports specific types of bugs, 2) an LLM-based false-positive filter that can identify false-positive bugs in the results of static bug detectors (e.g., the result of step 1) to improve detection accuracy, and 3) an LLM-based patch generator that can generate patches for the detected bugs above. As a proof-of-concept, SkipAnalyzer is built on ChatGPT, which has exhibited outstanding performance in various software engineering tasks. To evaluate SkipAnalyzer, we focus on two types of typical and critical bugs that are targeted by static bug detection, i.e., Null Dereference and Resource Leak as subjects. We employ Infer to aid the gathering of these two bug types from 10 open-source projects. Consequently, our experiment dataset contains 222 instances of Null Dereference bugs and 46 instances of Resource Leak bugs. Our study demonstrates that SkipAnalyzer achieves remarkable performance in the mentioned static analysis tasks, including bug detection, false-positive warning removal, and bug repair. In static bug detection, SkipAnalyzer achieves accuracy values of up to 68.37% for detecting Null Dereference bugs and 76.95% for detecting Resource Leak bugs, improving the precision of the current leading bug detector, Infer, by 12.86% and 43.13%, respectively. For removing false-positive warnings, SkipAnalyzer can reach a precision of up to 93.88% for Null Dereference bugs and 63.33% for Resource Leak bugs. Additionally, SkipAnalyzer surpasses state-of-the-art false-positive warning removal tools. Furthermore, in bug repair, SkipAnalyzer can generate syntactically correct patches to fix its detected bugs with a success rate of up to 97.30%.",
        "labels": [
            "static analysis",
            "bug detection",
            "agent design"
        ]
    },
    "LLMDFA: Analyzing Dataflow in Code with Large Language Model": {
        "type": "article",
        "key": "llmdfa",
        "author": "Chengpeng Wang and Wuqi Zhang and Zian Su and Xiangzhe Xu and Xiaoheng Xie and Xiangyu Zhang",
        "title": "LLMDFA: Analyzing Dataflow in Code with Large Language Model",
        "journal": "Advances in Neural Information Processing Systems",
        "venue": "NeurIPS2024",
        "abstract": "Dataflow analysis is a fundamental code analysis technique that identifies dependencies between program values. Traditional approaches typically necessitate successful compilation and expert customization, hindering their applicability and usability for analyzing uncompilable programs with evolving analysis needs in realworld scenarios. This paper presents LLMDFA, an LLM-powered compilation-free and customizable dataflow analysis framework. To address hallucinations for reliable results, we decompose the problem into several subtasks and introduce a series of novel strategies. Specifically, we leverage LLMs to synthesize code that outsources delicate reasoning to external expert tools, such as using a parsing library to extract program values of interest and invoking an automated theorem prover to validate path feasibility. Additionally, we adopt a few-shot chain-of-thought prompting to summarize dataflow facts in individual functions, aligning the LLMs with the program semantics of small code snippets to mitigate hallucinations. We evaluate LLMDFA on synthetic programs to detect three representative types of bugs and on real-world Android applications for customized bug detection. On average, LLMDFA achieves 87.10% precision and 80.77% recall, surpassing existing techniques with F1 score improvements of up to 0.35. We have open-sourced LLMDFA at https://github.com/chengpeng-wang/LLMDFA.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://chengpeng-wang.github.io/publications/LLMDFA_NeurIPS2024.pdf"
    },
    "Towards General Loop Invariant Generation: A Benchmark of Programs with Memory Manipulation": {
        "type": "article",
        "key": "shapeloopinvariant",
        "author": "Liu, Chang and Wu, Xiwei and Feng, Yuan and Cao, Qinxiang and Yan, Junchi",
        "title": "Towards General Loop Invariant Generation: A Benchmark of Programs with Memory Manipulation",
        "journal": "Advances in Neural Information Processing Systems",
        "venue": "NeurIPS2024",
        "abstract": "Program verification is vital for ensuring software reliability, especially in the context of increasingly complex systems. Loop invariants, remaining true before and after each iteration of loops, are crucial for this verification process. Traditional provers and machine learning based methods for generating loop invariants often require expert intervention or extensive labeled data, and typically only handle numerical property verification. These methods struggle with programs involving complex data structures and memory manipulations, limiting their applicability and automation capabilities. In this paper, we introduce a new benchmark named LIG-MM, specifically for programs with complex data structures and memory manipulations. We collect 312 programs from various sources, including daily programs from college homework, the international competition (SV-COMP), benchmarks from previous papers (SLING), and programs from real-world software systems (Linux Kernel, GlibC, LiteOS, and Zephyr). Based on LIG-MM, our findings indicate that previous methods, including GPT-4, fail to automate verification for these programs. Consequently, we propose a novel LLM-SE framework that coordinates LLM with symbolic execution, fine-tuned using self-supervised learning, to generate loop invariants. Experimental results on LIG-MM demonstrate that our LLM-SE outperforms state-of-the-art methods, offering a new direction toward automated program verification in real-world scenarios.",
        "labels": [
            "static analysis",
            "program verification",
            "benchmark"
        ],
        "url": "https://arxiv.org/pdf/2311.10483"
    },
    "The Mutators Reloaded: Fuzzing Compilers with Large Language Model Generated Mutation Operators": {
        "type": "article",
        "key": "asplosfuzz",
        "author": "Ou, Xianfei and Li, Cong and Jiang, Yanyan and Xu, Chang",
        "title": "The Mutators Reloaded: Fuzzing Compilers with Large Language Model Generated Mutation Operators",
        "journal": "ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "venue": "ASPLOS2024",
        "abstract": "Crafting high-quality mutators\u2013the core of mutation-based fuzzing that shapes the search space\u2013is challenging. It requires human expertise and creativity, and their implementation demands knowledge of compiler internals. This paper presents MetaMut framework for developing new, useful mutators for compiler fuzzing. It integrates our compilerdomain knowledge into prompts and processes that can best harness the capabilities of a large language model. With MetaMut, we have successfully created 118 semantic-aware mutators at approximately $0.5 each, with only moderate human effort. With these mutators, our fuzzer uncovered 131 bugs in GCC and Clang, 129 of which were confirmed or fixed. The success of MetaMut suggests that the integration of AI into software and system engineering tasks traditionally thought to require expert human intervention could be a promising research direction.",
        "labels": [
            "program testing",
            "fuzzing",
            "compiler testing"
        ],
        "url": "https://connglli.github.io/pdfs/metamut_asplos24.pdf"
    },
    "Unifying the perspectives of nlp and software engineering: A survey on language models for code": {
        "type": "article",
        "key": "antsurvey",
        "author": "Zhang, Ziyin and Chen, Chaoyu and Liu, Bingchang and Liao, Cong and Gong, Zi and Yu, Hang and Li, Jianguo and Wang, Rui",
        "title": "Unifying the perspectives of nlp and software engineering: A survey on language models for code",
        "journal": "Transactions on Machine Learning Research",
        "venue": "TMLR2024",
        "abstract": "In this work we systematically review the recent advancements in software engineering with language models, covering 70+ models, 40+ evaluation tasks, 180+ datasets, and 900 related works. Unlike previous works, we integrate software engineering (SE) with natural language processing (NLP) by discussing the perspectives of both sides: SE applies language models for development automation, while NLP adopts SE tasks for language model evaluation. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also go beyond programming and review LLMs\u2019 application in other software engineering activities including requirement engineering, testing, deployment, and operations in an endeavor to provide a global view of NLP in SE, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.",
        "labels": [
            "general coding task",
            "survey"
        ],
        "url": "https://arxiv.org/pdf/2311.07989"
    },
    "E{\\&}V: Prompting Large Language Models to Perform Static Analysis by Pseudo-code Execution and Verification": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2312-08477",
        "author": "Yu Hao and Weiteng Chen and Ziqiao Zhou and Weidong Cui",
        "title": "E{\\&}V: Prompting Large Language Models to Perform Static Analysis by Pseudo-code Execution and Verification",
        "journal": "CoRR",
        "volume": "abs/2312.08477",
        "year": "2023",
        "url": "https://doi.org/10.48550/arXiv.2312.08477",
        "doi": "10.48550/ARXIV.2312.08477",
        "eprinttype": "arXiv",
        "eprint": "2312.08477",
        "timestamp": "Mon, 08 Jan 2024 17:49:54 +0100",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2312-08477.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "Microsoft2023",
        "abstract": "Static analysis, the process of examining code without executing it, is crucial for identifying software issues. Yet, static analysis is hampered by its complexity and the need for customization for different targets. Traditional static analysis tools require extensive human effort and are often limited to specific target programs and programming languages. Recent advancements in Large Language Models (LLMs), such as GPT-4 and Llama, offer new capabilities for software engineering tasks. However, their application in static analysis, especially in understanding complex code structures, remains under-explored. This paper introduces a novel approach named E&V , which leverages LLMs to perform static analysis. Specifically, E&V employs LLMs to simulate the execution of pseudo-code, effectively conducting static analysis encoded in the pseudo-code with minimal human effort, thereby improving the accuracy of results. E&V includes a verification process for pseudo-code execution without needing an external oracle. This process allows E&V to mitigate hallucinations of LLMs and enhance the accuracy of static analysis results. We have implemented E&V in a prototype tool designed for triaging crashes through backward taint analysis. This prototype, paired with GPT-4-32k, has been applied to triage 170 recently fixed Linux kernel bugs across seven bug categories. Our experiments demonstrate that the prototype correctly identifies the blamed function in 81.2% of the cases. Additionally, we observe that our novel verification process significantly improves the accuracy, increasing it from 28.2% to 81.2%.",
        "labels": [
            "static analysis",
            "bug detection"
        ]
    },
    "LLM-based Resource-Oriented Intention Inference for Static Resource Detection": {
        "type": "article",
        "key": "wang2023boosting",
        "title": "LLM-based Resource-Oriented Intention Inference for Static Resource Detection",
        "author": "Wang, Chong and Liu, Jianan and Peng, Xin and Liu, Yang and Lou, Yiling",
        "journal": "arXiv preprint arXiv:2311.04448v2",
        "year": "2025",
        "venue": "ICSE2025",
        "abstract": "Resource leaks, caused by resources not being released after acquisition, often lead to performance issues and system crashes. Existing static detection techniques rely on mechanical matching of predefined resource acquisition/release APIs and null-checking conditions to find unreleased resources, suffering from both (1) false negatives caused by the incompleteness of predefined resource acquisition/release APIs and (2) false positives caused by the unsoundness of resource reachability validation analysis. To overcome these challenges, we propose InferROI, a novel approach that leverages the exceptional code comprehension capability of large language models (LLMs) to directly infer resource-oriented intentions (acquisition, release, and reachability validation) in code. InferROI first prompts the LLM to infer involved intentions for a given code snippet, and then incorporates a two-stage static analysis approach to check control-flow paths for resource leak detection based on the inferred intentions.We evaluate the effectiveness of InferROI in both resource-oriented intention inference and resource leak detection. Experimental results demonstrate that InferROI achieves a precision of 74.6% and a recall of 81.8% when inferring resource-oriented intentions for 172 code snippets from the DroidLeaks dataset. Additionally, InferROI covers a significant portion (i.e., 67.9% ) of concerned Android resources listed in the dataset. When applied to 86 bugs from the DroidLeaks dataset, InferROI exhibits both a high bug detection rate (53.5%) and a low false alarm rate (8.1%) compared to eight baseline detectors. Moreover, we apply InferROI to resource leak detection in 100 methods from real-world open-source projects, where InferROI identifies 12 unknown resource leak bugs, with 7 of them being confirmed by developers. Overall, this work demonstrates the promising potential of LLMs in resource leak detection.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/abs/2311.04448"
    },
    "Interleaving Static Analysis and LLM Prompting": {
        "type": "inproceedings",
        "key": "chapman2024interleaving",
        "title": "Interleaving Static Analysis and LLM Prompting",
        "author": "Chapman, Patrick J and Rubio-Gonz{\\'a}lez, Cindy and Thakur, Aditya V",
        "booktitle": "Proceedings of the 13th ACM SIGPLAN International Workshop on the State Of the Art in Program Analysis",
        "pages": "9--17",
        "year": "2024",
        "venue": "SOAP2024",
        "abstract": "This paper presents a new approach for using Large Language Models (LLMs) to improve static program analysis. Specifically, during program analysis, we interleave calls to the static analyzer and queries to the LLM: the prompt used to query the LLM is constructed using intermediate results from the static analysis, and the result from the LLM query is used for subsequent analysis of the program. We apply this novel approach to the problem of error-specification inference of functions in systems code written in C; i.e., inferring the set of values returned by each function upon error, which can aid in program understanding as well as in finding error-handling bugs. We evaluate our approach on real-world C programs, such as MbedTLS and zlib, by incorporating LLMs into EESI, a state-of-the-art static analysis for error-specification inference. Compared to EESI, our approach achieves higher recall across all benchmarks (from average of 52.55% to 77.83%) and higher F1-score (from average of 0.612 to 0.804) while maintaining precision (from average of 86.67% to 85.12%).",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://web.cs.ucdavis.edu/~rubio/includes/soap24.pdf"
    },
    "LLM-Assisted Static Analysis for Detecting Security Vulnerabilities": {
        "type": "article",
        "key": "li2024llm",
        "title": "LLM-Assisted Static Analysis for Detecting Security Vulnerabilities",
        "author": "Li, Ziyang and Dutta, Saikat and Naik, Mayur",
        "journal": "arXiv preprint arXiv:2405.17238",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Software is prone to security vulnerabilities. Program analysis tools to detect them have limited effectiveness in practice due to their reliance on human labeled specifications. Large language models (or LLMs) have shown impressive code generation capabilities but they cannot do complex reasoning over code to detect such vulnerabilities especially since this task requires whole-repository analysis. We propose IRIS, a neuro-symbolic approach that systematically combines LLMs with static analysis to perform whole-repository reasoning for security vulnerability detection. Specifically, IRIS leverages LLMs to infer taint specifications and perform contextual analysis, alleviating needs for human specifications and inspection. For evaluation, we curate a new dataset, CWE-Bench-Java, comprising 120 manually validated security vulnerabilities in real-world Java projects. A state-of-the-art static analysis tool CodeQL detects only 27 of these vulnerabilities whereas IRIS with GPT-4 detects 55 (+28) and improves upon CodeQL's average false discovery rate by 5% points. Furthermore, IRIS identifies 6 previously unknown vulnerabilities which cannot be found by existing tools.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/abs/2405.17238"
    },
    "Beware of the unexpected: Bimodal taint analysis": {
        "type": "inproceedings",
        "key": "chow2023beware",
        "title": "Beware of the unexpected: Bimodal taint analysis",
        "author": "Chow, Yiu Wai and Sch{\\\"a}fer, Max and Pradel, Michael",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "211--222",
        "year": "2023",
        "venue": "ISSTA2023",
        "abstract": "Static analysis is a powerful tool for detecting security vulnerabilities and other programming problems. Global taint tracking, in particular, can spot vulnerabilities arising from complicated data flow across multiple functions. However, precisely identifying which flows are problematic is challenging, and sometimes depends on factors beyond the reach of pure program analysis, such as conventions and informal knowledge. For example, learning that a parameter name of an API function locale ends up in a file path is surprising and potentially problematic. In contrast, it would be completely unsurprising to find that a parameter command passed to an API function execaCommand is eventually interpreted as part of an operating-system command. This paper presents Fluffy, a bimodal taint analysis that combines static analysis, which reasons about data flow, with machine learning, which probabilistically determines which flows are potentially problematic. The key idea is to let machine learning models predict from natural language information involved in a taint flow, such as API names, whether the flow is expected or unexpected, and to inform developers only about the latter. We present a general framework and instantiate it with four learned models, which offer different trade-offs between the need to annotate training data and the accuracy of predictions. We implement Fluffy on top of the CodeQL analysis framework and apply it to 250K JavaScript projects. Evaluating on five common vulnerability types, we find that Fluffy achieves an F1 score of 0.85 or more on four of them across a variety of datasets.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/pdf/2301.10545.pdf"
    },
    "Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications": {
        "type": "article",
        "key": "ma2024combining",
        "title": "Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications",
        "author": "Ma, Wei and Wu, Daoyuan and Sun, Yuqiang and Wang, Tianwen and Liu, Shangqing and Zhang, Jian and Xue, Yue and Liu, Yang",
        "journal": "arXiv preprint arXiv:2403.16073",
        "year": "2024",
        "venue": "ICSE2025",
        "abstract": "Smart contracts are decentralized applications built atop blockchains like Ethereum. Recent research has shown that large language models (LLMs) have potential in auditing smart contracts, but the state-of-the-art indicates that even GPT-4 can achieve only 30% precision (when both decision and justification are correct). This is likely because off-the-shelf LLMs were primarily pre-trained on a general text/code corpus and not fine-tuned on the specific domain of Solidity smart contract auditing. In this paper, we propose TrustLLM, a general framework that combines fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications. Specifically, TrustLLM is inspired by the observation that expert human auditors first perceive what could be wrong and then perform a detailed analysis of the code to identify the cause. As such, TrustLLM employs a two-stage fine-tuning approach: it first tunes a Detector model to make decisions and then tunes a Reasoner model to generate causes of vulnerabilities. However, fine-tuning alone faces challenges in accurately identifying the optimal cause of a vulnerability. Therefore, we introduce two LLM-based agents, the Ranker and Critic, to iteratively select and debate the most suitable cause of vulnerability based on the output of the fine-tuned Reasoner model. To evaluate TrustLLM, we collected a balanced dataset with 1,734 positive and 1,810 negative samples to fine-tune TrustLLM. We then compared it with traditional fine-tuned models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) as well as prompt learning-based LLMs (GPT4, GPT-3.5, and CodeLlama-13b/34b). On a dataset of 263 real smart contract vulnerabilities, TrustLLM achieves an F1 score of 91.21% and an accuracy of 91.11%. The causes generated by TrustLLM achieved a consistency of about 38% compared to the ground truth causes.",
        "labels": [
            "static analysis",
            "bug detection",
            "agent design"
        ],
        "url": "https://arxiv.org/pdf/2403.16073"
    },
    "Leveraging Semantic Relations in Code and Data to Enhance Taint Analysis of Embedded Systems": {
        "type": "inproceedings",
        "key": "zhao2024leveraging",
        "title": "Leveraging Semantic Relations in Code and Data to Enhance Taint Analysis of Embedded Systems",
        "author": "Zhao, Jiaxu and Li, Yuekang and Zou, Yanyan and Liang, Zhaohui and Xiao, Yang and Li, Yeting and Peng, Bingwei and Zhong, Nanyu and Wang, Xinyi and Wang, Wei and others",
        "booktitle": "33rd USENIX Security Symposium (USENIX Security 24)",
        "pages": "7067--7084",
        "year": "2024",
        "venue": "USENIXSec2024",
        "abstract": "IoT devices have significantly impacted our daily lives, and detecting vulnerabilities in embedded systems early on is critical for ensuring their security. Among the existing vulnerability detection techniques for embedded systems, static taint analysis has been proven effective in detecting severe vulnerabilities, such as command injection vulnerabilities, which can cause remote code execution. Nevertheless, static taint analysis is faced with the problem of identifying sources comprehensively and accurately.",
        "labels": [
            "static analysis",
            "bug detection",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://www.usenix.org/system/files/usenixsecurity24-zhao.pdf"
    },
    "When Threads Meet Interrupts: Effective Static Detection of Interrupt-Based Deadlocks in Linux": {
        "type": "inproceedings",
        "key": "chengfeng2024",
        "title": "When Threads Meet Interrupts: Effective Static Detection of Interrupt-Based Deadlocks in Linux",
        "author": "Chengfeng Ye, Yuandao Cai, and Charles Zhang",
        "booktitle": "33rd USENIX Security Symposium (USENIX Security 24)",
        "year": "2024",
        "venue": "USENIXSec2024",
        "abstract": "Deadlocking is an unresponsive state of software that arises when threads hold locks while trying to acquire other locks that are already held by other threads, resulting in a circular lock dependency. Interrupt-based deadlocks, a specific and prevalent type of deadlocks that occur within the OS kernel due to interrupt preemption, pose significant risks to system functionality, performance, and security. However, existing static analysis tools focus on resource-based deadlocks without characterizing the interrupt preemption. In this paper, we introduce Archerfish, the first static analysis approach for effectively identifying interrupt-based deadlocks in the large-scale Linux kernel. At its core, Archerfish utilizes an Interrupt-Aware Lock Graph (ILG) to capture both regular and interrupt-related lock dependencies, reducing the deadlock detection problem to graph cycle discovery and refinement. Furthermore, Archerfish incorporates four effective analysis components to construct ILG and refine the deadlock cycles, addressing three core challenges, including the extensive interrupt-involving concurrency space, identifying potential interrupt handlers, and validating the feasibility of deadlock cycles. Our experimental results show that Archerfish can precisely analyze the Linux kernel (19.8 MLoC) in approximately one hour. At the time of writing, we have discovered 76 previously unknown deadlocks, with 53 bugs confirmed, 46 bugs already fixed by the Linux community, and 2 CVE IDs assigned. Notably, those found deadlocks are long-latent, hiding for an average of 9.9 years.",
        "labels": [
            "static analysis",
            "bug detection",
            "specification inference"
        ],
        "url": "https://www.usenix.org/system/files/usenixsecurity24-zhao.pdf"
    },
    "Automatically Inspecting Thousands of Static Bug Warnings with Large Language Model: How Far Are We?": {
        "type": "inproceedings",
        "key": "cw2024TKDD",
        "title": "Automatically Inspecting Thousands of Static Bug Warnings with Large Language Model: How Far Are We?",
        "author": "Cheng Wen, Yuandao Cai, Bin Zhang, Jie Su, Zhiwu Xu, Dugang Liu, Shengchao Qin, Zhong Ming, Tian Cong",
        "booktitle": "TKDD",
        "year": "2024",
        "venue": "TKDD2024",
        "abstract": "Static analysis tools for capturing bugs and vulnerabilities in software programs are widely employed in practice, as they have the unique advantages of high coverage and independence from the execution environment. However, existing tools for analyzing large codebases often produce a great deal of false warnings over genuine bug reports. As a result, developers are required to manually inspect and confirm each warning, a challenging, time-consuming, and automation-essential task.\n This article advocates a fast, general, and easily extensible approach called Llm4sa that automatically inspects a sheer volume of static warnings by harnessing (some of) the powers of Large Language Models (LLMs). Our key insight is that LLMs have advanced program understanding capabilities, enabling them to effectively act as human experts in conducting manual inspections on bug warnings with their relevant code snippets. In this spirit, we propose a static analysis to effectively extract the relevant code snippets via program dependence traversal guided by the bug warning reports themselves. Then, by formulating customized questions that are enriched with domain knowledge and representative cases to query LLMs, Llm4sa can remove a great deal of false warnings and facilitate bug discovery significantly. Our experiments demonstrate that Llm4sa is practical in automatically inspecting thousands of static warnings from Juliet benchmark programs and 11 real-world C/C++ projects, showcasing a high precision (81.13%) and a recall rate (94.64%) for a total of 9,547 bug warnings. Our research introduces new opportunities and methodologies for using the LLMs to reduce human labor costs, improve the precision of static analyzers, and ensure software trustworthiness",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3653718"
    },
    "Fuzzing BusyBox: Leveraging LLM and Crash Reuse for Embedded Bug Unearthing": {
        "type": "inproceedings",
        "key": "Asmita2024",
        "title": "Fuzzing BusyBox: Leveraging LLM and Crash Reuse for Embedded Bug Unearthing",
        "author": "Asmita, Yaroslav Oliinyk,  Michael Scott, Ryan Tsang, Chongzhou Fang, and Houman Homayoun",
        "booktitle": "33rd USENIX Security Symposium (USENIX Security 24)",
        "year": "2024",
        "venue": "USENIXSec2024",
        "abstract": "BusyBox, an open-source software bundling over 300 essential Linux commands into a single executable, is ubiquitous in Linux-based embedded devices. Vulnerabilities in BusyBox can have far-reaching consequences, affecting a wide array of devices. This research, driven by the extensive use of BusyBox, delved into its analysis. The study revealed the prevalence of older BusyBox versions in real-world embedded products, prompting us to conduct fuzz testing on BusyBox. Fuzzing, a pivotal software testing method, aims to induce crashes that are subsequently scrutinized to uncover vulnerabilities. Within this study, we introduce two techniques to fortify software testing. The first technique enhances fuzzing by leveraging Large Language Models (LLM) to generate target-specific initial seeds. Our study showed a substantial increase in crashes when using LLM-generated initial seeds, highlighting the potential of LLM to efficiently tackle the typically labor-intensive task of generating target-specific initial seeds. The second technique involves repurposing previously acquired crash data from similar fuzzed targets before initiating fuzzing on a new target. This approach streamlines the time-consuming fuzz testing process by providing crash data directly to the new target before commencing fuzzing. We successfully identified crashes in the latest BusyBox target without conducting traditional fuzzing, emphasizing the effectiveness of LLM and crash reuse techniques in enhancing software testing and improving vulnerability detection in embedded systems. Additionally, manual triaging was performed to identify the nature of crashes in the latest BusyBox.",
        "labels": [
            "program testing",
            "fuzzing"
        ],
        "url": "https://www.usenix.org/system/files/usenixsecurity24-asmita.pdf"
    },
    "Gptscan: Detecting logic vulnerabilities in smart contracts by combining gpt with program analysis": {
        "type": "inproceedings",
        "key": "sun2024gptscan",
        "title": "Gptscan: Detecting logic vulnerabilities in smart contracts by combining gpt with program analysis",
        "author": "Sun, Yuqiang and Wu, Daoyuan and Xue, Yue and Liu, Han and Wang, Haijun and Xu, Zhengzi and Xie, Xiaofei and Liu, Yang",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "pages": "1--13",
        "year": "2024",
        "venue": "ICSE2024",
        "abstract": "Smart contracts are prone to various vulnerabilities, leading to substantial financial losses over time. Current analysis tools mainly target vulnerabilities with fixed control- or data-flow patterns, such as re-entrancy and integer overflow. However, a recent study on Web3 security bugs revealed that about 80% of these bugs cannot be audited by existing tools due to the lack of domain-specific property description and checking. Given recent advances in Large Language Models (LLMs), it is worth exploring how Generative Pre-training Transformer (GPT) could aid in detecting logic vulnerabilities.In this paper, we propose GPTScan, the first tool combining GPT with static analysis for smart contract logic vulnerability detection. Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge, we utilize GPT as a versatile code understanding tool. By breaking down each logic vulnerability type into scenarios and properties, GPTScan matches candidate vulnerabilities with GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation. Evaluation on diverse datasets with around 400 contract projects and 3K Solidity files shows that GPTScan achieves high precision (over 90%) for token contracts and acceptable precision (57.14%) for large projects like Web3Bugs. It effectively detects ground-truth logic vulnerabilities with a recall of over 70%, including 9 new vulnerabilities missed by human auditors. GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01 USD to scan per thousand lines of Solidity code. Moreover, static confirmation helps GPTScan reduce two-thirds of false positives.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/pdf/2308.03314.pdf?utm_source=Klaviyo&utm_medium=campaign&_kx="
    },
    "If At First You Don’t Succeed, Try, Try, Again...? Insights and LLM-informed Tooling for Detecting Retry Bugs in Software Systems": {
        "type": "inproceedings",
        "key": "Bogdan2024SOSP",
        "title": "If At First You Don’t Succeed, Try, Try, Again...? Insights and LLM-informed Tooling for Detecting Retry Bugs in Software Systems",
        "author": "Bogdan A. Stoica, Utsav Sethi, Yiming Su, Cyrus Zhou, Shan Lu, Jonathan Mace, Madanlal Musuvathi, Suman Nath",
        "booktitle": "SOSP",
        "pages": "1--13",
        "year": "2024",
        "venue": "SOSP2024",
        "abstract": "Retry—the re-execution of a task on failure—is a common mechanism to enable resilient software systems. Yet, despite its commonality and long history, retry remains difficult to implement and test. Guided by our study of real-world retry issues, we propose a novel suite of static and dynamic techniques to detect retry problems in software. We find that the ad-hoc nature of retry implementation poses challenges for traditional program analysis but can be well suited for large language models; and that carefully repurposing existing unit tests can, along with fault injection, expose various types of retry problems.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695971"
    },
    "An investigation into misuse of java security apis by large language models": {
        "type": "inproceedings",
        "key": "mousavi2024investigation",
        "title": "An investigation into misuse of java security apis by large language models",
        "author": "Mousavi, Zahra and Islam, Chadni and Moore, Kristen and Abuadbba, Alsharif and Babar, M Ali",
        "booktitle": "Proceedings of the 19th ACM Asia Conference on Computer and Communications Security",
        "pages": "1299--1315",
        "year": "2024",
        "venue": "ASIACCS2024",
        "abstract": "The increasing trend of using Large Language Models (LLMs) for code generation raises the question of their capability to generate trustworthy code. While many researchers are exploring the utility of code generation for uncovering software vulnerabilities, one crucial but often overlooked aspect is the security Application Programming Interfaces (APIs). APIs play an integral role in upholding software security, yet effectively integrating security APIs presents substantial challenges. This leads to inadvertent misuse by developers, thereby exposing software to vulnerabilities. To overcome these challenges, developers may seek assistance from LLMs. In this paper, we systematically assess ChatGPT's trustworthiness in code generation for security API use cases in Java. To conduct a thorough evaluation, we compile an extensive collection of 48 programming tasks for 5 widely used security APIs. We employ both automated and manual approaches to effectively detect security API misuse in the code generated by ChatGPT for these tasks. Our findings are concerning: around 70% of the code instances across 30 attempts per task contain security API misuse, with 20 distinct misuse types identified. Moreover, for roughly half of the tasks, this rate reaches 100%, indicating that there is a long way to go before developers can rely on ChatGPT to securely implement security API code.",
        "labels": [
            "code model",
            "code model security",
            "empirical study"
        ],
        "url": "https://arxiv.org/html/2404.03823v1"
    },
    "Smartinv: Multimodal learning for smart contract invariant inference": {
        "type": "inproceedings",
        "key": "wang2024smartinv",
        "title": "Smartinv: Multimodal learning for smart contract invariant inference",
        "author": "Wang, Sally Junsong and Pei, Kexin and Yang, Junfeng",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "pages": "126--126",
        "year": "2024",
        "organization": "IEEE Computer Society",
        "venue": "S&P2024",
        "abstract": "Smart contracts are software programs that enable diverse business activities on the blockchain. Recent research has identified new classes of \"machine un-auditable\" bugs that arise from source code not meeting underlying transaction contexts. Existing detection methods require human understanding of underlying transaction logic and manual reasoning across different sources of context (i.e., modalities), such as code and natural language specifying the expected transaction behavior.To automate the detection of \"machine un-auditable\" bugs, we present SmartInv, an accurate and fast smart contract invariant inference framework. Our key insight is that the expected behavior of smart contracts, as specified by invariants, relies on understanding and reasoning across multimodal information, such as source code and natural language. We propose a new finetuning and prompting strategy to foundation models, Tier of Thought (ToT), to reason across multiple modalities of smart contracts and to generate invariants. SmartInv then localizes potential vulnerabilities by checking the violation of those generated invariants.We evaluate SmartInv on real-world smart contract bugs that resulted in financial losses over the past 2.5 years (from January 1, 2021 to May 31, 2023). Extensive evaluation shows that SmartInv can generate useful invariants to effectively localize \"machine un-auditable\" bugs, from which SmartInv uncovers 119 zero-day bugs. We sampled eight bugs and reported them to the respective developers. Six vulnerabilities were quickly fixed by the developers, five of which are confirmed as \"high severity.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a126/1Ub23GNTAeQ"
    },
    "Harnessing the power of llm to support binary taint analysis": {
        "type": "article",
        "key": "liu2023harnessing",
        "title": "Harnessing the power of llm to support binary taint analysis",
        "author": "Liu, Puzhuo and Sun, Chengnian and Zheng, Yaowen and Feng, Xuan and Qin, Chuan and Wang, Yuncheng and Li, Zhi and Sun, Limin",
        "journal": "arXiv preprint arXiv:2310.08275",
        "year": "2024",
        "venue": "TOSEM2024",
        "abstract": "This paper proposes LATTE, the first static binary taint analysis that is powered by a large language model (LLM). LATTE is superior to the state of the art (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully automated while prior static binary taint analyzers need rely on human expertise to manually customize taint propagation rules and vulnerability inspection rules. Second, LATTE is significantly effective in vulnerability detection, demonstrated by our comprehensive evaluations. For example, LATTE has found 37 new bugs in real-world firmware which the baselines failed to find, and 7 of them have been assigned CVE numbers. Lastly, LATTE incurs remarkably low engineering cost, making it a cost-efficient and scalable solution for security researchers and practitioners. We strongly believe that LATTE opens up a new direction to harness the recent advance in LLMs to improve vulnerability analysis for binary programs.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/abs/2310.08275"
    },
    "AbsInt-AI: Language Models for Abstract Interpretation": {
        "type": "article",
        "key": "absint-ai",
        "title": "AbsInt-AI: Language Models for Abstract Interpretation",
        "author": "Michael Wang, Kexin Pei, Armando Solar-Lezama",
        "journal": "ICLR",
        "year": "2025",
        "venue": "ICLR2025",
        "abstract": "Static program analysis is a popular technique in software engineering. Traditional static analysis algorithms treat programs as sets of logical statements with well-defined semantics. These traditional analyzers can provide guarantees of their performance, such as guaranteeing that they will never miss a bug. However, they leave out lots of very rich information such as variable and field names. Language models for code on the other hand, take full advantage of information such as variable names, but it is extremely difficult to provide guarantees of their output. In this work, we present ABSINT-AI, a language model augmented static analyzer based on abstract interpretation that combines the best of both worlds. Using a language model in ABSINT-AI achieves up to a 70% decrease in false positives for bug detection while providing guarantees of never missing a bug.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://openreview.net/forum?id=3RP6YmKo59"
    },
    "LLM-Driven Multi-step Translation from C to Rust using Static Analysis": {
        "type": "article",
        "key": "absint-ai",
        "title": "LLM-Driven Multi-step Translation from C to Rust using Static Analysis",
        "author": "Tianyang Zhou, Haowen Lin, Somesh Jha, Mihai Christodorescu, Kirill Levchenko, Varun Chandrasekaran",
        "journal": "ICLR",
        "year": "2025",
        "venue": "ICLR2025",
        "abstract": "Translating software written in legacy languages to modern languages, such as C to Rust, has significant benefits in improving memory safety while maintaining high performance. However, manual translation is cumbersome, error-prone, and produces unidiomatic code. Large language models (LLMs) have demonstrated promise in producing idiomatic translations, but offer no correctness guarantees as they lack the ability to capture all the semantics differences between the source and target languages. To resolve this issue, we propose SACTOR, an LLM-driven C-to-Rust zero-shot translation tool using a two-step translation methodology: an “unidiomatic” step to translate C into Rust while preserving semantics, and an “idiomatic” step to refine the code to follow Rust’s semantic standards. SACTOR utilizes information provided by static analysis of the source C program to address challenges such as pointer semantics and dependency resolution. To validate the correctness of the translated result from each step, we use end-to-end testing via the foreign function interface to embed our translated code segment into the original code. We evaluate the translation of 200 programs from two datasets and two case studies, comparing the performance of GPT-4o, Claude 3.5 Sonnet, Gemini 2.0 Flash, Llama 3.3 70B and DeepSeek-R1 in SACTOR. Our results demonstrate that SACTOR achieves high correctness and improved idiomaticity, with the best-performing model (DeepSeek-R1) reaching 93% and (GPT-4o, Claude 3.5, DeepSeek-R1) reaching 84% correctness (on each dataset, respectively), while producing more natural and Rust-compliant translations compared to existing methods.",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "url": "https://arxiv.org/html/2503.12511v2"
    },
    "Do you still need a manual smart contract audit?": {
        "type": "article",
        "key": "david2023you",
        "title": "Do you still need a manual smart contract audit?",
        "author": "David, Isaac and Zhou, Liyi and Qin, Kaihua and Song, Dawn and Cavallaro, Lorenzo and Gervais, Arthur",
        "journal": "arXiv preprint arXiv:2306.12338",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "We investigate the feasibility of employing large language models (LLMs) for conducting the security audit of smart contracts, a traditionally time-consuming and costly process. Our research focuses on the optimization of prompt engineering for enhanced security analysis, and we evaluate the performance and accuracy of LLMs using a benchmark dataset comprising 52 Decentralized Finance (DeFi) smart contracts that have previously been compromised.     Our findings reveal that, when applied to vulnerable contracts, both GPT-4 and Claude models correctly identify the vulnerability type in 40% of the cases. However, these models also demonstrate a high false positive rate, necessitating continued involvement from manual auditors. The LLMs tested outperform a random model by 20% in terms of F1-score. To ensure the integrity of our study, we conduct mutation testing on five newly developed and ostensibly secure smart contracts, into which we manually insert two and 15 vulnerabilities each. This testing yielded a remarkable best-case 78.7% true positive rate for the GPT-4-32k model. We tested both, asking the models to perform a binary classification on whether a contract is vulnerable, and a non-binary prompt. We also examined the influence of model temperature variations and context length on the LLM's performance. Despite the potential for many further enhancements, this work lays the groundwork for a more efficient and economical approach to smart contract security audits.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/pdf/2306.12338.pdf"
    },
    "Automated Static Vulnerability Detection via a Holistic Neuro-symbolic Approach": {
        "type": "article",
        "key": "MoCQ2025",
        "title": "Automated Static Vulnerability Detection via a Holistic Neuro-symbolic Approach",
        "author": "Penghui Li, Songchen Yao, Josef Sarfati Korich, Changhua Luo, Jianjia Yu, Yinzhi Cao, Junfeng Yang",
        "journal": "arXiv preprint arXiv:2504.16057",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "Static vulnerability detection is still a challenging problem and demands excessive human efforts, e.g., manual curation of good vulnerability patterns. None of prior works, including classic program analysis or Large Language Model (LLM)-based approaches, have fully automated such vulnerability pattern generations with reasonable detection accuracy. In this paper, we design and implement, MoCQ, a novel holistic neuro-symbolic framework that combines the complementary strengths of LLMs and classical static analysis to enable scalable vulnerability detection. The key insight is that MoCQ leverages an LLM to automatically extract vulnerability patterns and translate them into detection queries, and then on static analysis to refine such queries in a feedback loop and eventually execute them for analyzing large codebases and mining vulnerabilities. We evaluate MoCQ on seven types of vulnerabilities spanning two programming languages. We found MoCQ-generated queries uncovered at least 12 patterns that were missed by experts. On a ground truth dataset, MoCQ achieved comparable precision and recall compared to expert-crafted queries. Moreover, MoCQ has identified seven previously unknown vulnerabilities in real-world applications, demonstrating its practical effectiveness. We have responsibly disclosed them to the corresponding developers.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/abs/2504.16057"
    },
    "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair": {
        "type": "article",
        "key": "williamt2025",
        "title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair",
        "author": "Han Zheng, Ilia Shumailov, Tianqi Fan, Aiden Hall, Mathias Payer",
        "journal": "arXiv preprint arXiv:2505.13103",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "The rapid advancement of bug-finding techniques has led to the discovery of more vulnerabilities than developers can reasonably fix, creating an urgent need for effective Automated Program Repair (APR) methods. However, the complexity of modern bugs often makes precise root cause analysis difficult and unreliable. To address this challenge, we propose crash-site repair to simplify the repair task while still mitigating the risk of exploitation. In addition, we introduce a template-guided patch generation approach that significantly reduces the token cost of Large Language Models (LLMs) while maintaining both efficiency and effectiveness. We implement our prototype system, WILLIAMT, and evaluate it against state-of-the-art APR tools. Our results show that, when combined with the top-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and increases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open source software vulnerabilities benchmark. Furthermore, we demonstrate that WILLIAMT can function effectively even without access to frontier LLMs: even a local model running on a Mac M4 Mini achieves a reasonable repair rate. These findings highlight the broad applicability and scalability of WILLIAMT.",
        "labels": [
            "code generation",
            "program repair"
        ],
        "url": "https://arxiv.org/abs/2505.13103"
    },
    "Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities": {
        "type": "article",
        "key": "EnIGMA",
        "title": "Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities",
        "author": "Talor Abramovich, Meet Udeshi, Minghao Shao, Kilian Lieret, Haoran Xi, Kimberly Milner, Sofija Jancheska, John Yang, Carlos E. Jimenez, Farshad Khorrami, Prashanth Krishnamurthy, Brendan Dolan-Gavitt, Muhammad Shafique, Karthik Narasimhan, Ramesh Karri, Ofir Press",
        "journal": "arXiv preprint arXiv:2409.16165",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "Although language model (LM) agents have demonstrated increased performance in multiple domains, including coding and web-browsing, their success in cybersecurity has been limited. We present EnIGMA, an LM agent for autonomously solving Capture The Flag (CTF) challenges. We introduce new tools and interfaces to improve the agent's ability to find and exploit security vulnerabilities, focusing on interactive terminal programs. These novel Interactive Agent Tools enable LM agents, for the first time, to run interactive utilities, such as a debugger and a server connection tool, which are essential for solving these challenges. Empirical analysis on 390 CTF challenges across four benchmarks demonstrate that these new tools and interfaces substantially improve our agent's performance, achieving state-of-the-art results on NYU CTF, Intercode-CTF, and CyBench. Finally, we analyze data leakage, developing new methods to quantify it and identifying a new phenomenon we term soliloquizing, where the model self-generates hallucinated observations without interacting with the environment. Our code and development dataset are available at this https URL and this https URL respectively.",
        "labels": [
            "program testing",
            "vulnerability exploitation",
            "benchmark"
        ],
        "url": "https://arxiv.org/abs/2409.16165"
    },
    "Can LLMs Implicitly Learn Numeric Parameter Constraints in Data Science APIs?": {
        "type": "article",
        "key": "yinling2024NeurIPS",
        "title": "Can LLMs Implicitly Learn Numeric Parameter Constraints in Data Science APIs?",
        "author": "Yinlin Deng, Chunqiu Steven Xia, Zhezhen Cao, Meiziniu Li, Lingming Zhang",
        "journal": "NeurIPS2024",
        "year": "2024",
        "venue": "NeurIPS2024",
        "abstract": "Data science (DS) programs, typically built on popular DS libraries (such as PyTorch and NumPy) with thousands of APIs, serve as the cornerstone for various mission-critical domains such as financial systems, autonomous driving software, and coding assistants. Recently, large language models (LLMs) have been widely applied to generate DS programs across diverse scenarios, such as assisting users for DS programming or detecting critical vulnerabilities in DS frameworks. Such applications have all operated under the assumption, that LLMs can implicitly model the numerical parameter constraints in DS library APIs and produce valid code. However, this assumption has not been rigorously studied in the literature. In this paper, we empirically investigate the proficiency of LLMs to handle these implicit numerical constraints when generating DS programs. We studied 28 widely used APIs from PyTorch and NumPy, and scrutinized the LLMs’ generation performance in different levels of granularity: full programs, all parameters, and individual parameters of a single API. We evaluated both state-of-the-art open-source and closed-source models. The results show that LLMs are great at generating simple DS programs, particularly those that follow common patterns seen in training data. However, as we increase the difficulty by providing more complex/unusual inputs, the performance of LLMs drops significantly. We also observe that GPT-4-Turbo can sustain much higher performance overall, but still cannot handle arithmetic API constraints well. In summary, while LLMs exhibit the ability to memorize common patterns of popular DS API usage through massive training, they overall lack genuine comprehension of the underlying numerical constraints.",
        "labels": [
            "static analysis",
            "specification inference"
        ],
        "url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/617ffb01ea5b57769b0d63d5e9fefd3f-Paper-Conference.pdf"
    },
    "Specification-Driven Code Translation Powered by Large Language Models: How Far Are We?": {
        "type": "article",
        "key": "arXiv:2412.04590",
        "title": "Specification-Driven Code Translation Powered by Large Language Models: How Far Are We?",
        "author": "Soumit Kanti Saha, Fazle Rabbi, Song Wang, Jinqiu Yang",
        "journal": "arXiv preprint arXiv:2412.04590",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Large Language Models (LLMs) are increasingly being applied across various domains, including code-related tasks such as code translation. Previous studies have explored using LLMs for translating code between different programming languages. Since LLMs are more effective with natural language, using natural language as an intermediate representation in code translation tasks presents a promising approach. In this work, we investigate using NL-specification as an intermediate representation for code translation. We evaluate our method using three datasets, five popular programming languages, and 29 language pair permutations. Our results show that using NL-specification alone does not lead to performance improvements. However, when combined with source code, it provides a slight improvement over the baseline in certain language pairs. Besides analyzing the performance of code translation, we also investigate the quality of the translated code and provide insights into the issues present in the translated code.",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "url": "https://arxiv.org/pdf/2412.04590"
    },
    "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities": {
        "type": "article",
        "key": "arXiv:2412.04590",
        "title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities",
        "author": "Yuxuan Zhu, Antony Kellermann, Dylan Bowman, Philip Li, Akul Gupta, Adarsh Danda, Richard Fang, Conner Jensen, Eric Ihli, Jason Benn, Jet Geronimo, Avi Dhir, Sudhit Rao, Kaicheng Yu, Twm Stone, Daniel Kang",
        "journal": "arXiv preprint arXiv:2503.17332",
        "year": "2025",
        "venue": "ICML2025",
        "abstract": "Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture the Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities involves both specialized expertise to reproduce exploits and a systematic approach to evaluating unpredictable threats. To address this challenge, we introduce CVE-Bench, a real-world cybersecurity benchmark based on critical-severity Common Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents to exploit vulnerable web applications in scenarios that mimic real-world conditions, while also providing effective evaluation of their exploits. Our evaluation shows that the state-of-the-art agent framework can resolve up to 13% of vulnerabilities.",
        "labels": [
            "program testing",
            "vulnerability exploitation"
        ],
        "url": "https://arxiv.org/abs/2503.17332"
    },
    "Leveraging LLMs for Program Verification": {
        "type": "article",
        "key": "fmcad2025",
        "title": "Leveraging LLMs for Program Verification",
        "author": "Adharsh Kamath, Aditya Senthilnathan, Saikat Chakraborty, Pantazis Deligiannis, Shuvendu Lahiri, Akash Lal, Aseem Rastogi, Subhajit Roy, Rahul Sharma",
        "year": "2024",
        "venue": "FMCAD2024",
        "abstract": "Loop invariants are fundamental to reasoning about programs with loops. They establish properties about a given loop’s behavior. When they additionally are inductive, they become useful for the task of formal verification that seeks to establish strong mathematical guarantees about program’s runtime behavior. The inductiveness ensures that the invariants can be checked locally without consulting the entire program, thus are indispensable artifacts in a formal proof of correctness. Finding inductive loop invariants is an undecidable problem, and despite a long history of research towards practical solutions, it remains far from a solved problem. This paper investigates the capabilities of the Large Language Models (LLMs) in offering a new solution towards this old, yet important problem. To that end, we first curate a dataset of verification problems on programs with loops. Next, we design a prompt for exploiting LLMs, obtaining inductive loop invariants, that are checked for correctness using sound symbolic tools. Finally, we explore the effectiveness of using an efficient combination of a symbolic tool and an LLM on our dataset and compare it against a purely symbolic baseline. Our results demonstrate that LLMs can help improve the state-of-the-art in automated program verification.",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "url": "https://www.microsoft.com/en-us/research/publication/finding-inductive-loop-invariants-using-large-language-models/"
    },
    "Artemis: Toward Accurate Detection of Server-Side Request Forgeries through LLM-Assisted Inter-procedural Path-Sensitive Taint Analysis": {
        "type": "article",
        "key": "artemis_oopsla2025",
        "title": "Artemis: Toward Accurate Detection of Server-Side Request Forgeries through LLM-Assisted Inter-procedural Path-Sensitive Taint Analysis",
        "author": "Yuchen Ji, Ting Dai, Zhichao Zhou, Yutian Tang, Jingzhu He",
        "journal": "OOPSLA 2025",
        "year": "2025",
        "venue": "OOPSLA2025",
        "abstract": "Server-side request forgery (SSRF) vulnerabilities are inevitable in PHP web applications. Existing static tools in detecting vulnerabilities in PHP web applications neither contain SSRF-related features to enhance detection accuracy nor consider PHP’s dynamic type features. In this paper, we present Artemis, a static taint analysis tool for detecting SSRF vulnerabilities in PHP web applications. First, Artemis extracts both PHP built-in and third-party functions as candidate source and sink functions. Second, Artemis constructs both explicit and implicit call graphs to infer functions’ relationships. Third, Artemis performs taint analysis based on a set of rules that prevent over-tainting and pauses when SSRF exploitation is impossible. Fourth, Artemis analyzes the compatibility of path conditions to prune false positives. We have implemented a prototype of Artemis and evaluated it on 250 PHP web applications. Artemis reports 207 true vulnerable paths (106 true SSRFs) with 15 false positives. Of the 106 detected SSRFs, 35 are newly found and reported to developers, with 24 confirmed and assigned CVE IDs.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://dl.acm.org/doi/10.1145/3720488"
    },
    "Laurel: Unblocking Automated Verification with Large Language Models": {
        "type": "article",
        "key": "laurel_oopsla2025",
        "title": "Laurel: Unblocking Automated Verification with Large Language Models",
        "author": "YEric Mugnier, Emmanuel Anaya Gonzalez, Nadia Polikarpova, Ranjit Jhala, Zhou Yuanyuan",
        "journal": "OOPSLA 2025",
        "year": "2025",
        "venue": "OOPSLA2025",
        "abstract": "Program verifiers such as Dafny automate proofs by outsourcing them to an SMT solver. This automation is not perfect, however, and the solver often requires hints in the form of assertions, creating a burden for the proof engineer. In this paper, we propose, a tool that alleviates this burden by automatically generating assertions using large language models (LLMs). To improve the success rate of LLMs in this task, we design two domain-specific prompting techniques. First, we help the LLM determine the location of the missing assertion by analyzing the verifier’s error message and inserting an assertion placeholder at that location. Second, we provide the LLM with example assertions from the same codebase, which we select based on a new proof similarity metric. We evaluate our techniques on our new benchmark, a dataset of complex lemmas we extracted from three real-world Dafny codebases. Our evaluation shows that is able to generate over 56.6% of the required assertions given only a few attempts, making LLMs an affordable tool for unblocking program verifiers without human intervention.",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "url": "https://dl.acm.org/doi/10.1145/3720499"
    },
    "DiffSpec: Differential Testing with LLMs using Natural Language Specifications and Code Artifacts": {
        "type": "article",
        "key": "DiffSpec_arxiv2024",
        "title": "DiffSpec: Differential Testing with LLMs using Natural Language Specifications and Code Artifacts",
        "author": "Nikitha Rao, Elizabeth Gilbert, Tahina Ramananandro, Nikhil Swamy, Claire Le Goues, Sarah Fakhoury",
        "journal": "arXiv2024",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Differential testing can be an effective way to find bugs in software systems with multiple implementations that conform to the same specification, like compilers, network protocol parsers, and language runtimes. Specifications for such systems are often standardized in natural language documents, like Instruction Set Architecture (ISA) specifications, Wasm specifications or IETF RFC's. Large Language Models (LLMs) have demonstrated potential in both generating tests and handling large volumes of natural language text, making them well-suited for utilizing artifacts like specification documents, bug reports, and code implementations. In this work, we leverage natural language and code artifacts to guide LLMs to generate targeted, meaningful tests that highlight meaningful behavioral differences between implementations, including those corresponding to bugs. We introduce DiffSpec, a framework for generating differential tests with LLMs using prompt chaining. We demonstrate the efficacy of DiffSpec on two different systems, namely, eBPF runtimes and Wasm validators. Using DiffSpec, we generated 359 differentiating tests, uncovering at least four distinct and confirmed bugs in eBPF, including a kernel memory leak, inconsistent behavior in jump instructions, and undefined behavior when using the stack pointer. We also found 279 differentiating tests in Wasm validators, that point to at least 2 confirmed and fixed bugs in Wizard Engine.",
        "labels": [
            "program testing",
            "differential testing",
            "static analysis",
            "specification inference"
        ],
        "url": "https://arxiv.org/abs/2410.04249"
    },
    "AlphaTrans: A Neuro-Symbolic Compositional Approach for Repository-Level Code Translation and Validation": {
        "type": "article",
        "key": "AlphaTrans_FSE25",
        "title": "AlphaTrans: A Neuro-Symbolic Compositional Approach for Repository-Level Code Translation and Validation",
        "author": "Ali Reza Ibrahimzada, Kaiyao Ke, Mrigank Pawagi, Muhammad Salman Abid, Rangeet Pan, Saurabh Sinha, Reyhaneh Jabbarvand",
        "journal": "FSE2025",
        "year": "2025",
        "venue": "FSE2025",
        "abstract": "Code translation transforms programs from one programming language (PL) to another. Several rule-based transpilers have been designed to automate code translation between different pairs of PLs. However, the rules can become obsolete as the PLs evolve and cannot generalize to other PLs. Recent studies have explored the automation of code translation using Large Language Models (LLMs). One key observation is that such techniques may work well for crafted benchmarks but fail to generalize to the scale and complexity of real-world projects with dependencies, custom types, PL-specific features, etc. We propose AlphaTrans, a neuro-symbolic approach to automate repository-level code translation. AlphaTrans translates both source and test code, and employs multiple levels of validation to ensure the translation preserves the functionality of the source program. To break down the problem for LLMs, AlphaTrans leverages program analysis to decompose the program into fragments and translates them in the reverse call order. We leveraged AlphaTrans to translate ten real-world open-source projects consisting of <836, 8575, 2719> classes, methods, and tests. AlphaTrans breaks down these projects into 17874 fragments and translates the entire repository. 96.40% of the translated fragments are syntactically correct, and AlphaTrans validates the translations' runtime behavior and functional correctness for 27.03% and 25.14% of fragments. On average, the integrated translation and validation take 34 hours to translate a project, showing its scalability in practice. For the incorrect translations, AlphaTrans generates a report including existing translation, stack trace, test errors, or assertion failures. We provided these artifacts to two developers to fix the translation bugs in four projects. They were able to fix the issues in 20.1 hours on average and achieve all passing tests.",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "url": "https://arxiv.org/pdf/2410.24117"
    },
    "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects": {
        "type": "article",
        "key": "arXiv:2412.10133",
        "title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects",
        "author": "Islem Bouzenia, Michael Pradel",
        "journal": "arXiv preprint arXiv:2412.10133",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that installs arbitrary projects, configures them to run test cases, and produces project-specific scripts to reproduce the setup. Inspired by the way a human developer would address this task, our approach is a large language model-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33/55 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of 0.16 dollars, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects.",
        "labels": [
            "program testing",
            "general testing",
            "agent design",
            "planning"
        ],
        "url": "https://arxiv.org/pdf/2412.10133"
    },
    "AI Software Engineer: Programming with Trust": {
        "type": "article",
        "key": "ahbik2025JanArXiv",
        "title": "AI Software Engineer: Programming with Trust",
        "author": "Abhik Roychoudhury, Corina Pasareanu, Michael Pradel, Baishakhi Ray",
        "journal": "arXiv preprint arXiv:2502.13767v1",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "Large Language Models (LLMs) have shown surprising proficiency in generating code snippets, promising to automate large parts of software engineering via artificial intelligence (AI). We argue that successfully deploying AI software engineers requires a level of trust equal to or even greater than the trust established by human-driven software engineering practices. The recent trend toward LLM agents offers a path toward integrating the power of LLMs to create new code with the power of analysis tools to increase trust in the code. This opinion piece comments on whether LLM agents could dominate software engineering workflows in the future and whether the focus of programming will shift from programming at scale to programming with trust.",
        "labels": [
            "code generation",
            "survey"
        ],
        "url": "https://arxiv.org/pdf/2502.13767"
    },
    "Large language model-powered smart contract vulnerability detection: New perspectives": {
        "type": "inproceedings",
        "key": "hu2023large",
        "title": "Large language model-powered smart contract vulnerability detection: New perspectives",
        "author": "Hu, Sihao and Huang, Tiansheng and {\\.I}lhan, Fatih and Tekin, Selim Furkan and Liu, Ling",
        "booktitle": "2023 5th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA)",
        "pages": "297--306",
        "year": "2023",
        "organization": "IEEE",
        "venue": "arXiv2023",
        "abstract": "This paper provides a systematic analysis of the opportunities, challenges, and potential solutions of harnessing Large Language Models (LLMs) such as GPT-4 to dig out vulnerabilities within smart contracts based on our ongoing research. For the task of smart contract vulnerability detection, achieving practical usability hinges on identifying as many true vulnerabilities as possible while minimizing the number of false positives. Nonetheless, our empirical study reveals contradictory yet interesting findings: generating more answers with higher randomness largely boosts the likelihood of producing a correct answer but inevitably leads to a higher number of false positives. To mitigate this tension, we propose an adversarial framework dubbed GPTL ENS that breaks the conventional one-stage detection into two synergistic stages - generation and discrimination, for progressive detection and refinement, wherein the LLM plays dual roles, i.e., AUDITOR and CRITIC, respectively. The goal of AUDITOR is to yield a broad spectrum of vulnerabilities with the hope of encompassing the correct answer, whereas the goal of CRITIC that evaluates the validity of identified vulnerabilities is to minimize the number of false positives. Experimental results and illustrative examples demonstrate that AUDITOR and CRITIC work together harmoniously to yield pronounced improvements over the conventional one-stage detection. GPTL ENS is intuitive, strategic, and entirely LLM-driven without relying on specialist expertise in smart contracts, showcasing its methodical generality and potential to detect a broad spectrum of vulnerabilities. Our code is available at: https://github.com/git-disl/GPTLens.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://arxiv.org/pdf/2310.01152.pdf"
    },
    "Continuous learning for android malware detection": {
        "type": "inproceedings",
        "key": "chen2023continuous",
        "title": "Continuous learning for android malware detection",
        "author": "Chen, Yizheng and Ding, Zhoujie and Wagner, David",
        "booktitle": "32nd USENIX Security Symposium (USENIX Security 23)",
        "pages": "1127--1144",
        "year": "2023",
        "venue": "USENIXSec2023",
        "abstract": "Machine learning methods can detect Android malware with very high accuracy. However, these classifiers have an Achilles heel, concept drift: they rapidly become out of date and ineffective, due to the evolution of malware apps and benign apps. Our research finds that, after training an Android malware classifier on one year's worth of data, the F1 score quickly dropped from 0.99 to 0.76 after 6 months of deployment on new test samples.",
        "labels": [
            "static analysis",
            "bug detection",
            "empirical study"
        ],
        "url": "https://surrealyz.github.io/files/pubs/sec23winter-active-learning-prepub.pdf"
    },
    "Enchanting program specification synthesis by large language models using static analysis and program verification": {
        "type": "inproceedings",
        "key": "wen2024enchanting",
        "title": "Enchanting program specification synthesis by large language models using static analysis and program verification",
        "author": "Wen, Cheng and Cao, Jialun and Su, Jie and Xu, Zhiwu and Qin, Shengchao and He, Mengda and Li, Haokun and Cheung, Shing-Chi and Tian, Cong",
        "booktitle": "International Conference on Computer Aided Verification",
        "pages": "302--328",
        "year": "2024",
        "organization": "Springer",
        "venue": "CAV2024",
        "abstract": "Formal verification provides a rigorous and systematic approach to ensure the correctness and reliability of software systems. Yet, constructing specifications for the full proof relies on domain expertise and non-trivial manpower. In view of such needs, an automated approach for specification synthesis is desired. While existing automated approaches are limited in their versatility, i.e., they either focus only on synthesizing loop invariants for numerical programs, or are tailored for specific types of programs or invariants. Programs involving multiple complicated data types (e.g., arrays, pointers) and code structures (e.g., nested loops, function calls) are often beyond their capabilities. To help bridge this gap, we present AutoSpec, an automated approach to synthesize specifications for automated program verification. It overcomes the shortcomings of existing work in specification versatility, synthesizing satisfiable and adequate specifications for full proof. It is driven by static analysis and program verification, and is empowered by large language models (LLMs). AutoSpec addresses the practical challenges in three ways: (1) driving AutoSpec by static analysis and program verification, LLMs serve as generators to generate candidate specifications, (2) programs are decomposed to direct the attention of LLMs, and (3) candidate specifications are validated in each round to avoid error accumulation during the interaction with LLMs. In this way, AutoSpec can incrementally and iteratively generate satisfiable and adequate specifications. The evaluation shows its effectiveness and usefulness, as it outperforms existing works by successfully verifying 79% of programs through automatic specification synthesis, a significant improvement of 1.592x. It can also be successfully applied to verify the programs in a real-world X509-parser project.",
        "labels": [
            "static analysis",
            "program verification",
            "specification inference"
        ],
        "url": "https://arxiv.org/pdf/2404.00762.pdf"
    },
    "Lemur: Integrating large language models in automated program verification": {
        "type": "article",
        "key": "wu2023lemur",
        "title": "Lemur: Integrating large language models in automated program verification",
        "author": "Wu, Haoze and Barrett, Clark and Narodytska, Nina",
        "journal": "arXiv preprint arXiv:2310.04870",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that typically demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "url": "https://openreview.net/forum?id=Q3YaCghZNt"
    },
    "Can ChatGPT support software verification?": {
        "type": "inproceedings",
        "key": "janssen2024can",
        "title": "Can ChatGPT support software verification?",
        "author": "Jan{\\ss}en, Christian and Richter, Cedric and Wehrheim, Heike",
        "booktitle": "International Conference on Fundamental Approaches to Software Engineering",
        "pages": "266--279",
        "year": "2024",
        "organization": "Springer",
        "venue": "FASE2024",
        "abstract": "Large language models have become increasingly effective in software engineering tasks such as code generation, debugging and repair. Language models like ChatGPT can not only generate code, but also explain its inner workings and in particular its correctness. This raises the question whether we can utilize ChatGPT to support formal software verification.",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "url": "https://arxiv.org/abs/2311.02433"
    },
    "Enhancing Automated Loop Invariant Generation for Complex Programs with Large Language Models": {
        "type": "inproceedings",
        "key": "Ruibang24Arxiv",
        "title": "Enhancing Automated Loop Invariant Generation for Complex Programs with Large Language Models",
        "author": "Ruibang Liu, Guoqiang Li, Minyu Chen, Ling-I Wu, Jingyu Ke",
        "booktitle": "arXiv",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Automated program verification has always been an important component of building trustworthy software. While the analysis of real-world programs remains a theoretical challenge, the automation of loop invariant analysis has effectively resolved the problem. However, real-world programs that often mix complex data structures and control flows pose challenges to traditional loop invariant generation tools. To enhance the applicability of invariant generation techniques, we proposed ACInv, an Automated Complex program loop Invariant generation tool, which combines static analysis with Large Language Models (LLMs) to generate the proper loop invariants. We utilize static analysis to extract the necessary information for each loop and embed it into prompts for the LLM to generate invariants for each loop. Subsequently, we employ an LLM-based evaluator to assess the generated invariants, refining them by either strengthening, weakening, or rejecting them based on their correctness, ultimately obtaining enhanced invariants. We conducted experiments on ACInv, which showed that ACInv outperformed previous tools on data sets with data structures, and maintained similar performance to the state-of-the-art tool AutoSpec on numerical programs without data structures. For the total data set, ACInv can solve 21% more examples than AutoSpec and can generate reference data structure templates.",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "url": "https://arxiv.org/pdf/2412.10483"
    },
    "Automated Program Refinement: Guide and Verify Code Large Language Model with Refinement Calculus": {
        "type": "inproceedings",
        "key": "popl25",
        "title": "Automated Program Refinement: Guide and Verify Code Large Language Model with Refinement Calculus",
        "author": "Cai, Yufan and Hou, Zhe and Luan, Xiaokun and Baena, David Miguel Sanan and Lin, Yun and Sun, Jun and Dong, Jin Song",
        "booktitle": "ACM SIGPLAN Symposium on Principles of Programming Languages",
        "pages": "",
        "year": "2025",
        "organization": "ACM",
        "venue": "POPL2025",
        "abstract": "Recently, the rise of code-centric large language models (LLMs) appears to have reshaped the software engineering world with low-barrier tools like Copilot that can generate code easily. However, there is no correctness guarantee for the code generated by LLMs, which suffer from the hallucination problem, and their output is fraught with risks. Besides, the end-to-end process from specification to code through LLMs is a non-transparent and uncontrolled black box. This opacity makes it difficult for users to understand and trust the generated code. Addressing these challenges is both necessary and critical. In contrast, program refinement transforms high-level specification statements into executable code while preserving correctness. Traditional tools for program refinement are primarily designed for formal methods experts and lack automation and extensibility. We apply program refinement to guide LLM and validate the LLM-generated code while transforming refinement into a more accessible and flexible framework. To initiate this vision, we propose Refine4LLM, an approach that aims to: (1) Formally refine the specifications, (2) Automatically prompt and guide the LLM using refinement calculus, (3) Interact with the LLM to generate the code and proof obligations, (4) Verify that the generated code satisfies the constraints, thus guaranteeing its correctness, (5) Learn and build more refinement laws to extend the refinement calculus. We evaluated Refine4LLM against the state-of-the-art baselines on program refinement and LLMs benchmarks. The experiment results show that Refine4LLM can efficiently generate more robust code and reduce the time for refinement and verification.",
        "labels": [
            "code generation",
            "program transformation",
            "static analysis",
            "program verification"
        ],
        "url": "https://arxiv.org/html/2406.18616v1"
    },
    "Can large language models reason about program invariants?": {
        "type": "inproceedings",
        "key": "pei2023can",
        "title": "Can large language models reason about program invariants?",
        "author": "Pei, Kexin and Bieber, David and Shi, Kensen and Sutton, Charles and Yin, Pengcheng",
        "booktitle": "International Conference on Machine Learning",
        "pages": "27496--27520",
        "year": "2023",
        "organization": "PMLR",
        "venue": "ICML2023",
        "abstract": "Identifying invariants is an important program analysis task with applications towards program understanding, bug finding, vulnerability analysis, and formal verification. Existing tools for identifying program invariants rely on dynamic analysis, requiring traces collected from multiple executions in order to produce reliable invariants. We study the application of large language models to invariant prediction, finding that models trained on source code and fine-tuned for invariant generation can perform invariant prediction as static rather than dynamic analysis. Using a scratchpad approach where invariants are predicted sequentially through a program gives the best performance, finding invariants statically of quality comparable to those obtained by a dynamic analysis tool with access to five program traces.",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "url": "https://openreview.net/pdf?id=mXv2aVqUGG"
    },
    "Ranking llm-generated loop invariants for program verification": {
        "type": "article",
        "key": "chakraborty2023ranking",
        "title": "Ranking llm-generated loop invariants for program verification",
        "author": "Chakraborty, Saikat and Lahiri, Shuvendu K and Fakhoury, Sarah and Musuvathi, Madanlal and Lal, Akash and Rastogi, Aseem and Senthilnathan, Aditya and Sharma, Rahul and Swamy, Nikhil",
        "journal": "arXiv preprint arXiv:2310.09342",
        "year": "2023",
        "venue": "EMNLP2023",
        "abstract": "Synthesizing inductive loop invariants is fundamental to automating program verification. In this work, we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number of calls to a program verifier to establish an invariant. To address this issue, we propose a {\\it re-ranking} approach for the generated results of LLMs. We have designed a ranker that can distinguish between correct inductive invariants and incorrect attempts based on the problem definition. The ranker is optimized as a contrastive ranker. Experimental results demonstrate that this re-ranking mechanism significantly improves the ranking of correct invariants among the generated candidates, leading to a notable reduction in the number of calls to a verifier.",
        "labels": [
            "static analysis",
            "program verification",
            "agent design",
            "prompt strategy",
            "sampling and ranking"
        ],
        "url": "https://aclanthology.org/2023.findings-emnlp.614.pdf"
    },
    "Finding inductive loop invariants using large language models": {
        "type": "article",
        "key": "kamath2023finding",
        "title": "Finding inductive loop invariants using large language models",
        "author": "Kamath, Adharsh and Senthilnathan, Aditya and Chakraborty, Saikat and Deligiannis, Pantazis and Lahiri, Shuvendu K and Lal, Akash and Rastogi, Aseem and Roy, Subhajit and Sharma, Rahul",
        "journal": "arXiv preprint arXiv:2311.07948",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "    Loop invariants are fundamental to reasoning about programs with loops. They establish properties about a given loop's behavior. When they additionally are inductive, they become useful for the task of formal verification that seeks to establish strong mathematical guarantees about program's runtime behavior. The inductiveness ensures that the invariants can be checked locally without consulting the entire program, thus are indispensable artifacts in a formal proof of correctness. Finding inductive loop invariants is an undecidable problem, and despite a long history of research towards practical solutions, it remains far from a solved problem. This paper investigates the capabilities of the Large Language Models (LLMs) in offering a new solution towards this old, yet important problem. To that end, we first curate a dataset of verification problems on programs with loops. Next, we design a prompt for exploiting LLMs, obtaining inductive loop invariants, that are checked for correctness using sound symbolic tools. Finally, we explore the effectiveness of using an efficient combination of a symbolic tool and an LLM on our dataset and compare it against a purely symbolic baseline. Our results demonstrate that LLMs can help improve the state-of-the-art in automated program verification.",
        "labels": [
            "static analysis",
            "program verification"
        ],
        "url": "https://arxiv.org/abs/2311.07948"
    },
    "SpecGen: Automated Generation of Formal Program Specifications via Large Language Models": {
        "type": "article",
        "key": "ma2024specgen",
        "title": "SpecGen: Automated Generation of Formal Program Specifications via Large Language Models",
        "author": "Ma, Lezhi and Liu, Shangqing and Li, Yi and Xie, Xiaofei and Bu, Lei",
        "journal": "arXiv preprint arXiv:2401.08807",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "In software development, formal program specifications play a crucial role in various stages. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models. Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM to generate appropriate specifications for a given program. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. To evaluate the performance of SpecGen, we manually construct a dataset containing 120 test cases. Our experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 100 out of 120 programs, outperforming the existing purely LLM-based approaches and conventional specification generation tools. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program.",
        "labels": [
            "static analysis",
            "specification inference"
        ],
        "url": "https://arxiv.org/pdf/2401.08807.pdf"
    },
    "SpecEval: Evaluating Code Comprehension in Large Language Models via Program Specifications": {
        "type": "article",
        "key": "ma2024speceval",
        "title": "SpecEval: Evaluating Code Comprehension in Large Language Models via Program Specifications",
        "author": "Ma, Lezhi and Liu, Shangqing and Bu, Lei and Li, Shangru and Wang, Yida and Liu, Yang",
        "journal": "arXiv preprint arXiv:2409.12866",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Large Language models have achieved impressive performance in automated software engineering. Extensive efforts have been made to evaluate the abilities of code LLMs in various aspects, with an increasing number of benchmarks and evaluation frameworks proposed. Apart from the most sought-after capability of code generation, the capability of code comprehension is being granted growing attention. Nevertheless, existing works assessing the code comprehension capability of LLMs exhibit varied limitations. Evaluation frameworks like CRUXEval and REval usually focus on code reasoning tasks over a certain input case, leading to a limited range of execution traces covered, resulting in a loss in code semantics examined and the inability to assess the comprehensive understanding of LLMs concerning the target program. To tackle the challenges above, we propose SpecEval, a novel black-box evaluation framework to evaluate code comprehension in LLMs via program specifications. Inspired by the idea that specifications can comprehensively articulate program behaviors concerning all possible execution traces, we employ formal specifications to represent program semantics and perform thorough evaluations. In particular, four specification-related tasks are designed to assess the capability of LLMs from basic to advanced levels. Moreover, counterfactual analysis is conducted to study the performance variance of LLMs under semantics-preserving perturbations, and progressive consistency analysis is performed to study the performance consistency of LLMs over a series of tasks with sequential dependence. Systematic experiments are conducted on six state-of-the-art LLMs. Experimental results present a below-satisfactory performance of LLMs on specification-related tasks, revealing the limitations of existing LLMs in articulating program semantics, underscoring future directions for enhancement.",
        "labels": [
            "static analysis",
            "specification inference"
        ],
        "url": "https://arxiv.org/abs/2409.12866"
    },
    "Impact of large language models on generating software specifications": {
        "type": "article",
        "key": "xie2023impact",
        "title": "Impact of large language models on generating software specifications",
        "author": "Xie, Danning and Yoo, Byungwoo and Jiang, Nan and Kim, Mijung and Tan, Lin and Zhang, Xiangyu and Lee, Judy S",
        "journal": "arXiv preprint arXiv:2306.03324",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "Software specifications are essential for ensuring the reliability of software systems. Existing specification extraction approaches, however, suffer from limited generalizability and require manual efforts. The recent emergence of Large Language Models (LLMs), which have been successfully applied to numerous software engineering tasks, offers a promising avenue for automating this process. In this paper, we conduct the first empirical study to evaluate the capabilities of LLMs for generating software specifications from software comments or documentation. We evaluate LLMs' performance with Few Shot Learning (FSL), enabling LLMs to generalize from a small number of examples, as well as different prompt construction strategies, and compare the performance of LLMs with traditional approaches. Additionally, we conduct a comparative diagnosis of the failure cases from both LLMs and traditional methods, identifying their unique strengths and weaknesses. Lastly, we conduct extensive experiments on 15 state of the art LLMs, evaluating their performance and cost effectiveness for generating software specifications. Our results show that with FSL, LLMs outperform traditional methods (by 5.6%), and more sophisticated prompt construction strategies can further enlarge this performance gap (up to 5.1 to 10.0%). Yet, LLMs suffer from their unique challenges, such as ineffective prompts and the lack of domain knowledge, which together account for 53 to 60% of LLM unique failures. The strong performance of open source models (e.g., StarCoder) makes closed source models (e.g., GPT 3 Davinci) less desirable due to size and cost. Our study offers valuable insights for future research to improve specification generation.",
        "labels": [
            "static analysis",
            "specification inference"
        ],
        "url": "https://arxiv.org/pdf/2306.03324.pdf"
    },
    "A Learning-Based Approach to Static Program Slicing": {
        "type": "article",
        "key": "yadavally2024learning",
        "title": "A Learning-Based Approach to Static Program Slicing",
        "author": "Yadavally, Aashish and Li, Yi and Wang, Shaohua and Nguyen, Tien N",
        "journal": "Proceedings of the ACM on Programming Languages",
        "volume": "8",
        "number": "OOPSLA1",
        "pages": "83--109",
        "year": "2024",
        "publisher": "ACM New York, NY, USA",
        "venue": "OOPSLA2024",
        "abstract": "Traditional program slicing techniques are crucial for early bug detection and manual/automated debugging of online code snippets. Nevertheless, their inability to handle incomplete code hinders their real-world applicability in such scenarios. To overcome these challenges, we present NS-Slicer, a novel learning-based approach that predicts static program slices for both complete and partial code Our tool leverages a pre-trained language model to exploit its understanding of fine-grained variable-statement dependencies within source code. With this knowledge, given a variable at a specific location and a statement in a code snippet, NS-Slicer determines whether the statement belongs to the backward slice or forward slice, respectively. We conducted a series of experiments to evaluate NS-Slicer's performance. On complete code, it predicts the backward and forward slices with an F1-score of 97.41% and 95.82%, respectively, while achieving an overall F1-score of 96.77%. Notably, in 85.20% of the cases, the static program slices predicted by NS-Slicer exactly match entire slices from the oracle. For partial programs, it achieved an F1-score of 96.77%\u201397.49% for backward slicing, 92.14%\u201395.40% for forward slicing, and an overall F1-score of 94.66%\u201396.62%. Furthermore, we demonstrate NS-Slicer's utility in vulnerability detection (VD), integrating its predicted slices into an automated VD tool. In this setup, the tool detected vulnerabilities in Java code with a high F1-score of 73.38%. We also include the analyses studying NS-Slicer\u2019s promising performance and limitations, providing insights into its understanding of intrinsic code properties such as variable aliasing, leading to better slicing.",
        "labels": [
            "static analysis",
            "data-flow analysis",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://aashishyadavally.github.io/assets/pdf/pub-oopsla2024.pdf"
    },
    "Dependency-Aware Code Naturalness": {
        "type": "article",
        "key": "chen2024dependency",
        "title": "Dependency-Aware Code Naturalness",
        "author": "Chen Yang, Junjie Chen, Jiajun Jiang, Yuliang Huang",
        "journal": "Proceedings of the ACM on Programming Languages",
        "year": "2024",
        "publisher": "ACM New York, NY, USA",
        "venue": "OOPSLA2024",
        "abstract": "Code naturalness, which captures repetitiveness and predictability in programming languages, has proven valuable for various code-related tasks in software engineering. However, precisely measuring code naturalness remains a fundamental challenge. Existing methods measure code naturalness over individual lines of code while ignoring the deep semantic relations among different lines, e.g., program dependency, which may negatively affect the precision of the measure. Despite the intuitive appeal of extending the code naturalness measure to the code dependency domain (as there are some work that have initiated the utilization of code dependency for diverse code-related tasks), this assumption remains unexplored and warrants direct investigation. In this study, we aim to perform the first empirical study to investigate whether incorporating code dependency, instead of analyzing individual lines, can enhance the precision of measuring code naturalness. To achieve that, we first propose a new method named DAN for measuring code naturalness by incorporating the rich dependency information in the code. Specifically, DAN extracts multiple sequences of code lines by traversing the program dependency graph, where different code lines are connected by dependencies in each sequence, and then the code naturalness will be measured by taking each sequence as a whole. In this way, the dependency information can be well captured. Finally, we have conducted an extensive study to evaluate the influence of code dependency for measuring code naturalness with DAN, and compared it with the state-of-the-art methods under three emerging application scenarios of code naturalness. The results demonstrate that DAN can not only better distinguish natural and unnatural code, but also substantially boost two important downstream applications of code naturalness, i.e., distinguishing buggy and non-buggy code lines and data cleansing for training better code models, reflecting the significance of code dependency in measuring code naturalness.",
        "labels": [
            "static analysis",
            "bug detection",
            "code model",
            "empirical study"
        ],
        "url": "https://dl.acm.org/doi/10.1145/3689794"
    },
    "Evaluating the effectiveness of deep learning models for foundational program analysis tasks": {
        "type": "article",
        "key": "chen2024evaluating",
        "title": "Evaluating the effectiveness of deep learning models for foundational program analysis tasks",
        "author": "Chen, Qian and Yu, Chenyang and Liu, Ruyan and Zhang, Chi and Wang, Yu and Wang, Ke and Su, Ting and Wang, Linzhang",
        "journal": "Proceedings of the ACM on Programming Languages",
        "volume": "8",
        "number": "OOPSLA1",
        "pages": "500--528",
        "year": "2024",
        "publisher": "ACM New York, NY, USA",
        "venue": "OOPSLA2024",
        "abstract": "While deep neural networks provide state-of-the-art solutions to a wide range of programming language tasks, their effectiveness in dealing with foundational program analysis tasks remains under explored. In this paper, we present an empirical study that evaluates four prominent models of code (i.e., CuBERT, CodeBERT, GGNN, and Graph Sandwiches) in two such foundational tasks: (1) alias prediction, in which models predict whether two pointers must alias, may alias or must not alias; and (2) equivalence prediction, in which models predict whether or not two programs are semantically equivalent. At the core of this study is CodeSem, a dataset built upon the source code of real-world flagship software (e.g., Linux Kernel, GCC, MySQL) and manually validated for the two prediction tasks. Results show that all models are accurate in both prediction tasks, especially CuBERT with an accuracy of 89% and 84% in alias prediction and equivalence prediction, respectively. We also conduct a comprehensive, in-depth analysis of the results of all models in both tasks, concluding that deep learning models are generally capable of performing foundational tasks in program analysis even though in specific cases their weaknesses are also evident.",
        "labels": [
            "static analysis",
            "pointer analysis",
            "equivalence checking",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3649829"
    },
    "Program Slicing in the Era of Large Language Models": {
        "type": "article",
        "key": "shahandashti2024program",
        "title": "Program Slicing in the Era of Large Language Models",
        "author": "Shahandashti, Kimya Khakzad and Mohajer, Mohammad Mahdi and Belle, Alvine Boaye and Wang, Song and Hemmati, Hadi",
        "journal": "arXiv preprint arXiv:2409.12369",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Program slicing is a critical technique in software engineering, enabling developers to isolate relevant portions of code for tasks such as bug detection, code comprehension, and debugging. In this study, we investigate the application of large language models (LLMs) to both static and dynamic program slicing, with a focus on Java programs. We evaluate the performance of four state-of-the-art LLMs- GPT-4o, GPT-3.5 Turbo, Llama-2, and Gemma-7B leveraging advanced prompting techniques, including few-shot learning and chain-of-thought reasoning. Using a dataset of 100 Java programs derived from LeetCode problems, our experiments reveal that GPT-4o performs the best in both static and dynamic slicing across other LLMs, achieving an accuracy of 60.84% and 59.69%, respectively. Our results also show that the LLMs we experimented with are yet to achieve reasonable performance for either static slicing or dynamic slicing. Through a rigorous manual analysis, we developed a taxonomy of root causes and failure locations to explore the unsuccessful cases in more depth. We identified Complex Control Flow as the most frequent root cause of failures, with the majority of issues occurring in Variable Declarations and Assignments locations. To improve the performance of LLMs, we further examined two independent strategies for prompting guided by our taxonomy, including prompt crafting, which involved refining the prompts to better guide the LLM through the slicing process, and iterative prompting, where the model receives feedback on the root cause and location of the failure and re-generates its responses. Our evaluation shows these two prompting enhancement approaches can improve accuracy by 4% and 3.9%, respectively.",
        "labels": [
            "static analysis",
            "data-flow analysis"
        ],
        "url": "https://arxiv.org/pdf/2409.12369"
    },
    "Teaching large language models to self-debug": {
        "type": "article",
        "key": "chen2023teaching",
        "title": "Teaching large language models to self-debug",
        "author": "Chen, Xinyun and Lin, Maxwell and Sch{\\\"a}rli, Nathanael and Zhou, Denny",
        "journal": "arXiv preprint arXiv:2304.05128",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.",
        "labels": [
            "program testing",
            "debugging"
        ],
        "url": "https://openreview.net/forum?id=KuPixIqPiq"
    },
    "When fuzzing meets llms: Challenges and opportunities": {
        "type": "inproceedings",
        "key": "jiang2024fuzzing",
        "title": "When fuzzing meets llms: Challenges and opportunities",
        "author": "Jiang, Yu and Liang, Jie and Ma, Fuchen and Chen, Yuanliang and Zhou, Chijin and Shen, Yuheng and Wu, Zhiyong and Fu, Jingzhou and Wang, Mingzhe and Li, Shanshan and others",
        "booktitle": "Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering",
        "pages": "492--496",
        "year": "2024",
        "venue": "FSE2024",
        "abstract": "Fuzzing, a widely-used technique for bug detection, has seen advancements through Large Language Models (LLMs). Despite their potential, LLMs face specific challenges in fuzzing. In this paper, we identified five major challenges of LLM-assisted fuzzing. To support our findings, we revisited the most recent papers from top-tier conferences, confirming that these challenges are widespread. As a remedy, we propose some actionable recommendations to help improve applying LLM in Fuzzing and conduct preliminary evaluations on DBMS fuzzing. The results demonstrate that our recommendations effectively address the identified challenges.",
        "labels": [
            "program testing",
            "fuzzing",
            "survey"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3663529.3663784"
    },
    "DAInfer: Inferring API Aliasing Specifications from Library Documentation via Neurosymbolic Optimization": {
        "type": "inproceedings",
        "key": "aliasspecinfer",
        "title": "DAInfer: Inferring API Aliasing Specifications from Library Documentation via Neurosymbolic Optimization",
        "author": "Wang, Chengpeng and Zhang, Jipeng and Wu, Rongxin and Zhang, Charles",
        "booktitle": "32nd ACM International Conference on the Foundations of Software Engineering",
        "year": "2024",
        "venue": "FSE2024",
        "abstract": "Modern software systems heavily rely on various libraries, necessitating understanding API semantics in static analysis. However, summarizing API semantics remains challenging due to complex implementations or the unavailability of library code. This paper presents DAInfer, a novel approach for inferring API aliasing specifications from library documentation. Specifically, we employ Natural Language Processing (NLP) models to interpret informal semantic information provided by the documentation, which enables us to reduce the specification inference to an optimization problem. Furthermore, we propose a new technique called neurosymbolic optimization to efficiently solve the optimization problem, yielding the desired API aliasing specifications. We have implemented DAInfer as a tool and evaluated it upon Java classes from several popular libraries. The results indicate that DAInfer infers the API aliasing specifications with a precision of 79.78% and a recall of 82.29%, averagely consuming 5.35 seconds per class. These obtained aliasing specifications further facilitate alias analysis, revealing 80.05% more alias facts for API return values in 15 Java projects. Additionally, the tool supports taint analysis, identifying 85 more taint flows in 23 Android apps. These results demonstrate the practical value of DAInfer in library-aware static analysis",
        "labels": [
            "static analysis",
            "specification inference"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3660816"
    },
    "Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation": {
        "type": "inproceedings",
        "key": "jiang2024towards",
        "title": "Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation",
        "author": "Jiang, Zongze and Wen, Ming and Cao, Jialun and Shi, Xuanhua and Jin, Hai",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1408--1420",
        "year": "2024",
        "venue": "ASE2024",
        "abstract": "Automatic testing has garnered significant attention and success over the past few decades. Techniques such as unit testing and coverage-guided fuzzing have revealed numerous critical software bugs and vulnerabilities. However, a long-standing, formidable challenge for existing techniques is how to achieve higher testing coverage. Constraint-based techniques, such as symbolic execution and concolic testing, have been well-explored and integrated into the existing approaches. With the popularity of Large Language Models (LLMs), recent research efforts to design tailored prompts to generate inputs that can reach more uncovered target branches. However, the effectiveness of using LLMs for generating such directed inputs and the comparison with the proven constraint-based solutions has not been systematically explored.To bridge this gap, we conduct the first systematic study on the mainstream LLMs and constraint-based tools for directed input generation with a comparative perspective. We find that LLMs such as ChatGPT are comparable to or even better than the constraint-based tools, succeeding in 43.40%-58.57% samples in our dataset. Meanwhile, there are also limitations for LLMs in specific scenarios such as sequential calculation, where constraint-based tools are in a position of strength. Based on these findings, we propose a simple yet effective method to combine these two types of tools and implement a prototype based on ChatGPT and constraint-based tools. Our evaluation shows that our approach can outperform the baselines by 1.4x to 2.3x relatively. We believe our study can provide novel insights into directed input generation using LLMs, and our findings are essential for future testing research.",
        "labels": [
            "program testing",
            "unit testing",
            "empirical study"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3691620.3695513e"
    },
    "Prompt Fuzzing for Fuzz Driver Generation": {
        "type": "article",
        "key": "lyu2023prompt",
        "title": "Prompt Fuzzing for Fuzz Driver Generation",
        "author": "Lyu, Yunlong and Xie, Yuxuan and Chen, Peng and Chen, Hao",
        "journal": "arXiv preprint arXiv:2312.17677",
        "year": "2023",
        "venue": "CCS2023",
        "abstract": "Writing high-quality fuzz drivers is time-consuming and requires a deep understanding of the library. However, the performance of the state-of-the-art automatic fuzz driver generation techniques leaves a lot to be desired. Fuzz drivers, which are learned from consumer code, can reach deep states but are restricted to their external inputs. On the other hand, interpretative fuzzing can explore most APIs but requires numerous attempts in a vast search space. We propose PromptFuzz, a coverage-guided fuzzer for prompt fuzzing that iteratively generates fuzz drivers to explore undiscovered library code. To explore API usage in fuzz drivers during prompt fuzzing, we proposed several key techniques: instructive program generation, erroneous program sanitization, coverage-guided prompt mutation, and constrained fuzzer fusion. We implemented PromptFuzz and evaluated its effectiveness on 14 real-world libraries, comparing it against OSS-Fuzz and the state-of-the-art fuzz driver generation solution (i.e., Hopper). The experiment results demonstrate that the fuzz drivers generated by PromptFuzz achieve higher branch coverage that is 1.61 times greater than that of OSS-Fuzz and 1.67 times greater than that of Hopper. In addition, the fuzz drivers generated by PromptFuzz successfully detect 33 true bugs out of a total of 44 crashes, which were previously unknown, and 27 of these bugs have been confirmed by the respective communities.",
        "labels": [
            "program testing",
            "fuzzing"
        ],
        "url": "https://arxiv.org/pdf/2312.17677.pdf"
    },
    "Formal Mathematical Reasoning: A New Frontier in AI": {
        "type": "article",
        "key": "formalmathreason",
        "title": "Formal Mathematical Reasoning: A New Frontier in AI",
        "author": "Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, Dawn Song",
        "journal": "arXiv preprint arXiv:2412.16075",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "AI for Mathematics (AI4Math) is not only intriguing intellectually but also crucial for AI-driven discovery in science, engineering, and beyond. Extensive efforts on AI4Math have mirrored techniques in NLP, in particular, training large language models on carefully curated math datasets in text form. As a complementary yet less explored avenue, formal mathematical reasoning is grounded in formal systems such as proof assistants, which can verify the correctness of reasoning and provide automatic feedback. In this position paper, we advocate for formal mathematical reasoning and argue that it is indispensable for advancing AI4Math to the next level. In recent years, we have seen steady progress in using AI to perform formal reasoning, including core tasks such as theorem proving and autoformalization, as well as emerging applications such as verifiable generation of code and hardware designs. However, significant challenges remain to be solved for AI to truly master mathematics and achieve broader impact. We summarize existing progress, discuss open challenges, and envision critical milestones to measure future success. At this inflection point for formal mathematical reasoning, we call on the research community to come together to drive transformative advancements in this field.",
        "labels": [
            "hallucination in reasoning",
            "survey"
        ],
        "url": "https://arxiv.org/pdf/2412.16075"
    },
    "Llm4fuzz: Guided fuzzing of smart contracts with large language models": {
        "type": "article",
        "key": "shou2024llm4fuzz",
        "title": "Llm4fuzz: Guided fuzzing of smart contracts with large language models",
        "author": "Shou, Chaofan and Liu, Jing and Lu, Doudou and Sen, Koushik",
        "journal": "arXiv preprint arXiv:2401.11108",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "As blockchain platforms grow exponentially, millions of lines of smart contract code are being deployed to manage extensive digital assets. However, vulnerabilities in this mission-critical code have led to significant exploitations and asset losses. Thorough automated security analysis of smart contracts is thus imperative. This paper introduces LLM4Fuzz to optimize automated smart contract security analysis by leveraging large language models (LLMs) to intelligently guide and prioritize fuzzing campaigns. While traditional fuzzing suffers from low efficiency in exploring the vast state space, LLM4Fuzz employs LLMs to direct fuzzers towards high-value code regions and input sequences more likely to trigger vulnerabilities. Additionally, LLM4Fuzz can leverage LLMs to guide fuzzers based on user-defined invariants, reducing blind exploration overhead. Evaluations of LLM4Fuzz on real-world DeFi projects show substantial gains in efficiency, coverage, and vulnerability detection compared to baseline fuzzing. LLM4Fuzz also uncovered five critical vulnerabilities that can lead to a loss of more than $247k.",
        "labels": [
            "program testing",
            "fuzzing"
        ],
        "url": "https://arxiv.org/pdf/2401.11108.pdf"
    },
    "LLMorpheus: Mutation Testing using Large Language Models": {
        "type": "article",
        "key": "tip2024llmorpheus",
        "title": "LLMorpheus: Mutation Testing using Large Language Models",
        "author": "Tip, Frank and Bell, Jonathan and Sch{\\\"a}fer, Max",
        "journal": "arXiv preprint arXiv:2404.09952",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "In mutation testing, the quality of a test suite is evaluated by introducing faults into a program and determining whether the program's tests detect them. Most existing approaches for mutation testing involve the application of a fixed set of mutation operators, e.g., replacing a \"+\" with a \"-\" or removing a function's body. However, certain types of real-world bugs cannot easily be simulated by such approaches, limiting their effectiveness. This paper presents a technique where a Large Language Model (LLM) is prompted to suggest mutations by asking it what placeholders that have been inserted in source code could be replaced with. The technique is implemented in LLMorpheus, a mutation testing tool for JavaScript, and evaluated on 13 subject packages, considering several variations on the prompting strategy, and using several LLMs. We find LLMorpheus to be capable of producing mutants that resemble existing bugs that cannot be produced by StrykerJS, a state-of-the-art mutation testing tool. Moreover, we report on the running time, cost, and number of mutants produced by LLMorpheus, demonstrating its practicality.",
        "labels": [
            "program testing",
            "mutation testing"
        ],
        "url": "https://arxiv.org/pdf/2404.09952"
    },
    "UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing": {
        "type": "inproceedings",
        "key": "he2024unitsyn",
        "title": "UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing",
        "author": "He, Yifeng and Huang, Jiabo and Rong, Yuyang and Guo, Yiwen and Wang, Ethan and Chen, Hao",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1061--1072",
        "year": "2024",
        "venue": "ISSTA2024",
        "abstract": "The remarkable capability of large language models (LLMs) in generating high-quality code has drawn increasing attention in the software testing community. However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate, complete tests since they were trained on code snippets collected without differentiating between code for testing and for other purposes. In this paper, we present a large-scale dataset, UniTSyn, which can enhance LLMs for Unit Test Synthesis. Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified. By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics, which tend to be fragile and difficult to scale. Containing 2.7 million focal-test pairs across five mainstream programming languages, it can enhance the test generation ability of LLMs. Our experiments demonstrate that, by building an autoregressive LLM based on UniTSyn, we can achieve significant benefits in learning and understanding unit test representations, resulting in improved generation accuracy and code coverage across all the evaluated programming languages.",
        "labels": [
            "program testing",
            "unit testing",
            "benchmark"
        ],
        "url": "https://haochen.org/publications/he2024unitsyn.pdf"
    },
    "CAT-LM training language models on aligned code and tests": {
        "type": "inproceedings",
        "key": "rao2023cat",
        "title": "CAT-LM training language models on aligned code and tests",
        "author": "Rao, Nikitha and Jain, Kush and Alon, Uri and Le Goues, Claire and Hellendoorn, Vincent J",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "pages": "409--420",
        "year": "2023",
        "organization": "IEEE",
        "venue": "ASE2023",
        "abstract": "Testing is an integral but often neglected part of the software development process. Classical test generation tools such as EvoSuite generate behavioral test suites by optimizing for coverage, but tend to produce tests that are hard to understand. Language models trained on code can generate code that is highly similar to that written by humans, but current models are trained to generate each file separately, as is standard practice in natural language processing, and thus fail to consider the code-under-test context when producing a test file. In this work, we propose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style language model with 2.7 Billion parameters, trained on a corpus of Python and Java projects. We utilize a novel pretraining signal that explicitly considers the mapping between code and test files when available. We also drastically increase the maximum sequence length of inputs to 8,192 tokens, 4x more than typical code generation models, to ensure that the code context is available to the model when generating test code. We analyze its usefulness for realistic applications, showing that sampling with filtering (e.g., by compilability, coverage) allows it to efficiently produce tests that achieve coverage similar to ones written by developers while resembling their writing style. By utilizing the code context, CAT-LM generates more valid tests than even much larger language models trained with more data (CodeGen 16B and StarCoder) and substantially outperforms a recent test-specific model (TeCo) at test completion. Overall, our work highlights the importance of incorporating software-specific insights when training language models for code and paves the way to more powerful automated test generation.",
        "labels": [
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://arxiv.org/pdf/2310.01602"
    },
    "Predictive Program Slicing via Execution Knowledge-Guided Dynamic Dependence Learning": {
        "type": "article",
        "key": "yadavally2024predictive",
        "title": "Predictive Program Slicing via Execution Knowledge-Guided Dynamic Dependence Learning",
        "author": "Yadavally, Aashish and Li, Yi and Nguyen, Tien N",
        "journal": "Proceedings of the ACM on Software Engineering",
        "volume": "1",
        "number": "FSE",
        "pages": "271--292",
        "year": "2024",
        "publisher": "ACM New York, NY, USA",
        "venue": "FSE2024",
        "abstract": "Program slicing, the process of extracting program statements that influence values at a designated location (known as the slicing criterion), is helpful in both manual and automated debugging. However, such slicing techniques prove ineffective in scenarios where executing specific inputs is prohibitively expensive, or even impossible, as with partial code. In this paper, we introduce ND-Slicer, a predictive slicing methodology that caters to specific executions based on a particular input, overcoming the need for actual execution. We enable such a process by leveraging execution-aware pre-training to learn the dynamic program dependencies, including both dynamic data and control dependencies between variables in the slicing criterion and the remaining program statements. Such knowledge forms the cornerstone for constructing a predictive backward slice. Our empirical evaluation revealed a high accuracy in predicting program slices, achieving an exact-match accuracy of 81.3% and a ROUGE-LCS F1-score of 95.4% on Python programs. As an extrinsic evaluation, we illustrate ND-Slicer\u2019s usefulness in crash detection, with it locating faults with an accuracy of 63.9%. Furthermore, we include an in-depth qualitative evaluation, assessing ND-Slicer\u2019s understanding of branched structures such as if-else blocks and loops, as well as the control flow in inter-procedural calls.",
        "labels": [
            "program testing",
            "debugging",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://aashishyadavally.github.io/assets/pdf/pub-fse2024.pdf"
    },
    "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities": {
        "type": "article",
        "key": "fang2024teams",
        "title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities",
        "author": "Fang, Richard and Bindu, Rohan and Gupta, Akul and Zhan, Qiusi and Kang, Daniel",
        "journal": "arXiv preprint arXiv:2406.01637",
        "year": "2024",
        "venue": "arXiv2024",
        "url": "https://arxiv.org/abs/2406.01637",
        "abstract": "LLM agents have become increasingly sophisticated, especially in the realm of cybersecurity. Researchers have shown that LLM agents can exploit real-world vulnerabilities when given a description of the vulnerability and toy capture-the-flag problems. However, these agents still perform poorly on real-world vulnerabilities that are unknown to the agent ahead of time (zero-day vulnerabilities). In this work, we show that teams of LLM agents can exploit real-world, zero-day vulnerabilities. Prior agents struggle with exploring many different vulnerabilities and long-range planning when used alone. To resolve this, we introduce HPTSA, a system of agents with a planning agent that can launch subagents. The planning agent explores the system and determines which subagents to call, resolving long-term planning issues when trying different vulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and show that our team of agents improve over prior work by up to 4.5.",
        "labels": [
            "program testing",
            "vulnerability exploitation"
        ]
    },
    "From Naptime to Big Sleep: Using Large Language Models To Catch Vulnerabilities In Real-World Code": {
        "type": "blog",
        "key": "google01",
        "title": "From Naptime to Big Sleep: Using Large Language Models To Catch Vulnerabilities In Real-World Code",
        "author": "Google",
        "journal": "Google",
        "year": "2024",
        "venue": "Google2024",
        "url": "https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html",
        "abstract": "In our previous post, Project Naptime: Evaluating Offensive Security Capabilities of Large Language Models, we introduced our framework for large-language-model-assisted vulnerability research and demonstrated its potential by improving the state-of-the-art performance on Meta's CyberSecEval2 benchmarks. Since then, Naptime has evolved into Big Sleep, a collaboration between Google Project Zero and Google DeepMind.",
        "labels": [
            "program testing",
            "vulnerability exploitation"
        ]
    },
    "Function Argument Nullability Using an LLM": {
        "type": "blog",
        "key": "Galois01",
        "title": "Function Argument Nullability Using an LLM",
        "author": "Galois",
        "journal": "Galois",
        "year": "2024",
        "venue": "Galois2024",
        "url": "https://galois.com/blog/2024/11/function-argument-nullability-using-an-llm/",
        "abstract": "We think that Rust is a great language, and maybe you agree! Unfortunately, even if you do, there\u2019s a good chance whatever application you\u2019re working on is written in some older language such as C. To help with this, Galois has been developing c2rust, an automated transpiler (source-to-source translator) from C code into Rust code. c2rust can take almost any C and turn it into C-like Rust code, the first step in creating a new Rust application. And we\u2019re building more features to turn C into safe, idiomatic Rust code. Recently, we have been experimenting with LLMs to help with transpilation from C to Rust. This blog describes one such experiment, where we built an analysis for determining nullability of function arguments in C. This is a necessary stage in the c2rust translation pipeline, and we already have an existing interprocedural static analysis tool that performs this task. We built a companion LLM-based tool using GPT-4o, and compared the performance between our static and LLM-based analysis. ",
        "labels": [
            "static analysis",
            "pointer analysis"
        ]
    },
    "Evaluating Offensive Security Capabilities of Large Language Models": {
        "type": "article",
        "key": "google02",
        "title": "Evaluating Offensive Security Capabilities of Large Language Models",
        "author": "Google",
        "journal": "Google",
        "year": "2024",
        "venue": "Google2024",
        "url": "https://googleprojectzero.blogspot.com/2024/06/project-naptime.html",
        "abstract": "At Project Zero, we constantly seek to expand the scope and effectiveness of our vulnerability research. Though much of our work still relies on traditional methods like manual source code audits and reverse engineering, we're always looking for new approaches.",
        "labels": [
            "program testing",
            "vulnerability exploitation"
        ]
    },
    "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models": {
        "type": "article",
        "key": "zhang2024cybench",
        "title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models",
        "author": "Zhang, Andy K and Perry, Neil and Dulepet, Riya and Ji, Joey and Lin, Justin W and Jones, Eliot and Menders, Celeste and Hussein, Gashon and Liu, Samantha and Jasper, Donovan and others",
        "journal": "arXiv preprint arXiv:2408.08926",
        "year": "2024",
        "venue": "arXiv2024",
        "url": "https://arxiv.org/abs/2408.08926",
        "abstract": "Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have the potential to cause real-world impact. Policymakers, model providers, and other researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute bash commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps for a more detailed evaluation. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that took human teams up to 11 minutes to solve. In comparison, the most difficult task took human teams 24 hours and 54 minutes to solve.",
        "labels": [
            "program testing",
            "vulnerability exploitation",
            "benchmark"
        ]
    },
    "Vulnhuntr: Autonomous AI Finds First 0-Day Vulnerabilities in Wild": {
        "type": "blog",
        "key": "protectAIblog01",
        "title": "Vulnhuntr: Autonomous AI Finds First 0-Day Vulnerabilities in Wild",
        "author": "Dan McInerney and Marcello Salvati",
        "journal": "online blog",
        "year": "2024",
        "venue": "ProtectAI2024",
        "url": "https://protectai.com/threat-research/vulnhuntr-first-0-day-vulnerabilities",
        "abstract": "Today, we introduce [Vulnhuntr](https://github.com/protectai/vulnhuntr), a Python static code analyzer that leverages the power of large language models (LLMs) to find and explain complex, multistep vulnerabilities. Thanks to the capabilities of models like Claude 3.5, AI has now uncovered more than a dozen remotely exploitable 0-day vulnerabilities targeting open-source projects in the AI ecosystem with over 10,000 GitHub stars in just a few hours of running it. These discoveries include full-blown Remote Code Execution. If you\u2019d like to get paid for using Vulnhuntr then head on over to https://huntr.com which is an AI bug bounty program helping secure the exploding open source AI ecosystem.",
        "labels": [
            "program testing",
            "vulnerability exploitation"
        ]
    },
    "Language agents as hackers: Evaluating cybersecurity skills with capture the flag": {
        "type": "inproceedings",
        "key": "yang2023language",
        "title": "Language agents as hackers: Evaluating cybersecurity skills with capture the flag",
        "author": "Yang, John and Prabhakar, Akshara and Yao, Shunyu and Pei, Kexin and Narasimhan, Karthik R",
        "booktitle": "Multi-Agent Security Workshop@ NeurIPS'23",
        "year": "2023",
        "venue": "NeurIPS2023",
        "url": "https://openreview.net/forum?id=KOZwk7BFc3&noteId=OIANITRY6R",
        "abstract": "Amidst the advent of language models (LMs) and their wide-ranging capabilities, concerns have been raised about their implications with regards to privacy and security. In particular, the emergence of language agents as a promising aid for automating and augmenting digital work poses immediate questions concerning their misuse as malicious cybersecurity actors. With their exceptional compute efficiency and execution speed relative to human counterparts, language agents may be extremely adept at locating vulnerabilities, performing complex social engineering, and hacking real world systems. Understanding and guiding the development of language agents in the cybersecurity space requires a grounded understanding of their capabilities founded on empirical data and demonstrations. To address this need, we introduce InterCode-CTF, a novel task environment and benchmark for evaluating language agents on the Capture the Flag (CTF) task. Built as a facsimile of real world CTF competitions, in the InterCode-CTF environment, a language agent is tasked with finding a flag from a purposely-vulnerable computer program. We manually collect and verify a benchmark of 100 task instances that require a number of cybersecurity skills such as reverse engineering, forensics, and binary exploitation, then evaluate current top-notch LMs on this evaluation set. Our preliminary findings indicate that while language agents possess rudimentary cybersecurity knowledge, they are not able to perform multi-step cybersecurity tasks out-of-the-box.",
        "labels": [
            "program testing",
            "vulnerability exploitation",
            "benchmark"
        ]
    },
    "Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models": {
        "type": "article",
        "key": "mirzadeh2024gsm",
        "title": "Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models",
        "author": "Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad",
        "journal": "arXiv preprint arXiv:2410.05229",
        "year": "2024",
        "venue": "Apple2024",
        "abstract": "Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.",
        "labels": [
            "hallucination in reasoning",
            "empirical study"
        ],
        "url": "https://arxiv.org/pdf/2410.05229"
    },
    "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?": {
        "type": "inproceedings",
        "key": "li2024deceptive",
        "title": "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?",
        "author": "Li, Bangzheng and Zhou, Ben and Wang, Fei and Fu, Xingyu and Roth, Dan and Chen, Muhao",
        "booktitle": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
        "pages": "7668--7681",
        "year": "2024",
        "venue": "NAACL2024",
        "abstract": "Despite the high performances of large language models (LLMs) across numerous benchmarks, recent research has unveiled their suffering from hallucinations and unfaithful reasoning. This work studies a type of hallucination induced by semantic associations. We investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following correct reasoning paths. To quantify this phenomenon, we propose a novel probing method and benchmark called EUREQA. EUREQA is an entity-searching task where a model finds a missing entity based on described multi-hop relations with other entities. These deliberately designed multi-hop relations create deceptive semantic associations, and models must stick to the correct reasoning path instead of incorrect shortcuts to find the correct answer. Experiments show that existing LLMs cannot follow correct reasoning paths and resist the attempt of greedy shortcuts, with GPT-4 only achieving 62% accuracy. Analyses provide further evidence that LLMs rely on semantic biases to solve the task instead of proper reasoning, questioning the validity and generalizability of current LLMs\u2019 high performances.",
        "labels": [
            "hallucination in reasoning",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.naacl-long.424/"
    },
    "LocAgent: Graph-Guided LLM Agents for Code Localization": {
        "type": "inproceedings",
        "key": "tang-etal-2024-codeagent",
        "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
        "author": "Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, Xingyao Wang",
        "year": "2025",
        "venue": "ACL2025",
        "abstract": "Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at this https URL.",
        "labels": [
            "agent design",
            "prompt strategy",
            "retrieval-augmented generation",
            "planning"
        ],
        "url": "https://arxiv.org/abs/2503.09089"
    },
    "RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph": {
        "type": "inproceedings",
        "key": "repograph-2025",
        "title": "RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph",
        "author": "Siru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, Dong Yu",
        "year": "2025",
        "venue": "ICLR2025",
        "abstract": "Large Language Models (LLMs) excel in code generation yet struggle with modern AI software engineering tasks. Unlike traditional function-level or file-level coding tasks, AI software engineering requires not only basic coding proficiency but also advanced skills in managing and interacting with code repositories. However, existing methods often overlook the need for repository-level code understanding, which is crucial for accurately grasping the broader context and developing effective solutions. On this basis, we present RepoGraph, a plug-in module that manages a repository-level structure for modern AI software engineering solutions. RepoGraph offers the desired guidance and serves as a repository-wide navigation for AI software engineers. We evaluate RepoGraph on the SWE-bench by plugging it into four different methods of two lines of approaches, where RepoGraph substantially boosts the performance of all systems, leading to a new state-of-the-art among open-source frameworks. Our analyses also demonstrate the extensibility and flexibility of RepoGraph by testing on another repo-level coding benchmark, CrossCodeEval. Our code is available at https://github.com/ozyyshr/RepoGraph.",
        "labels": [
            "agent design",
            "prompt strategy",
            "retrieval-augmented generation",
            "planning"
        ],
        "url": "https://openreview.net/forum?id=dw9VUsSHGB"
    },
    "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation": {
        "type": "article",
        "key": "mundler2023self",
        "title": "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
        "author": "M{\\\"u}ndler, Niels and He, Jingxuan and Jenko, Slobodan and Vechev, Martin",
        "journal": "arXiv preprint arXiv:2305.15852",
        "year": "2024",
        "venue": "ICLR2024",
        "abstract": "Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context. In this work, we present a comprehensive investigation into self-contradiction for various instruction-tuned LMs, covering evaluation, detection, and mitigation. Our primary evaluation task is open-domain text generation, but we also demonstrate the applicability of our approach to shorter question answering. Our analysis reveals the prevalence of self-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT. We then propose a novel prompting-based framework designed to effectively detect and mitigate self-contradictions. Our detector achieves high accuracy, e.g., around 80% F1 score when prompting ChatGPT. The mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness. Importantly, our entire framework is applicable to black-box LMs and does not require retrieval of external knowledge. Rather, our method complements retrieval-based methods, as a large portion of self-contradictions (e.g., 35.2% for ChatGPT) cannot be verified using online text.",
        "labels": [
            "hallucination in reasoning",
            "empirical study"
        ],
        "url": "https://arxiv.org/abs/2305.15852"
    },
    "From System 1 to System 2: A Survey of Reasoning Large Language Models": {
        "type": "article",
        "key": "system1to2_survey",
        "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "author": "Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhiwei Li, Bao-Long Bi, Ling-Rui Mei, Junfeng Fang, Zhijiang Guo, Le Song, Cheng-Lin Liu",
        "journal": "arXiv preprint arXiv:2502.17419",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.",
        "labels": [
            "agent design",
            "hallucination in reasoning",
            "survey"
        ],
        "url": "https://arxiv.org/abs/2502.17419"
    },
    "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions": {
        "type": "article",
        "key": "huang2023survey",
        "title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
        "author": "Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others",
        "journal": "arXiv preprint arXiv:2311.05232",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.",
        "labels": [
            "hallucination in reasoning",
            "survey"
        ],
        "url": "https://arxiv.org/pdf/2406.19508"
    },
    "Large Language Model powered Symbolic Execution": {
        "type": "article",
        "key": "llmexe",
        "title": "Large Language Model powered Symbolic Execution",
        "author": "Yihe Li, Ruijie Meng, Gregory J. Duck",
        "journal": "arXiv preprint arXiv:2505.13452",
        "year": "2025",
        "venue": "arXiv2025",
        "abstract": "Large Language Models (LLMs) have emerged as a promising alternative to traditional static program analysis methods, such as symbolic execution, offering the ability to reason over code directly without relying on theorem provers or SMT solvers. However, LLMs are also inherently probabilistic by nature, and therefore face significant challenges in relation to the accuracy and scale of the analysis in real-world application. Such issues often necessitate the use of larger LLMs with higher token limits, but this requires enterprise-grade hardware (GPUs) and thus limits accessibility for many users. In this paper, we propose LLM-based symbolic execution -- a novel approach that enhances LLM inference via a path-based decomposition of the program analysis tasks into smaller (more tractable) sub-tasks. The core idea is to generalize path constraints using a generic code-based representation that the LLM can directly reason over, and without translation into another (less-expressive) formal language. We implement our approach in the form of AutoExe, an LLM-based symbolic execution engine that is lightweight and language-agnostic, making it a practical tool for analyzing code that is challenging for traditional approaches. We show that AutoExe can improve both the accuracy and scale of LLM-based program analysis, especially for smaller LLMs that can run on consumer grade hardware.",
        "labels": [
            "static analysis",
            "symbolic execution"
        ],
        "url": "https://arxiv.org/pdf/2505.13452"
    },
    "Chain of code: Reasoning with a language model-augmented code emulator": {
        "type": "article",
        "key": "li2023chain",
        "title": "Chain of code: Reasoning with a language model-augmented code emulator",
        "author": "Li, Chengshu and Liang, Jacky and Zeng, Andy and Chen, Xinyun and Hausman, Karol and Sadigh, Dorsa and Levine, Sergey and Fei-Fei, Li and Xia, Fei and Ichter, Brian",
        "journal": "arXiv preprint arXiv:2312.04474",
        "year": "2023",
        "venue": "Google2023",
        "abstract": "Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter -- we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for linguistic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for \"detect_sarcasm(string)\" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they are used not only to write the code, but also to selectively \"emulate\" the interpreter by generating the expected output of \"detect_sarcasm(string)\" and other lines of code (e.g., that the interpreter could not compile). In this work, we propose Chain of Code (CoT), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format linguistic sub-tasks in a program as flexible pseudocode that the compiler can explicitly catch undefined behaviors and hand off to simulate with an LM (as an \"LMulator\"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. CoT scales well with large and small models alike, and broadens the scope of reasoning questions that LMs can correctly answer by \"thinking in code\". Project webpage: https://chain-of-code.github.io/.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://arxiv.org/pdf/2312.04474.pdf"
    },
    "When Do Program-of-Thought Works for Reasoning?": {
        "type": "inproceedings",
        "key": "bi2024program",
        "title": "When Do Program-of-Thought Works for Reasoning?",
        "author": "Bi, Zhen and Zhang, Ningyu and Jiang, Yinuo and Deng, Shumin and Zheng, Guozhou and Chen, Huajun",
        "booktitle": "Proceedings of the AAAI Conference on Artificial Intelligence",
        "volume": "38",
        "number": "16",
        "pages": "17691--17699",
        "year": "2024",
        "venue": "AAAI2024",
        "abstract": "In the realm of embodied artificial intelligence, the reasoning capabilities of Large Language Models (LLMs) play a pivotal role. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score CIRS, which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing and stratifying algorithm, and apply it to instruction generation for mathematical reasoning and code data filtering for code generation tasks. Extensive results demonstrates the effectiveness of our proposed approach.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code",
            "empirical study"
        ],
        "url": "https://arxiv.org/pdf/2308.15452"
    },
    "Explanation selection using unlabeled data for chain-of-thought prompting": {
        "type": "article",
        "key": "ye2023explanation",
        "title": "Explanation selection using unlabeled data for chain-of-thought prompting",
        "author": "Ye, Xi and Durrett, Greg",
        "journal": "arXiv preprint arXiv:2302.04813",
        "year": "2023",
        "venue": "EMNLP2023",
        "abstract": "Recent work has shown how to prompt large language models with explanations to obtain strong performance on textual reasoning tasks, i.e., the chain-of-thought paradigm. However, subtly different explanations can yield widely varying downstream task accuracy. Explanations that have not been \"tuned\" for a task, such as off-the-shelf explanations written by nonexperts, may lead to mediocre performance. This paper tackles the problem of how to optimize explanation-infused prompts in a blackbox fashion. We first generate sets of candidate explanations for each example in the prompt using a leave-one-out scheme, then find an effective combination of these explanations with a two-stage framework. We first evaluate explanations for each in-context example in isolation according to two proxy metrics, log likelihood and accuracy on new examples. Then, we search over combinations of explanations to find one that yields high performance against a silver-labeled development set. Across four textual reasoning tasks spanning question answering, mathematical reasoning, and natural language inference, results show that our proxy metrics correlate with ground truth accuracy and our overall method can effectively improve prompts over crowdworker annotations and naive search strategies.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code",
            "empirical study"
        ],
        "url": "https://arxiv.org/abs/2302.04813"
    },
    "Complementary explanations for effective in-context learning": {
        "type": "article",
        "key": "ye2022complementary",
        "title": "Complementary explanations for effective in-context learning",
        "author": "Ye, Xi and Iyer, Srinivasan and Celikyilmaz, Asli and Stoyanov, Ves and Durrett, Greg and Pasunuru, Ramakanth",
        "journal": "arXiv preprint arXiv:2211.13892",
        "year": "2022",
        "venue": "ACL2023",
        "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code",
            "empirical study"
        ],
        "url": "https://arxiv.org/abs/2211.13892"
    },
    "Self-evaluation guided beam search for reasoning": {
        "type": "article",
        "key": "xie2023self",
        "title": "Self-evaluation guided beam search for reasoning",
        "author": "Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael",
        "journal": "Advances in Neural Information Processing Systems",
        "volume": "36",
        "year": "2023",
        "venue": "NeurIPS2023",
        "abstract": "Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by %, %, and % on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at [https://guideddecoding.github. io/](https://guideddecoding.github.io/).",
        "labels": [
            "hallucination in reasoning",
            "agent design",
            "prompt strategy"
        ],
        "url": "https://arxiv.org/pdf/2305.00633.pdf"
    },
    "Tree of thoughts: Deliberate problem solving with large language models": {
        "type": "article",
        "key": "yao2023tree",
        "title": "Tree of thoughts: Deliberate problem solving with large language models",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "journal": "Advances in Neural Information Processing Systems",
        "volume": "36",
        "year": "2023",
        "venue": "NeurIPS2023",
        "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in  tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, \u201cTree of Thoughts\u201d (ToT), which generalizes over the popular \u201cChain of Thought\u201d approach to prompting language models, and enables exploration over coherent units of text (\u201cthoughts\u201d) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models\u2019 problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
        "labels": [
            "hallucination in reasoning",
            "agent design",
            "prompt strategy"
        ],
        "url": "https://arxiv.org/abs/2305.10601"
    },
    "React: Synergizing reasoning and acting in language models": {
        "type": "article",
        "key": "yao2022react",
        "title": "React: Synergizing reasoning and acting in language models",
        "author": "Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan",
        "journal": "arXiv preprint arXiv:2210.03629",
        "year": "2022",
        "venue": "NeurIPS2022",
        "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",
        "labels": [
            "hallucination in reasoning",
            "agent design",
            "prompt strategy"
        ],
        "url": "https://arxiv.org/abs/2210.03629"
    },
    "Reflexion: Language agents with verbal reinforcement learning": {
        "type": "article",
        "key": "shinn2023reflexion",
        "title": "Reflexion: Language agents with verbal reinforcement learning",
        "author": "Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu",
        "journal": "Advances in Neural Information Processing Systems",
        "volume": "36",
        "year": "2023",
        "venue": "NeurIPS2023",
        "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.",
        "labels": [
            "hallucination in reasoning",
            "agent design",
            "prompt strategy"
        ],
        "url": "https://arxiv.org/abs/2303.11366"
    },
    "Satlm: Satisfiability-aided language models using declarative prompting": {
        "type": "article",
        "key": "ye2023satlm",
        "title": "Satlm: Satisfiability-aided language models using declarative prompting",
        "author": "Ye, Xi and Chen, Qiaochu and Dillig, Isil and Durrett, Greg",
        "journal": "Advances in Neural Information Processing Systems",
        "volume": "36",
        "year": "2023",
        "venue": "NeurIPS2023",
        "abstract": "Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving problems that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling (SatLM) approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it out of the description more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness of the answer with respect to the parsed specification and avoid planning errors in the solving process. We evaluate SATLM on 8 different datasets and show that it consistently outperforms program-aided LMs in the imperative paradigm. In particular, SATLM outperforms program-aided LMs by 23% on a challenging subset of the GSM arithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT and BoardgameQA, surpassing previous models that are trained on the respective training sets.",
        "labels": [
            "hallucination in reasoning",
            "agent design",
            "prompt strategy"
        ],
        "url": "https://arxiv.org/pdf/2305.09656.pdf"
    },
    "Cumulative reasoning with large language models": {
        "type": "article",
        "key": "zhang2023cumulative",
        "title": "Cumulative reasoning with large language models",
        "author": "Zhang, Yifan and Yang, Jingqin and Yuan, Yang and Yao, Andrew Chi-Chih",
        "journal": "arXiv preprint arXiv:2308.04371",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "While language models are powerful and versatile, they often fail to address highly complex problems. This is because solving complex problems requires deliberate thinking, which has been only minimally guided during training. In this paper, we propose a new method called Cumulative Reasoning (CR), which employs language models in a cumulative and iterative manner to emulate human thought processes. By decomposing tasks into smaller components, \\ournameb streamlines the problem-solving process, rendering it both more manageable and effective. For logical inference tasks, CR consistently outperforms existing methods with an improvement up to 9.3\\%, and achieves the astonishing accuracy of 98.04\\% on the curated FOLIO wiki dataset. In the context of the Game of 24, CR achieves an accuracy of 94\\%, which signifies a substantial enhancement of 20\\% over the previous state-of-the-art method.",
        "labels": [
            "hallucination in reasoning",
            "agent design",
            "prompt strategy"
        ],
        "url": "https://arxiv.org/pdf/2308.04371.pdf"
    },
    "Self-consistency improves chain of thought reasoning in language models": {
        "type": "article",
        "key": "wang2022self",
        "title": "Self-consistency improves chain of thought reasoning in language models",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "journal": "Advances in Neural Information Processing Systems",
        "year": "2022",
        "venue": "NeurIPS2022",
        "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
        "labels": [
            "hallucination in reasoning",
            "agent design",
            "prompt strategy"
        ],
        "url": "https://arxiv.org/abs/2203.11171"
    },
    "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing": {
        "type": "article",
        "key": "liu2023pre",
        "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "author": "Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham",
        "journal": "ACM Computing Surveys",
        "volume": "55",
        "number": "9",
        "pages": "1--35",
        "year": "2023",
        "publisher": "ACM New York, NY",
        "venue": "ACMSurvey2023",
        "abstract": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub \u201cprompt-based learning.\u201d Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x\u2032 that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x\u0302, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia\u2013Pretrain including constantly updated survey and paperlist.",
        "labels": [
            "survey",
            "agent design",
            "prompt strategy"
        ],
        "url": "https://arxiv.org/pdf/2107.13586"
    },
    "Steering Large Language Models between Code Execution and Textual Reasoning": {
        "type": "article",
        "key": "chen2024steering",
        "title": "Steering Large Language Models between Code Execution and Textual Reasoning",
        "author": "Chen, Yongchao and Jhamtani, Harsh and Sharma, Srinagesh and Fan, Chuchu and Wang, Chi",
        "journal": "arXiv preprint arXiv:2410.03524",
        "year": "2024",
        "venue": "Microsoft2024",
        "abstract": "While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching. Textual reasoning has inherent limitations in solving tasks with challenges in math, logics, optimization, and searching, which is unlikely to be solved by simply scaling up the model and data size. The recently released OpenAI GPT Code Interpreter and multi-agent frameworks such as AutoGen have demonstrated remarkable proficiency of integrating code generation and execution to solve complex tasks using LLMs. However, based on our experiments on 7 existing popular methods for steering code/text generation in both single- and multi-turn settings with 14 tasks and 6 types of LLMs (including the new O1-preview), currently there is no optimal method to correctly steer LLMs to write code when needed. We discover some interesting patterns on when models use code vs. textual reasoning with the evolution to task complexity and model sizes, which even result in an astonishingly inverse scaling law. We also discover that results from LLM written code are not always better than using textual reasoning, even if the task could be solved through code. To mitigate the above issues, we propose three methods to better steer LLM code/text generation and achieve a notable improvement. The costs of token lengths and runtime are thoroughly discussed for all the methods. We believe the problem of steering LLM code/text generation is critical for future research and has much space for further improvement. Project Page, Datasets, and Codes are available at https://yongchao98.github.io/CodeSteer/.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://arxiv.org/pdf/2410.03524"
    },
    "Don't Transform the Code, Code the Transforms: Towards Precise Code Rewriting using LLMs": {
        "type": "article",
        "key": "cummins2024don",
        "title": "Don't Transform the Code, Code the Transforms: Towards Precise Code Rewriting using LLMs",
        "author": "Cummins, Chris and Seeker, Volker and Armengol-Estap{\\'e}, Jordi and Markosyan, Aram H and Synnaeve, Gabriel and Leather, Hugh",
        "journal": "arXiv preprint arXiv:2410.08806",
        "year": "2024",
        "venue": "Meta2024",
        "abstract": "Tools for rewriting, refactoring and optimizing code should be fast and correct. Large language models (LLMs), by their nature, possess neither of these qualities. Yet, there remains tremendous opportunity in using LLMs to improve code. We explore the use of LLMs not to transform code, but to code transforms. We propose a chain-of-thought approach to synthesizing code transformations from a small number of input/output code examples that incorporates execution and feedback. Unlike the direct rewrite approach, LLM-generated transformations are easy to inspect, debug, and validate. The logic of the rewrite is explicitly coded and easy to adapt. The compute required to run code transformations is minute compared to that of LLM rewriting. We test our approach on 16 Python code transformations and find that LLM- generated transforms are perfectly precise for 7 of them and less imprecise than direct LLM rewriting on the others. We hope to encourage further research to improving the precision of LLM code rewriting.",
        "labels": [
            "agent design",
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://arxiv.org/pdf/2410.08806"
    },
    "Leandojo: Theorem proving with retrieval-augmented language models": {
        "type": "article",
        "key": "yang2024leandojo",
        "title": "Leandojo: Theorem proving with retrieval-augmented language models",
        "author": "Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan J and Anandkumar, Animashree",
        "journal": "Advances in Neural Information Processing Systems",
        "volume": "36",
        "year": "2023",
        "venue": "NeurIPS2023",
        "abstract": "Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection\u2014a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.",
        "labels": [
            "agent design",
            "prompt strategy",
            "retrieval-augmented generation"
        ],
        "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/4441469427094f8873d0fecb0c4e1cee-Paper-Datasets_and_Benchmarks.pdf"
    },
    "Large language model-based agents for software engineering: A survey": {
        "type": "article",
        "key": "liu2024large",
        "title": "Large language model-based agents for software engineering: A survey",
        "author": "Liu, Junwei and Wang, Kaixin and Chen, Yixuan and Peng, Xin and Chen, Zhenpeng and Zhang, Lingming and Lou, Yiling",
        "journal": "arXiv preprint arXiv:2409.02977",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools. To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE). The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems. In this work, we present a comprehensive and systematic survey on LLM-based agents for SE. We collect 106 papers and categorize them from two perspectives, i.e., the SE and agent perspectives. In addition, we discuss open challenges and future directions in this critical domain. The repository of this survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.",
        "labels": [
            "survey",
            "agent design"
        ],
        "url": "https://arxiv.org/pdf/2409.02977"
    },
    "Large language models for software engineering: A systematic literature review": {
        "type": "article",
        "key": "hou2023large",
        "title": "Large language models for software engineering: A systematic literature review",
        "author": "Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu",
        "journal": "ACM Transactions on Software Engineering and Methodology",
        "year": "2023",
        "publisher": "ACM New York, NY",
        "venue": "TOSEM2023",
        "abstract": "Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at https://github.com/xinyi-hou/LLM4SE_SLR.",
        "labels": [
            "survey",
            "general coding task"
        ],
        "url": "https://arxiv.org/pdf/2308.10620"
    },
    "If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents": {
        "type": "article",
        "key": "yang2024if",
        "title": "If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents",
        "author": "Yang, Ke and Liu, Jiateng and Wu, John and Yang, Chaoqi and Fung, Yi R and Li, Sha and Huang, Zixuan and Cao, Xu and Wang, Xingyao and Wang, Yiquan and others",
        "journal": "arXiv preprint arXiv:2401.00812",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.",
        "labels": [
            "survey",
            "agent design",
            "reason with code"
        ],
        "url": "https://arxiv.org/pdf/2401.00812.pdf"
    },
    "Cognitive architectures for language agents": {
        "type": "article",
        "key": "sumers2023cognitive",
        "title": "Cognitive architectures for language agents",
        "author": "Sumers, Theodore R and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L",
        "journal": "arXiv preprint arXiv:2309.02427",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.",
        "labels": [
            "agent design",
            "survey"
        ],
        "url": "https://arxiv.org/pdf/2309.02427.pdf"
    },
    "The rise and potential of large language model based agents: A survey": {
        "type": "article",
        "key": "xi2023rise",
        "title": "The rise and potential of large language model based agents: A survey",
        "author": "Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others",
        "journal": "arXiv preprint arXiv:2309.07864",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field.",
        "labels": [
            "survey",
            "agent design"
        ],
        "url": "https://arxiv.org/abs/2309.07864"
    },
    "Improving Automated Program Repair with Domain Adaptation": {
        "type": "article",
        "key": "10.1145/3631972",
        "author": "Zirak, Armin and Hemmati, Hadi",
        "title": "Improving Automated Program Repair with Domain Adaptation",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3631972",
        "doi": "10.1145/3631972",
        "abstract": "Automated Program Repair (APR) is defined as the process of fixing a bug/defect in the source code, by an automated tool. APR tools have recently experienced promising results by leveraging state-of-the-art Neural Language Processing (NLP) techniques. APR tools such as TFix and CodeXGLUE that combine text-to-text transformers with software-specific techniques are outperforming alternatives, these days. However, in most APR studies, the train and test sets are chosen from the same set of projects (i.e., when APR fixes a bug in the test set from project A, the model has already seen example fixed bugs from project A in the training set). In the real world, however, APR models are meant to be generalizable to new and different projects. Therefore, there is a potential threat that reported APR models with high effectiveness perform poorly when the characteristics of the new project or its bugs are different than the training set\u2019s (\u201cDomain Shift\u201d).In this study, we first define the problem of domain shift in automated program repair. Next, we measure the potential damage of domain shift on two recent APR models (TFix and CodeXGLUE). Based on this observation, we then propose a domain adaptation framework that can adapt an APR model for a given target project. We conduct an empirical study with three domain adaptation methods FullFineTuning, TuningWithLightWeightAdapterLayers, and CurriculumLearning and two APR models on 2,672 bugs from 12 projects.The results show that our proposed framework on average can improve the effectiveness of TFix by 13.05\\% and CodeXGLUE by 48.78\\%, in terms of \u201cExact Match\u201d. Through experiments, we also show that the framework provides high efficiency and reliability (in terms of \u201cExposure Bias\u201d). Using synthetic data to domain adapt TFix and CodeXGLUE on the projects with no data (Zero-shot learning), also results in an average improvement of 5.76\\% and 17.62\\% for TFix and CodeXGLUE, respectively.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "65",
        "numpages": "43",
        "labels": [
            "code generation",
            "program repair"
        ],
        "venue": "TOSEM2024"
    },
    "Vision Transformer Inspired Automated Vulnerability Repair": {
        "type": "article",
        "key": "10.1145/3632746",
        "author": "Fu, Michael and Nguyen, Van and Tantithamthavorn, Chakkrit and Phung, Dinh and Le, Trung",
        "title": "Vision Transformer Inspired Automated Vulnerability Repair",
        "year": "2024",
        "issue_date": "March 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "3",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3632746",
        "doi": "10.1145/3632746",
        "abstract": "Recently, automated vulnerability repair approaches have been widely adopted to combat increasing software security issues. In particular, transformer-based encoder-decoder models achieve competitive results. Whereas vulnerable programs may only consist of a few vulnerable code areas that need repair, existing AVR approaches lack a mechanism guiding their model to pay more attention to vulnerable code areas during repair generation. In this article, we propose a novel vulnerability repair framework inspired by the Vision Transformer based approaches for object detection in the computer vision domain. Similar to the object queries used to locate objects in object detection in computer vision, we introduce and leverage vulnerability queries (VQs) to locate vulnerable code areas and then suggest their repairs. In particular, we leverage the cross-attention mechanism to achieve the cross-match between VQs and their corresponding vulnerable code areas. To strengthen our cross-match and generate more accurate vulnerability repairs, we propose to learn a novel vulnerability mask (VM) and integrate it into decoders\u2019 cross-attention, which makes our VQs pay more attention to vulnerable code areas during repair generation. In addition, we incorporate our VM into encoders\u2019 self-attention to learn embeddings that emphasize the vulnerable areas of a program. Through an extensive evaluation using the real-world 5,417 vulnerabilities, our approach outperforms all of the automated vulnerability repair baseline methods by 2.68\\% to 32.33\\%. Additionally, our analysis of the cross-attention map of our approach confirms the design rationale of our VM and its effectiveness. Finally, our survey study with 71 software practitioners highlights the significance and usefulness of AI-generated vulnerability repairs in the realm of software security. The training code and pre-trained models are available at https://github.com/awsm-research/VQM.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "mar",
        "articleno": "78",
        "numpages": "29",
        "labels": [
            "code generation",
            "program repair"
        ],
        "venue": "TOSEM2024"
    },
    "Survey of Code Search Based on Deep Learning": {
        "type": "article",
        "key": "10.1145/3628161",
        "author": "Xie, Yutao and Lin, Jiayi and Dong, Hande and Zhang, Lei and Wu, Zhonghai",
        "title": "Survey of Code Search Based on Deep Learning",
        "year": "2023",
        "issue_date": "February 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "2",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3628161",
        "doi": "10.1145/3628161",
        "abstract": "Code writing is repetitive and predictable, inspiring us to develop various code intelligence techniques. This survey focuses on code search, that is, to retrieve code that matches a given natural language query by effectively capturing the semantic similarity between the query and code. Deep learning, being able to extract complex semantics information, has achieved great success in this field. Recently, various deep learning methods, such as graph neural networks and pretraining models, have been applied to code search with significant progress. Deep learning is now the leading paradigm for code search. In this survey, we provide a comprehensive overview of deep learning-based code search. We review the existing deep learning-based code search framework that maps query/code to vectors and measures their similarity. Furthermore, we propose a new taxonomy to illustrate the state-of-the-art deep learning-based code search in a three-step process: query semantics modeling, code semantics modeling, and matching modeling, which involves the deep learning model training. Finally, we suggest potential avenues for future research in this promising field.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "dec",
        "articleno": "54",
        "numpages": "42",
        "labels": [
            "survey",
            "static analysis",
            "code search"
        ],
        "venue": "TOSEM2024"
    },
    "BinCola: Diversity-Sensitive Contrastive Learning for Binary Code Similarity Detection": {
        "type": "article",
        "key": "10.1109/TSE.2024.3411072",
        "author": "Jiang, Shuai and Fu, Cai and He, Shuai and Lv, Jianqiang and Han, Lansheng and Hu, Hong",
        "title": "BinCola: Diversity-Sensitive Contrastive Learning for Binary Code Similarity Detection",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3411072",
        "doi": "10.1109/TSE.2024.3411072",
        "abstract": "Binary Code Similarity Detection (BCSD) is a fundamental binary analysis technique in the area of software security. Recently, advanced deep learning algorithms are integrated into BCSD platforms to achieve superior performance on well-known benchmarks. However, real-world large programs embed more complex diversities due to different compilers, various optimization levels, multiple architectures and even obfuscations. Existing BCSD solutions suffer from low accuracy issues in such complicated real-world application scenarios. In this paper, we propose BinCola, a novel Transformer-based dual diversity-sensitive contrastive learning framework that comprehensively considers the diversity of compiler options and candidate functions in the real-world application scenarios and employs the attention mechanism to fuse multi-granularity function features for enhancing generality and scalability. BinCola simultaneously compares multiple candidate functions across various compilation option scenarios to learn the differences caused by distinct compiler options and different candidate functions. We evaluate BinCola's performance in a variety of ways, including binary similarity detection and real-world vulnerability search in multiple application scenarios. The results demonstrate that BinCola achieves superior performance compared to state-of-the-art (SOTA) methods, with improvements of 2.80\\%, 33.62\\%, 22.41\\%, and 34.25\\% in cross-architecture, cross-optimization level, cross-compiler, and cross-obfuscation scenarios, respectively.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2485\u20132497",
        "numpages": "13",
        "venue": "TSE2024",
        "labels": [
            "static analysis",
            "code similarity analysis",
            "code model",
            "code model training",
            "binary code model"
        ]
    },
    "Domain Adaptation for Code Model-Based Unit Test Case Generation": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680354",
        "author": "Shin, Jiho and Hashtroudi, Sepehr and Hemmati, Hadi and Wang, Song",
        "title": "Domain Adaptation for Code Model-Based Unit Test Case Generation",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680354",
        "doi": "10.1145/3650212.3680354",
        "abstract": "Recently, deep learning-based test case generation approaches have been proposed to automate the generation of unit test cases. In this study, we leverage Transformer-based code models to generate               unit tests with the help of Domain Adaptation (DA) at a project level. Specifically, we use CodeT5, a relatively small language model trained on source code data, and fine-tune it on the test generation               task. Then, we apply domain adaptation to each target project data to learn project-specific knowledge (project-level DA). We use the Methods2test dataset to fine-tune CodeT5 for the test generation               task and the Defects4j dataset for project-level domain adaptation and evaluation. We compare our approach with (a) CodeT5 fine-tuned on the test generation without DA, (b) the A3Test tool, and (c)               GPT-4 on five projects from the Defects4j dataset. The results show that tests generated using DA can increase the line coverage by 18.62\\%, 19.88\\%, and 18.02\\% and mutation score by 16.45\\%, 16.01\\%",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1211\u20131222",
        "numpages": "12",
        "labels": [
            "program testing",
            "unit testing"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "EGFE: End-to-end Grouping of Fragmented Elements in UI Designs with Multimodal Learning": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623313",
        "author": "Chen, Liuqing and Chen, Yunnong and Xiao, Shuhong and Song, Yaxuan and Sun, Lingyun and Zhen, Yankun and Zhou, Tingting and Chang, Yanfang",
        "title": "EGFE: End-to-end Grouping of Fragmented Elements in UI Designs with Multimodal Learning",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623313",
        "doi": "10.1145/3597503.3623313",
        "abstract": "When translating UI design prototypes to code in industry, automatically generating code from design prototypes can expedite the development of applications and GUI iterations. However, in design prototypes without strict design specifications, UI components may be composed of fragmented elements. Grouping these fragmented elements can greatly improve the readability and maintainability of the generated code. Current methods employ a two-stage strategy that introduces hand-crafted rules to group fragmented elements. Unfortunately, the performance of these methods is not satisfying due to visually overlapped and tiny UI elements. In this study, we propose EGFE, a novel method for automatically End-to-end Grouping Fragmented Elements via UI sequence prediction. To facilitate the UI understanding, we innovatively construct a Transformer encoder to model the relationship between the UI elements with multi-modal representation learning. The evaluation on a dataset of 4606 UI prototypes collected from professional UI designers shows that our method outperforms the state-of-the-art baselines in the precision (by 29.75\\%), recall (by 31.07\\%), and F1-score (by 30.39\\%) at edit distance threshold of 4. In addition, we conduct an empirical study to assess the improvement of the generated front-end code. The results demonstrate the effectiveness of our method on a real software engineering application. Our end-to-end fragmented elements grouping method creates opportunities for improving UI-related software engineering tasks.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "11",
        "numpages": "12",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "GrammarT5: Grammar-Integrated Pretrained Encoder-Decoder Neural Model for Code": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639125",
        "author": "Zhu, Qihao and Liang, Qingyuan and Sun, Zeyu and Xiong, Yingfei and Zhang, Lu and Cheng, Shengyu",
        "title": "GrammarT5: Grammar-Integrated Pretrained Encoder-Decoder Neural Model for Code",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639125",
        "doi": "10.1145/3597503.3639125",
        "abstract": "Pretrained models for code have exhibited promising performance across various code-related tasks, such as code summarization, code completion, code translation, and bug detection. However, despite their success, the majority of current models still represent code as a token sequence, which may not adequately capture the essence of the underlying code structure.In this work, we propose GrammarT5, a grammar-integrated encoder-decoder pretrained neural model for code. GrammarT5 employs a novel grammar-integrated representation, Tokenized Grammar Rule Sequence (TGRS), for code. TGRS is constructed based on the grammar rule sequence utilized in syntax-guided code generation and integrates syntax information with code tokens within an appropriate input length. Furthermore, we suggest attaching language flags to help GrammarT5 differentiate between grammar rules of various programming languages. Finally, we introduce two novel pretraining tasks---Edge Prediction (EP), and Sub-Tree Prediction (STP) to learn syntactic information.Experiments were conducted on five code-related tasks using eleven datasets, demonstrating that GrammarT5 achieves state-of-the-art (SOTA) performance on most tasks in comparison to models of the same scale. Additionally, the paper illustrates that the proposed pretraining tasks and language flags can enhance GrammarT5 to better capture the syntax and semantics of code.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "76",
        "numpages": "13",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "On Calibration of Pre-trained Code Models": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639126",
        "author": "Zhou, Zhenhao and Sha, Chaofeng and Peng, Xin",
        "title": "On Calibration of Pre-trained Code Models",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639126",
        "doi": "10.1145/3597503.3639126",
        "abstract": "Pre-trained code models have achieved notable success in the field of Software Engineering (SE). However, existing studies have predominantly focused on improving model performance, with limited attention given to other critical aspects such as model calibration. Model calibration, which refers to the accurate estimation of predictive uncertainty, is a vital consideration in practical applications. Therefore, in order to advance the understanding of model calibration in SE, we conduct a comprehensive investigation into the calibration of pre-trained code models in this paper. Our investigation focuses on five pre-trained code models and four code understanding tasks, including analyses of calibration in both in-distribution and out-of-distribution settings. Several key insights are uncovered: (1) pre-trained code models may suffer from the issue of over-confidence; (2) temperature scaling and label smoothing are effective in calibrating code models in in-distribution data; (3) the issue of over-confidence in pre-trained code models worsens in different out-of-distribution settings, and the effectiveness of temperature scaling and label smoothing diminishes. All materials used in our experiments are available at https://github.com/queserasera22/Calibration-of-Pretrained-Code-Models.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "77",
        "numpages": "13",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Language Models for Code Completion: A Practical Evaluation": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639138",
        "author": "Izadi, Maliheh and Katzy, Jonathan and Van Dam, Tim and Otten, Marc and Popescu, Razvan Mihai and Van Deursen, Arie",
        "title": "Language Models for Code Completion: A Practical Evaluation",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639138",
        "doi": "10.1145/3597503.3639138",
        "abstract": "Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies.Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java. InCoder outperformed the other models across all programming languages, highlighting the significance of training data and objectives. Our study also revealed that offline evaluations do not accurately reflect real-world scenarios. Upon qualitative analysis of the models' predictions, we found that 66.3\\% of failures were due to models' limitations, 24.4\\% occurred due to inappropriate model usage in a development context, and 9.3\\% were valid requests that developers overwrote. Given these findings, we propose several strategies to overcome the current limitations. These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "79",
        "numpages": "13",
        "labels": [
            "code generation",
            "code completion"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Out of Context: How important is Local Context in Neural Program Repair?": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639086",
        "author": "Prenner, Julian Aron and Robbes, Romain",
        "title": "Out of Context: How important is Local Context in Neural Program Repair?",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639086",
        "doi": "10.1145/3597503.3639086",
        "abstract": "Deep learning source code models have been applied very successfully to the problem of automated program repair. One of the standing issues is the small input window of current models which often cannot fully fit the context code required for a bug fix (e.g., method or class declarations of a project). Instead, input is often restricted to the local context, that is, the lines below and above the bug location. In this work we study the importance of this local context on repair success: how much local context is needed?; is context before or after the bug location more important? how is local context tied to the bug type? To answer these questions we train and evaluate Transformer models in many different local context configurations on three datasets and two programming languages. Our results indicate that overall repair success increases with the size of the local context (albeit not for all bug types) and confirm the common practice that roughly 50--60\\% of the input window should be used for context leading the bug. Our results are not only relevant for researchers working on Transformer-based APR tools but also for benchmark and dataset creators who must decide what and how much context to include in their datasets.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "83",
        "numpages": "13",
        "labels": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Out of Sight, Out of Mind: Better Automatic Vulnerability Repair by Broadening Input Ranges and Sources": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639222",
        "author": "Zhou, Xin and Kim, Kisub and Xu, Bowen and Han, Donggyun and Lo, David",
        "title": "Out of Sight, Out of Mind: Better Automatic Vulnerability Repair by Broadening Input Ranges and Sources",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639222",
        "doi": "10.1145/3597503.3639222",
        "abstract": "The advances of deep learning (DL) have paved the way for automatic software vulnerability repair approaches, which effectively learn the mapping from the vulnerable code to the fixed code. Nevertheless, existing DL-based vulnerability repair methods face notable limitations: 1) they struggle to handle lengthy vulnerable code, 2) they treat code as natural language texts, neglecting its inherent structure, and 3) they do not tap into the valuable expert knowledge present in the expert system. To address this, we propose VulMaster, a Transformer-based neural network model that excels at generating vulnerability repairs by comprehensively understanding the entire vulnerable code, irrespective of its length. This model also integrates diverse information, encompassing vulnerable code structures and expert knowledge from the CWE system. We evaluated VulMaster on a real-world C/C++ vulnerability repair dataset comprising 1,754 projects with 5,800 vulnerable functions. The experimental results demonstrated that VulMaster exhibits substantial improvements compared to the learning-based state-of-the-art vulnerability repair approach. Specifically, VulMaster improves the EM, BLEU, and CodeBLEU scores from 10.2\\% to 20.0\\%, 21.3\\% to 29.3\\%, and 32.5\\% to 40.9\\%, respectively.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "88",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "labels": [
            "code generation",
            "program repair"
        ]
    },
    "VGX: Large-Scale Sample Generation for Boosting Learning-Based Software Vulnerability Analyses": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639116",
        "author": "Nong, Yu and Fang, Richard and Yi, Guangbei and Zhao, Kunsong and Luo, Xiapu and Chen, Feng and Cai, Haipeng",
        "title": "VGX: Large-Scale Sample Generation for Boosting Learning-Based Software Vulnerability Analyses",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639116",
        "doi": "10.1145/3597503.3639116",
        "abstract": "Accompanying the successes of learning-based defensive software vulnerability analyses is the lack of large and quality sets of labeled vulnerable program samples, which impedes further advancement of those defenses. Existing automated sample generation approaches have shown potentials yet still fall short of practical expectations due to the high noise in the generated samples. This paper proposes VGX, a new technique aimed for large-scale generation of high-quality vulnerability datasets. Given a normal program, VGX identifies the code contexts in which vulnerabilities can be injected, using a customized Transformer featured with a new value-flow-based position encoding and pre-trained against new objectives particularly for learning code structure and context. Then, VGX materializes vulnerability-injection code editing in the identified contexts using patterns of such edits obtained from both historical fixes and human knowledge about real-world vulnerabilities.Compared to four state-of-the-art (SOTA) (i.e., pattern-, Transformer-, GNN-, and pattern+Transformer-based) baselines, VGX achieved 99.09--890.06\\% higher F1 and 22.45\\%-328.47\\% higher label accuracy. For in-the-wild sample production, VGX generated 150,392 vulnerable samples, from which we randomly chose 10\\% to assess how much these samples help vulnerability detection, localization, and repair. Our results show SOTA techniques for these three application tasks achieved 19.15--330.80\\% higher F1, 12.86--19.31\\% higher top-10 accuracy, and 85.02--99.30\\% higher top-50 accuracy, respectively, by adding those samples to their original training data. These samples also helped a SOTA vulnerability detector discover 13 more real-world vulnerabilities (CVEs) in critical systems (e.g., Linux kernel) that would be missed by the original model.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "149",
        "numpages": "13",
        "labels": [
            "static analysis",
            "bug detection",
            "code model",
            "code model training",
            "source code model"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Pre-training by Predicting Program Dependencies for Vulnerability Analysis Tasks": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639142",
        "author": "Liu, Zhongxin and Tang, Zhijie and Zhang, Junwei and Xia, Xin and Yang, Xiaohu",
        "title": "Pre-training by Predicting Program Dependencies for Vulnerability Analysis Tasks",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639142",
        "doi": "10.1145/3597503.3639142",
        "abstract": "Vulnerability analysis is crucial for software security. Inspired by the success of pre-trained models on software engineering tasks, this work focuses on using pre-training techniques to enhance the understanding of vulnerable code and boost vulnerability analysis. The code understanding ability of a pre-trained model is highly related to its pre-training objectives. The semantic structure, e.g., control and data dependencies, of code is important for vulnerability analysis. However, existing pre-training objectives either ignore such structure or focus on learning to use it. The feasibility and benefits of learning the knowledge of analyzing semantic structure have not been investigated. To this end, this work proposes two novel pre-training objectives, namely Control Dependency Prediction (CDP) and Data Dependency Prediction (DDP), which aim to predict the statement-level control dependencies and token-level data dependencies, respectively, in a code snippet only based on its source code. During pre-training, CDP and DDP can guide the model to learn the knowledge required for analyzing fine-grained dependencies in code. After pre-training, the pre-trained model can boost the understanding of vulnerable code during fine-tuning and can directly be used to perform dependence analysis for both partial and complete functions. To demonstrate the benefits of our pre-training objectives, we pre-train a Transformer model named PDBERT with CDP and DDP, fine-tune it on three vulnerability analysis tasks, i.e., vulnerability detection, vulnerability classification, and vulnerability assessment, and also evaluate it on program dependence analysis. Experimental results show that PDBERT benefits from CDP and DDP, leading to state-of-the-art performance on the three downstream tasks. Also, PDBERT achieves F1-scores of over 99\\% and 94\\% for predicting control and data dependencies, respectively, in partial and complete functions.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "151",
        "numpages": "13",
        "labels": [
            "static analysis",
            "bug detection",
            "code model",
            "code model training",
            "source code model"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "BinaryAI: Binary Software Composition Analysis via Intelligent Binary Source Code Matching": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639100",
        "author": "Jiang, Ling and An, Junwen and Huang, Huihui and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun",
        "title": "BinaryAI: Binary Software Composition Analysis via Intelligent Binary Source Code Matching",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639100",
        "doi": "10.1145/3597503.3639100",
        "abstract": "While third-party libraries (TPLs) are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. Software composition analysis (SCA), proposed to identify reused TPLs for reducing such risks, has become an essential procedure within modern DevSecOps. As one of the mainstream SCA techniques, binary-to-source SCA identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. The existing binary-to-source SCA techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale TPL dataset, leading to inevitable false positives and compromised recall. To mitigate these limitations, we introduce BinaryAI, a novel binary-to-source SCA technique with two-phase binary source code matching to capture both syntactic and semantic code features. First, BinaryAI trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. Then by applying the link-time locality to facilitate function matching, BinaryAI detects the reused TPLs based on the ratio of matched source functions. Our experimental results demonstrate the superior performance of BinaryAI in terms of binary source code matching and the downstream SCA task. Specifically, our embedding model outperforms the state-of-the-art model CodeCMR, i.e., achieving 22.54\\% recall@1 and 0.34 MRR compared with 10.75\\% and 0.17 respectively. Additionally, BinaryAI outperforms all existing binary-to-source SCA tools in TPL detection, increasing the precision from 73.36\\% to 85.84\\% and recall from 59.81\\% to 64.98\\% compared with the well-recognized commercial SCA product Black Duck.https://www.binaryai.net",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "224",
        "numpages": "13",
        "labels": [
            "static analysis",
            "software composition analysis",
            "code model",
            "code model training",
            "binary code model"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Semantic GUI Scene Learning and Video Alignment for Detecting Duplicate Video-based Bug Reports": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639163",
        "author": "Yan, Yanfu and Cooper, Nathan and Chaparro, Oscar and Moran, Kevin and Poshyvanyk, Denys",
        "title": "Semantic GUI Scene Learning and Video Alignment for Detecting Duplicate Video-based Bug Reports",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639163",
        "doi": "10.1145/3597503.3639163",
        "abstract": "Video-based bug reports are increasingly being used to document bugs for programs centered around a graphical user interface (GUI). However, developing automated techniques to manage video-based reports is challenging as it requires identifying and understanding often nuanced visual patterns that capture key information about a reported bug. In this paper, we aim to overcome these challenges by advancing the bug report management task of duplicate detection for video-based reports. To this end, we introduce a new approach, called Janus, that adapts the scene-learning capabilities of vision transformers to capture subtle visual and textual patterns that manifest on app UI screens --- which is key to differentiating between similar screens for accurate duplicate report detection. Janus also makes use of a video alignment technique capable of adaptive weighting of video frames to account for typical bug manifestation patterns. In a comprehensive evaluation on a benchmark containing 7,290 duplicate detection tasks derived from 270 video-based bug reports from 90 Android app bugs, the best configuration of our approach achieves an overall mRR/mAP of 89.8\\%/84.7\\%, and for the large majority of duplicate detection tasks, outperforms prior work by \u22489\\% to a statistically significant degree. Finally, we qualitatively illustrate how the scene-learning capabilities provided by Janus benefits its performance.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "232",
        "numpages": "13",
        "labels": [
            "software maintenance and deployment",
            "code review"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "EyeTrans: Merging Human and Machine Attention for Neural Code Summarization": {
        "type": "article",
        "key": "10.1145/3643732",
        "author": "Zhang, Yifan and Li, Jiliang and Karas, Zachary and Bansal, Aakash and Li, Toby Jia-Jun and McMillan, Collin and Leach, Kevin and Huang, Yu",
        "title": "EyeTrans: Merging Human and Machine Attention for Neural Code Summarization",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643732",
        "doi": "10.1145/3643732",
        "abstract": "Neural code summarization leverages deep learning models to automatically generate brief natural language summaries of code snippets. The development of Transformer models has led to extensive use of attention during model design. While existing work has primarily and almost exclusively focused on static properties of source code and related structural representations like the Abstract Syntax Tree (AST), few studies have considered human attention \u2014 that is, where programmers focus while examining and comprehending code. In this paper, we develop a method for incorporating human attention into machine attention to enhance neural code summarization. To facilitate this incorporation and vindicate this hypothesis, we introduce EyeTrans, which consists of three steps: (1) we conduct an extensive eye-tracking human study to collect and pre-analyze data for model training, (2) we devise a data-centric approach to integrate human attention with machine attention in the Transformer architecture, and (3) we conduct comprehensive experiments on two code summarization tasks to demonstrate the effectiveness of incorporating human attention into Transformers. Integrating human attention leads to an improvement of up to 29.91\\% in Functional Summarization and up to 6.39\\% in General Code Summarization performance, demonstrating the substantial benefits of this combination. We further explore performance in terms of robustness and efficiency by creating challenging summarization scenarios in which EyeTrans exhibits interesting properties. We also visualize the attention map to depict the simplifying effect of machine attention in the Transformer by incorporating human attention. This work has the potential to propel AI research in software engineering by introducing more human-centered approaches and data.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "6",
        "numpages": "22",
        "labels": [
            "static analysis",
            "code summarization",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Learning to Detect and Localize Multilingual Bugs": {
        "type": "article",
        "key": "10.1145/3660804",
        "author": "Yang, Haoran and Nong, Yu and Zhang, Tao and Luo, Xiapu and Cai, Haipeng",
        "title": "Learning to Detect and Localize Multilingual Bugs",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660804",
        "doi": "10.1145/3660804",
        "abstract": "Increasing studies have shown bugs in multi-language software as a critical loophole in modern software quality assurance, especially those induced by language interactions (i.e., multilingual bugs). Yet existing tool support for bug detection/localization remains largely limited to single-language software, despite the long-standing prevalence of multi-language systems in various real-world software domains. Extant static/dynamic analysis and deep learning (DL) based approaches all face major challenges in addressing multilingual bugs. In this paper, we present xLoc, a DL-based technique/tool for detecting and localizing multilingual bugs. Motivated by results of our bug-characteristics study on top locations of multilingual bugs, xLoc first learns the general knowledge relevant to differentiating various multilingual control-flow structures. This is achieved by pre-training a Transformer model with customized position encoding against novel objectives. Then, xLoc learns task-specific knowledge for the task of multilingual bug detection/localization, through another new position encoding scheme (based on cross-language API vicinity) that allows for the model to attend particularly to control-flow constructs that bear most multilingual bugs during fine-tuning. We have implemented xLoc for Python-C software and curated a dataset of 3,770 buggy and 15,884 non-buggy Python-C samples, which enabled our extensive evaluation of xLoc against two state-of-the-art baselines: fine-tuned CodeT5 and zero-shot ChatGPT. Our results show that xLoc achieved 94.98\\% F1 and 87.24\\%@Top-1 accuracy, which are significantly (up to 162.88\\% and 511.75\\%) higher than the baselines. Ablation studies further confirmed significant contributions of each of the novel design elements in xLoc. With respective bug-location characteristics and labeled bug datasets for fine-tuning, our design may be applied to other language combinations beyond Python-C.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "97",
        "numpages": "24",
        "labels": [
            "static analysis",
            "bug detection",
            "code model",
            "code model training",
            "source code model"
        ],
        "venue": "FSE2024"
    },
    "The Emergence of Large Language Models in Static Analysis: A First Look through Micro-Benchmarks": {
        "type": "inproceedings",
        "key": "venkatesh2024emergence",
        "author": "Venkatesh, Ashwin Prasad Shivarpatna and Sabu, Samkutty and Mir, Amir M and Reis, Sofia and Bodden, Eric",
        "title": "The Emergence of Large Language Models in Static Analysis: A First Look through Micro-Benchmarks",
        "year": "2024",
        "publisher": "ACM",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650105.3652288",
        "doi": "10.1145/3650105.3652288",
        "abstract": "Binary code similarity detection(BCSD), as a fundamental technique in software security, has various applications, including malware family detection, known vulnerability detection and code plagiarism detection. Recent deep learning-based BCSD approaches have demonstrated promising performance. However, they face two significant challenges that limit detection performance. First, most approaches that use sequence networks (like RNN and Transformer) utilize coarse-grained tokenization methods, which results in large vocabulary size and severe out-of-vocabulary (OOV) problem. Second, CFG-based methods typically use variants of graph convolutional networks, which only consider local structural information and discard long-distance dependencies between basic blocks.To address these challenges, this paper proposes Syntax Tree-based instruction embedding and introduces the acyclic graph neural network. The former decomposes assembly instructions into fine-grained tokens and employs a tree-structured neural network to generate vector representations for instructions. The latter transforms CFGs into directed acyclic graphs based on their reducibility, and further captures the dependency between basic blocks with a directed acyclic graph neural network. We implemented these two techniques in a prototype named RCFG2Vec and conducted comprehensive evaluation on two public datasets. The experiment results demonstrate that RCFG2Vec outperforms almost all baselines and achieves detection performance comparable with jTrans, a large model-based approach. Meanwhile, when integrated with our proposed techniques, several baseline approaches exhibit significant improvements in detection performance.",
        "booktitle": "Proceedings of the 2024 {IEEE/ACM} First International Conference on AI Foundation Models and Software Engineering",
        "labels": [
            "static analysis",
            "bug detection",
            "empirical study"
        ],
        "venue": "Forge2024"
    },
    "RCFG2Vec: Considering Long-Distance Dependency for Binary Code Similarity Detection": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695070",
        "author": "Li, Weilong and Lu, Jintian and Xiao, Ruizhi and Shao, Pengfei and Jin, Shuyuan",
        "title": "RCFG2Vec: Considering Long-Distance Dependency for Binary Code Similarity Detection",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695070",
        "doi": "10.1145/3691620.3695070",
        "abstract": "Binary code similarity detection(BCSD), as a fundamental technique in software security, has various applications, including malware family detection, known vulnerability detection and code plagiarism detection. Recent deep learning-based BCSD approaches have demonstrated promising performance. However, they face two significant challenges that limit detection performance. First, most approaches that use sequence networks (like RNN and Transformer) utilize coarse-grained tokenization methods, which results in large vocabulary size and severe out-of-vocabulary (OOV) problem. Second, CFG-based methods typically use variants of graph convolutional networks, which only consider local structural information and discard long-distance dependencies between basic blocks.To address these challenges, this paper proposes Syntax Tree-based instruction embedding and introduces the acyclic graph neural network. The former decomposes assembly instructions into fine-grained tokens and employs a tree-structured neural network to generate vector representations for instructions. The latter transforms CFGs into directed acyclic graphs based on their reducibility, and further captures the dependency between basic blocks with a directed acyclic graph neural network. We implemented these two techniques in a prototype named RCFG2Vec and conducted comprehensive evaluation on two public datasets. The experiment results demonstrate that RCFG2Vec outperforms almost all baselines and achieves detection performance comparable with jTrans, a large model-based approach. Meanwhile, when integrated with our proposed techniques, several baseline approaches exhibit significant improvements in detection performance.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "770\u2013782",
        "numpages": "13",
        "labels": [
            "static analysis",
            "code similarity analysis",
            "code model",
            "code model training",
            "binary code model"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "ART: A Unified Unsupervised Framework for Incident Management in Microservice Systems": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695495",
        "author": "Sun, Yongqian and Shi, Binpeng and Mao, Mingyu and Ma, Minghua and Xia, Sibo and Zhang, Shenglin and Pei, Dan",
        "title": "ART: A Unified Unsupervised Framework for Incident Management in Microservice Systems",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695495",
        "doi": "10.1145/3691620.3695495",
        "abstract": "Automated incident management is critical for large-scale microservice systems, including tasks such as anomaly detection (AD), failure triage (FT), and root cause localization (RCL). Currently, most techniques focus only on a single task, overlooking shared knowledge across closely related tasks. However, employing isolated models for managing multiple tasks may result in inefficiencies, delayed responses, a lack of systemic perspective, and complexity in updates and operations. Therefore we propose ART, an unsupervised framework that integrates a full-process solution covering Anomaly detection, failure Triage, and Root cause localization. It reaches the unification of multiple tasks by extracting the shared knowledge. Specifically, we first conduct an empirical study to analyze how the shared knowledge embedded in anomalous deviations manifests in AD, FT, and RCL. To better calculate deviations and extract shared knowledge, we sequentially model channel, temporal, and call dependencies using Transformer Encoder, GRU, and GraphSAGE, respectively. Then unified failure representations enhance the interpretability of abstract features with explicit semantic information, serving as the basis for unsupervised multitask solutions. Our evaluations on the datasets generated from two benchmark microservice systems demonstrate that ART outperforms existing methods in terms of AD (improving by 5.65\\% to 60.8\\%), FT (improving by 13.2\\% to 95.7\\%), and RCL (improving by 13.3\\% to 205\\%).",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1183\u20131194",
        "numpages": "12",
        "labels": [
            "general coding task"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Attribution-guided Adversarial Code Prompt Generation for Code Completion Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695517",
        "author": "Li, Xueyang and Meng, Guozhu and Liu, Shangqing and Xiang, Lu and Sun, Kun and Chen, Kai and Luo, Xiapu and Liu, Yang",
        "title": "Attribution-guided Adversarial Code Prompt Generation for Code Completion Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695517",
        "doi": "10.1145/3691620.3695517",
        "abstract": "Large language models have made significant progress in code completion, which may further remodel future software development. However, these code completion models are found to be highly risky as they may introduce vulnerabilities unintentionally or be induced by a special input, i.e., adversarial code prompt. Prior studies mainly focus on the robustness of these models, but their security has not been fully analyzed.In this paper, we propose a novel approach AdvPro that can automatically generate adversarial code prompts for these code completion models. AdvPro incorporates 14 code mutation strategies at the granularity of five levels. The mutation strategies are ensured to make no modifications to code semantics, which should be insensitive to the models. Moreover, we leverage gradient attribution to localize the important code as mutation points and speed up adversarial prompt generation. Extensive experiments are conducted on 13 state-of-the-art models belonging to 7 families. The results show that our approach can effectively generate adversarial prompts, with an increased rate of 69.6\\% beyond the baseline ALERT. By comparing the results of attribution-guided localization, we find that the recognition results of important tokens in input codes are almost identical among different models. This finding reduces the limitation of using open-source alternative models to guide adversarial attacks against closed-source models. The results of the ablation study on the components of AdvPro show that CCMs focus on variable names, but other structures are equally crucial.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1460\u20131471",
        "numpages": "12",
        "labels": [
            "code generation",
            "code completion",
            "code model",
            "code model security"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Improving Binary Code Similarity Transformer Models by Semantics-Driven Instruction Deemphasis": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598121",
        "author": "Xu, Xiangzhe and Feng, Shiwei and Ye, Yapeng and Shen, Guangyu and Su, Zian and Cheng, Siyuan and Tao, Guanhong and Shi, Qingkai and Zhang, Zhuo and Zhang, Xiangyu",
        "title": "Improving Binary Code Similarity Transformer Models by Semantics-Driven Instruction Deemphasis",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598121",
        "doi": "10.1145/3597926.3598121",
        "abstract": "Given a function in the binary executable form, binary code similarity analysis determines a set of similar functions from a large pool of candidate functions. These similar functions are usually compiled from the same source code with different compilation setups. Such analysis has a large number of applications, such as malware detection, code clone detection, and automatic software patching. The state-of-the art methods utilize complex Deep Learning models such as Transformer models. We observe that these models suffer from undesirable instruction distribution biases caused by specific compiler conventions. We develop a novel technique to detect such biases and repair them by removing the corresponding instructions from the dataset and finetuning the models. This entails synergy between Deep Learning model analysis and program analysis. Our results show that we can substantially improve the state-of-the-art models\u2019 performance by up to 14.4\\% in the most challenging cases where test data may be out of the distributions of training data.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1106\u20131118",
        "numpages": "13",
        "labels": [
            "code model",
            "code model training",
            "binary code model",
            "static analysis",
            "code similarity analysis"
        ],
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023",
        "venue": "ISSTA2023"
    },
    "Explaining Software Bugs Leveraging Code Structures in Neural Machine Translation": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00063",
        "author": "Mahbub, Parvez and Shuvo, Ohiduzzaman and Rahman, Mohammad Masudur",
        "title": "Explaining Software Bugs Leveraging Code Structures in Neural Machine Translation",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00063",
        "doi": "10.1109/ICSE48619.2023.00063",
        "abstract": "Software bugs claim \u2248 50\\% of development time and cost the global economy billions of dollars. Once a bug is reported, the assigned developer attempts to identify and understand the source code responsible for the bug and then corrects the code. Over the last five decades, there has been significant research on automatically finding or correcting software bugs. However, there has been little research on automatically explaining the bugs to the developers, which is essential but a highly challenging task. In this paper, we propose Bugsplainer, a transformer-based generative model, that generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits. Bugsplainer can leverage structural information and buggy patterns from the source code to generate an explanation for a bug. Our evaluation using three performance metrics shows that Bugsplainer can generate understandable and good explanations according to Google's standard, and can outperform multiple baselines from the literature. We also conduct a developer study involving 20 participants where the explanations from Bugsplainer were found to be more accurate, more precise, more concise and more useful than the baselines.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "640\u2013652",
        "numpages": "13",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023"
    },
    "Concrat: An Automatic C-to-Rust Lock API Translator for Concurrent Programs": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00069",
        "author": "Hong, Jaemin and Ryu, Sukyoung",
        "title": "Concrat: An Automatic C-to-Rust Lock API Translator for Concurrent Programs",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00069",
        "doi": "10.1109/ICSE48619.2023.00069",
        "abstract": "Concurrent programs suffer from data races. To prevent data races, programmers use locks. However, programs can eliminate data races only when they acquire and release correct locks at correct timing. The lock API of C, in which people have developed a large portion of legacy system programs, does not validate the correct use of locks. On the other hand, Rust, a recently developed system programming language, provides a lock API that guarantees the correct use of locks via type checking. This makes rewriting legacy system programs in Rust a promising way to retrofit safety into them. Unfortunately, manual C-to-Rust translation is extremely laborious due to the discrepancies between their lock APIs. Even the state-of-the-art automatic C-to-Rust translator retains the C lock API, expecting developers to replace them with the Rust lock API. In this work, we propose an automatic tool to replace the C lock API with the Rust lock API. It facilitates C-to-Rust translation of concurrent programs with less human effort than the current practice. Our tool consists of a Rust code transformer that takes a lock summary as an input and a static analyzer that efficiently generates precise lock summaries. We show that the transformer is scalable and widely applicable while preserving the semantics; it transforms 66 KLOC in 2.6 seconds and successfully handles 74\\% of real-world programs. We also show that the analyzer is scalable and precise; it analyzes 66 KLOC in 4.3 seconds.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "716\u2013728",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "labels": [
            "code generation",
            "program transformation"
        ]
    },
    "VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners": {
        "type": "article",
        "key": "vert",
        "title": "VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners",
        "author": "Aidan Z.H. Yang, Yoshiki Takashima, Brandon Paulsen, Josiah Dodds, and Daniel Kroening",
        "journal": "arXiv preprint arXiv:2404.18852",
        "year": "2024",
        "abstract": "Rust is a programming language that combines memory safety and low-level control, providing C-like performance while guaranteeing the absence of undefined behaviors by default. Rust's growing popularity has prompted research on safe and correct transpiling of existing code-bases to Rust. Existing work falls into two categories: rule-based and large language model (LLM)-based. While rule-based approaches can theoretically produce correct transpilations that maintain input-output equivalence to the original, they often yield unreadable Rust code that uses unsafe subsets of the Rust language. On the other hand, while LLM-based approaches typically produce more readable, maintainable, and safe code, they do not provide any guarantees about correctness. In this work, we present VERT, a tool that can produce readable Rust transpilations with formal guarantees of correctness. VERT's only requirement is that there is Web Assembly compiler for the source language, which is true for most major languages. VERT first uses the Web Assembly compiler to obtain an oracle Rust program. In parallel, VERT uses an LLM to generate a readable candidate Rust program. This candidate is verified against the oracle, and if verification fails, we regenerate a new candidate transpilation until verification succeeds. We evaluate VERT by transpiling a suite of 1,394 programs taken from competitive programming style benchmarks. Combining Anthropic's Claude-2 and VERT increases Rust transpilations passing property-based testing from 31% to 54% and bounded model-checking from 1% to 42% compared to using Claude alone. In addition, we evaluate VERT's ability to generate non-trivial safe Rust on programs taken from real-world C projects that make significant use of pointers. Our results provide insights into the limitations of LLMs to write safe Rust.",
        "venue": "arXiv2024",
        "labels": [
            "code generation",
            "program transformation",
            "static analysis",
            "program verification"
        ],
        "url": "https://arxiv.org/abs/2404.18852"
    },
    "Context-aware Code Segmentation for C-to-Rust Translation using Large Language Models": {
        "type": "article",
        "key": "10506v1",
        "title": "Context-aware Code Segmentation for C-to-Rust Translation using Large Language Models",
        "author": "Momoko Shiraishi and Takahiro Shinagawa",
        "journal": "arXiv preprint arXiv:2409.10506v1",
        "year": "2024",
        "abstract": "There is strong motivation to translate C code into Rust code due to the continuing threat of memory safety vulnerabilities in existing C programs and the significant attention paid to Rust as an alternative to the C language. While large language models (LLMs) show promise for automating this translation by generating more natural and safer code than rule-based methods, previous studies have shown that LLM-generated Rust code often fails to compile, even for relatively small C programs, due to significant differences between the two languages and context window limitations. We propose an LLM-based translation scheme that improves the success rate of translating large-scale C code into compilable Rust code. Our approach involves three key techniques: (1) pre-processing the C code to better align its structure and expressions with Rust, (2) segmenting the code into optimally sized translation units to avoid exceeding the LLM's context window limits, and (3) iteratively compiling and repairing errors while maintaining consistency between translation units using context-supplementing prompts. Compilation success is an essential first step in achieving functional equivalence, as only compilable code can be further tested. In experiments with 20 benchmark C programs, including those exceeding 4 kilo lines of code, we successfully translated all programs into compilable Rust code without losing corresponding parts of the original code.",
        "venue": "arXiv2024",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "url": "https://arxiv.org/abs/2409.10506v1"
    },
    "Towards Translating Real-World Code with LLMs: A Study of Translating to Rust": {
        "type": "article",
        "key": "2405.11514",
        "title": "Towards Translating Real-World Code with LLMs: A Study of Translating to Rust",
        "author": "Hasan Ferit Eniser, Hanliang Zhang, Cristina David, Meng Wang, Maria Christakis, Brandon Paulsen, Joey Dodds, and Daniel Kroening",
        "journal": "arXiv preprint arXiv:2405.11514",
        "year": "2024",
        "abstract": "Large language models (LLMs) show promise in code translation - the task of translating code written in one programming language to another language - due to their ability to write code in most programming languages. However, LLM's effectiveness on translating real-world code remains largely unstudied. In this work, we perform the first substantial study on LLM-based translation to Rust by assessing the ability of five state-of-the-art LLMs, GPT4, Claude 3, Claude 2.1, Gemini Pro, and Mixtral. We conduct our study on code extracted from real-world open source projects. To enable our study, we develop FLOURINE, an end-to-end code translation tool that uses differential fuzzing to check if a Rust translation is I/O equivalent to the original source program, eliminating the need for pre-existing test cases. As part of our investigation, we assess both the LLM's ability to produce an initially successful translation, as well as their capacity to fix a previously generated buggy one. If the original and the translated programs are not I/O equivalent, we apply a set of automated feedback strategies, including feedback to the LLM with counterexamples. Our results show that the most successful LLM can translate 47% of our benchmarks, and also provides insights into next steps for improvements.",
        "venue": "arXiv2024",
        "labels": [
            "code generation",
            "program transformation",
            "program testing", 
            "fuzzing"
        ],
        "url": "https://arxiv.org/abs/2405.11514"
    },
    "On the Applicability of Language Models to Block-Based Programs": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00199",
        "author": "Griebl, Elisabeth and Fein, Benedikt and Oberm\\\"{u}ller, Florian and Fraser, Gordon and Just, Ren\\'{e",
        "title": "On the Applicability of Language Models to Block-Based Programs",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00199",
        "doi": "10.1109/ICSE48619.2023.00199",
        "abstract": "Block-based programming languages like SCRATCH are increasingly popular for programming education and end-user programming. Recent program analyses build on the insight that source code can be modelled using techniques from natural language processing. Many of the regularities of source code that support this approach are due to the syntactic overhead imposed by textual programming languages. This syntactic overhead, however, is precisely what block-based languages remove in order to simplify programming. Consequently, it is unclear how well this modelling approach performs on block-based programming languages. In this paper, we investigate the applicability of language models for the popular block-based programming language SCRATCH. We model SCRATCH programs using n-gram models, the most essential type of language model, and transformers, a popular deep learning model. Evaluation on the example tasks of code completion and bug finding confirm that blocks inhibit predictability, but the use of language models is nevertheless feasible. Our findings serve as foundation for improving tooling and analyses for block-based languages.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "2374\u20132386",
        "numpages": "13",
        "labels": [
            "code generation",
            "code completion"
        ],
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023"
    },
    "Automating Code-Related Tasks Through Transformers: The Impact of Pre-Training": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00203",
        "author": "Tufano, Rosalia and Pascarella, Luca and Bavota, Gabriele",
        "title": "Automating Code-Related Tasks Through Transformers: The Impact of Pre-Training",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00203",
        "doi": "10.1109/ICSE48619.2023.00203",
        "abstract": "Transformers have gained popularity in the software engineering (SE) literature. These deep learning models are usually pre-trained through a self-supervised objective, meant to provide the model with basic knowledge about a language of interest (e.g., Java). A classic pre-training objective is the masked language model (MLM), in which a percentage of tokens from the input (e.g., a Java method) is masked, with the model in charge of predicting them. Once pre-trained, the model is then fine-tuned to support the specific downstream task of interest (e.g., code summarization). While there is evidence suggesting the boost in performance provided by pre-training, little is known about the impact of the specific pre-training objective(s) used. Indeed, MLM is just one of the possible pre-training objectives and recent work from the natural language processing field suggest that pre-training objectives tailored for the specific downstream task of interest may substantially boost the model's performance. For example, in the case of code summarization, a tailored pre-training objective could be the identification of an appropriate name for a given method, considering the method name to generate as an extreme summary. In this study, we focus on the impact of pre-training objectives on the performance of transformers when automating code-related tasks. We start with a systematic literature review aimed at identifying the pre-training objectives used in SE. Then, we pre-train 32 transformers using both (i) generic pre-training objectives usually adopted in SE; and (ii) pre-training objectives tailored to specific code-related tasks subject of our experimentation, namely bug-fixing, code summarization, and code completion. We also compare the pre-trained models with non pre-trained ones and show the advantage brought by pre-training in different scenarios, in which more or less fine-tuning data are available. Our results show that: (i) pre-training helps in boosting performance only if the amount of fine-tuning data available is small; (ii) the MLM objective is usually sufficient to maximize the prediction performance of the model, even when comparing it with pre-training objectives specialized for the downstream task at hand.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "2425\u20132437",
        "numpages": "13",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model",
            "empirical study"
        ],
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023"
    },
    "Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00205",
        "author": "Nashid, Noor and Sintaha, Mifta and Mesbah, Ali",
        "title": "Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00205",
        "doi": "10.1109/ICSE48619.2023.00205",
        "abstract": "Large language models trained on massive code corpora can generalize to new tasks without the need for task-specific fine-tuning. In few-shot learning, these models take as input a prompt, composed of natural language instructions, a few instances of task demonstration, and a query and generate an output. However, the creation of an effective prompt for code-related tasks in few-shot learning has received little attention. We present a technique for prompt creation that automatically retrieves code demonstrations similar to the developer task, based on embedding or frequency analysis. We apply our approach, CEDAR, to two different programming languages, statically and dynamically typed, and two different tasks, namely, test assertion generation and program repair. For each task, we compare CEDAR with state-of-the-art task-specific and fine-tuned models. The empirical results show that, with only a few relevant code demonstrations, our prompt creation technique is effective in both tasks with an accuracy of 76\\% and 52\\% for exact matches in test assertion generation and program repair tasks, respectively. For assertion generation, CEDAR outperforms existing task-specific and fine-tuned models by 333\\% and 11\\%, respectively. For program repair, CEDAR yields 189\\% better accuracy than task-specific models and is competitive with recent fine-tuned models. These findings have practical implications for practitioners, as CEDAR could potentially be applied to multilingual and multitask settings without task or language-specific training with minimal examples and effort.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "2450\u20132462",
        "numpages": "13",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "agent design",
            "prompt strategy",
            "retrieval-augmented generation"
        ],
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023"
    },
    "VULGEN: Realistic Vulnerability Generation Via Pattern Mining and Deep Learning": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00211",
        "author": "Nong, Yu and Ou, Yuzhe and Pradel, Michael and Chen, Feng and Cai, Haipeng",
        "title": "VULGEN: Realistic Vulnerability Generation Via Pattern Mining and Deep Learning",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00211",
        "doi": "10.1109/ICSE48619.2023.00211",
        "abstract": "Building new, powerful data-driven defenses against prevalent software vulnerabilities needs sizable, quality vulnerability datasets, so does large-scale benchmarking of existing defense solutions. Automatic data generation would promisingly meet the need, yet there is little work aimed to generate much-needed quality vulnerable samples. Meanwhile, existing similar and adaptable techniques suffer critical limitations for that purpose. In this paper, we present VULGEN, the first injection-based vulnerability-generation technique that is not limited to a particular class of vulnerabilities. VULGEN combines the strengths of deterministic (pattern-based) and probabilistic (deep-learning/DL-based) program transformation approaches while mutually overcoming respective weaknesses. This is achieved through close collaborations between pattern mining/application and DL-based injection localization, which separates the concerns with how and where to inject. By leveraging large, pretrained programming language modeling and only learning locations, VULGEN mitigates its own needs for quality vulnerability data (for training the localization model). Extensive evaluations show that VULGEN significantly outperforms a state-of-the-art (SOTA) pattern-based peer technique as well as both Transformer- and GNN-based approaches in terms of the percentages of generated samples that are vulnerable and those also exactly matching the ground truth (by 38.0--430.1\\% and 16.3--158.2\\%, respectively). The VULGEN-generated samples led to substantial performance improvements for two SOTA DL-based vulnerability detectors (by up to 31.8\\% higher in F1), close to those brought by the ground-truth real-world samples and much higher than those by the same numbers of existing synthetic samples.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "2527\u20132539",
        "numpages": "13",
        "labels": [
            "static analysis",
            "bug detection",
            "benchmark"
        ],
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023"
    },
    "VulExplainer: A Transformer-Based Hierarchical Distillation for Explaining Vulnerability Types": {
        "type": "article",
        "key": "10.1109/TSE.2023.3305244",
        "author": "Fu, Michael and Nguyen, Van and Tantithamthavorn, Chakkrit Kla and Le, Trung and Phung, Dinh",
        "title": "VulExplainer: A Transformer-Based Hierarchical Distillation for Explaining Vulnerability Types",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3305244",
        "doi": "10.1109/TSE.2023.3305244",
        "abstract": "Deep learning-based vulnerability prediction approaches are proposed to help under-resourced security practitioners to detect vulnerable functions. However, security practitioners still do not know what type of vulnerabilities correspond to a given prediction (aka CWE-ID). Thus, a novel approach to explain the type of vulnerabilities for a given prediction is imperative. In this paper, we propose &lt;italic&gt;VulExplainer&lt;/italic&gt;, an approach to explain the type of vulnerabilities. We represent &lt;italic&gt;VulExplainer&lt;/italic&gt; as a vulnerability classification task. However, vulnerabilities have diverse characteristics (i.e., CWE-IDs) and the number of labeled samples in each CWE-ID is highly imbalanced (known as a highly imbalanced multi-class classification problem), which often lead to inaccurate predictions. Thus, we introduce a Transformer-based hierarchical distillation for software vulnerability classification in order to address the highly imbalanced types of software vulnerabilities. Specifically, we split a complex label distribution into sub-distributions based on CWE abstract types (i.e., categorizations that group similar CWE-IDs). Thus, similar CWE-IDs can be grouped and each group will have a more balanced label distribution. We learn TextCNN teachers on each of the simplified distributions respectively, however, they only perform well in their group. Thus, we build a transformer student model to generalize the performance of TextCNN teachers through our hierarchical knowledge distillation framework. Through an extensive evaluation using the real-world 8,636 vulnerabilities, our approach outperforms all of the baselines by 5\\%\u201329\\%. The results also demonstrate that our approach can be applied to Transformer-based architectures such as CodeBERT, GraphCodeBERT, and CodeGPT. Moreover, our method maintains compatibility with any Transformer-based model without requiring any architectural modifications but only adds a special distillation token to the input. These results highlight our significant contributions towards the fundamental and practical problem of explaining software vulnerability.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "4550\u20134565",
        "numpages": "16",
        "venue": "TSE2023",
        "labels": [
            "static analysis",
            "bug detection"
        ]
    },
    "CombTransformers: Statement-Wise Transformers for Statement-Wise Representations": {
        "type": "article",
        "key": "10.1109/TSE.2023.3310793",
        "author": "Bertolotti, Francesco and Cazzola, Walter",
        "title": "CombTransformers: Statement-Wise Transformers for Statement-Wise Representations",
        "year": "2023",
        "issue_date": "Oct. 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3310793",
        "doi": "10.1109/TSE.2023.3310793",
        "abstract": "This study presents a novel category of Transformer architectures known as comb transformers, which effectively reduce the space complexity of the self-attention layer from a quadratic to a subquadratic level. This is achieved by processing sequence segments independently and incorporating &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$mathcal{X}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=\"script\"&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"cazzola-ieq1-3310793.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-word embeddings to merge cross-segment information. The reduction in attention memory requirements enables the deployment of deeper architectures, potentially leading to more competitive outcomes. Furthermore, we design an abstract syntax tree (AST)-based code representation to effectively exploit comb transformer properties. To explore the potential of our approach, we develop nine specific instances based on three popular architectural concepts: funnel, hourglass, and encoder-decoder. These architectures are subsequently trained on three code-related tasks: method name generation, code search, and code summarization. These tasks encompass a range of capabilities: short/long sequence generation and classification. In addition to the proposed comb transformers, we also evaluate several baseline architectures for comparative analysis. Our findings demonstrate that the comb transformers match the performance of the baselines and frequently perform better.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "4677\u20134690",
        "numpages": "14",
        "venue": "TSE2023",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model"
        ]
    },
    "MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3267446",
        "author": "Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav",
        "title": "MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3267446",
        "doi": "10.1109/TSE.2023.3267446",
        "abstract": "Large language models have demonstrated the ability to generate both natural language and programming language text. Although contemporary code generation models are trained on corpora with several programming languages, they are tested using benchmarks that are typically monolingual. The most widely used code generation benchmarks only target Python, so there is little quantitative evidence of how code generation models perform on other programming languages. We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages. We use MultiPL-E to extend the HumanEval benchmark (Chen et al., 2021) and MBPP benchmark (Austin et al., 2021) to 18 languages that encompass a range of programming paradigms and popularity. Using these new parallel benchmarks, we evaluate the multi-language performance of three state-of-the-art code generation models: Codex (Chen et al., 2021), CodeGen (Nijkamp et al., 2022) and InCoder (Fried et al., 2022). We find that Codex matches or even exceeds its performance on Python for several other languages. The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance. Finally, the MultiPL-E approach of compiling code generation benchmarks to new programming languages is both scalable and extensible, making it straightforward to evaluate new models, benchmarks, and languages.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "3675\u20133691",
        "numpages": "17",
        "venue": "TSE2023",
        "labels": [
            "code generation",
            "program synthesis",
            "benchmark"
        ]
    },
    "CoSS: Leveraging Statement Semantics for Code Summarization": {
        "type": "article",
        "key": "10.1109/TSE.2023.3256362",
        "author": "Shi, Chaochen and Cai, Borui and Zhao, Yao and Gao, Longxiang and Sood, Keshav and Xiang, Yong",
        "title": "CoSS: Leveraging Statement Semantics for Code Summarization",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3256362",
        "doi": "10.1109/TSE.2023.3256362",
        "abstract": "Automated code summarization tools allow generating descriptions for code snippets in natural language, which benefits software development and maintenance. Recent studies demonstrate that the quality of generated summaries can be improved by using additional code representations beyond token sequences. The majority of contemporary approaches mainly focus on extracting code syntactic and structural information from abstract syntax trees (ASTs). However, from the view of macro-structures, it is challenging to identify and capture semantically meaningful features due to fine-grained syntactic nodes involved in ASTs. To fill this gap, we investigate how to learn more code semantics and control flow features from the perspective of code statements. Accordingly, we propose a novel model entitled CoSS for code summarization. CoSS adopts a Transformer-based encoder and a graph attention network-based encoder to capture token-level and statement-level semantics from code token sequence and control flow graph, respectively. Then, after receiving two-level embeddings from encoders, a joint decoder with a multi-head attention mechanism predicts output sequences verbatim. Performance evaluations on Java, Python, and Solidity datasets validate that CoSS outperforms nine state-of-the-art (SOTA) neural code summarization models in effectiveness and is competitive in execution efficiency. Further, the ablation study reveals the contribution of each model component.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "3472\u20133486",
        "numpages": "15",
        "venue": "TSE2023",
        "labels": [
            "static analysis",
            "code summarization",
            "code model",
            "code model training",
            "source code model"
        ]
    },
    "Using Transfer Learning for Code-Related Tasks": {
        "type": "article",
        "key": "10.1109/TSE.2022.3183297",
        "author": "Mastropaolo, Antonio and Cooper, Nathan and Palacio, David Nader and Scalabrino, Simone and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele",
        "title": "Using Transfer Learning for Code-Related Tasks",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3183297",
        "doi": "10.1109/TSE.2022.3183297",
        "abstract": "Deep learning (DL) techniques have been used to support several code-related tasks such as code summarization and bug-fixing. In particular, pre-trained transformer models are on the rise, also thanks to the excellent results they achieved in Natural Language Processing (NLP) tasks. The basic idea behind these models is to first pre-train them on a generic dataset using a self-supervised task (e.g., filling masked words in sentences). Then, these models are fine-tuned to support specific tasks of interest (e.g., language translation). A single model can be fine-tuned to support multiple tasks, possibly exploiting the benefits of &lt;italic&gt;transfer learning&lt;/italic&gt;. This means that knowledge acquired to solve a specific task (e.g., language translation) can be useful to boost performance on another task (e.g., sentiment classification). While the benefits of transfer learning have been widely studied in NLP, limited empirical evidence is available when it comes to code-related tasks. In this paper, we assess the performance of the Text-To-Text Transfer Transformer (T5) model in supporting four different code-related tasks: (i) automatic bug-fixing, (ii) injection of code mutants, (iii) generation of assert statements, and (iv) code summarization. We pay particular attention in studying the role played by pre-training and multi-task fine-tuning on the model's performance. We show that (i) the T5 can achieve better performance as compared to state-of-the-art baselines; and (ii) while pre-training helps the model, not all tasks benefit from a multi-task fine-tuning.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1580\u20131598",
        "numpages": "19",
        "venue": "TSE2023",
        "labels": [
            "general coding task",
            "code model",
            "code model training",
            "source code model"
        ]
    },
    "On the Effectiveness of Transfer Learning for Code Search": {
        "type": "article",
        "key": "10.1109/TSE.2022.3192755",
        "author": "Salza, Pasquale and Schwizer, Christoph and Gu, Jian and Gall, Harald C.",
        "title": "On the Effectiveness of Transfer Learning for Code Search",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "IEEE Press",
        "volume": "49",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2022.3192755",
        "doi": "10.1109/TSE.2022.3192755",
        "abstract": "The Transformer architecture and transfer learning have marked a quantum leap in natural language processing, improving the state of the art across a range of text-based tasks. This paper examines how these advancements can be applied to and improve code search. To this end, we pre-train a BERT-based model on combinations of natural language and source code data and fine-tune it on pairs of StackOverflow question titles and code answers. Our results show that the pre-trained models consistently outperform the models that were not pre-trained. In cases where the model was pre-trained on natural language \u201cand\u201d source code data, it also outperforms an information retrieval baseline based on Lucene. Also, we demonstrated that the combined use of an information retrieval-based approach followed by a Transformer leads to the best results overall, especially when searching into a large search pool. Transfer learning is particularly effective when much pre-training data is available and fine-tuning data is limited. We demonstrate that natural language processing models based on the Transformer architecture can be directly applied to source code analysis tasks, such as code search. With the development of Transformer models designed more specifically for dealing with source code data, we believe the results of source code analysis tasks can be further improved.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1804\u20131822",
        "numpages": "19",
        "venue": "TSE2023",
        "labels": [
            "static analysis",
            "code search",
            "code model",
            "code model training",
            "source code model"
        ]
    },
    "The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches": {
        "type": "article",
        "key": "10.1145/3576039",
        "author": "Tian, Haoye and Liu, Kui and Li, Yinghua and Kabor\\'{e}, Abdoul Kader and Koyuncu, Anil and Habib, Andrew and Li, Li and Wen, Junhao and Klein, Jacques and Bissyand\\'{e}, Tegawend\\'{e} F.",
        "title": "The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches",
        "year": "2023",
        "issue_date": "July 2023",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "32",
        "number": "4",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3576039",
        "doi": "10.1145/3576039",
        "abstract": "A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state-of-the-art explores research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower Leopard (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based learned embeddings associated with XGBoost achieves an AUC value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. By combining deep learned embeddings and engineered features, Panther (the upgraded version of Leopard implemented in this work) outperforms Leopard with higher scores in terms of AUC, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable ML technique, SHAP, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "may",
        "articleno": "92",
        "numpages": "34",
        "labels": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "venue": "TOSEM2023"
    },
    "Code Structure\u2013Guided Transformer for Source Code Summarization": {
        "type": "article",
        "key": "10.1145/3522674",
        "author": "Gao, Shuzheng and Gao, Cuiyun and He, Yulan and Zeng, Jichuan and Nie, Lunyiu and Xia, Xin and Lyu, Michael",
        "title": "Code Structure\u2013Guided Transformer for Source Code Summarization",
        "year": "2023",
        "issue_date": "January 2023",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "32",
        "number": "1",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3522674",
        "doi": "10.1145/3522674",
        "abstract": "Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4\\% and 2.0\\% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "feb",
        "articleno": "23",
        "numpages": "32",
        "labels": [
            "static analysis",
            "code summarization"
        ],
        "venue": "TOSEM2023"
    },
    "On the Evaluation of Neural Code Translation: Taxonomy and Benchmark": {
        "type": "INPROCEEDINGS",
        "key": "10298408",
        "author": "Jiao, Mingsheng and Yu, Tingrui and Li, Xuan and Qiu, Guanjie and Gu, Xiaodong and Shen, Beijun",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "On the Evaluation of Neural Code Translation: Taxonomy and Benchmark",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1529-1541",
        "abstract": "In recent years, neural code translation has gained increasing attention. While most of the research focuses on improving model architectures and training processes, we notice that the evaluation process and benchmark for code translation models are severely limited: they primarily treat source code as natural languages and provide a holistic accuracy score while disregarding the full spectrum of model capabilities across different translation types and complexity. In this paper, we present a comprehensive investigation of four state-of-the-art models and analyze in-depth the advantages and limitations of three existing benchmarks. Based on the empirical results, we develop a taxonomy that categorizes code translation tasks into four primary types according to their complexity and knowledge dependence: token level (type 1), syntactic level (type 2), library level (type 3), and algorithm level (type 4). We then conduct a thorough analysis of how existing approaches perform across these four categories. Our findings indicate that while state-of-the-art code translation models excel in type-1 and type-2 translations, they struggle with knowledge-dependent ones such as type-3 and type-4. Existing benchmarks are biased towards trivial translations, such as keyword mapping. To overcome these limitations, we construct G-TransEval, a new benchmark by manually curating type-3 and type-4 translation pairs and unit test cases. Results on our new benchmark suggest that G-TransEval can exhibit more comprehensive and finer-grained capability of code translation models and thus provide a more rigorous evaluation. Our studies also provide more insightful findings and suggestions for future research, such as building type-3 and type-4 training data and ensembling multiple pretraining approaches.",
        "labels": [
            "code generation",
            "code completion",
            "empirical study"
        ],
        "doi": "10.1109/ASE56229.2023.00114",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://arxiv.org/pdf/2308.08961"
    },
    "VALAR: Streamlining Alarm Ranking in Static Analysis with Value-Flow Assisted Active Learning": {
        "type": "INPROCEEDINGS",
        "key": "10298558",
        "author": "Liu, Pengcheng and Lu, Yifei and Yang, Wenhua and Pan, Minxue",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "VALAR: Streamlining Alarm Ranking in Static Analysis with Value-Flow Assisted Active Learning",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1940-1951",
        "abstract": "Static analyzers play a critical role in program defects and security vulnerabilities detection. Despite their importance, the widespread adoption of static analysis techniques in industrial development faces numerous obstacles, among which the high rate of false alarms constitutes a significant one. To address this issue, we propose a novel approach called Valar, which performs alarm ranking for advanced value-flow analysis using the active learning technique. Active learning algorithms minimize the manual effort for alarm inspection by maximizing the effect of each user labeling in recognizing true/false alarms. Meanwhile, the value-flows provide Valar with a concise and comprehensive summary of the operational semantics about programs. Based on this, Valar is able to reason about the potential correlations between alarms and prioritize the most profitable unlabeled alarm. Additionally, the accuracy of Valar increases as more user labels are given and Valar's active learning model is further refined. We evaluate Valar on 20 real-world C/C++ programs using three value-flow based checkers. Our experimental results demonstrated that Valar significantly lowers the priorities of false alarms with most true alarms ranked high. Notably, Valar ranked all true alarms in the top 47% in 90% projects and ranked 90% true alarms in the top 22% in 75% projects. Furthermore, Valar has no requirement for pretraining and has a negligible computation time of less than 0.1s for each alarm prioritization.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "doi": "10.1109/ASE56229.2023.00098",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10298558"
    },
    "Twin Graph-Based Anomaly Detection via Attentive Multi-Modal Learning for Microservice System": {
        "type": "INPROCEEDINGS",
        "key": "10298321",
        "author": "Huang, Jun and Yang, Yang and Yu, Hang and Li, Jianguo and Zheng, Xiao",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Twin Graph-Based Anomaly Detection via Attentive Multi-Modal Learning for Microservice System",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "66-78",
        "abstract": "Microservice architecture has sprung up over recent years for managing enterprise applications, due to its ability to independently deploy and scale services. Despite its benefits, ensuring the reliability and safety of a microservice system remains highly challenging. Existing anomaly detection algorithms based on a single data modality (i.e., metrics, logs, or traces) fail to fully account for the complex correlations and interactions between different modalities, leading to false negatives and false alarms, whereas incorporating more data modalities can offer opportunities for further performance gain. As a fresh attempt, we propose in this paper a semi-supervised graph-based anomaly detection method, MSTGAD, which seamlessly integrates all available data modalities via attentive multi-modal learning. First, we extract and normalize features from the three modalities, and further integrate them using a graph, namely MST (microservice system twin) graph, where each node represents a service instance and the edge indicates the scheduling relationship between different service instances. The MST graph provides a virtual representation of the status and scheduling relationships among service instances of a real-world microservice system. Second, we construct a transformer-based neural network with both spatial and temporal attention mechanisms to model the inter-correlations between different modalities and temporal dependencies between the data points. This enables us to detect anomalies automatically and accurately in real-time. Extensive experiments on two real-world datasets verify the effectiveness of our proposed MSTGAD method, achieving competitive performance against state-of-the-art approaches, with a 0.961 F1-score and an average increase of 4.85%. The source code of MST-GAD is publicly available at https://github.com/ant-research/microservice_system_twin_graph_based_anomaly_detection.",
        "labels": [
            "static analysis",
            "bug detection"
        ],
        "doi": "10.1109/ASE56229.2023.00138",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://arxiv.org/pdf/2310.04701"
    },
    "Cell2Doc: ML Pipeline for Generating Documentation in Computational Notebooks": {
        "type": "INPROCEEDINGS",
        "key": "10298542",
        "author": "Mondal, Tamal and Barnett, Scott and Lal, Akash and Vedurada, Jyothi",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Cell2Doc: ML Pipeline for Generating Documentation in Computational Notebooks",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "384-396",
        "abstract": "Computational notebooks have become the go-to way for solving data-science problems. While they are designed to combine code and documentation, prior work shows that documentation is largely ignored by the developers because of the manual effort. Automated documentation generation can help, but existing techniques fail to capture algorithmic details and developers often end up editing the generated text to provide more explanation and sub-steps. This paper proposes a novel machine-learning pipeline, Cell2Doc, for code cell documentation in Python data science notebooks. Our approach works by identifying different logical contexts within a code cell, generating documentation for them separately, and finally combining them to arrive at the documentation for the entire code cell. Cell2Doc takes advantage of the capabilities of existing pre-trained language models and improves their efficiency for code cell documentation. We also provide a new benchmark dataset for this task, along with a data-preprocessing pipeline that can be used to create new datasets. We also investigate an appropriate input representation for this task. Our automated evaluation suggests that our best input representation improves the pre-trained model's performance by 2.5x on average. Further, Cell2Doc achieves 1.33x improvement during human evaluation in terms of correctness, informativeness, and readability against the corresponding standalone pretrained model.",
        "labels": [
            "software maintenance and deployment",
            "documentation generation"
        ],
        "doi": "10.1109/ASE56229.2023.00200",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://dl.acm.org/doi/pdf/10.1109/ASE56229.2023.00200"
    },
    "Domain Adaptive Code Completion via Language Models and Decoupled Domain Databases": {
        "type": "INPROCEEDINGS",
        "key": "10298575",
        "author": "Tang, Ze and Ge, Jidong and Liu, Shangqing and Zhu, Tingwei and Xu, Tongtong and Huang, Liguo and Luo, Bin",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Domain Adaptive Code Completion via Language Models and Decoupled Domain Databases",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "421-433",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in code completion. However, due to the lack of domain-specific knowledge, they may not be optimal in completing code that requires intensive domain knowledge for example completing the library names. Although there are several works that have confirmed the effectiveness of fine-tuning techniques to adapt language models for code completion in specific domains. They are limited by the need for constant fine-tuning of the model when the project is in constant iteration. To address this limitation, in this paper, we propose $k$ NM-LM, a retrieval-augmented language model (R-LM), that integrates domain knowledge into language models without fine-tuning. Different from previous techniques, our approach is able to automatically adapt to different language models and domains. Specifically, it utilizes the in-domain code to build the retrieval-based database decoupled from LM, and then combines it with LM through Bayesian inference to complete the code. The extensive experiments on the completion of intra-project and intra-scenario have confirmed that $k$ NM-LM brings about appreciable enhancements when compared to CodeGPT and UnixCoder. A deep analysis of our tool including the responding speed, storage usage, specific type code completion, and API invocation completion has confirmed that $k$ NM-LM provides satisfactory performance, which renders it highly appropriate for domain adaptive code completion. Furthermore, our approach operates without the requirement for direct access to the language model's parameters. As a result, it can seamlessly integrate with black-box code completion models, making it easy to integrate our approach as a plugin to further enhance the performance of these models.",
        "labels": [
            "code generation",
            "code completion",
            "code model",
            "code model training",
            "source code model"
        ],
        "doi": "10.1109/ASE56229.2023.00076",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023",
        "url": "https://arxiv.org/pdf/2308.09313"
    },
    "Poster: Boosting Adversarial Robustness by Adversarial Pre-training": {
        "type": "inproceedings",
        "key": "10.1145/3576915.3624370",
        "author": "Xu, Xiaoyun and Picek, Stjepan",
        "title": "Poster: Boosting Adversarial Robustness by Adversarial Pre-training",
        "year": "2023",
        "isbn": "9798400700507",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3576915.3624370",
        "doi": "10.1145/3576915.3624370",
        "abstract": "Vision Transformer (ViT) shows superior performance on various tasks, but, similar to other deep learning techniques, it is vulnerable to adversarial attacks. Due to the differences between ViT and traditional CNNs, previous works designed new adversarial training methods as defenses according to the design of ViT, such as blocking attention to individual patches or dropping embeddings with low attention. However, these methods usually focus on fine-tuning stage or the training of the model itself. Improving robustness at the pre-training stage, especially with lower overhead, has yet to be thoroughly investigated. This paper proposes a novel method, Adv-MAE, which increases adversarial robustness by masked adversarial pre-training without a penalty to performance on clean data. We design a simple method to generate adversarial perturbation for the autoencoder, as the autoencoder does not provide classification results. Then, we use masked inputs with perturbation to conduct adversarial training for the autoencoder. The pre-trained autoencoder can be used to build a ViT with better robustness. Our experimental results show that, when using adversarial fine-tuning, Adv-MAE offers better accuracy under adversarial attack than the non-adversarial pre-training method (3.46\\% higher on CIFAR-10, 1.12\\% higher on Tiny ImageNet). It also shows better accuracy on clean data (4.94\\% higher on CIFAR-10, 1.74\\% higher on Tiny ImageNet), meaning Adv-MAE does not deteriorate performance on clean inputs. In addition, masked pre-training also shows much lower time consumption at each training epoch.",
        "booktitle": "Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "3540\u20133542",
        "numpages": "3",
        "labels": [
            "code model",
            "code model security"
        ],
        "location": "Copenhagen, Denmark",
        "series": "CCS '23",
        "venue": "CCS2023"
    },
    "PELICAN: exploiting backdoors of naturally trained deep learning models in binary code analysis": {
        "type": "inproceedings",
        "key": "10.5555/3620237.3620370",
        "author": "Zhang, Zhuo and Tao, Guanhong and Shen, Guangyu and An, Shengwei and Xu, Qiuling and Liu, Yingqi and Ye, Yapeng and Wu, Yaoxuan and Zhang, Xiangyu",
        "title": "PELICAN: exploiting backdoors of naturally trained deep learning models in binary code analysis",
        "year": "2023",
        "isbn": "978-1-939133-37-3",
        "publisher": "USENIX Association",
        "address": "USA",
        "abstract": "Deep Learning (DL) models are increasingly used in many cyber-security applications and achieve superior performance compared to traditional solutions. In this paper, we study backdoor vulnerabilities in naturally trained models used in binary analysis. These backdoors are not injected by attackers but rather products of defects in datasets and/or training processes. The attacker can exploit these vulnerabilities by injecting some small fixed input pattern (e.g., an instruction) called backdoor trigger to their input (e.g., a binary code snippet for a malware detection DL model) such that misclassification can be induced (e.g., the malware evades the detection). We focus on transformer models used in binary analysis. Given a model, we leverage a trigger inversion technique particularly designed for these models to derive trigger instructions that can induce misclassification. During attack, we utilize a novel trigger injection technique to insert the trigger instruction(s) to the input binary code snippet. The injection makes sure that the code snippets' original program semantics are preserved and the trigger becomes an integral part of such semantics and hence cannot be easily eliminated. We evaluate our prototype PELICAN on 5 binary analysis tasks and 15 models. The results show that PELICAN can effectively induce misclassification on all the evaluated models in both white-box and black-box scenarios. Our case studies demonstrate that PELICAN can exploit the backdoor vulnerabilities of two closed-source commercial tools.",
        "booktitle": "Proceedings of the 32nd USENIX Conference on Security Symposium",
        "articleno": "133",
        "numpages": "18",
        "location": "Anaheim, CA, USA",
        "series": "SEC '23",
        "venue": "USENIXSec2023",
        "labels": [
            "code model",
            "code model security",
            "code model",
            "code model training",
            "binary code model"
        ],
        "url": "https://www.usenix.org/system/files/usenixsecurity23-zhang-zhuo-pelican.pdf"
    },
    "Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained Language Models": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-towards-low",
        "author": "Wang, Weishi and Wang, Yue and Hoi, Steven and Joty, Shafiq",
        "booktitle": "EMNLP2023",
        "title": "Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Automatic program repair (APR) has gained increasing attention as an essential technique in software development to reduce manual debugging efforts and boost developers\u2019 productivity. Recent advances in deep learning (DL) based models have demonstrated promising results by learning from large-scale bug-fix examples in a data-driven manner. However, in practical scenarios, software bugs have an imbalanced distribution, and the fixing knowledge learned by APR models often only capture the patterns of frequent error types, making it inapplicable to handle the rare error types. To address this limitation, we investigate a novel task of low-resource APR, and propose Meta-APR, a new meta-learning framework integrated with code pretrained language models to generate fixes for low-resource bugs with limited training samples. Our Meta-APR learns better error-specific knowledge from high-resource bugs through efficient first-order meta-learning optimization, which allows for a faster adaptation to the target low-resource bugs. Besides, while we adopt CodeT5, a pretrained code-aware encoder-decoder Transformer, as the backbone model for Meta-APR, it is a model-agnostic framework that can be integrated with any neural models. Extensive experimental results on three benchmarks in various programming languages verify the superiority of our method over existing DL-based APR approaches.",
        "labels": [
            "code generation",
            "program repair",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.430",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code": {
        "type": "INPROCEEDINGS",
        "key": "zhou-etal-2023-codebertscore",
        "author": "Zhou, Shuyan and Alon, Uri and Agarwal, Sumit and Neubig, Graham",
        "booktitle": "EMNLP2023",
        "title": "CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Since the rise of neural natural-language-to-code models (NL\\rightarrowCode) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than **1,000,000** times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score",
        "labels": [
            "code generation",
            "code model",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.859",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Improving Transformer-based Program Repair Model through False Behavior Diagnosis": {
        "type": "INPROCEEDINGS",
        "key": "kim-etal-2023-improving",
        "author": "Kim, Youngkyoung and Kim, Misoo and Lee, Eunseok",
        "booktitle": "EMNLP2023",
        "title": "Improving Transformer-based Program Repair Model through False Behavior Diagnosis",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Research on automated program repairs using transformer-based models has recently gained considerable attention. The comprehension of the erroneous behavior of a model enables the identification of its inherent capacity and provides insights for improvement. However, the current landscape of research on program repair models lacks an investigation of their false behavior. Thus, we propose a methodology for diagnosing and treating the false behaviors of transformer-based program repair models. Specifically, we propose 1) a behavior vector that quantifies the behavior of the model when it generates an output, 2) a behavior discriminator (BeDisc) that identifies false behaviors, and 3) two methods for false behavior treatment. Through a large-scale experiment on 55,562 instances employing four datasets and three models, the BeDisc exhibited a balanced accuracy of 86.6% for false behavior classification. The first treatment, namely, early abortion, successfully eliminated 60.4% of false behavior while preserving 97.4% repair accuracy. Furthermore, the second treatment, namely, masked bypassing, resulted in an average improvement of 40.5% in the top-1 repair accuracy. These experimental results demonstrated the importance of investigating false behaviors in program repair models.",
        "labels": [
            "code generation",
            "program repair"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.865",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "On Sample-Efficient Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "han-etal-2023-sample",
        "author": "Han, Hojae and Kim, Yu Jin and Kim, Byoungjip and Lee, Youngwon and Lee, Kyungjae and Lee, Kyungmin and Lee, Moontae and Bae, Kyunghoon and Hwang, Seung-won",
        "booktitle": "EMNLP2023",
        "title": "On Sample-Efficient Code Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models often struggle to predict runtime behavior in code generation tasks, leading to a reliance on rejection sampling (best-of-n) to generate multiple code snippets then select the best. Our distinction is reducing sampling costs, without compromising generation quality. We introduce EFFICODE, a novel framework that prioritizes sampling on test problems that models can solve. We show how EFFICODE estimates solvability to optimize computational costs during multiple sampling. Based on empirical evidence, EFFICODE consistently demonstrates reduced sampling budgets while maintaining comparable code generation performance, especially when problems are challenging. In addition, utilizing EFFICODE to rank sampled code snippets also shows its effectiveness in answer code selection for reducing temporal costs, by not requiring any execution or test case generation.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-industry.73",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Code4Struct: Code Generation for Few-Shot Event Structure Prediction": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-code4struct",
        "author": "Wang, Xingyao and Li, Sha and Ji, Heng",
        "booktitle": "ACL2023",
        "title": "Code4Struct: Code Generation for Few-Shot Event Structure Prediction",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Model (LLM) trained on a mixture of text and code has demonstrated impressive capability in translating natural language (NL) into structured code. We observe that semantic structures can be conveniently translated into code and propose Code4Struct to leverage such text-to-structure translation capability to tackle structured prediction tasks. As a case study, we formulate Event Argument Extraction (EAE) as converting text into event-argument structures that can be represented as a class object using code. This alignment between structures and code enables us to take advantage of Programming Language (PL) features such as inheritance and type annotation to introduce external knowledge or add constraints. We show that, with sufficient in-context examples, formulating EAE as a code generation problem is advantageous over using variants of text-based prompts. Despite only using 20 training event instances for each event type, Code4Struct is comparable to supervised models trained on 4,202 instances and outperforms current state-of-the-art (SOTA) trained on 20-shot data by 29.5% absolute F1. Code4Struct can use 10-shot training data from a sibling event type to predict arguments for zero-resource event types and outperforms the zero-shot baseline by 12% absolute F1.",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.202",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Multitask Pretraining with Structured Knowledge for Text-to-SQL Generation": {
        "type": "INPROCEEDINGS",
        "key": "giaquinto-etal-2023-multitask",
        "author": "Giaquinto, Robert and Zhang, Dejiao and Kleiner, Benjamin and Li, Yang and Tan, Ming and Bhatia, Parminder and Nallapati, Ramesh and Ma, Xiaofei",
        "booktitle": "ACL2023",
        "title": "Multitask Pretraining with Structured Knowledge for Text-to-SQL Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Many machine learning-based low-code or no-code applications involve generating code that interacts with structured knowledge. For example, one of the most studied tasks in this area is generating SQL code from a natural language statement. Prior work shows that incorporating context information from the database schema, such as table and column names, is beneficial to model performance on this task. In this work we present a large pretraining dataset and strategy for learning representations of text, tables, and SQL code that leverages the entire context of the problem. Specifically, we build on existing encoder-decoder architecture by introducing a multitask pretraining framework that complements the unique attributes of our diverse pretraining data. Our work represents the first study on large-scale pretraining of encoder-decoder models for interacting with structured knowledge, and offers a new state-of-the-art foundation model in text-to-SQL generation. We validate our approach with experiments on two SQL tasks, showing improvement over existing methods, including a 1.7 and 2.2 percentage point improvement over prior state-of-the-arts on Spider and CoSQL.",
        "labels": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.620",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Python Code Generation by Asking Clarification Questions": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-python",
        "author": "Li, Haau-Sing (Xiaocheng) and Mesgar, Mohsen and Martins, Andr\u00e9 and Gurevych, Iryna",
        "booktitle": "ACL2023",
        "title": "Python Code Generation by Asking Clarification Questions",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code generation from text requires understanding the user\u2019s intent from a natural languagedescription and generating an executable code snippet that satisfies this intent. While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that the under-specification of a natural language description can be resolved by asking clarification questions. Therefore, we collect and introduce a new dataset named CodeClarQA containing pairs of natural language descriptions and code with created synthetic clarification questions and answers. The empirical results of our evaluation of pretrained language model performance on code generation show that clarifications result in more precisely generated code, as shown by the substantial improvement of model performance in all evaluation metrics. Alongside this, our task and dataset introduce new challenges to the community, including when and what clarification questions should be asked. Our code and dataset are available on GitHub.",
        "labels": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.799",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Domain-specific transformer models for query translation": {
        "type": "INPROCEEDINGS",
        "key": "kulkarni-etal-2023-domain",
        "author": "Kulkarni, Mandar and Garera, Nikesh and Trivedi, Anusua",
        "booktitle": "ACL2023",
        "title": "Domain-specific transformer models for query translation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Due to the democratization of e-commerce, many product companies are listing their goods for online shopping. For periodic buying within a domain such as Grocery, consumers are generally inclined to buy certain brands of products. Due to a large non-English speaking population in India, we observe a significant percentage of code-mix Hinglish search queries e.g., sasta atta. An intuitive approach to dealing with code-mix queries is to train an encoder-decoder model to translate the query to English to perform the search. However, the problem becomes non-trivial when the brand names themselves have Hinglish names and possibly have a literal English translation. In such queries, only the context (non-brand name) Hinglish words needs to be translated. In this paper, we propose a simple yet effective modification to the transformer training to preserve/correct Grocery brand names in the output while selectively translating the context words. To achieve this, we use an additional dataset of popular Grocery brand names. Brand names are added as tokens to the model vocabulary, and the token embeddings are randomly initialized. Further, we introduce a Brand loss in training the translation model. Brand loss is a cross entropy loss computed using a denoising auto-encoder objective with brand name data. We warm-start the training from a public pre-trained checkpoint (such as BART/T5) and further adapt it for query translation using the domain data. The proposed model is generic and can be used with English as well as code-mix Hinglish queries alleviating the need for language detection. To reduce the latency of the model for the production deployment, we use knowledge distillation and quantization. Experimental evaluation indicates that the proposed approach improves translation results by preserving/correcting English/Hinglish brand names. After positive results with A/B testing, the model is currently deployed in production.",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-industry.10",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "A Static Evaluation of Code Completion by Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ding-etal-2023-static",
        "author": "Ding, Hantian and Kumar, Varun and Tian, Yuchen and Wang, Zijian and Kwiatkowski, Rob and Li, Xiaopeng and Ramanathan, Murali Krishna and Ray, Baishakhi and Bhatia, Parminder and Sengupta, Sudipta",
        "booktitle": "ACL2023",
        "title": "A Static Evaluation of Code Completion by Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the other hand, static analysis tools such as linters, which can detect errors without running the program, haven\u2019t been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models. Through extensive studies, we also show the impact of sampling temperature, model size, and context on static errors in code completions.",
        "labels": [
            "code generation",
            "code completion",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-industry.34",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Prompting Is Programming: A Query Language for Large Language Models": {
        "type": "article",
        "key": "10.1145/3591300",
        "author": "Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin",
        "title": "Prompting Is Programming: A Query Language for Large Language Models",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "7",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3591300",
        "doi": "10.1145/3591300",
        "abstract": "Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation.  On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.  Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics.  To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model.  We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85\\% cost savings).",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "186",
        "numpages": "24",
        "labels": [
            "PL design for LLMs"
        ],
        "venue": "PLDI2023"
    },
    "Scallop: A Language for Neurosymbolic Programming": {
        "type": "article",
        "key": "10.1145/3591300",
        "author": "Li, Ziyang and Huang, Jiani and Naik, Mayur",
        "title": "Scallop: A Language for Neurosymbolic Programming",
        "year": "2023",
        "issue_date": "June 2023",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "7",
        "number": "PLDI",
        "url": "https://doi.org/10.1145/3591280",
        "doi": "10.1145/3591280",
        "abstract": "We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-the-art models in terms of accuracy. Furthermore, Scallop's solutions outperform these models in aspects such as runtime and data efficiency, interpretability, and generalizability.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "jun",
        "articleno": "166",
        "numpages": "25",
        "labels": [
            "PL design for LLMs"
        ],
        "venue": "PLDI2023"
    },
    "Relational Programming with Foundational Models": {
        "type": "article",
        "key": "LiHLZZDVAN24",
        "author": "Ziyang Li and Jiani Huang and Jason Liu and Felix Zhu and Eric Zhao and William Dodds and Neelay Velingker and Rajeev Alur and Mayur Naik",
        "title": "Relational Programming with Foundational Models",
        "year": "2024",
        "issue_date": "Oct 2024",
        "publisher": "Thirty-Eighth {AAAI} Conference on Artificial Intelligence",
        "address": "Vancouver, Canada",
        "url": "https://doi.org/10.1609/aaai.v38i9.28934",
        "doi": "10.1609/AAAI.V38I9.28934",
        "abstract": "Foundation models have vast potential to enable diverse AI applications. The powerful yet incomplete nature of these models has spurred a wide range of mechanisms to augment them with capabilities such as in-context learning, information retrieval, and code interpreting. We propose VIEIRA, a declarative framework that unifies these mechanisms in a general solution for programming with foundation models. VIEIRA follows a probabilistic relational paradigm and treats foundation models as stateless functions with relational inputs and outputs. It supports neuro-symbolic applications by enabling the seamless combination of such models with logic programs, as well as complex, multi-modal applications by streamlining the composition of diverse sub-models. We implement VIEIRA by extending the SCALLOP compiler with a foreign interface that supports foundation models as plugins. We implement plugins for 12 foundation models including GPT, CLIP, and SAM. We evaluate VIEIRA on 9 challenging tasks that span language, vision, and structured and vector databases. Our evaluation shows that programs in VIEIRA are concise, can incorporate modern foundation models, and have comparable or better accuracy than competitive baselines.",
        "journal": "AAAI",
        "labels": [
            "PL design for LLMs"
        ],
        "venue": "AAAI2024"
    },
    "Towards Greener Yet Powerful Code Generation via Quantization: An Empirical Study": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616302",
        "author": "Wei, Xiaokai and Gonugondla, Sujan Kumar and Wang, Shiqi and Ahmad, Wasi and Ray, Baishakhi and Qian, Haifeng and Li, Xiaopeng and Kumar, Varun and Wang, Zijian and Tian, Yuchen and Sun, Qing and Athiwaratkun, Ben and Shang, Mingyue and Ramanathan, Murali Krishna and Bhatia, Parminder and Xiang, Bing",
        "title": "Towards Greener Yet Powerful Code Generation via Quantization: An Empirical Study",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616302",
        "doi": "10.1145/3611643.3616302",
        "abstract": "ML-powered code generation aims to assist developers to write code in a more productive manner by intelligently generating code blocks based on natural language prompts. Recently, large pretrained deep learning models have pushed the boundary of code generation and achieved impressive performance. However, the huge number of model parameters poses a significant challenge to their adoption in a typical software development environment, where a developer might use a standard laptop or mid-size server to develop code. Such large models cost significant resources in terms of memory, latency, dollars, as well as carbon footprint. Model compression is a promising approach to address these challenges. We have identified quantization as one of the most promising compression techniques for code-generation as it avoids expensive retraining costs. As quantization represents model parameters with lower-bit integer (e.g., int8), the model size and runtime latency would both benefit. We empirically evaluate quantized models on code generation tasks across different dimensions: (i) resource usage and carbon footprint, (ii) accuracy, and (iii) robustness. Through systematic experiments we find a code-aware quantization recipe that could run even a 6-billion-parameter model in a regular laptop without significant accuracy or robustness degradation. We find that the recipe is readily applicable to code summarization task as well.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "224\u2013236",
        "numpages": "13",
        "labels": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Self-Supervised Query Reformulation for Code Search": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616306",
        "author": "Mao, Yuetian and Wan, Chengcheng and Jiang, Yuze and Gu, Xiaodong",
        "title": "Self-Supervised Query Reformulation for Code Search",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616306",
        "doi": "10.1145/3611643.3616306",
        "abstract": "Automatic query reformulation is a widely utilized technology for enriching user requirements and enhancing the outcomes of code search. It can be conceptualized as a machine translation task, wherein the objective is to rephrase a given query into a more comprehensive alternative. While showing promising results, training such a model typically requires a large parallel corpus of query pairs (i.e., the original query and a reformulated query) that are confidential and unpublished by online code search engines. This restricts its practicality in software development processes. In this paper, we propose SSQR, a self-supervised query reformulation method that does not rely on any parallel query corpus. Inspired by pre-trained models, SSQR treats query reformulation as a masked language modeling task conducted on an extensive unannotated corpus of queries. SSQR extends T5 (a sequence-to-sequence model based on Transformer) with a new pre-training objective named corrupted query completion (CQC), which randomly masks words within a complete query and trains T5 to predict the masked content. Subsequently, for a given query to be reformulated, SSQR identifies potential locations for expansion and leverages the pre-trained T5 model to generate appropriate content to fill these gaps. The selection of expansions is then based on the information gain associated with each candidate. Evaluation results demonstrate that SSQR outperforms unsupervised baselines significantly and achieves competitive performance compared to supervised methods.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "363\u2013374",
        "numpages": "12",
        "labels": [
            "static analysis",
            "code search"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Log Parsing with Generalization Ability under New Log Types": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616355",
        "author": "Yu, Siyu and Wu, Yifan and Li, Zhijing and He, Pinjia and Chen, Ningjiang and Liu, Changjian",
        "title": "Log Parsing with Generalization Ability under New Log Types",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616355",
        "doi": "10.1145/3611643.3616355",
        "abstract": "Log parsing, which converts semi-structured logs into structured logs, is the first step for automated log analysis.  Existing parsers are still unsatisfactory in real-world systems due to new log types in new-coming logs.  In practice, available logs collected during system runtime often do not contain all the possible log types of a system because log types related to infrequently activated system states are unlikely to be recorded and new log types are frequently introduced with system updates.  Meanwhile, most existing parsers require preprocessing to extract variables in advance, but preprocessing is based on the operator\u2019s prior knowledge of available logs and therefore may not work well on new log types.  In addition, parser parameters set based on available logs are difficult to generalize to new log types.  To support new log types, we propose a variable generation imitation strategy to craft a novel log parsing approach with generalization ability, called Log3T. Log3T employs a pre-trained transformer encoder-based model to extract log templates and can update parameters at parsing time to adapt to new log types by a modified test-time training.  Experimental results on 16 benchmark datasets show that Log3T outperforms the state-of-the-art parsers in terms of parsing accuracy. In addition, Log3T can automatically adapt to new log types in new-coming logs.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "425\u2013437",
        "numpages": "13",
        "labels": [
            "software maintenance and deployment",
            "system log analysis"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "An Extensive Study on Adversarial Attack against Pre-trained Models of Code": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616356",
        "author": "Du, Xiaohu and Wen, Ming and Wei, Zichao and Wang, Shangwen and Jin, Hai",
        "title": "An Extensive Study on Adversarial Attack against Pre-trained Models of Code",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616356",
        "doi": "10.1145/3611643.3616356",
        "abstract": "Transformer-based pre-trained models of code (PTMC) have been widely utilized and have achieved state-of-the-art performance in many mission-critical applications. However, they can be vulnerable to adversarial attacks through identifier substitution or coding style transformation, which can significantly degrade accuracy and may further incur security concerns. Although several approaches have been proposed to generate adversarial examples for PTMC, the effectiveness and efficiency of such approaches, especially on different code intelligence tasks, has not been well understood. To bridge this gap, this study systematically analyzes five state-of-the-art adversarial attack approaches from three perspectives: effectiveness, efficiency, and the quality of generated examples. The results show that none of the five approaches balances all these perspectives. Particularly, approaches with a high attack success rate tend to be time-consuming; the adversarial code they generate often lack naturalness, and vice versa. To address this limitation, we explore the impact of perturbing identifiers under different contexts and find that identifier substitution within for and if statements is the most effective. Based on these findings, we propose a new approach that prioritizes different types of statements for various tasks and further utilizes beam search to generate adversarial examples. Evaluation results show that it outperforms the state-of-the-art ALERT in terms of both effectiveness and efficiency while preserving the naturalness of the generated adversarial examples.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "489\u2013501",
        "numpages": "13",
        "labels": [
            "code model",
            "code model security",
            "empirical study"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616304",
        "author": "Grishina, Anastasiia and Hort, Max and Moonen, Leon",
        "title": "The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616304",
        "doi": "10.1145/3611643.3616304",
        "abstract": "The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection accuracy on Devign with only 3 out of 12 layers of CodeBERT and a 3.3x speed-up of fine-tuning. These findings show that early layers can be used to obtain better results using the same resources, as well as to reduce resource usage during fine-tuning and inference.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "895\u2013907",
        "numpages": "13",
        "labels": [
            "static analysis",
            "bug detection",
            "code model",
            "code model training",
            "source code model",
            "empirical study"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Evaluating Transfer Learning for Simplifying GitHub READMEs": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616291",
        "author": "Gao, Haoyu and Treude, Christoph and Zahedi, Mansooreh",
        "title": "Evaluating Transfer Learning for Simplifying GitHub READMEs",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616291",
        "doi": "10.1145/3611643.3616291",
        "abstract": "Software documentation captures detailed knowledge about a software product, e.g., code, technologies, and design. It plays an important role in the coordination of development teams and in conveying ideas to various stakeholders. However, software documentation can be hard to comprehend if it is written with jargon and complicated sentence structure. In this study, we explored the potential of text simplification techniques in the domain of software engineering to automatically simplify GitHub README files. We collected software-related pairs of GitHub README files consisting of 14,588 entries, aligned difficult sentences with their simplified counterparts, and trained a Transformer-based model to automatically simplify difficult versions. To mitigate the sparse and noisy nature of the software-related simplification dataset, we applied general text simplification knowledge to this field. Since many general-domain difficult-to-simple Wikipedia document pairs are already publicly available, we explored the potential of transfer learning by first training the model on the Wikipedia data and then fine-tuning it on the README data. Using automated BLEU scores and human evaluation, we compared the performance of different transfer learning schemes and the baseline models without transfer learning. The transfer learning model using the best checkpoint trained on a general topic corpus achieved the best performance of 34.68 BLEU score and statistically significantly higher human annotation scores compared to the rest of the schemes and baselines. We conclude that using transfer learning is a promising direction to circumvent the lack of data and drift style problem in software README files simplification and achieved a better trade-off between simplification and preservation of meaning.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1548\u20131560",
        "numpages": "13",
        "labels": [
            "software maintenance and deployment",
            "documentation generation"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "ReCode: Robustness Evaluation of Code Generation Models": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-recode",
        "author": "Wang, Shiqi and Li, Zheng and Qian, Haifeng and Yang, Chenghao and Wang, Zijian and Shang, Mingyue and Kumar, Varun and Tan, Samson and Ray, Baishakhi and Bhatia, Parminder and Nallapati, Ramesh and Ramanathan, Murali Krishna and Roth, Dan and Xiang, Bing",
        "booktitle": "ACL2023",
        "title": "ReCode: Robustness Evaluation of Code Generation Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model\u2019s robustness performance. With human annotators, we verified that over 90% of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval.",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.773",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023",
        "labels": [
            "code model",
            "code model training",
            "source code model",
            "code model",
            "code model robustness"
        ]
    },
    "Hierarchical Repository-Level Code Summarization for Business Applications Using Local LLMs": {
        "type": "INPROCEEDINGS",
        "key": "nilesh2025",
        "author": "Nilesh Dhulshette, Sapan Shah, Vinay Kulkarni",
        "title": "Hierarchical Repository-Level Code Summarization for Business Applications Using Local LLMs",
        "url": "https://arxiv.org/pdf/2501.07857",
        "abstract": "In large-scale software development, understanding the functionality and intent behind complex codebases is critical for effective development and maintenance. While code summarization has been widely studied, existing methods primarily focus on smaller code units, such as functions, and struggle with larger code artifacts like files and packages. Additionally, current summarization models tend to emphasize low-level implementation details, often overlooking the domain and business context that are crucial for real-world applications. This paper proposes a two-step hierarchical approach for repository-level code summarization, tailored to business applications. First, smaller code units such as functions and variables are identified using syntax analysis and summarized with local LLMs. These summaries are then aggregated to generate higher-level file and package summaries. To ensure the summaries are grounded in business context, we design custom prompts that capture the intended purpose of code artifacts based on the domain and problem context of the business application. We evaluate our approach on a business support system (BSS) for the telecommunications domain, showing that syntax analysis-based hierarchical summarization improves coverage, while business-context grounding enhances the relevance of the generated summaries.",
        "labels": [
            "static analysis",
            "code summarization",
            "agent design",
            "prompt strategy",
            "retrieval-augmented generation"
        ],
        "venue": "arXiv2025"
    },
    "Utilizing Precise and Complete Code Context to Guide LLM in Automatic False Positive Mitigation": {
        "type": "INPROCEEDINGS",
        "key": "jinbao2024",
        "author": "Jinbao Chen, Hongjing Xiang, Luhao Li, Yu Zhang, Boyao Ding, Qingwei Li",
        "title": "Utilizing Precise and Complete Code Context to Guide LLM in Automatic False Positive Mitigation",
        "url": "https://arxiv.org/pdf/2411.03079",
        "abstract": "Static Application Security Testing(SAST) tools are crucial for early bug detection and code quality but often generate false positives that slow development. Automating false positive mitigation is thus essential for advancing SAST tools. Past efforts use static/dynamic analysis or machine learning. The advent of Large Language Models, adept at understanding natural language and code, offers promising ways to improve the accuracy and usability of SAST tools. However, existing LLM-based methods need improvement in two key areas: first, extracted code snippets related to warnings are often cluttered with irrelevant control and data flows, reducing precision; second, critical code contexts are often missing, leading to incomplete representations that can mislead LLMs and cause inaccurate assessments. To ensure the use of precise and complete code context, thereby avoiding misguidance and enabling LLMs to reach accurate conclusions, we propose LLM4FPM. One of its core components is eCPG-Slicer, which builds an extended code property graph and extracts line-level, precise code context. Moreover, LLM4FPM incorporates FARF algorithm, which builds a file reference graph and then efficiently detects all files related to a warning in linear time, enabling eCPG-Slicer to gather complete code context across these files. We evaluate LLM4FPM on Juliet dataset, where it comprehensively outperforms the baseline, achieving an F1 score above 99% across various CWEs. LLM4FPM leverages a free, open-source model, avoiding costly alternatives and reducing inspection costs by up to $2758 per run on Juliet, with an average inspection time of 4.7 seconds per warning. Our work emphasizes the critical impact of precise and complete code context and highlights the potential of combining program analysis with LLMs, improving the quality and efficiency of software development.",
        "labels": [
          "static analysis",
          "bug detection"
        ],
        "venue": "arXiv2024"
    },
    "Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications": {
        "type": "INPROCEEDINGS",
        "key": "hermes2024",
        "author": "Abdullah Al Ishtiaq, Sarkar Snigdha Sarathi Das, Syed Md Mukit Rashid, Ali Ranjbar, Kai Tu, Tianwei Wu, Zhezheng Song, Weixuan Wang, Mujtahid Akon, Rui Zhang, Syed Rafiul Hussain",
        "title": "Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications",
        "url": "https://arxiv.org/abs/2310.04381",
        "abstract": "In this paper, we present Hermes, an end-to-end framework to automatically generate formal representations from natural language cellular specifications. We first develop a neural constituency parser, NEUTREX, to process transition-relevant texts and extract transition components (i.e., states, conditions, and actions). We also design a domain-specific language to translate these transition components to logical formulas by leveraging dependency parse trees. Finally, we compile these logical formulas to generate transitions and create the formal model as finite state machines. To demonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and 5G RRC specifications and obtain an overall accuracy of 81-87%, which is a substantial improvement over the state-of-the-art. Our security analysis of the extracted models uncovers 3 new vulnerabilities and identifies 19 previous attacks in 4G and 5G specifications, and 7 deviations in commercial 4G basebands.",
        "labels": [
          "static analysis",
          "bug detection",
          "specification inference"
        ],
        "venue": "USENIXSec2024"
    },
    "CellularLint: A Systematic Approach to Identify Inconsistent Behavior in Cellular Network Specifications": {
        "type": "INPROCEEDINGS",
        "key": "CellularLint2024",
        "author": "Mirza Masfiqur Rahman, Imtiaz Karim, and Elisa Bertino",
        "title": "CellularLint: A Systematic Approach to Identify Inconsistent Behavior in Cellular Network Specifications",
        "url": "https://www.usenix.org/system/files/usenixsecurity24-rahman.pdf",
        "abstract": "In recent years, there has been a growing focus on scrutinizing the security of cellular networks, often attributing security vulnerabilities to issues in the underlying protocol design descriptions. These protocol design specifications, typically extensive documents that are thousands of pages long, can harbor inaccuracies, underspecifications, implicit assumptions, and internal inconsistencies. In light of the evolving landscape, we introduce CellularLint—a semi-automatic framework for inconsistency detection within the standards of 4G and 5G, capitalizing on a suite of natural language processing techniques. Our proposed method uses a revamped few-shot learning mechanism on domain-adapted large language models. Pre-trained on a vast corpus of cellular network protocols, this method enables CellularLint to simultaneously detect inconsistencies at various levels of semantics and practical use cases. In doing so, CellularLint significantly advances the automated analysis of protocol specifications in a scalable fashion. In our investigation, we focused on the Non-Access Stratum (NAS) and the security specifications of 4G and 5G networks, ultimately uncovering 157 inconsistencies with 82.67% accuracy. After verification of these inconsistencies on open-source implementations and 17 commercial devices, we confirm that they indeed have a substantial impact on design decisions, potentially leading to concerns related to privacy, integrity, availability, and interoperability.",
        "labels": [
          "static analysis",
          "bug detection",
          "specification inference"
        ],
        "venue": "USENIXSec2024"
    },
    "C2SaferRust: Transforming C Projects into Safer Rust with NeuroSymbolic Techniques": {
        "author": "Vikram Nitin, Rahul Krishna, Luiz Lemos do Valle, Baishakhi Ray",
        "title": "C2SaferRust: Transforming C Projects into Safer Rust with NeuroSymbolic Techniques",
        "url": "https://arxiv.org/pdf/2501.14257",
        "abstract": "In recent years, there has been a lot of interest in converting C code to Rust, to benefit from the memory and thread safety guarantees of Rust. C2Rust is a rule-based system that can automatically convert C code to functionally identical Rust, but the Rust code that it produces is non-idiomatic, i.e., makes extensive use of unsafe Rust, a subset of the language that doesn't have memory or thread safety guarantees. At the other end of the spectrum are LLMs, which produce idiomatic Rust code, but these have the potential to make mistakes and are constrained in the length of code they can process. In this paper, we present C2SaferRust, a novel approach to translate C to Rust that combines the strengths of C2Rust and LLMs. We first use C2Rust to convert C code to non-idiomatic, unsafe Rust. We then decompose the unsafe Rust code into slices that can be individually translated to safer Rust by an LLM. After processing each slice, we run end-to-end test cases to verify that the code still functions as expected. We also contribute a benchmark of 7 real-world programs, translated from C to unsafe Rust using C2Rust. Each of these programs also comes with end-to-end test cases. On this benchmark, we are able to reduce the number of raw pointers by up to 38%, and reduce the amount of unsafe code by up to 28%, indicating an increase in safety. The resulting programs still pass all test cases. C2SaferRust also shows convincing gains in performance against two previous techniques for making Rust code safer.",
        "labels": [
            "code generation",
            "program transformation"
        ],
        "venue": "arXiv2025"
    },
    "ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model": {
        "type": "inproceedings",
        "key": "10.1145/3658644.3690231",
        "venue": "CCS2024",
        "author": "Wang, Dawei and Zhou, Geng and Chen, Li and Li, Dan and Miao, Yukai",
        "title": "ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model",
        "year": "2024",
        "isbn": "9798400706363",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3658644.3690231",
        "doi": "10.1145/3658644.3690231",
        "abstract": "Vulnerabilities related to option combinations pose a significant challenge in software security testing due to their vast search space. Previous research primarily addressed this challenge through mutation or filtering techniques, which inefficiently treated all option combinations as having equal potential for vulnerabilities, thus wasting considerable time on non-vulnerable targets and resulting in low testing efficiency. In this paper, we utilize carefully designed prompt engineering to drive the large language model (LLM) to predict high-risk option combinations (i.e., more likely to contain vulnerabilities) and perform fuzz testing automatically without human intervention. We developed a tool called ProphetFuzz and evaluated it on a dataset comprising 52 programs collected from three related studies. The entire experiment consumed 10.44 CPU years. ProphetFuzz successfully predicted 1748 high-risk option combinations at an average cost of only 8.69 per program. Results show that after 72 hours of fuzzing, ProphetFuzz discovered 364 unique vulnerabilities associated with 12.30\\% of the predicted high-risk option combinations, which was 32.85\\% higher than that found by state-of-the-art in the same timeframe. Additionally, using ProphetFuzz, we conducted persistent fuzzing on the latest versions of these programs, uncovering 140 vulnerabilities, with 93 confirmed by developers and 21 awarded CVE numbers.",
        "booktitle": "Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "735\u2013749",
        "numpages": "15",
        "keywords": "fuzzing, large language model, option-aware, vulnerability",
        "location": "Salt Lake City, UT, USA",
        "series": "CCS '24",
        "labels": [
            "program testing",
            "fuzzing"
        ]
    },
    "Demystifying RCE Vulnerabilities in LLM-Integrated Apps": {
        "type": "inproceedings",
        "key": "10.1145/3658644.3690338",
        "venue": "CCS2024",
        "author": "Liu, Tong and Deng, Zizhuang and Meng, Guozhu and Li, Yuekang and Chen, Kai",
        "title": "Demystifying RCE Vulnerabilities in LLM-Integrated Apps",
        "year": "2024",
        "isbn": "9798400706363",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3658644.3690338",
        "doi": "10.1145/3658644.3690338",
        "abstract": "Large Language Models (LLMs) show promise in transforming software development, with a growing interest in integrating them into more intelligent apps. Frameworks like LangChain aid LLM-integrated app development, offering code execution utility/APIs for custom actions. However, these capabilities theoretically introduce Remote Code Execution (RCE) vulnerabilities, enabling remote code execution through prompt injections. No prior research systematically investigates these frameworks' RCE vulnerabilities or their impact on applications and exploitation consequences. Therefore, there is a huge research gap in this field.In this study, we propose LLMSmith to detect, validate and exploit the RCE vulnerabilities in LLM-integrated frameworks and apps. To achieve this goal, we develop two novel techniques, including 1) a lightweight static analysis to construct call chains to identify RCE vulnerabilities in frameworks; 2) a systematical prompt-based exploitation method to verify and exploit the found vulnerabilities in LLM-integrated apps. This technique involves various strategies to control LLM outputs, trigger RCE vulnerabilities and launch subsequent attacks. Our research has uncovered a total of 20 vulnerabilities in 11 LLM-integrated frameworks, comprising 19 RCE vulnerabilities and 1 arbitrary file read/write vulnerability. Of these, 17 have been confirmed by the framework developers, with 13 vulnerabilities being assigned CVE IDs, 6 of which have a CVSS score of 9.8, and we were also awarded a bug bounty of 1350. For the 51 apps potentially affected by RCE, we successfully executed attacks on 17 apps, 16 of which are vulnerable to RCE and 1 to SQL injection. Furthermore, we conduct a comprehensive analysis of these vulnerabilities and construct practical attacks to demonstrate the hazards in reality, e.g., app output hijacking, user data leakage, even the potential to take full control of systems. Last, we propose several mitigation measures for both framework and app developers to counteract such attacks.",
        "booktitle": "Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "1716\u20131730",
        "numpages": "15",
        "keywords": "LLM-integrated applications, RCE, large language model",
        "location": "Salt Lake City, UT, USA",
        "series": "CCS '24",
        "labels": [
            "code model",
            "code model security"
        ]
    },
    "PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)": {
        "type": "inproceedings",
        "key": "10.1145/3658644.3690298",
        "venue": "CCS2024",
        "author": "Nazzal, Mahmoud and Khalil, Issa and Khreishah, Abdallah and Phan, NhatHai",
        "title": "PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)",
        "year": "2024",
        "isbn": "9798400706363",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3658644.3690298",
        "doi": "10.1145/3658644.3690298",
        "abstract": "The capability of generating high-quality source code using large language models (LLMs) reduces software development time and costs. However, recent literature and our empirical investigation in this work show that while LLMs can generate functioning code, they inherently tend to introduce security vulnerabilities, limiting their potential. This problem is mainly due to their training on massive open-source corpora exhibiting insecure and inefficient programming practices. Therefore, automatic optimization of LLM prompts for generating secure and functioning code is a demanding need. This paper introduces PromSec, an algorithm for <u>prom</u>pt optimization for <u>sec</u>ure and functioning code generation using LLMs. In PromSec, we combine 1) code vulnerability clearing using a generative adversarial graph neural network, dubbed as gGAN, to fix and reduce security vulnerabilities in generated codes and 2) code generation using an LLM into an interactive loop, such that the outcome of the gGAN drives the LLM with enhanced prompts to generate secure codes while preserving their functionality. Introducing a new contrastive learning approach in gGAN, we formulate the code-clearing and generation loop as a dual-objective optimization problem, enabling PromSec to notably reduce the number of LLM inferences. As a result, PromSec becomes a cost-effective and practical solution for generating secure and functioning codes.Extensive experiments conducted on Python and Java code datasets confirm that PromSec effectively enhances code security while upholding its intended functionality. Our experiments show that despite the comprehensive application of a state-of-the-art approach, it falls short in addressing all vulnerabilities within the code, whereas PromSec effectively resolves each of them. Moreover, PromSec achieves more than an order-of-magnitude reduction in operational time, number of LLM queries, and security analysis costs. Furthermore, prompts optimized with PromSec for a certain LLM are transferable to other LLMs across programming languages and generalizable to unseen vulnerabilities in training. This study presents an essential step towards improving the trustworthiness of LLMs for secure and functioning code generation, significantly enhancing their large-scale integration in real-world software code development practices.",
        "booktitle": "Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "2266\u20132280",
        "numpages": "15",
        "keywords": "LLMs, code generation, graph generative adversarial networks, secure and functioning codes",
        "location": "Salt Lake City, UT, USA",
        "series": "CCS '24",
        "labels": [
            "general coding task",
            "code generation",
            "code model",
            "code model security"
        ]
    },
    "An Exploration of Large Language Models in Malicious Source Code Detection": {
        "type": "inproceedings",
        "key": "10.1145/3658644.3691374",
        "venue": "CCS2024",
        "author": "Xue, Di and Zhao, Gang and Fan, Zhongqi and Li, Wei and Xu, Yahong and Liu, Zhen and Liu, Yin and Yuan, Zhongliang",
        "title": "Poster: An Exploration of Large Language Models in Malicious Source Code Detection",
        "year": "2024",
        "isbn": "9798400706363",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3658644.3691374",
        "doi": "10.1145/3658644.3691374",
        "abstract": "Embedding malicious code within the software supply chain has become a significant concern in the information technology field. Current methods for detecting malicious code, based on signatures, behavior analysis, and traditional machine learning models, lack result interpretability. This study proposes a novel malicious code detection framework, Mal-LLM, which leverages the cost advantages of traditional machine learning models and the interpretability of LLMs. Initially, traditional machine learning models filter vast amounts of malicious source code in the software supply chain. Subsequently, LLMs analyze and interpret the filtered malicious source code using a customized prompt template incorporating role-playing and chain-of-thought techniques. The feasibility of the Mal-LLM framework is validated through extensive experimental analyses, examining the ambiguity and redundancy of the LLM in the framework, the significance of ''experience'' and ''malicious'' prompts, and exploring methods to reduce the cost of using LLMs from an enterprise perspective.",
        "booktitle": "Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "4940\u20134942",
        "numpages": "3",
        "keywords": "llms, malicious code detection, prompt engineering, software supply chain",
        "location": "Salt Lake City, UT, USA",
        "series": "CCS '24",
        "labels": [
            "static analysis",
            "bug detection"
        ]
    },
    "Repairing Bugs with the Introduction of New Variables: A Multi-Agent Large Language Model": {
        "type": "inproceedings",
        "key": "10.1145/3658644.3691412",
        "venue": "CCS2024",
        "author": "Zhang, Elisa and Sun, Shiyu and Xing, Yunlong and Sun, Kun",
        "title": "Poster: Repairing Bugs with the Introduction of New Variables: A Multi-Agent Large Language Model",
        "year": "2024",
        "isbn": "9798400706363",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3658644.3691412",
        "doi": "10.1145/3658644.3691412",
        "abstract": "Trained on billions of tokens, large language models (LLMs) have a broad range of empirical knowledge which enables them to generate software patches with complex repair patterns. We leverage the powerful code-fixing capabilities of LLMs and propose VarPatch, a multi-agent conversational automated program repair (APR) technique that iteratively queries the LLM to generate software patches by providing various prompts and context information. VarPatch focuses on the variable addition repair pattern, as previous APR tools struggle to introduce and use new variables to fix buggy code. Additionally, we summarize commonly used APIs and identify four repair patterns involving new variable addition. Our evaluation on the Defects4J 1.2 dataset shows that VarPatch can repair 69\\% more bugs than baseline tools and over 8 times more bugs than GPT-4.",
        "booktitle": "Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "4961\u20134963",
        "numpages": "3",
        "keywords": "automated program repair, large language model, multiple agents",
        "location": "Salt Lake City, UT, USA",
        "series": "CCS '24",
        "labels": [
            "code generation",
            "program repair"
        ]
    },
    "TAPChecker: Model Checking in Trigger-Action Rules Generation Using Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3658644.3691416",
        "venue": "CCS2024",
        "author": "Bui, Huan and Lienerth, Harper and Fu, Chenglong and Sridhar, Meera",
        "title": "Poster: TAPChecker: Model Checking in Trigger-Action Rules Generation Using Large Language Models",
        "year": "2024",
        "isbn": "9798400706363",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3658644.3691416",
        "doi": "10.1145/3658644.3691416",
        "abstract": "The integration of large language models (LLMs) in smart home systems holds significant promise for automating the generation of Trigger-Action Programming (TAP) rules, potentially streamlining smart home user experiences and enhancing convenience. However, LLMs lack of holistic view of smart home IoT deployments and may introduce TAP rules that result in hazards. This paper explores the application of LLM for generating TAP rules and applying formal verification to validate and ensure the safety of TAP rules generated by LLMs. By systematically analyzing and verifying these rules, we aim to identify and mitigate potential security vulnerabilities. Furthermore, we propose a feedback mechanism to refine the LLM's output, enhancing its reliability and safety in generating automation rules. Through this approach, we seek to bridge the gap between the efficiency of LLMs and the stringent security requirements of smart IoT systems, fostering a safer automation environment.",
        "booktitle": "Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "4994\u20134996",
        "numpages": "3",
        "keywords": "large language model, model checking, trigger-action programming",
        "location": "Salt Lake City, UT, USA",
        "series": "CCS '24",
        "labels": [
            "static analysis",
            "specification inference"
        ]
    },
    "Enhance Hardware Domain Specific Large Language Model with Reinforcement Learning for Resilience": {
        "type": "inproceedings",
        "key": "10.1145/3658644.3691384",
        "venue": "CCS2024",
        "author": "Fu, Weimin and Zhao, Yifang and Jin, Yier and Guo, Xiaolong",
        "title": "Poster: Enhance Hardware Domain Specific Large Language Model with Reinforcement Learning for Resilience",
        "year": "2024",
        "isbn": "9798400706363",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3658644.3691384",
        "doi": "10.1145/3658644.3691384",
        "abstract": "To enhance the performance of large language models (LLMs) on hardware design tasks, we focus on training with reinforcement learning(RL) to improve LLMs' syntax synthesis and functional verification performance. We observed significant gains in power, performance, and area (PPA) metrics by applying RL. Specifically, DeepSeek Code saw a 23.6\\% performance increase, while the RTLCoder improved by 7.86\\%. Our findings demonstrate the effectiveness of RL in refining LLMs for more accurate hardware generation, considering power and area consumption. This approach offers a promising direction for generating hardware resilient to side-channel attacks in computer systems.",
        "booktitle": "Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "5060\u20135062",
        "numpages": "3",
        "keywords": "eda tools, hardware security, large language model",
        "location": "Salt Lake City, UT, USA",
        "series": "CCS '24",
        "labels": [
            "code generation",
            "program synthesis"
        ]
    },
    "SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code": {
        "type": "inproceedings",
        "key": "10.1145/3658644.3691367",
        "venue": "CCS2024",
        "author": "Ton, Khiem and Nguyen, Nhi and Nazzal, Mahmoud and Khreishah, Abdallah and Borcea, Cristian and Phan, NhatHai and Jin, Ruoming and Khalil, Issa and Shen, Yelong",
        "title": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code",
        "year": "2024",
        "isbn": "9798400706363",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3658644.3691367",
        "doi": "10.1145/3658644.3691367",
        "abstract": "This paper introduces SGCode, a flexible prompt-optimizing system to generate secure code with large language models (LLMs). SGCode integrates recent prompt-optimization approaches with LLMs in a unified system accessible through front-end and back-end APIs, enabling users to 1) generate secure code, which is free of vulnerabilities, 2) review and share security analysis, and 3) easily switch from one prompt optimization approach to another, while providing insights on model and system performance. We populated SGCode on an AWS server with PromSec, an approach that optimizes prompts by combining an LLM and security tools with a lightweight generative adversarial graph neural network to detect and fix security vulnerabilities in the generated code. Extensive experiments show that SGCode is practical as a public tool to gain insights into the trade-offs between model utility, secure code generation, and system cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is available at: http://3.131.141.63:8501/.",
        "booktitle": "Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "5078\u20135080",
        "numpages": "3",
        "keywords": "demonstration system, llms, prompt optimization, secure code",
        "location": "Salt Lake City, UT, USA",
        "series": "CCS '24",
        "labels": [
            "code generation",
            "program synthesis",
            "agent design",
            "prompt strategy",
            "code model",
            "code model security"
        ]
    },
    "Generating API Parameter Security Rules with LLM for API Misuse Detection": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Jinghua Liu and Chinese Academy of Sciences and Beijing and China; School of Cyber Security and University of Chinese Academy of Sciences and China) and Yi Yang and Chinese Academy of Sciences and Beijing and China; School of Cyber Security and University of Chinese Academy of Sciences and China) and Kai Chen and Chinese Academy of Sciences and Beijing and China; School of Cyber Security and University of Chinese Academy of Sciences and China) and Miaoqian Lin and Chinese Academy of Sciences and Beijing and China; School of Cyber Security and University of Chinese Academy of Sciences and China)",
        "booktitle": "NDSS2025",
        "title": "Generating API Parameter Security Rules with LLM for API Misuse Detection",
        "year": "2025",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "When utilizing library APIs, developers should follow the API security rules to mitigate the risk of API misuse. API Parameter Security Rule (APSR) is a common type of security rule that specifies how API parameters should be safely used and places constraints on their values. Failure to comply with the APSRs can lead to severe security issues, including null pointer dereference and memory corruption. Manually analyzing numerous APIs and their parameters to construct APSRs is labor-intensive and needs to be automated. Existing studies generate APSRs from documentation and code, but the missing information and limited analysis heuristics result in missing APSRs. Due to the superior Large Language Model\u2019s (LLM) capability in code analysis and text generation without predefined heuristics, we attempt to utilize it to address the challenge encountered in API misuse detection. However, directly utilizing LLMs leads to incorrect APSRs which may lead to false bugs in detection, and overly general APSRs that could not generate applicable detection code resulting in many security bugs undiscovered.In this paper, we present a new framework, named GPTAid, for automatic APSRs generation by analyzing API source code with LLM and detecting API misuse caused by incorrect parameter use. To validate the correctness of the LLM-generated APSRs, we propose an execution feedback-checking approach based on the observation that security-critical API misuse is often caused by APSRs violations, and most of them result in runtime errors. Specifically, GPTAid first uses LLM to generate raw APSRs and the Right calling code, and then generates Violation code for each raw APSR by modifying the Right calling code using LLM. Subsequently, GPTAid performs dynamic execution on each piece of Violation code and further filters out the incorrect APSRs based on runtime errors. To further generate concrete APSRs, GPTAid employs a code differential analysis to refine the filtered ones. Particularly, as the programming language is more precise than natural language, GPTAid identifies the key operations within Violation code by differential analysis, and then generates the corresponding concrete APSR based on the aforementioned operations. These concrete APSRs could be precisely interpreted into applicable detection code, which proven to be effective in API misuse detection. Implementing on the dataset containing 200 randomly selected APIs from eight popular libraries, GPTAid achieves a precision of 92.3%. Moreover, it generates 6 times more APSRs than state-of-the-art detectors on a comparison dataset of previously reported bugs and APSRs. We further evaluated GPTAid on 47 applications, 210 unknown security bugs were found potentially resulting in severe security issues (e.g., system crashes), 150 of which have been confirmed by developers after our reports.When utilizing library APIs, developers should follow the API security rules to mitigate the risk of API misuse. API Parameter Security Rule (APSR) is a common type of security rule that specifies how API parameters should be safely used and places constraints on their values. Failure to comply with the APSRs can lead to severe security issues, including null pointer dereference and memory corruption. Manually analyzing numerous APIs and their parameters to construct APSRs is labor-intensive and needs to be automated. Existing studies generate APSRs from documentation and code, but the missing information and limited analysis heuristics result in missing APSRs. Due to the superior Large Language Model\u2019s (LLM) capability in code analysis and text generation without predefined heuristics, we attempt to utilize it to address the challenge encountered in API misuse detection. However, directly utilizing LLMs leads to incorrect APSRs which may lead to false bugs in detection, and overly general APSRs that could not generate applicable detection code resulting in many security bugs undiscovered.In this paper, we present a new framework, named GPTAid, for automatic APSRs generation by analyzing API source code with LLM and detecting API misuse caused by incorrect parameter use. To validate the correctness of the LLM-generated APSRs, we propose an execution feedback-checking approach based on the observation that security-critical API misuse is often caused by APSRs violations, and most of them result in runtime errors. Specifically, GPTAid first uses LLM to generate raw APSRs and the Right calling code, and then generates Violation code for each raw APSR by modifying the Right calling code using LLM. Subsequently, GPTAid performs dynamic execution on each piece of Violation code and further filters out the incorrect APSRs based on runtime errors. To further generate concrete APSRs, GPTAid employs a code differential analysis to refine the filtered ones. Particularly, as the programming language is more precise than natural language, GPTAid identifies the key operations within Violation code by differential analysis, and then generates the corresponding concrete APSR based on the aforementioned operations. These concrete APSRs could be precisely interpreted into applicable detection code, which proven to be effective in API misuse detection. Implementing on the dataset containing 200 randomly selected APIs from eight popular libraries, GPTAid achieves a precision of 92.3%. Moreover, it generates 6 times more APSRs than state-of-the-art detectors on a comparison dataset of previously reported bugs and APSRs. We further evaluated GPTAid on 47 applications, 210 unknown security bugs were found potentially resulting in severe security issues (e.g., system crashes), 150 of which have been confirmed by developers after our reports.",
        "keywords": "",
        "url": "https://www.ndss-symposium.org/ndss-paper/generating-api-parameter-security-rules-with-llm-for-api-misuse-detection",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2025",
        "labels": [
            "static analysis",
            "bug detection",
            "specification inference"
        ]
    },
    "RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Jiangyi Deng and Xinfeng Li and Yanjiao Chen and Yijie Bai and Haiqin Weng and Yan Liu and Tao Wei and Wenyuan Xu",
        "booktitle": "NDSS2025",
        "title": "RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer",
        "year": "2025",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Malicious shell commands are linchpins to many cyber-attacks, but may not be easy to understand by security analysts due to complicated and often disguised code structures. Advances in large language models (LLMs) have unlocked the possibility of generating understandable explanations for shell commands. However, existing general-purpose LLMs suffer from a lack of expert knowledge and a tendency to hallucinate in the task of shell command explanation. In this paper, we present Raconteur, a knowledgeable, expressive and portable shell command explainer powered by LLM. Raconteur is infused with professional knowledge to provide comprehensive explanations on shell commands, including not only what the command does (i.e., behavior) but also why the command does it (i.e., purpose). To shed light on the high-level intent of the command, we also translate the natural-language-based explanation into standard technique & tactic defined by MITRE ATT&CK, the worldwide knowledge base of cybersecurity. To enable Raconteur to explain unseen private commands, we further develop a documentation retriever to obtain relevant information from complementary documentations to assist the explanation process. We have created a large-scale dataset for training and conducted extensive experiments to evaluate the capability of Raconteur in shell command explanation. The experiments verify that Raconteur is able to provide high-quality explanations and in-depth insight of the intent of the command.Malicious shell commands are linchpins to many cyber-attacks, but may not be easy to understand by security analysts due to complicated and often disguised code structures. Advances in large language models (LLMs) have unlocked the possibility of generating understandable explanations for shell commands. However, existing general-purpose LLMs suffer from a lack of expert knowledge and a tendency to hallucinate in the task of shell command explanation. In this paper, we present Raconteur, a knowledgeable, expressive and portable shell command explainer powered by LLM. Raconteur is infused with professional knowledge to provide comprehensive explanations on shell commands, including not only what the command does (i.e., behavior) but also why the command does it (i.e., purpose). To shed light on the high-level intent of the command, we also translate the natural-language-based explanation into standard technique & tactic defined by MITRE ATT&CK, the worldwide knowledge base of cybersecurity. To enable Raconteur to explain unseen private commands, we further develop a documentation retriever to obtain relevant information from complementary documentations to assist the explanation process. We have created a large-scale dataset for training and conducted extensive experiments to evaluate the capability of Raconteur in shell command explanation. The experiments verify that Raconteur is able to provide high-quality explanations and in-depth insight of the intent of the command.",
        "keywords": "",
        "url": "https://www.ndss-symposium.org/ndss-paper/raconteur-a-knowledgeable-insightful-and-portable-llm-powered-shell-command-explainer",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2025",
        "labels": [
            "static analysis",
            "code summarization",
            "agent design",
            "prompt strategy",
            "retrieval-augmented generation"
        ]
    },
    "The Midas Touch: Triggering the Capability of LLMs for RM-API Misuse Detection": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Yi Yang and Chinese Academy of Sciences and Beijing and China; School of Cyber Security and University of Chinese Academy of Sciences and China) and Jinghua Liu and Chinese Academy of Sciences and Beijing and China; School of Cyber Security and University of Chinese Academy of Sciences and China) and Kai Chen and Chinese Academy of Sciences and Beijing and China; School of Cyber Security and University of Chinese Academy of Sciences and China) and Miaoqian Lin and Chinese Academy of Sciences and Beijing and China; School of Cyber Security and University of Chinese Academy of Sciences and China)",
        "booktitle": "NDSS2025",
        "title": "The Midas Touch: Triggering the Capability of LLMs for RM-API Misuse Detection",
        "year": "2025",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "As the basis of software resource management (RM), strictly following the RM-API constraints guarantees secure resource management and software. To enhance the RM-API application, researchers find it effective in detecting RM-API misuse on open-source software according to RM-API constraints retrieved from documentation and code. However, the current pattern-matching constraint retrieval methods have limitations: the documentation-based methods leave many API constraints irregularly distributed or involving neutral sentiment undiscovered; the code-based methods result in many false bugs due to incorrect API usage since not all high-frequency usages are correct.Therefore, people propose to utilize Large Language Models (LLMs) for RM-API constraint retrieval with their potential on text analysis and generation. However, directly using LLMs has limitations due to the hallucinations. The LLMs fabricate answers without expertise leaving many RM APIs undiscovered and generating incorrect answers even with evidence introducing incorrect RM-API constraints and false bugs.In this paper, we propose an LLM-empowered RM-API misuse detection solution,ChatDetector, which fully automates LLMs for documentation understanding which helps RM-API constraints retrieval and RM-API misuse detection. To correctly retrieve the RM-API constraints,ChatDetectoris inspired by the ReAct framework which is optimized based on Chain-of-Thought (CoT) to decompose the complex task into allocation APIs identification, RM-object (allocated/released by RM APIs) extraction and RM-APIs pairing (RM APIs usually exist in pairs). It first verifies the semantics of allocation APIs based on the retrieved RM sentences from API documentation through LLMs.Inspired by the LLMs' performance on various prompting methods,ChatDetectoradopts a two-dimensional prompting approach for cross-validation. At the same time, an inconsistency-checking approach between the LLMs' output and the reasoning process is adopted for the allocation APIs confirmation with an off-the-shelf Natural Language Processing (NLP) tool. To accurately pair the RM-APIs,ChatDetectordecomposes the task again and identifies the RM-object type first, with which it can then accurately pair the releasing APIs and further construct the RM-API constraints for misuse detection. With the diminished hallucinations,ChatDetectoridentifies 165 pairs of RM-APIs with a precision of 98.21% compared with the state-of-the-art API detectors. By employing a static detector CodeQL, we ethically report 115 security bugs on the applications integrating on six popular libraries to the developers, which may result in severe issues, such as Denial-of-Services (DoS) and memory corruption. Compared with the end-to-end benchmark method, the result shows thatChatDetectorcan retrieve at least 47% more RM sentences and 80.85% more RM-API constraints. Since no work exists specified in utilizing LLMs for RM-API misuse detection to our best knowledge, the inspiring results show that LLMs can assist in generating more constraints beyond expertise and can be used for bug detection. It also indicates that future research could transfer from overcoming the bottlenecks of traditional NLP tools to creatively utilizing LLMs for security research.As the basis of software resource management (RM), strictly following the RM-API constraints guarantees secure resource management and software. To enhance the RM-API application, researchers find it effective in detecting RM-API misuse on open-source software according to RM-API constraints retrieved from documentation and code. However, the current pattern-matching constraint retrieval methods have limitations: the documentation-based methods leave many API constraints irregularly distributed or involving neutral sentiment undiscovered; the code-based methods result in many false bugs due to incorrect API usage since not all high-frequency usages are correct.Therefore, people propose to utilize Large Language Models (LLMs) for RM-API constraint retrieval with their potential on text analysis and generation. However, directly using LLMs has limitations due to the hallucinations. The LLMs fabricate answers without expertise leaving many RM APIs undiscovered and generating incorrect answers even with evidence introducing incorrect RM-API constraints and false bugs.In this paper, we propose an LLM-empowered RM-API misuse detection solution,ChatDetector, which fully automates LLMs for documentation understanding which helps RM-API constraints retrieval and RM-API misuse detection. To correctly retrieve the RM-API constraints,ChatDetectoris inspired by the ReAct framework which is optimized based on Chain-of-Thought (CoT) to decompose the complex task into allocation APIs identification, RM-object (allocated/released by RM APIs) extraction and RM-APIs pairing (RM APIs usually exist in pairs). It first verifies the semantics of allocation APIs based on the retrieved RM sentences from API documentation through LLMs.Inspired by the LLMs' performance on various prompting methods,ChatDetectoradopts a two-dimensional prompting approach for cross-validation. At the same time, an inconsistency-checking approach between the LLMs' output and the reasoning process is adopted for the allocation APIs confirmation with an off-the-shelf Natural Language Processing (NLP) tool. To accurately pair the RM-APIs,ChatDetectordecomposes the task again and identifies the RM-object type first, with which it can then accurately pair the releasing APIs and further construct the RM-API constraints for misuse detection. With the diminished hallucinations,ChatDetectoridentifies 165 pairs of RM-APIs with a precision of 98.21% compared with the state-of-the-art API detectors. By employing a static detector CodeQL, we ethically report 115 security bugs on the applications integrating on six popular libraries to the developers, which may result in severe issues, such as Denial-of-Services (DoS) and memory corruption. Compared with the end-to-end benchmark method, the result shows thatChatDetectorcan retrieve at least 47% more RM sentences and 80.85% more RM-API constraints. Since no work exists specified in utilizing LLMs for RM-API misuse detection to our best knowledge, the inspiring results show that LLMs can assist in generating more constraints beyond expertise and can be used for bug detection. It also indicates that future research could transfer from overcoming the bottlenecks of traditional NLP tools to creatively utilizing LLMs for security research.",
        "keywords": "",
        "url": "https://www.ndss-symposium.org/ndss-paper/the-midas-touch-triggering-the-capability-of-llms-for-rm-api-misuse-detection",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2025",
        "labels": [
            "static analysis",
            "bug detection",
            "specification inference"
        ]
    },
    "Beyond Classification: Inferring Function Names in Stripped Binaries via Domain Adapted LLMs": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Linxi Jiang and Xin Jin and Zhiqiang Lin",
        "booktitle": "NDSS2025",
        "title": "Beyond Classification: Inferring Function Names in Stripped Binaries via Domain Adapted LLMs",
        "year": "2025",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Function name inference in stripped binaries is an important yet challenging task for many security applications, such as malware analysis and vulnerability discovery, due to the need to grasp binary code semantics amidst diverse instruction sets, architectures, compiler optimizations, and obfuscations. While machine learning has made significant progress in this field, existing methods often struggle with unseen data, constrained by their reliance on a limited vocabulary-based classification approach. In this paper, we present SymGen, a novel framework employing an autoregressive generation paradigm powered by domain-adapted generative large language models (LLMs) for enhanced binary code interpretation. We have evaluated SymGen on a dataset comprising 2,237,915 binary functions across four architectures (x86-64, x86-32, ARM, MIPS) with four levels of optimizations (O0-O3) where it surpasses the state-of-the-art with up to 409.3%, 553.5%, and 489.4% advancement in precision, recall, and F1 score, respectively, showing superior effectiveness and generalizability. Our ablation and case studies also demonstrate the significant performance boosts achieved by our design, e.g., the domain adaptation approach, alongside showcasing SymGen\u2019s practicality in analyzing real-world binaries, e.g., obfuscated binaries and malware executables.Function name inference in stripped binaries is an important yet challenging task for many security applications, such as malware analysis and vulnerability discovery, due to the need to grasp binary code semantics amidst diverse instruction sets, architectures, compiler optimizations, and obfuscations. While machine learning has made significant progress in this field, existing methods often struggle with unseen data, constrained by their reliance on a limited vocabulary-based classification approach. In this paper, we present SymGen, a novel framework employing an autoregressive generation paradigm powered by domain-adapted generative large language models (LLMs) for enhanced binary code interpretation. We have evaluated SymGen on a dataset comprising 2,237,915 binary functions across four architectures (x86-64, x86-32, ARM, MIPS) with four levels of optimizations (O0-O3) where it surpasses the state-of-the-art with up to 409.3%, 553.5%, and 489.4% advancement in precision, recall, and F1 score, respectively, showing superior effectiveness and generalizability. Our ablation and case studies also demonstrate the significant performance boosts achieved by our design, e.g., the domain adaptation approach, alongside showcasing SymGen\u2019s practicality in analyzing real-world binaries, e.g., obfuscated binaries and malware executables.",
        "keywords": "",
        "url": "https://www.ndss-symposium.org/ndss-paper/beyond-classification-inferring-function-names-in-stripped-binaries-via-domain-adapted-llms",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2025",
        "labels": [
            "code model",
            "code model training",
            "binary code model",
            "static analysis",
            "program decompilation"
        ]
    },
    "EAGLEYE: Exposing Hidden Web Interfaces in IoT Devices via Routing Analysis": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Hangtian Liu and Lei Zheng and Tsinghua University) and Shuitao Gan and Chao Zhang and Tsinghua University) and Zicong Gao and Hongqi Zhang and Yishun Zeng and Tsinghua University) and Zhiyuan Jiang and Jiahai Yang and Tsinghua University)",
        "booktitle": "NDSS2025",
        "title": "EAGLEYE: Exposing Hidden Web Interfaces in IoT Devices via Routing Analysis",
        "year": "2025",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Hidden web interfaces, i.e., undisclosed access channels in IoT devices, introduce great security risks and have resulted in severe attacks in recent years. However, the definition of such threats is vague, and few solutions are able to discover them. Due to their hidden nature, traditional bug detection solutions (e.g., taint analysis, fuzzing) are hard to detect them. In this paper, we present a novel solution EAGLEYE to automatically expose hidden web interfaces in IoT devices. By analyzing input requests to public interfaces, we first identify routing tokens within the requests, i.e., those values (e.g., actions or file names) that are referenced and used as index by the firmware code (routing mechanism) to find associated handler functions. Then, we utilize modern large language models to analyze the contexts of such routing tokens and deduce their common pattern, and then infer other candidate values (e.g., other actions or file names) of these tokens. Lastly, we perform a hidden-interface directed black-box fuzzing, which mutates the routing tokens in input requests with these candidate values as the high-quality dictionary. We have implemented a prototype of EAGLEYE and evaluated it on 13 different commercial IoT devices. EAGLEYE successfully found 79 hidden interfaces, 25X more than the state-of-the-art (SOTA) solution IoTScope. Among them, we further discovered 29 unknown vulnerabilities including backdoor, XSS (cross-site scripting), command injection, and information leakage, and have received 7 CVEs.Hidden web interfaces, i.e., undisclosed access channels in IoT devices, introduce great security risks and have resulted in severe attacks in recent years. However, the definition of such threats is vague, and few solutions are able to discover them. Due to their hidden nature, traditional bug detection solutions (e.g., taint analysis, fuzzing) are hard to detect them. In this paper, we present a novel solution EAGLEYE to automatically expose hidden web interfaces in IoT devices. By analyzing input requests to public interfaces, we first identify routing tokens within the requests, i.e., those values (e.g., actions or file names) that are referenced and used as index by the firmware code (routing mechanism) to find associated handler functions. Then, we utilize modern large language models to analyze the contexts of such routing tokens and deduce their common pattern, and then infer other candidate values (e.g., other actions or file names) of these tokens. Lastly, we perform a hidden-interface directed black-box fuzzing, which mutates the routing tokens in input requests with these candidate values as the high-quality dictionary. We have implemented a prototype of EAGLEYE and evaluated it on 13 different commercial IoT devices. EAGLEYE successfully found 79 hidden interfaces, 25X more than the state-of-the-art (SOTA) solution IoTScope. Among them, we further discovered 29 unknown vulnerabilities including backdoor, XSS (cross-site scripting), command injection, and information leakage, and have received 7 CVEs.",
        "keywords": "",
        "url": "https://www.ndss-symposium.org/ndss-paper/eagleye-exposing-hidden-web-interfaces-in-iot-devices-via-routing-analysis",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2025",
        "labels": [
            "static analysis",
            "bug detection",
            "specification inference"
        ]
    },
    "Enhancing Security in Third-Party Library Reuse \u2013 Comprehensive Detection of 1-day Vulnerability through Code Patch Analysis": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Shangzhi Xu and Jialiang Dong and Weiting Cai and Juanru Li and Arash Shaghaghi and Nan Sun and Siqi Ma",
        "booktitle": "NDSS2025",
        "title": "Enhancing Security in Third-Party Library Reuse \u2013 Comprehensive Detection of 1-day Vulnerability through Code Patch Analysis",
        "year": "2025",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Nowadays, software development progressesrapidly to incorporate new features. To facilitate such growthand provide convenience for developers when creating andupdating software, reusing open-source software (i.e., thirdpartylibrary reuses) has become one of the most effectiveand efficient methods. Unfortunately, the practice of reusingthird-party libraries (TPLs) can also introduce vulnerabilities(known as 1-day vulnerabilities) because of the low maintenanceof TPLs, resulting in many vulnerable versions remaining inuse. If the software incorporating these TPLs fails to detect theintroduced vulnerabilities and leads to delayed updates, it willexacerbate the security risks. However, the complicated codedependencies and flexibility of TPL reuses make the detection of1-day vulnerability a challenging task. To support developers insecurely reusing TPLs during software development, we designand implement VULTURE, an effective and efficient detectiontool, aiming at identifying 1-day vulnerabilities that arise fromthe reuse of vulnerable TPLs. It first executes a database creationmethod, TPLFILTER, which leverages the Large LanguageModel (LLM) to automatically build a unique database for thetargeted platform. Instead of relying on code-level similaritycomparison, VULTURE employs hashing-based comparison toexplore the dependencies among the collected TPLs and identifythe similarities between the TPLs and the target projects.Recognizing that developers have the flexibility to reuse TPLsexactly or in a custom manner, VULTURE separately conductsversion-based comparison and chunk-based analysis to capturefine-grained semantic features at the function levels. We appliedVULTURE to 10 real-world projects to assess its effectivenessand efficiency in detecting 1-day vulnerabilities. VULTUREsuccessfully identified 175 vulnerabilities from 178 reused TPLs.Nowadays, software development progressesrapidly to incorporate new features. To facilitate such growthand provide convenience for developers when creating andupdating software, reusing open-source software (i.e., thirdpartylibrary reuses) has become one of the most effectiveand efficient methods. Unfortunately, the practice of reusingthird-party libraries (TPLs) can also introduce vulnerabilities(known as 1-day vulnerabilities) because of the low maintenanceof TPLs, resulting in many vulnerable versions remaining inuse. If the software incorporating these TPLs fails to detect theintroduced vulnerabilities and leads to delayed updates, it willexacerbate the security risks. However, the complicated codedependencies and flexibility of TPL reuses make the detection of1-day vulnerability a challenging task. To support developers insecurely reusing TPLs during software development, we designand implement VULTURE, an effective and efficient detectiontool, aiming at identifying 1-day vulnerabilities that arise fromthe reuse of vulnerable TPLs. It first executes a database creationmethod, TPLFILTER, which leverages the Large LanguageModel (LLM) to automatically build a unique database for thetargeted platform. Instead of relying on code-level similaritycomparison, VULTURE employs hashing-based comparison toexplore the dependencies among the collected TPLs and identifythe similarities between the TPLs and the target projects.Recognizing that developers have the flexibility to reuse TPLsexactly or in a custom manner, VULTURE separately conductsversion-based comparison and chunk-based analysis to capturefine-grained semantic features at the function levels. We appliedVULTURE to 10 real-world projects to assess its effectivenessand efficiency in detecting 1-day vulnerabilities. VULTUREsuccessfully identified 175 vulnerabilities from 178 reused TPLs.",
        "keywords": "",
        "url": "https://www.ndss-symposium.org/ndss-paper/enhancing-security-in-third-party-library-reuse-comprehensive-detection-of-1-day-vulnerability-through-code-patch-analysis",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2025",
        "labels": [
            "static analysis",
            "bug detection",
            "specification inference"
        ]
    },
    "From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Jie Lin and David Mohaisen",
        "booktitle": "NDSS2025",
        "title": "From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection",
        "year": "2025",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated strong potential in tasks such as code understanding and generation. This study evaluates several advanced LLMs\u2014such as LLaMA-2, CodeLLaMA, LLaMA-3, Mistral, Mixtral, Gemma, CodeGemma, Phi-2, Phi-3, and GPT-4\u2014for vulnerability detection, primarily in Java, with additional tests in C/C++ to assess generalization. We transition from basic positive sample detection to a more challenging task involving both positive and negative samples and evaluate the LLMs\u2019 ability to identify specific vulnerability types. Performance is analyzed using runtime and detection accuracy in zero-shot and few-shot settings with custom and generic metrics. Key insights include the strong performance of models like Gemma and LLaMA-2 in identifying vulnerabilities, though this success varies, with some configurations performing no better than random guessing. Performance also fluctuates significantly across programming languages and learning modes (zero- vs. few-shot). We further investigate the impact of model parameters, quantization methods, context window (CW) sizes, and architectural choices on vulnerability detection. While CW consistently enhances performance, benefits from other parameters, such as quantization, are more limited. Overall, our findings underscore the potential of LLMs in automated vulnerability detection, the complex interplay of model parameters, and the current limitations in varied scenarios and configurations.Large Language Models (LLMs) have demonstrated strong potential in tasks such as code understanding and generation. This study evaluates several advanced LLMs\u2014such as LLaMA-2, CodeLLaMA, LLaMA-3, Mistral, Mixtral, Gemma, CodeGemma, Phi-2, Phi-3, and GPT-4\u2014for vulnerability detection, primarily in Java, with additional tests in C/C++ to assess generalization. We transition from basic positive sample detection to a more challenging task involving both positive and negative samples and evaluate the LLMs\u2019 ability to identify specific vulnerability types. Performance is analyzed using runtime and detection accuracy in zero-shot and few-shot settings with custom and generic metrics. Key insights include the strong performance of models like Gemma and LLaMA-2 in identifying vulnerabilities, though this success varies, with some configurations performing no better than random guessing. Performance also fluctuates significantly across programming languages and learning modes (zero- vs. few-shot). We further investigate the impact of model parameters, quantization methods, context window (CW) sizes, and architectural choices on vulnerability detection. While CW consistently enhances performance, benefits from other parameters, such as quantization, are more limited. Overall, our findings underscore the potential of LLMs in automated vulnerability detection, the complex interplay of model parameters, and the current limitations in varied scenarios and configurations.",
        "keywords": "",
        "url": "https://www.ndss-symposium.org/ndss-paper/from-large-to-mammoth-a-comparative-evaluation-of-large-language-models-in-vulnerability-detection",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2025",
        "labels": [
            "static analysis",
            "bug detection",
            "empirical study"
        ]
    },
    "PropertyGPT: LLM-driven Formal Verification of Smart Contracts through Retrieval-Augmented Property Generation": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Ye Liu and Yue Xue and Daoyuan Wu and Yuqiang Sun and Yi Li and Miaolei Shi and Yang Liu",
        "booktitle": "NDSS2025",
        "title": "PropertyGPT: LLM-driven Formal Verification of Smart Contracts through Retrieval-Augmented Property Generation",
        "year": "2025",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Formal verification is a technique that can prove the correctness of a system with respect to a certain specification or property. It is especially valuable for security-sensitive smart contracts that manage billions in cryptocurrency assets. Although existing research has developed various static verification tools (or provers) for smart contracts, a key missing component is theautomated generation of comprehensive properties, including invariants, pre-/post-conditions, and rules. Hence, industry-leading players like Certora have to rely on their own or crowdsourced experts to manually write properties case by case.With recent advances in large language models (LLMs), this paper explores the potential of leveraging state-of-the-art LLMs, such as GPT-4, to transfer existing human-written properties (e.g., those from Certora auditing reports) and automatically generate customized properties for unknown code. To this end, we embed existing properties into a vector database and retrieve a reference property for LLM-based in-context learning to generate a new property for a given code. While this basic process is relatively straightforward, ensuring that the generated properties are (i) compilable, (ii) appropriate, and (iii) verifiable presents challenges. To address (i), we use the compilation and static analysis feedback as an external oracle to guide LLMs in iteratively revising the generated properties. For (ii), we consider multiple dimensions ofsimilarity to rank the properties and employ a weighted algorithm to identify the top-K properties as the final result. For (iii), we design a dedicated prover to formally verify the correctness of the generated properties. We have implemented these strategies into a novel LLM-based property generation tool called PropertyGPT. Our experiments show that PropertyGPT can generate comprehensive and high-quality properties, achieving an 80% recall compared to the ground truth. It successfully detected 26 CVEs/attack incidents out of 37 tested and also uncovered 12 zero-day vulnerabilities, leading to $8,256 in bug bounty rewards.Formal verification is a technique that can prove the correctness of a system with respect to a certain specification or property. It is especially valuable for security-sensitive smart contracts that manage billions in cryptocurrency assets. Although existing research has developed various static verification tools (or provers) for smart contracts, a key missing component is theautomated generation of comprehensive properties, including invariants, pre-/post-conditions, and rules. Hence, industry-leading players like Certora have to rely on their own or crowdsourced experts to manually write properties case by case.With recent advances in large language models (LLMs), this paper explores the potential of leveraging state-of-the-art LLMs, such as GPT-4, to transfer existing human-written properties (e.g., those from Certora auditing reports) and automatically generate customized properties for unknown code. To this end, we embed existing properties into a vector database and retrieve a reference property for LLM-based in-context learning to generate a new property for a given code. While this basic process is relatively straightforward, ensuring that the generated properties are (i) compilable, (ii) appropriate, and (iii) verifiable presents challenges. To address (i), we use the compilation and static analysis feedback as an external oracle to guide LLMs in iteratively revising the generated properties. For (ii), we consider multiple dimensions ofsimilarity to rank the properties and employ a weighted algorithm to identify the top-K properties as the final result. For (iii), we design a dedicated prover to formally verify the correctness of the generated properties. We have implemented these strategies into a novel LLM-based property generation tool called PropertyGPT. Our experiments show that PropertyGPT can generate comprehensive and high-quality properties, achieving an 80% recall compared to the ground truth. It successfully detected 26 CVEs/attack incidents out of 37 tested and also uncovered 12 zero-day vulnerabilities, leading to $8,256 in bug bounty rewards.",
        "keywords": "",
        "url": "https://www.ndss-symposium.org/ndss-paper/propertygpt-llm-driven-formal-verification-of-smart-contracts-through-retrieval-augmented-property-generation",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2025",
        "labels": [
            "static analysis",
            "bug detection",
            "program verification"
        ]
    }
}