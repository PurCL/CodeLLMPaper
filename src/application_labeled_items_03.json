{
    "AI-Assisted Code Authoring at Scale: Fine-Tuning, Deploying, and Mixed Methods Evaluation": {
        "type": "article",
        "key": "10.1145/3643774",
        "author": "Murali, Vijayaraghavan and Maddila, Chandra and Ahmad, Imad and Bolin, Michael and Cheng, Daniel and Ghorbani, Negar and Fernandez, Renuka and Nagappan, Nachiappan and Rigby, Peter C.",
        "title": "AI-Assisted Code Authoring at Scale: Fine-Tuning, Deploying, and Mixed Methods Evaluation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643774",
        "doi": "10.1145/3643774",
        "abstract": "Generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 9 programming languages and several coding surfaces. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges.        To release a LLM model at this scale, we needed to first ensure that it is sufficiently accurate. In a random sample of 20K source code files, depending on the language, we are able to reproduce hidden lines between 40\\% and 58\\% of the time, an improvement of 1.4\\texttimes{} and 4.1\\texttimes{} over a model trained only on public data.        We gradually rolled CodeCompose out to developers. At the time of this writing, 16K developers have used it with 8\\% of their code coming directly from CodeCompose.        To triangulate our numerical findings, we conduct a thematic analysis on the feedback from 70 developers. We find that 91.5\\% of the feedback is positive, with the most common themes being discovering APIs, dealing with boilerplate code, and accelerating coding. Meta continues to integrate this feedback into CodeCompose.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "48",
        "numpages": "20",
        "keywords": [
            "code generation",
            "code model training",
            "source code model"
        ],
        "venue": "FSE2024"
    },
    "Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models": {
        "type": "article",
        "key": "10.1145/3643776",
        "author": "Zhang, Zejun and Xing, Zhenchang and Ren, Xiaoxue and Lu, Qinghua and Xu, Xiwei",
        "title": "Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643776",
        "doi": "10.1145/3643776",
        "abstract": "Pythonic idioms are highly valued and widely used in the Python programming community. However, many  Python users find it challenging to use Pythonic idioms. Adopting rule-based approach or LLM-only approach is not sufficient to overcome three persistent challenges of code idiomatization including code miss, wrong detection and wrong refactoring. Motivated by the determinism of rules and adaptability of LLMs, we propose a hybrid approach consisting of three modules. We not only write prompts to instruct LLMs to complete tasks, but we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks. The ARIs are Python code generated by prompting LLMs to generate code. We first construct a knowledge module with three elements including ASTscenario, ASTcomponent and Condition, and prompt LLMs to generate Python code for incorporation into an ARI library for subsequent use. After that, for any syntax-error-free Python code, we invoke ARIs from the ARI library to extract ASTcomponent from the ASTscenario, and then filter out ASTcomponent that does not meet the condition. Finally, we design prompts to instruct LLMs to abstract and idiomatize code, and then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code. Next, we conduct a comprehensive evaluation of our approach, RIdiom, and Prompt-LLM on nine established Pythonic idioms in RIdiom. Our approach exhibits superior accuracy, F1 score, and recall, while maintaining precision levels comparable to RIdiom, all of which consistently exceed or come close to 90\\% for each metric of each idiom. Lastly, we extend our evaluation to encompass four new Pythonic idioms. Our approach consistently outperforms Prompt-LLM, achieving metrics with values consistently exceeding 90\\% for accuracy, F1-score, precision, and recall.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "50",
        "numpages": "22",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "venue": "FSE2024"
    },
    "Can GPT-4 Replicate Empirical Software Engineering Research?": {
        "type": "article",
        "key": "10.1145/3660767",
        "author": "Liang, Jenny T. and Badea, Carmen and Bird, Christian and DeLine, Robert and Ford, Denae and Forsgren, Nicole and Zimmermann, Thomas",
        "title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660767",
        "doi": "10.1145/3660767",
        "abstract": "Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners could benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help replicate and thus democratize empirical software engineering research.  In this paper, we examine GPT-4\u2019s abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodologies, as well as their ability to plan and generate code for analysis pipelines on seven empirical software engineering papers. We perform a user study with 14 participants with software engineering research expertise, who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module specifications) from the papers. We find that GPT-4 is able to surface correct assumptions, but struggles to generate ones that apply common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering knowledge. Our findings have implications for leveraging LLMs for software engineering research as well as practitioner data scientists in software teams.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "60",
        "numpages": "24",
        "keywords": [
            "empirical study",
            "agent design"
        ],
        "venue": "FSE2024"
    },
    "SimLLM: Calculating Semantic Similarity in Code Summaries using a Large Language Model-Based Approach": {
        "type": "article",
        "key": "10.1145/3660769",
        "author": "Jin, Xin and Lin, Zhiqiang",
        "title": "SimLLM: Calculating Semantic Similarity in Code Summaries using a Large Language Model-Based Approach",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660769",
        "doi": "10.1145/3660769",
        "abstract": "Code summaries are pivotal in software engineering, serving to improve code readability, maintainability, and collaboration. While recent advancements in Large Language Models (LLMs) have opened new avenues for automatic code summarization, existing metrics for evaluating summary quality, such as BLEU and BERTScore, have notable limitations. Specifically, these existing metrics either fail to capture the nuances of semantic meaning in summaries or are further limited in understanding domain-specific terminologies and expressions prevalent in code summaries. In this paper, we present SimLLM, a novel LLM-based approach designed to more precisely evaluate the semantic similarity of code summaries. Built upon an autoregressive LLM using a specialized pretraining task on permutated inputs and a pooling-based pairwise similarity measure, SimLLM overcomes the shortcomings of existing metrics. Our empirical evaluations demonstrate that SimLLM not only outperforms existing metrics but also shows a significantly high correlation with human ratings.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "62",
        "numpages": "24",
        "keywords": [
            "static analysis",
            "code summarization"
        ],
        "venue": "FSE2024"
    },
    "A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization": {
        "type": "article",
        "key": "10.1145/3660771",
        "author": "Kang, Sungmin and An, Gabin and Yoo, Shin",
        "title": "A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660771",
        "doi": "10.1145/3660771",
        "abstract": "Fault Localization (FL), in which a developer seeks to identify which part of the code is malfunctioning and needs to be fixed, is a recurring challenge in debugging. To reduce developer burden, many automated FL techniques have been proposed. However, prior work has noted that existing techniques fail to provide rationales for the suggested locations, hindering developer adoption of these techniques. With this in mind, we propose AutoFL, a Large Language Model (LLM)-based FL technique that generates an explanation of the bug along with a suggested fault location. AutoFL prompts an LLM to use function calls to navigate a repository, so that it can effectively localize faults over a large software repository and overcome the limit of the LLM context length. Extensive experiments on 798 real-world bugs in Java and Python reveal AutoFL improves method-level acc@1 by up to 233.3\\% over baselines. Furthermore, developers were interviewed on their impression of AutoFL-generated explanations, showing that developers generally liked the natural language explanations of AutoFL, and that they preferred reading a few, high-quality explanations instead of many.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "64",
        "numpages": "23",
        "keywords": [
            "program testing",
            "debugging"
        ],
        "venue": "FSE2024"
    },
    "Exploring and Unleashing the Power of Large Language Models in Automated Code Translation": {
        "type": "article",
        "key": "10.1145/3660778",
        "author": "Yang, Zhen and Liu, Fang and Yu, Zhongxing and Keung, Jacky Wai and Li, Jia and Liu, Shuo and Hong, Yifan and Ma, Xiaoxue and Jin, Zhi and Li, Ge",
        "title": "Exploring and Unleashing the Power of Large Language Models in Automated Code Translation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660778",
        "doi": "10.1145/3660778",
        "abstract": "Code translation tools, namely transpilers, are developed for automatic source-to-source translation. Latest learning-based transpilers have shown impressive enhancement against rule-based counterparts in both translation accuracy and readability, owing to their task-specific pre-training on extensive monolingual corpora. Nevertheless, their current performance still remains unsatisfactory for practical deployment, and the associated training resources are also prohibitively expensive. Large Language Models (LLMs), pre-trained on huge amounts of human-written code/text, have shown remarkable performance in many code intelligence tasks due to their powerful generality, even without task-specific re-training/fine-tuning. Thus, LLMs can potentially circumvent the above limitations, but they have not been exhaustively explored yet. This paper investigates diverse LLMs and learning-based transpilers for automated code translation tasks, finding that: although certain LLMs have outperformed current transpilers, they still have some accuracy issues, where most of the failures are induced by a lack of comprehension of source programs (38.51\\%), missing clear instructions on I/O types in translation (14.94\\%), and ignoring discrepancies between source and target programs (41.38\\%).  Enlightened by the above findings, we further propose UniTrans, a Unified code Translation framework, applicable to various LLMs, for unleashing their power in this field. Specifically, UniTrans first crafts a series of test cases for target programs with the assistance of source programs. Next, it harnesses the above auto-generated test cases to augment the code translation and then evaluate their correctness via execution. Afterward, UniTrans further (iteratively) repairs incorrectly translated programs prompted by test case execution results. Extensive experiments are conducted on six settings of translation datasets between Python, Java, and C++. Three recent LLMs of diverse sizes, including GPT-3.5 and LLaMA-13B/7B, are tested with UniTrans, and all achieve substantial improvements in terms of computational accuracy and exact match accuracy among almost all translation settings, showing the universal effectiveness of UniTrans in practice.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "71",
        "numpages": "24",
        "keywords": [
            "code generation",
            "program transformation",
            "code model training",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Evaluating and Improving ChatGPT for Unit Test Generation": {
        "type": "article",
        "key": "10.1145/3660783",
        "author": "Yuan, Zhiqiang and Liu, Mingwei and Ding, Shiji and Wang, Kaixin and Chen, Yixuan and Peng, Xin and Lou, Yiling",
        "title": "Evaluating and Improving ChatGPT for Unit Test Generation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660783",
        "doi": "10.1145/3660783",
        "abstract": "Unit testing plays an essential role in detecting bugs in functionally-discrete program units (e.g., methods). Manually writing high-quality unit tests is time-consuming and laborious. Although the traditional techniques are able to generate tests with reasonable coverage, they are shown to exhibit low readability and still cannot be directly adopted by developers in practice. Recent work has shown the large potential of large language models (LLMs) in unit test generation. By being pre-trained on a massive developer-written code corpus, the models are capable of generating more human-like and meaningful test code.                                                                                                In this work, we perform the first empirical study to evaluate the capability of ChatGPT (i.e., one of the most representative LLMs with outstanding performance in code generation and comprehension) in unit test generation. In particular, we conduct both a quantitative analysis and a user study to systematically investigate the quality of its generated tests in terms of correctness, sufficiency, readability, and usability. We find that the tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures (mostly caused by incorrect assertions); but the passing tests generated by ChatGPT almost resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved.                                                                                                Inspired by our findings above, we further propose ChatTester, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTester incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTester by generating 34.3\\% more compilable tests and 18.7\\% more tests with correct assertions than the default ChatGPT. In addition to ChatGPT, we further investigate the generalization capabilities of ChatTester by applying it to two recent open-source LLMs (i.e., CodeLLama-Instruct and CodeFuse) and our results show that ChatTester can also improve the quality of tests generated by these LLMs.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "76",
        "numpages": "24",
        "keywords": [
            "program testing",
            "unit testing",
            "empirical study",
            "code generation"
        ],
        "venue": "FSE2024"
    },
    "Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice": {
        "type": "article",
        "key": "10.1145/3660788",
        "author": "Khojah, Ranim and Mohamad, Mazen and Leitner, Philipp and de Oliveira Neto, Francisco Gomes",
        "title": "Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660788",
        "doi": "10.1145/3660788",
        "abstract": "Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering. Currently, there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry. We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey). We find that rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms. We also propose a theoretical framework for how the (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust). We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "81",
        "numpages": "22",
        "keywords": [
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?": {
        "type": "article",
        "key": "10.1145/3660791",
        "author": "Endres, Madeline and Fakhoury, Sarah and Chakraborty, Saikat and Lahiri, Shuvendu K.",
        "title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660791",
        "doi": "10.1145/3660791",
        "abstract": "Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a program\u2019s intent. However, there is typically no guarantee that a program\u2019s implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language, which makes natural language intent challenging to check programmatically. The \u201cemergent abilities\u201d of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice.     In this paper, we describe nl2postcondition, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions.   We introduce and validate metrics to measure and compare different nl2postcondition approaches, using the correctness and discriminative power of generated postconditions.   We then use qualitative and quantitative methods to assess the quality of nl2postcondition postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that  via LLMs has the potential to be helpful in practice;  generated postconditions were able to catch 64 real-world historical bugs from Defects4J.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "84",
        "numpages": "24",
        "keywords": [
            "static analysis",
            "specification inference",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "An Empirical Study on Code Review Activity Prediction and Its Impact in Practice": {
        "type": "article",
        "key": "10.1145/3660806",
        "author": "Olewicki, Doriane and Habchi, Sarra and Adams, Bram",
        "title": "An Empirical Study on Code Review Activity Prediction and Its Impact in Practice",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660806",
        "doi": "10.1145/3660806",
        "abstract": "During code reviews, an essential step in software quality assurance, reviewers have the difficult task of understanding and evaluating code changes to validate their quality and prevent introducing faults to the codebase. This is a tedious process where the effort needed is highly dependent on the code submitted, as well as the author\u2019s and the reviewer\u2019s experience, leading to median wait times for review feedback of 15-64 hours. Through an initial user study carried with 29 experts, we found that re-ordering the files changed by a patch within the review environment has potential to improve review quality, as more comments are written (+23\\%), and participants\u2019 file-level hot-spot precision and recall increases to 53\\% (+13\\%) and 28\\% (+8\\%), respectively, compared to the alphanumeric ordering. Hence, this paper aims to help code reviewers by predicting which files in a submitted patch need to be (1) commented, (2) revised, or (3) are hot-spots (commented or revised). To predict these tasks, we evaluate two different types of text embeddings (i.e., Bag-of-Words and Large Language Models encoding) and review process features (i.e., code size-based and history-based features). Our empirical study on three open-source and two industrial datasets shows that combining the code embedding and review process features leads to better results than the state-of-the-art approach. For all tasks, F1-scores (median of 40-62\\%) are significantly better than the state-of-the-art (from +1 to +9\\%).",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "99",
        "numpages": "23",
        "keywords": [
            "software maintenance and deployment",
            "code review",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?": {
        "type": "article",
        "key": "10.1145/3660807",
        "author": "Kou, Bonan and Chen, Shengmai and Wang, Zhijie and Ma, Lei and Zhang, Tianyi",
        "title": "Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660807",
        "doi": "10.1145/3660807",
        "abstract": "Large Language Models (LLMs) have recently been widely used for code generation. Due to the complexity and opacity of LLMs, little is known about how these models generate code. We made the first attempt to bridge this knowledge gap by investigating whether LLMs attend to the same parts of a task description as human programmers during code generation. An analysis of six LLMs, including GPT-4, on two popular code generation benchmarks revealed a consistent misalignment between LLMs' and programmers' attention. We manually analyzed 211 incorrect code snippets and found five attention patterns that can be used to explain many code generation errors. Finally, a user study showed that model attention computed by a perturbation-based method is often favored by human programmers. Our findings highlight the need for human-aligned LLMs for better interpretability and programmer trust.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "100",
        "numpages": "24",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Mining Action Rules for Defect Reduction Planning": {
        "type": "article",
        "key": "10.1145/3660809",
        "author": "Oueslati, Khouloud and Laberge, Gabriel and Lamothe, Maxime and Khomh, Foutse",
        "title": "Mining Action Rules for Defect Reduction Planning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660809",
        "doi": "10.1145/3660809",
        "abstract": "Defect reduction planning plays a vital role in enhancing software quality and minimizing software maintenance costs. By training a black box machine learning model and \u201cexplaining\u201d its predictions, explainable AI for software engineering aims to identify the code characteristics that impact maintenance risks. However, post-hoc explanations do not always faithfully reflect what the original model computes. In this paper, we introduce CounterACT, a Counterfactual ACTion rule mining approach that can generate defect reduction plans without black-box models. By leveraging action rules, CounterACT provides a course of action that can be considered as a counterfactual explanation for the class (e.g., buggy or not buggy) assigned to a piece of code. We compare the effectiveness of CounterACT with the original action rule mining algorithm and six established defect reduction approaches on 9 software projects. Our evaluation is based on (a) overlap scores between proposed code changes and actual developer modifications; (b) improvement scores in future releases; and (c) the precision, recall, and F1-score of the plans. Our results show that, compared to competing approaches, CounterACT\u2019s explainable plans achieve higher overlap scores at the release level (median 95\\%) and commit level (median 85.97\\%), and they offer better trade-off between precision and recall (median F1-score 88.12\\%). Finally, we venture beyond planning and explore leveraging Large Language models (LLM) for generating code edits from our generated plans. Our results show that suggested LLM code edits supported by our plans are actionable and are more likely to pass relevant test cases than vanilla LLM code recommendations.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "102",
        "numpages": "23",
        "keywords": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification": {
        "type": "article",
        "key": "10.1145/3660810",
        "author": "Mu, Fangwen and Shi, Lin and Wang, Song and Yu, Zhuohao and Zhang, Binquan and Wang, ChenXue and Liu, Shichao and Wang, Qing",
        "title": "ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660810",
        "doi": "10.1145/3660810",
        "abstract": "Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96\\% to 80.80\\% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43\\% to 69.60\\% and from 54.32\\% to 62.37\\%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "103",
        "numpages": "23",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "venue": "FSE2024"
    },
    "Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning": {
        "type": "article",
        "key": "10.1145/3660811",
        "author": "Mai, Yubo and Gao, Zhipeng and Hu, Xing and Bao, Lingfeng and Liu, Yu and Sun, JianLing",
        "title": "Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660811",
        "doi": "10.1145/3660811",
        "abstract": "Inspired by the great potential of Large Language Models (LLMs) for solving complex coding tasks, in this paper, we propose a novel approach, named Code2API, to automatically perform APIzation for Stack Overflow code snippets. Code2API does not require additional model training or any manual crafting rules and can be easily deployed on personal computers without relying on other external tools. Specifically, Code2API guides the LLMs through well-designed prompts to generate well-formed APIs for given code snippets. To elicit knowledge and logical reasoning from LLMs, we used chain-of-thought (CoT) reasoning and few-shot in-context learning, which can help the LLMs fully understand the APIzation task and solve it step by step in a manner similar to a developer. Our evaluations show that Code2API achieves a remarkable accuracy in identifying method parameters (65\\%) and return statements (66\\%) equivalent to human-generated ones, surpassing the current state-of-the-art approach, APIzator, by 15.0\\% and 16.5\\% respectively. Moreover, compared with APIzator, our user study demonstrates that Code2API exhibits superior performance in generating meaningful method names, even surpassing the human-level performance, and developers are more willing to use APIs generated by our approach, highlighting the applicability of our tool in practice. Finally, we successfully extend our framework to the Python dataset, achieving a comparable performance with Java, which verifies the generalizability of our tool.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "104",
        "numpages": "23",
        "keywords": [
            "code generation",
            "program synthesis",
            "source code model"
        ],
        "venue": "FSE2024"
    },
    "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-unleashing",
        "author": "Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng",
        "booktitle": "NAACL2024",
        "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds\u2019 strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
        "keywords": [
            "agent design",
            "planning"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.15",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "Program-Aided Reasoners (Better) Know What They Know": {
        "type": "INPROCEEDINGS",
        "key": "kabra-etal-2024-program",
        "author": "Kabra, Anubha and Rangreji, Sanketh and Mathur, Yash and Madaan, Aman and Liu, Emmy and Neubig, Graham",
        "booktitle": "NAACL2024",
        "title": "Program-Aided Reasoners (Better) Know What They Know",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to \u201cknow what they know\u201d, which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types - LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.",
        "keywords": [
            "prompting strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.125",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning": {
        "type": "INPROCEEDINGS",
        "key": "zhu-etal-2024-pad",
        "author": "Zhu, Xuekai and Qi, Biqing and Zhang, Kaiyan and Long, Xinwei and Lin, Zhouhan and Zhou, Bowen",
        "booktitle": "NAACL2024",
        "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.",
        "keywords": [
            "prompting strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.142",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "Evaluating In-Context Learning of Libraries for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "patel-etal-2024-evaluating",
        "author": "Patel, Arkil and Reddy, Siva and Bahdanau, Dzmitry and Dasigi, Pradeep",
        "booktitle": "NAACL2024",
        "title": "Evaluating In-Context Learning of Libraries for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability. A particularly promising area is their ability to interpret code modules from unfamiliar libraries for solving user-instructed tasks. Recent work has shown that large proprietary LLMs can learn novel library usage in-context from demonstrations. These results raise several open questions: whether demonstrations of library usage is required, whether smaller (and more open) models also possess such capabilities, etc. In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context. Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context. Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions, which are often cheaper to obtain than demonstrations. Overall, our results pave the way for harnessing LLMs in more adaptable and dynamic coding environments.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.161",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback": {
        "type": "INPROCEEDINGS",
        "key": "wu-2024-uicoder",
        "author": "Wu, Jason and Schoop, Eldon and Leung, Alan and Barik, Titus and Bigham, Jeffrey and Nichols, Jeffrey",
        "booktitle": "NAACL2024",
        "title": "UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Many large language models (LLMs) struggle to consistently generate UI code that compiles and produces visually relevant designs. Existing approaches to improve generation rely either on expensive human feedback or distilling a proprietary model. In this paper, we explore the use of automated feedback (compilers and multi-modal models) to guide LLMs to generate high-quality UI code. Our method starts with an existing LLM and iteratively produces improved models by self-generating a large synthetic dataset using an original model, applying automated tools to aggressively filter, score, and de-duplicate the data into a refined higher quality dataset, and producing a new LLM by finetuning the original on the refined dataset.We applied our approach to several open-source LLMs and compared the resulting performance to baseline models with both automated metrics and human preferences.Our results show the resulting models outperform all other downloadable baselines and approach the performance of larger proprietary models.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.417",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation": {
        "type": "INPROCEEDINGS",
        "key": "salim-etal-2024-impeding",
        "author": "Salim, Saiful Islam and Yang, Rubin Yuchan and Cooper, Alexander and Ray, Suryashree and Debray, Saumya and Rahaman, Sazzadur",
        "booktitle": "EMNLP-main2024",
        "title": "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at measuring the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77%, ii) the drop in correctness caused by these perturbations was affected based on their detectability.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.27",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-amr",
        "author": "Luo, Ziyang and Li, Xin and Lin, Hongzhan and Ma, Jing and Bing, Lidong",
        "booktitle": "EMNLP-main2024",
        "title": "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The impressive performance of proprietary LLMs like GPT4 in code generation has led to a trend to replicate these capabilities in open-source models through knowledge distillation (e.g. Code Evol-Instruct). However, these efforts often neglect the crucial aspect of response quality, relying heavily on teacher models for direct response distillation. This paradigm, especially for complex instructions, can degrade the quality of synthesized data, compromising the knowledge distillation process. To this end, our study introduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which employs a two-stage process to refine response distillation. The first stage, modular decomposition, breaks down the direct response into more manageable sub-modules. The second stage, adaptive response evolution, automatically evolves the response with the related function modules. Our experiments with three popular code benchmarks\u2014HumanEval, MBPP, and EvalPlus\u2014attests to the superiority of the AMR-Evol framework over baseline response distillation methods. By comparing with the open-source Code LLMs trained on a similar scale of data, we observed performance enhancements: more than +3.0 points on HumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the effectiveness of our framework. Our codes are available at https://github.com/ChiYeungLaw/AMR-Evol.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model training",
            "source code model"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.66",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "LLM4Decompile: Decompiling Binary Code with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "tan-etal-2024-llm4decompile",
        "author": "Tan, Hanzhuo and Luo, Qi and Li, Jing and Zhang, Yuqun",
        "booktitle": "EMNLP-main2024",
        "title": "LLM4Decompile: Decompiling Binary Code with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results.",
        "keywords": [
            "code generation",
            "program decompilation",
            "code model training",
            "binary code model"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.203",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-ptd",
        "author": "Luo, Ruilin and Wang, Liyuan and Lin, Binghuai and Lin, Zicheng and Yang, Yujiu",
        "booktitle": "EMNLP-main2024",
        "title": "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problem and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-the-art (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress. Code is available at https://github.com/lrlbbzl/PTD-SQL.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.221",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "How Do Humans Write Code? Large Models Do It the Same Way Too": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-humans",
        "author": "Li, Long and He, Xuzheng and Wang, Haozhe and Wang, Linlin and He, Liang",
        "booktitle": "EMNLP-main2024",
        "title": "How Do Humans Write Code? Large Models Do It the Same Way Too",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model\u2019s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.267",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    }
}