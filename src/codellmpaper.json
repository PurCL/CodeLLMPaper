{
    "Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities": {
        "type": "article",
        "key": "10.1145/3664606",
        "author": "Ma, Wei and Liu, Shangqing and Zhao, Mengjie and Xie, Xiaofei and Wang, Wenhang and Hu, Qiang and Zhang, Jie and Liu, Yang",
        "title": "Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664606",
        "doi": "10.1145/3664606",
        "abstract": "Code models have made significant advancements in code intelligence by encoding knowledge about programming languages. While previous studies have explored the capabilities of these models in learning code syntax, there has been limited investigation on their ability to understand code semantics. Additionally, existing analyses assume that the number of edges between nodes at the abstract syntax tree&nbsp;(AST) is related to syntax distance, and also often require transforming the high-dimensional space of deep learning models to a low-dimensional one, which may introduce inaccuracies. To study how code models represent code syntax and semantics, we conduct a comprehensive analysis of seven code models, including four representative code pre-trained models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) and three large language models (LLMs) (StarCoder, CodeLlama and CodeT5+). We design four probing tasks to assess the models\u2019 capacities in learning both code syntax and semantics. These probing tasks reconstruct code syntax and semantics structures (AST, control dependence graph (CDG), data dependence graph (DDG), and control flow graph (CFG)) in the representation space. These structures are core concepts for code understanding. We also investigate the syntax token role in each token representation and the long dependency between the code tokens. Additionally, we analyze the distribution of attention weights related to code semantic structures. Through extensive analysis, our findings highlight the strengths and limitations of different code models in learning code syntax and semantics. The results demonstrate that these models excel in learning code syntax, successfully capturing the syntax relationships between tokens and the syntax roles of individual tokens. However, their performance in encoding code semantics varies. CodeT5 and CodeBERT demonstrate proficiency in capturing control and data dependencies, whereas UnixCoder shows weaker performance in this aspect. We do not observe LLMs generally performing much better than pre-trained models. The shallow layers of LLMs perform better than their deep layers. The investigation of attention weights reveals that different attention heads play distinct roles in encoding code semantics. Our research findings emphasize the need for further enhancements in code models to better learn code semantics. This study contributes to the understanding of code models\u2019 abilities in syntax and semantics analysis. Our findings provide guidance for future improvements in code models, facilitating their effective application in various code-related tasks.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "169",
        "numpages": "29",
        "keywords": "Code model analysis, syntax and semantic encoding"
    },
    "Self-Planning Code Generation with Large Language Models": {
        "type": "article",
        "key": "10.1145/3672456",
        "author": "Jiang, Xue and Dong, Yihong and Wang, Lecheng and Fang, Zheng and Shang, Qiwei and Li, Ge and Jin, Zhi and Jiao, Wenpin",
        "title": "Self-Planning Code Generation with Large Language Models",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672456",
        "doi": "10.1145/3672456",
        "abstract": "Although large language models (LLMs) have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule solution steps prior to implementation. To this end, we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem-solving. This paper proposes a self-planning code generation approach with large language models, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, LLM plans out concise solution steps from the intent combined with few-shot prompting. Subsequently, in the implementation phase, the model generates code step by step, guided by the preceding solution steps. We conduct extensive experiments on various code-generation benchmarks across multiple programming languages. Experimental results show that self-planning code generation achieves a relative improvement of up to 25.4\\% in Pass@1 compared to direct code generation, and up to 11.9\\% compared to Chain-of-Thought of code generation. Moreover, our self-planning approach also enhances the quality of the generated code with respect to correctness, readability, and robustness, as assessed by humans.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "182",
        "numpages": "30",
        "keywords": "Code Generation, Large language models, Planning"
    },
    "Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Dataflows to Generate Dataflow Graphs in Dynamically Typed Code": {
        "type": "article",
        "key": "10.1145/3672458",
        "author": "Huang, Qing and Luo, Zhiwen and Xing, Zhenchang and Zeng, Jinshan and Chen, Jieshan and Xu, Xiwei and Chen, Yong",
        "title": "Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Dataflows to Generate Dataflow Graphs in Dynamically Typed Code",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672458",
        "doi": "10.1145/3672458",
        "abstract": "Dataflow graphs (DFGs) capture definitions (defs) and uses across program blocks, which is a fundamental program representation for program analysis, testing and maintenance. However, dynamically typed programming languages like Python present implicit dataflow issues that make it challenging to determine def-use flow information at compile time. Static analysis methods like Soot and WALA are inadequate for handling these issues, and manually enumerating comprehensive heuristic rules is impractical. Large pre-trained language models (LLMs) offer a potential solution, as they have powerful language understanding and pattern matching abilities, allowing them to predict implicit dataflow by analyzing code context and relationships between variables, functions, and statements in code. We propose leveraging LLMs\u2019 in-context learning ability to learn implicit rules and patterns from code representation and contextual information to solve implicit dataflow problems. To further enhance the accuracy of LLMs, we design a five-step chain of thought (CoT) and break it down into an Artificial Intelligence (AI) chain, with each step corresponding to a separate AI unit to generate accurate DFGs for Python code. Our approach\u2019s performance is thoroughly assessed, demonstrating the effectiveness of each AI unit in the AI Chain. Compared to static analysis, our method achieves 82\\% higher def coverage and 58\\% higher use coverage in DFG generation on implicit dataflow. We also prove the indispensability of each unit in the AI Chain. Overall, our approach offers a promising direction for building software engineering tools by utilizing foundation models, eliminating significant engineering and maintenance effort, but focusing on identifying problems for AI to solve.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "183",
        "numpages": "29",
        "keywords": "Dataflow graph, AI chain, Large Language Models"
    },
    "LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models": {
        "type": "article",
        "key": "10.1145/3664812",
        "author": "Feng, Xiaoning and Han, Xiaohong and Chen, Simin and Yang, Wei",
        "title": "LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664812",
        "doi": "10.1145/3664812",
        "abstract": "Large Language Models (LLMs) have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of LLMs, which is of paramount importance due to often vast generation demands and real-time requirements, has surprisingly received little attention. In this article, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art LLMs. By analyzing the working mechanism and implementation of 20,543 public-accessible LLMs, we observe a fundamental property in LLMs that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of LLMs instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime-generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that LLMs would have to go through enough iterations to satisfy the pre-configured threshold. We present LLMEffiChecker, which can work under both white-box setting and black-box setting. In the white-box scenario, LLMEffiChecker develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level. In the black-box scenario, LLMEffiChecker employs a causal inference-based approach to find critical tokens and similarly applies three levels of imperceptible perturbation to them. Both the white-box and black-box settings effectively delay the appearance of EOS, compelling these inputs to reach the naturally unreachable threshold. To demonstrate the effectiveness of LLMEffiChecker, we conduct a systematic evaluation on nine publicly available LLMs: Google T5, AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT, and Salesforce CodeGen. Experimental results show that LLMEffiChecker can increase on average LLMs\u2019 response latency and energy consumption by 325\\% to 3,244\\% and 344\\% to 3,616\\%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by LLMEffiChecker significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "186",
        "numpages": "38",
        "keywords": "Machine learning, software testing, large language model"
    },
    "Self-Collaboration Code Generation via ChatGPT": {
        "type": "article",
        "key": "10.1145/3672459",
        "author": "Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge",
        "title": "Self-Collaboration Code Generation via ChatGPT",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672459",
        "doi": "10.1145/3672459",
        "abstract": "Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct \u201cexperts,\u201d each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other\u2019s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development\u2019s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9\u201347.1\\% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "189",
        "numpages": "38",
        "keywords": "Code generation, large language models, multi-agent collaboration, software development"
    },
    "Risky Dynamic Typing-related Practices in Python: An Empirical Study": {
        "type": "article",
        "key": "10.1145/3649593",
        "author": "Chen, Zhifei and Chen, Lin and Yang, Yibiao and Feng, Qiong and Li, Xuansong and Song, Wei",
        "title": "Risky Dynamic Typing-related Practices in Python: An Empirical Study",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649593",
        "doi": "10.1145/3649593",
        "abstract": "Python\u2019s dynamic typing nature provides developers with powerful programming abstractions. However, many type-related bugs are accumulated in code bases of Python due to the misuse of dynamic typing. The goal of this article is to aid in the understanding of developers\u2019 high-risk practices toward dynamic typing and the early detection of type-related bugs. We first formulate the rules of six types of risky dynamic typing-related practices (type smells for short) in Python. We then develop a rule-based tool named RUPOR, which builds an accurate type base to detect type smells. Our evaluation shows that RUPOR outperforms the existing type smell detection techniques (including the Large Language Models\u2013based approaches, Mypy, and PYDYPE) on a benchmark of 900 Python methods. Based on RUPOR, we conduct an empirical study on 25 real-world projects. We find that type smells are significantly related to the occurrence of post-release faults. The fault-proneness prediction model built with type smell features slightly outperforms the model built without them. We also summarize the common patterns, including inserting type check to fix type smell bugs. These findings provide valuable insights for preventing and fixing type-related bugs in the programs written in dynamic-typed languages.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "140",
        "numpages": "35",
        "keywords": "Dynamic typing, Python, empirical study, bug fixing"
    },
    "Early and Realistic Exploitability Prediction of Just-Disclosed Software Vulnerabilities: How Reliable Can It Be?": {
        "type": "article",
        "key": "10.1145/3654443",
        "author": "Iannone, Emanuele and Sellitto, Giulia and Iaccarino, Emanuele and Ferrucci, Filomena and De Lucia, Andrea and Palomba, Fabio",
        "title": "Early and Realistic Exploitability Prediction of Just-Disclosed Software Vulnerabilities: How Reliable Can It Be?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3654443",
        "doi": "10.1145/3654443",
        "abstract": "With the rate of discovered and disclosed vulnerabilities escalating, researchers have been experimenting with machine learning to predict whether a vulnerability will be exploited. Existing solutions leverage information unavailable when a CVE is created, making them unsuitable just after the disclosure. This paper experiments with early exploitability prediction models driven exclusively by the initial CVE record, i.e., the original description and the linked online discussions. Leveraging NVD and Exploit Database, we evaluate 72 prediction models trained using six traditional machine learning classifiers, four feature representation schemas, and three data balancing algorithms. We also experiment with five pre-trained large language models (LLMs). The models leverage seven different corpora made by combining three data sources, i.e., CVE description, Security Focus, and BugTraq. The models are evaluated in a realistic, time-aware fashion by removing the training and test instances that cannot be labeled \u201cneutral\u201d&nbsp;with sufficient confidence. The validation reveals that CVE descriptions and Security Focus discussions are the best data to train on. Pre-trained LLMs do not show the expected performance, requiring further pre-training in the security domain. We distill new research directions, identify possible room for improvement, and envision automated systems assisting security experts in assessing the exploitability.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "146",
        "numpages": "41",
        "keywords": "Exploitability prediction, software vulnerabilities, machine learning, text mining, natural language processing"
    },
    "Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains": {
        "type": "article",
        "key": "10.1145/3638247",
        "author": "Cheng, Yu and Chen, Jieshan and Huang, Qing and Xing, Zhenchang and Xu, Xiwei and Lu, Qinghua",
        "title": "Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "5",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3638247",
        "doi": "10.1145/3638247",
        "abstract": "The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e., prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we systematically review, summarise, refine and extend the concept of AI chain by incorporating the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematize AI chain engineering methodology. We also develop a no-code integrated development environment, , which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "124",
        "numpages": "24",
        "keywords": "AI chain engineering, visual programming, large language models, No/Low code, SE for AI"
    },
    "Statically Contextualizing Large Language Models with Typed Holes": {
        "type": "article",
        "key": "10.1145/3689728",
        "author": "Blinn, Andrew and Li, Xiang and Kim, June Hyung and Omar, Cyrus",
        "title": "Statically Contextualizing Large Language Models with Typed Holes",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689728",
        "doi": "10.1145/3689728",
        "abstract": "Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate code context, particularly when working with definitions that are neither in the training data nor near the cursor. This paper demonstrates that tighter integration with the type and binding structure of the programming language in use, as exposed by its language server, can help address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server is able to identify the type and typing context of the hole that the programmer is filling, with Hazel's total syntax and type error correction ensuring that a meaningful program sketch is available whenever the developer requests a completion. This allows the system to prompt the LLM with codebase-wide contextual information that is not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server, which provides error localization and error messages. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications with accompanying unit tests that have been written from scratch to avoid data contamination, and that can easily be ported to new languages because they do not have large external library dependencies. These applications serve as challenge problems due to their extensive reliance on application-specific data structures. Through an ablation study, we examine the impact of contextualization with type definitions, function headers, and errors messages, individually and in combination. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel, a low-resource language, we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource mainstream languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "288",
        "numpages": "31",
        "keywords": "Large Language Models, Program Repair, Program Synthesis"
    },
    "Artifact for Statically Contextualizing Large Language Models with Typed Holes": {
        "type": "software",
        "key": "10.5281/zenodo.12669479",
        "author": "Blinn, Andrew and Li, Xiang and Kim, June Hyung and Omar, Cyrus",
        "title": "Artifact for Statically Contextualizing Large Language Models with Typed Holes",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.12669479",
        "abstract": "    <p>Artifact for the paper \u2018Artifact for Statically Contextualizing LargeLanguage Models with Typed Holes\u2019, to be published at OOPSLA2024. Theartifact consists of raw data from the experiments, the testing harnessused, the source for the Hazel Editor and Langauge Server, and a copy ofthe StarCoder2 LLM weights. The archive is password protected to preventthe benchmark data set from being automatically scrapped; see the Zenododescription for the password.</p>",
        "keywords": "Large Language Models, Program Synthesis, Programming Languages, Types"
    },
    "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs": {
        "type": "article",
        "key": "10.1145/3689735",
        "author": "Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Freeman, Anders and Anderson, Carolyn Jane and Feldman, Molly Q and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun",
        "title": "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689735",
        "doi": "10.1145/3689735",
        "abstract": "Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as building blocks for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming language. Code LLMs produce impressive results on high-resource programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages that have limited training data available (e.g., OCaml, Racket, and several others).                                                                                                                                                                                                                                                              This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach, called MultiPL-T, generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. MultiPL-T translates training data from high-resource languages into training data for low-resource languages in the following way. 1) We use a Code LLM to synthesize unit tests for commented code from a high-resource source language, filtering out faulty tests and code with low test coverage. 2) We use a Code LLM to translate the code from the high-resource source language to a target low-resource language. This gives us a corpus of candidate training data in the target language, but many of these translations are wrong. 3) We use a lightweight compiler to compile the test cases generated in (1) from the source language to the target language, which allows us to filter our obviously wrong translations. The result is a training corpus in the target low-resource language where all items have been validated with test cases. We apply this approach to generate tens of thousands of new, validated training items for five low-resource languages: Julia, Lua, OCaml, R, and Racket, using Python as the source high-resource language. Furthermore, we use an open Code LLM (StarCoderBase) with open training data (The Stack), which allows us to decontaminate benchmarks, train models without violating licenses, and run experiments that could not otherwise be done.                                                                                                                                                                                                                                                              Using datasets generated with MultiPL-T, we present fine-tuned versions of StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket that outperform other fine-tunes of these base models on the natural language to code task. We also present Racket fine-tunes for two very recent models, DeepSeek Coder and StarCoder2, to show that MultiPL-T continues to outperform other fine-tuning approaches for low-resource languages. The MultiPL-T approach is easy to apply to new languages, and is significantly more efficient and effective than alternatives such as training longer.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "295",
        "numpages": "32",
        "keywords": "Large Language Models trained on Code"
    },
    "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models": {
        "type": "article",
        "key": "10.1145/3689736",
        "author": "Yang, Chenyuan and Deng, Yinlin and Lu, Runyu and Yao, Jiayi and Liu, Jiawei and Jabbarvand, Reyhaneh and Zhang, Lingming",
        "title": "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689736",
        "doi": "10.1145/3689736",
        "abstract": "Compiler correctness is crucial, as miscompilation can falsify program behaviors, leading to serious consequences over the software supply chain. In the literature, fuzzing has been extensively studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates test programs without sufficient understanding of internal compiler behaviors. As such, they often fail to construct test programs to exercise intricate optimizations. Meanwhile, traditional white-box techniques, such as symbolic execution, are computationally inapplicable to the giant codebase of compiler systems. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks and even have achieved state-of-the-art performance in black-box fuzzing. Nonetheless, guiding LLMs with compiler source-code information remains a missing piece of research in compiler testing.              To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization, with a spotlight on detecting deep logic bugs in the emerging deep learning (DL) compilers. WhiteFox adopts a multi-agent framework: (i) an LLM-based analysis agent examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; (ii) an LLM-based generation agent produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are also used as feedback to further enhance the test generation prompt on the fly. Our evaluation on the three most popular DL compilers (i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows that WhiteFox can generate high-quality test programs to exercise deep optimizations requiring intricate conditions, practicing up to 8 times more optimizations than state-of-the-art fuzzers. To date, WhiteFox has found in total 101 bugs for the compilers under test, with 92 confirmed as previously unknown and 70 already fixed. Notably, WhiteFox has been recently acknowledged by the PyTorch team, and is in the process of being incorporated into its development workflow. Finally, beyond DL compilers, WhiteFox can also be adapted for compilers in different domains, such as LLVM, where WhiteFox has already found multiple bugs.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "296",
        "numpages": "27",
        "keywords": "Code Analysis, Fuzzing, Large Language Models, White-box Testing"
    },
    "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models": {
        "type": "article",
        "key": "10.1145/3689776",
        "author": "Li, Ningke and Li, Yuekang and Liu, Yi and Shi, Ling and Wang, Kailong and Wang, Haoyu",
        "title": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689776",
        "doi": "10.1145/3689776",
        "abstract": "Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations \u2014 coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges: 1) Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases. 2) Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations.    To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth.    Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7\\% to 59.8\\%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations.    To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "336",
        "numpages": "30",
        "keywords": "Hallucination, Large Language Model, Software Testing"
    },
    "Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach": {
        "type": "article",
        "key": "10.1145/3649828",
        "author": "Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun",
        "title": "Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA1",
        "url": "https://doi.org/10.1145/3649828",
        "doi": "10.1145/3649828",
        "abstract": "While static analysis is instrumental in uncovering software bugs, its precision in analyzing large and intricate codebases remains challenging. The emerging prowess of Large Language Models (LLMs) offers a promising avenue to address these complexities. In this paper, we present LLift, a pioneering framework that synergizes static analysis and LLMs, with a spotlight on identifying use-before-initialization (UBI) bugs within the Linux kernel. Drawing from our insights into variable usage conventions in Linux, we enhance path analysis using post-constraint guidance. This approach, combined with our methodically crafted procedures, empowers LLift to adeptly handle the challenges of bug-specific modeling, extensive codebases, and the unpredictable nature of LLMs. Our real-world evaluations identified four previously undiscovered UBI bugs in the mainstream Linux kernel, which the Linux community has acknowledged. This study reaffirms the potential of marrying static analysis with LLMs, setting a compelling direction for future research in this area.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "apr",
        "articleno": "111",
        "numpages": "26",
        "keywords": "Static analysis, bug detection, large language model"
    },
    "Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach (Artifact)": {
        "type": "software",
        "key": "10.5281/zenodo.10780591",
        "author": "Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun",
        "title": "Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach (Artifact)",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.10780591",
        "abstract": "    <p>This repo contains all the code and test cases for the paper \u201cEnhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach\u201d. LLift is an automated framework enhancing static analysis in bug detection with LLMs.</p>",
        "keywords": "bug detection, large language model, Static analysis"
    },
    "PyDex: Repairing Bugs in Introductory Python Assignments using LLMs": {
        "type": "article",
        "key": "10.1145/3649850",
        "author": "Zhang, Jialu and Cambronero, Jos\\'{e} Pablo and Gulwani, Sumit and Le, Vu and Piskac, Ruzica and Soares, Gustavo and Verbruggen, Gust",
        "title": "PyDex: Repairing Bugs in Introductory Python Assignments using LLMs",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA1",
        "url": "https://doi.org/10.1145/3649850",
        "doi": "10.1145/3649850",
        "abstract": "Students often make mistakes in their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex (a version of GPT), to build an APR system -- PyDex -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate PyDex on 286 real student programs and compare to three baselines, including one that combines a state-of-the-art Python syntax repair engine, BIFI, and a state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that PyDex can fix more programs and produce smaller patches on average.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "apr",
        "articleno": "133",
        "numpages": "25",
        "keywords": "AI for programming education, automated program repair, large language models"
    },
    "SEAVER: Attention Reallocation for Mitigating Distractions in Language Models for Conditional Semantic Textual Similarity Measurement": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-seaver",
        "author": "Li, Baixuan and Fan, Yunlong and Gao, Zhiqiang",
        "booktitle": "EMNLP-findings2024",
        "title": "SEAVER: Attention Reallocation for Mitigating Distractions in Language Models for Conditional Semantic Textual Similarity Measurement",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Conditional Semantic Textual Similarity (C-STS) introduces specific limiting conditions to the traditional Semantic Textual Similarity (STS) task, posing challenges for STS models. Language models employing cross-encoding demonstrate satisfactory performance in STS, yet their effectiveness significantly diminishes in C-STS. In this work, we argue that the failure is due to the fact that the redundant information in the text distracts language models from the required condition-relevant information. To alleviate this, we propose Self-Augmentation via Self-Reweighting (SEAVER), which, based solely on models\u2019 internal attention and without the need for external auxiliary information, adaptively reallocates the model\u2019s attention weights by emphasizing the importance of condition-relevant tokens. On the C-STS-2023 test set, SEAVER consistently improves performance of all million-scale fine-tuning baseline models (up to around 3 points), and even surpasses performance of billion-scale few-shot prompted large language models (such as GPT-4). Our code is available at https://github.com/BaixuanLi/SEAVER.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.5",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent": {
        "type": "INPROCEEDINGS",
        "key": "ji-etal-2024-srap",
        "author": "Ji, Jiarui and Li, Yang and Liu, Hongtao and Du, Zhicheng and Wei, Zhewei and Qi, Qi and Shen, Weiran and Lin, Yankai",
        "booktitle": "EMNLP-findings2024",
        "title": "SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Public scarce resource allocation plays a crucial role in economics as it directly influences the efficiency and equity in society. Traditional studies including theoretical model-based, empirical study-based and simulation-based methods encounter limitations due to the idealized assumption of complete information and individual rationality, as well as constraints posed by limited available data. In this work, we propose an innovative framework, SRAP-Agent, which integrates Large Language Models (LLMs) into economic simulations, aiming to bridge the gap between theoretical models and real-world dynamics. Using public housing allocation scenarios as a case study, we conduct extensive policy simulation experiments to verify the feasibility and effectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm with certain optimization objectives. The source code can be found in https://github.com/jijiarui-cather/SRAPAgent_Framework.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.15",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "LLM Questionnaire Completion for Automatic Psychiatric Assessment": {
        "type": "INPROCEEDINGS",
        "key": "rosenman-etal-2024-llm",
        "author": "Rosenman, Gony and Hendler, Talma and Wolf, Lior",
        "booktitle": "EMNLP-findings2024",
        "title": "LLM Questionnaire Completion for Automatic Psychiatric Assessment",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We employ a Large Language Model (LLM) to convert unstructured psychological interviews into structured questionnaires spanning various psychiatric and personality domains. The LLM is prompted to answer these questionnaires by impersonating the interviewee. The obtained answers are coded as features, which are used to predict standardized psychiatric measures of depression (PHQ-8) and PTSD (PCL-C), using a Random Forest regressor. Our approach is shown to enhance diagnostic accuracy compared to multiple baselines. It thus establishes a novel framework for interpreting unstructured psychological interviews, bridging the gap between narrative-driven and data-driven approaches for mental health assessment.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.23",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "TransLLaMa: LLM-based Simultaneous Translation System": {
        "type": "INPROCEEDINGS",
        "key": "koshkin-etal-2024-transllama",
        "author": "Koshkin, Roman and Sudoh, Katsuhito and Nakamura, Satoshi",
        "booktitle": "EMNLP-findings2024",
        "title": "TransLLaMa: LLM-based Simultaneous Translation System",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers. This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special \u201cwait\u201d token. This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.27",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Reformatted Alignment": {
        "type": "INPROCEEDINGS",
        "key": "fan-etal-2024-reformatted",
        "author": "Fan, Run-Ze and Li, Xuefeng and Zou, Haoyang and Li, Junlong and He, Shwai and Chern, Ethan and Hu, Jiewen and Liu, Pengfei",
        "booktitle": "EMNLP-findings2024",
        "title": "Reformatted Alignment",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B\u2019s mathematical reasoning ability on GSM8K can be improved **from 46.77% to 56.63%** in accuracy. Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the Alpaca dataset. This work highlights the need for further research into the science and mechanistic interpretability of LLMs. We have made the associated code and data publicly accessible to support future studies at https://github.com/GAIR-NLP/ReAlign.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.32",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization": {
        "type": "INPROCEEDINGS",
        "key": "schnabel-neville-2024-symbolic",
        "author": "Schnabel, Tobias and Neville, Jennifer",
        "booktitle": "EMNLP-findings2024",
        "title": "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In many modern LLM applications, such as retrieval augmented generation, prompts have become programs themselves. In these settings, prompt programs are repeatedly called with different user queries or data instances. A big practical challenge is optimizing such prompt programs. Recent work has mostly focused on either simple prompt programs or assumed that the structure of a prompt program is fixed.We introduce SAMMO, a framework to perform symbolic prompt program search for compile-time optimizations of prompt programs. SAMMO represents prompt programs on a symbolic level which allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs. We make all code available open-source at https://anonymous.4open.science/r/sammo-4003/.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.37",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "LLM-supertagger: Categorial Grammar Supertagging via Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "zhao-penn-2024-llm",
        "author": "Zhao, Jinman and Penn, Gerald",
        "booktitle": "EMNLP-findings2024",
        "title": "LLM-supertagger: Categorial Grammar Supertagging via Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Supertagging is an essential task in Categorical grammar parsing and is crucial for dissecting sentence structures. Our research explores the capacity of Large Language Models (LLMs) in supertagging for both Combinatory Categorial Grammar (CCG) and Lambek Categorial Grammar (LCG). We also present a simple method that significantly boosts LLMs, enabling them to outperform LSTM and encoder-based models and achieve state-of-the-art performance. This advancement highlights LLMs\u2019 potential in classification tasks, showcasing their adaptability beyond generative capabilities. Our findings demonstrate the evolving utility of LLMs in natural language processing, particularly in complex tasks like supertagging.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.39",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-introducing",
        "author": "Zhang, Shuoming and Zhao, Jiacheng and Xia, Chunwei and Wang, Zheng and Chen, Yunji and Cui, Huimin",
        "booktitle": "EMNLP-findings2024",
        "title": "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Compilers are complex software containing millions of lines of code, taking years to develop. This paper investigates to what extent Large Language Models (LLMs) can replace hand-crafted compilers in translating high-level programming languages to machine instructions, using C to x86 assembly as a case study. We identify two challenges of using LLMs for code translation and introduce two novel data pre-processing techniques to address the challenges: numerical value conversion and training data resampling. While only using a 13B model, our approach achieves a behavioral accuracy of over 91%, outperforming the much larger GPT-4 Turbo model by over 50%. Our results are encouraging, showing that LLMs have the potential to transform how compilation tools are constructed.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.55",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "SAFETY-J: Evaluating Safety with Critique": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-safety",
        "author": "Liu, Yixiu and Zheng, Yuxiang and Xia, Shijie and Li, Jiajun and Tu, Yi and Song, Chaoling and Liu, Pengfei",
        "booktitle": "EMNLP-findings2024",
        "title": "SAFETY-J: Evaluating Safety with Critique",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The deployment of Large Language Models (LLMs) in content generation raises significant safety concerns, particularly regarding the transparency and interpretability of content evaluations. Current methods, primarily focused on binary safety classifications, lack mechanisms for detailed critique, limiting their utility for model improvement and user trust. To address these limitations, we introduce SAFETY-J, a bilingual generative safety evaluator for English and Chinese with critique-based judgment. SAFETY-J utilizes a robust training dataset that includes diverse dialogues and augmented query-response pairs to assess safety across various scenarios comprehensively. We establish an automated meta-evaluation benchmark that objectively assesses the quality of critiques with minimal human intervention, facilitating scalable and continuous improvement. Additionally, SAFETY-Jemploys an iterative preference learning technique to dynamically refine safety assessments based on meta-evaluations and critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced and accurate safety evaluations, thereby enhancing both critique quality and predictive reliability in complex content scenarios. To facilitate further research and application, we have released SAFETY-J\u2019s training protocols, datasets, and code at https://github.com/GAIR-NLP/Safety-J.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.64",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Dynamic Planning for LLM-based Graphical User Interface Automation": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-dynamic",
        "author": "Zhang, Shaoqing and Zhang, Zhuosheng and Chen, Kehai and Ma, Xinbei and Yang, Muyun and Zhao, Tiejun and Zhang, Min",
        "booktitle": "EMNLP-findings2024",
        "title": "Dynamic Planning for LLM-based Graphical User Interface Automation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The advent of large language models (LLMs) has spurred considerable interest in advancing autonomous LLMs-based agents, particularly in intriguing applications within smartphone graphical user interfaces (GUIs). When presented with a task goal, these agents typically emulate human actions within a GUI environment until the task is completed. However, a key challenge lies in devising effective plans to guide action prediction in GUI tasks, though planning have been widely recognized as effective for decomposing complex tasks into a series of steps. Specifically, given the dynamic nature of environmental GUIs following action execution, it is crucial to dynamically adapt plans based on environmental feedback and action history.We show that the widely-used ReAct approach fails due to the excessively long historical dialogues. To address this challenge, we propose a novel approach called Dynamic Planning of Thoughts (D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of planning based on the environmental feedback and execution history. Experimental results reveal that the proposed D-PoT significantly surpassed the strong GPT-4V baseline by +12.7% (34.66% \\rightarrow 47.36%) in accuracy. The analysis highlights the generality of dynamic planning in different backbone LLMs, as well as the benefits in mitigating hallucinations and adapting to unseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.70",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Large Language Model-based Human-Agent Collaboration for Complex Task Solving": {
        "type": "INPROCEEDINGS",
        "key": "feng-etal-2024-large",
        "author": "Feng, Xueyang and Chen, Zhi-Yuan and Qin, Yujia and Lin, Yankai and Chen, Xu and Liu, Zhiyuan and Wen, Ji-Rong",
        "booktitle": "EMNLP-findings2024",
        "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. To tackle the problem, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC, which trains a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We conduct experiments under real and simulated human-agent collaboration scenarios. Experimental results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC/.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.72",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards": {
        "type": "INPROCEEDINGS",
        "key": "hwang-etal-2024-self",
        "author": "Hwang, Hyeonbin and Kim, Doyoung and Kim, Seungone and Ye, Seonghyeon and Seo, Minjoon",
        "booktitle": "EMNLP-findings2024",
        "title": "Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Training on large amounts of rationales (i.e., CoT Fine-tuning) has been found effective for improving mathematical reasoning of large language models (LLMs). However, acquiring human-authored solutions or augmenting rationales from proprietary models is costly and not scalable. In this paper, we study the problem of whether LLMs could self-improve mathematical reasoning capabilities. To this end, we propose Self-Explore, where the LLM is tasked to explore the first wrong step (i.e., the first pit) within the rationale and use such signals as fine-grained rewards for further improvement. On the GSM8K and MATH test set, Self-Explore achieves 11.57% and 2.89% improvement on average across three LLMs compared to supervised fine-tuning (SFT). Our code is available here]9.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.78",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Tokenization Falling Short: On Subword Robustness in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chai-etal-2024-tokenization",
        "author": "Chai, Yekun and Fang, Yewei and Peng, Qiwei and Li, Xuhong",
        "booktitle": "EMNLP-findings2024",
        "title": "Tokenization Falling Short: On Subword Robustness in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Language models typically tokenize raw text into sequences of subword identifiers from a predefined vocabulary, a process inherently sensitive to typographical errors, length variations, and largely oblivious to the internal structure of tokens\u2014issues we term *the curse of tokenization*. In this study, we delve into these drawbacks and demonstrate that large language models (LLMs) remain susceptible to these problems. This study systematically investigates these challenges and their impact on LLMs through three critical research questions: (1) complex problem solving, (2) token structure probing, and (3) resilience to typographical variation. Our findings reveal that scaling model parameters can mitigate the issue of tokenization; however, LLMs still suffer from biases induced by typos and other text format variations. Our experiments show that subword regularization such as BPE-dropout can mitigate this issue. We release our evaluation code and data at https://github.com/FloatAI/TKEval.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.86",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MATE: Meet At The Embedding - Connecting Images with Long Texts": {
        "type": "INPROCEEDINGS",
        "key": "jang-etal-2024-mate",
        "author": "Jang, Young Kyun and Kang, Junmo and Lee, Yong Jae and Kim, Donghyun",
        "booktitle": "EMNLP-findings2024",
        "title": "MATE: Meet At The Embedding - Connecting Images with Long Texts",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While advancements in Vision Language Models (VLMs) have significantly improved the alignment of visual and textual data, these models primarily focus on aligning images with short descriptive captions. This focus limits their ability to handle complex text interactions, particularly with longer texts such as lengthy captions or documents, which have not been extensively explored yet. In this paper, we introduce Meet At The Embedding (MATE), a novel approach that combines the capabilities of VLMs with Large Language Models (LLMs) to overcome this challenge without the need for additional image-long text pairs. Specifically, we replace the text encoder of the VLM with a pretrained LLM-based encoder that excels in understanding long texts. To bridge the gap between VLM and LLM, MATE incorporates a projection module that is trained in a multi-stage manner. It starts by aligning the embeddings from the VLM text encoder with those from the LLM using extensive text pairs. This module is then employed to seamlessly align image embeddings closely with LLM embeddings. We propose two new cross-modal retrieval benchmarks to assess the task of connecting images with long texts (lengthy captions / documents). Extensive experimental results demonstrate that MATE effectively connects images with long texts, uncovering diverse semantic relationships.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.90",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-sifo",
        "author": "Chen, Xinyi and Liao, Baohao and Qi, Jirui and Eustratiadis, Panagiotis and Monz, Christof and Bisazza, Arianna and de Rijke, Maarten",
        "booktitle": "EMNLP-findings2024",
        "title": "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Following multiple instructions is a crucial ability for large language models (LLMs). Evaluating this ability comes with significant challenges: (i) limited coherence between multiple instructions, (ii) positional bias where the order of instructions affects model performance, and (iii) a lack of objectively verifiable tasks. To address these issues, we introduce a benchmark designed to evaluate models\u2019 abilities to follow multiple instructions through sequential instruction following (SIFo) tasks. In SIFo, the successful completion of multiple instructions is verifiable by examining only the final instruction. Our benchmark evaluates instruction following using four tasks (text modification, question answering, mathematics, and security rule following), each assessing different aspects of sequential instruction following. Our evaluation of popular LLMs, both closed-source and open-source, shows that more recent and larger models significantly outperform their older and smaller counterparts on the SIFo tasks, validating the benchmark\u2019s effectiveness. All models struggle with following sequences of instructions, hinting at an important lack of robustness of today\u2019s language models.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.92",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Exploring the Best Practices of Query Expansion with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-exploring-best",
        "author": "Zhang, Le and Wu, Yihong and Yang, Qian and Nie, Jian-Yun",
        "booktitle": "EMNLP-findings2024",
        "title": "Exploring the Best Practices of Query Expansion with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) are foundational in language technologies, particularly in information retrieval (IR). In this paper, we thoroughly explore the best practice of leveraging LLMs for query expansion. To this end, we introduce a training-free, straightforward yet effective framework called Multi-Text Generation Integration (MuGI). This approach leverages LLMs to generate multiple pseudo-references, which are then integrated with the original queries to enhance both sparse and dense retrieval methods. Additionally, we introduce a retrieval pipeline based on MuGI, which combines the strengths of sparse and dense retrievers to achieve superior performance without the need for costly pre-indexing. Our empirical findings reveal that: (1) Increasing the number of samples from LLMs benefits IR systems; (2) A balance between the query and pseudo-documents, and an effective integration strategy, is critical for high performance; (3) Contextual information from LLMs is essential, even boost a 23M model to outperform a 7B baseline model; (4) Pseudo relevance feedback can further calibrate queries for improved performance; and (5) Query expansion is widely applicable and versatile, consistently enhancing models ranging from 23M to 7B parameters. Our code and all generated references are made available at https://github.com/lezhang7/Retrieval_MuGI.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.103",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "yao-etal-2024-layer",
        "author": "Yao, Kai and Gao, Penglei and Li, Lichun and Zhao, Yuan and Wang, Xiaofeng and Wang, Wei and Zhu, Jianke",
        "booktitle": "EMNLP-findings2024",
        "title": "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant popularity for adapting pre-trained Large Language Models (LLMs) to downstream tasks, primarily due to their potential to significantly reduce memory and computational overheads. However, a common limitation in most PEFT approaches is their application of a uniform architectural design across all layers. This uniformity involves identical trainable modules and ignores the varying importance of each layer, leading to sub-optimal fine-tuning results. To overcome the above limitation and obtain better performance, we develop a novel approach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent sparsity and select the most important subset of full layers with effective layer-wise importance scoring. The proposed IST is a versatile and plug-and-play technique compatible with various PEFT methods that operate on a per-layer basis. By leveraging the estimated importance scores, IST dynamically updates these selected layers in PEFT modules, leading to reduced memory demands. We further provide theoretical proof of convergence and empirical evidence of superior performance to demonstrate the advantages of IST over uniform updating strategies. Extensive experiments on a range of LLMs, PEFTs, and downstream tasks substantiate the effectiveness of our proposed method, showcasing IST\u2019s capacity to enhance existing layer-based PEFT methods. Our code is available at https://github.com/Kaiseem/IST",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.109",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-controlling",
        "author": "Chen, Lu and Zhang, Ruqing and Guo, Jiafeng and Fan, Yixing and Cheng, Xueqi",
        "booktitle": "EMNLP-findings2024",
        "title": "Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Retrieval-augmented generation (RAG) has emerged as a popular solution to mitigate the hallucination issues of large language models. However, existing studies on RAG seldom address the issue of predictive uncertainty, i.e., how likely it is that a RAG model\u2019s prediction is incorrect, resulting in uncontrollable risks in real-world applications. In this work, we emphasize the importance of risk control, ensuring that RAG models proactively refuse to answer questions with low confidence. Our research identifies two critical latent factors affecting RAG\u2019s confidence in its predictions: the quality of the retrieved results and the manner in which these results are utilized. To guide RAG models in assessing their own confidence based on these two latent factors, we develop a counterfactual prompting framework that induces the models to alter these factors and analyzes the effect on their answers. We also introduce a benchmarking procedure to collect answers with the option to abstain, facilitating a series of experiments. For evaluation, we introduce several risk-related metrics and the experimental results demonstrate the effectiveness of our approach. Our code and benchmark dataset are available at https://github.com/ict-bigdatalab/RC-RAG.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.133",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Learning to Paraphrase for Alignment with LLM Preference": {
        "type": "INPROCEEDINGS",
        "key": "fu-etal-2024-learning",
        "author": "Fu, Junbo and Zhao, Guoshuai and Deng, Yimin and Mi, Yunqi and Qian, Xueming",
        "booktitle": "EMNLP-findings2024",
        "title": "Learning to Paraphrase for Alignment with LLM Preference",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) exhibit the issue of paraphrase divergence. This means that when a question is phrased in a slightly different but semantically similar way, LLM may output a wrong response despite being able to answer the original question correctly. Previous research has regarded this issue as a problem of the model\u2019s robustness to question paraphrase and proposed a retraining method to address it. However, retraining faces challenges in meeting the computational costs and privacy security demands of LLMs. In this paper, we regard this issue as a problem of alignment with model preferences and propose PEARL (Preference-drivEn pAraphRase Learning). This is a black-box method that enhances model performance by paraphrasing questions in expressions preferred by the model. We validate PEARL across six datasets spanning three tasks: open-domain QA, commonsense reasoning, and math word problem. Extensive experiments demonstrated not only the outstanding performance but also the composability, transferability, and immense potential of PEARL, shedding new light on the black-box tuning of LLMs.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.134",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "EvoR: Evolving Retrieval for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "su-etal-2024-evor",
        "author": "Su, Hongjin and Jiang, Shuyang and Lai, Yuhang and Wu, Haoyuan and Shi, Boao and Liu, Che and Liu, Qian and Yu, Tao",
        "booktitle": "EMNLP-findings2024",
        "title": "EvoR: Evolving Retrieval for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently the retrieval-augmented generation (RAG) has been successfully applied in code generation. However, existing pipelines for retrieval-augmented code generation (RACG) employ static knowledge bases with a single source, limiting the adaptation capabilities of Large Language Models (LLMs) to domains they have insufficient knowledge of. In this work, we develop a novel pipeline, EVOR, that employs the synchronous evolution of both queries and diverse knowledge bases. On two realistic settings where the external knowledge is required to solve code generation tasks, we compile four new datasets associated with frequently updated libraries and long-tail programming languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR achieves two to four times of execution accuracy compared to other methods such as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We demonstrate that EVOR is flexible and can be easily combined with them to achieve further improvement. Further analysis reveals that EVOR benefits from the synchronous evolution of queries and documents and the diverse information sources in the knowledge base. We hope that our studies will inspire more insights into the design of advanced RACG pipelines in future research.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.143",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models": {
        "type": "INPROCEEDINGS",
        "key": "zhou-etal-2024-navigating-shortcut",
        "author": "Zhou, Yuqing and Tang, Ruixiang and Yao, Ziyu and Zhu, Ziwei",
        "booktitle": "EMNLP-findings2024",
        "title": "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Language models (LMs), despite their advances, often depend on spurious correlations, undermining their accuracy and generalizability. This study addresses the overlooked impact of subtler, more complex shortcuts that compromise model reliability beyond oversimplified shortcuts. We introduce a comprehensive benchmark that categorizes shortcuts into occurrence, style, and concept, aiming to explore the nuanced ways in which these shortcuts influence the performance of LMs. Through extensive experiments across traditional LMs, large language models, and state-of-the-art robust models, our research systematically investigates models\u2019 resilience and susceptibilities to sophisticated shortcuts. Our benchmark and code can be found at: https://github.com/yuqing-zhou/shortcut-learning-in-text-classification.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.146",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Privacy Evaluation Benchmarks for NLP Models": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-privacy",
        "author": "Huang, Wei and Wang, Yinggui and Chen, Cen",
        "booktitle": "EMNLP-findings2024",
        "title": "Privacy Evaluation Benchmarks for NLP Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "By inducing privacy attacks on NLP models, attackers can obtain sensitive information such as training data and model parameters, etc. Although researchers have studied, in-depth, several kinds of attacks in NLP models, they are non-systematic analyses. It lacks a comprehensive understanding of the impact caused by the attacks. For example, we must consider which scenarios can apply to which attacks, what the common factors are that affect the performance of different attacks, the nature of the relationships between different attacks, and the influence of various datasets and models on the effectiveness of the attacks, etc. Therefore, we need a benchmark to holistically assess the privacy risks faced by NLP models. In this paper, we present a privacy attack and defense evaluation benchmark in the field of NLP, which includes the conventional/small models and large language models (LLMs). This benchmark supports a variety of models, datasets, and protocols, along with standardized modules for comprehensive evaluation of attacks and defense strategies. Based on the above framework, we present a study on the association between auxiliary data from different domains and the strength of privacy attacks. And we provide an improved attack method in this scenario with the help of Knowledge Distillation (KD). Furthermore, we propose a chained framework for privacy attacks. Allowing a practitioner to chain multiple attacks to achieve a higher-level attack objective. Based on this, we provide some defense and enhanced attack strategies. The code for reproducing the results can be found at https://anonymous.4open.science/r/nlp_doctor-AF48",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.147",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Typos that Broke the RAG\u2019s Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations": {
        "type": "INPROCEEDINGS",
        "key": "cho-etal-2024-typos",
        "author": "Cho, Sukmin and Jeong, Soyeong and Seo, Jeongyeon and Hwang, Taeho and Park, Jong C.",
        "booktitle": "EMNLP-findings2024",
        "title": "Typos that Broke the RAG\u2019s Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (GARAG), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our GARAG to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world. Code is available at https://github.com/zomss/GARAG.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.161",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Enhancing Temporal Modeling of Video LLMs via Time Gating": {
        "type": "INPROCEEDINGS",
        "key": "hu-etal-2024-enhancing",
        "author": "Hu, Zi-Yuan and Zhong, Yiwu and Huang, Shijia and Lyu, Michael and Wang, Liwei",
        "booktitle": "EMNLP-findings2024",
        "title": "Enhancing Temporal Modeling of Video LLMs via Time Gating",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Video Large Language Models (Video LLMs) have achieved impressive performance on video-and-language tasks, such as video question answering. However, most existing Video LLMs neglect temporal information in video data, leading to struggles with temporal-aware video understanding. To address this gap, we propose a Time Gating Video LLM (TG-Vid) designed to enhance temporal modeling through a novel Time Gating module (TG). The TG module employs a time gating mechanism on its sub-modules, comprising gating spatial attention, gating temporal attention, and gating MLP. This architecture enables our model to achieve a robust understanding of temporal information within videos. Extensive evaluation of temporal-sensitive video benchmarks (i.e., MVBench, TempCompass, and NExT-QA) demonstrates that our TG-Vid model significantly outperforms the existing Video LLMs. Further, comprehensive ablation studies validate that the performance gains are attributed to the designs of our TG module. Our code is available at https://github.com/LaVi-Lab/TG-Vid.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.162",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-ruler",
        "author": "Li, Jiaming and Zhang, Lei and Li, Yunshui and Liu, Ziqiang and Bai, Yuelin and Luo, Run and Chen, Longze and Yang, Min",
        "booktitle": "EMNLP-findings2024",
        "title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users\u2019 needs due to their inherent difficulty in accurately perceiving numerical constraints. To explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the model\u2019s performance in adhering to specified response lengths. Furthermore, we introduce a novel, model-agnostic approach called Ruler, which employs Meta Length Tokens (MLTs) to enhance the instruction-following ability of large language models under length-constrained instructions. Specifically, Ruler equips LLMs with the ability to generate responses of a specified length based on length constraints within the instructions. Moreover, Ruler can automatically generate appropriate MLT when length constraints are not explicitly provided, demonstrating excellent versatility and generalization. Comprehensive experiments show the effectiveness of Ruler across different LLMs on Target Length Generation Task, e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In addition, we conduct extensive ablation experiments to further substantiate the efficacy and generalization of Ruler. Our code and data is available on the internet.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.172",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding": {
        "type": "INPROCEEDINGS",
        "key": "hu-etal-2024-mplug",
        "author": "Hu, Anwen and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Zhang, Liang and Zhang, Bo and Zhang, Ji and Jin, Qin and Huang, Fei and Zhou, Jingren",
        "booktitle": "EMNLP-findings2024",
        "title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts. Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images. In this work, we emphasize the importance of structure information in Visual Document Understanding and propose Unified Structure Learning to boost the performance of MLLMs. Based on publicly available text-rich images, we build a comprehensive training set DocStruct4M to support structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image. To better encode structure information, we design a simple and effective vision-to-text module H-Reducer, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently. Our model DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks. All codes, models, and datasets are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.175",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Double-Checker: Large Language Model as a Checker for Few-shot Named Entity Recognition": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-double",
        "author": "Chen, Wei and Zhao, Lili and Zheng, Zhi and Xu, Tong and Wang, Yang and Chen, Enhong",
        "booktitle": "EMNLP-findings2024",
        "title": "Double-Checker: Large Language Model as a Checker for Few-shot Named Entity Recognition",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, few-shot Named Entity Recognition (NER) has attracted significant attention due to the high cost of obtaining high-quality labeled data. Decomposition-based methods have demonstrated remarkable performance on this task, which initially train a type-independent span detector and subsequently classify the detected spans based on their types. However, this framework has an evident drawback as a domain-agnostic detector cannot ensure the identification of only those entity spans that are specific to the target domain. To address this issue, we propose Double-Checker, which leverages collaboration between Large Language Models (LLMs) and small models. Specifically, we employ LLMs to verify candidate spans predicted by the small model and eliminate any spans that fall outside the scope of the target domain. Extensive experiments validate the effectiveness of our method, consistently yielding improvements over two baseline approaches. Our code is available at https://github.com/fanshu6hao/Double-Checker.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.180",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization": {
        "type": "INPROCEEDINGS",
        "key": "nahid-rafiei-2024-normtab",
        "author": "Nahid, Md and Rafiei, Davood",
        "booktitle": "EMNLP-findings2024",
        "title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in parsing textual data and generating code. However, their performance in tasks involving tabular data, especially those requiring symbolic reasoning, faces challenges due to the structural variance and inconsistency in table cell values often found in web tables. In this paper, we introduce NormTab, a novel framework aimed at enhancing the symbolic reasoning performance of LLMs by normalizing web tables. We study table normalization as a stand-alone, one-time preprocessing step using LLMs to support symbolic reasoning on tabular data. Our experimental evaluation, conducted on challenging web table datasets such as WikiTableQuestion and TabFact, demonstrates that leveraging NormTab significantly improves symbolic reasoning performance, showcasing the importance and effectiveness of web table normalization for enhancing LLM-based symbolic reasoning tasks.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.203",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Sanitizing Large Language Models in Bug Detection with Data-Flow": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-sanitizing",
        "author": "Wang, Chengpeng and Zhang, Wuqi and Su, Zian and Xu, Xiangzhe and Zhang, Xiangyu",
        "booktitle": "EMNLP-findings2024",
        "title": "Sanitizing Large Language Models in Bug Detection with Data-Flow",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) show potential in code reasoning tasks, facilitating the customization of detecting bugs in software development. However, the hallucination effect can significantly compromise the reliability of bug reports. This work formulates a new schema of bug detection and presents a novel sanitization technique that detects false positives for hallucination mitigation. Our key idea is to enforce LLMs to emit data-flow paths in few-shot chain-of-thought prompting and validate them via the program-property decomposition. Specifically, we dissect data-flow paths into basic properties upon concise code snippets and leverage parsing-based analysis and LLMs for validation. Our approach averagely achieves 91.03% precision and 74.00% recall upon synthetic benchmarks and boosts the precision by 21.99% with the sanitization. The evaluation upon real-world Android malware applications also demonstrates the superiority over an industrial analyzer, surpassing the precision and recall by 15.36% and 3.61%, respectively.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.217",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "When and Where Did it Happen? An Encoder-Decoder Model to Identify Scenario Context": {
        "type": "INPROCEEDINGS",
        "key": "noriega-atala-etal-2024-happen",
        "author": "Noriega-Atala, Enrique and Vacareanu, Robert and Ashton, Salena Torres and Pyarelal, Adarsh and Morrison, Clayton T. and Surdeanu, Mihai",
        "booktitle": "EMNLP-findings2024",
        "title": "When and Where Did it Happen? An Encoder-Decoder Model to Identify Scenario Context",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We introduce a neural architecture finetuned for the task of scenario context generation: The relevant location and time of an event or entity mentioned in text. Contextualizing information extraction helps to scope the validity of automated finings when aggregating them as knowledge graphs. Our approach uses a high-quality curated dataset of time and location annotations in a corpus of epidemiology papers to train an encoder-decoder architecture. We also explored the use of data augmentation techniques during training. Our findings suggest that a relatively small fine-tuned encoder-decoder model performs better than out-of-the-box LLMs and semantic role labeling parsers to accurate predict the relevant scenario information of a particular entity or event.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.219",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation": {
        "type": "INPROCEEDINGS",
        "key": "fu-etal-2024-autorag",
        "author": "Fu, Jia and Qin, Xiaoting and Yang, Fangkai and Wang, Lu and Zhang, Jue and Lin, Qingwei and Chen, Yubo and Zhang, Dongmei and Rajmohan, Saravan and Zhang, Qi",
        "booktitle": "EMNLP-findings2024",
        "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advancements in Large Language Models have transformed ML/AI development, necessitating a reevaluation of AutoML principles for the Retrieval-Augmented Generation (RAG) systems. To address the challenges of hyper-parameter optimization and online adaptation in RAG, we propose the AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical MAB (Hier-MAB) method for efficient exploration of large search spaces. We conduct extensive experiments on tuning hyper-parameters, such as top-k retrieved documents, prompt compression ratio, and embedding methods, using the ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly optimization all three hyper-parameters demonstrate that MAB-based online learning methods can achieve Recall@5 \\approx 0.8 for scenarios with prominent gradients in search space, using only ~20% of the LLM API calls required by the Grid Search approach. Additionally, the proposed Hier-MAB approach outperforms other baselines in more challenging optimization scenarios. The code will be made available at https://aka.ms/autorag.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.223",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Revisiting Query Variation Robustness of Transformer Models": {
        "type": "INPROCEEDINGS",
        "key": "hagen-etal-2024-revisiting",
        "author": "Hagen, Tim and Scells, Harrisen and Potthast, Martin",
        "booktitle": "EMNLP-findings2024",
        "title": "Revisiting Query Variation Robustness of Transformer Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The most commonly used transformers for retrieval at present, BERT and T5, have been shown not to be robust to query variations such as typos or paraphrases. Although this is an important prerequisite for their practicality, this problem has hardly been investigated. More recent large language models (LLMs), including instruction-tuned LLMs, have not been analyzed yet, and only one study looks beyond typos. We close this gap by reproducing this study and extending it with a systematic analysis of more recent models, including Sentence-BERT, CharacterBERT, E5-Mistral, AnglE, and Ada v2. We further investigate if instruct-LLMs can be prompted for robustness. Our results are mixed in that the previously observed robustness issues for cross-encoders also apply to bi-encoders that use much larger LLMs, albeit to a lesser extent. While further LLM scaling may improve their embeddings, their cost-effective use for all but large deployments is limited. Training data that includes query variations allows LLMs to be fine-tuned for more robustness, but focusing on a single category of query variation may even degrade the effectiveness on others. Our code, results, and artifacts can be found at https://github.com/webis-de/EMNLP-24",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.248",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully": {
        "type": "INPROCEEDINGS",
        "key": "kai-etal-2024-sh2",
        "author": "Kai, Jushi and Zhang, Tianhang and Hu, Hai and Lin, Zhouhan",
        "booktitle": "EMNLP-findings2024",
        "title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that these low-confidence tokens are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to \u201dhighlight\u201d the factual information by selecting key tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in output probabilities brought by the hesitation. Experimental results demonstrate that our SH2, requiring no additional data or models, can effectively help LLMs elicit factual knowledge and distinguish hallucinated contexts by themselves. Significant and consistent improvements are achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on various hallucination tasks.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.260",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "WavLLM: Towards Robust and Adaptive Speech Large Language Model": {
        "type": "INPROCEEDINGS",
        "key": "hu-etal-2024-wavllm",
        "author": "Hu, Shujie and Zhou, Long and Liu, Shujie and Chen, Sanyuan and Meng, Lingwei and Hao, Hongkun and Pan, Jing and Liu, Xunying and Li, Jinyu and Sivasankaran, Sunit and Liu, Linquan and Wei, Furu",
        "booktitle": "EMNLP-findings2024",
        "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advancements in large language models (LLMs) have expanded their scope in natural language processing (NLP) to encompass multimodal functions. However, integrating listening capabilities effectively remains a significant challenge for generalization and complex auditory task execution. In this work, we introduce WavLLM, a robust and adaptive speech large language model featuring dual encoders\u2014a Whisper encoder for semantics and a WavLM encoder for speaker characteristics. Within the two-stage curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks and also apply it to specialized speech-question-answer (SQA) dataset, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. The codes, models, audio samples, and SQA evaluation set can be accessed at https://github.com/microsoft/SpeechT5/tree/main/WavLLM.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.263",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Improving Argument Effectiveness Across Ideologies using Instruction-tuned Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "el-baff-etal-2024-improving",
        "author": "El Baff, Roxanne and Khatib, Khalid Al and Alshomary, Milad and Konen, Kai and Stein, Benno and Wachsmuth, Henning",
        "booktitle": "EMNLP-findings2024",
        "title": "Improving Argument Effectiveness Across Ideologies using Instruction-tuned Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Different political ideologies (e.g., liberal and conservative Americans) hold different worldviews, which leads to opposing stances on different issues (e.g., gun control) and, thereby, fostering societal polarization. Arguments are a means of bringing the perspectives of people with different ideologies closer together, depending on how well they reach their audience. In this paper, we study how to computationally turn ineffective arguments into effective arguments for people with certain ideologies by using instruction-tuned large language models (LLMs), looking closely at style features. For development and evaluation, we collect ineffective arguments per ideology from debate.org, and we generate about 30k, which we rewrite using three LLM methods tailored to our task: zero-shot prompting, few-shot prompting, and LLM steering. Our experiments provide evidence that LLMs naturally improve argument effectiveness for liberals. Our LLM-based and human evaluation show a clear preference towards the rewritten arguments. Code and link to the data are available here: https://github.com/roxanneelbaff/emnlp2024-iesta.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.265",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches": {
        "type": "INPROCEEDINGS",
        "key": "yuan-etal-2024-kv",
        "author": "Yuan, Jiayi and Liu, Hongyi and Zhong, Shaochen and Chuang, Yu-Neng and Li, Songchen and Wang, Guanchu and Le, Duy and Jin, Hongye and Chaudhary, Vipin and Xu, Zhaozhuo and Liu, Zirui and Hu, Xia",
        "booktitle": "EMNLP-findings2024",
        "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Long context capability is a crucial competency for large language models (LLMs) as it mitigates the human struggle to digest long-form texts. This capability enables complex task-solving scenarios such as book summarization, code assistance, and many more tasks that are traditionally manpower-intensive. However, transformer-based LLMs face significant challenges with long context input due to the growing size of the KV cache and the intrinsic complexity of attending to extended inputs; where multiple schools of efficiency-driven approaches \u2014 such as KV cache quantization, token dropping, prompt compression, linear-time sequence models, and hybrid architectures \u2014 have been proposed to produce efficient yet long context-capable models. Despite these advancements, no existing work has comprehensively benchmarked these methods in a reasonably aligned environment. In this work, we fill this gap by providing a taxonomy of current methods and evaluating 10+ state-of-the-art approaches across seven categories of long context tasks. Our work reveals numerous previously unknown phenomena and offers insights \u2014 as well as a friendly workbench \u2014 for the future development of long context-capable LLMs. The source code is available at https://github.com/henryzhongsc/longctx_bench.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.266",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "shi-etal-2024-math",
        "author": "Shi, Wenhao and Hu, Zhiqiang and Bin, Yi and Liu, Junhua and Yang, Yang and Ng, See-Kiong and Bing, Lidong and Lee, Roy Ka-Wei",
        "booktitle": "EMNLP-findings2024",
        "title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated impressive reasoning capabilities, particularly in textual mathematical problem-solving. However, existing open-source image instruction fine-tuning datasets, containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap, we address the lack of high-quality, diverse multimodal mathematical datasets by collecting 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs, creating the MathV360K dataset, which enhances both the breadth and depth of multimodal mathematical questions. We introduce Math-LLaVA, a LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista\u2019s minitest split, and yielding leading performance on Math-V and MathVerse. Furthermore, Math-LLaVA demonstrates enhanced generalizability, showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs\u2019 mathematical reasoning abilities. The code and data are available at: https://github.com/HZQ950419/Math-LLaVA.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.268",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Geneverse: A Collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-geneverse",
        "author": "Liu, Tianyu and Xiao, Yijia and Luo, Xiao and Xu, Hua and Zheng, Wenjin and Zhao, Hongyu",
        "booktitle": "EMNLP-findings2024",
        "title": "Geneverse: A Collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The applications of large language models (LLMs) are promising for biomedical and healthcare research. Despite the availability of open-source LLMs trained using a wide range of biomedical data, current research on the applications of LLMs to genomics and proteomics is still limited. To fill this gap, we propose a collection of finetuned LLMs and multimodal LLMs (MLLMs), known as Geneverse, for three novel tasks in genomic and proteomic research. The models in Geneverse are trained and evaluated based on domain-specific datasets, and we use advanced parameter-efficient finetuning techniques to achieve the model adaptation for tasks including the generation of descriptions for gene functions, protein function inference from its structure, and marker gene selection from spatial transcriptomic data. We demonstrate that adapted LLMs and MLLMs perform well for these tasks and may outperform closed-source large-scale models based on our evaluations focusing on both truthfulness and structural correctness. All of the training strategies and base models we used are freely accessible. Our codes can be found at https://github.com/HelloWorldLTY/Geneverse.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.277",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Stanceformer: Target-Aware Transformer for Stance Detection": {
        "type": "INPROCEEDINGS",
        "key": "garg-caragea-2024-stanceformer",
        "author": "Garg, Krishna and Caragea, Cornelia",
        "booktitle": "EMNLP-findings2024",
        "title": "Stanceformer: Target-Aware Transformer for Stance Detection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The task of Stance Detection involves discerning the stance expressed in a text towards a specific subject or target. Prior works have relied on existing transformer models that lack the capability to prioritize targets effectively. Consequently, these models yield similar performance regardless of whether we utilize or disregard target information, undermining the task\u2019s significance. To address this challenge, we introduce Stanceformer, a target-aware transformer model that incorporates enhanced attention towards the targets during both training and inference. Specifically, we design a Target Awareness matrix that increases the self-attention scores assigned to the targets. We demonstrate the efficacy of the Stanceformer with various BERT-based models, including state-of-the-art models and Large Language Models (LLMs), and evaluate its performance across three stance detection datasets, alongside a zero-shot dataset. Our approach Stanceformer not only provides superior performance but also generalizes even to other domains, such as Aspect-based Sentiment Analysis. We make the code publicly available.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.286",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Learning Autonomous Driving Tasks via Human Feedbacks with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ma-etal-2024-learning",
        "author": "Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Cui, Can and Mei, Kai and Wang, Ziran",
        "booktitle": "EMNLP-findings2024",
        "title": "Learning Autonomous Driving Tasks via Human Feedbacks with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Traditional autonomous driving systems have mainly focused on making driving decisions without human interaction, overlooking human-like decision-making and human preference required in complex traffic scenarios. To bridge this gap, we introduce a novel framework leveraging Large Language Models (LLMs) for learning human-centered driving decisions from diverse simulation scenarios and environments that incorporate human feedback. Our contributions include a GPT-4-based programming planner that integrates seamlessly with the existing CARLA simulator to understand traffic scenes and react to human instructions. Specifically, we build a human-guided learning pipeline that incorporates human driver feedback directly into the learning process and stores optimal driving programming policy using Retrieval Augmented Generation (RAG). Impressively, our programming planner, with only 50 saved code snippets, can match the performance of baseline extensively trained reinforcement learning (RL) models. Our paper highlights the potential of an LLM-powered shared-autonomy system, pushing the frontier of autonomous driving system development to be more interactive and intuitive.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.287",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies": {
        "type": "INPROCEEDINGS",
        "key": "shi-etal-2024-culturebank",
        "author": "Shi, Weiyan and Li, Ryan and Zhang, Yutong and Ziems, Caleb and Yu, Sunny and Horesh, Raya and Paula, Rog\u00e9rio Abreu De and Yang, Diyi",
        "booktitle": "EMNLP-findings2024",
        "title": "CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "To enhance language models\u2019 cultural awareness, we design a generalizable pipeline to construct cultural knowledge bases from different online communities on a massive scale. With the pipeline, we construct CultureBank, a knowledge base built upon users\u2019 self-narratives with 12K cultural descriptors sourced from TikTok and 11K from Reddit. Unlike previous cultural knowledge resources, CultureBank contains diverse views on cultural descriptors to allow flexible interpretation of cultural knowledge, and contextualized cultural scenarios to help grounded evaluation. With CultureBank, we evaluate different LLMs\u2019 cultural awareness, and identify areas for improvement. We also fine-tune a language model on CultureBank: experiments show that it achieves better performances on two downstream cultural tasks in a zero-shot setting. Finally, we offer recommendations for future culturally aware language technologies. We release the CultureBank dataset, code and models at https://github.com/SALT-NLP/CultureBank. Our project page is at culturebank.github.io",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.288",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-defending-large",
        "author": "Zhao, Wei and Li, Zhe and Li, Yige and Zhang, Ye and Sun, Jun",
        "booktitle": "EMNLP-findings2024",
        "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs respond to harmful prompts and propose a novel defense method termed Layer-specific Editing (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical safety layers exist among the early layers of LLMs. We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from identified toxic layers can significantly improve the alignment of LLMs against jailbreak attacks. Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. Our code is available at https://github.com/ledllm/ledllm.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.293",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents": {
        "type": "INPROCEEDINGS",
        "key": "wu-etal-2024-shall",
        "author": "Wu, Zengqing and Peng, Run and Zheng, Shuyuan and Liu, Qianying and Han, Xu and Kwon, Brian I. and Onizuka, Makoto and Tang, Shaojie and Xiao, Chuan",
        "booktitle": "EMNLP-findings2024",
        "title": "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have increasingly been utilized in social simulations, where they are often guided by carefully crafted instructions to stably exhibit human-like behaviors during simulations. Nevertheless, we doubt the necessity of shaping agents\u2019 behaviors for accurate social simulations. Instead, this paper emphasizes the importance of spontaneous phenomena, wherein agents deeply engage in contexts and make adaptive decisions without explicit directions. We explored spontaneous cooperation across three competitive scenarios and successfully simulated the gradual emergence of cooperation, findings that align closely with human behavioral data. This approach not only aids the computational social science community in bridging the gap between simulations and real-world dynamics but also offers the AI community a novel method to assess LLMs\u2019 capability of deliberate reasoning.Our source code is available at https://github.com/wuzengqing001225/SABM_ShallWeTeamUp",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.297",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-calibrating",
        "author": "Li, Jiazheng and Xu, Hainiu and Sun, Zhaoyue and Zhou, Yuxiang and West, David and Aloisi, Cesare and He, Yulan",
        "booktitle": "EMNLP-findings2024",
        "title": "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Generating rationales that justify scoring decisions has been a promising way to facilitate explainability in automated scoring systems. However, existing methods do not match the accuracy of classifier-based methods. Plus, the generated rationales often contain hallucinated information. To address these issues, we propose a novel framework capable of generating more faithful rationales and, more importantly, matching performance with classifier-based black-box scoring systems. We first mimic the human assessment process by querying Large Language Models (LLMs) to generate a thought tree. We then summarise intermediate assessment decisions from each thought tree path for creating synthetic rationale data and rationale preference data. Finally, we utilise the generated synthetic data to calibrate LLMs through a two-step training process: supervised fine-tuning and preference optimization. Extensive experimental results demonstrate that our framework achieves a 38% assessment performance improvement in the QWK score compared to prior work while producing higher-quality rationales, as recognised by human evaluators and LLMs. Our work sheds light on the effectiveness of performing preference optimization using synthetic preference data obtained from thought tree paths. Data and code are available at: https://github.com/lijiazheng99/thought_tree_assessment.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.313",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Merely Judging Metaphor is Not Enough: Research on Reasonable Metaphor Detection": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-merely",
        "author": "Chen, Puli and Yang, Cheng and Huang, Qingbao",
        "booktitle": "EMNLP-findings2024",
        "title": "Merely Judging Metaphor is Not Enough: Research on Reasonable Metaphor Detection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Metaphor, as an advanced form of cognition, is challenging to understand their meaning. Current metaphor detection tasks only provide labels (i.e., metaphor or literal) without interpreting how to understand them. In this paper, we improve the metaphor detection task and explore the reason of metaphor. To the best of our knowledge, we are the first work to reason about metaphor using mainstream Large Language Models (LLMs). Specifically, we utilized ChatGPT3.5 to expand the mainstream datasets in current metaphor detection, including VUA ALL, TroFi, and MOH-X. We input the original sentence, target word, and usage (metaphor or literal) into ChatGPT, guiding it to generate corresponding metaphor reason. Then, we designed supervised baseline experiments (e.g., RoBERTa, GPT-2) and zero-shot experiments with LLMs (e.g., LLaMA3). For the results generated by the above experiments, we provided the case study. We devised four methods that include manual evaluation to evaluate the reason performance of the model, and discussed extensively the advantages and disadvantages of these evaluation methods. Our code is available at https://github.com/yc-cy/Metaphorical-Reasoning.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.336",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "SecureSQL: Evaluating Data Leakage of Large Language Models as Natural Language Interfaces to Databases": {
        "type": "INPROCEEDINGS",
        "key": "song-etal-2024-securesql",
        "author": "Song, Yanqi and Liu, Ruiheng and Chen, Shu and Ren, Qianhao and Zhang, Yu and Yu, Yongqi",
        "booktitle": "EMNLP-findings2024",
        "title": "SecureSQL: Evaluating Data Leakage of Large Language Models as Natural Language Interfaces to Databases",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the widespread application of Large Language Models (LLMs) in Natural Language Interfaces to Databases (NLIDBs), concerns about security issues in NLIDBs have been increasing gradually. However, research on sensitive data leakage in NLIDBs is relatively limited. Therefore, we propose a benchmark to assess the potential of language models to leak sensitive data when generating SQL queries. This benchmark covers 932 samples from 34 different domains, including medical, legal, financial, and political aspects. We evaluate 15 models from six LLM families, and the results show that the model with the best performance has an accuracy of 61.7%, whereas humans achieve an accuracy of 94%. Most models perform close to or even below the level of random selection. We also evaluate two common attack methods, namely prompt injection and inference attacks, as well as a defense method based on chain-of-thoughts (COT) prompting. Experimental results show that both attack methods significantly impact the model, while the defense method based on COT prompting dose not significantly improve accuracy, further highlighting the severity of sensitive data leakage issues in NLIDBs. We hope this research will draw more attention and further study from the researchers on this issue.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.346",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-llama",
        "author": "Chen, Tianxiang and Tan, Zhentao and Gong, Tao and Wu, Yue and Chu, Qi and Liu, Bin and Ye, Jieping and Yu, Nenghai",
        "booktitle": "EMNLP-findings2024",
        "title": "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "As a manner to augment pretrained large language models (LLM), knowledge injection is critical to develop vertical domain large models and has been widely studied. While most current approaches, including parameter-efficient fine-tuning (PEFT) and block expansion methods, uniformly apply knowledge across all LLM layers, it raises the question: are all layers equally crucial for knowledge injection? We embark upon evaluating the importance of each layer to locate the optimal layer range for knowledge injection. Intuitively, more important layers should play more critical roles in knowledge injection and deserve denser injection. We observe performance dips in question-answering benchmarks after the removal or expansion of the shallow layers, and the degradation shrinks as the layer gets deeper, indicating that the shallow layers hold the key to knowledge injection. This insight leads us to propose the S strategy, a post-pretraining strategy of selectively enhancing shallow layers while pruning the less effective deep ones. Based on this strategy, we introduce Llama Slayer 8B. We experimented on the corpus of code &amp; math and demonstrated the effectiveness of our strategy. Further experiments across different LLM, Mistral-7B, and a legal corpus confirmed the approach\u2019s general applicability, underscoring its wide-ranging efficacy.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.347",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "lv-etal-2024-coggpt",
        "author": "Lv, Yaojia and Pan, Haojie and Wang, Zekun and Liang, Jiafeng and Liu, Yuanxing and Fu, Ruiji and Liu, Ming and Wang, Zhongyuan and Qin, Bing",
        "booktitle": "EMNLP-findings2024",
        "title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Cognitive dynamics, which refer to the evolution in human cognitive processes, are pivotal to advance human understanding of the world. Recent advancements in large language models (LLMs) highlight their potential for cognitive simulation. However, these LLM-based cognitive studies primarily focus on replicating human cognition in specific contexts, overlooking the inherently dynamic nature of cognition. To bridge this gap, we explore the cognitive dynamics of LLMs and present a corresponding task inspired by longitudinal studies. Toward the task, we develop CogBench, a novel benchmark to assess the cognitive dynamics of LLMs and validate it through participant surveys. We also design two evaluation metrics for CogBench, including Authenticity and Rationality. Recognizing the inherent static nature of LLMs, we further introduce CogGPT for the task, which features an innovative iterative cognitive mechanism to develop lifelong cognitive dynamics. Empirical results demonstrate the superiority of CogGPT over several existing methods, particularly in its ability to facilitate role-specific cognitive dynamics under continuous information flows. We will release the code and data to enable further research.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.352",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric": {
        "type": "INPROCEEDINGS",
        "key": "koh-etal-2024-llms",
        "author": "Koh, Hyukhun and Kim, Dohyung and Lee, Minwoo and Jung, Kyomin",
        "booktitle": "EMNLP-findings2024",
        "title": "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to detect the toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets, which are susceptible to out-of-distribution (OOD) problems and depend on the dataset\u2019s definition of toxicity. In this paper, we introduce a robust metric grounded on LLMs to flexibly measure toxicity according to the given definition. We first analyze the toxicity factors, followed by an examination of the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Finally, we evaluate the performance of our metric with detailed analysis. Our empirical results demonstrate outstanding performance in measuring toxicity within verified factors, improving on conventional metrics by 12 points in the F1 score. Our findings also indicate that upstream toxicity significantly influences downstream metrics, suggesting that LLMs are unsuitable for toxicity evaluations within unverified factors.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.353",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models": {
        "type": "INPROCEEDINGS",
        "key": "zeng-etal-2024-adamoe",
        "author": "Zeng, Zihao and Miao, Yibo and Gao, Hongcheng and Zhang, Hao and Deng, Zhijie",
        "booktitle": "EMNLP-findings2024",
        "title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce a constant top-k routing for all tokens, which is arguably restrictive because various tokens (e.g., \u201d\\textlessEOS\\textgreater\u201d vs. \u201capple\u201d) may require various numbers of experts for feature abstraction. Lifting such a constraint can help make the most of limited resources and unleash the potential of the model for downstream tasks. In this sense, we introduce **AdaMoE** to realize token-adaptive routing for MoE, where different tokens are permitted to select a various number of experts. AdaMoE makes minimal modifications to the vanilla MoE with top-k routing\u2014it simply introduces a fixed number of *null experts*, which do not consume any FLOPs, to the expert set and increases the value of k. AdaMoE does not force each token to occupy a fixed number of null experts but ensures the average usage of the null experts with a load-balancing loss, leading to an adaptive number of null/true experts used by each token. AdaMoE exhibits a strong resemblance to MoEs with expert choice routing while allowing for trivial auto-regressive modeling. AdaMoE is easy to implement and can be effectively applied to pre-trained (MoE-)LLMs. Extensive studies show that AdaMoE can reduce average expert load (FLOPs) while achieving superior performance. For example, on the ARC-C dataset, applying our method to fine-tuning Mixtral-8x7B can reduce FLOPs by 14.5% while increasing accuracy by 1.69%.Code is available at [this link](https://github.com/CengZihao/AdaMoE).",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.361",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Llamipa: An Incremental Discourse Parser": {
        "type": "INPROCEEDINGS",
        "key": "thompson-etal-2024-llamipa",
        "author": "Thompson, Kate and Chaturvedi, Akshay and Hunter, Julie and Asher, Nicholas",
        "booktitle": "EMNLP-findings2024",
        "title": "Llamipa: An Incremental Discourse Parser",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper provides the first discourse parsing experiments with a large language model (LLM) finetuned on corpora annotated in the style of SDRT (Segmented Discourse Representation Theory, Asher (1993), Asher and Lascarides (2003)). The result is a discourse parser, Llamipa (Llama Incremental Parser), that leverages discourse context, leading to substantial performance gains over approaches that use encoder-only models to provide local, context-sensitive representations of discourse units. Furthermore, it is able to process discourse data incrementally, which is essential for the eventual use of discourse information in downstream tasks.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.373",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Nebula: A discourse aware Minecraft Builder": {
        "type": "INPROCEEDINGS",
        "key": "chaturvedi-etal-2024-nebula",
        "author": "Chaturvedi, Akshay and Thompson, Kate and Asher, Nicholas",
        "booktitle": "EMNLP-findings2024",
        "title": "Nebula: A discourse aware Minecraft Builder",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "When engaging in collaborative tasks, humans efficiently exploit the semantic structure of a conversation to optimize verbal and nonverbal interactions. But in recent \u201clanguage to code\u201d or \u201clanguage to action\u201d models, this information is lacking. We show how incorporating the prior discourse and nonlinguistic context of a conversation situated in a nonlinguistic environment can improve the \u201clanguage to action\u201d component of such interactions. We finetune an LLM to predict actions based on prior context; our model, Nebula, doubles the net-action F1 score over the baseline on this task of Jayannavar et al. (2020). We also investigate our model\u2019s ability to construct shapes and understand location descriptions using a synthetic dataset.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.374",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs": {
        "type": "INPROCEEDINGS",
        "key": "liao-etal-2024-videoinsta",
        "author": "Liao, Ruotong and Erler, Max and Wang, Huiyu and Zhai, Guangyao and Zhang, Gengyuan and Ma, Yunpu and Tresp, Volker",
        "booktitle": "EMNLP-findings2024",
        "title": "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In the video-language domain, recent works in leveraging zero-shot Large Language Model-based reasoning for video understanding have become competitive challengers to previous end-to-end models. However, long video understanding presents unique challenges due to the complexity of reasoning over extended timespans, even for zero-shot LLM-based approaches. The challenge of information redundancy in long videos prompts the question of what specific information is essential for large language models (LLMs) and how to leverage them for complex spatial-temporal reasoning in long-form video analysis. We propose a framework VideoINSTA , i.e. INformative Spatial-TemporAl Reasoning for zero-shot long-form video understanding.VideoINSTA contributes (1) a zero-shot framework for long video understanding using LLMs; (2) an event-based temporalreasoning and content-based spatial reasoning approach for LLMs to reason over spatial-temporal information in videos; (3) a self-reflective information reasoning scheme based on information sufficiency and prediction confidence while balancing temporal factors.Our model significantly improves the state-of-the-art on three long video question-answering benchmarks: EgoSchema, NextQA, and IntentQA, and the open question answering dataset ActivityNetQA. Code is released: https://github.com/mayhugotong/VideoINSTA.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.384",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement": {
        "type": "INPROCEEDINGS",
        "key": "feng-etal-2024-self",
        "author": "Feng, Yunlong and Teng, Dechuan and Xu, Yang and Mu, Honglin and Xu, Xiao and Qin, Libo and Zhu, Qingfu and Che, Wanxiang",
        "booktitle": "EMNLP-findings2024",
        "title": "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decompilation transforms compiled code back into a high-level programming language for analysis when source code is unavailable. Previous work has primarily focused on enhancing decompilation performance by increasing the scale of model parameters or training data for pre-training. Based on the characteristics of the decompilation task, we propose two methods: (1) Without fine-tuning, the Self-Constructed Context Decompilation (sc\u00b2dec) method recompiles the LLM\u2019s decompilation results to construct pairs for in-context learning, helping the model improve decompilation performance. (2) Fine-grained Alignment Enhancement (FAE), which meticulously aligns assembly code with source code at the statement level by leveraging debugging information, is employed during the fine-tuning phase to achieve further improvements in decompilation. By integrating these two methods, we achieved a Re-Executability performance improvement of approximately 3.90% on the Decompile-Eval benchmark, establishing a new state-of-the-art performance of 52.41%. The code, data, and models are available at https://github.com/AlongWY/sccdec.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.385",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Learning Semantic Structure through First-Order-Logic Translation": {
        "type": "INPROCEEDINGS",
        "key": "chaturvedi-asher-2024-learning",
        "author": "Chaturvedi, Akshay and Asher, Nicholas",
        "booktitle": "EMNLP-findings2024",
        "title": "Learning Semantic Structure through First-Order-Logic Translation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we study whether transformer-based language models can extract predicate argument structure from simple sentences. We firstly show that language models sometimes confuse which predicates apply to which objects. To mitigate this, we explore two tasks: question answering (Q/A), and first order logic (FOL) translation, and two regimes, prompting and finetuning. In FOL translation, we finetune several large language models on synthetic datasets designed to gauge their generalization abilities. For Q/A, we finetune encoder models like BERT and RoBERTa and use prompting for LLMs. The results show that FOL translation for LLMs is better suited to learn predicate argument structure.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.390",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "A Training Data Recipe to Accelerate A* Search with Language Models": {
        "type": "INPROCEEDINGS",
        "key": "gupta-li-2024-training",
        "author": "Gupta, Devaansh and Li, Boyang",
        "booktitle": "EMNLP-findings2024",
        "title": "A Training Data Recipe to Accelerate A* Search with Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Combining Large Language Models (LLMs) with heuristic search algorithms like A* holds the promise of enhanced LLM reasoning and scalable inference. To accelerate training and reduce computational demands, we investigate the coreset selection problem for the training data of LLM heuristic learning. Few methods to learn the heuristic functions consider the interaction between the search algorithm and the machine learning model. In this work, we empirically disentangle the requirements of A* search algorithm from the requirements of the LLM to generalise on this task. Surprisingly, we find an overlap between their requirements; A* requires more accurate predictions on search nodes near the goal, and LLMs need the same set of nodes for effective generalisation. With these insights, we derive a data-selection distribution for learning LM-based heuristics. On three classical planning domains, maze navigation, Sokoban and sliding tile puzzles, our technique reduces the number of iterations required to find the solutions by up to 15x, with a wall-clock speed-up of search up to 5x. The code has been made available at https://github.com/devaansh100/a_star.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.391",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages": {
        "type": "INPROCEEDINGS",
        "key": "schmidt-etal-2024-self",
        "author": "Schmidt, Fabian David and Borchert, Philipp and Vuli\u0107, Ivan and Glava\u0161, Goran",
        "booktitle": "EMNLP-findings2024",
        "title": "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "LLMs have become a go-to solution not just for text generation, but also for natural language understanding (NLU) tasks. Acquiring extensive knowledge through language modeling on web-scale corpora, they excel on English NLU, yet struggle to extend their NLU capabilities to underrepresented languages. In contrast, machine translation models (MT) produce excellent multilingual representations, resulting in strong translation performance even for low-resource languages. MT encoders, however, lack the knowledge necessary for comprehensive NLU that LLMs obtain through language modeling training on immense corpora. In this work, we get the best both worlds by integrating MT encoders directly into LLM backbones via sample-efficient self-distillation. The resulting MT-LLMs preserve the inherent multilingual representational alignment from the MT encoder, allowing lower-resource languages to tap into the rich knowledge embedded in English-centric LLMs. Merging the MT encoder and LLM in a single model, we mitigate the propagation of translation errors and inference overhead of MT decoding inherent to discrete translation-based cross-lingual transfer (e.g., translate-test). Evaluation spanning three prominent NLU tasks and 127 predominantly low-resource languages renders MT-LLMs highly effective in cross-lingual transfer. MT-LLMs substantially and consistently outperform translation-test based on the same MT model, showing that we truly unlock multilingual language understanding for LLMs.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.394",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "cheng-etal-2024-autodetect",
        "author": "Cheng, Jiale and Lu, Yida and Gu, Xiaotao and Ke, Pei and Liu, Xiao and Dong, Yuxiao and Wang, Hongning and Tang, Jie and Huang, Minlie",
        "booktitle": "EMNLP-findings2024",
        "title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction-following or coding tasks.As these unexpected errors could lead to severe consequences in practical deployments, it is crucial to investigate the limitations within LLMs systematically.Traditional benchmarking approaches cannot thoroughly pinpoint specific model deficiencies, while manual inspections are costly and not scalable. In this paper, we introduce a unified framework, AutoDetect, to automatically expose weaknesses in LLMs across various tasks. Inspired by the educational assessment process that measures students\u2019 learning outcomes, AutoDetect consists of three LLM-powered agents: Examiner, Questioner, and Assessor.The collaboration among these three agents is designed to realize comprehensive and in-depth weakness identification. Our framework demonstrates significant success in uncovering flaws, with an identification success rate exceeding 30% in prominent models such as ChatGPT and Claude.More importantly, these identified weaknesses can guide specific model improvements, proving more effective than untargeted data augmentation methods like Self-Instruct. Our approach has led to substantial enhancements in popular LLMs, including the Llama series and Mistral-7b, boosting their performance by over 10% across several benchmarks.Code and data are publicly available at https://github.com/thu-coai/AutoDetect.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.397",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "cui-wang-2024-ada",
        "author": "Cui, Wanyun and Wang, Qianle",
        "booktitle": "EMNLP-findings2024",
        "title": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Instructions augmentation is a crucial step for unleashing the full potential of large language models (LLMs) in downstream tasks. Existing Self-Instruct methods primarily simulate new instructions from a few initial instructions with in-context learning. However, our study identifies a critical flaw in this approach: even with GPT4o, it cannot generate complex instructions of length \\ge 100, which is necessary in complex tasks such as code completion.To address this issue, our key insight is that fine-tuning open source LLMs with only ten examples can produce complex instructions that maintain distributional consistency for complex reasoning tasks. We introduce Ada-Instruct, an adaptive instruction generator developed through fine-tuning. We empirically validated Ada-Instruct\u2019s efficacy across different applications. The results highlight Ada-Instruct\u2019s capacity to generate long, intricate, and distributionally consistent instructions.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.409",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish": {
        "type": "INPROCEEDINGS",
        "key": "yuksel-etal-2024-turkishmmlu",
        "author": "Y\u00fcksel, Arda and K\u00f6ksal, Abdullatif and Senel, L\u00fctfi Kerem and Korhonen, Anna and Schuetze, Hinrich",
        "booktitle": "EMNLP-findings2024",
        "title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multiple choice question answering tasks evaluate the reasoning, comprehension, and mathematical abilities of Large Language Models (LLMs). While existing benchmarks employ automatic translation for multilingual evaluation, this approach is error-prone and potentially introduces culturally biased questions, especially in social sciences. We introduce the first multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs\u2019 understanding of the Turkish language. TurkishMMLU includes over 10,000 questions, covering 9 different subjects from Turkish high-school education curricula. These questions are written by curriculum experts, suitable for the high-school curricula in Turkey, covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic. We evaluate over 20 LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5), closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol) models. We provide an extensive evaluation, including zero-shot and few-shot evaluation of LLMs, chain-of-thought reasoning, and question difficulty analysis along with model performance. We provide an in-depth analysis of the Turkish capabilities and limitations of current LLMs to provide insights for future LLMs for the Turkish language. We publicly release our code for the dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.413",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective on Molecule Graphs": {
        "type": "INPROCEEDINGS",
        "key": "he-etal-2024-explaining",
        "author": "He, Yinhan and Zheng, Zaiyi and Soga, Patrick and Zhu, Yaochen and Dong, Yushun and Li, Jundong",
        "booktitle": "EMNLP-findings2024",
        "title": "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective on Molecule Graphs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In recent years, Graph Neural Networks (GNNs) have become successful in molecular property prediction tasks such as toxicity analysis. However, due to the black-box nature of GNNs, their outputs can be concerning in high-stakes decision-making scenarios, e.g., drug discovery. Facing such an issue, Graph Counterfactual Explanation (GCE) has emerged as a promising approach to improve GNN transparency. However, current GCE methods usually fail to take domain-specific knowledge into consideration, which can result in outputs that are not easily comprehensible by humans. To address this challenge, we propose a novel GCE method, LLM-GCE, to unleash the power of large language models (LLMs) in explaining GNNs for molecular property prediction. Specifically, we utilize an autoencoder to generate the counterfactual graph topology from a set of counterfactual text pairs (CTPs) based on an input graph. Meanwhile, we also incorporate a CTP dynamic feedback module to mitigate LLM hallucination, which provides intermediate feedback derived from the generated counterfactuals as an attempt to give more faithful guidance. Extensive experiments demonstrate the superior performance of LLM-GCE. Our code is released on https://github.com/YinhanHe123/new_LLM4GNNExplanation.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.415",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "The Effect of Sampling Temperature on Problem Solving in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "renze-2024-effect",
        "author": "Renze, Matthew",
        "booktitle": "EMNLP-findings2024",
        "title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used nine popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.432",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "ALIGN-SIM: A Task-Free Test Bed for Evaluating and Interpreting Sentence Embeddings through Semantic Similarity Alignment": {
        "type": "INPROCEEDINGS",
        "key": "mahajan-etal-2024-align",
        "author": "Mahajan, Yash and Bansal, Naman and Blanco, Eduardo and Karmaker, Santu",
        "booktitle": "EMNLP-findings2024",
        "title": "ALIGN-SIM: A Task-Free Test Bed for Evaluating and Interpreting Sentence Embeddings through Semantic Similarity Alignment",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Sentence embeddings play a pivotal role in a wide range of NLP tasks, yet evaluating and interpreting these real-valued vectors remains an open challenge to date, especially in a task-free setting. To address this challenge, we introduce a novel task-free test bed for evaluating and interpreting sentence embeddings. Our test bed consists of five semantic similarity alignment criteria, namely, *semantic distinction, synonym replacement, antonym replacement, paraphrasing without negation, and sentence jumbling*. Using these criteria, we examined five classical (e.g., Sentence-BERT, Universal Sentence Encoder (USE), etc.) and eight LLM-induced sentence embedding techniques (e.g., LLaMA2, GPT-3, OLMo, etc.) to test whether their semantic similarity spaces align with what a human mind would naturally expect. Our extensive experiments with 13 different sentence encoders revealed that none of the studied embeddings aligned with all the five semantic similarity alignment criteria. Yet, most encoders performed highly on the SentEval dataset, a popular task-specific benchmark. This finding demonstrates a significant limitation of the current practice in sentence embedding evaluation and associated popular benchmarks, a critical issue that needs careful attention and reassessment by the NLP community. Finally, we conclude the paper by highlighting the utility of the proposed alignment-based test bed for analyzing sentence embeddings in a novel way, especially in a task-free setting.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.436",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Insights into LLM Long-Context Failures: When Transformers Know but Don\u2019t Tell": {
        "type": "INPROCEEDINGS",
        "key": "gao-etal-2024-insights",
        "author": "Gao, Muhan and Lu, TaiMing and Yu, Kuai and Byerly, Adam and Khashabi, Daniel",
        "booktitle": "EMNLP-findings2024",
        "title": "Insights into LLM Long-Context Failures: When Transformers Know but Don\u2019t Tell",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) exhibit positional bias, struggling to utilize information from the middle or end of long contexts. Our study explores LLMs\u2019 long-context reasoning by probing their hidden representations. We find that while LLMs encode the position of target information, they often fail to leverage this in generating accurate responses. This reveals a disconnect between information retrieval and utilization, a \u201cknow but don\u2019t tell\u201d phenomenon. We further analyze the relationship between extraction time and final accuracy, offering insights into the underlying mechanics of transformer models.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.447",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues": {
        "type": "INPROCEEDINGS",
        "key": "hua-etal-2024-assistive",
        "author": "Hua, Yuncheng and Qu, Lizhen and Haf, Reza",
        "booktitle": "EMNLP-findings2024",
        "title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We develop assistive agents based on Large Language Models (LLMs) that aid interlocutors in business negotiations.Specifically, we simulate business negotiations by letting two LLM-based agents engage in role play. A third LLM acts as a remediator agent to rewrite utterances violating norms for improving negotiation outcomes.We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL exemplars for the remediator, where we propose a novel select criteria, called value impact, to measure the quality of the negotiation outcomes. We provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different negotiation topics. We have released our source code and the generated dataset at: https://github.com/tk1363704/SADAS.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.473",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2024-advancing",
        "author": "Yang, Linyan and Cheng, Jingwei and Zhang, Fu",
        "booktitle": "EMNLP-findings2024",
        "title": "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In recent years, the advent of large language models (LLMs) like GPT and Llama has significantly influenced numerous domains, particularly in advancing natural language processing (NLP) capabilities. LLMs have shown remarkable performance in NLP tasks such as relation extraction (RE) and knowledge graph completion (KGC), enhancing activities related to knowledge graphs. As a result, there is a growing interest in integrating LLMs into cross-lingual entity alignment (EA) task, which aims to identify equivalent entities across various knowledge graphs, thereby improving the performance of current baselines. However, employing LLMs for entity alignment poses challenges in efficiently handling large-scale data, generating suitable data samples, and adapting prompts for the EA task. To tackle these challenges, we propose Seg-Align, an innovative framework that integrating distance feature extraction, sample **Seg**mentation, and zero-shot prompts. Through extensive experiments on two widely used cross-lingual benchmark datasets, we have not only demonstrated the effectiveness of our proposed sample segmentation algorithm but also highlighted the state-of-the-art performance of Seg-Align. Code is available at https://github.com/yangxiaoxiaoly/Seg-Align.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.475",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "LaRA: Large Rank Adaptation for Speech and Text Cross-Modal Learning in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "shaik-etal-2024-lara",
        "author": "Shaik, Zuhair Hasan and Hegde, Pradyoth and Bannulmath, Prashant and T, Deepak K.",
        "booktitle": "EMNLP-findings2024",
        "title": "LaRA: Large Rank Adaptation for Speech and Text Cross-Modal Learning in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Integrating speech and text capabilities into large language models (LLMs) is a challenging task and we present Large Rank Adaptation (LaRA) for effective cross-modal integration of speech and text in the LLM framework. Unlike conventional LoRA, our method requires significantly larger ranks comparable to the pretrained weights to accommodate the complexities of speech-text cross-modality learning. The approach utilizes HuBERT to convert speech into discrete tokens and fine-tunes the pretrained LLM to adapt to cross-modal inputs and outputs. The work employs a Hi-Fi GAN vocoder to synthesize speech waveforms from the generated speech units. The initial studies use the Librispeech corpus to teach the model the relationships between speech and text, and Daily Talk, which involves dialog conversations, to adapt for interaction. The proposed work demonstrates adaptation for spoken and text conversations. However, the proposed framework can be easily extended to other cross-modal applications.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.480",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "PropTest: Automatic Property Testing for Improved Visual Programming": {
        "type": "INPROCEEDINGS",
        "key": "koo-etal-2024-proptest",
        "author": "Koo, Jaywon and Yang, Ziyan and Cascante-Bonilla, Paola and Ray, Baishakhi and Ordonez, Vicente",
        "booktitle": "EMNLP-findings2024",
        "title": "PropTest: Automatic Property Testing for Improved Visual Programming",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Visual Programming has recently emerged as an alternative to end-to-end black-box visual reasoning models. This type of method leverages Large Language Models (LLMs) to generate the source code for an executable computer program that solves a given problem. This strategy has the advantage of offering an interpretable reasoning path and does not require finetuning a model with task-specific data. We propose PropTest, a general strategy that improves visual programming by further using an LLM to generate code that tests for visual properties in an initial round of proposed solutions. Our method generates tests for data-type consistency, output syntax, and semantic properties. PropTest achieves comparable results to state-of-the-art methods while using publicly available LLMs. This is demonstrated across different benchmarks on visual question answering and referring expression comprehension. Particularly, PropTest improves ViperGPT by obtaining 46.1% accuracy (+6.0%) on GQA using Llama3-8B and 59.5% (+8.1%) on RefCOCO+ using CodeLlama-34B.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.483",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Weak-to-Strong Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2024-weak",
        "author": "Yang, Yuqing and Ma, Yan and Liu, Pengfei",
        "booktitle": "EMNLP-findings2024",
        "title": "Weak-to-Strong Reasoning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "When large language models (LLMs) surpass human capabilities, supervising them effectively becomes difficult. Weak-to-strong learning, where a less capable model enhances a stronger one, proves valuable in this context. Yet, the efficacy of this paradigm for complex reasoning tasks is still unexplored. In this paper, we introduce a progressive weak-to-strong reasoning framework that enables the strong model to autonomously refine its training data, maximizing the use of weak signals and unlocking its latent abilities. This framework begins with supervised fine-tuning on a selective small but high-quality dataset, followed by preference optimization on contrastive samples identified by the strong model itself. Experiments on the GSM8K and MATH datasets verify that our method can effectively improve the reasoning capabilities of Llama2-70b using three separate weak models. This work paves the way for a more scalable and sophisticated strategy to enhance AI reasoning powers. All relevant code and resources are available in https://github.com/GAIR-NLP/weak-to-strong-reasoning.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.490",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer": {
        "type": "INPROCEEDINGS",
        "key": "sun-etal-2024-psst",
        "author": "Sun, Huashan and Wu, Yixiao and Yang, Yizhe and Li, Yinghao and Li, Jiawei and Ye, Yuhao and Gao, Yang",
        "booktitle": "EMNLP-findings2024",
        "title": "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Language style is necessary for AI systems to accurately understand and generate diverse human language. However, previous text style transfer primarily focused on sentence-level data-driven approaches, limiting exploration of potential problems in large language models (LLMs) and the ability to meet complex application needs. To overcome these limitations, we introduce a novel task called Public-Speaking Style Transfer (PSST), which aims to simulate humans to transform passage-level, official texts into a public-speaking style. Grounded in the analysis of real-world data from a linguistic perspective, we decompose public-speaking style into key sub-styles to pose challenges and quantify the style modeling capability of LLMs. For such intricate text style transfer, we further propose a fine-grained evaluation framework to analyze the characteristics and identify the problems of stylized texts. Comprehensive experiments suggest that current LLMs struggle to generate public speaking texts that align with human preferences, primarily due to excessive stylization and loss of semantic information. We will release our data, code, and model upon acceptance.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.495",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "meng-etal-2024-traffic",
        "author": "Meng, Rui and Liu, Ye and Tu, Lifu and He, Daqing and Zhou, Yingbo and Yavuz, Semih",
        "booktitle": "EMNLP-findings2024",
        "title": "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Phrases are fundamental linguistic units through which humans convey semantics. This study critically examines the capacity of API-based large language models (LLMs) to comprehend phrase semantics, utilizing three human-annotated datasets. We assess the performance of LLMs in executing phrase semantic reasoning tasks guided by natural language instructions and explore the impact of common prompting techniques, including few-shot demonstrations and Chain-of-Thought reasoning. Our findings reveal that LLMs greatly outperform traditional embedding methods across the datasets; however, they do not show a significant advantage over fine-tuned methods. The effectiveness of advanced prompting strategies shows variability. We conduct detailed error analyses to interpret the limitations faced by LLMs in comprehending phrase semantics. Code and data can be found at https://github.com/memray/llm_phrase_semantics/.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.503",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-far",
        "author": "Huang, Heyan and Li, Yinghao and Sun, Huashan and Bai, Yu and Gao, Yang",
        "booktitle": "EMNLP-findings2024",
        "title": "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent studies have demonstrated that In-Context Learning (ICL), through the use of specific demonstrations, can align Large Language Models (LLMs) with human preferences known as In-Context Alignment (ICA), indicating that models can comprehend human instructions without requiring parameter adjustments. However, the exploration of the mechanism and applicability of ICA remains limited. In this paper, we begin by dividing the context text used in ICA into three categories: format, system prompt, and example. Through ablation experiments, we investigate the effectiveness of each part in enabling ICA to function effectively. We then examine how variants in these parts impact the model\u2019s alignment performance. Our findings indicate that the example part is crucial for enhancing the model\u2019s alignment capabilities, with changes in examples significantly affecting alignment performance. We also conduct a comprehensive evaluation of ICA\u2019s zero-shot capabilities in various alignment tasks. The results indicate that compared to parameter fine-tuning methods, ICA demonstrates superior performance in knowledge-based tasks and tool-use tasks. However, it still exhibits certain limitations in areas such as multi-turn dialogues and instruction following. Source codes and scripts are available at https://github.com/li-aolong/how-far-can-ica-go.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.504",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Can Language Models Recognize Convincing Arguments?": {
        "type": "INPROCEEDINGS",
        "key": "rescala-etal-2024-language",
        "author": "Rescala, Paula and Ribeiro, Manoel Horta and Hu, Tiancheng and West, Robert",
        "booktitle": "EMNLP-findings2024",
        "title": "Can Language Models Recognize Convincing Arguments?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The capabilities of large language models (LLMs) have raised concerns about their potential to create and propagate convincing narratives. Here, we study their performance in detecting convincing arguments to gain insights into LLMs\u2019 persuasive capabilities without directly engaging in experimentation with humans. We extend a dataset by Durmus and Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs\u2019 ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, surpassing human performance. The data and code released with this paper contribute to the crucial effort of continuously evaluating and monitoring LLMs\u2019 capabilities and potential impact. (https://go.epfl.ch/persuasion-llm)",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.515",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature": {
        "type": "INPROCEEDINGS",
        "key": "katz-etal-2024-knowledge",
        "author": "Katz, Uri and Levy, Mosh and Goldberg, Yoav",
        "booktitle": "EMNLP-findings2024",
        "title": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The exponential growth of scientific literature necessitates advanced tools for effective knowledge exploration. We present Knowledge Navigator, a system designed to enhance exploratory search abilities by organizing and structuring the retrieved documents from broad topical queries into a navigable, two-level hierarchy of named and descriptive scientific topics and subtopics. This structured organization provides an overall view of the research themes in a domain, while also enabling iterative search and deeper knowledge discovery within specific subtopics by allowing users to refine their focus and retrieve additional relevant documents. Knowledge Navigator combines LLM capabilities with cluster-based methods to enable an effective browsing method. We demonstrate our approach\u2019s effectiveness through automatic and manual evaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC Our code, prompts, and benchmarks are made publicly available.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.516",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention": {
        "type": "INPROCEEDINGS",
        "key": "lu-etal-2024-hit",
        "author": "Lu, Wenxuan and Jiang, Songhao and Yijing, Wang and Zang, Tianning",
        "booktitle": "EMNLP-findings2024",
        "title": "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) on small Pre-trained Language Models (PLMs) has emerged as a promising approach to enhance their multi-tasking capabilities. Prevalent methods simultaneously train additional modules (i.e., one task-shared module and multiple task-specific modules) for adapting PLMs to downstream tasks. However, their adaptability to new tasks is constrained, as the task-specific modules independently adapt to each task, overlooking the potential for knowledge transfer across tasks. In this paper, we propose a novel multi-task learning framework, Inspirational Pointer (IP), that enables the transfer of prior knowledge across tasks through human language intervention. Specifically, we attach task descriptions to the input samples, which are then mapped to corresponding task embeddings. Based on those embeddings, we adapt PLMs for downstream tasks. Similar tasks share akin descriptions, allowing new task samples close to similar trained tasks in the task embedding space, hitting the memory about trained tasks of the model. Our experiments on the T5 model demonstrate performance improvements of our method in multi-task learning and few-shot transfer learning. Further, we implemented the IP in decoder-only models including GPT2 and large language models (LLMs), and the results show that IP enhances the capabilities of decoder-only models.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.518",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code": {
        "type": "INPROCEEDINGS",
        "key": "guan-etal-2024-codeip",
        "author": "Guan, Batu and Wan, Yao and Bi, Zhangqian and Wang, Zheng and Zhang, Hongyu and Zhou, Pan and Sun, Lichao",
        "booktitle": "EMNLP-findings2024",
        "title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress in code generation. It now becomes crucial to identify whether the code is AI-generated and to determine the specific model used, particularly for purposes such as protecting Intellectual Property (IP) in industry and preventing cheating in programming exercises. To this end, several attempts have been made to insert watermarks into machine-generated code. However, existing approaches are limited to inserting only a single bit of information. In this paper, we introduce CodeIP, a novel multi-bit watermarking technique that embeds additional information to preserve crucial provenance details, such as the vendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation. Furthermore, to ensure the syntactical correctness of the generated code, we propose constraining the sampling process for predicting the next token by training a type predictor. Experiments conducted on a real-world dataset across five programming languages demonstrate the effectiveness of CodeIP in watermarking LLMs for code generation while maintaining the syntactical correctness of code.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.541",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging": {
        "type": "INPROCEEDINGS",
        "key": "kargupta-etal-2024-instruct",
        "author": "Kargupta, Priyanka and Agarwal, Ishika and Tur, Dilek Hakkani and Han, Jiawei",
        "booktitle": "EMNLP-findings2024",
        "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student\u2019s conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes\u2013 all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct\u2019s state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct\u2019s ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.553",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Tutor-ICL: Guiding Large Language Models for Improved In-Context Learning Performance": {
        "type": "INPROCEEDINGS",
        "key": "cho-etal-2024-tutor",
        "author": "Cho, Ikhyun and Kwon, Gaeul and Hockenmaier, Julia",
        "booktitle": "EMNLP-findings2024",
        "title": "Tutor-ICL: Guiding Large Language Models for Improved In-Context Learning Performance",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "There has been a growing body of work focusing on the in-context learning (ICL) abilities of large language models (LLMs). However, it is an open question how effective ICL can be. This paper presents Tutor-ICL, a simple prompting method for classification tasks inspired by how effective instructors might engage their students in learning a task. Specifically, we propose presenting exemplar answers in a *comparative format* rather than the traditional single-answer format. We also show that including the test instance before the exemplars can improve performance, making it easier for LLMs to focus on relevant exemplars. Lastly, we include a summarization step before attempting the test, following a common human practice. Experiments on various classification tasks, conducted across both decoder-only LLMs (Llama 2, 3) and encoder-decoder LLMs (Flan-T5-XL, XXL), show that Tutor-ICL consistently boosts performance, achieving up to a 13.76% increase in accuracy.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.554",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "A LLM-based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation": {
        "type": "INPROCEEDINGS",
        "key": "zubiaga-etal-2024-llm",
        "author": "Zubiaga, Irune and Soroa, Aitor and Agerri, Rodrigo",
        "booktitle": "EMNLP-findings2024",
        "title": "A LLM-based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper proposes a novel approach to evaluate Counter Narrative (CN) generation using a Large Language Model (LLM) as an evaluator. We show that traditional automatic metrics correlate poorly with human judgements and fail to capture the nuanced relationship between generated CNs and human perception. To alleviate this, we introduce a model ranking pipeline based on pairwise comparisons of generated CNs from different models, organized in a tournament-style format. The proposed evaluation method achieves a high correlation with human preference, with a \u03c1 score of 0.88. As an additional contribution, we leverage LLMs as zero-shot CN generators and provide a comparative analysis of chat, instruct, and base models, exploring their respective strengths and limitations. Through meticulous evaluation, including fine-tuning experiments, we elucidate the differences in performance and responsiveness to domain-specific data. We conclude that chat-aligned models in zero-shot are the best option for carrying out the task, provided they do not refuse to generate an answer due to security concerns.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.559",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation": {
        "type": "INPROCEEDINGS",
        "key": "azizi-etal-2024-lamda",
        "author": "Azizi, Seyedarmin and Kundu, Souvik and Pedram, Massoud",
        "booktitle": "EMNLP-findings2024",
        "title": "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Low-rank adaptation (LoRA) has become the default approach to fine-tune large language models (LLMs) due to its significant reduction in trainable parameters. However, trainable parameter demand for LoRA increases with increasing model embedding dimensions, leading to high compute costs. Additionally, its backward updates require storing high-dimensional intermediate activations and optimizer states, demanding high peak GPU memory. In this paper, we introduce _LaMDA_, a novel approach to fine-tuning large language models, which leverages low-dimensional adaptation to achieve significant reductions in trainable parameters and peak GPU memory footprint. LaMDA freezes a first projection matrix (PMA) in the adaptation path while introducing a low-dimensional trainable square matrix, resulting in substantial reductions in trainable parameters and peak GPU memory usage. LaMDA gradually freezes a second projection matrix (PMB) during the early fine-tuning stages, reducing the compute cost associated with weight updates to enhance parameter efficiency further.We also present an enhancement, LaMDA++, incorporating a \u201clite-weight\u201d adaptive rank allocation for the LoRA path via normalized spectrum analysis of pre-trained model weights. We evaluate LaMDA/LaMDA++ across various tasks, including natural language understanding with the GLUE benchmark, text summarization, natural language generation, and complex reasoning on different LLMs.Results show that LaMDA matches or surpasses the performance of existing alternatives while requiring up to **17.7\\times** fewer parameter updates and up to **1.32\\times** lower peak GPU memory usage during fine-tuning. Code will be publicly available at https://github.com/ArminAzizi98/LaMDA.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.563",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "A Survey on Detection of LLMs-Generated Content": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2024-survey",
        "author": "Yang, Xianjun and Pan, Liangming and Zhao, Xuandong and Chen, Haifeng and Petzold, Linda Ruth and Wang, William Yang and Cheng, Wei",
        "booktitle": "EMNLP-findings2024",
        "title": "A Survey on Detection of LLMs-Generated Content",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, and we have maintained a website to consistently update the latest research as a guiding reference for researchers and practitioners.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.572",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Can LLMs Reason in the Wild with Programs?": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2024-llms",
        "author": "Yang, Yuan and Xiong, Siheng and Payani, Ali and Shareghi, Ehsan and Fekri, Faramarz",
        "booktitle": "EMNLP-findings2024",
        "title": "Can LLMs Reason in the Wild with Programs?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown superior capability to solve reasoning problems with programs. While being a promising direction, most of such frameworks are trained and evaluated in settings with a prior knowledge of task requirements. However, as LLMs become more capable, it is necessary to assess their reasoning abilities in more realistic scenarios where many real-world problems are open-ended with ambiguous scope, and often require multiple formalisms to solve. To investigate this, we introduce the task of reasoning in the wild, where an LLM is tasked to solve a reasoning problem of unknown type by identifying the sub-problems and their corresponding formalisms, and writing a program to solve each sub-problem, guided by a tactic. We create a large tactic-guided trajectory dataset containing detailed solutions to a diverse set of reasoning problems, ranging from well-defined single-form reasoning (e.g., math, logic), to ambiguous and hybrid ones (e.g., commonsense, combined math and logic). This allows us to test various aspects of LLMs reasoning at the fine-grained level such as the selection and execution of tactics, and the tendency to take undesired shortcuts. In experiments, we highlight that existing LLMs fail significantly on problems with ambiguous and mixed scope, revealing critical limitations and overfitting issues (e.g. accuracy on GSM8K drops by at least 50%). We further show the potential of finetuning a local LLM on the tactic-guided trajectories in achieving better performance. Project repo is available at https://github.com/gblackout/Reason-in-the-Wild.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.573",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs": {
        "type": "INPROCEEDINGS",
        "key": "wu-etal-2024-vdebugger",
        "author": "Wu, Xueqing and Lin, Zongyu and Zhao, Songyan and Wu, Te-Lin and Lu, Pan and Peng, Nanyun and Chang, Kai-Wei",
        "booktitle": "EMNLP-findings2024",
        "title": "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Visual programs are executable code generated by large language models to address visual reasoning problems. They decompose complex questions into multiple reasoning steps and invoke specialized models for each step to solve the problems. However, these programs are prone to logic errors, with our preliminary evaluation showing that 58% of the total errors are caused by program logic errors. Debugging complex visual programs remains a major bottleneck for visual reasoning. To address this, we introduce **VDebugger**, a novel critic-refiner framework trained to localize and debug visual programs by tracking execution step by step. VDebugger identifies and corrects program errors leveraging detailed execution feedback, improving interpretability and accuracy. The training data is generated through an automated pipeline that injects errors into correct visual programs using a novel mask-best decoding technique. Evaluations on six datasets demonstrate VDebugger\u2019s effectiveness, showing performance improvements of up to 3.2% in downstream task accuracy. Further studies show VDebugger\u2019s ability to generalize to unseen tasks, bringing a notable improvement of 2.3% on the unseen COVR task.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.575",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy": {
        "type": "INPROCEEDINGS",
        "key": "dumitru-etal-2024-change",
        "author": "Dumitru, Razvan-Gabriel and Clotan, Paul Ioan and Yadav, Vikas and Peteleaza, Darius and Surdeanu, Mihai",
        "booktitle": "EMNLP-findings2024",
        "title": "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper introduces a novel model compression approach through dynamic layer-specific pruning in Large Language Models (LLMs), enhancing the traditional methodology established by SliceGPT. By transitioning from constant to dynamic slicing, our method leverages the newly proposed Layer Redundancy (LR) score, which assesses how much change each layer changes its input by measuring the cosine similarity of the input to the output of the layer. We use this score to prune parts of individual layers based on redundancy in such a way that the average pruned percentage for all layers is a fixed value. We conducted extensive experiments using models like Llama3-8B and Mistral-7B on multiple datasets, evaluating different slicing bases and percentages to determine optimal configurations that balance efficiency and performance. Our findings show that our dynamic slicing approach not only maintains but, in many cases, enhances model performance compared to the baseline established by constant slicing methods. For instance, in several settings, we see performance improvements of up to 5% over the SliceGPT baseline.Additionally, a perplexity decrease by as much as 7% was observed across multiple benchmarks, validating the effectiveness of our method. The code, model weights, and datasets are open-sourced at - https://github.com/RazvanDu/DynamicSlicing",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.579",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "TrustAgent: Towards Safe and Trustworthy LLM-based Agents": {
        "type": "INPROCEEDINGS",
        "key": "hua-etal-2024-trustagent",
        "author": "Hua, Wenyue and Yang, Xianjun and Jin, Mingyu and Li, Zelong and Cheng, Wei and Tang, Ruixiang and Zhang, Yongfeng",
        "booktitle": "EMNLP-findings2024",
        "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The rise of LLM-based agents shows great potential to revolutionize task planning, capturing significant attention. Given that these agents will be integrated into high-stake domains, ensuring their reliability and safety is crucial. This paper presents an Agent-Constitution-based agent framework, TrustAgent, with a particular focus on improving the LLM-based agent safety. The proposed framework ensures strict adherence to the Agent Constitution through three strategic components: pre-planning strategy which injects safety knowledge to the model before plan generation, in-planning strategy which enhances safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Our experimental results demonstrate that the proposed framework can effectively enhance an LLM agent\u2019s safety across multiple domains by identifying and mitigating potential dangers during the planning. Further analysis reveals that the framework not only improves safety but also enhances the helpfulness of the agent. Additionally, we highlight the importance of the LLM reasoning ability in adhering to the Constitution. This paper sheds light on how to ensure the safe integration of LLM-based agents into human-centric environments. Data and code are available at https://anonymous.4open.science/r/TrustAgent-06DC.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.585",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes": {
        "type": "INPROCEEDINGS",
        "key": "cao-etal-2024-presto",
        "author": "Cao, He and Shao, Yanjun and Liu, Zhiyuan and Liu, Zijing and Tang, Xiangru and Yao, Yuan and Li, Yu",
        "booktitle": "EMNLP-findings2024",
        "title": "PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multimodal Large Language Models (MLLMs) have seen growing adoption across various scientific disciplines. These advancements encourage the investigation of molecule-text modeling within synthetic chemistry, a field dedicated to designing and conducting chemical reactions to synthesize new compounds with desired properties and applications. Current approaches, however, often neglect the critical role of multi-molecule graph interaction in understanding chemical reactions, leading to suboptimal performance in synthetic chemistry tasks. This study introduces PRESTO (Progressive Pretraining Enhances Synthetic Chemistry Outcomes), a new framework that bridges the molecule-text modality gap by integrating a comprehensive benchmark of pretraining strategies and dataset configurations. It progressively improves multimodal LLMs through cross-modal alignment and multi-graph understanding. Our extensive experiments demonstrate that PRESTO offers competitive results in downstream synthetic chemistry tasks. The code can be found at https://github.com/IDEA-XL/PRESTO.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.597",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning": {
        "type": "INPROCEEDINGS",
        "key": "zou-etal-2024-promptintern",
        "author": "Zou, Jiaru and Zhou, Mengyu and Li, Tao and Han, Shi and Zhang, Dongmei",
        "booktitle": "EMNLP-findings2024",
        "title": "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advances in fine-tuning large language models (LLMs) have greatly enhanced their usage in domain-specific tasks. Despite the success, fine-tuning continues to rely on repeated and lengthy prompts, which escalate computational expenses, require more resources, and lead to slower inference. In this paper, we present a novel approach, PromptIntern, which internalizes prompt knowledge during model fine-tuning to achieve efficient inference and save costs. Instead of compressing the prompts for a vanilla model, PromptIntern aims to embed the recurrent prompt directly into the model parameters. We design a fine-tuning pipeline that includes instruction template compression, few-shot example absorption, and a progressive internalization strategy, effectively diminishing the need for intricate prompts during inference. Comprehensive experiments on challenging NL2Code tasks demonstrate that our method reduces input tokens by more than 90%, accelerates inference by 4.2 times, and reduces monetary inference costs by 88.3%.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.602",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Exploring Design Choices for Building Language-Specific LLMs": {
        "type": "INPROCEEDINGS",
        "key": "tejaswi-etal-2024-exploring",
        "author": "Tejaswi, Atula and Gupta, Nilesh and Choi, Eunsol",
        "booktitle": "EMNLP-findings2024",
        "title": "Exploring Design Choices for Building Language-Specific LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Despite rapid progress in large language models (LLMs), their performance on a vast majority of languages remains unsatisfactory. In this paper, we study building language-specific LLMs by adapting monolingual and multilingual LLMs. We conduct systematic experiments on how design choices (base model selection, vocabulary extension, and continued pretraining) impact the adapted LLM, both in terms of efficiency (how many tokens are needed to encode the same amount of information) and end task performance. We find that (1) the initial performance of LLM does not always correlate with the final performance after the adaptation. Adapting an English-centric models can yield better results than adapting multilingual models despite their worse initial performance on low-resource languages. (2) Efficiency can easily improved with simple vocabulary extension and continued pretraining in most LLMs we study, and (3) The optimal adaptation method (choice of the base model, new vocabulary size, training data, initialization strategy) is highly language-dependent, and the simplest embedding initialization works well across various experimental settings. Together, our work lays foundations on efficiently building language-specific LLMs by adapting existing LLMs.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.614",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA": {
        "type": "INPROCEEDINGS",
        "key": "jianhao-etal-2024-promoting",
        "author": "JianHao, Zhu and Lv, Changze and Wang, Xiaohua and Wu, Muling and Liu, Wenhao and Li, Tianlong and Ling, Zixuan and Zhang, Cenyuan and Zheng, Xiaoqing and Huang, Xuanjing",
        "booktitle": "EMNLP-findings2024",
        "title": "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Conventional federated learning primarily aims to secure the privacy of data distributed across multiple edge devices, with the global model dispatched to edge devices for parameter updates during the learning process. However, the development of large language models (LLMs) requires substantial data and computational resources, rendering them valuable intellectual properties for their developers and owners. To establish a mechanism that protects both data and model privacy in a federated learning context, we introduce a method that just needs to distribute a quantized version of the model\u2019s parameters during training. This method enables accurate gradient estimations for parameter updates while preventing clients from accessing a model whose performance is comparable to the centrally hosted one. Moreover, we combine this quantization strategy with LoRA, a popular and parameter-efficient fine-tuning method, to significantly reduce communication costs in federated learning. The proposed framework, named FedLPP, successfully ensures both data and model privacy in the federated learning context. Additionally, the learned central model exhibits good generalization and can be trained in a resource-efficient manner.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.615",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation": {
        "type": "INPROCEEDINGS",
        "key": "liao-etal-2024-medcare",
        "author": "Liao, Yusheng and Jiang, Shuyang and Chen, Zhe and Wang, Yu and Wang, Yanfeng",
        "booktitle": "EMNLP-findings2024",
        "title": "MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have shown substantial progress in natural language understanding and generation, proving valuable especially in the medical field. Despite advancements, challenges persist due to the complexity and diversity inherent in medical tasks, which can be categorized as knowledge-intensive tasks and alignment-required tasks. Previous approaches either ignore the latter task or focus on a minority of tasks and hence lose generalization. To address these drawbacks, we propose a progressive fine-tuning pipeline. This pipeline employs a and a to encode diverse knowledge in the first stage and filter out detrimental information. In the second stage, we drop the to avoid the interference of suboptimal representation and leverage an additional alignment module optimized towards an orthogonal direction to the knowledge space to mitigate knowledge forgetting. Based on this two-stage paradigm, we proposed a Medical LLM through decoupling Clinical Alignment and Knowledge Aggregation (), which is designed to achieve promising performance on over 20 medical tasks, as well as results on specific medical alignment tasks. Various model sizes of (1.8B, 7B, 14B) all demonstrate significant improvements over existing models with similar model sizes. Our code and datasets are available at https://github.com/BlueZeros/MedCare.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.619",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-beyond-natural",
        "author": "Chen, Weize and Yuan, Chenfei and Yuan, Jiarui and Su, Yusheng and Qian, Chen and Yang, Cheng and Xie, Ruobing and Liu, Zhiyuan and Sun, Maosong",
        "booktitle": "EMNLP-findings2024",
        "title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL\u2019s status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7% improvement in reasoning efficiency for different LLMs, and up to a 72.7% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code will be released to facilitate further exploration.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.623",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages": {
        "type": "INPROCEEDINGS",
        "key": "lu-etal-2024-llamax",
        "author": "Lu, Yinquan and Zhu, Wenhao and Li, Lei and Qiao, Yu and Yuan, Fei",
        "booktitle": "EMNLP-findings2024",
        "title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) demonstrate remarkable translation capabilities in high-resource language tasks, yet their performance in low-resource languages is hindered by insufficient multilingual data during pre-training. To address this, we conduct extensive multilingual continual pre-training on the LLaMA series models, enabling translation support across more than 100 languages. Through a comprehensive analysis of training strategies, such as vocabulary expansion and data augmentation, we develop LLaMAX. Remarkably, without sacrificing its generalization ability, LLaMAX achieves significantly higher translation performance compared to existing open-source LLMs (by more than 10 spBLEU points) and performs on-par with specialized translation model (M2M-100-12B) on the Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve as a robust multilingual foundation model. The code and the models are publicly available.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.631",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Rethinking Code Refinement: Learning to Judge Code Efficiency": {
        "type": "INPROCEEDINGS",
        "key": "seo-etal-2024-rethinking",
        "author": "Seo, Minju and Baek, Jinheon and Hwang, Sung Ju",
        "booktitle": "EMNLP-findings2024",
        "title": "Rethinking Code Refinement: Learning to Judge Code Efficiency",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating codes. Due to these capabilities, many recent methods are proposed to automatically refine the codes with LLMs. However, we should rethink that the refined codes (from LLMs and even humans) are not always more efficient than their original versions. On the other hand, running two different versions of codes and comparing them every time is not ideal and time-consuming. Therefore, in this work, we propose a novel method based on the code language model that is trained to judge the efficiency between two different codes (generated across humans and machines) by either classifying the superior one or predicting the relative improvement. We validate our method on multiple programming languages with multiple refinement steps, demonstrating that the proposed method can effectively distinguish between more and less efficient versions of code.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.645",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MAVEN-FACT: A Large-scale Event Factuality Detection Dataset": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-maven",
        "author": "Li, Chunyang and Peng, Hao and Wang, Xiaozhi and Qi, Yunjia and Hou, Lei and Xu, Bin and Li, Juanzi",
        "booktitle": "EMNLP-findings2024",
        "title": "MAVEN-FACT: A Large-scale Event Factuality Detection Dataset",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Event Factuality Detection (EFD) task determines the factuality of textual events, i.e., classifying whether an event is a fact, possibility, or impossibility, which is essential for faithfully understanding and utilizing event knowledge. However, due to the lack of high-quality large-scale data, event factuality detection is under-explored in event understanding research, which limits the development of EFD community. To address these issues and provide faithful event understanding, we introduce MAVEN-FACT, a large-scale and high-quality EFD dataset based on the MAVEN dataset. MAVEN-FACT includes factuality annotations of 112,276 events, making it the largest EFD dataset. Extensive experiments demonstrate that MAVEN-FACT is challenging for both conventional fine-tuned models and large language models (LLMs). Thanks to the comprehensive annotations of event arguments and relations in MAVEN, MAVEN-FACT also supports some further analyses and we find that adopting event arguments and relations helps in event factuality detection for fine-tuned models but does not benefit LLMs. Furthermore, we preliminarily study an application case of event factuality detection and find it helps in mitigating event-related hallucination in LLMs. We will release our dataset and codes to facilitate further research on event factuality detection.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.651",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Leveraging Web-Crawled Data for High-Quality Fine-Tuning": {
        "type": "INPROCEEDINGS",
        "key": "zhou-etal-2024-leveraging",
        "author": "Zhou, Jing and Jiang, Chenglin and Shen, Wei and Zhou, Xiao and He, Xiaonan",
        "booktitle": "EMNLP-findings2024",
        "title": "Leveraging Web-Crawled Data for High-Quality Fine-Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Most large language models are fine-tuned using either expensive human-annotated data or GPT-4 generated data which cannot guarantee performance in certain domains. We argue that although the web-crawled data often has formatting errors causing semantic inaccuracies, it can still serve as a valuable source for high-quality supervised fine-tuning in specific domains without relying on advanced models like GPT-4. To this end, we create a paired training dataset automatically by aligning web-crawled data with a smaller set of high-quality data. By training a language model on this dataset, we can convert web data with irregular formats into high-quality ones. Our experiments show that training with the model-transformed data yields better results, surpassing training with only high-quality data by an average score of 9.4% in Chinese math problems. Additionally, our 7B model outperforms several open-source models larger than 32B and surpasses well-known closed-source models such as GPT-3.5, highlighting the efficacy of our approach. We have released our code at https://github.com/zhouj8553/Web_to_SFT.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.660",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Revisiting the Impact of Pursuing Modularity for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "kang-etal-2024-revisiting",
        "author": "Kang, Deokyeong and Seo, KiJung and Kim, Taeuk",
        "booktitle": "EMNLP-findings2024",
        "title": "Revisiting the Impact of Pursuing Modularity for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Modular programming, which aims to construct the final program by integrating smaller, independent building blocks, has been regarded as a desirable practice in software development. However, with the rise of recent code generation agents built upon large language models (LLMs), a question emerges: is this traditional practice equally effective for these new tools? In this work, we assess the impact of modularity in code generation by introducing a novel metric for its quantitative measurement. Surprisingly, unlike conventional wisdom on the topic, we find that modularity is not a core factor for improving the performance of code generation models. We also explore potential explanations for why LLMs do not exhibit a preference for modular code compared to non-modular code.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.676",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "\u201cVorbe\\textcommabelowsti Rom\u00e2ne\\textcommabelowste?\u201d A Recipe to Train Powerful Romanian LLMs with English Instructions": {
        "type": "INPROCEEDINGS",
        "key": "masala-etal-2024-vorbesti",
        "author": "Masala, Mihai and Ilie-Ablachim, Denis and Dima, Alexandru and Corlatescu, Dragos Georgian and Zavelca, Miruna-Andreea and Olaru, Ovio and Terian, Simina-Maria and Terian, Andrei and Leordeanu, Marius and Velicu, Horia and Popescu, Marius and Dascalu, Mihai and Rebedea, Traian",
        "booktitle": "EMNLP-findings2024",
        "title": "\u201cVorbe\\textcommabelowsti Rom\u00e2ne\\textcommabelowste?\u201d A Recipe to Train Powerful Romanian LLMs with English Instructions",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In recent years, Large Language Models (LLMs) have achieved almost human-like performance on various tasks. While some LLMs have been trained on multilingual data, most of the training data is in English; hence, their performance in English greatly exceeds other languages. To our knowledge, we are the first to collect and translate a large collection of texts, instructions, and benchmarks and train, evaluate, and release open-source LLMs tailored for Romanian. We evaluate our methods on four different categories, including academic benchmarks, MT-Bench (manually translated), and a professionally built historical, cultural, and social benchmark adapted to Romanian. We argue for the usefulness and high performance of RoLLMs by obtaining state-of-the-art results across the board. We publicly release all resources (i.e., data, training and evaluation code, models) with the goal of supporting and encouraging research on Romanian LLMs while concurrently creating a generalizable recipe adequate for other low or less-resourced languages.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.681",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "zhu-etal-2024-fastmem",
        "author": "Zhu, Junyi and Liu, Shuochen and Yu, Yu and Tang, Bo and Yan, Yibo and Li, Zhiyu and Xiong, Feiyu and Xu, Tong and Blaschko, Matthew B.",
        "booktitle": "EMNLP-findings2024",
        "title": "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) excel in generating coherent text, but they often struggle with context awareness, leading to inaccuracies in tasks requiring faithful adherence to provided information. We introduce FastMem, a novel method designed to enhance instruction fine-tuned LLMs\u2019 context awareness through fast memorization of the prompt. FastMem maximizes the likelihood of the prompt before inference by updating only the last Feed-Forward Network (FFN) module. This targeted approach ensures efficient optimization without overfitting, significantly improving the model\u2019s ability to comprehend and accurately follow the context. Our experiments demonstrate substantial gains in reading comprehension, text summarization and adherence to output structures. For instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAP dataset from 59.1% to 71.6%, and reduces the output structure failure rate of Qwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlight FastMem\u2019s potential to offer a robust solution to enhance the reliability and accuracy of LLMs in various applications. Our code is available at: https://github.com/IAAR-Shanghai/FastMem.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.687",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Virtual Context Enhancing Jailbreak Attacks with Special Token Injection": {
        "type": "INPROCEEDINGS",
        "key": "zhou-etal-2024-virtual",
        "author": "Zhou, Yuqi and Lu, Lin and Sun, Ryan and Zhou, Pan and Sun, Lichao",
        "booktitle": "EMNLP-findings2024",
        "title": "Virtual Context Enhancing Jailbreak Attacks with Special Token Injection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Jailbreak attacks on large language models (LLMs) involve inducing these models to generate harmful content that violates ethics or laws, posing a significant threat to LLM security. Current jailbreak attacks face two main challenges: low success rates due to defensive measures and high resource requirements for crafting specific prompts. This paper introduces Virtual Context, which leverages special tokens, previously overlooked in LLM security, to improve jailbreak attacks. Virtual Context addresses these challenges by significantly increasing the success rates of existing jailbreak methods and requiring minimal background knowledge about the target model, thus enhancing effectiveness in black-box settings without additional overhead. Comprehensive evaluations show that Virtual Context-assisted jailbreak attacks can improve the success rates of four widely used jailbreak methods by approximately 40% across various LLMs. Additionally, applying Virtual Context to original malicious behaviors still achieves a notable jailbreak effect. In summary, our research highlights the potential of special tokens in jailbreak attacks and recommends including this threat in red-teaming testing to comprehensively enhance LLM security.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.692",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural Communication": {
        "type": "INPROCEEDINGS",
        "key": "white-etal-2024-communicate",
        "author": "White, Isadora and Pandey, Sashrika and Pan, Michelle",
        "booktitle": "EMNLP-findings2024",
        "title": "Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural Communication",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we study how culture leads to differences in common ground and how this influences communication. During communication, cultural differences in common ground during communication may result in pragmatic failure and misunderstandings. We develop our method Rational Speech Acts for Cross-Cultural Communication (RSA+C3) to resolve cross-cultural differences in common ground. To measure the success of our method, we study RSA+C3 in the collaborative referential game of Codenames Duet and show that our method successfully improves collaboration between simulated players of different cultures. Our contributions are threefold: (1) creating Codenames players using contrastive learning of an embedding space and LLM prompting that are aligned with human patterns of play, (2) studying culturally induced differences in common ground reflected in our trained models, and (3) demonstrating that our method RSA+C3 can ease cross-cultural communication in gameplay by inferring sociocultural context from interaction.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.711",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation": {
        "type": "INPROCEEDINGS",
        "key": "shen-etal-2024-enhancing",
        "author": "Shen, Zizhuo and Shao, Yanqiu and Li, Wei",
        "booktitle": "EMNLP-findings2024",
        "title": "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Due to the high complexity of Discourse Dependency Parsing (DDP) tasks, their existing annotation resources are relatively scarce compared to other NLP tasks, and different DDP tasks also have significant differences in annotation schema. These issues have led to the dilemma of low resources for DDP tasks. Thanks to the powerful capabilities of Large Language Models (LLMs) in cross-task learning, we can use LLMs to model dependency parsing under different annotation schema in an unified manner, in order to alleviate the dilemma of low resources for DDP tasks. However, enabling LLMs to deeply comprehend dependency parsing tasks is a challenge that remains underexplored. Inspired by the application of code-based methods in complex tasks, we propose a code-based unified dependency parsing method. We treat the process of dependency parsing as a search process of dependency paths and use code to represent this search process. Furthermore, we use a curriculum-learning based instruction tuning strategy for joint training of multiple dependency parsing tasks. The experimental results show that our proposed code-based DDP system has achieved good performance on two Chinese DDP tasks (especially significant improvement on the DDP task with relatively less training data).",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.729",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "\u201cKnowing When You Don\u2019t Know\u201d: A Multilingual Relevance Assessment Dataset for Robust Retrieval-Augmented Generation": {
        "type": "INPROCEEDINGS",
        "key": "thakur-etal-2024-knowing",
        "author": "Thakur, Nandan and Bonifacio, Luiz and Zhang, Crystina and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Chen, Boxing and Rezagholizadeh, Mehdi and Lin, Jimmy",
        "booktitle": "EMNLP-findings2024",
        "title": "\u201cKnowing When You Don\u2019t Know\u201d: A Multilingual Relevance Assessment Dataset for Robust Retrieval-Augmented Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM) output by leveraging external knowledge sources to reduce factual hallucinations. However, prior work lacks a comprehensive evaluation of different language families, making it challenging to evaluate LLM robustness against errors in external retrieved knowledge. To overcome this, we establish **NoMIRACL**, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages. NoMIRACL includes both a non-relevant and a relevant subset. Queries in the non-relevant subset contain passages judged as non-relevant, whereas queries in the relevant subset include at least a single judged relevant passage. We measure relevance assessment using: (i) *hallucination rate*, measuring model tendency to hallucinate when the answer is not present in passages in the non-relevant subset, and (ii) *error rate*, measuring model inaccuracy to recognize relevant passages in the relevant subset. In our work, we observe that most models struggle to balance the two capacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination rate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can achieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is observed to provide the best tradeoff on both subsets, highlighting future work necessary to improve LLM robustness. NoMIRACL dataset and evaluation code are available at: https://github.com/project-miracl/nomiracl.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.730",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Can We Instruct LLMs to Compensate for Position Bias?": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-instruct",
        "author": "Zhang, Meiru and Meng, Zaiqiao and Collier, Nigel",
        "booktitle": "EMNLP-findings2024",
        "title": "Can We Instruct LLMs to Compensate for Position Bias?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Position bias in large language models (LLMs) leads to difficulty in accessing information retrieved from the retriever, thus downgrading the effectiveness of Retrieval-Augmented Generation (RAG) approaches in open-question answering. Recent studies reveal that this bias is related to disproportional attention across the context. In this work, we examine how to direct LLMs to allocate more attention towards a selected segment of the context through prompting, aiming to compensate for the shortage of attention. We find that language models do not have relative position awareness of the context but can be directed by promoting instruction with an exact document index. Our analysis contributes to a deeper understanding of position bias in LLMs and provides a pathway to mitigate this bias by instruction, thus benefiting LLMs in locating and utilizing relevant information from retrieved documents in RAG applications. The code and data in our study have been made publicly available.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.732",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models": {
        "type": "INPROCEEDINGS",
        "key": "lee-etal-2024-train",
        "author": "Lee, Jaeyoung and Lu, Ximing and Hessel, Jack and Brahman, Faeze and Yu, Youngjae and Bisk, Yonatan and Choi, Yejin and Gabriel, Saadia",
        "booktitle": "EMNLP-findings2024",
        "title": "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Given the growing influx of misinformation across news and social media, there is a critical need for systems that can provide effective real-time verification of news claims. Large language or multimodal model based verification has been proposed to scale up online policing mechanisms for mitigating spread of false and harmful content. While these can potentially reduce burden on human fact-checkers, such efforts may be hampered by foundation model training data becoming outdated. In this work, we test the limits of improving foundation model performance without continual updating through an initial study of knowledge transfer using either existing intra- and inter-domain benchmarks or explanations generated from large language models (LLMs).We evaluate on 12 public benchmarks for fact-checking and misinformation detection as well as two other tasks relevant to content moderation - toxicity and stance detection. Our results on two recent multi-modal fact-checking benchmarks, Mocheg and Fakeddit, indicate that knowledge transfer strategies can improve Fakeddit performance over the state-of-the-art by up to 1.7% and Mocheg performance by up to 2.9%. The code, model checkpoints, and dataset are available: https://github.com/given131/ fact-verifier-knowledge-transfer.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.764",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "What Matters in Memorizing and Recalling Facts? Multifaceted Benchmarks for Knowledge Probing in Language Models": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-matters",
        "author": "Zhao, Xin and Yoshinaga, Naoki and Oba, Daisuke",
        "booktitle": "EMNLP-findings2024",
        "title": "What Matters in Memorizing and Recalling Facts? Multifaceted Benchmarks for Knowledge Probing in Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Language models often struggle with handling factual knowledge, exhibiting factual hallucination issue. This makes it vital to evaluate the models\u2019 ability to recall its parametric knowledge about facts. In this study, we introduce a knowledge probing benchmark, BELIEF(ICL), to evaluate the knowledge recall ability of both encoder- and decoder-based pre-trained language models (PLMs) from diverse perspectives. BELIEFs utilize a multi-prompt dataset to evaluate PLM\u2019s accuracy, consistency, and reliability in factual knowledge recall. To enable a more reliable evaluation with BELIEFs, we semi-automatically create MyriadLAMA, which has massively diverse prompts. We validate the effectiveness of BELIEFs in comprehensively evaluating PLM\u2019s knowledge recall ability on diverse PLMs, including recent large language models (LLMs). We then investigate key factors in memorizing and recalling facts in PLMs, such as model size, pretraining strategy and corpora, instruction-tuning process and in-context learning settings. Finally, we reveal the limitation of the prompt-based knowledge probing. The MyriadLAMA is publicized.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.771",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "On Leakage of Code Generation Evaluation Datasets": {
        "type": "INPROCEEDINGS",
        "key": "matton-etal-2024-leakage",
        "author": "Matton, Alexandre and Sherborne, Tom and Aumiller, Dennis and Tommasone, Elena and Alizadeh, Milad and He, Jingyi and Ma, Raymond and Voisin, Maxime and Gilsenan-McMahon, Ellen and Gall\u00e9, Matthias",
        "booktitle": "EMNLP-findings2024",
        "title": "On Leakage of Code Generation Evaluation Datasets",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we consider contamination by code generation test sets, in particular in their use in modern large language models.We discuss three possible sources of such contamination and show findings supporting each of them: (i) direct data leakage, (ii) indirect data leakage through the use of synthetic data and (iii) overfitting to evaluation sets during model selection.To address this, we release Less Basic Python Problems (LBPP): an uncontaminated new benchmark of 161 prompts with their associated Python solutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.772",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization": {
        "type": "INPROCEEDINGS",
        "key": "xie-etal-2024-v",
        "author": "Xie, Yuxi and Li, Guanzhen and Xu, Xiao and Kan, Min-Yen",
        "booktitle": "EMNLP-findings2024",
        "title": "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large vision-language models (LVLMs) suffer from hallucination, resulting in misalignment between the output textual response and the input visual content. Recent research indicates that the over-reliance on the Large Language Model (LLM) backbone, as one cause of the LVLM hallucination, inherently introduces bias from language priors, leading to insufficient context attention to the visual inputs.We tackle this issue of hallucination by mitigating such over-reliance through preference learning. We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual context learning at training time. To interpret the effectiveness and generalizability of V-DPO on different types of training data, we construct a synthetic dataset containing both response- and image-contrast preference pairs, compared against existing human-annotated hallucination samples. Our approach achieves significant improvements compared with baseline methods across various hallucination benchmarks. Our analysis indicates that V-DPO excels in learning from image-contrast preference data, demonstrating its superior ability to elicit and understand nuances of visual context. Our code is publicly available at https://github.com/YuxiXie/V-DPOhttps://github.com/YuxiXie/V-DPO.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.775",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Pedagogical Alignment of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "sonkar-etal-2024-pedagogical",
        "author": "Sonkar, Shashank and Ni, Kangqi and Chaudhary, Sapana and Baraniuk, Richard",
        "booktitle": "EMNLP-findings2024",
        "title": "Pedagogical Alignment of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs), when used in educational settings without pedagogical fine-tuning, often provide immediate answers rather than guiding students through the problem-solving process. This approach falls short of pedagogically best practices and limits their effectiveness as educational tools. We term the objective of training LLMs to emulate effective teaching strategies as \u2018pedagogical alignment.\u2019 In this paper, we investigate Learning from Human Preferences () algorithms to achieve this alignment objective. A key challenge in this process is the scarcity of high-quality preference datasets to guide the alignment. To address this, we propose a novel approach for constructing a large-scale dataset using synthetic data generation techniques, eliminating the need for time-consuming and costly manual annotation. Leveraging this dataset, our experiments with Llama and Mistral models demonstrate that LHP methods outperform standard supervised fine-tuning (SFT), improving pedagogical alignment accuracy by 13.1% and 8.7% respectively.Existing evaluation methods also lack quantitative metrics to adequately measure the pedagogical alignment of LLMs. To address this gap, we propose novel perplexity-based metrics that quantify LLMs\u2019 tendency to provide scaffolded guidance versus direct answers, offering a robust measure of pedagogical alignment. Our analysis provides compelling evidence for the superiority of methods over SFT in optimizing LLMs\u2019 behavior, underscoring the potential of methods in better aligning LLMs with educational objectives and fostering effective learning experiences. Code and models are available here.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.797",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "TOWER: Tree Organized Weighting for Evaluating Complex Instructions": {
        "type": "INPROCEEDINGS",
        "key": "ziems-etal-2024-tower",
        "author": "Ziems, Noah and Zhang, Zhihan and Jiang, Meng",
        "booktitle": "EMNLP-findings2024",
        "title": "TOWER: Tree Organized Weighting for Evaluating Complex Instructions",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Evaluating the ability of large language models (LLMs) to follow complex human-written instructions is essential for their deployment in real-world applications. While benchmarks like Chatbot Arena use human judges to assess model performance, they are resource-intensive and time-consuming. Alternative methods using LLMs as judges, such as AlpacaEval, MT Bench, WildBench, and InFoBench offer improvements but still do not capture that certain complex instruction aspects are more important than others to follow.To address this gap, we propose a novel evaluation metric, TOWER, that incorporates human-judged importance into the assessment of complex instruction following. We show that human annotators agree with tree-based representations of these complex instructions nearly as much as they agree with other human annotators. We release tree-based annotations of the InFoBench dataset and the corresponding evaluation code to facilitate future research.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.809",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "QEFT: Quantization for Efficient Fine-Tuning of LLMs": {
        "type": "INPROCEEDINGS",
        "key": "lee-etal-2024-qeft",
        "author": "Lee, Changhun and Jin, Jun-gyu and Cho, YoungHyun and Park, Eunhyeok",
        "booktitle": "EMNLP-findings2024",
        "title": "QEFT: Quantization for Efficient Fine-Tuning of LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the rapid growth in the use of fine-tuning for large language models (LLMs), optimizing fine-tuning while keeping inference efficient has become highly important. However, this is a challenging task as it requires improvements in all aspects, including inference speed, fine-tuning speed, memory consumption, and, most importantly, model quality. Previous studies have attempted to achieve this by combining quantization with fine-tuning, but they have failed to enhance all four aspects simultaneously. In this study, we propose a new lightweight technique called Quantization for Efficient Fine-Tuning (QEFT). QEFT accelerates both inference and fine-tuning, is supported by robust theoretical foundations, offers high flexibility, and maintains good hardware compatibility. Our extensive experiments demonstrate that QEFT matches the quality and versatility of full-precision parameter-efficient fine-tuning, while using fewer resources. Our code is available at https://github.com/xvyaward/qeft.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.811",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-drattack",
        "author": "Li, Xirui and Wang, Ruochen and Cheng, Minhao and Zhou, Tianyi and Hsieh, Cho-Jui",
        "booktitle": "EMNLP-findings2024",
        "title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Safety-aligned Large Language Models (LLMs) are still vulnerable to some manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, existing jailbreaking methods usually view a harmful prompt as a whole but they are not effective at reducing LLMs\u2019 attention on combinations of words with malice, which well-aligned LLMs can easily reject. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively reduce LLMs\u2019 attention on harmful words by presenting them to LLMs in a fragmented form, thereby addressing these limitations and improving attack effectiveness. We introduce an automatic prompt Decomposition and Reconstruction framework for jailbreaking Attack (DrAttack). DrAttack consists of three key components: (a) \u2018Decomposition\u2019 of the original prompt into sub-prompts, (b) \u2018Reconstruction\u2019 of these sub-prompts implicitly by In-Context Learning with semantically similar but benign reassembling example, and (c) \u2018Synonym Search\u2019 of sub-prompts, aiming to find sub-prompts\u2019 synonyms that maintain the original intent while jailbreaking LLMs. An extensive empirical study across multiple open-source and closed-source LLMs demonstrates that, with fewer queries, DrAttack obtains a substantial gain of success rate on powerful LLMs over prior SOTA attackers. Notably, the success rate of 80% on GPT-4 surpassed previous art by 65%. Code and data are made publicly available at https://turningpoint-ai.github.io/DrAttack/.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.813",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-factcheck",
        "author": "Wang, Yuxia and Gangi Reddy, Revanth and Mujahid, Zain Muhammad and Arora, Arnav and Rubashevskii, Aleksandr and Geng, Jiahui and Mohammed Afzal, Osama and Pan, Liangming and Borenstein, Nadav and Pillai, Aditya and Augenstein, Isabelle and Gurevych, Iryna and Nakov, Preslav",
        "booktitle": "EMNLP-findings2024",
        "title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present Factcheck-Bench, a holistic end-to-end framework for annotating and evaluating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels for fact-checking and correcting not just the final prediction, but also the intermediate steps that a fact-checking system might need to take. Based on this framework, we construct an open-domain factuality benchmark in three-levels of granularity: claim, sentence, and document. We further propose a system, Factcheck-GPT, which follows our framework, and we show that it outperforms several popular LLM fact-checkers. We make our annotation tool, annotated data, benchmark, and code available at https://github.com/yuxiaw/Factcheck-GPT.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.830",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory": {
        "type": "INPROCEEDINGS",
        "key": "lee-etal-2024-cactus",
        "author": "Lee, Suyeon and Kim, Sunghwan and Kim, Minju and Kang, Dongjin and Yang, Dongil and Kim, Harim and Kang, Minseok and Jung, Dayi and Kim, Min Hee and Lee, Seungbeen and Chung, Kyong-Mee and Yu, Youngjae and Lee, Dongha and Yeo, Jinyoung",
        "booktitle": "EMNLP-findings2024",
        "title": "Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, the demand for psychological counseling has significantly increased as more individuals express concerns about their mental health. This surge has accelerated efforts to improve the accessibility of counseling by using large language models (LLMs) as counselors. To ensure client privacy, training open-source LLMs faces a key challenge: the absence of realistic counseling datasets. To address this, we introduce Cactus, a multi-turn dialogue dataset that emulates real-life interactions using the goal-oriented and structured approach of Cognitive Behavioral Therapy (CBT).We create a diverse and realistic dataset by designing clients with varied, specific personas, and having counselors systematically apply CBT techniques in their interactions. To assess the quality of our data, we benchmark against established psychological criteria used to evaluate real counseling sessions, ensuring alignment with expert evaluations.Experimental results demonstrate that Camel, a model trained with Cactus, outperforms other models in counseling skills, highlighting its effectiveness and potential as a counseling agent.We make our data, model, and code publicly available.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.832",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-self-supervised-preference",
        "author": "Li, Jian and Huang, Haojing and Zhang, Yujia and Xu, Pengfei and Chen, Xi and Song, Rui and Shi, Lida and Wang, Jingwen and Xu, Hao",
        "booktitle": "EMNLP-findings2024",
        "title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, there has been significant interest in replacing the reward model in Reinforcement Learning with Human Feedback (RLHF) methods for Large Language Models (LLMs), such as Direct Preference Optimization (DPO) and its variants. These approaches commonly use a binary cross-entropy mechanism on pairwise samples, i.e., minimizing and maximizing the loss based on preferred or dis-preferred responses, respectively. However, while this training strategy omits the reward model, it also overlooks the varying preference degrees within different responses. We hypothesize that this is a key factor hindering LLMs from sufficiently understanding human preferences. To address this problem, we propose a novel Self-supervised Preference Optimization (SPO) framework, which constructs a self-supervised preference degree loss combined with the alignment loss, thereby helping LLMs improve their ability to understand the degree of preference. Extensive experiments are conducted on two widely used datasets of different tasks. The results demonstrate that SPO can be seamlessly integrated with existing preference optimization methods and significantly boost their performance to achieve state-of-the-art performance. We also conduct detailed analyses to offer comprehensive insights into SPO, which verifies its effectiveness. The code is available at https://github.com/lijian16/SPO.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.845",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Mitigating Hallucination in Fictional Character Role-Play": {
        "type": "INPROCEEDINGS",
        "key": "sadeq-etal-2024-mitigating",
        "author": "Sadeq, Nafis and Xie, Zhouhang and Kang, Byungkyu and Lamba, Prarit and Gao, Xiang and McAuley, Julian",
        "booktitle": "EMNLP-findings2024",
        "title": "Mitigating Hallucination in Fictional Character Role-Play",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Role-playing has wide-ranging applications in customer support, embodied agents, and computational social science. The influence of parametric world knowledge of large language models (LLMs) often causes role-playing characters to act out of character and to hallucinate about things outside the scope of their knowledge. In this work, we focus on the evaluation and mitigation of hallucination in fictional character role-play. We introduce a dataset with over 2,000 characters and 72,000 interviews, including 18,000 adversarial questions. We propose RoleFact, a role-playing method that mitigates hallucination by modulating the influence of parametric knowledge using a pre-calibrated confidence threshold. Experiments show that the proposed method improves the factual precision of generated responses by 18% for adversarial questions with a 44% reduction in temporal hallucination for time-sensitive interviews. The code and the dataset are available at https://github.com/NafisSadeq/rolefact.git.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.846",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "POSIX: A Prompt Sensitivity Index For Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chatterjee-etal-2024-posix",
        "author": "Chatterjee, Anwoy and Renduchintala, H. S. V. N. S. Kowndinya and Bhatia, Sumit and Chakraborty, Tanmoy",
        "booktitle": "EMNLP-findings2024",
        "title": "POSIX: A Prompt Sensitivity Index For Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling errors, alteration of wording or the prompt template. However, while assessing the quality of an LLM, the focus often tends to be solely on its performance on downstream tasks, while very little to no attention is paid to prompt sensitivity. To fill this gap, we propose POSIX \u2013 a novel PrOmpt Sensitivity IndeX as a reliable measure of prompt sensitivity, thereby offering a more comprehensive evaluation of LLM performance. The key idea behind POSIX is to capture the relative change in loglikelihood of a given response upon replacing the corresponding prompt with a different intent-preserving prompt. We provide thorough empirical evidence demonstrating the efficacy of POSIX in capturing prompt sensitivity and subsequently use it to measure and thereby compare prompt sensitivity of various open source LLMs. We find that merely increasing the parameter count or instruction tuning does not necessarily reduce prompt sensitivity whereas adding some few-shot exemplars, even just one, almost always leads to significant decrease in prompt sensitivity. We also find that alterations to prompt template lead to the highest sensitivity in the case of MCQ type tasks, whereas paraphrasing results in the highest sensitivity in open-ended generation tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinya-renduchintala/POSIX.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.852",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "LEGOBench: Scientific Leaderboard Generation Benchmark": {
        "type": "INPROCEEDINGS",
        "key": "singh-etal-2024-legobench",
        "author": "Singh, Shruti and Alam, Shoaib and Malwat, Husain and Singh, Mayank",
        "booktitle": "EMNLP-findings2024",
        "title": "LEGOBench: Scientific Leaderboard Generation Benchmark",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce LEGOBench, a benchmark for evaluating systems that generate scientific leaderboards. LEGOBench is curated from 22 years of preprint submission data on arXiv and more than 11k machine learning leaderboards on the PapersWithCode portal. We present a language model-based and four graph-based leaderboard generation task configuration. We evaluate popular encoder-only scientific language models as well as decoder-only large language models across these task configurations. State-of-the-art models showcase significant performance gaps in automatic leaderboard generation on LEGOBench. The code is available on GitHub and the dataset is hosted on OSF.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.855",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning": {
        "type": "INPROCEEDINGS",
        "key": "feng-etal-2024-long",
        "author": "Feng, Aosong and Ying, Rex and Tassiulas, Leandros",
        "booktitle": "EMNLP-findings2024",
        "title": "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "As the demand for processing extended textual data grows, the ability to handle long-range dependencies and maintain computational efficiency is more critical than ever. One of the key issues for long-sequence modeling using attention-based model is the mismatch between the limited-range modeling power of full attention and the long-range token dependency in the input sequence. In this work, we propose to scale up the attention receptive field by tensorizing long input sequences into compact tensor representations followed by attention on each transformed dimension. The resulting Tensorized Attention can be adopted as efficient transformer backbones to extend input context length with improved memory and time efficiency. We show that the proposed attention tensorization encodes token dependencies as a multi-hop attention process, and is equivalent to Kronecker decomposition of full attention. Extensive experiments show that tensorized attention can be used to adapt pretrained LLMs with improved efficiency. Notably, using customized Triton kernels, tensorization enables Llama-8B training under 32,768 context length and can steadily extrapolate to 128k length during inference with 11 times speedup (compared to full attention with FlashAttention-2).",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.858",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression": {
        "type": "INPROCEEDINGS",
        "key": "choi-etal-2024-reading",
        "author": "Choi, Eunseong and Lee, Sunkyung and Choi, Minjin and Park, Jun and Lee, Jongwuk",
        "booktitle": "EMNLP-findings2024",
        "title": "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have achieved significant performance gains using advanced prompting techniques over various tasks. However, the increasing length of prompts leads to high computational costs and often obscures crucial information. Prompt compression has been proposed to alleviate these issues, but it faces challenges in (i) capturing the global context and (ii) training the compressor effectively. To tackle these challenges, we introduce a novel prompt compression method, namely Reading To Compressing (R2C), utilizing the Fusion-in-Decoder (FiD) architecture to identify the important information in the prompt. Specifically, the cross-attention scores of the FiD are used to discern essential chunks and sentences from the prompt. R2C effectively captures the global context without compromising semantic consistency while detouring the necessity of pseudo-labels for training the compressor. Empirical results show that R2C retains key contexts, enhancing the LLM performance by 6% in out-of-domain evaluations while reducing the prompt length by 80%.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.864",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization": {
        "type": "INPROCEEDINGS",
        "key": "zhan-etal-2024-unlocking",
        "author": "Zhan, Heshen and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu",
        "booktitle": "EMNLP-findings2024",
        "title": "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Prompt optimization emerges as an important technique for adapting Large Language Models (LLMs) to specific tasks. Unfortunately, LLM proprietors often limit access to models\u2019 internal weights, confining users to inference API services. This restriction poses a significant challenge for prompt optimization, as conventional optimization-based algorithms rely heavily on gradient information, which is unavailable via inference APIs. Addressing this challenge, this paper presents the Zeroth-Order Tuning (ZOT) approach, which enables efficient prompt tuning solely via inference APIs. ZOT adopts the zeroth-order optimization framework, utilizing finite differences to approximate gradient information. We further incorporate ZOT with gradient clipping and momentum techniques to enhance the tuning effectiveness. Experimental results show that ZOT outperforms existing black-box prompt tuning methods in terms of both task-specific performance and convergence speed. Furthermore, we provide a theoretical explanation for the unexpectedly strong performance of zeroth-order methods on LLM prompt tuning. By introducing the concept of effective dimension, we establish a strong connection between the inherently low effective dimension of prompt spaces and the superior convergence speed of zeroth-order methods. Our code is available at https://github.com/ZhanHeshen/ZOT.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.871",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Analyzing Context Contributions in LLM-based Machine Translation": {
        "type": "INPROCEEDINGS",
        "key": "zaranis-etal-2024-analyzing",
        "author": "Zaranis, Emmanouil and Guerreiro, Nuno M. and Martins, Andre",
        "booktitle": "EMNLP-findings2024",
        "title": "Analyzing Context Contributions in LLM-based Machine Translation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have achieved state-of-the-art performance in machine translation (MT) and demonstrated the ability to leverage in-context learning through few-shot examples. However, the mechanisms by which LLMs use different parts of the input context remain largely unexplored. In this work, we provide a comprehensive analysis of context utilization in MT, studying how LLMs use various context parts, such as few-shot examples and the source text, when generating translations. We highlight several key findings: (1) the source part of few-shot examples appears to contribute more than its corresponding targets, irrespective of translation direction; (2) finetuning LLMs with parallel data alters the contribution patterns of different context parts; and (3) there is a positional bias where earlier few-shot examples have higher contributions to the translated sequence. Finally, we demonstrate that inspecting anomalous context contributions can potentially uncover pathological translations, such as hallucinations. Our findings shed light on the internal workings of LLM-based MT which go beyond those known for standard encoder-decoder MT models.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.876",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation": {
        "type": "INPROCEEDINGS",
        "key": "garces-arias-etal-2024-adaptive",
        "author": "Garces Arias, Esteban and Rodemann, Julian and Li, Meimingwei and Heumann, Christian and A\u00dfenmacher, Matthias",
        "booktitle": "EMNLP-findings2024",
        "title": "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Despite the remarkable capabilities of large language models, generating high-quality text remains a challenging task. Numerous decoding strategies\u2014such as beam search, sampling with temperature, top\u2010k sampling, nucleus (top\u2010p) sampling, typical decoding, contrastive decoding, and contrastive search\u2014have been proposed to address these challenges by improving coherence, diversity, and resemblance to human-generated text. In this study, we introduce Adaptive Contrastive Search (ACS), a novel decoding strategy that extends contrastive search (CS) by incorporating an adaptive degeneration penalty informed by the model\u2019s estimated uncertainty at each generation step. ACS aims to enhance creativity and diversity while maintaining coherence to produce high-quality outputs. Extensive experiments across various model architectures, languages, and datasets demonstrate that our approach improves both creativity and coherence, underscoring its effectiveness in text-generation tasks. We release our code, datasets, and models to facilitate further research.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.885",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "rathore-etal-2024-ssp",
        "author": "Rathore, Vipul Kumar and Deb, Aniruddha and Chandresh, Ankish Kumar and Singla, Parag and ., Mausam",
        "booktitle": "EMNLP-findings2024",
        "title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, very large language models (LLMs) have shown exceptional performance on several English NLP tasks with just in-context learning (ICL), but their utility in other languages is still underexplored. We investigate their effectiveness for NLP tasks in low-resource languages (LRLs), especially in the setting of zero-labelled cross-lingual transfer (0-CLT), where no labelled training data for the target language is available \u2013 however training data from one or more related medium-resource languages (MRLs) is utilized, alongside the available unlabeled test data for a target language. We introduce Self-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT setting. SSP is based on the key observation that LLMs output more accurate labels if in-context exemplars are from the target language (even if their labels are slightly noisy). To operationalize this, since target language training data is not available in 0-CLT, SSP operates in two stages. In Stage I, using source MRL training data, target language\u2019s test data is noisily labeled. In Stage II, these noisy test data points are used as exemplars in ICL for further improved labelling. Additionally, our implementation of SSP uses a novel Integer Linear Programming (ILP)-based exemplar selection that balances similarity, prediction confidence (when available) and label coverage. Experiments on three tasks and eleven LRLs (from three regions) demonstrate that SSP strongly outperforms existing SOTA fine-tuned and prompting-based baselines in 0-CLT setup.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.886",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "When \u201dA Helpful Assistant\u201d Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "zheng-etal-2024-helpful",
        "author": "Zheng, Mingqian and Pei, Jiaxin and Logeswaran, Lajanugen and Lee, Moontae and Jurgens, David",
        "booktitle": "EMNLP-findings2024",
        "title": "When \u201dA Helpful Assistant\u201d Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Prompting serves as the major way humans interact with Large Language Models (LLM). Commercial AI systems commonly define the role of the LLM in system prompts. For example, ChatGPT uses \u201dYou are a helpful assistant\u201d as part of its default system prompt. Despite current practices of adding personas to system prompts, it remains unclear how different personas affect a model\u2019s performance on objective tasks. In this study, we present a systematic evaluation of personas in system prompts. We curate a list of 162 roles covering 6 types of interpersonal relationships and 8 domains of expertise. Through extensive analysis of 4 popular families of LLMs and 2,410 factual questions, we demonstrate that adding personas in system prompts does not improve model performance across a range of questions compared to the control setting where no persona is added. Nevertheless, further analysis suggests that the gender, type, and domain of the persona can all influence the resulting prediction accuracies. We further experimented with a list of persona search strategies and found that, while aggregating results from the best persona for each question significantly improves prediction accuracy, automatically identifying the best persona is challenging, with predictions often performing no better than random selection. Overall, our findings suggest that while adding a persona may lead to performance gains in certain settings, the effect of each persona can be largely random. %Our results can help inform the design of system prompts for AI systems. Code and data are available at https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.888",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks": {
        "type": "INPROCEEDINGS",
        "key": "kim-etal-2024-towards-efficient",
        "author": "Kim, Sungkyung and Lee, Adam and Park, Junyoung and Chung, Andrew and Oh, Jusang and Lee, Jay-Yoon",
        "booktitle": "EMNLP-findings2024",
        "title": "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advancements in large language models have demonstrated enhanced capabilities in visual reasoning tasks by employing additional encoders for aligning different modalities. While the Q-Former has been widely used as a general encoder for aligning several modalities including image, video, audio, and 3D with large language models, previous works on its efficient training and the analysis of its individual components have been limited. In this work, we investigate the effectiveness of parameter efficient fine-tuning (PEFT) the Q-Former using InstructBLIP with visual reasoning benchmarks ScienceQA and IconQA. We observe that applying PEFT to the Q-Former achieves comparable performance to full fine-tuning using under 2% of the trainable parameters. Additionally, we employ AdaLoRA for dynamic parameter budget reallocation to examine the relative importance of the Q-Former\u2019s sublayers with 4 different benchmarks. Our findings reveal that the self-attention layers are noticeably more important in perceptual visual-language reasoning tasks, and relative importance of FFN layers depends on the complexity of visual-language patterns involved in tasks. The code is available at https://github.com/AttentionX/InstructBLIP_PEFT.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.889",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference": {
        "type": "INPROCEEDINGS",
        "key": "monteiro-etal-2024-xc",
        "author": "Monteiro, Joao and Marcotte, \u00c9tienne and Noel, Pierre-Andre and Zantedeschi, Valentina and Vazquez, David and Chapados, Nicolas and Pal, Christopher and Taslakian, Perouz",
        "booktitle": "EMNLP-findings2024",
        "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Prompts are often employed to condition decoder-only language model generation on reference information. Just-in-time processing of a context is inefficient due to the quadratic cost of self-attention operations, and caching is desirable. However, caching transformer states can easily require almost as much space as the model parameters. When the right context is not known in advance, caching the prompt can be challenging. This work addresses these limitations by introducing models that, inspired by the encoder-decoder architecture, use cross-attention to condition generation on reference text without the prompt. More precisely, we leverage pre-trained decoder-only models and only train a small number of added layers. We use Question-Answering (QA) as a testbed to evaluate the ability of our models to perform conditional generation and observe that they outperform prompt-based inference methods, are comparable to fine-tuned prompted LLMs, and drastically reduce the space footprint relative to standard KV caching by two orders of magnitude. Specifically, we introduced XC-Llama which converts a pre-trained Llama 2 into an encoder-decoder architecture by integrating cross-attention layers interleaved in between existing self-attention layers.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.896",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction": {
        "type": "INPROCEEDINGS",
        "key": "george-etal-2024-probing",
        "author": "George, Sonny and Sypherd, Chris and Cashman, Dylan",
        "booktitle": "EMNLP-findings2024",
        "title": "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language model (LLM) agents show promise in an increasing number of domains. In many proposed applications, it is expected that the agent reasons over accumulated experience presented in an input prompt. We propose the OEDD (Operationalize Experience Despite Distraction) corpus, a human-annotator-validated body of scenarios with pre-scripted agent histories where the agent must make a decision based on disparate experiential information in the presence of a distractor. We evaluate three state-of-the-art LLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal chain-of-thought prompting strategy and observe that when (1) the input context contains over 1,615 tokens of historical interactions, (2) a crucially decision-informing premise is the rightful conclusion over two disparate environment premises, and (3) a trivial, but distracting red herring fact follows, all LLMs perform worse than random choice at selecting the better of two actions. Our code and test corpus are publicly available at: [github.com/sonnygeorge/OEDD](github.com/sonnygeorge/OEDD).",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.905",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "chidambaram-etal-2024-socratic",
        "author": "Chidambaram, Subramanian and Li, Li Erran and Bai, Min and Li, Xiaopeng and Lin, Kaixiang and Zhou, Xiong and Williams, Alex C.",
        "booktitle": "EMNLP-findings2024",
        "title": "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) are increasingly used for generating code solutions, empowered by features like self-debugging and self-reflection. However, LLMs often struggle with complex programming problems without human guidance. This paper investigates the strategies employed by expert programmers to steer code-generating LLMs toward successful outcomes. Through a study involving experts using natural language to guide GPT-4, Gemini Ultra, and, Claude 3.5 Sonnet on highly difficult programming challenges, we frame our analysis using the \u201cSocratic Feedback\u201d paradigm for understanding effective steering strategies. By analyzing 30 conversational transcripts across all three models, we map observed feedback strategies to five stages of Socratic Questioning: Definition, Elenhus, Maieutic, Dialectic, and Counter-factual reasoning. We find evidence that by employing a combination of different Socratic feedback strategies across multiple turns, programmers successfully guided the models to solve 74% of the problems that the models initially failed to solve on their own.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.908",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets": {
        "type": "INPROCEEDINGS",
        "key": "walsh-etal-2024-sonnet",
        "author": "Walsh, Melanie and Antoniak, Maria and Preus, Anna",
        "booktitle": "EMNLP-findings2024",
        "title": "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) can now generate and recognize poetry. But what do LLMs really know about poetry? We develop a task to evaluate how well LLMs recognize one aspect of English-language poetry\u2014poetic form\u2014which captures many different poetic features, including rhyme scheme, meter, and word or line repetition. By using a benchmark dataset of over 4.1k human expert-annotated poems, we show that state-of-the-art LLMs can successfully identify both common and uncommon fixed poetic forms\u2014such as sonnets, sestinas, and pantoums\u2014with surprisingly high accuracy. However, performance varies significantly by poetic form; the models struggle to identify unfixed poetic forms, especially those based on topic or visual features. We additionally measure how many poems from our benchmark dataset are present in popular pretraining datasets or memorized by GPT-4, finding that pretraining presence and memorization may improve performance on this task, but results are inconclusive. We release a benchmark evaluation dataset with 1.4k public domain poems and form annotations, results of memorization experiments and data audits, and code.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.914",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs": {
        "type": "INPROCEEDINGS",
        "key": "akash-chang-2024-enhancing",
        "author": "Akash, Pritom Saha and Chang, Kevin Chen-Chuan",
        "booktitle": "EMNLP-findings2024",
        "title": "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Topic modeling is a powerful technique for uncovering hidden themes within a collection of documents. However, the effectiveness of traditional topic models often relies on sufficient word co-occurrence, which is lacking in short texts. Therefore, existing approaches, whether probabilistic or neural, frequently struggle to extract meaningful patterns from such data, resulting in incoherent topics. To address this challenge, we propose a novel approach that leverages large language models (LLMs) to extend short texts into more detailed sequences before applying topic modeling. To further improve the efficiency and solve the problem of semantic inconsistency from LLM-generated texts, we propose to use prefix tuning to train a smaller language model coupled with a variational autoencoder for short-text topic modeling. Our method significantly improves short-text topic modeling performance, as demonstrated by extensive experiments on real-world datasets with extreme data sparsity, outperforming current state-of-the-art topic models.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.917",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs": {
        "type": "INPROCEEDINGS",
        "key": "nag-etal-2024-cost",
        "author": "Nag, Arijit and Mukherjee, Animesh and Ganguly, Niloy and Chakrabarti, Soumen",
        "booktitle": "EMNLP-findings2024",
        "title": "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) exhibit impressive zero/few-shot inference and generation quality for high-resource languages (HRLs). A few of them have been trained on low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The number of tokens strongly depends on the script and language, as well as the LLM\u2019s subword vocabulary. We show that LRLs are at a pricing disadvantage, because the well-known LLMs produce more tokens for LRLs than HRLs. This is because most currently popular LLMs are optimized for HRL vocabularies. Our objective is to level the playing field: reduce the cost of processing LRLs in contemporary LLMs while ensuring that predictive and generative qualities are not compromised. As means to reduce the number of tokens processed by the LLM, we consider code-mixing, translation, and transliteration of LRLs to HRLs. We perform an extensive study using the IndicXTREME classification and six generative tasks dataset, covering 15 Indic and 3 other languages, while using GPT-4 (one of the costliest LLM services released so far) as a commercial LLM. We observe and analyze interesting patterns involving token count, cost, and quality across a multitude of languages and tasks. We show that choosing the best policy to interact with the LLM can reduce cost by ~90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.920",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Multi-label Sequential Sentence Classification via Large Language Model": {
        "type": "INPROCEEDINGS",
        "key": "lan-etal-2024-multi",
        "author": "Lan, Mengfei and Zheng, Lecheng and Ming, Shufan and Kilicoglu, Halil",
        "booktitle": "EMNLP-findings2024",
        "title": "Multi-label Sequential Sentence Classification via Large Language Model",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Sequential sentence classification (SSC) in scientific publications is crucial for supporting downstream tasks such as fine-grained information retrieval and extractive summarization. However, current SSC methods are constrained by model size, sequence length, and single-label setting. To address these limitations, this paper proposes LLM-SSC, a large language model (LLM)-based framework for both single- and multi-label SSC tasks. Unlike previous approaches that employ small- or medium-sized language models, the proposed framework utilizes LLMs to generate SSC labels through designed prompts, which enhance task understanding by incorporating demonstrations and a query to describe the prediction target. We also present a multi-label contrastive learning loss with auto-weighting scheme, enabling the multi-label classification task. To support our multi-label SSC analysis, we introduce and release a new dataset, biorc800, which mainly contains unstructured abstracts in the biomedical domain with manual annotations. Experiments demonstrate LLM-SSC\u2019s strong performance in SSC under both in-context learning and task-specific tuning settings. We release biorc800 and our code at: https://github.com/ScienceNLP-Lab/LLM-SSC.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.944",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "The Overlooked Repetitive Lengthening Form in Sentiment Analysis": {
        "type": "INPROCEEDINGS",
        "key": "wang-dragut-2024-overlooked",
        "author": "Wang, Lei and Dragut, Eduard",
        "booktitle": "EMNLP-findings2024",
        "title": "The Overlooked Repetitive Lengthening Form in Sentiment Analysis",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Individuals engaging in online communication frequently express personal opinions with informal styles (e.g., memes and emojis). While Language Models (LMs) with informal communications have been widely discussed, a unique and emphatic style, the Repetitive Lengthening Form (RLF), has been overlooked for years. In this paper, we explore answers to two research questions: 1) Is RLF important for SA? 2) Can LMs understand RLF? Inspired by previous linguistic research, we curate **Lengthening**, the first multi-domain dataset with 850k samples focused on RLF for sentiment analysis. Moreover, we introduce **Explnstruct**, a two-stage Explainable Instruction Tuning framework aimed at improving both the performance and explainability of LLMs for RLF. We further propose a novel unified approach to quantify LMs\u2019 understanding of informal expressions. We show that RLF sentences are expressive expressions and can serve as signatures of document-level sentiment. Additionally, RLF has potential value for online content analysis. Our comprehensive results show that fine-tuned Pre-trained Language Models (PLMs) can surpass zero-shot GPT-4 in performance but not in explanation for RLF. Finally, we show ExpInstruct can improve the open-sourced LLMs to match zero-shot GPT-4 in performance and explainability for RLF with limited samples. Code and sample data are available at https://github.com/Tom-Owl/OverlookedRLF",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.952",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "FactAlign: Long-form Factuality Alignment of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "huang-chen-2024-factalign",
        "author": "Huang, Chao-Wei and Chen, Yun-Nung",
        "booktitle": "EMNLP-findings2024",
        "title": "FactAlign: Long-form Factuality Alignment of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs\u2019 long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.955",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MiRAGeNews: Multimodal Realistic AI-Generated News Detection": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-miragenews",
        "author": "Huang, Runsheng and Dugan, Liam and Yang, Yue and Callison-Burch, Chris",
        "booktitle": "EMNLP-findings2024",
        "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The proliferation of inflammatory or misleading \u201cfake\u201d news content has become increasingly common in recent years. Simultaneously, it has become easier than ever to use AI tools to generate photorealistic images depicting any scene imaginable. Combining these two\u2014AI-generated fake news content\u2014is particularly potent and dangerous. To combat the spread of AI-generated fake news, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real and AI-generated image-caption pairs from state-of-the-art generators. We find that our dataset poses a significant challenge to humans (60% F-1) and state-of-the-art multi-modal LLMs (\\textless 24% F-1). Using our dataset we train a multi-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art baselines on image-caption pairs from out-of-domain image generators and news publishers. We release our code and data to aid future work on detecting AI-generated content.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.959",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Narrative-of-Thought: Improving Temporal Reasoning of Large Language Models via Recounted Narratives": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-narrative",
        "author": "Zhang, Xinliang Frederick and Beauchamp, Nicholas and Wang, Lu",
        "booktitle": "EMNLP-findings2024",
        "title": "Narrative-of-Thought: Improving Temporal Reasoning of Large Language Models via Recounted Narratives",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reasoning about time and temporal relations is an integral aspect of human cognition, essential for perceiving the world and navigating our experiences. Though large language models (LLMs) have demonstrated impressive performance in many reasoning tasks, temporal reasoning remains challenging due to its intrinsic complexity. In this work, we first study an essential task of temporal reasoning\u2014temporal graph generation, to unveil LLMs\u2019 inherent, global reasoning capabilities. We show that this task presents great challenges even for the most powerful LLMs, such as GPT-3.5/4. We also notice a significant performance gap by small models (\\textless 10B) that lag behind LLMs by 50%. Next, we study how to close this gap with a budget constraint, e.g., not using model finetuning. We propose a new prompting technique tailored for temporal reasoning, Narrative-of-Thought (NoT), that first converts the events set to a Python class, then prompts a small model to generate a temporally grounded narrative, guiding the final generation of a temporal graph. Extensive experiments showcase the efficacy of NoT in improving various metrics. Notably, NoT attains the highest F1 on the Schema-11 evaluation set, while securing an overall F1 on par with GPT-3.5. NoT also achieves the best structural similarity across the board, even compared with GPT-3.5/4.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.963",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "See Detail Say Clear: Towards Brain CT Report Generation via Pathological Clue-driven Representation Learning": {
        "type": "INPROCEEDINGS",
        "key": "zheng-etal-2024-see",
        "author": "Zheng, Chengxin and Ji, Junzhong and Shi, Yanzhao and Zhang, Xiaodan and Qu, Liangqiong",
        "booktitle": "EMNLP-findings2024",
        "title": "See Detail Say Clear: Towards Brain CT Report Generation via Pathological Clue-driven Representation Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Brain CT report generation is significant to aid physicians in diagnosing cranial diseases.Recent studies concentrate on handling the consistency between visual and textual pathological features to improve the coherence of report.However, there exist some challenges: 1) Redundant visual representing: Massive irrelevant areas in 3D scans distract models from representing salient visual contexts.2) Shifted semantic representing: Limited medical corpus causes difficulties for models to transfer the learned textual representations to generative layers. This study introduces a Pathological Clue-driven Representation Learning (PCRL) model to build cross-modal representations based on pathological clues and naturally adapt them for accurate report generation.Specifically, we construct pathological clues from perspectives of segmented regions, pathological entities, and report themes, to fully grasp visual pathological patterns and learn cross-modal feature representations. To adapt the representations for the text generation task, we bridge the gap between representation learning and report generation by using a unified large language model (LLM) with task-tailored instructions. These crafted instructions enable the LLM to be flexibly fine-tuned across tasks and smoothly transfer the semantic representation for report generation.Experiments demonstrate that our method outperforms previous methods and achieves SoTA performance.Our code is available at https://github.com/Chauncey-Jheng/PCRL-MRG.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.965",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Look Who\u2019s Talking Now: Covert Channels From Biased LLMs": {
        "type": "INPROCEEDINGS",
        "key": "silva-etal-2024-look",
        "author": "Silva, Daniel and Sala, Frederic and Gabrys, Ryan",
        "booktitle": "EMNLP-findings2024",
        "title": "Look Who\u2019s Talking Now: Covert Channels From Biased LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language model-based steganography encodes hidden messages into model-generated tokens. The key tradeoff is between how much hidden information can be introduced and how much the model can be perturbed. To address this tradeoff, we show how to adapt strategies previously used for LLM watermarking to encode large amounts of information. We tackle the practical (but difficult) setting where we do not have access to the full model when trying to recover the hidden information. Theoretically, we study the fundamental limits in how much steganographic information can be inserted into LLM-created outputs. We provide practical encoding schemes and present experimental results showing that our proposed strategies are nearly optimal.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.971",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths": {
        "type": "INPROCEEDINGS",
        "key": "chia-etal-2024-reasoning",
        "author": "Chia, Yew Ken and Chen, Guizhen and Xu, Weiwen and Luu, Anh Tuan and Poria, Soujanya and Bing, Lidong",
        "booktitle": "EMNLP-findings2024",
        "title": "Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Advanced models such as OpenAI o1 exhibit impressive problem-solving capabilities through step-by-step reasoning. However, they may still falter on more complex problems, making errors that disrupt their reasoning paths. We attribute this to the expansive solution space, where each step has the risk of diverging into mistakes. To enhance language model reasoning, we introduce a specialized training framework called Reasoning Paths Optimization (RPO), which enables learning to reason and explore from diverse paths. Our approach encourages favorable branches at each reasoning step while penalizing unfavorable ones, enhancing the model\u2019s overall problem-solving performance. Reasoning Paths Optimization does not rely on large-scale human-annotated rationales or outputs from closed-source models, making it scalable and data-efficient. We focus on multi-step reasoning tasks, such as math word problems and science-based exam questions. The experiments demonstrate that our framework significantly enhances the reasoning performance of large language models, with up to 3.1% and 4.3% improvement on GSM8K and MMLU (STEM) respectively. Our data and code can be found at https://reasoning-paths.github.io.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.977",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "BiMediX: Bilingual Medical Mixture of Experts LLM": {
        "type": "INPROCEEDINGS",
        "key": "pieri-etal-2024-bimedix",
        "author": "Pieri, Sara and Mullappilly, Sahal Shaji and Khan, Fahad Shahbaz and Anwer, Rao Muhammad and Khan, Salman and Baldwin, Timothy and Cholakkal, Hisham",
        "booktitle": "EMNLP-findings2024",
        "title": "BiMediX: Bilingual Medical Mixture of Experts LLM",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we introduce BiMediX, the first bilingual medical mixture of experts LLM designed for seamless interaction in both English and Arabic. Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice question answering, and open-ended question answering. We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations. We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs. Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual instruction set that covers 1.3 Million diverse medical interactions, including 200k synthesized multi-turn doctor-patient chats, in a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art Med42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively, computed across multiple medical evaluation benchmarks in English, while operating at 8-times faster inference. Moreover, our BiMediX outperforms the generic Arabic-English bilingual LLM, Jais-30B, by average absolute gains of 10% on our Arabic and 15% on our bilingual evaluations across multiple datasets. Additionally, BiMediX exceeds the accuracy of GPT4 by 4.4% in open-ended question UPHILL evaluation and largely outperforms state-of-the-art open source medical LLMs in human evaluations of multi-turn conversations. Our trained models, instruction set, and source code are available at https://github.com/mbzuai-oryx/BiMediX.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.989",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs": {
        "type": "INPROCEEDINGS",
        "key": "yadav-etal-2024-pythonsaga",
        "author": "Yadav, Ankit and Beniwal, Himanshu and Singh, Mayank",
        "booktitle": "EMNLP-findings2024",
        "title": "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of *HumanEval* and *MBPP*, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks that can inflate model performance estimations. To address these limitations, we propose a novel benchmark, *PythonSaga*, featuring 185 hand-crafted prompts in a balanced representation of 38 programming concepts across diverse difficulty levels. The robustness of our benchmark is demonstrated by the poor performance of existing Code-LLMs. The code and data set are openly available to the NLP community at this [URL](https://github.com/PythonSaga/PythonSaga).",
        "keywords": "",
        "url": "https://aclanthology.org/2024.findings-emnlp.996",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Towards Efficient Fine-Tuning of Language Models With Organizational Data for Automated Software Review": {
        "type": "article",
        "key": "10.1109/TSE.2024.3428324",
        "author": "Nashaat, Mona and Miller, James",
        "title": "Towards Efficient Fine-Tuning of Language Models With Organizational Data for Automated Software Review",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3428324",
        "doi": "10.1109/TSE.2024.3428324",
        "abstract": "Large language models like BERT and GPT possess significant capabilities and potential impacts across various applications. Software engineers often use these models for code-related tasks, including generating, debugging, and summarizing code. Nevertheless, large language models still have several flaws, including model hallucination. (e.g., generating erroneous code and producing outdated and inaccurate programs) and the substantial computational resources and energy required for training and fine-tuning. To tackle these challenges, we propose CodeMentor, a framework for few-shot learning to train large language models with the data available within the organization. We employ the framework to train a language model for code review activities, such as code refinement and review generation. The framework utilizes heuristic rules and weak supervision techniques to leverage available data, such as previous review comments, issue reports, and related code updates. Then, the framework employs the constructed dataset to fine-tune LLMs for code review tasks. Additionally, the framework integrates domain expertise by employing reinforcement learning with human feedback. This allows domain experts to assess the generated code and enhance the model performance. Also, to assess the performance of the proposed model, we evaluate it with four state-of-the-art techniques in various code review tasks. The experimental results attest that CodeMentor enhances the performance in all tasks compared to the state-of-the-art approaches, with an improvement of up to 22.3%, 43.4%, and 24.3% in code quality estimation, review generation, and bug report summarization tasks, respectively.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2240\u20132253",
        "numpages": "14"
    },
    "LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3428972",
        "author": "Fakhoury, Sarah and Naik, Aaditya and Sakkas, Georgios and Chakraborty, Saikat and Lahiri, Shuvendu K.",
        "title": "LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3428972",
        "doi": "10.1109/TSE.2024.3428972",
        "abstract": "Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow &lt;sc&gt;TiCoder&lt;/sc&gt; for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97\\% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2254\u20132268",
        "numpages": "15"
    },
    "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3440503",
        "author": "Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Zhuo, Terry Yue and Chen, Taolue",
        "title": "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3440503",
        "doi": "10.1109/TSE.2024.3440503",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models (&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq1-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq2-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach &lt;monospace&gt;COTTON&lt;/monospace&gt; which can leverage &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq3-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by &lt;monospace&gt;COTTON&lt;/monospace&gt; outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by &lt;monospace&gt;COTTON&lt;/monospace&gt; boost various &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq4-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that &lt;monospace&gt;COTTON&lt;/monospace&gt; not only improves the performance of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq5-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs, but also enhances the performance of LLMs. Our study showcases the potential of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq6-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs in software engineering applications.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2437\u20132457",
        "numpages": "21"
    },
    "Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3397822",
        "author": "Tu, Haoxin and Zhou, Zhide and Jiang, He and Yusuf, Imam Nur Bani and Li, Yuxian and Jiang, Lingxiao",
        "title": "Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3397822",
        "doi": "10.1109/TSE.2024.3397822",
        "abstract": "Compiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task. Existing compiler bug isolation approaches convert the problem into a test program mutation problem, but they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named &lt;sc&gt;LLM4CBI&lt;/sc&gt; to utilize LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation may not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts. To overcome the challenges, three new components are designed in &lt;sc&gt;LLM4CBI&lt;/sc&gt;. First, &lt;sc&gt;LLM4CBI&lt;/sc&gt; utilizes a program complexity-guided prompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in programs for mutation. Second, &lt;sc&gt;LLM4CBI&lt;/sc&gt; employs a memorized prompt selection component, which adopts reinforcement learning to select specialized prompts for mutating test programs continuously. Third, a test program validation component is proposed to select specialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with the state-of-the-art approaches (DiWi and RecBi) over 120 real bugs from the two most popular compilers, namely GCC and LLVM, our evaluation demonstrates the advantages of &lt;sc&gt;LLM4CBI&lt;/sc&gt;: It can isolate 69.70\\%/21.74\\% and 24.44\\%/8.92\\% more bugs than DiWi and RecBi within Top-1/Top-5 ranked results. Additionally, we demonstrate that the LLMs component (i.e., GPT-3.5) used in &lt;sc&gt;LLM4CBI&lt;/sc&gt; can be easily replaced by other LLMs while still achieving reasonable results in comparison to related studies.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1768\u20131788",
        "numpages": "21"
    },
    "LUNA: A Model-Based Universal Analysis Framework for Large Language Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3411928",
        "author": "Song, Da and Xie, Xuan and Song, Jiayang and Zhu, Derui and Huang, Yuheng and Juefei-Xu, Felix and Ma, Lei",
        "title": "LUNA: A Model-Based Universal Analysis Framework for Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3411928",
        "doi": "10.1109/TSE.2024.3411928",
        "abstract": "Over the past decade, Artificial Intelligence (AI) has had great success recently and is being used in a wide range of academic and industrial fields. More recently, Large Language Models (LLMs) have made rapid advancements that have propelled AI to a new level, enabling and empowering even more diverse applications and industrial domains with intelligence, particularly in areas like software engineering and natural language processing. Nevertheless, a number of emerging trustworthiness concerns and issues exhibited in LLMs, e.g., robustness and hallucination, have already recently received much attention, without properly solving which the widespread adoption of LLMs could be greatly hindered in practice. The distinctive characteristics of LLMs, such as the self-attention mechanism, extremely large neural network scale, and autoregressive generation usage contexts, differ from classic AI software based on Convolutional Neural Networks and Recurrent Neural Networks and present new challenges for quality analysis. Up to the present, it still lacks universal and systematic analysis techniques for LLMs despite the urgent industrial demand across diverse domains. Towards bridging such a gap, in this paper, we initiate an early exploratory study and propose a universal analysis framework for LLMs, named &lt;italic&gt;LUNA&lt;/italic&gt;, which is designed to be general and extensible and enables versatile analysis of LLMs from multiple quality perspectives in a human-interpretable manner. In particular, we first leverage the data from desired trustworthiness perspectives to construct an abstract model as an auxiliary analysis asset and proxy, which is empowered by various abstract model construction methods built-in &lt;italic&gt;LUNA&lt;/italic&gt;. To assess the quality of the abstract model, we collect and define a number of evaluation metrics, aiming at both the abstract model level and the semantics level. Then, the semantics, which is the degree of satisfaction of the LLM w.r.t. the trustworthiness perspective, is bound to and enriches the abstract model with semantics, which enables more detailed analysis applications for diverse purposes, e.g., abnormal behavior detection. To better understand the potential usefulness of our analysis framework &lt;italic&gt;LUNA&lt;/italic&gt;, we conduct a large-scale evaluation, the results of which demonstrate that 1) the abstract model has the potential to distinguish normal and abnormal behavior in LLM, 2) &lt;italic&gt;LUNA&lt;/italic&gt; is effective for the real-world analysis of LLMs in practice, and the hyperparameter settings influence the performance, 3) different evaluation metrics are in different correlations with the analysis performance. In order to encourage further studies in the quality assurance of LLMs, we made all of the code and more detailed experimental results data available on the supplementary website of this paper &lt;uri&gt;https://sites.google.com/view/llm-luna&lt;/uri&gt;.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jun",
        "pages": "1921\u20131948",
        "numpages": "28"
    },
    "ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3382365",
        "author": "Tang, Yutian and Liu, Zhijie and Zhou, Zhichao and Luo, Xiapu",
        "title": "ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3382365",
        "doi": "10.1109/TSE.2024.3382365",
        "abstract": "Recent advancements in large language models (LLMs) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, LLMs have shown potential in various software engineering applications. In this study, we present a systematic comparison of test suites generated by the ChatGPT LLM and the state-of-the-art SBST tool EvoSuite. Our comparison is based on several critical factors, including correctness, readability, code coverage, and bug detection capability. By highlighting the strengths and weaknesses of LLMs (specifically ChatGPT) in generating unit test cases compared to EvoSuite, this work provides valuable insights into the performance of LLMs in solving software engineering problems. Overall, our findings underscore the potential of LLMs in software engineering and pave the way for further research in this area.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1340\u20131359",
        "numpages": "20"
    },
    "No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT": {
        "type": "article",
        "key": "10.1109/TSE.2024.3392499",
        "author": "Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng",
        "title": "No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3392499",
        "doi": "10.1109/TSE.2024.3392499",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks, such as machine translation, question answering, summarization, and so on. Additionally, LLMs are also highly valuable in supporting software engineering tasks, particularly in the field of code generation. Automatic code generation is a process of automatically generating source code or executable code based on given specifications or requirements, improving developer productivity. In this study, we perform a systematic empirical assessment to the quality of code generation using &lt;italic&gt;ChatGPT&lt;/italic&gt;, a recent state-of-the-art product LLM. We leverage 728 algorithm problems in five languages (i.e., C, C++, Java, Python, and JavaScript) and 18 CWEs with 54 code scenarios for the code generation task. Our evaluation encompasses a comprehensive analysis of code snippets generated by &lt;italic&gt;ChatGPT&lt;/italic&gt;, focusing on three critical aspects: correctness, complexity, and security. We also specifically investigate &lt;italic&gt;ChatGPT&lt;/italic&gt;'s ability to engage in multi-round fixing process (i.e., &lt;italic&gt;ChatGPT&lt;/italic&gt;'s dialog ability, chatting between users and &lt;italic&gt;ChatGPT&lt;/italic&gt; for fixing generated buggy code) of facilitating code generation. By delving into the generated code and examining the experimental results, this work provides valuable insights into the performance of &lt;italic&gt;ChatGPT&lt;/italic&gt; in tackling code generation tasks over the three critical aspects. The experimental results demonstrate that (1) &lt;italic&gt;ChatGPT&lt;/italic&gt; is better at generating functionally correct code for problems before 2021 in different languages than problems after 2021 with &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$48.14\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;48.14&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"tang-ieq1-3392499.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; advantage in &lt;italic&gt;Accepted&lt;/italic&gt; rate on judgment platform, but &lt;italic&gt;ChatGPT&lt;/italic&gt;'s ability to directly fix erroneous code with multi-round fixing process to achieve correct functionality is relatively weak; (2) the distribution of cyclomatic and cognitive complexity levels for code snippets in different languages varies. Furthermore, the multi-round fixing process with &lt;italic&gt;ChatGPT &lt;/italic&gt; generally preserves or increases the complexity levels of code snippets; (3) in algorithm scenarios with languages of C, C++, and Java, and CWE scenarios with languages of C and Python3, the code generated by &lt;italic&gt;ChatGPT &lt;/italic&gt; has relevant vulnerabilities. However, the multi-round fixing process for vulnerable code snippets demonstrates promising results, with more than &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$89\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;89&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"tang-ieq2-3392499.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; of vulnerabilities successfully addressed; and (4) code generation may be affected by &lt;italic&gt;ChatGPT&lt;/italic&gt;'s non-determinism factor, resulting in variations of code snippets in functional correctness, complexity, and security. Overall, our findings uncover potential issues and limitations that arise in the &lt;italic&gt;ChatGPT&lt;/italic&gt;-based code generation and lay the groundwork for improving AI and LLM-based code generation techniques.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1548\u20131584",
        "numpages": "37"
    },
    "Automatic Commit Message Generation: A Critical Review and Directions for Future Work": {
        "type": "article",
        "key": "10.1109/TSE.2024.3364675",
        "author": "Zhang, Yuxia and Qiu, Zhiqing and Stol, Klaas-Jan and Zhu, Wenhui and Zhu, Jiaxin and Tian, Yingchen and Liu, Hui",
        "title": "Automatic Commit Message Generation: A Critical Review and Directions for Future Work",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3364675",
        "doi": "10.1109/TSE.2024.3364675",
        "abstract": "Commit messages are critical for code comprehension and software maintenance. Writing a high-quality message requires skill and effort. To support developers and reduce their effort on this task, several approaches have been proposed to automatically generate commit messages. Despite the promising performance reported, we have identified three significant and prevalent threats in these automated approaches: 1) the datasets used to train and evaluate these approaches contain a considerable amount of \u2018noise\u2019; 2) current approaches only consider commits of a limited diff size; and 3) current approaches can only generate the subject of a commit message, not the message body. The first limitation may let the models \u2018learn\u2019 inappropriate messages in the training stage, and also lead to inflated performance results in their evaluation. The other two threats can considerably weaken the practical usability of these approaches. Further, with the rapid emergence of large language models (LLMs) that show superior performance in many software engineering tasks, it is worth asking: can LLMs address the challenge of long diffs and whole message generation? This article first reports the results of an empirical study to assess the impact of these three threats on the performance of the state-of-the-art auto generators of commit messages. We collected commit data of the Top 1,000 most-starred Java projects in GitHub and systematically removed noisy commits with bot-submitted and meaningless messages. We then compared the performance of four approaches representative of the state-of-the-art before and after the removal of noisy messages, or with different lengths of commit diffs. We also conducted a qualitative survey with developers to investigate their perspectives on simply generating message subjects. Finally, we evaluate the performance of two representative LLMs, namely UniXcoder and ChatGPT, in generating more practical commit messages. The results demonstrate that generating commit messages is of great practical value, considerable work is needed to mature the current state-of-the-art, and LLMs can be an avenue worth trying to address the current limitations. Our analyses provide insights for future work to achieve better performance in practice.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "816\u2013835",
        "numpages": "20"
    },
    "Software Testing With Large Language Models: Survey, Landscape, and Vision": {
        "type": "article",
        "key": "10.1109/TSE.2024.3368208",
        "author": "Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing",
        "title": "Software Testing With Large Language Models: Survey, Landscape, and Vision",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3368208",
        "doi": "10.1109/TSE.2024.3368208",
        "abstract": "Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "911\u2013936",
        "numpages": "26"
    },
    "Code Review Automation: Strengths and Weaknesses of the State of the Art": {
        "type": "article",
        "key": "10.1109/TSE.2023.3348172",
        "author": "Tufano, Rosalia and Dabi\\'{c}, Ozren and Mastropaolo, Antonio and Ciniselli, Matteo and Bavota, Gabriele",
        "title": "Code Review Automation: Strengths and Weaknesses of the State of the Art",
        "year": "2024",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3348172",
        "doi": "10.1109/TSE.2023.3348172",
        "abstract": "The automation of code review has been tackled by several researchers with the goal of reducing its cost. The adoption of deep learning in software engineering pushed the automation to new boundaries, with techniques &lt;italic&gt;imitating&lt;/italic&gt; developers in generative tasks, such as commenting on a code change as a reviewer would do or addressing a reviewer's comment by modifying code. The performance of these techniques is usually assessed through quantitative metrics, &lt;italic&gt;e.g.,&lt;/italic&gt; the percentage of instances in the test set for which correct predictions are generated, leaving many open questions on the techniques\u2019 capabilities. For example, knowing that an approach is able to correctly address a reviewer's comment in 10\\% of cases is of little value without knowing what was asked by the reviewer: What if in all successful cases the code change required to address the comment was just the removal of an empty line? In this paper we aim at characterizing the cases in which three code review automation techniques tend to succeed or fail in the two above-described tasks. The study has a strong qualitative focus, with &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$sim$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u223c&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"bavota-ieq1-3348172.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;105 man-hours of manual inspection invested in manually analyzing correct and wrong predictions generated by the three techniques, for a total of 2,291 inspected predictions. The output of this analysis are two taxonomies reporting, for each of the two tasks, the types of code changes on which the experimented techniques tend to succeed or to fail, pointing to areas for future work. A result of our manual analysis was also the identification of several issues in the datasets used to train and test the experimented techniques. Finally, we assess the importance of researching in techniques specialized for code review automation by comparing their performance with ChatGPT, a general purpose large language model, finding that ChatGPT struggles in commenting code as a human reviewer would do.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "338\u2013353",
        "numpages": "16"
    },
    "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3334955",
        "author": "Sch\\\"{a}fer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank",
        "title": "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation",
        "year": "2023",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3334955",
        "doi": "10.1109/TSE.2023.3334955",
        "abstract": "Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in &lt;sc&gt;TestPilot&lt;/sc&gt;, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate &lt;sc&gt;TestPilot&lt;/sc&gt; using OpenAI's &lt;italic&gt;gpt3.5-turbo&lt;/italic&gt; LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2\\% and branch coverage of 52.8\\%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3\\% statement coverage and 25.6\\% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8\\% of &lt;sc&gt;TestPilot&lt;/sc&gt;'s generated tests have &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$leq$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u2264&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"schaefer-ieq1-3334955.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; 50\\% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run &lt;sc&gt;TestPilot&lt;/sc&gt; with two additional LLMs, OpenAI's older &lt;italic&gt;code-cushman-002&lt;/italic&gt; LLM and &lt;italic&gt;StarCoder&lt;/italic&gt;, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2\\% median statement coverage), and somewhat worse results with the latter (54.0\\% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "85\u2013105",
        "numpages": "21"
    },
    "Learning to Generate Structured Code Summaries From Hybrid Code Context": {
        "type": "article",
        "key": "10.1109/TSE.2024.3439562",
        "author": "Zhou, Ziyi and Li, Mingchen and Yu, Huiqun and Fan, Guisheng and Yang, Penghui and Huang, Zijie",
        "title": "Learning to Generate Structured Code Summaries From Hybrid Code Context",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3439562",
        "doi": "10.1109/TSE.2024.3439562",
        "abstract": "Code summarization aims to automatically generate natural language descriptions for code, and has become a rapidly expanding research area in the past decades. Unfortunately, existing approaches mainly focus on the \u201cone-to-one\u201d mapping from methods to short descriptions, which hinders them from becoming practical tools: 1) The program context is ignored, so they have difficulty in predicting keywords outside the target method; 2) They are typically trained to generate brief function descriptions with only one sentence in length, and therefore have difficulty in providing specific information. These drawbacks are partially due to the limitations of public code summarization datasets. In this paper, we first build a large code summarization dataset including different code contexts and summary content annotations, and then propose a deep learning framework that learns to generate structured code summaries from hybrid program context, named StructCodeSum. It provides both an LLM-based approach and a lightweight approach which are suitable for different scenarios. Given a target method, StructCodeSum predicts its function description, return description, parameter description, and usage description through hybrid code context, and ultimately builds a Javadoc-style code summary. The hybrid code context consists of path context, class context, documentation context and call context of the target method. Extensive experimental results demonstrate: 1) The hybrid context covers more than 70\\% of the summary tokens in average and significantly boosts the model performance; 2) When generating function descriptions, StructCodeSum outperforms the state-of-the-art approaches by a large margin; 3) According to human evaluation, the quality of the structured summaries generated by our approach is better than the documentation generated by Code Llama.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2512\u20132528",
        "numpages": "17"
    },
    "Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration": {
        "type": "article",
        "key": "10.1109/TSE.2024.3445338",
        "author": "Paltenghi, Matteo and Pandita, Rahul and Henley, Austin Z. and Ziegler, Albert",
        "title": "Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3445338",
        "doi": "10.1109/TSE.2024.3445338",
        "abstract": "Recent neural models of code, such as OpenAI Codex and AlphaCode, have demonstrated remarkable proficiency at code generation due to the underlying attention mechanism. However, it often remains unclear how the models actually process code, and to what extent their reasoning and the way their attention mechanism scans the code matches the patterns of developers. A poor understanding of the model reasoning process limits the way in which current neural models are leveraged today, so far mostly for their raw prediction. To fill this gap, this work studies how the processed attention signal of three open large language models - CodeGen, InCoder and GPT-J - agrees with how developers look at and explore code when each answers the same sensemaking questions about code. Furthermore, we contribute an open-source eye-tracking dataset comprising 92 manually-labeled sessions from 25 developers engaged in sensemaking tasks. We empirically evaluate five heuristics that do not use the attention and ten attention-based post-processing approaches of the attention signal of CodeGen against our ground truth of developers exploring code, including the novel concept of &lt;italic&gt;follow-up attention&lt;/italic&gt; which exhibits the highest agreement between model and human attention. Our follow-up attention method can predict the next line a developer will look at with 47\\% accuracy. This outperforms the baseline prediction accuracy of 42.3\\%, which uses the session history of other developers to recommend the next line. These results demonstrate the potential of leveraging the attention signal of pre-trained models for effective code exploration.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2568\u20132582",
        "numpages": "15"
    },
    "Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction": {
        "type": "article",
        "key": "10.1109/TSE.2024.3450837",
        "author": "Kang, Sungmin and Yoon, Juyeon and Askarbekkyzy, Nargiz and Yoo, Shin",
        "title": "Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3450837",
        "doi": "10.1109/TSE.2024.3450837",
        "abstract": "Bug reproduction is a critical developer activity that is also challenging to automate, as bug reports are often in natural language and thus can be difficult to transform to test cases consistently. As a result, existing techniques mostly focused on crash bugs, which are easier to automatically detect and verify. In this work, we overcome this limitation by using large language models (LLMs), which have been demonstrated to be adept at natural language processing and code generation. By prompting LLMs to generate bug-reproducing tests, and via a post-processing pipeline to automatically identify promising generated tests, our proposed technique &lt;sc&gt;Libro&lt;/sc&gt; could successfully reproduce about one-third of all bugs in the widely used Defects4J benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11 open-source LLMs, suggests that open-source LLMs also demonstrate substantial potential, with the StarCoder LLM achieving 70\\% of the reproduction performance of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J benchmark, and 90\\% of performance on a held-out bug dataset likely not part of any LLM's training data. In addition, our experiments on LLMs of different sizes show that bug reproduction using &lt;sc&gt;Libro&lt;/sc&gt; improves as LLM size increases, providing information as to which LLMs can be used with the &lt;sc&gt;Libro&lt;/sc&gt; pipeline.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "2677\u20132694",
        "numpages": "18"
    },
    "DeGPT: Optimizing Decompiler Output with LLM": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Peiwei Hu and Chinese Academy of Sciences and Beijing and China) and Ruigang Liang and Chinese Academy of Sciences and Beijing and China) and Kai Chen and Chinese Academy of Sciences and China)",
        "booktitle": "NDSS2024",
        "title": "DeGPT: Optimizing Decompiler Output with LLM",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reverse engineering is essential in malware analysis, vulnerability discovery, etc. Decompilers assist the reverse engineers by lifting the assembly to the high-level programming language, which highly boosts binary comprehension. However, decompilers suffer from problems such as meaningless variable names, redundant variables, and lacking comments describing the purpose of the code. Previous studies have shown promising performance in refining the decompiler output by training the models with huge datasets containing various decompiler outputs. However, even datasets that take much time to construct cover limited binaries in the real world. The performance degrades severely facing the binary migration.In this paper, we present DeGPT, an end-to-end framework aiming to optimize the decompiler output to improve its readability and simplicity and further assist the reverse engineers in understanding the binaries better. The Large Language Model (LLM) can mitigate performance degradation with its extraordinary ability endowed by large model size and training set containing rich multi-modal data. However, its potential is difficult to unlock through one-shot use. Thus, we propose the three-role mechanism, which includes referee (R_ref), advisor (R_adv), and operator (R_ope), to adapt the LLM to our optimization tasks. Specifically, R_ref provides the optimization scheme for the target decompiler output, while R_adv gives the rectification measures based on the scheme, and R_ope inspects whether the optimization changes the original function semantics and concludes the final verdict about whether to accept the optimizations. We evaluate DeGPT on the datasets containing decompiler outputs of various software, such as the practical command line tools, malware, a library for audio processing, and implementations of algorithms. The experimental results show that even on the output of the current top-level decompiler (Ghidra), DeGPT can achieve 24.4% reduction in the cognitive burden of understanding the decompiler outputs and provide comments of which 62.9% can provide practical semantics for the reverse engineers to help the understanding of binaries. Our user surveys also show that the optimizations can significantly simplify the code and add helpful semantic information (variable names and comments), facilitating a quick and accurate understanding of the binary.",
        "keywords": "",
        "url": "https://www.ndss-symposium.org/ndss-paper/degpt-optimizing-decompiler-output-with-llm",
        "doi": "",
        "ISSN": "",
        "month": ""
    },
    "Large Language Model guided Protocol Fuzzing": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Ruijie Meng and Singapore) and Martin Mirchev and Marcel B\u00f6hme and Germany and Monash University and Australia) and Abhik Roychoudhury",
        "booktitle": "NDSS2024",
        "title": "Large Language Model guided Protocol Fuzzing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "How to find security flaws in a protocol implementation without a machine-readable specification of the protocol? Facing the internet, protocol implementations are particularly security-critical software systems where inputs must adhere to a specific structure and order that is often informally specified in hundreds of pages in natural language (RFC). Without some machine-readable version of that protocol, it is difficult to automatically generate valid test inputs for its implementation that follow the required structure and order. It is possible to partially alleviate this challenge using mutational fuzzing on a set of recorded message sequences as seed inputs. However, the set of available seeds is often quite limited and will hardly cover the great diversity of protocol states and input structures.In this paper, we explore the opportunities of systematic interaction with a pre-trained large language models (LLM) which has ingested millions of pages of human-readable protocol specifications, to draw out machine-readable information about the protocol that can be used during protocol fuzzing.  We use the knowledge of the LLMs about protocol message types for well-known protocols. We also checked the LLM's capability in detecting ``states\" for stateful protocol implementations by generating sequences of messages and predicting response codes. Based on these observations, we have developed an LLM-guided protocol implementation fuzzing engine. Our protocol fuzzer ChatAFL constructs grammars for each message type in a protocol, and then mutates messages or predicts the next messages in a message sequence via interactions with LLMs. Experiments on a wide range of real-world protocols from ProFuzzbench show significant efficacy in state and code coverage. Our LLM-guided stateful fuzzer was compared with state-of-the-art fuzzers AFLNet and NSFuzz. ChatAFL covers 47.6% and 42.7% more state transitions, 29.6% and 25.8% more states, and 5.8% and 6.7% more code, respectively. Apart from enhanced coverage, ChatAFL discovered nine distinct and previously unknown vulnerabilities in widely-used and extensively-tested protocol implementations while AFLNet and NSFuzz only discover three and four of them, respectively.",
        "keywords": "",
        "url": "https://www.ndss-symposium.org/ndss-paper/large-language-model-guided-protocol-fuzzing",
        "doi": "",
        "ISSN": "",
        "month": ""
    },
    "Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652106",
        "author": "Shan, Shiwen and Huo, Yintong and Su, Yuxin and Li, Yichen and Li, Dan and Zheng, Zibin",
        "title": "Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652106",
        "doi": "10.1145/3650212.3652106",
        "abstract": "Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis. To the best of our knowledge, this is the first work to localize the root-cause configuration properties for end-users based on Large Language Models (LLMs) and logs. We evaluate the proposed strategy on Hadoop by LogConfigLocalizer and prove its efficiency with an average accuracy as high as 99.91\\%. Additionally, we also demonstrate the effectiveness and necessity of different phases of the methodology by comparing it with two other variants and a baseline tool. Moreover, we validate the proposed methodology through a practical case study to demonstrate its effectiveness and feasibility.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "13\u201325",
        "numpages": "13",
        "keywords": "Configuration Errors, Large Language Model, Log Analysis",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652115",
        "author": "Zeng, Zhengran and Wang, Yidong and Xie, Rui and Ye, Wei and Zhang, Shikun",
        "title": "CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652115",
        "doi": "10.1145/3650212.3652115",
        "abstract": "In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "124\u2013136",
        "numpages": "13",
        "keywords": "Benchmark, Code Generation, Large Language Models",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652124",
        "author": "Wen, Xin-Cheng and Gao, Cuiyun and Gao, Shuzheng and Xiao, Yang and Lyu, Michael R.",
        "title": "SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652124",
        "doi": "10.1145/3650212.3652124",
        "abstract": "Recently, there has been a growing interest in automatic software vulnerability detection.     Pre-trained model-based approaches have demonstrated superior performance than other Deep Learning (DL)-based approaches in detecting vulnerabilities.     However, the existing pre-trained model-based approaches generally employ code sequences as input during prediction, and may ignore vulnerability-related structural information, as reflected in the following two aspects.     First, they tend to fail to infer the semantics of the code statements with complex logic such as those containing multiple operators and pointers.     Second, they are hard to comprehend various code execution sequences, which is essential for precise vulnerability detection.         To mitigate the challenges, we propose a Structured Natural Language Comment tree-based vulnerAbiLity dEtection framework based on the pre-trained models, named . The proposed Structured Natural Language Comment Tree (SCT) integrates the semantics of code statements with code execution sequences based on the Abstract Syntax Trees (ASTs).Specifically, comprises three main modules:     (1) Comment Tree Construction, which aims at enhancing the model\u2019s ability to infer the semantics of code statements by first incorporating Large Language Models (LLMs) for comment generation and then adding the comment node to ASTs.     (2) Structured Natural Language Comment Tree Construction, which aims at explicitly involving code execution sequence by combining the code syntax templates with the comment tree.     (3) SCT-Enhanced Representation, which finally incorporates the constructed SCTs for well capturing vulnerability patterns.     Experimental results demonstrate that outperforms the best-performing baseline, including the pre-trained model and LLMs, with improvements of 2.96\\%, 13.47\\%, and 3.75\\% in terms of F1 score on the FFMPeg+Qemu, Reveal, and SVulD datasets, respectively. Furthermore, can be applied to different pre-trained models, such as CodeBERT and UniXcoder, yielding the F1 score performance enhancements ranging from 1.37\\% to 10.87\\%.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "235\u2013247",
        "numpages": "13",
        "keywords": "Deep Learning, Large Language Model, Vulnerability Detection",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "LPR: Large Language Models-Aided Program Reduction": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652126",
        "author": "Zhang, Mengxiao and Tian, Yongqiang and Xu, Zhenyang and Dong, Yiwen and Tan, Shin Hwei and Sun, Chengnian",
        "title": "LPR: Large Language Models-Aided Program Reduction",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652126",
        "doi": "10.1145/3650212.3652126",
        "abstract": "Program reduction is a widely used technique to facilitate debugging                compilers by automatically minimizing programs that trigger                compiler bugs. Existing program reduction techniques are either                generic to a wide range of languages (such as Perses and Vulcan)                or specifically optimized for one certain language by exploiting                language-specific knowledge (e.g., C-Reduce). However, synergistically                combining both generality across languages and optimality                to a specific language in program reduction is yet to be explored.                This paper proposes LPR, the first LLMs-aided technique leveraging                LLMs to perform language-specific program reduction for                multiple languages. The key insight is to utilize both the language                generality of program reducers such as Perses and the languagespecific                semantics learned by LLMs. Concretely, language-generic                program reducers can efficiently reduce programs into a small size                that is suitable for LLMs to process; LLMs can effectively transform                programs via the learned semantics to create new reduction opportunities                for the language-generic program reducers to further                reduce the programs.                Our thorough evaluation on 50 benchmarks across three programming                languages (i.e., C, Rust and JavaScript) has demonstrated                LPR\u2019s practicality and superiority over Vulcan, the state-of-the-art                language-generic program reducer. For effectiveness, LPR surpasses                Vulcan by producing 24.93\\%, 4.47\\%, and 11.71\\% smaller programs                on benchmarks in C, Rust and JavaScript, separately. Moreover, LPR                and Vulcan have the potential to complement each other. For the C                language for which C-Reduce is optimized, by applying Vulcan to                the output produced by LPR, we can attain program sizes that are                on par with those achieved by C-Reduce. For efficiency perceived                by users, LPR is more efficient when reducing large and complex                programs, taking 10.77\\%, 34.88\\%, 36.96\\% less time than Vulcan to                finish all the benchmarks in C, Rust and JavaScript, separately.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "261\u2013273",
        "numpages": "13",
        "keywords": "Large Language Models, Program Reduction, Program Semantics",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Artifact for \"LPR: Large Language Models-Aided Program Reduction": {
        "type": "software",
        "key": "10.5281/zenodo.12669964",
        "author": "Zhang, Mengxiao and Tian, Yongqiang and Xu, Zhenyang and Dong, Yiwen and Tan, Shin Hwei and Sun, Chengnian",
        "title": "Artifact for \"LPR: Large Language Models-Aided Program Reduction",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.12669964",
        "abstract": "    <p>This is the artifact for the paper \u201cLPR: Large Language Models-AidedProgram Reduction\u201d</p>",
        "keywords": "compiler testing, debugging, LLM, program reduction"
    },
    "Automating Zero-Shot Patch Porting for Hard Forks": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652134",
        "author": "Pan, Shengyi and Wang, You and Liu, Zhongxin and Hu, Xing and Xia, Xin and Li, Shanping",
        "title": "Automating Zero-Shot Patch Porting for Hard Forks",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652134",
        "doi": "10.1145/3650212.3652134",
        "abstract": "Forking is a typical way of code reuse, which provides a simple way for developers to create a variant software (denoted as hard fork) by copying and modifying an existing codebase. Despite of the benefits, forking also leads to duplicate efforts in software maintenance. Developers need to port patches across the hard forks to address similar bugs or implement similar features. Due to the divergence between the source project and the hard fork, patch porting is complicated, which requires an adaption regarding different implementations of the same functionality. In this work, we take the first step to automate patch porting for hard forks under a zero-shot setting. We first conduct an empirical study of the patches ported from Vim to Neovim over the last ten years to investigate the necessities of patch porting and the potential flaws in the current practice. We then propose a large language model (LLM) based approach (namely PPatHF) to automatically port patches for hard forks on a function-wise basis. Specifically, PPatHF is composed of a reduction module and a porting module. Given the pre- and post-patch versions of a function from the reference project and the corresponding function from the target project, the reduction module first slims the input functions by removing code snippets less relevant to the patch. Then, the porting module leverages a LLM to apply the patch to the function from the target project. To better elicit the power of the LLM on patch porting, we design a prompt template to enable efficient in-context learning. We further propose an instruction-tuning based training task to better guide the LLM to port the patch and inject task-specific knowledge. We evaluate PPatHF on 310 Neovim patches ported from Vim. The experimental results show that PPatHF outperforms the baselines significantly. Specifically, PPatHF can correctly port 131 (42.3\\%) patches and automate 57\\% of the manual edits required for the developer to port the patch.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "363\u2013375",
        "numpages": "13",
        "keywords": "Hard Fork, Large Language Model, Patch Porting",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Benchmarking Automated Program Repair: An Extensive Study on Both Real-World and Artificial Bugs": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652140",
        "author": "Ouyang, Yicheng and Yang, Jun and Zhang, Lingming",
        "title": "Benchmarking Automated Program Repair: An Extensive Study on Both Real-World and Artificial Bugs",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652140",
        "doi": "10.1145/3650212.3652140",
        "abstract": "As bugs are inevitable and prevalent in real-world programs, many Automated Program Repair (APR) techniques have been proposed to generate patches for them. However, due to the lack of a standard for evaluating APR techniques, prior works tend to use different settings and benchmarks in evaluation, threatening the trustworthiness of the evaluation results. Additionally, they typically only adopt plausibility and genuineness as evaluation metrics, which may potentially mask some underlying issues in APR techniques. To overcome these issues, in this paper, we conduct an extensive and multi-dimensional evaluation of nine learning-based and three traditional state-of-the-art APR techniques under the same environment and settings. We employ the widely studied Defects4J V2.0.0 benchmark and a newly constructed large-scale mutation-based benchmark named MuBench, derived from Defects4J and including 1,700 artificial bugs generated by various mutators, to uncover potential limitations in these APR techniques. We also apply multi-dimensional metrics, including compilability/plausibility/genuineness metrics, as well as SYE (SYntactic Equivalence) and TCE (Trivial Compiler Equivalence) metrics, to thoroughly analyze the 1,814,652 generated patches. This paper presents noteworthy findings from the extensive evaluation: Firstly, Large Language Model (LLM) based APR demonstrates less susceptibility to overfitting on the Defects4J V1.2.0 dataset and fixes the most number of bugs. Secondly, the study suggests a promising future for combining traditional and learning-based APR techniques, as they exhibit complementary advantages in fixing different types of bugs. Additionally, this work highlights the necessity for further enhancing patch compilability of learning-based APR techniques, despite the presence of various existing strategies attempting to improve it. The study also reveals other guidelines for enhancing APR techniques, including the need for handling unresolvable symbol compilability issues and reducing duplicate/no-op patch generation. Finally, our study uncovers seven implementation issues in the studied techniques, with five of them confirmed and fixed by the corresponding authors.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "440\u2013452",
        "numpages": "13",
        "keywords": "Empirical assessment, Mutation testing, Program repair",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652142",
        "author": "Liu, Chenyan and Cai, Yufan and Lin, Yun and Huang, Yuhuan and Pei, Yunrui and Jiang, Bo and Yang, Ping and Dong, Jin Song and Mei, Hong",
        "title": "CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652142",
        "doi": "10.1145/3650212.3652142",
        "abstract": "Recent years have seen the development of LLM-based code generation. Compared to generating code in a software project, incremental code edits are empirically observed to be more frequent. The emerging code editing approaches usually formulate the problem as generating an edit based on known relevant prior edits and context. However, practical code edits can be more complicated. First, an editing session can include multiple (ir)relevant edits to the code under edit. Second, the inference of the subsequent edits is non-trivial as the scope of its ripple effect can be the whole project.        In this work, we propose CoEdPilot, an LLM-driven solution to recommend code edits by discriminating the relevant edits, exploring their interactive natures, and estimating its ripple effect in the project. Specifically, CoEdPilot orchestrates multiple neural transformers to identify what and how to edit in the project regarding both edit location and edit content. When a user accomplishes an edit with an optional editing description, an Subsequent Edit Analysis first reports the most relevant files in the project with what types of edits (e.g., keep, insert, and replace) can happen for each line of their code. Next, an Edit-content Generator generates concrete edit options for the lines of code, regarding its relevant prior changes reported by an Edit-dependency Analyzer. Last, both the Subsequent Edit Analysis and the Edit-content Generator capture relevant prior edits as feedback to readjust their recommendations. We train our models by collecting over 180K commits from 471 open-source projects in 5 programming languages. Our extensive experiments show that (1) CoEdPilot can well predict the edits (i.e., predicting edit location with accuracy of 70.8\\%-85.3\\%, and the edit content with exact match rate of 41.8\\% and BLEU4 score of 60.7); (2) CoEdPilot can well boost existing edit generators such as GRACE and CCT5 on exact match rate by 8.57\\% points and BLEU4 score by 18.08. Last, our user study on 18 participants with 3 editing tasks (1) shows that CoEdPilot can be effective in assisting users to edit code in comparison with Copilot, and (2) sheds light on the future improvement of the tool design. The video demonstration of our tool is available at https://sites.google.com/view/coedpilot/home.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "466\u2013478",
        "numpages": "13",
        "keywords": "code edit generation, edit location, interaction, language model",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "DistillSeq: A Framework for Safety Alignment Testing in Large Language Models using Knowledge Distillation": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680304",
        "author": "Yang, Mingke and Chen, Yuqi and Liu, Yi and Shi, Ling",
        "title": "DistillSeq: A Framework for Safety Alignment Testing in Large Language Models using Knowledge Distillation",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680304",
        "doi": "10.1145/3650212.3680304",
        "abstract": "Large Language Models (LLMs) have showcased their remarkable capabilities in diverse domains, encompassing natural language understanding, translation, and even code generation. The potential for LLMs to generate harmful content is a significant concern. This risk necessitates rigorous testing and comprehensive evaluation of LLMs to ensure safe and responsible use. However, extensive testing of LLMs requires substantial computational resources, making it an expensive endeavor. Therefore, exploring cost-saving strategies during the testing phase is crucial to balance the need for thorough evaluation with the constraints of resource availability. To address this, our approach begins by transferring the moderation knowledge from an LLM to a small model. Subsequently, we deploy two distinct strategies for generating malicious queries: one based on a syntax tree approach, and the other leveraging an LLM-based method. Finally, our approach incorporates a sequential filter-test process designed to identify test cases that are prone to eliciting toxic responses. By doing so, we significantly curtail unnecessary or unproductive interactions with LLMs, thereby streamlining the testing process. Our research evaluated the efficacy of DistillSeq across four LLMs: GPT-3.5, GPT-4.0, Vicuna-13B, and Llama-13B. In the absence of DistillSeq, the observed attack success rates on these LLMs stood at 31.5\\% for GPT-3.5, 21.4\\% for GPT-4.0, 28.3\\% for Vicuna-13B, and 30.9\\% for Llama-13B. However, upon the application of DistillSeq, these success rates notably increased to 58.5\\%, 50.7\\%, 52.5\\%, and 54.4\\%, respectively. This translated to an average escalation in attack success rate by a factor of 93.0\\% when compared to scenarios without the use of DistillSeq. Such findings highlight the significant enhancement DistillSeq offers in terms of reducing the time and resource investment required for effectively testing LLMs.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "578\u2013589",
        "numpages": "12",
        "keywords": "Automated Testing, Knowledge Distillation, Large Language Models",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Oracle-Guided Program Selection from Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680308",
        "author": "Fan, Zhiyu and Ruan, Haifeng and Mechtaev, Sergey and Roychoudhury, Abhik",
        "title": "Oracle-Guided Program Selection from Large Language Models",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680308",
        "doi": "10.1145/3650212.3680308",
        "abstract": "While large language models (LLMs) have shown significant advancements in code generation, their susceptibility to producing incorrect code poses a significant challenge to the adoption of LLM-generated programs. This issue largely stems from the reliance on natural language descriptions as informal oracles in code generation. Current strategies to mitigate this involve selecting the best program from multiple LLM-generated alternatives, judged by criteria like the consistency of their execution results on an LLM-generated test suite. However, this approach has crucial limitations: (1) LLMs often generate redundant tests or tests that cannot distinguish between correct and incorrect solutions, (2) the used consistency criteria, such as the majority vote, fail to foster developer trust due to the absence of transparent rationale behind the made choices. In this work, we propose a new perspective on increasing the quality of LLM-generated code via program selection using the LLM as a test oracle. Our method is based on our experimentally confirmed observation that LLMs serve more effectively as oracles when tasked with selecting the correct output from multiple choices. Leveraging this insight, we first generate distinguishing inputs that capture semantic discrepancies of programs sampled from an LLM, and record outputs produced by the programs on these inputs. An LLM then selects the most likely to be correct output from these, guided by the natural language problem description. We implemented this idea in a tool LLMCodeChoice and evaluated its accuracy in generating and selecting standalone programs. Our experiments demonstrated its effectiveness in improving pass@1 by 3.6-7\\% on HumanEval and MBPP benchmarks compared to the state-of-art CodeT. Most interestingly, the selected input-output specifications helped us to uncover incompleteness and ambiguities in task descriptions and also identify incorrect ground-truth implementations in the benchmarks.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "628\u2013640",
        "numpages": "13",
        "keywords": "code generation, differential testing, large language model, oracle inference",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680323",
        "author": "Xia, Chunqiu Steven and Zhang, Lingming",
        "title": "Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680323",
        "doi": "10.1145/3650212.3680323",
        "abstract": "Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Traditional APR techniques suffer from a lack of patch variety as they rely heavily on handcrafted or mined bug fixing patterns and cannot easily generalize to other bug/fix types. To address this limitation, recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then querying the LLM to either fill-in (cloze-style APR) the correct code at the bug location or to produce a completely new code snippet as the patch. While the LLM-based APR tools are able to achieve state-of-the-art results, they still follow the classic Generate and Validate (GV) repair paradigm of first generating lots of patches by sampling from the same initial prompt and then validating each one afterwards. This not only leads to many repeated patches that are incorrect, but also misses the crucial and yet previously ignored information in test failures as well as in plausible patches.        To address these aforementioned limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same    mistakes. For earlier patches that passed all the tests (i.e., plausible patches), we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM \u2013 ChatGPT. Our evaluation on the widely studied Defects4j dataset shows that ChatRepair is able to achieve the new state-of-the-art in repair performance, achieving 114 and 48 correct fixes on Defects4j 1.2 and 2.0 respectively. By calculating the cost    of accessing ChatGPT, we can fix 162 out of 337 bugs for $0.42 each!",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "819\u2013831",
        "numpages": "13",
        "keywords": "Automated Program Repair, Large Language Model",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "CREF: An LLM-Based Conversational Software Repair Framework for Programming Tutors": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680328",
        "author": "Yang, Boyang and Tian, Haoye and Pian, Weiguo and Yu, Haoran and Wang, Haitao and Klein, Jacques and Bissyand\\'{e}, Tegawend\\'{e} F. and Jin, Shunfu",
        "title": "CREF: An LLM-Based Conversational Software Repair Framework for Programming Tutors",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680328",
        "doi": "10.1145/3650212.3680328",
        "abstract": "With the proven effectiveness of Large Language Models (LLMs) in code-related tasks, researchers have explored their potential for program repair. However, existing repair benchmarks might have influenced LLM training data, potentially causing data leakage. To evaluate LLMs\u2019 realistic repair capabilities, (i) we introduce an extensive, non-crawled benchmark TutorCode, comprising 1,239 C++ defect codes and associated information such as tutor guidance, solution description, failing test cases, and the corrected code. Our work assesses LLM\u2019s repair performance on TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision (RPSR). (ii) We then provide a comprehensive investigation into which types of extra information can help LLMs improve their repair performance. Among these types, tutor guidance was the most effective information. To fully harness LLMs\u2019 conversational capabilities and the benefits of augmented information, (iii) we introduce a novel conversational semi-automatic repair framework CREF assisting human programming tutors. It demonstrates a remarkable AVG-5 improvement of 17.2\\%-24.6\\% compared to the baseline, achieving an impressive AVG-5 of 76.6\\% when utilizing GPT-4. These results highlight the potential for enhancing LLMs\u2019 repair capabilities through tutor interactions and historical conversations. The successful application of CREF in a real-world educational setting demonstrates its effectiveness in reducing tutors\u2019 workload and improving students\u2019 learning experience, showing promise for code review and other software engineering tasks.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "882\u2013894",
        "numpages": "13",
        "keywords": "Large Language Model, Open Source, Program Repair",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680343",
        "author": "Guo, Lianghong and Wang, Yanlin and Shi, Ensheng and Zhong, Wanjun and Zhang, Hongyu and Chen, Jiachi and Zhang, Ruikai and Ma, Yuchi and Zheng, Zibin",
        "title": "When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680343",
        "doi": "10.1145/3650212.3680343",
        "abstract": "Code generation aims to automatically generate code snippets that meet given natural language requirements and plays an important role in software development. Although Code LLMs have shown excellent performance in this domain, their long generation time poses a signification limitation in practice use. In this paper, we first conduct an in-depth preliminary study with different Code LLMs on code generation task and identify a significant efficiency issue, i.e., continual generation of excess tokens. It harms the developer productivity and leads to huge computational wastes. To address it, we introduce CodeFast, an inference acceleration approach for Code LLMs on code generation. The key idea of CodeFast is to terminate the inference process in time when unnecessary excess tokens are detected. First, we propose an automatic data construction framework to obtain training data. Then, we train a unified lightweight model GenGuard applicable to multiple programming languages to predict whether to terminate inference at the current step. Finally, we enhance Code LLM with GenGuard to accelerate its inference in code generation task. We conduct extensive experiments with CodeFast on five representative Code LLMs across four widely used code generation datasets. Experimental results show that (1) CodeFast can significantly improve the inference speed of various Code LLMs in code generation, ranging form 34\\% to 452\\%, without compromising the quality of generated code. (2) CodeFast is stable across different parameter settings and can generalize to untrained datasets. Our code and data are available at https://github.com/DeepSoftwareAnalytics/CodeFast.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1073\u20131085",
        "numpages": "13",
        "keywords": "Machine learning for analysis, Testing and development processes",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "AI Coders Are among Us: Rethinking Programming Language Grammar towards Efficient Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680347",
        "author": "Sun, Zhensu and Du, Xiaoning and Yang, Zhou and Li, Li and Lo, David",
        "title": "AI Coders Are among Us: Rethinking Programming Language Grammar towards Efficient Code Generation",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680347",
        "doi": "10.1145/3650212.3680347",
        "abstract": "Artificial Intelligence (AI) models have emerged as another important audience for programming languages alongside humans and machines, as we enter the era of large language models (LLMs). LLMs can now perform well in coding competitions and even write programs like developers to solve various tasks, including mathematical problems. However, the grammar and layout of current programs are designed to cater the needs of human developers -- with many grammar tokens and formatting tokens being used to make the code easier for humans to read. While this is helpful, such a design adds unnecessary computational work for LLMs, as each token they either use or produce consumes computational resources.               To improve inference efficiency and reduce computational costs, we propose the concept of AI-oriented grammar.This aims to represent code in a way that better suits the working mechanism of AI models. Code written with AI-oriented grammar discards formats and uses a minimum number of tokens to convey code semantics effectively. To demonstrate the feasibility of this concept, we explore and implement the first AI-oriented grammar for Python, named Simple Python (SimPy). SimPy is crafted by revising the original Python grammar through a series of heuristic rules. Programs written in SimPy maintain identical Abstract Syntax Tree (AST) structures to those in standard Python. This allows for not only execution via a modified AST parser, but also seamless transformation between programs written in Python and SimPy, enabling human developers and LLMs to use Python and SimPy, respectively, when they need to collaborate. We also look into methods to help existing LLMs understand and use SimPy effectively. In the experiments, compared with Python, SimPy enables a reduction in token usage by 13.5\\% and 10.4\\% for CodeLlama and GPT-4, respectively, when completing the same set of code-related tasks. Additionally, these models can maintain or even improve their performance when using SimPy instead of Python for these tasks. With these promising results, we call for further contributions to the development of AI-oriented program grammar within our community.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1124\u20131136",
        "numpages": "13",
        "keywords": "Code Generation, Large Language Model, Programming Language",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680355",
        "author": "Zhang, Cen and Zheng, Yaowen and Bai, Mingqiang and Li, Yeting and Ma, Wei and Xie, Xiaofei and Li, Yuekang and Sun, Limin and Liu, Yang",
        "title": "How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680355",
        "doi": "10.1145/3650212.3680355",
        "abstract": "Fuzz drivers are essential for library API fuzzing. However, automatically generating fuzz drivers is a complex task, as it demands the creation of high-quality, correct, and robust API usage code. An LLM-based (Large Language Model) approach for generating fuzz drivers is a promising area of research. Unlike traditional program analysis-based generators, this text-based approach is more generalized and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers. However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges.  To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers. Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects. Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings. In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs ($8,000+ charged tokens). Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year). Our study uncovered that:  1) While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications; 2) LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics. Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process; 3) While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bug detection.  Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1223\u20131235",
        "numpages": "13",
        "keywords": "Fuzz Driver Generation, Fuzz Testing, Large Language Model",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "ThinkRepair: Self-Directed Automated Program Repair": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680359",
        "author": "Yin, Xin and Ni, Chao and Wang, Shaohua and Li, Zhenhao and Zeng, Limin and Yang, Xiaohu",
        "title": "ThinkRepair: Self-Directed Automated Program Repair",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680359",
        "doi": "10.1145/3650212.3680359",
        "abstract": "Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor.   To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase. The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt. The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information.   Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs. Notably, ThinkRepair fixes 98 bugs and improves baselines by 27\\%\u223c344.4\\% on Defects4J V1.2. On Defects4J V2.0, ThinkRepair fixes 12\u223c65 more bugs than the SOTA APRs. Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most).",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1274\u20131286",
        "numpages": "13",
        "keywords": "Automated Program Repair, Large Language Model, Prompt Engineering",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "SelfPiCo: Self-Guided Partial Code Execution with LLMs": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680368",
        "author": "Xue, Zhipeng and Gao, Zhipeng and Wang, Shaohua and Hu, Xing and Xia, Xin and Li, Shanping",
        "title": "SelfPiCo: Self-Guided Partial Code Execution with LLMs",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680368",
        "doi": "10.1145/3650212.3680368",
        "abstract": "Code executability plays a vital role in software debugging and testing (e.g., detecting runtime exceptions or assertion violations). However, code execution, especially partial or arbitrary code execution, is a non-trivial task due to missing definitions and complex third-party dependencies. To make partial code (such as code snippets posted on the web or code fragments deep inside complex software projects) executable, the existing study has proposed a machine learning model to predict the undefined element types and inject the pre-defined dummy values into execution. However, the performance of their tool is limited due to its simply designed dummy values and the inability to continue learning. In this paper, we design and implement a novel framework, named SelfPiCo (Self-Guided Partial Code Executor), to dynamically guide partial code execution by incorporating the open-source LLM (i.e., Code Llama) within an interactive loop. Particularly, SelfPiCo leverages few-shot in-context learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning based on fine-tuning the Code Llama model. SelfPiCo continuously learns from code execution results and refines its predictions step after step. Our evaluations demonstrate that SelfPiCo can execute 72.7\\% and 83.3\\% of all lines in the open-source code and Stack Overflow snippets, outperforming the most recent state-of-the-art Lexecutor by 37.9\\% and 33.5\\%, respectively. Moreover, SelfPiCo successfully detected 18 and 33 runtime type error issues by executing the partial code from eight GitHub software projects and 43 Stack Overflow posts, demonstrating the practical usage and potential application of our framework in practice.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1389\u20131401",
        "numpages": "13",
        "keywords": "Dynamic Analysis, Large Language Model, Partial Code Execution, Prompt Engineering",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Neurosymbolic Repair of Test Flakiness": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680369",
        "author": "Chen, Yang and Jabbarvand, Reyhaneh",
        "title": "Neurosymbolic Repair of Test Flakiness",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680369",
        "doi": "10.1145/3650212.3680369",
        "abstract": "Test flakiness, a non-deterministic behavior of builds irrelevant to code changes, is a major and continuing impediment to deliver- ing reliable software. The very few techniques for the automated repair of test flakiness are specifically crafted to repair either Order- Dependent (OD) or Implementation-Dependent (ID) flakiness. They are also all symbolic approaches, i.e., they leverage program analy- sis to detect and repair known test flakiness patterns and root causes, failing to generalize. To bridge the gap, we propose FlakyDoctor, a neuro-symbolic technique that combines the power of LLMs\u2014 generalizability\u2014and program analysis\u2014soundness\u2014to fix different types of test flakiness. Our extensive evaluation using 873 confirmed flaky tests (332 OD and 541 ID) from 243 real-world projects demonstrates the ability of FlakyDoctor in repairing flakiness, achieving 57\\% (OD) and 59\\% (ID) success rate. Comparing to three alternative flakiness repair approaches, FlakyDoctor can repair 8\\% more ID tests than DexFix, 12\\% more OD flaky tests than ODRepair, and 17\\% more OD flaky tests than iFixFlakies. Regardless of underlying LLM, the non-LLM components of FlakyDoctor contribute to 12\u201331 \\% of the overall performance, i.e., while part of the FlakyDoctor power is from using LLMs, they are not good enough to repair flaky tests in real-world projects alone. What makes the proposed technique superior to related research on test flakiness mitigation specifically and program repair, in general, is repairing 79 previously unfixed flaky tests in real-world projects. We opened pull requests for all cases with corresponding patches; 19 of them were accepted and merged at the time of submission.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1402\u20131414",
        "numpages": "13",
        "keywords": "Large Language Models, Program Repair, Test Flakiness",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "CoSec: On-the-Fly Security Hardening of Code LLMs via Supervised Co-decoding": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680371",
        "author": "Li, Dong and Yan, Meng and Zhang, Yaosheng and Liu, Zhongxin and Liu, Chao and Zhang, Xiaohong and Chen, Ting and Lo, David",
        "title": "CoSec: On-the-Fly Security Hardening of Code LLMs via Supervised Co-decoding",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680371",
        "doi": "10.1145/3650212.3680371",
        "abstract": "Large Language Models (LLMs) specialized in code have shown exceptional proficiency across various programming-related tasks, particularly code generation. Nonetheless, due to its nature of pretraining on massive uncritically filtered data, prior studies have shown that code LLMs are prone to generate code with potential vulnerabilities. Existing approaches to mitigate this risk involve crafting data without vulnerability and subsequently retraining or fine-tuning the model. As the number of parameters exceeds a billion, the computation and data demands of the above approaches will be enormous. Moreover, an increasing number of code LLMs tend to be distributed as services, where the internal representation is not accessible, and the API is the only way to reach the LLM, making the prior mitigation strategies non-applicable.    To cope with this, we propose CoSec, an on-the-fly Security hardening method of code LLMs based on security model-guided Co-decoding, to reduce the likelihood of code LLMs to generate code containing vulnerabilities. Our key idea is to train a separate but much smaller security model to co-decode with a target code LLM. Since the trained secure model has higher confidence for secure tokens, it guides the generation of the target base model towards more secure code generation. By adjusting the probability distributions of tokens during each step of the decoding process, our approach effectively influences the tendencies of generation without accessing the internal parameters of the target code LLM. We have conducted extensive experiments across various parameters in multiple code LLMs (i.e., CodeGen, StarCoder, and DeepSeek-Coder), and the results show that our approach is effective in security hardening. Specifically, our approach improves the average security ratio of six base models by 5.02\\%-37.14\\%, while maintaining the functional correctness of the target model.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1428\u20131439",
        "numpages": "12",
        "keywords": "AI Safety, Code Generation, Large Language Models, Software Security",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "One-to-One or One-to-Many? Suggesting Extract Class Refactoring Opportunities with Intra-class Dependency Hypergraph Neural Network": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680379",
        "author": "Cui, Di and Wang, Qiangqiang and Zhao, Yutong and Wang, Jiaqi and Wei, Minjie and Hu, Jingzhao and Wang, Luqiao and Li, Qingshan",
        "title": "One-to-One or One-to-Many? Suggesting Extract Class Refactoring Opportunities with Intra-class Dependency Hypergraph Neural Network",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680379",
        "doi": "10.1145/3650212.3680379",
        "abstract": "Excessively large classes that encapsulate multiple responsibilities are challenging to comprehend and maintain. Addressing this issue, several Extract Class refactoring tools have been proposed, employing a two-phase process: identifying suitable fields or methods for extraction, and implementing the mechanics of refactoring. These tools traditionally generate an intra-class dependency graph to analyze the class structure, applying hard-coded rules based on this graph to unearth refactoring opportunities. Yet, the graph-based approach predominantly illuminates direct, \u201cone-to-one\u201d relationship between pairwise entities. Such a perspective is restrictive as it overlooks the complex, \u201cone-to-many\u201d dependencies among multiple entities that are prevalent in real-world classes. This narrow focus can lead to refactoring suggestions that may diverge from developers\u2019 actual needs, given their multifaceted nature. To bridge this gap, our paper leverages the concept of intra-class dependency hypergraph to model one-to-many dependency relationship and proposes a hypergraph learning-based approach to suggest Extract Class refactoring opportunities named HECS. For each target class, we first construct its intra-class dependency hypergraph and assign attributes to nodes with a pre-trained code model. All the attributed hypergraphs are fed into an enhanced hypergraph neural network for training. Utilizing this trained neural network alongside a large language model (LLM), we construct a refactoring suggestion system. We trained HECS on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 38.5\\% in precision, 9.7\\% in recall, and 44.4\\% in f1-measure compared to 3 state-of-the-art refactoring tools including JDeodorant, SSECS, and LLMRefactor, which is more useful for 64\\% of participants. The results also unveil practical suggestions and new insights that benefit existing extract-related refactoring techniques.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1529\u20131540",
        "numpages": "12",
        "keywords": "Extract Class Refactoring, Hypergraph Neural Network",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "HECS: A Hypergraph Learning-based System for Detecting Extract Class Refactoring Opportunities": {
        "type": "software",
        "key": "10.5281/zenodo.12662219",
        "author": "Cui, Di and Wang, Qiangqiang and Zhao, Yutong and Wang, Jiaqi and Wei, Minjie and Hu, Jingzhao and Wang, Luqiao and Li, Qingshan",
        "title": "HECS: A Hypergraph Learning-based System for Detecting Extract Class Refactoring Opportunities",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.12662219",
        "abstract": "    <p>HECS is an advanced tool designed for Extract Class refactoring byleveraging hypergraph learning to model complex dependencies withinlarge classes. Unlike traditional tools that rely on direct oneto-onedependency graphs, HECS uses intra-class dependency hypergraphs tocapture one-to-many relationships. This allows HECS to provide moreaccurate and relevant refactoring suggestions. The tool constructshypergraphs for each target class, attributes nodes using a pre-trainedcode model, and trains an enhanced hypergraph neural network. Coupledwith a large language model, HECS delivers practical refactoringsuggestions. In evaluations on largescale and real-world datasets, HECSachieved a 38.5\\% increase in precision, 9.7\\% in recall, and 44.4\\% inf1-measure compared to JDeodorant, SSECS, and LLMRefactor. Theseimprovements make HECS a valuable tool for developers, offeringpractical insights and enhancing existing refactoring techniques.</p>",
        "keywords": "Extract Class Refactoring, Hypergraph Neural Network"
    },
    "Large Language Models Can Connect the Dots: Exploring Model Optimization Bugs with Domain Knowledge-Aware Prompts": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680383",
        "author": "Guan, Hao and Bai, Guangdong and Liu, Yepang",
        "title": "Large Language Models Can Connect the Dots: Exploring Model Optimization Bugs with Domain Knowledge-Aware Prompts",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680383",
        "doi": "10.1145/3650212.3680383",
        "abstract": "Model optimization, such as pruning and quantization, has become the de facto pre-deployment phase when deploying deep learning&nbsp;(DL) models on resource-constrained platforms.         However, the complexity of DL models often leads to non-trivial bugs in model optimizers, known as model optimization bugs&nbsp;(MOBs).         These MOBs are characterized by involving complex data types and layer structures inherent to DL models, causing significant hurdles in detecting them through traditional static analysis and dynamic testing techniques.        In this work, we leverage Large Language Models (LLMs) with prompting techniques to generate test cases for MOB detection.        We explore how LLMs can draw an understanding of the MOB domain from scattered bug instances and generalize to detect new ones, a paradigm we term as concentration and diffusion.        We extract MOB domain knowledge from the artifacts of known MOBs, such as their issue reports and fixes, and design knowledge-aware prompts to guide LLMs in generating effective test cases.         The domain knowledge of code structure and error description provides precise in-depth depictions of the problem domain, i.e., the concentration, and heuristic directions to generate innovative test cases, i.e., the diffusion.         Our approach is implemented as a tool named YanHui and benchmarked against existing few-shot LLM-based fuzzing techniques.         Test cases generated by YanHui demonstrate enhanced capability to find relevant API and data combinations for exposing MOBs, leading to an 11.4\\% increase in generating syntactically valid code and a 22.3\\% increase in generating on-target code specific to model optimization.         YanHui detects 17 MOBs, and among them, five are deep MOBs that are difficult to reveal without our prompting technique.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1579\u20131591",
        "numpages": "13",
        "keywords": "Large Language Model, Library Testing, Model Optimization",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Prototype of YanHui, the tools in `Large Language Models Can Connect the Dots: Exploring Model Optimization Bugs with Domain Knowledge-Aware Prompts'": {
        "type": "software",
        "key": "10.5281/zenodo.12671638",
        "author": "Guan, Hao and Bai, Guangdong and Liu, Yepang",
        "title": "Prototype of YanHui, the tools in `Large Language Models Can Connect the Dots: Exploring Model Optimization Bugs with Domain Knowledge-Aware Prompts'",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.12671638",
        "abstract": "    <p>Prototype of YanHui, including the data and labels for GitHub issues.Code to generate test cases with large language models.</p>",
        "keywords": "Large Language Model, Library Testing, Model Optimization"
    },
    "AutoCodeRover: Autonomous Program Improvement": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680384",
        "author": "Zhang, Yuntong and Ruan, Haifeng and Fan, Zhiyu and Roychoudhury, Abhik",
        "title": "AutoCodeRover: Autonomous Program Improvement",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680384",
        "doi": "10.1145/3650212.3680384",
        "abstract": "Researchers have made significant progress in automating the software development process in the past decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM\u2019s understanding of the issue\u2019s root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on the recently proposed SWE-bench-lite (300 real-life Github issues) show increased efficacy in solving Github issues (19\\% on SWE-bench-lite), which is higher than the efficacy of the recently reported Swe-agent. Interestingly, our approach resolved 57 GitHub issues in about 4 minutes each (pass@1), whereas developers spent more than 2.68 days on average. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1592\u20131604",
        "numpages": "13",
        "keywords": "automatic program repair, autonomous software engineering, autonomous software improvement, large language model",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680388",
        "author": "Xue, Zhiyi and Li, Liangguo and Tian, Senyue and Chen, Xiaohong and Li, Pingping and Chen, Liangyu and Jiang, Tingting and Zhang, Min",
        "title": "LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680388",
        "doi": "10.1145/3650212.3680388",
        "abstract": "FinTech software, crucial for both safety and timely market deployment, presents a compelling case for automated acceptance testing against regulatory business rules. However, the inherent challenges of comprehending unstructured natural language descriptions of these rules and crafting comprehensive test cases demand human intelligence. The emergence of Large Language Models (LLMs) holds promise for automated test case generation, leveraging their natural language processing capabilities. Yet, their dependence on human intervention for effective prompting hampers efficiency.    In response, we introduce a groundbreaking, fully automated approach for generating high-coverage test cases from natural language business rules. Our methodology seamlessly integrates the versatility of LLMs with the predictability of algorithmic methods. We fine-tune pre-trained LLMs for improved information extraction accuracy and algorithmically generate comprehensive testable scenarios for the extracted business rules.\tOur prototype, LLM4Fin, is designed for testing real-world stock-trading software. Experimental results demonstrate LLM4Fin\u2019s superiority over both state-of-the-art LLM, such as ChatGPT, and skilled testing engineers. We achieve remarkable performance, with up to 98.18\\% and an average of 20\\%\u2212110\\% improvement on business scenario coverage, and up to 93.72\\% on code coverage, while reducing the time cost from 20 minutes to a mere 7 seconds. These results provide robust evidence of the framework\u2019s practical applicability and efficiency, marking a significant advancement in FinTech software testing.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1643\u20131655",
        "numpages": "13",
        "keywords": "Software acceptance testing, fintech software, large language model, test case generation",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Fuzzing JavaScript Interpreters with Coverage-Guided Reinforcement Learning for LLM-Based Mutation": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680389",
        "author": "Eom, Jueon and Jeong, Seyeon and Kwon, Taekyoung",
        "title": "Fuzzing JavaScript Interpreters with Coverage-Guided Reinforcement Learning for LLM-Based Mutation",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680389",
        "doi": "10.1145/3650212.3680389",
        "abstract": "JavaScript interpreters, crucial for modern web browsers, require an effective fuzzing method to identify security-related bugs. However, the strict grammatical requirements for input present significant challenges. Recent efforts to integrate language models for context- aware mutation in fuzzing are promising but lack the necessary coverage guidance to be fully effective. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with Reinforcement Learning (RL) from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the generation of test cases that are more likely to discover new coverage areas, thus improving bug detection while minimizing syntax and semantic errors, all without needing extra post-processing. Our evaluation results show that CovRL-Fuzz outperforms the state-of-the-art fuzzers in enhancing code coverage and identifying bugs in JavaScript interpreters: CovRL-Fuzz identified 58 real-world security-related bugs in the latest JavaScript interpreters, including 50 previously unknown bugs and 15 CVEs.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1656\u20131668",
        "numpages": "13",
        "keywords": "coverage, fuzzing, large language model, reinforcement learning",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Large Language Models for Equivalent Mutant Detection: How Far Are We?": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680395",
        "author": "Tian, Zhao and Shu, Honglin and Wang, Dong and Cao, Xuejie and Kamei, Yasutaka and Chen, Junjie",
        "title": "Large Language Models for Equivalent Mutant Detection: How Far Are We?",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680395",
        "doi": "10.1145/3650212.3680395",
        "abstract": "Mutation testing is vital for ensuring software quality. However, the presence of equivalent mutants is known to introduce redundant cost and bias issues, hindering the effectiveness of mutation testing in practical use. Although numerous equivalent mutant detection (EMD) techniques have been proposed, they exhibit limitations due to the scarcity of training data and challenges in generalizing to unseen mutants. Recently, large language models (LLMs) have been extensively adopted in various code-related tasks and have shown superior performance by more accurately capturing program semantics. Yet the performance of LLMs in equivalent mutant detection remains largely unclear. In this paper, we conduct an empirical study on 3,302 method-level Java mutant pairs to comprehensively investigate the effectiveness and efficiency of LLMs for equivalent mutant detection. Specifically, we assess the performance of LLMs compared to existing EMD techniques, examine the various strategies of LLMs, evaluate the orthogonality between EMD techniques, and measure the time overhead of training and inference. Our findings demonstrate that LLM-based techniques significantly outperform existing techniques (i.e., the average improvement of 35.69\\% in terms of F1-score), with the fine-tuned code embedding strategy being the most effective. Moreover, LLM-based techniques offer an excellent balance between cost (relatively low training and inference time) and effectiveness. Based on our findings, we further discuss the impact of model size and embedding quality, and provide several promising directions for future research. This work is the first to examine LLMs in equivalent mutant detection, affirming their effectiveness and efficiency.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1733\u20131745",
        "numpages": "13",
        "keywords": "Empirical Study, Equivalent Mutant Detection, Large Language Model, Mutation Testing",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Maltracker: A Fine-Grained NPM Malware Tracker Copiloted by LLM-Enhanced Dataset": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680397",
        "author": "Yu, Zeliang and Wen, Ming and Guo, Xiaochen and Jin, Hai",
        "title": "Maltracker: A Fine-Grained NPM Malware Tracker Copiloted by LLM-Enhanced Dataset",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680397",
        "doi": "10.1145/3650212.3680397",
        "abstract": "As the largest package registry, Node Package Manager (NPM) has become the prime target for various supply chain attacks recently and has been flooded with numerous malicious packages, posing significant security risks to end-users. Learning-based methods have demonstrated promising performance with good adaptability to various types of attacks. However, they suffer from two main limitations. First, they often utilize metadata features or coarse-grained code features extracted at the package level while overlooking complex code semantics. Second, the dataset used to train the model often suffers from a lack of variety both in quantity and diversity, and thus cannot detect significant types of attacks.      To address these problems, we introduce Maltracker, a learningbased NPM malware tracker based on fine-grained features empowered by LLM-enhanced dataset. First, Maltracker constructs precise call graphs to extract suspicious functions that are reachable to a pre-defined set of sensitive APIs, and then utilizes community detection algorithm to identify suspicious code gadgets based on program dependency graph, from which fine-grained features are then extracted. To address the second limitation, we extend the dataset using advanced large language models (LLM) to translate malicious functions from other languages (e.g., C/C++, Python, and Go) into JavaScript. Evaluations shows that Maltracker can achieve an improvement of about 12.6\\% in terms of F1-score at the package level and 31.0\\% at the function level compared with the SOTA learning-based methods. Moreover, the key components of \ud835\udc40\ud835\udc4e\ud835\udc59\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc5f all contribute to the effectiveness of its performance. Finally, Maltracker has also detected 230 new malicious packages in NPM and received 61 thanks letters, among which some contain new malicious behaviors that cannot be detected by existing tools.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1759\u20131771",
        "numpages": "13",
        "keywords": "Code Translation, Large Language Model, Malware Detection",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Calico: Automated Knowledge Calibration and Diagnosis for Elevating AI Mastery in Code Tasks": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680399",
        "author": "Qiu, Yuxin and Hu, Jie and Zhang, Qian and Yin, Heng",
        "title": "Calico: Automated Knowledge Calibration and Diagnosis for Elevating AI Mastery in Code Tasks",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680399",
        "doi": "10.1145/3650212.3680399",
        "abstract": "Recent advancements in large language models (LLMs) have exhibited promising capabilities in addressing various tasks such as defect detection and program repair. Despite their prevalence, LLMs still face limitations in effectively handling these tasks. Common strategies to adapt them and improve their performance for specific tasks involve fine-tuning models based on user data or employing in-context learning with examples of desired inputs and outputs.    However, they pose challenges for practical adoption due to the need for extensive computational resources, high-quality data, and continuous maintenance. Furthermore, neither strategy can explain or reason about the deficiencies of LLMs in the given tasks.         We propose Calico to address the high cost of fine-tuning, eliminate the necessity for task-specific examples, and provide explanations of LLM deficiency. At the heart of Calico is an evolutionary approach that interleaves knowledge calibration and AI deficiency diagnosis. The key essence of Calico is as follows. First, it focuses on identifying knowledge gaps in LLMs\u2019 program comprehension. Second, it conducts automated code refactoring to integrate the overlooked knowledge into the source code for mitigating those gaps. Third, it employs what-if analysis and counterfactual reasoning to determine a minimum set of overlooked knowledge necessary to improve the performance of LLMs in code tasks.        We have extensively evaluated Calico over 8,938 programs on three most commonly seen code tasks. Our experimental results show that vanilla ChatGPT cannot fully understand code structures. With knowledge calibration, Calico improves it by 20\\% and exhibits comparable proficiency compared to fine-tuned LLMs. Deficiency diagnosis contributes to 8\\% reduction in program sizes while ensuring performance. These impressive results demonstrate the feasibility of utilizing a vanilla LLM for automated software engineering (SE) tasks, thereby avoiding the high computational costs associated with a fine-tuned model.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1785\u20131797",
        "numpages": "13",
        "keywords": "Software engineering, large language model, software testing",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "HECS: A Hypergraph Learning-Based System for Detecting Extract Class Refactoring Opportunities": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3685307",
        "author": "Wang, Luqiao and Wang, Qiangqiang and Wang, Jiaqi and Zhao, Yutong and Wei, Minjie and Quan, Zhou and Cui, Di and Li, Qingshan",
        "title": "HECS: A Hypergraph Learning-Based System for Detecting Extract Class Refactoring Opportunities",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3685307",
        "doi": "10.1145/3650212.3685307",
        "abstract": "HECS is an advanced tool designed for Extract Class refactoring by leveraging hypergraph learning to model complex dependencies within large classes. Unlike traditional tools that rely on direct one-to-one dependency graphs, HECS uses intra-class dependency hypergraphs to capture one-to-many relationships. This allows HECS to provide more accurate and relevant refactoring suggestions. The tool constructs hypergraphs for each target class, attributes nodes using a pre-trained code model, and trains an enhanced hypergraph neural network. Coupled with a large language model, HECS delivers practical refactoring suggestions. In evaluations on large-scale and real-world datasets, HECS achieved a 38.5\\% increase in precision, 9.7\\% in recall, and 44.4\\% in f1-measure compared to JDeodorant, SSECS, and LLMRefactor. These improvements make HECS a valuable tool for developers, offering practical insights and enhancing existing refactoring techniques.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1851\u20131855",
        "numpages": "5",
        "keywords": "Extract Class Refactoring, Hypergraph Neural Network",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Leveraging Natural Language Processing and Data Mining to Augment and Validate APIs": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3685554",
        "author": "Decrop, Alix",
        "title": "Leveraging Natural Language Processing and Data Mining to Augment and Validate APIs",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3685554",
        "doi": "10.1145/3650212.3685554",
        "abstract": "APIs are increasingly prominent for modern web applications, allowing millions of users around the world to access data. Reducing the risk of API defects - and consequently failures - is key, notably for security, availability, and maintainability purposes. Documenting an API is crucial, allowing the user to better understand it. Moreover, API testing techniques often require formal documentation as input. However, documenting is a time-consuming and error-prone task, often overlooked by developers. Natural Language Processing (NLP) could assist API development, as recent Large Language Models (LLMs) demonstrated exceptional abilities to automate tasks based on their colossal training data. Data mining could also be utilized, synthesizing API information scattered across the web. Hence, I present my PhD project aimed at exploring the usage of NLP-related technologies and data mining to augment and validate APIs. The research questions of this PhD project are: (1) What types of APIs can benefit from NLP and data mining assistance? (2) What API problems can be solved with such methods? (3) How effective are the methods (i.e. LLMs) in assisting APIs? (4) How efficient are the methods in assisting APIs (i.e. time and costs)?",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1906\u20131908",
        "numpages": "3",
        "keywords": "API, Automation, Data Mining, LLM, NLP, Software Testing",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "Collaboration to Repository-Level Vulnerability Detection": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3685562",
        "author": "Wen, Xin-Cheng",
        "title": "Collaboration to Repository-Level Vulnerability Detection",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3685562",
        "doi": "10.1145/3650212.3685562",
        "abstract": "Large Language Model (LLM)-based methods have proven to be effective for many software engineering domains, with a potential for substantial productivity effective for software vulnerability detection.    However, due to the limitation of the length of input contexts of LLM, the existing LLM-based methods mainly focus on detecting function-level and leveraging the in-file context information for vulnerability detection (i.e., intra-procedural vulnerabilities), ignoring the more complex inter-procedural vulnerability detection scenarios in practice.    For instance, in real-world scenarios, developers routinely engage with program analysis to detect vulnerabilities that span multiple cross-file information within repositories.       Since complex processes tend to have redundancy dependencies from spanning multiple files in the repository level and invoking multiple static analysis tools, the ideal goal of vulnerability detection is to extract the vulnerability-related information from the repository and provide potential possible explanations for vulnerability triggers.   However, such a goal is hard to achieve, and thus in this work, we design three works through multi-agent collaboration to approach the goal of repository-level vulnerability detection.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1926\u20131928",
        "numpages": "3",
        "keywords": "Large Language Model, Software Vulnerability Detection",
        "location": "Vienna, Austria",
        "series": "ISSTA 2024"
    },
    "UniLog: Automatic Logging via LLM and In-Context Learning": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623326",
        "author": "Xu, Junjielong and Cui, Ziang and Zhao, Yuan and Zhang, Xu and He, Shilin and He, Pinjia and Li, Liqun and Kang, Yu and Lin, Qingwei and Dang, Yingnong and Rajmohan, Saravan and Zhang, Dongmei",
        "title": "UniLog: Automatic Logging via LLM and In-Context Learning",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623326",
        "doi": "10.1145/3597503.3623326",
        "abstract": "Logging, which aims to determine the position of logging statements, the verbosity levels, and the log messages, is a crucial process for software reliability enhancement. In recent years, numerous automatic logging tools have been designed to assist developers in one of the logging tasks (e.g., providing suggestions on whether to log in try-catch blocks). These tools are useful in certain situations yet cannot provide a comprehensive logging solution in general. Moreover, although recent research has started to explore end-to-end logging, it is still largely constrained by the high cost of fine-tuning, hindering its practical usefulness in software development. To address these problems, this paper proposes UniLog, an automatic logging framework based on the in-context learning (ICL) paradigm of large language models (LLMs). Specifically, UniLog can generate an appropriate logging statement with only a prompt containing five demonstration examples without any model tuning. In addition, UniLog can further enhance its logging ability after warmup with only a few hundred random samples. We evaluated UniLog on a large dataset containing 12,012 code snippets extracted from 1,465 GitHub repositories. The results show that UniLog achieved the state-of-the-art performance in automatic logging: (1) 76.9\\% accuracy in selecting logging positions, (2) 72.3\\% accuracy in predicting verbosity levels, and (3) 27.1 BLEU-4 score in generating log messages. Meanwhile, UniLog requires less than 4\\% of the parameter tuning time needed by fine-tuning the same LLM.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "14",
        "numpages": "12",
        "keywords": "logging, large language model, in-context learning",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623345",
        "author": "Steenhoek, Benjamin and Gao, Hongyang and Le, Wei",
        "title": "Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623345",
        "doi": "10.1145/3597503.3623345",
        "abstract": "Deep learning-based vulnerability detection has shown great performance and, in some studies, outperformed static analysis tools. However, the highest-performing approaches use token-based transformer models, which are not the most efficient to capture code semantics required for vulnerability detection. Classical program analysis techniques such as dataflow analysis can detect many types of bugs based on their root causes. In this paper, we propose to combine such causal-based vulnerability detection algorithms with deep learning, aiming to achieve more efficient and effective vulnerability detection. Specifically, we designed DeepDFA, a dataflow analysis-inspired graph learning framework and an embedding technique that enables graph learning to simulate dataflow computation. We show that DeepDFA is both performant and efficient. DeepDFA outperformed all non-transformer baselines. It was trained in 9 minutes, 75x faster than the highest-performing baseline model. When using only 50+ vulnerable and several hundreds of total examples as training data, the model retained the same performance as 100\\% of the dataset. DeepDFA also generalized to real-world vulnerabilities in DbgBench; it detected 8.7 out of 17 vulnerabilities on average across folds and was able to distinguish between patched and buggy versions, while the highest-performing baseline models did not detect any vulnerabilities. By combining DeepDFA with a large language model, we surpassed the state-of-the-art vulnerability detection performance on the Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Our replication package is located at https://doi.org/10.6084/m9.figshare.21225413.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "16",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Large Language Models for Test-Free Fault Localization": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623342",
        "author": "Yang, Aidan Z. H. and Le Goues, Claire and Martins, Ruben and Hellendoorn, Vincent",
        "title": "Large Language Models for Test-Free Fault Localization",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623342",
        "doi": "10.1145/3597503.3623342",
        "abstract": "Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3\\%--54.4\\%, and Top-5 results by 14.4\\%-35.6\\%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "17",
        "numpages": "12",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3608134",
        "author": "Geng, Mingyang and Wang, Shangwen and Dong, Dezun and Wang, Haotian and Li, Ge and Jin, Zhi and Mao, Xiaoguang and Liao, Xiangke",
        "title": "Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3608134",
        "doi": "10.1145/3597503.3608134",
        "abstract": "Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers' program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers' diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers' intents. We thus postulate that the LLMs can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the LLM (e.g., providing it with ten or more examples), the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM's performances, which shed light on future research directions for using LLMs to achieve comment generation.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "39",
        "numpages": "13",
        "keywords": "code summarization, large language model, in-context learning",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623343",
        "author": "Deng, Yinlin and Xia, Chunqiu Steven and Yang, Chenyuan and Zhang, Shizhuo Dylan and Yang, Shujing and Zhang, Lingming",
        "title": "Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623343",
        "doi": "10.1145/3597503.3623343",
        "abstract": "Bugs in Deep Learning (DL) libraries may affect almost all downstream DL applications, and it is crucial to ensure the quality of such systems. It is challenging to generate valid input programs for fuzzing DL libraries, since the input programs need to satisfy both the syntax/semantics of the supported languages (e.g., Python) and the tensor/operator constraints for constructing valid computational graphs. Recently, the TitanFuzz work demonstrates that modern Large Language Models (LLMs) can be directly leveraged to implicitly learn all the language and DL computation constraints to generate valid programs for fuzzing DL libraries (and beyond). However, LLMs tend to generate ordinary programs following similar patterns/tokens with typical programs seen in their massive pre-training corpora (e.g., GitHub), while fuzzing favors unusual inputs that cover edge cases or are unlikely to be manually produced.To fill this gap, this paper proposes FuzzGPT, the first approach to priming LLMs to synthesize unusual programs for fuzzing. FuzzGPT is mainly built on the well-known hypothesis that historical bug-triggering programs may include rare/valuable code ingredients important for bug finding. Meanwhile, while traditional techniques leveraging such historical information require intensive human efforts to both design dedicated generators and ensure the syntactic/semantic validity of generated programs, FuzzGPT demonstrates that this process can be fully automated via the intrinsic capabilities of LLMs (including fine-tuning and in-context learning), while being generalizable and applicable to challenging domains. While FuzzGPT can be applied with different LLMs, this paper focuses on the powerful GPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potential of directly leveraging the instruction-following capability of the recent ChatGPT for effective fuzzing. The experimental study on two popular DL libraries (PyTorch and TensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz, detecting 76 bugs, with 49 already confirmed as previously unknown bugs, including 11 high-priority bugs or security vulnerabilities.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "70",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639091",
        "author": "Li, Zongjie and Wang, Chaozheng and Ma, Pingchuan and Liu, Chaowei and Wang, Shuai and Wu, Daoyuan and Gao, Cuiyun and Liu, Yang",
        "title": "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639091",
        "doi": "10.1145/3597503.3639091",
        "abstract": "Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models.In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as \"code synthesis\" and \"code translation.\" We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "74",
        "numpages": "13",
        "keywords": "large language models, imitation attacks",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639120",
        "author": "Sun, Zhensu and Du, Xiaoning and Song, Fu and Wang, Shangwen and Li, Li",
        "title": "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639120",
        "doi": "10.1145/3597503.3639120",
        "abstract": "Leveraging recent advancements in large language models, modern neural code completion models have demonstrated the capability to generate highly accurate code suggestions. However, their massive size poses challenges in terms of computational costs and environmental impact, hindering their widespread adoption in practical scenarios. Dynamic inference emerges as a promising solution, as it allocates minimal computation during inference while maintaining the model's performance. In this research, we explore dynamic inference within the context of code completion. Initially, we conducted an empirical investigation on GPT-2, focusing on the inference capabilities of intermediate layers for code completion. We found that 54.4\\% of tokens can be accurately generated using just the first layer, signifying significant computational savings potential. Moreover, despite using all layers, the model still fails to predict 14.5\\% of tokens correctly, and the subsequent completions continued from them are rarely considered helpful, with only a 4.2\\% Acceptance Rate. These findings motivate our exploration of dynamic inference in code completion and inspire us to enhance it with a decision-making mechanism that stops the generation of incorrect code. We thus propose a novel dynamic inference method specifically tailored for code completion models. This method aims not only to produce correct predictions with largely reduced computation but also to prevent incorrect predictions proactively. Our extensive evaluation shows that it can averagely skip 1.7 layers out of 16 layers in the models, leading to an 11.2\\% speedup with only a marginal 1.1\\% reduction in ROUGE-L.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "75",
        "numpages": "12",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Traces of Memorisation in Large Language Models for Code": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639133",
        "author": "Al-Kaswan, Ali and Izadi, Maliheh and van Deursen, Arie",
        "title": "Traces of Memorisation in Large Language Models for Code",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639133",
        "doi": "10.1145/3597503.3639133",
        "abstract": "Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentially extractable we were able to extract 47\\% from a CodeGen-Mono-16B code completion model. We also observe that models memorise more, as their parameter count grows, and that their pre-training data are also vulnerable to attack. We also find that data carriers are memorised at a higher rate than regular code or documentation and that different model architectures memorise different samples. Data leakage has severe outcomes, so we urge the research community to further investigate the extent of this phenomenon using a wider range of models and extraction techniques in order to build safeguards to mitigate this issue.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "78",
        "numpages": "12",
        "keywords": "large language models, privacy, memorisation, data leakage",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Evaluating Large Language Models in Class-Level Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639219",
        "author": "Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling",
        "title": "Evaluating Large Language Models in Class-Level Code Generation",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639219",
        "doi": "10.1145/3597503.3639219",
        "abstract": "Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a simple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios.To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "81",
        "numpages": "13",
        "keywords": "class-level code generation, large language model, benchmark",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639226",
        "author": "Pan, Rangeet and Ibrahimzada, Ali Reza and Krishna, Rahul and Sankar, Divya and Wassi, Lambert Pouguem and Merler, Michele and Sobolev, Boris and Pavuluri, Raju and Sinha, Saurabh and Jabbarvand, Reyhaneh",
        "title": "Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639226",
        "doi": "10.1145/3597503.3639226",
        "abstract": "Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation---with correct translations ranging from 2.1\\% to 47.3\\% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5\\% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset---consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs---can help drive research in this area.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "82",
        "numpages": "13",
        "keywords": "code translation, bug taxonomy, llm",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Rust-lancet: Automated Ownership-Rule-Violation Fixing with Behavior Preservation": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639103",
        "author": "Yang, Wenzhang and Song, Linhai and Xue, Yinxing",
        "title": "Rust-lancet: Automated Ownership-Rule-Violation Fixing with Behavior Preservation",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639103",
        "doi": "10.1145/3597503.3639103",
        "abstract": "As a relatively new programming language, Rust is designed to provide both memory safety and runtime performance. To achieve this goal, Rust conducts rigorous static checks against its safety rules during compilation, effectively eliminating memory safety issues that plague C/C++ programs. Although useful, the safety rules pose programming challenges to Rust programmers, since programmers can easily violate safety rules when coding in Rust, leading their code to be rejected by the Rust compiler, a fact underscored by a recent user study. There exists a desire to automate the process of fixing safety-rule violations to enhance Rust's programmability.In this paper, we concentrate on Rust's ownership rules and develop rust-lancet to automatically fix their violations. We devise three strategies for altering code, each intended to modify a Rust program and make it pass Rust's compiler checks. Additionally, we introduce mental semantics to model the behaviors of Rust programs that cannot be compiled due to ownership-rule violations. We design an approach to verify whether modified programs preserve their original behaviors before patches are applied. We apply rust-lancet to 160 safety-rule violations from two sources, successfully fixing 102 violations under the optimal configuration --- more than rustc and six LLM-based techniques. Notably, rust-lancet avoids generating any incorrect patches, a distinction from all other baseline techniques. We also verify the effectiveness of each fixing strategy and behavior preservation validation and affirm the rationale behind these components.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "85",
        "numpages": "13",
        "keywords": "rust, program repair, compiler error",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "PyTy: Repairing Static Type Errors in Python": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639184",
        "author": "Chow, Yiu Wai and Di Grazia, Luca and Pradel, Michael",
        "title": "PyTy: Repairing Static Type Errors in Python",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639184",
        "doi": "10.1145/3597503.3639184",
        "abstract": "Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4\\% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "87",
        "numpages": "13",
        "keywords": "automatic program repair, type annotation, transfer learning",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Programming Assistant for Exception Handling with CodeBERT": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639188",
        "author": "Cai, Yuchen and Yadavally, Aashish and Mishra, Abhishek and Montejo, Genesis and Nguyen, Tien",
        "title": "Programming Assistant for Exception Handling with CodeBERT",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639188",
        "doi": "10.1145/3597503.3639188",
        "abstract": "With practical code reuse, the code fragments from developers' forums often migrate to applications. Owing to the incomplete nature of such fragments, they often lack the details on exception handling. The adaptation for exception handling to the codebase is not trivial as developers must learn and memorize what API methods could cause exceptions and what exceptions need to be handled. We propose Neurex, an exception handling recommender that learns from complete code, and accepts a given Java code snippet and recommends 1) if a try-catch block is needed, 2) what statements need to be placed in a try block, and 3) what exception types need to be caught in the catch clause. Inspired by the sequence chunking techniques in natural language processing, we design Neurex via a multi-tasking model with the fine-tuning of the large language model CodeBERT for these three exception handling recommendation tasks. Via the large language model, Neurex can learn the surrounding context, leading to better learning the dependencies among the API elements, and the relations between the statements and the corresponding exception types needed to be handled.Our empirical evaluation shows that Neurex correctly performs all three exception handling recommendation tasks in 71.5\\% of the cases with a F1-score of 70.2\\%, which is a relative improvement of 166\\% over the baseline. It achieves high F1-score from 98.2\\%-99.7\\% in try-catch block necessity checking (a relative improvement of up to 55.9\\% over the baselines). It also correctly decides both the need for try-catch block(s) and the statements to be placed in try blocks with the F1-scores of 74.7\\% and 87.1\\% at the instance and statement levels, an improvement of 129.1\\% and 44.9\\% over the baseline, respectively. Our extrinsic evaluation shows that Neurex relatively improves over the baseline by 56.5\\% in F1-score for detecting exception-related bugs in incomplete Android code snippets.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "94",
        "numpages": "13",
        "keywords": "AI4SE, large language models, automated exception handling",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Using an LLM to Help With Code Understanding": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639187",
        "author": "Nam, Daye and Macvean, Andrew and Hellendoorn, Vincent and Vasilescu, Bogdan and Myers, Brad",
        "title": "Using an LLM to Help With Code Understanding",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639187",
        "doi": "10.1145/3597503.3639187",
        "abstract": "Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5-turbo model with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "97",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Fuzz4All: Universal Fuzzing with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639121",
        "author": "Xia, Chunqiu Steven and Paltenghi, Matteo and Le Tian, Jia and Pradel, Michael and Zhang, Lingming",
        "title": "Fuzz4All: Universal Fuzzing with Large Language Models",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639121",
        "doi": "10.1145/3597503.3639121",
        "abstract": "Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are well-suited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java, and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 64 bugs already confirmed by developers as previously unknown.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "126",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639210",
        "author": "Fu, Jingzhou and Liang, Jie and Wu, Zhiyong and Jiang, Yu",
        "title": "Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639210",
        "doi": "10.1145/3597503.3639210",
        "abstract": "Effective DBMS fuzzing relies on high-quality initial seeds, which serve as the starting point for mutation. These initial seeds should incorporate various DBMS features to explore the state space thoroughly. While built-in test cases are typically used as initial seeds, many DBMSs lack comprehensive test cases, making it difficult to apply state-of-the-art fuzzing techniques directly.To address this, we propose Sedar which produces initial seeds for a target DBMS by transferring test cases from other DBMSs. The underlying insight is that many DBMSs share similar functionalities, allowing seeds that cover deep execution paths in one DBMS to be adapted for other DBMSs. The challenge lies in converting these seeds to a format supported by the grammar of the target database. Sedar follows a three-step process to generate seeds. First, it executes existing SQL test cases within the DBMS they were designed for and captures the schema information during execution. Second, it utilizes large language models (LLMs) along with the captured schema information to guide the generation of new test cases based on the responses from the LLM. Lastly, to ensure that the test cases can be properly parsed and mutated by fuzzers, Sedar temporarily comments out unparsable sections for the fuzzers and uncomments them after mutation. We integrate Sedar into the DBMS fuzzers Sqirrel and Griffin, targeting DBMSs such as Virtuoso, MonetDB, DuckDB, and ClickHouse. Evaluation results demonstrate significant improvements in both fuzzers. Specifically, compared to Sqirrel and Griffin with non-transferred seeds, Sedar enhances code coverage by 72.46\\%-214.84\\% and 21.40\\%-194.46\\%; compared to Sqirrel and Griffin with native test cases of these DBMSs as initial seeds, incorporating the transferred seeds of Sedar results in an improvement in code coverage by 4.90\\%-16.20\\% and 9.73\\%-28.41\\%. Moreover, Sedar discovered 70 new vulnerabilities, with 60 out of them being uniquely found by Sedar with transferred seeds, and 19 of them have been assigned with CVEs.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "146",
        "numpages": "12",
        "keywords": "DBMS fuzzing, initial seeds, vulnerability detection",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639117",
        "author": "Sun, Yuqiang and Wu, Daoyuan and Xue, Yue and Liu, Han and Wang, Haijun and Xu, Zhengzi and Xie, Xiaofei and Liu, Yang",
        "title": "GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639117",
        "doi": "10.1145/3597503.3639117",
        "abstract": "Smart contracts are prone to various vulnerabilities, leading to substantial financial losses over time. Current analysis tools mainly target vulnerabilities with fixed control- or data-flow patterns, such as re-entrancy and integer overflow. However, a recent study on Web3 security bugs revealed that about 80\\% of these bugs cannot be audited by existing tools due to the lack of domain-specific property description and checking. Given recent advances in Large Language Models (LLMs), it is worth exploring how Generative Pre-training Transformer (GPT) could aid in detecting logic vulnerabilities.In this paper, we propose GPTScan, the first tool combining GPT with static analysis for smart contract logic vulnerability detection. Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge, we utilize GPT as a versatile code understanding tool. By breaking down each logic vulnerability type into scenarios and properties, GPTScan matches candidate vulnerabilities with GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation. Evaluation on diverse datasets with around 400 contract projects and 3K Solidity files shows that GPTScan achieves high precision (over 90\\%) for token contracts and acceptable precision (57.14\\%) for large projects like Web3Bugs. It effectively detects ground-truth logic vulnerabilities with a recall of over 70\\%, including 9 new vulnerabilities missed by human auditors. GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01 USD to scan per thousand lines of Solidity code. Moreover, static confirmation helps GPTScan reduce two-thirds of false positives.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "166",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Where is it? Tracing the Vulnerability-relevant Files from Vulnerability Reports": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639202",
        "author": "Sun, Jiamou and Chen, Jieshan and Xing, Zhenchang and Lu, Qinghua and Xu, Xiwei and Zhu, Liming",
        "title": "Where is it? Tracing the Vulnerability-relevant Files from Vulnerability Reports",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639202",
        "doi": "10.1145/3597503.3639202",
        "abstract": "With the widely usage of open-source software, supply-chain-based vulnerability attacks, including SolarWind and Log4Shell, have posed significant risks to software security. Currently, people rely on vulnerability advisory databases or commercial software bill of materials (SBOM) to defend against potential risks. Unfortunately, these datasets do not provide finer-grained file-level vulnerability information, compromising their effectiveness. Previous works have not adequately addressed this issue, and mainstream vulnerability detection methods have their drawbacks that hinder resolving this gap. Driven by the real needs, we propose a framework that can trace the vulnerability-relevant file for each disclosed vulnerability. Our approach uses NVD descriptions with metadata as the inputs, and employs a series of strategies with a LLM model, search engine, heuristic-based text matching method and a deep learning classifier to recommend the most likely vulnerability-relevant file, effectively enhancing the completeness of existing NVD data. Our experiments confirm that the efficiency of the proposed framework, with CodeBERT achieving 0.92 AUC and 0.85 MAP, and our user study proves our approach can help with vulnerability-relevant file detection effectively. To the best of our knowledge, our work is the first one focusing on tracing vulnerability-relevant files, laying the groundwork of building finer-grained vulnerability-aware software bill of materials.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "200",
        "numpages": "13",
        "keywords": "vulnerability-relevant file, security, software supply chain",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Shedding Light on Software Engineering-specific Metaphors and Idioms": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639585",
        "author": "Imran, Mia Mohammad and Chatterjee, Preetha and Damevski, Kostadin",
        "title": "Shedding Light on Software Engineering-specific Metaphors and Idioms",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639585",
        "doi": "10.1145/3597503.3639585",
        "abstract": "Use of figurative language, such as metaphors and idioms, is common in our daily-life communications, and it can also be found in Software Engineering (SE) channels, such as comments on GitHub. Automatically interpreting figurative language is a challenging task, even with modern Large Language Models (LLMs), as it often involves subtle nuances. This is particularly true in the SE domain, where figurative language is frequently used to convey technical concepts, often bearing developer affect (e.g., 'spaghetti code). Surprisingly, there is a lack of studies on how figurative language in SE communications impacts the performance of automatic tools that focus on understanding developer communications, e.g., bug prioritization, incivility detection. Furthermore, it is an open question to what extent state-of-the-art LLMs interpret figurative expressions in domain-specific communication such as software engineering. To address this gap, we study the prevalence and impact of figurative language in SE communication channels. This study contributes to understanding the role of figurative language in SE, the potential of LLMs in interpreting them, and its impact on automated SE communication analysis. Our results demonstrate the effectiveness of fine-tuning LLMs with figurative language in SE and its potential impact on automated tasks that involve affect. We found that, among three state-of-the-art LLMs, the best improved fine-tuned versions have an average improvement of 6.66\\% on a GitHub emotion classification dataset, 7.07\\% on a GitHub incivility classification dataset, and 3.71\\% on a Bugzilla bug report prioritization dataset.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "207",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639183",
        "author": "Ahmed, Toufique and Pai, Kunal Suresh and Devanbu, Premkumar and Barr, Earl",
        "title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639183",
        "doi": "10.1145/3597503.3639183",
        "abstract": "Large Language Models (LLM) are a new class of computation engines, \"programmed\" via prompt engineering. Researchers are still learning how to best \"program\" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of \"code analysis\" and extracting such information, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps.Prior work shows that LLM performance on code summarization benefits from embedding a few code \\&amp; summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization.We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU1. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs' line completion performance.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "220",
        "numpages": "13",
        "keywords": "LLM, code summarization, program analysis, prompt engineering",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24"
    },
    "LILAC: Log Parsing using LLMs with Adaptive Parsing Cache": {
        "type": "article",
        "key": "10.1145/3643733",
        "author": "Jiang, Zhihan and Liu, Jinyang and Chen, Zhuangbin and Li, Yichen and Huang, Junjie and Huo, Yintong and He, Pinjia and Gu, Jiazhen and Lyu, Michael R.",
        "title": "LILAC: Log Parsing using LLMs with Adaptive Parsing Cache",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643733",
        "doi": "10.1145/3643733",
        "abstract": "Log parsing transforms log messages into structured formats, serving as the prerequisite step for various log analysis tasks. Although a variety of log parsing approaches have been proposed, their performance on complicated log data remains compromised due to the use of human-crafted rules or learning-based models with limited training data. The recent emergence of powerful large language models (LLMs) demonstrates their vast pre-trained knowledge related to code and logging, making it promising to apply LLMs for log parsing. However, their lack of specialized log parsing capabilities currently hinders their parsing accuracy. Moreover, the inherent inconsistent answers, as well as the substantial overhead, prevent the practical adoption of LLM-based log parsing.   To address these challenges, we propose LILAC, the first practical Log parsIng framework using LLMs with Adaptive parsing Cache. To facilitate accurate and robust log parsing, LILAC leverages the in-context learning (ICL) capability of the LLM by performing a hierarchical candidate sampling algorithm and selecting high-quality demonstrations. Furthermore, LILAC incorporates a novel component, an adaptive parsing cache, to store and refine the templates generated by the LLM. It helps mitigate LLM's inefficiency issue by enabling rapid retrieval of previously processed log templates. In this process, LILAC adaptively updates the templates within the parsing cache to ensure the consistency of parsed results. The extensive evaluation on public large-scale datasets shows that LILAC outperforms state-of-the-art methods by 69.5\\% in terms of the average F1 score of template accuracy. In addition, LILAC reduces the query times to LLMs by several orders of magnitude, achieving a comparable efficiency to the fastest baseline.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "7",
        "numpages": "24",
        "keywords": "large language models, log analysis, log parsing"
    },
    "DiffCoder: Enhancing Large Language Model on API Invocation via Analogical Code Exercises": {
        "type": "article",
        "key": "10.1145/3643745",
        "author": "Zan, Daoguang and Yu, Ailun and Shen, Bo and Chen, Bei and Li, Wei and Gong, Yongshun and Chen, Xiaolin and Yao, Yafen and Luo, Weihua and Guan, Bei and Liu, Yan and Wang, Yongji and Wang, Qianxiang and Cui, Lizhen",
        "title": "DiffCoder: Enhancing Large Language Model on API Invocation via Analogical Code Exercises",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643745",
        "doi": "10.1145/3643745",
        "abstract": "The task of code generation aims to generate code solutions based on given programming problems. Recently, code large language models (code LLMs) have shed new light on this task, owing to their formidable code generation capabilities. While these models are powerful, they seldom focus on further improving the accuracy of library-oriented API invocation. Nonetheless, programmers frequently invoke APIs in routine coding tasks. In this paper, we aim to enhance the proficiency of existing code LLMs regarding API invocation by mimicking analogical learning, which is a critical learning strategy for humans to learn through differences among multiple instances. Motivated by this, we propose a simple yet effective approach, namely DiffCoder, which excels in API invocation by effectively training on the differences (diffs) between analogical code exercises. To assess the API invocation capabilities of code LLMs, we conduct experiments on seven existing benchmarks that focus on mono-library API invocation. Additionally, we construct a new benchmark, namely PanNumEval, to evaluate the performance of multi-library API invocation. Extensive experiments on eight benchmarks demonstrate the impressive performance of DiffCoder. Furthermore, we develop a VSCode plugin for DiffCoder, and the results from twelve invited participants further verify the practicality of DiffCoder.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "19",
        "numpages": "21",
        "keywords": "Code Generation, Code Library, Instruction Tuning, Large Language Model"
    },
    "Natural Is the Best: Model-Agnostic Code Simplification for Pre-trained Large Language Models": {
        "type": "article",
        "key": "10.1145/3643753",
        "author": "Wang, Yan and Li, Xiaoning and Nguyen, Tien N. and Wang, Shaohua and Ni, Chao and Ding, Ling",
        "title": "Natural Is the Best: Model-Agnostic Code Simplification for Pre-trained Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643753",
        "doi": "10.1145/3643753",
        "abstract": "Pre-trained Large Language Models (LLM) have achieved remarkable successes in several domains. However, code-oriented LLMs are often heavy in computational complexity, and quadratically with the length of the input code sequence. Toward simplifying the input program of an LLM, the state-of-the-art approach has the strategies to filter the input code tokens based on the attention scores given by the LLM. The decision to simplify the input program should not rely on the attention patterns of an LLM, as these patterns are influenced by both the model architecture and the pre-training dataset. Since the model and dataset are part of the solution domain, not the problem domain where the input program belongs, the outcome may differ when the model is pre-trained on a different dataset. We propose SlimCode, a model-agnostic code simplification solution for LLMs that depends on the nature of input code tokens. As an empirical study on the LLMs including CodeBERT, CodeT5, and GPT-4 for two main tasks: code search and summarization, we reported that 1) the removal ratio of code has a linear-like relation with the saving ratio on training time, 2) the impact of categorized tokens on code simplification can vary significantly, 3) the impact of categorized tokens on code simplification is task-specific but model-agnostic, and 4) the above findings hold for the paradigm\u2013prompt engineering and interactive in-context learning. The empirical results showed that SlimCode can improve the state-of-the-art technique by 9.46\\% and 5.15\\% in terms of MRR and BLEU score on code search and summarization, respectively. More importantly, SlimCode is 133 times faster than the state-of-the-art approach. Additionally, SlimCode can reduce the cost of invoking GPT-4 by up to 24\\% per API query, while still producing comparable results to those with the original code. With this result, we call for a new direction on code-based, model-agnostic code simplification solutions to further empower LLMs.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "27",
        "numpages": "23",
        "keywords": "AI4SE, Code Simplification, Machine Learning, Neural Networks, Pre-trained Large Language Models"
    },
    "Go Static: Contextualized Logging Statement Generation": {
        "type": "article",
        "key": "10.1145/3643754",
        "author": "Li, Yichen and Huo, Yintong and Zhong, Renyi and Jiang, Zhihan and Liu, Jinyang and Huang, Junjie and Gu, Jiazhen and He, Pinjia and Lyu, Michael R.",
        "title": "Go Static: Contextualized Logging Statement Generation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643754",
        "doi": "10.1145/3643754",
        "abstract": "Logging practices have been extensively investigated to assist developers in writing appropriate logging statements for documenting software behaviors. Although numerous automatic logging approaches have been proposed, their performance remains unsatisfactory due to the constraint of the single-method input, without informative programming context outside the method. Specifically, we identify three inherent limitations with single-method context: limited static scope of logging statements, inconsistent logging styles, and missing type information of logging variables.                                To tackle these limitations, we propose SCLogger, the first contextualized logging statement generation approach with inter-method static contexts.First, SCLogger extracts inter-method contexts with static analysis to construct the contextualized prompt for language models to generate a tentative logging statement. The contextualized prompt consists of an extended static scope and sampled similar methods, ordered by the chain-of-thought (COT) strategy. Second, SCLogger refines the access of logging variables by formulating a new refinement prompt for language models, which incorporates detailed type information of variables in the tentative logging statement.                                The evaluation results show that SCLogger surpasses the state-of-the-art approach by 8.7\\% in logging position accuracy, 32.1\\% in level accuracy, 19.6\\% in variable precision, and 138.4\\% in text BLEU-4 score. Furthermore, SCLogger consistently boosts the performance of logging statement generation across a range of large language models, thereby showcasing the generalizability of this approach.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "28",
        "numpages": "22",
        "keywords": "code generation, large language models, software maintenance"
    },
    "Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example": {
        "type": "article",
        "key": "10.1145/3643755",
        "author": "Dilhara, Malinda and Bellur, Abhiram and Bryksin, Timofey and Dig, Danny",
        "title": "Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643755",
        "doi": "10.1145/3643755",
        "abstract": "Software developers often repeat the same code changes within a project or across different projects. These repetitive changes are known as \u201ccode change patterns\u201d (CPATs). Automating CPATs is crucial to expedite the software development process. While current Transformation by Example (TBE) techniques can automate CPATs, they are limited by the quality and quantity of the provided input examples. Thus, they miss transforming code variations that do not have the exact syntax, data-, or control-flow of the provided input examples, despite being semantically similar. Large Language Models (LLMs), pre-trained on extensive source code datasets, offer a potential solution. Harnessing the capability of LLMs to generate semantically equivalent, yet previously unseen variants of the original CPAT could significantly increase the effectiveness of TBE systems.                In this paper, we first discover best practices for harnessing LLMs to generate code variants that meet three criteria: correctness (semantic equivalence to the original CPAT), usefulness (reflecting what developers typically write), and applicability (aligning with the primary intent of the original CPAT). We then implement these practices in our tool PyCraft, which synergistically combines static code analysis, dynamic analysis, and LLM capabilities. By employing chain-of-thought reasoning, PyCraft generates variations of input examples and comprehensive test cases that identify correct variations with an F-measure of 96.6\\%. Our algorithm uses feedback iteration to expand the original input examples by an average factor of 58x. Using these richly generated examples, we inferred transformation rules and then automated these changes, resulting in an increase of up to 39x, with an average increase of 14x in target codes compared to a previous state-of-the-art tool that relies solely on static analysis. We submitted patches generated by PyCraft to a range of projects, notably esteemed ones like microsoft/DeepSpeed and IBM/inFairness. Their developers accepted and merged 83\\% the 86 CPAT instances submitted through 44 pull requests. This confirms the usefulness of these changes.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "29",
        "numpages": "23",
        "keywords": "Automation, Code Changes, Code Clone, Generative AI, Large Language Models, Machine Learning, Program by Example, Python, Test Case Generation, Transformation by Example"
    },
    "CodePlan: Repository-Level Coding using LLMs and Planning": {
        "type": "article",
        "key": "10.1145/3643757",
        "author": "Bairi, Ramakrishna and Sonwane, Atharv and Kanade, Aditya and C., Vageesh D. and Iyer, Arun and Parthasarathy, Suresh and Rajamani, Sriram and Ashok, B. and Shet, Shashank",
        "title": "CodePlan: Repository-Level Coding using LLMs and Planning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643757",
        "doi": "10.1145/3643757",
        "abstract": "Software engineering activities such as package migration, fixing error reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code.     We formulate these activities as repository-level coding tasks.         Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems.     Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt.     We frame repository-level coding as a planning problem and present a task-agnostic, neuro-symbolic framework called CodePlan to solve it.     CodePlan synthesizes a multi-step chain-of-edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions.     CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm (symbolic components) with the neural LLMs.         We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2\u201397 files).     Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines.     CodePlan is able to get 5/7 repositories to pass the validity checks (i.e., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them.     We provide our (non-proprietary) data, evaluation scripts and supplementary material at https://github.com/microsoft/codeplan.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "31",
        "numpages": "24",
        "keywords": "Automated coding, LLMs, chain of edits, neuro-symbolic AI, plan, repositories, static analysis"
    },
    "Only diff Is Not Enough: Generating Commit Messages Leveraging Reasoning and Action of Large Language Model": {
        "type": "article",
        "key": "10.1145/3643760",
        "author": "Li, Jiawei and Farag\\'{o}, David and Petrov, Christian and Ahmed, Iftekhar",
        "title": "Only diff Is Not Enough: Generating Commit Messages Leveraging Reasoning and Action of Large Language Model",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643760",
        "doi": "10.1145/3643760",
        "abstract": "Commit messages play a vital role in software development and maintenance. While previous research has introduced various Commit Message Generation (CMG) approaches, they often suffer from a lack of consideration for the broader software context associated with code changes. This limitation resulted in generated commit messages that contained insufficient information and were poorly readable. To address these shortcomings, we approached CMG as a knowledge-intensive reasoning task. We employed ReAct prompting with a cutting-edge Large Language Model (LLM) to generate high-quality commit messages. Our tool retrieves a wide range of software context information, enabling the LLM to create commit messages that are factually grounded and comprehensive. Additionally, we gathered commit message quality expectations from software practitioners, incorporating them into our approach to further enhance message quality. Human evaluation demonstrates the overall effectiveness of our CMG approach, which we named Omniscient Message Generator (OMG). It achieved an average improvement of 30.2\\% over human-written messages and a 71.6\\% improvement over state-of-the-art CMG methods.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "34",
        "numpages": "22",
        "keywords": "Commit message generation, large language model"
    },
    "CORE: Resolving Code Quality Issues using LLMs": {
        "type": "article",
        "key": "10.1145/3643762",
        "author": "Wadhwa, Nalin and Pradhan, Jui and Sonwane, Atharv and Sahu, Surya Prakash and Natarajan, Nagarajan and Kanade, Aditya and Parthasarathy, Suresh and Rajamani, Sriram",
        "title": "CORE: Resolving Code Quality Issues using LLMs",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643762",
        "doi": "10.1145/3643762",
        "abstract": "As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues.    We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The proposer LLM of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, the LLM may introduce subtle, unintended functionality changes which may go un-detected by the static analysis. The ranker LLM evaluates the changes made by the proposer using a rubric that closely follows the acceptance criteria that a developer would enforce. CORE uses the scores assigned by the ranker LLM to rank the candidate revisions before presenting them to the developer.    We conduct a variety of experiments on two public benchmarks to show the ability of CORE:  (1) to generate code revisions acceptable to both static analysis tools and human reviewers (the latter evaluated with user study on a subset of the Python benchmark),  (2) to reduce human review efforts by detecting and eliminating revisions with unintended changes,  (3) to readily work across multiple languages (Python and Java), static analysis tools (CodeQL and SonarQube) and quality checks (52 and 10 checks, respectively),  and  (4) to achieve fix rate comparable to a rule-based automated program repair tool but with much smaller engineering efforts (on the Java benchmark).  CORE could revise 59.2\\% Python files (across 52 quality checks) so that they pass scrutiny by both a tool and a human reviewer. The ranker LLM reduced false positives by 25.8\\% in these cases. CORE produced revisions that passed the static analysis tool in 76.8\\% Java files (across 10 quality checks) comparable to 78.3\\% of a specialized program repair tool, with significantly much less engineering efforts. We release code, data, and supplementary material publicly at http://aka.ms/COREMSRI.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "36",
        "numpages": "23",
        "keywords": "Code quality, LLMs, code revision, static analysis"
    },
    "Towards AI-Assisted Synthesis of Verified Dafny Methods": {
        "type": "article",
        "key": "10.1145/3643763",
        "author": "Misu, Md Rakib Hossain and Lopes, Cristina V. and Ma, Iris and Noble, James",
        "title": "Towards AI-Assisted Synthesis of Verified Dafny Methods",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643763",
        "doi": "10.1145/3643763",
        "abstract": "Large language models show great promise in many domains, including programming. A promise is easy to make but hard to keep, and language models often fail to keep their promises, generating erroneous code. A promising avenue to keep models honest is to incorporate formal verification: generating programs\u2019 specifications as well as code so that the code can be proved correct with respect to the specifications. Unfortunately, existing large language models show a severe lack of proficiency in verified programming.        In this paper, we demonstrate how to improve two pretrained models\u2019 proficiency in the Dafny verification-aware language. Using 178 problems from the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to synthesize Dafny methods. We use three different types of prompts: a direct Contextless prompt; a Signature prompt that includes a method signature and test cases, and a Chain of Thought (CoT) prompt that decomposes the problem into steps and includes retrieval augmentation generated example problems and solutions. Our results show that GPT-4 performs better than PaLM-2 on these tasks and that both models perform best with the retrieval augmentation generated CoT prompt. GPT-4 was able to generate verified, human-evaluated, Dafny methods for 58\\% of the problems, however, GPT-4 managed only 19\\% of the problems with the Contextless prompt, and even fewer (10\\%) for the Signature prompt. We are thus able to contribute 153 verified Dafny solutions to MBPP problems, 50 that we wrote manually, and 103 synthesized by GPT-4.        Our results demonstrate that the benefits of formal program verification are now within reach of code generating large language models. Likewise, program verification systems can benefit from large language models, whether to synthesize code wholesale, to generate specifications, or to act as a \"programmer\u2019s verification apprentice\", to construct annotations such as loop invariants which are hard for programmers to write or verification tools to find. Finally, we expect that the approach we have pioneered here \u2014 generating candidate solutions that are subsequently formally checked for correctness \u2014 should transfer to other domains (e.g., legal arguments, transport signaling, structural engineering) where solutions must be correct, where that correctness must be demonstrated, explained and understood by designers and end-users.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "37",
        "numpages": "24",
        "keywords": "Dafny, LLM, Program Synthesis, Program Verification"
    },
    "Artifacts@FSE24: Towards AI-Assisted Synthesis of Verified Dafny Methods": {
        "type": "software",
        "key": "10.6084/m9.figshare.25999807.v1",
        "author": "Misu, Md Rakib Hossain and Lopes, Cristina V. and Ma, Iris and Noble, James",
        "title": "Artifacts@FSE24: Towards AI-Assisted Synthesis of Verified Dafny Methods",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.6084/m9.figshare.25999807.v1",
        "abstract": "    <p>This release presents the artifact of our work \u201cTowards AI-Assisted Synthesis of Verified Dafny Methods.\u201d This release has been reviewed in FSE24@Artifacts-Evaluation.</p><p>In this work, we conduct the first empirical study of LLMs synthesizing verifiable Dafny methods. Using 178 programming problems from the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to synthesize methods in Dafny. We demonstrate that a prompt following the principles of Chain of Thought (CoT) with semantically similar few-shot examples explaining how to decompose a problem step-by-step, can synthesize verified and correct Dafny methods with meaningful specifications for 58\\% of problems in our test dataset.</p><p>The primary purpose of this artifact is to provide a benchmark dataset MBPP-DFY-153, a collection of 153 programming problems with specifications, solutions, and tests in/for Dafny, based on the MBPP (Mostly Basic Python Programming) dataset curated by Google Research. By executing our scripts, researchers should be able to try different prompts and synthesize Dafny methods from natural language problem descriptions. Scripts are also available to run verification and tests for all verified Dafny methods.</p><p>This release has been prepared to claim three badges: \u201cAvailable\u201d, \u201cReusable\u201d and \u201cFunctional\u201d. For a thorough evaluation of our artifact, we suggest that reviewers have familiarity with Dafny, and its installation, Bash scripts, and UNIX command-line operations. Artifacts related to our paper are publicly accessible in our GitHub repository at Artifacts@FSE24-Reviewed.</p>",
        "keywords": "Dafny, LLM, Program Synthesis, Program Verification"
    },
    "COSTELLO: Contrastive Testing for Embedding-Based Large Language Model as a Service Embeddings": {
        "type": "article",
        "key": "10.1145/3643767",
        "author": "Jiang, Weipeng and Zhai, Juan and Ma, Shiqing and Zhang, Xiaoyu and Shen, Chao",
        "title": "COSTELLO: Contrastive Testing for Embedding-Based Large Language Model as a Service Embeddings",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643767",
        "doi": "10.1145/3643767",
        "abstract": "Large language models have gained significant popularity and are often provided as a service (i.e., LLMaaS).  Companies like OpenAI and Google provide online APIs of LLMs to allow downstream users to create innovative applications.  Despite its popularity, LLM safety and quality assurance is a well-recognized concern in the real world, requiring extra efforts for testing these LLMs.  Unfortunately, while end-to-end services like ChatGPT have garnered rising attention in terms of testing, the LLMaaS embeddings have comparatively received less scrutiny.  We state the importance of testing and uncovering problematic individual embeddings without considering downstream applications.  The abstraction and non-interpretability of embedded vectors, combined with the black-box inaccessibility of LLMaaS, make testing a challenging puzzle.  This paper proposes COSTELLO, a black-box approach to reveal potential defects in abstract embedding vectors from LLMaaS by contrastive testing.  Our intuition is that high-quality LLMs can adequately capture the semantic relationships of the input texts and properly represent their relationships in the high-dimensional space.  For the given interface of LLMaaS and seed inputs, COSTELLO can automatically generate test suites and output words with potential problematic embeddings.  The idea is to synthesize contrastive samples with guidance, including positive and negative samples, by mutating seed inputs.  Our synthesis guide will leverage task-specific properties to control the mutation procedure and generate samples with known partial relationships in the high-dimensional space.  Thus, we can compare the expected relationship (oracle) and embedding distance (output of LLMs) to locate potential buggy cases.  We evaluate COSTELLO on 42 open-source (encoder-based) language models and two real-world commercial LLMaaS.  Experimental results show that COSTELLO can effectively detect semantic violations, where more than 62\\% of violations on average result in erroneous behaviors (e.g., unfairness) of downstream applications.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "41",
        "numpages": "23",
        "keywords": "Contrastive Testing, Embeddings, LLMaaS"
    },
    "Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM": {
        "type": "article",
        "key": "10.1145/3643769",
        "author": "Ryan, Gabriel and Jain, Siddhartha and Shang, Mingyue and Wang, Shiqi and Ma, Xiaofei and Ramanathan, Murali Krishna and Ray, Baishakhi",
        "title": "Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643769",
        "doi": "10.1145/3643769",
        "abstract": "Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt\u2019s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26\\% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "43",
        "numpages": "21",
        "keywords": "Large Language Models, Test Generation"
    },
    "AI-Assisted Code Authoring at Scale: Fine-Tuning, Deploying, and Mixed Methods Evaluation": {
        "type": "article",
        "key": "10.1145/3643774",
        "author": "Murali, Vijayaraghavan and Maddila, Chandra and Ahmad, Imad and Bolin, Michael and Cheng, Daniel and Ghorbani, Negar and Fernandez, Renuka and Nagappan, Nachiappan and Rigby, Peter C.",
        "title": "AI-Assisted Code Authoring at Scale: Fine-Tuning, Deploying, and Mixed Methods Evaluation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643774",
        "doi": "10.1145/3643774",
        "abstract": "Generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 9 programming languages and several coding surfaces. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges.        To release a LLM model at this scale, we needed to first ensure that it is sufficiently accurate. In a random sample of 20K source code files, depending on the language, we are able to reproduce hidden lines between 40\\% and 58\\% of the time, an improvement of 1.4\\texttimes{} and 4.1\\texttimes{} over a model trained only on public data.        We gradually rolled CodeCompose out to developers. At the time of this writing, 16K developers have used it with 8\\% of their code coming directly from CodeCompose.        To triangulate our numerical findings, we conduct a thematic analysis on the feedback from 70 developers. We find that 91.5\\% of the feedback is positive, with the most common themes being discovering APIs, dealing with boilerplate code, and accelerating coding. Meta continues to integrate this feedback into CodeCompose.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "48",
        "numpages": "20",
        "keywords": "AI, Developer productivity, Neural code completion, Program synthesis"
    },
    "Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models": {
        "type": "article",
        "key": "10.1145/3643776",
        "author": "Zhang, Zejun and Xing, Zhenchang and Ren, Xiaoxue and Lu, Qinghua and Xu, Xiwei",
        "title": "Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643776",
        "doi": "10.1145/3643776",
        "abstract": "Pythonic idioms are highly valued and widely used in the Python programming community. However, many  Python users find it challenging to use Pythonic idioms. Adopting rule-based approach or LLM-only approach is not sufficient to overcome three persistent challenges of code idiomatization including code miss, wrong detection and wrong refactoring. Motivated by the determinism of rules and adaptability of LLMs, we propose a hybrid approach consisting of three modules. We not only write prompts to instruct LLMs to complete tasks, but we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks. The ARIs are Python code generated by prompting LLMs to generate code. We first construct a knowledge module with three elements including ASTscenario, ASTcomponent and Condition, and prompt LLMs to generate Python code for incorporation into an ARI library for subsequent use. After that, for any syntax-error-free Python code, we invoke ARIs from the ARI library to extract ASTcomponent from the ASTscenario, and then filter out ASTcomponent that does not meet the condition. Finally, we design prompts to instruct LLMs to abstract and idiomatize code, and then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code. Next, we conduct a comprehensive evaluation of our approach, RIdiom, and Prompt-LLM on nine established Pythonic idioms in RIdiom. Our approach exhibits superior accuracy, F1 score, and recall, while maintaining precision levels comparable to RIdiom, all of which consistently exceed or come close to 90\\% for each metric of each idiom. Lastly, we extend our evaluation to encompass four new Pythonic idioms. Our approach consistently outperforms Prompt-LLM, achieving metrics with values consistently exceeding 90\\% for accuracy, F1-score, precision, and recall.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "50",
        "numpages": "22",
        "keywords": "Code Change, Large Language Model, Pythonic Idioms"
    },
    "Can GPT-4 Replicate Empirical Software Engineering Research?": {
        "type": "article",
        "key": "10.1145/3660767",
        "author": "Liang, Jenny T. and Badea, Carmen and Bird, Christian and DeLine, Robert and Ford, Denae and Forsgren, Nicole and Zimmermann, Thomas",
        "title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660767",
        "doi": "10.1145/3660767",
        "abstract": "Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners could benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help replicate and thus democratize empirical software engineering research.  In this paper, we examine GPT-4\u2019s abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodologies, as well as their ability to plan and generate code for analysis pipelines on seven empirical software engineering papers. We perform a user study with 14 participants with software engineering research expertise, who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module specifications) from the papers. We find that GPT-4 is able to surface correct assumptions, but struggles to generate ones that apply common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering knowledge. Our findings have implications for leveraging LLMs for software engineering research as well as practitioner data scientists in software teams.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "60",
        "numpages": "24",
        "keywords": "Large language models, empirical software engineering, study replication"
    },
    "SimLLM: Calculating Semantic Similarity in Code Summaries using a Large Language Model-Based Approach": {
        "type": "article",
        "key": "10.1145/3660769",
        "author": "Jin, Xin and Lin, Zhiqiang",
        "title": "SimLLM: Calculating Semantic Similarity in Code Summaries using a Large Language Model-Based Approach",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660769",
        "doi": "10.1145/3660769",
        "abstract": "Code summaries are pivotal in software engineering, serving to improve code readability, maintainability, and collaboration. While recent advancements in Large Language Models (LLMs) have opened new avenues for automatic code summarization, existing metrics for evaluating summary quality, such as BLEU and BERTScore, have notable limitations. Specifically, these existing metrics either fail to capture the nuances of semantic meaning in summaries or are further limited in understanding domain-specific terminologies and expressions prevalent in code summaries. In this paper, we present SimLLM, a novel LLM-based approach designed to more precisely evaluate the semantic similarity of code summaries. Built upon an autoregressive LLM using a specialized pretraining task on permutated inputs and a pooling-based pairwise similarity measure, SimLLM overcomes the shortcomings of existing metrics. Our empirical evaluations demonstrate that SimLLM not only outperforms existing metrics but also shows a significantly high correlation with human ratings.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "62",
        "numpages": "24",
        "keywords": "automated code summarization, large language models, summary semantic similarity"
    },
    "SimLLM: Calculating Semantic Similarity in Code Summaries Using a Large Language Model-Based Approach": {
        "type": "software",
        "key": "10.5281/zenodo.11095396",
        "author": "Jin, Xin and Lin, Zhiqiang",
        "title": "SimLLM: Calculating Semantic Similarity in Code Summaries Using a Large Language Model-Based Approach",
        "year": "2024",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.11095396",
        "abstract": "    <p>This is the artifact for the ACM FSE\u20192024 paper: \u201cSimLLM: Calculating Semantic Similarity in Code Summaries Using a Large Language Model-Based Approach\u201d.</p><p>After downloading the zip file, please read and follow the README.md file to install and use it.</p>",
        "keywords": "automated code summarization, large language models, summary semantic similarity"
    },
    "A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization": {
        "type": "article",
        "key": "10.1145/3660771",
        "author": "Kang, Sungmin and An, Gabin and Yoo, Shin",
        "title": "A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660771",
        "doi": "10.1145/3660771",
        "abstract": "Fault Localization (FL), in which a developer seeks to identify which part of the code is malfunctioning and needs to be fixed, is a recurring challenge in debugging. To reduce developer burden, many automated FL techniques have been proposed. However, prior work has noted that existing techniques fail to provide rationales for the suggested locations, hindering developer adoption of these techniques. With this in mind, we propose AutoFL, a Large Language Model (LLM)-based FL technique that generates an explanation of the bug along with a suggested fault location. AutoFL prompts an LLM to use function calls to navigate a repository, so that it can effectively localize faults over a large software repository and overcome the limit of the LLM context length. Extensive experiments on 798 real-world bugs in Java and Python reveal AutoFL improves method-level acc@1 by up to 233.3\\% over baselines. Furthermore, developers were interviewed on their impression of AutoFL-generated explanations, showing that developers generally liked the natural language explanations of AutoFL, and that they preferred reading a few, high-quality explanations instead of many.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "64",
        "numpages": "23",
        "keywords": "debugging, fault localization, language models"
    },
    "Exploring and Unleashing the Power of Large Language Models in Automated Code Translation": {
        "type": "article",
        "key": "10.1145/3660778",
        "author": "Yang, Zhen and Liu, Fang and Yu, Zhongxing and Keung, Jacky Wai and Li, Jia and Liu, Shuo and Hong, Yifan and Ma, Xiaoxue and Jin, Zhi and Li, Ge",
        "title": "Exploring and Unleashing the Power of Large Language Models in Automated Code Translation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660778",
        "doi": "10.1145/3660778",
        "abstract": "Code translation tools, namely transpilers, are developed for automatic source-to-source translation. Latest learning-based transpilers have shown impressive enhancement against rule-based counterparts in both translation accuracy and readability, owing to their task-specific pre-training on extensive monolingual corpora. Nevertheless, their current performance still remains unsatisfactory for practical deployment, and the associated training resources are also prohibitively expensive. Large Language Models (LLMs), pre-trained on huge amounts of human-written code/text, have shown remarkable performance in many code intelligence tasks due to their powerful generality, even without task-specific re-training/fine-tuning. Thus, LLMs can potentially circumvent the above limitations, but they have not been exhaustively explored yet. This paper investigates diverse LLMs and learning-based transpilers for automated code translation tasks, finding that: although certain LLMs have outperformed current transpilers, they still have some accuracy issues, where most of the failures are induced by a lack of comprehension of source programs (38.51\\%), missing clear instructions on I/O types in translation (14.94\\%), and ignoring discrepancies between source and target programs (41.38\\%).  Enlightened by the above findings, we further propose UniTrans, a Unified code Translation framework, applicable to various LLMs, for unleashing their power in this field. Specifically, UniTrans first crafts a series of test cases for target programs with the assistance of source programs. Next, it harnesses the above auto-generated test cases to augment the code translation and then evaluate their correctness via execution. Afterward, UniTrans further (iteratively) repairs incorrectly translated programs prompted by test case execution results. Extensive experiments are conducted on six settings of translation datasets between Python, Java, and C++. Three recent LLMs of diverse sizes, including GPT-3.5 and LLaMA-13B/7B, are tested with UniTrans, and all achieve substantial improvements in terms of computational accuracy and exact match accuracy among almost all translation settings, showing the universal effectiveness of UniTrans in practice.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "71",
        "numpages": "24",
        "keywords": "Automated Code Translation, Large Language Models, Transformer"
    },
    "Evaluating and Improving ChatGPT for Unit Test Generation": {
        "type": "article",
        "key": "10.1145/3660783",
        "author": "Yuan, Zhiqiang and Liu, Mingwei and Ding, Shiji and Wang, Kaixin and Chen, Yixuan and Peng, Xin and Lou, Yiling",
        "title": "Evaluating and Improving ChatGPT for Unit Test Generation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660783",
        "doi": "10.1145/3660783",
        "abstract": "Unit testing plays an essential role in detecting bugs in functionally-discrete program units (e.g., methods). Manually writing high-quality unit tests is time-consuming and laborious. Although the traditional techniques are able to generate tests with reasonable coverage, they are shown to exhibit low readability and still cannot be directly adopted by developers in practice. Recent work has shown the large potential of large language models (LLMs) in unit test generation. By being pre-trained on a massive developer-written code corpus, the models are capable of generating more human-like and meaningful test code.                                                                                                In this work, we perform the first empirical study to evaluate the capability of ChatGPT (i.e., one of the most representative LLMs with outstanding performance in code generation and comprehension) in unit test generation. In particular, we conduct both a quantitative analysis and a user study to systematically investigate the quality of its generated tests in terms of correctness, sufficiency, readability, and usability. We find that the tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures (mostly caused by incorrect assertions); but the passing tests generated by ChatGPT almost resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved.                                                                                                Inspired by our findings above, we further propose ChatTester, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTester incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTester by generating 34.3\\% more compilable tests and 18.7\\% more tests with correct assertions than the default ChatGPT. In addition to ChatGPT, we further investigate the generalization capabilities of ChatTester by applying it to two recent open-source LLMs (i.e., CodeLLama-Instruct and CodeFuse) and our results show that ChatTester can also improve the quality of tests generated by these LLMs.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "76",
        "numpages": "24",
        "keywords": "Large language model, Test generation, Unit testing"
    },
    "Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice": {
        "type": "article",
        "key": "10.1145/3660788",
        "author": "Khojah, Ranim and Mohamad, Mazen and Leitner, Philipp and de Oliveira Neto, Francisco Gomes",
        "title": "Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660788",
        "doi": "10.1145/3660788",
        "abstract": "Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering. Currently, there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry. We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey). We find that rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms. We also propose a theoretical framework for how the (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust). We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "81",
        "numpages": "22",
        "keywords": "Chatbots, Large Language Models (LLMs), Software Development Bots"
    },
    "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?": {
        "type": "article",
        "key": "10.1145/3660791",
        "author": "Endres, Madeline and Fakhoury, Sarah and Chakraborty, Saikat and Lahiri, Shuvendu K.",
        "title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660791",
        "doi": "10.1145/3660791",
        "abstract": "Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a program\u2019s intent. However, there is typically no guarantee that a program\u2019s implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language, which makes natural language intent challenging to check programmatically. The \u201cemergent abilities\u201d of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice.     In this paper, we describe nl2postcondition, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions.   We introduce and validate metrics to measure and compare different nl2postcondition approaches, using the correctness and discriminative power of generated postconditions.   We then use qualitative and quantitative methods to assess the quality of nl2postcondition postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that  via LLMs has the potential to be helpful in practice;  generated postconditions were able to catch 64 real-world historical bugs from Defects4J.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "84",
        "numpages": "24",
        "keywords": "Formal Specifications, Large Language Models, Postconditions"
    },
    "An Empirical Study on Code Review Activity Prediction and Its Impact in Practice": {
        "type": "article",
        "key": "10.1145/3660806",
        "author": "Olewicki, Doriane and Habchi, Sarra and Adams, Bram",
        "title": "An Empirical Study on Code Review Activity Prediction and Its Impact in Practice",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660806",
        "doi": "10.1145/3660806",
        "abstract": "During code reviews, an essential step in software quality assurance, reviewers have the difficult task of understanding and evaluating code changes to validate their quality and prevent introducing faults to the codebase. This is a tedious process where the effort needed is highly dependent on the code submitted, as well as the author\u2019s and the reviewer\u2019s experience, leading to median wait times for review feedback of 15-64 hours. Through an initial user study carried with 29 experts, we found that re-ordering the files changed by a patch within the review environment has potential to improve review quality, as more comments are written (+23\\%), and participants\u2019 file-level hot-spot precision and recall increases to 53\\% (+13\\%) and 28\\% (+8\\%), respectively, compared to the alphanumeric ordering. Hence, this paper aims to help code reviewers by predicting which files in a submitted patch need to be (1) commented, (2) revised, or (3) are hot-spots (commented or revised). To predict these tasks, we evaluate two different types of text embeddings (i.e., Bag-of-Words and Large Language Models encoding) and review process features (i.e., code size-based and history-based features). Our empirical study on three open-source and two industrial datasets shows that combining the code embedding and review process features leads to better results than the state-of-the-art approach. For all tasks, F1-scores (median of 40-62\\%) are significantly better than the state-of-the-art (from +1 to +9\\%).",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "99",
        "numpages": "23",
        "keywords": "Classification, Code review automation, Large Language Models, Review process features, Text embedding"
    },
    "Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?": {
        "type": "article",
        "key": "10.1145/3660807",
        "author": "Kou, Bonan and Chen, Shengmai and Wang, Zhijie and Ma, Lei and Zhang, Tianyi",
        "title": "Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660807",
        "doi": "10.1145/3660807",
        "abstract": "Large Language Models (LLMs) have recently been widely used for code generation. Due to the complexity and opacity of LLMs, little is known about how these models generate code. We made the first attempt to bridge this knowledge gap by investigating whether LLMs attend to the same parts of a task description as human programmers during code generation. An analysis of six LLMs, including GPT-4, on two popular code generation benchmarks revealed a consistent misalignment between LLMs' and programmers' attention. We manually analyzed 211 incorrect code snippets and found five attention patterns that can be used to explain many code generation errors. Finally, a user study showed that model attention computed by a perturbation-based method is often favored by human programmers. Our findings highlight the need for human-aligned LLMs for better interpretability and programmer trust.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "100",
        "numpages": "24",
        "keywords": "Attention, Code Generation, Large Language Models"
    },
    "Mining Action Rules for Defect Reduction Planning": {
        "type": "article",
        "key": "10.1145/3660809",
        "author": "Oueslati, Khouloud and Laberge, Gabriel and Lamothe, Maxime and Khomh, Foutse",
        "title": "Mining Action Rules for Defect Reduction Planning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660809",
        "doi": "10.1145/3660809",
        "abstract": "Defect reduction planning plays a vital role in enhancing software quality and minimizing software maintenance costs. By training a black box machine learning model and \u201cexplaining\u201d its predictions, explainable AI for software engineering aims to identify the code characteristics that impact maintenance risks. However, post-hoc explanations do not always faithfully reflect what the original model computes. In this paper, we introduce CounterACT, a Counterfactual ACTion rule mining approach that can generate defect reduction plans without black-box models. By leveraging action rules, CounterACT provides a course of action that can be considered as a counterfactual explanation for the class (e.g., buggy or not buggy) assigned to a piece of code. We compare the effectiveness of CounterACT with the original action rule mining algorithm and six established defect reduction approaches on 9 software projects. Our evaluation is based on (a) overlap scores between proposed code changes and actual developer modifications; (b) improvement scores in future releases; and (c) the precision, recall, and F1-score of the plans. Our results show that, compared to competing approaches, CounterACT\u2019s explainable plans achieve higher overlap scores at the release level (median 95\\%) and commit level (median 85.97\\%), and they offer better trade-off between precision and recall (median F1-score 88.12\\%). Finally, we venture beyond planning and explore leveraging Large Language models (LLM) for generating code edits from our generated plans. Our results show that suggested LLM code edits supported by our plans are actionable and are more likely to pass relevant test cases than vanilla LLM code recommendations.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "102",
        "numpages": "23",
        "keywords": "Action rule mining, Counterfactual explanations, Defect reduction planning, Explainability, Software analytics"
    },
    "ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification": {
        "type": "article",
        "key": "10.1145/3660810",
        "author": "Mu, Fangwen and Shi, Lin and Wang, Song and Yu, Zhuohao and Zhang, Binquan and Wang, ChenXue and Liu, Shichao and Wang, Qing",
        "title": "ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660810",
        "doi": "10.1145/3660810",
        "abstract": "Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96\\% to 80.80\\% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43\\% to 69.60\\% and from 54.32\\% to 62.37\\%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "103",
        "numpages": "23",
        "keywords": "Code Generation, Large Language Model, Prompt Engineering"
    },
    "Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning": {
        "type": "article",
        "key": "10.1145/3660811",
        "author": "Mai, Yubo and Gao, Zhipeng and Hu, Xing and Bao, Lingfeng and Liu, Yu and Sun, JianLing",
        "title": "Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660811",
        "doi": "10.1145/3660811",
        "abstract": "Inspired by the great potential of Large Language Models (LLMs) for solving complex coding tasks, in this paper, we propose a novel approach, named Code2API, to automatically perform APIzation for Stack Overflow code snippets. Code2API does not require additional model training or any manual crafting rules and can be easily deployed on personal computers without relying on other external tools. Specifically, Code2API guides the LLMs through well-designed prompts to generate well-formed APIs for given code snippets. To elicit knowledge and logical reasoning from LLMs, we used chain-of-thought (CoT) reasoning and few-shot in-context learning, which can help the LLMs fully understand the APIzation task and solve it step by step in a manner similar to a developer. Our evaluations show that Code2API achieves a remarkable accuracy in identifying method parameters (65\\%) and return statements (66\\%) equivalent to human-generated ones, surpassing the current state-of-the-art approach, APIzator, by 15.0\\% and 16.5\\% respectively. Moreover, compared with APIzator, our user study demonstrates that Code2API exhibits superior performance in generating meaningful method names, even surpassing the human-level performance, and developers are more willing to use APIs generated by our approach, highlighting the applicability of our tool in practice. Finally, we successfully extend our framework to the Python dataset, achieving a comparable performance with Java, which verifies the generalizability of our tool.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "104",
        "numpages": "23",
        "keywords": "APIs, Chain-of-thought, In-context learning, Large language models, Stack Overflow"
    },
    "Named Entity Recognition Under Domain Shift via Metric Learning for Life Sciences": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-named",
        "author": "Liu, Hongyi and Wang, Qingyun and Karisani, Payam and Ji, Heng",
        "booktitle": "NAACL2024",
        "title": "Named Entity Recognition Under Domain Shift via Metric Learning for Life Sciences",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Named entity recognition is a key component of Information Extraction (IE), particularly in scientific domains such as biomedicine and chemistry, where large language models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability of transfer learning for enhancing a named entity recognition model trained in the biomedical domain (the source domain) to be used in the chemical domain (the target domain). A common practice for training such a model in a few-shot learning setting is to pretrain the model on the labeled source data, and then, to finetune it on a hand-full of labeled target examples. In our experiments, we observed that such a model is prone to mislabeling the source entities, which can often appear in the text, as the target entities. To alleviate this problem, we propose a model to transfer the knowledge from the source domain to the target domain, but, at the same time, to project the source entities and target entities into separate regions of the feature space. This diminishes the risk of mislabeling the source entities as the target entities. Our model consists of two stages: 1) entity grouping in the source domain, which incorporates knowledge from annotated events to establish relations between entities, and 2) entity discrimination in the target domain, which relies on pseudo labeling and contrastive learning to enhance discrimination between the entities in the two domains. We conduct our extensive experiments across three source and three target datasets, demonstrating that our method outperforms the baselines by up to 5% absolute value. Code, data, and resources are publicly available for research purposes: https://github.com/Lhtie/Bio-Domain-Transfer .",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.1",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries": {
        "type": "INPROCEEDINGS",
        "key": "lee-etal-2024-effective",
        "author": "Lee, Seanie and Cheng, Jianpeng and Driesen, Joris and Coca, Alexandru and Johannsen, Anders",
        "booktitle": "NAACL2024",
        "title": "Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations.A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We validate our retrieval approach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B. The experimental results show a significant improvement over relevant baselines in real few-shot DST settings.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.6",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL": {
        "type": "INPROCEEDINGS",
        "key": "shao-nakashole-2024-linearizing",
        "author": "Shao, Yutong and Nakashole, Ndapa",
        "booktitle": "NAACL2024",
        "title": "On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Structured data, prevalent in tables, databases, and knowledge graphs, poses a significant challenge in its representation. With the advent of large language models (LLMs), there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly model structure, often as a graph. Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear.This work investigates the linear handling of structured data in encoder-decoder language models, specifically T5. Our findings reveal the model\u2019s ability to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing. We also uncover insights into the model\u2019s internal mechanisms, including the ego-centric nature of structure node encodings and the potential for model compression due to modality fusion redundancy. Overall, this work sheds light on the inner workings of linearization-based methods and could potentially provide guidance for future research.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.8",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Adaptive Rank Selections for Low-Rank Approximation of Language Models": {
        "type": "INPROCEEDINGS",
        "key": "gao-etal-2024-adaptive",
        "author": "Gao, Shangqian and Hua, Ting and Hsu, Yen-Chang and Shen, Yilin and Jin, Hongxia",
        "booktitle": "NAACL2024",
        "title": "Adaptive Rank Selections for Low-Rank Approximation of Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Singular Value Decomposition (SVD) or its weighted variants has significantly progressed in compressing language models. Previous works assume the same importance for all operations and assign the same number of ranks for different layers in a language model. However, such a uniform rank selection is sub-optimal since different operations (layers) have non-uniform demand in capacity. In other words, a desired SVD strategy should allocate more ranks for important operations and vice versa. However, a globally-optimized selection of ranks for neural networks is still an open problem, and this is a non-trivial challenge since the selection is discrete. In this work, we propose a novel binary masking mechanism for optimizing the number of ranks in a differentiable framework. Our strategy uses a novel regularization to enable the masking to comply with the SVD property where the ranks have sorted singular values. The experiments examined both types of language models, encoder-only and decoder-only models, including large language models like LLaMA. Our compressed model achieves much better accuracy than previous SVD and their SOTA variants. More interestingly, our method retains significantly better accuracy with zero or limited fine-tuning, proving the substantial advantage of adaptive rank selection.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.13",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-unleashing",
        "author": "Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng",
        "booktitle": "NAACL2024",
        "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds\u2019 strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.15",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Self-Prompting Large Language Models for Zero-Shot Open-Domain QA": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-self-prompting",
        "author": "Li, Junlong and Wang, Jinyuan and Zhang, Zhuosheng and Zhao, Hai",
        "booktitle": "NAACL2024",
        "title": "Self-Prompting Large Language Models for Zero-Shot Open-Domain QA",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Open-Domain Question Answering (ODQA) aims to answer questions without explicitly providing specific background documents. This task becomes notably challenging in a zero-shot setting where no data is available to train tailored retrieval-reader models.While recent Large Language Models (LLMs) like GPT-3 have demonstrated their effectiveness in zero-shot ODQA using direct prompting methods, these methods still fall short of fully harnessing the potential of LLMs when implicitly invoked.In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge encoded in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations entirely from scratch.These generated elements are then utilized for in-context learning. Experimental results show that our method significantly surpasses previous state-of-the-art zero-shot methods on three widely-used ODQA datasets and even achieves comparable performance with various customized fine-tuned models on full training data. Our code is available at https://github.com/lockon-n/self-prompting.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.17",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "kNN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest Neighbor In-Context Learning": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-knn",
        "author": "Zhao, Wenting and Liu, Ye and Wan, Yao and Wang, Yibo and Wu, Qingyang and Deng, Zhongfen and Du, Jiangshu and Liu, Shuaiqi and Xu, Yunlong and Yu, Philip",
        "booktitle": "NAACL2024",
        "title": "kNN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest Neighbor In-Context Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Task-Oriented Parsing (TOP) enables conversational assistants to interpret user commands expressed in natural language, transforming them into structured outputs that combine elements of both natural language and intent/slot tags. Recently, Large Language Models (LLMs) have achieved impressive performance in synthesizing computer programs based on a natural-language prompt, mitigating the gap between natural language and structured programs. Our paper focuses on harnessing the capabilities of LLMs for semantic parsing tasks, addressing the following three key research questions: 1) How can LLMs be effectively utilized for semantic parsing tasks? 2) What defines an effective prompt? and 3) How can LLM overcome the length constraint and streamline prompt design by including all examples as prompts? We introduce k Nearest Neighbor In-Context Learning (kNN-ICL), which simplifies prompt engineering by allowing it to be built on top of any design strategy while providing access to all demo examples. Extensive experiments show that: 1) Simple ICL without kNN search can achieve a comparable performance with strong supervised models on the TOP tasks, and 2) kNN-ICL significantly improves the comprehension of complex requests by seamlessly integrating ICL with a nearest-neighbor approach. Notably, this enhancement is achieved without the need for additional data or specialized prompts.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.19",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "An Examination of the Compositionality of Large Generative Vision-Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ma-etal-2024-examination",
        "author": "Ma, Teli and Li, Rong and Liang, Junwei",
        "booktitle": "NAACL2024",
        "title": "An Examination of the Compositionality of Large Generative Vision-Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics ( VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a **SyntaxBias Score**, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely **S**ynt**A**ctically **DE**-biased benchmark (SADE). Our study provides an unbiased benchmark for the compositionality of GVLMs, facilitating future research in this direction. Code and dataset are available at https://github.com/TeleeMa/SADE.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.39",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Neurocache: Efficient Vector Retrieval for Long-range Language Modeling": {
        "type": "INPROCEEDINGS",
        "key": "safaya-yuret-2024-neurocache",
        "author": "Safaya, Ali and Yuret, Deniz",
        "booktitle": "NAACL2024",
        "title": "Neurocache: Efficient Vector Retrieval for Long-range Language Modeling",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper introduces Neurocache, an approach to extend the effective context size of large language models (LLMs) using an external vector cache to store its past states. Like recent vector retrieval approaches, Neurocache uses an efficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states and incorporate them into the attention process. Neurocache improves upon previous methods by (1) storing compressed states, which reduces cache size; (2) performing a single retrieval operation per token which increases inference speed; and (3) extending the retrieval window to neighboring states, which improves both language modeling and downstream task accuracy. Our experiments show the effectiveness of Neurocache both for models trained from scratch and for pre-trained models such as Llama2-7B and Mistral-7B when enhanced with the cache mechanism. We also compare Neurocache with text retrieval methods and show improvements in single-document question-answering and few-shot learning tasks. We made the source code available under: https://github.com/alisafaya/neurocache",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.50",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "das-etal-2024-mathsensei",
        "author": "Das, Debrup and Banerjee, Debopriyo and Aditya, Somak and Kulkarni, Ashish",
        "booktitle": "NAACL2024",
        "title": "MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Tool-augmented Large Language Models (TALMs) are known to enhance the skillset of large language models (LLMs), thereby, leading to their improved reasoning abilities across many tasks. While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complementary benefits offered by tools for knowledge retrieval and mathematical equation solving are open research questions. In this work, we present MathSensei, a tool-augmented large language model for mathematical reasoning. We study the complementary benefits of the tools - knowledge retriever (Bing Web Search), program generator + executor (Python), and symbolic equation solver (Wolfram-Alpha API) through evaluations on mathematical reasoning datasets. We perform exhaustive ablations on MATH, a popular dataset for evaluating mathematical reasoning on diverse mathematical disciplines. We also conduct experiments involving well-known tool planners to study the impact of tool sequencing on the model performance. MathSensei achieves 13.5% better accuracy over gpt-3.5-turbo with Chain-of-Thought on the MATH dataset. We further observe that TALMs are not as effective for simpler math word problems (in GSM-8K), and the benefit increases as the complexity and required knowledge increases (progressively over AQuA, MMLU-Math, and higher level complex questions in MATH). The code and data are available at https://github.com/Debrup-61/MathSensei.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.54",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "mEdIT: Multilingual Text Editing via Instruction Tuning": {
        "type": "INPROCEEDINGS",
        "key": "raheja-etal-2024-medit",
        "author": "Raheja, Vipul and Alikaniotis, Dimitris and Kulkarni, Vivek and Alhafni, Bashar and Kumar, Dhruv",
        "booktitle": "NAACL2024",
        "title": "mEdIT: Multilingual Text Editing via Instruction Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We introduce mEdIT, a multi-lingual extension to CoEdIT \u2013 the recent state-of-the-art text editing models for writing assistance. mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning. They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as \u201cGrammatik korrigieren\u201d (German) or \u201c\uc774 \ud14d\uc2a4 \ud2b8\ub97c \ub2e8\uc21c\ud654\u201d (Korean). We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families. We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs. We also find that mEdIT generalizes effectively to new languages over multilingual baselines. We publicly release our data, code, and trained models.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.56",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-ensuring",
        "author": "Luo, Yi and Lin, Zhenghao and Zhang, YuHao and Sun, Jiashuo and Lin, Chen and Xu, Chengjin and Su, Xiangdong and Shen, Yelong and Guo, Jian and Gong, Yeyun",
        "booktitle": "NAACL2024",
        "title": "Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, establishing a comprehensive library of guidelines and a model for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with relevant guidelines, which guide LLMs in response generation to ensure safe and high-quality outputs, thereby aligning with human values. An additional optional stage involves fine-tuning a model with well-aligned datasets generated through the process implemented in the second stage.Our method customizes guidelines to accommodate diverse inputs, thereby enhancing the fine-grainedness and comprehensiveness of the guideline library. Furthermore, it incorporates safety expertise from a safety-trained LLM through a lightweight retrieval model.We evaluate our approach on three benchmarks, demonstrating significant improvements in LLM security and quality. Notably, our fine-tuned model, Labrador, even at 13 billion parameters, outperforms GPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.65",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "E\u2075: Zero-shot Hierarchical Table Analysis using Augmented LLMs via Explain, Extract, Execute, Exhibit and Extrapolate": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-e5",
        "author": "Zhang, Zhehao and Gao, Yan and Lou, Jian-Guang",
        "booktitle": "NAACL2024",
        "title": "E\u2075: Zero-shot Hierarchical Table Analysis using Augmented LLMs via Explain, Extract, Execute, Exhibit and Extrapolate",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Analyzing large hierarchical tables with multi-level headers presents challenges due to their complex structure, implicit semantics, and calculation relationships. While recent advancements in large language models (LLMs) have shown promise in flat table analysis, their application to hierarchical tables is constrained by the reliance on manually curated exemplars and the model\u2019s token capacity limitations. Addressing these challenges, we introduce a novel code-augmented LLM-based framework, E\u2075, for zero-shot hierarchical table question answering. This approach encompasses self-explaining the table\u2019s hierarchical structures, code generation to extract relevant information and apply operations, external code execution to prevent hallucinations, and leveraging LLMs\u2019 reasoning for final answer derivation. Empirical results indicate that our method, based on GPT-4, outperforms state-of-the-art fine-tuning methods with a 44.38 Exact Match improvement. Furthermore, we present F\u00b3, an adaptive algorithm designed for token-limited scenarios, effectively condensing large tables while maintaining useful information. Our experiments prove its efficiency, enabling the processing of large tables even with models having limited context lengths. The code is available at https://github.com/zzh-SJTU/E5-Hierarchical-Table-Analysis.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.68",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-mmc",
        "author": "Liu, Fuxiao and Wang, Xiaoyang and Yao, Wenlin and Chen, Jianshu and Song, Kaiqiang and Cho, Sangwoo and Yacoob, Yaser and Yu, Dong",
        "booktitle": "NAACL2024",
        "title": "MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the rapid development of large language models (LLMs) and their integration into large multimodal models (LMMs), there has beenimpressive progress in zero-shot completion of user-oriented vision-language tasks. However, a gap remains in the domain of chartimage understanding due to the distinct abstract components in charts. To address this, we introduce a large-scale MultiModal ChartInstruction (MMC-Instruction) dataset comprising 600k instances supporting diverse tasks and chart types. Leveraging this data, we de-velop MultiModal Chart Assistant (MMCA), an LMM that achieves state-of-the-art performance on existing chart QA benchmarks. Recognizing the need for a comprehensive evaluation of LMM chart understanding, we also propose a MultiModal Chart Benchmark (MMC-Benchmark), a comprehensive human-annotated benchmark with nine distinct tasks evaluating reasoning capabilities over charts.Extensive experiments on MMC-Benchmark reveal the limitations of existing LMMs on correctly interpreting charts, even for the mostrecent GPT-4V model. Our work provides an instruction-tuning methodology and benchmark to advance multimodal understanding ofcharts. Code and data are available at https://github.com/FuxiaoLiu/MMC.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.70",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Fine-Tuning Language Models with Reward Learning on Policy": {
        "type": "INPROCEEDINGS",
        "key": "lang-etal-2024-fine",
        "author": "Lang, Hao and Huang, Fei and Li, Yongbin",
        "booktitle": "NAACL2024",
        "title": "Fine-Tuning Language Models with Reward Learning on Policy",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences.RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially.Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs\u2019 data distribution.Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize.In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution.Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples.Meanwhile, a synthetic preference generation approach is developed to simulate high-quality preference data with policy outputs.Extensive experiments on three benchmark datasets show that RLP consistently outperforms the state-of-the-art.Our code is available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.75",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference": {
        "type": "INPROCEEDINGS",
        "key": "sadat-caragea-2024-mscinli",
        "author": "Sadat, Mobashir and Caragea, Cornelia",
        "booktitle": "NAACL2024",
        "title": "MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The task of scientific Natural Language Inference (NLI) involves predicting the semantic relation between two sentences extracted from research articles. This task was recently proposed along with a new dataset called SciNLI derived from papers published in the computational linguistics domain. In this paper, we aim to introduce diversity in the scientific NLI task and present MSciNLI, a dataset containing 132,320 sentence pairs extracted from five new scientific domains. The availability of multiple domains makes it possible to study domain shift for scientific NLI. We establish strong baselines on MSciNLI by fine-tuning Pre-trained Language Models (PLMs) and prompting Large Language Models (LLMs). The highest Macro F1 scores of PLM and LLM baselines are 77.21% and 51.77%, respectively, illustrating that MSciNLI is challenging for both types of models. Furthermore, we show that domain shift degrades the performance of scientific NLI models which demonstrates the diverse characteristics of different domains in our dataset. Finally, we use both scientific NLI datasets in an intermediate task transfer learning setting and show that they can improve the performance of downstream tasks in the scientific domain. We make our dataset and code available on Github.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.90",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "End-to-End Beam Retrieval for Multi-Hop Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-end",
        "author": "Zhang, Jiahao and Zhang, Haiyang and Zhang, Dongmei and Yong, Liu and Huang, Shen",
        "booktitle": "NAACL2024",
        "title": "End-to-End Beam Retrieval for Multi-Hop Question Answering",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multi-hop question answering (QA) involves finding multiple relevant passages and step-by-step reasoning to answer complex questions, indicating a retrieve-and-read paradigm. However, previous retrievers were customized for two-hop questions, and most of them were trained separately across different hops, resulting in a lack of supervision over the entire multi-hop retrieval process and leading to poor performance in complicated scenarios beyond two hops. In this work, we introduce Beam Retrieval, an end-to-end beam retrieval framework for multi-hop QA. This approach models the multi-hop retrieval process in an end-to-end manner by jointly optimizing an encoder and two classification heads across all hops. Moreover, Beam Retrieval maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. To establish a complete QA system, we incorporate a supervised reader or a large language model (LLM). Experimental results demonstrate that Beam Retrieval achieves a nearly 50% improvement compared with baselines on challenging MuSiQue-Ans, and it also surpasses all previous retrievers on HotpotQA and achieves 99.9% precision on 2WikiMultiHopQA. Providing high-quality context, Beam Retrieval helps our supervised reader achieve new state-of-the-art performance and substantially improves the few-shot QA performance of LLMs.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.96",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction": {
        "type": "INPROCEEDINGS",
        "key": "tang-etal-2024-ungrammatical",
        "author": "Tang, Chenming and Qu, Fanyi and Wu, Yunfang",
        "booktitle": "NAACL2024",
        "title": "Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In the era of large language models (LLMs), in-context learning (ICL) stands out as an effective prompting strategy that explores LLMs\u2019 potency across various tasks. However, applying LLMs to grammatical error correction (GEC) is still a challenging task. In this paper, we propose a novel ungrammatical-syntax-based in-context example selection strategy for GEC. Specifically, we measure similarity of sentences based on their syntactic structures with diverse algorithms, and identify optimal ICL examples sharing the most similar ill-formed syntax to the test input. Additionally, we carry out a two-stage process to further improve the quality of selection results. On benchmark English GEC datasets, empirical results show that our proposed ungrammatical-syntax-based strategies outperform commonly-used word-matching or semantics-based methods with multiple LLMs. This indicates that for a syntax-oriented task like GEC, paying more attention to syntactic information can effectively boost LLMs\u2019 performance. Our code is available at https://github.com/JamyDon/SynICL4GEC.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.99",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "QualEval: Qualitative Evaluation for Model Improvement": {
        "type": "INPROCEEDINGS",
        "key": "murahari-etal-2024-qualeval",
        "author": "Murahari, Vishvak and Deshpande, Ameet and Clark, Peter and Rajpurohit, Tanmay and Sabharwal, Ashish and Narasimhan, Karthik and Kalyan, Ashwin",
        "booktitle": "NAACL2024",
        "title": "QualEval: Qualitative Evaluation for Model Improvement",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Quantitative evaluation metrics have been pivotal in gauging the advancements of AI systems like large language models (LLMs).However, due to the intricate nature of real-world tasks, a single scalar to quantify and compare performance trivializes the fine-grained nuances of model behavior. Additionally, metrics do not yield actionable diagnostics for model improvement, thus requiring extensive manual efforts of scientists, involving sifting through vast datasets and attempting hit-or-miss adjustments to training data or setups. In this work, we address the shortcomings of quantitative metrics by proposing QualEval, which uses automated qualitative evaluation as a vehicle for model improvement. QualEval uses a powerful LLM reasoner and our novel flexible linear programming solver to generate human-readable insights that when applied, accelerate model improvement. The insights are supported by a dashboard report with fine-grained visualizations and human-interpretable analyses. We corroborate the faithfulness of QualEval by demonstrating that leveraging its insights, for example, improves the absolute performance of the Llama 2 model by up to 15% points relative on a challenging dialogue task (DialogSum) when compared to baselines. QualEval successfully increases the pace and quality of model development by eliminating the need of arduous manual analysis, thus serving as a data-scientist-in-a-box.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.115",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "A Wolf in Sheep\u2019s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily": {
        "type": "INPROCEEDINGS",
        "key": "ding-etal-2024-wolf",
        "author": "Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian",
        "booktitle": "NAACL2024",
        "title": "A Wolf in Sheep\u2019s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as \u2018jailbreaks\u2019 can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, which compromises either generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze the failure of LLMs defense from the perspective of prompt execution priority, and propose corresponding defense strategies. We hope that our research can catalyze both the academic community and LLMs developers towards the provision of safer and more regulated LLMs. The code is available at https://github.com/NJUNLP/ReNeLLM.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.118",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-bridging",
        "author": "Wang, Rose and Zhang, Qingyang and Robinson, Carly and Loeb, Susanna and Demszky, Dorottya",
        "booktitle": "NAACL2024",
        "title": "Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert\u2019s latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student\u2019s error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert\u2019s decision-making model is critical for LLMs to close the gap: responses from GPT4 with expert decisions (e.g., \u201csimplify the problem\u201d) are +76% more preferred than without. Additionally, context-sensitive decisions are critical to closing pedagogical gaps: random decisions decrease GPT4\u2019s response quality by -97% than expert decisions. Our work shows the potential of embedding expert thought processes in LLM generations to enhance their capability to bridge novice-expert knowledge gaps. Our dataset and code can be found at: https://github.com/rosewang2008/bridge.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.120",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "ReTA: Recursively Thinking Ahead to Improve the Strategic Reasoning of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "duan-etal-2024-reta",
        "author": "Duan, Jinhao and Wang, Shiqi and Diffenderfer, James and Sun, Lichao and Chen, Tianlong and Kailkhura, Bhavya and Xu, Kaidi",
        "booktitle": "NAACL2024",
        "title": "ReTA: Recursively Thinking Ahead to Improve the Strategic Reasoning of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Current logical reasoning evaluations of Large Language Models (LLMs) primarily focus on single-turn and static environments, such as arithmetic problems. The crucial problem of multi-turn, strategic reasoning is under-explored. In this work, we analyze the multi-turn strategic reasoning of LLMs through text-driven complete- and incomplete-information gaming, e.g., board games (Tic-Tac-Toe, Connect-4) and poker games (Texas Hold\u2019em Poker). Specifically, we consider two distinct scenarios: 1) Online Racing, featuring multiple LLMs/agents to facilitate direct competition and comparison; 2) Offline Probing, constructing targeted questions with verified ground truth to evaluate LLMs\u2019 strategic behaviors. Experimental results demonstrate that existing state-of-the-art LLMs and reasoning schemes are largely ineffective for strategic reasoning tasks. To mitigate these limitations, we propose a simple yet effective Recursively Thinking-Ahead (ReTA) agent, incorporating a recursive prompting mechanism that automatically analyzes the opponents\u2019 future moves/actions and assigns reward signals for these situations, to strengthen the strategic reasoning of LLMs. We hope our work could spur further research and exploration in the multi-turn strategic reasoning of LLMs. The code is available at https://github.com/jinhaoduan/ReTA.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.123",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Program-Aided Reasoners (Better) Know What They Know": {
        "type": "INPROCEEDINGS",
        "key": "kabra-etal-2024-program",
        "author": "Kabra, Anubha and Rangreji, Sanketh and Mathur, Yash and Madaan, Aman and Liu, Emmy and Neubig, Graham",
        "booktitle": "NAACL2024",
        "title": "Program-Aided Reasoners (Better) Know What They Know",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to \u201cknow what they know\u201d, which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types - LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.125",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "tang-etal-2024-found",
        "author": "Tang, Raphael and Zhang, Crystina and Ma, Xueguang and Lin, Jimmy and Ture, Ferhan",
        "booktitle": "NAACL2024",
        "title": "Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) exhibit positional bias in how they use context, which especially affects listwise ranking. To address this, we propose permutation self-consistency, a form of self-consistency over the ranking list outputs of black-box LLMs. Our key idea is to marginalize out different list orders in the prompt to produce an order-independent ranking with less positional bias. First, given some input prompt, we repeatedly shuffle the list in the prompt and pass it through the LLM while holding the instructions the same. Next, we aggregate the resulting sample of rankings by computing the central ranking closest in distance to all of them, marginalizing out prompt order biases in the process. Theoretically, we prove the robustness of our method, showing convergence to the true ranking under random perturbations.Empirically, on five datasets in sorting and passage reranking, our approach improves scores from conventional inference by up to 34-52% for Mistral, 7-18% for GPT-3.5, 8-16% for LLaMA v2 (70B). Our code is at https://github.com/castorini/perm-sc.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.129",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning": {
        "type": "INPROCEEDINGS",
        "key": "wu-etal-2024-language",
        "author": "Wu, Xuansheng and Yao, Wenlin and Chen, Jianshu and Pan, Xiaoman and Wang, Xiaoyang and Liu, Ninghao and Yu, Dong",
        "booktitle": "NAACL2024",
        "title": "From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution, and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts of user prompts, and promotes the response generation constantly conditioned on the instructions. 2) It encourages the self-attention heads to capture more word-word relationships about instruction verbs. 3) It encourages the feed-forward networks to rotate their pre-trained knowledge toward user-oriented tasks. These insights contribute to a more comprehensive understanding of instruction tuning and lay the groundwork for future work that aims at explaining and optimizing LLMs for various applications. Our code and data are publicly available at https://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.130",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion": {
        "type": "INPROCEEDINGS",
        "key": "jia-etal-2024-mill",
        "author": "Jia, Pengyue and Liu, Yiding and Zhao, Xiangyu and Li, Xiaopeng and Hao, Changying and Wang, Shuaiqiang and Yin, Dawei",
        "booktitle": "NAACL2024",
        "title": "MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Query expansion, pivotal in search engines, enhances the representation of user information needs with additional terms. While existing methods expand queries using retrieved or generated contextual documents, each approach has notable limitations. Retrieval-based methods often fail to accurately capture search intent, particularly with brief or ambiguous queries. Generation-based methods, utilizing large language models (LLMs), generally lack corpus-specific knowledge and entail high fine-tuning costs. To address these gaps, we propose a novel zero-shot query expansion framework utilizing LLMs for mutual verification. Specifically, we first design a query-query-document generation method, leveraging LLMs\u2019 zero-shot reasoning ability to produce diverse sub-queries and corresponding documents. Then, a mutual verification process synergizes generated and retrieved documents for optimal expansion. Our proposed method is fully zero-shot, and extensive experiments on three public benchmark datasets are conducted to demonstrate its effectiveness over existing methods. Our code is available online at https://github.com/Applied-Machine-Learning-Lab/MILL to ease reproduction.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.138",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning": {
        "type": "INPROCEEDINGS",
        "key": "zhu-etal-2024-pad",
        "author": "Zhu, Xuekai and Qi, Biqing and Zhang, Kaiyan and Long, Xinwei and Lin, Zhouhan and Zhou, Bowen",
        "booktitle": "NAACL2024",
        "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.142",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Unlocking Emergent Modularity in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "qiu-etal-2024-unlocking",
        "author": "Qiu, Zihan and Huang, Zeyu and Fu, Jie",
        "booktitle": "NAACL2024",
        "title": "Unlocking Emergent Modularity in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Modular Neural Networks (MNNs) demonstrate various advantages over monolithic models.Existing MNNs are generally explicit: their modular architectures are pre-defined, with individual modules expected to implement distinct functions.Recent works reveal that there exists implicit modularity in standard pre-trained transformers, namely Emergent Modularity.They indicate that such modular structures spontaneously exhibit during the early pre-training phase.Despite the benefits of modularity, most Language Models (LMs) are still treated as monolithic models in the pre-train and fine-tune paradigm, with their emergent modularity locked and underutilized.In this work, focusing on unlocking the emergent modularity in LMs, we showcase that standard LMs could be fine-tuned as their Mixture-of-Expert (MoEs) counterparts without introducing any extra parameters. Such MoEs are derived from emergent modularity and are referred to as Emergent MoEs (EMoE).Our experiments demonstrate that fine-tuning EMoE effectively improves downstream in-domain and out-of-domain generalization compared with vanilla fine-tuning.Our analysis and ablation studies further illustrate that it is robust to various configurations and can scale up to Large Language Models (i.e., Llama2-7B and Llama-30B). Code is available at https://github.com/qiuzh20/EMoE.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.144",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Evaluating In-Context Learning of Libraries for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "patel-etal-2024-evaluating",
        "author": "Patel, Arkil and Reddy, Siva and Bahdanau, Dzmitry and Dasigi, Pradeep",
        "booktitle": "NAACL2024",
        "title": "Evaluating In-Context Learning of Libraries for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability. A particularly promising area is their ability to interpret code modules from unfamiliar libraries for solving user-instructed tasks. Recent work has shown that large proprietary LLMs can learn novel library usage in-context from demonstrations. These results raise several open questions: whether demonstrations of library usage is required, whether smaller (and more open) models also possess such capabilities, etc. In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context. Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context. Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions, which are often cheaper to obtain than demonstrations. Overall, our results pave the way for harnessing LLMs in more adaptable and dynamic coding environments.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.161",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Uncertainty Quantification for In-Context Learning of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ling-etal-2024-uncertainty",
        "author": "Ling, Chen and Zhao, Xujiang and Zhang, Xuchao and Cheng, Wei and Liu, Yanchi and Sun, Yiyou and Oishi, Mika and Osaki, Takao and Matsuda, Katsushi and Ji, Jie and Bai, Guangji and Zhao, Liang and Chen, Haifeng",
        "booktitle": "NAACL2024",
        "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM\u2019s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM\u2019s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model\u2019s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.184",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Fair Abstractive Summarization of Diverse Perspectives": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-fair",
        "author": "Zhang, Yusen and Zhang, Nan and Liu, Yixin and Fabbri, Alexander and Liu, Junru and Kamoi, Ryo and Lu, Xiaoxin and Xiong, Caiming and Zhao, Jieyu and Radev, Dragomir and McKeown, Kathleen and Zhang, Rui",
        "booktitle": "NAACL2024",
        "title": "Fair Abstractive Summarization of Diverse Perspectives",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "People from different social and demographic groups express diverse perspectives and conflicting opinions on a broad set of topics such as product reviews, healthcare, law, and politics. A fair summary should provide a comprehensive coverage of diverse perspectives without underrepresenting certain groups. However, current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization. In this paper, we systematically investigate fair abstractive summarization for user-generated data. We first formally define fairness in abstractive summarization as not underrepresenting perspectives of any groups of people, and we propose four reference-free automatic metrics by measuring the differences between target and source perspectives. We evaluate nine LLMs, including three GPT models, four LLaMA models, PaLM 2, and Claude, on six datasets collected from social media, online reviews, and recorded transcripts. Experiments show that both the model-generated and the human-written reference summaries suffer from low fairness. We conduct a comprehensive analysis of the common factors influencing fairness and propose three simple but effective methods to alleviate unfair summarization. Our dataset and code are available at https://github.com/psunlpgroup/FairSumm.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.187",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Event Causality Is Key to Computational Story Understanding": {
        "type": "INPROCEEDINGS",
        "key": "sun-etal-2024-event",
        "author": "Sun, Yidan and Chao, Qin and Li, Boyang",
        "booktitle": "NAACL2024",
        "title": "Event Causality Is Key to Computational Story Understanding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Cognitive science and symbolic AI research suggest that event causality provides vital information for story understanding. However, machine learning systems for story understanding rarely employ event causality, partially due to the lack of methods that reliably identify open-world causal event relations. Leveraging recent progress in large language models, we present the first method for event causality identification that leads to material improvements in computational story understanding. Our technique sets a new state of the art on the COPES dataset (Wang et al., 2023c) for causal event relation identification. Further, in the downstream story quality evaluation task, the identified causal relations lead to 3.6-16.6% relative improvement on correlation with human ratings. In the multimodal story video-text alignment task, we attain 4.1-10.9% increase on Clip Accuracy and 4.2-13.5% increase on Sentence IoU. The findings indicate substantial untapped potential for event causality in computational story understanding. The codebase is at https://github.com/insundaycathy/Event-Causality-Extraction.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.191",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-ada",
        "author": "Wang, Chonghua and Duan, Haodong and Zhang, Songyang and Lin, Dahua and Chen, Kai",
        "booktitle": "NAACL2024",
        "title": "Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs\u2019 capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models\u2019 long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs\u2019 long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.205",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Analyzing the Role of Semantic Representations in the Era of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "jin-etal-2024-analyzing",
        "author": "Jin, Zhijing and Chen, Yuen and Gonzalez Adauto, Fernando and Liu, Jiarui and Zhang, Jiayi and Michael, Julian and Sch\u00f6lkopf, Bernhard and Diab, Mona",
        "booktitle": "NAACL2024",
        "title": "Analyzing the Role of Semantic Representations in the Era of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Traditionally, natural language processing (NLP) models often use a rich set of features created by linguistic expertise, such as semantic representations. However, in the era of large language models (LLMs), more and more tasks are turned into generic, end-to-end sequence generation problems. In this paper, we investigate the question: what is the role of semantic representations in the era of LLMs? Specifically, we investigate the effect of Abstract Meaning Representation (AMR) across five diverse NLP tasks. We propose an AMR-driven chain-of-thought prompting method, which we call AMRCOT, and find that it generally hurts performance more than it helps. To investigate what AMR may have to offer on these tasks, we conduct a series of analysis experiments. We find that it is difficult to predict which input examples AMR may help or hurt on, but errors tend to arise with multi-word expressions, named entities, and in the final inference step where the LLM must connect its reasoning over the AMR to its prediction. We recommend focusing on these areas for future work in semantic representations for LLMs. Our code: https://github.com/causalNLP/amr_llm",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.209",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "On-the-fly Definition Augmentation of LLMs for Biomedical NER": {
        "type": "INPROCEEDINGS",
        "key": "munnangi-etal-2024-fly",
        "author": "Munnangi, Monica and Feldman, Sergey and Wallace, Byron and Amir, Silvio and Hope, Tom and Naik, Aakanksha",
        "booktitle": "NAACL2024",
        "title": "On-the-fly Definition Augmentation of LLMs for Biomedical NER",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Despite their general capabilities, LLMs still struggle on biomedicalNER tasks, which are difficult due to the presence of specialized terminology and lack of training data. In this work we set out to improve LLM performance on biomedical NER in limited data settings via a new knowledge augmentation approach which incorporates definitions of relevant concepts on-the-fly. During this process, to provide a test bed for knowledge augmentation, we perform a comprehensive exploration of prompting strategies. Our experiments show that definition augmentation is useful for both open source and closed LLMs.For example, it leads to a relative improvement of 15% (on average) in GPT-4 performance (F1) across all (six) of our test datasets. We conduct extensive ablations and analyses to demonstrate that our performance improvements stem from adding relevant definitional knowledge. We find that careful prompting strategies also improve LLM performance, allowing them to outperform fine-tuned language models in few-shot settings. To facilitate future research in this direction, we release our code at https://github.com/allenai/beacon.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.212",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "This Land is Your, My Land: Evaluating Geopolitical Bias in Language Models through Territorial Disputes": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-land",
        "author": "Li, Bryan and Haider, Samar and Callison-Burch, Chris",
        "booktitle": "NAACL2024",
        "title": "This Land is Your, My Land: Evaluating Geopolitical Bias in Language Models through Territorial Disputes",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Do the Spratly Islands belong to China, the Philippines, or Vietnam? A pretrained large language model (LLM) may answer differently if asked in the languages of each claimant country: Chinese, Tagalog, or Vietnamese. This contrasts with a multilingual human, who would likely answer consistently. In this paper, we show that LLMs recall certain geographical knowledge inconsistently when queried in different languages\u2014a phenomenon we term geopolitical bias. As a targeted case study, we consider territorial disputes, an inherently controversial and multilingual task. We introduce BorderLines, a dataset of territorial disputes which covers 251 territories, each associated with a set of multiple-choice questions in the languages of each claimant country (49 languages in total). We also propose a suite of evaluation metrics to precisely quantify bias and consistency in responses across different languages. We then evaluate various multilingual LLMs on our dataset and metrics to probe their internal knowledge and use the proposed metrics to discover numerous inconsistencies in how these models respond in different languages. Finally, we explore several prompt modification strategies, aiming to either amplify or mitigate geopolitical bias, which highlights how brittle LLMs are and how they tailor their responses depending on cues from the interaction context. Our code and data are available at https://github.com/manestay/borderlines.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.213",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Okay, Let\u2019s Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation": {
        "type": "INPROCEEDINGS",
        "key": "nath-etal-2024-okay",
        "author": "Nath, Abhijnan and Manafi Avari, Shadi and Chelle, Avyakta and Krishnaswamy, Nikhil",
        "booktitle": "NAACL2024",
        "title": "Okay, Let\u2019s Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In NLP, Event Coreference Resolution (ECR) is the task of connecting event clusters that refer to the same underlying real-life event, usually via neural systems. In this work, we investigate using abductive free-text rationales (FTRs) generated by modern autoregressive LLMs as distant supervision of smaller student models for cross-document coreference (CDCR) of events. We implement novel rationale-oriented event clustering and knowledge distillation methods for event coreference scoring that leverage enriched information from the FTRs for improved CDCR without additional annotation or expensive document clustering. Our model using coreference-specific knowledge distillation achieves SOTA B\u00b3 F\u2081 on the ECB+ and GVC corpora and we establish a new baseline on the AIDA Phase 1 corpus. Our code can be found at https://github.com/csu-signal/llama_cdcr.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.218",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "han-etal-2024-lm",
        "author": "Han, Chi and Wang, Qifan and Peng, Hao and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong",
        "booktitle": "NAACL2024",
        "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Today\u2019s large language models (LLMs) typically train on short text segments (e.g., \\textless4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encod- ing scientific articles, code repositories, or long dialogues. Through both theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis reveals that commonly used techniques like using a sliding-window attention pattern or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-Infinite, a simple and effective method for enhancing LLMs\u2019 capabilities of handling long contexts. LM-Infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-Infinite brings substantial efficiency improvements: it achieves 2.7\\times decoding speed up and 7.5\\times memory saving over the original model. Our code will be publicly available upon publication.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.222",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Better Zero-Shot Reasoning with Role-Play Prompting": {
        "type": "INPROCEEDINGS",
        "key": "kong-etal-2024-better",
        "author": "Kong, Aobo and Zhao, Shiwan and Chen, Hao and Li, Qicheng and Qin, Yong and Sun, Ruiqi and Zhou, Xin and Wang, Enzhi and Dong, Xiaohang",
        "booktitle": "NAACL2024",
        "title": "Better Zero-Shot Reasoning with Role-Play Prompting",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Modern large language models (LLMs) exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs\u2019 reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks. Our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, in experiments conducted using ChatGPT, accuracy on AQuA rises from 53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%. Upon further comparison with the Zero-Shot-CoT technique, which prompts the model to \u201cthink step by step\u201d, our study demonstrates that role-play prompting acts as a more effective trigger for the CoT process.This highlights its potential to augment the reasoning capabilities of LLMs. We release our code at https://github.com/NKU-HLT/Role-Play-Prompting.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.228",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "XNLIeu: a dataset for cross-lingual NLI in Basque": {
        "type": "INPROCEEDINGS",
        "key": "heredia-etal-2024-xnlieu",
        "author": "Heredia, Maite and Etxaniz, Julen and Zulaika, Muitze and Saralegi, Xabier and Barnes, Jeremy and Soroa, Aitor",
        "booktitle": "NAACL2024",
        "title": "XNLIeu: a dataset for cross-lingual NLI in Basque",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches. The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step. We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation. The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch. Our code and datasets are publicly available under open licenses.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.234",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis": {
        "type": "INPROCEEDINGS",
        "key": "lee-etal-2024-exploring-cross",
        "author": "Lee, Nayeon and Jung, Chani and Myung, Junho and Jin, Jiho and Camacho-Collados, Jose and Kim, Juho and Oh, Alice",
        "booktitle": "NAACL2024",
        "title": "Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Most hate speech datasets neglect the cultural diversity within a single language, resulting in a critical shortcoming in hate speech detection. To address this, we introduce CREHate, a CRoss-cultural English Hate speech dataset. To construct CREHate, we follow a two-step procedure: 1) cultural post collection and 2) cross-cultural annotation. We sample posts from the SBIC dataset, which predominantly represents North America, and collect posts from four geographically diverse English-speaking countries (Australia, United Kingdom, Singapore, and South Africa) using culturally hateful keywords we retrieve from our survey. Annotations are collected from the four countries plus the United States to establish representative labels for each country. Our analysis highlights statistically significant disparities across countries in hate speech annotations. Only 56.2% of the posts in CREHate achieve consensus among all countries, with the highest pairwise label difference rate of 26%. Qualitative analysis shows that label disagreement occurs mostly due to different interpretations of sarcasm and the personal bias of annotators on divisive topics. Lastly, we evaluate large language models (LLMs) under a zero-shot setting and show that current LLMs tend to show higher accuracies on Anglosphere country labels in CREHate.Our dataset and codes are available at: https://github.com/nlee0212/CREHate",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.236",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-enhancing",
        "author": "Zhao, Zheng and Monti, Emilio and Lehmann, Jens and Assem, Haytham",
        "booktitle": "NAACL2024",
        "title": "Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) tend to inadequately integrate input context during text generation, relying excessively on encoded prior knowledge in model parameters, potentially resulting in generated text with factual inconsistencies or contextually unfaithful content. LLMs utilize two primary knowledge sources: 1) prior (parametric) knowledge from pretraining, and 2) contextual (non-parametric) knowledge from input prompts. The study addresses the open question of how LLMs effectively balance these knowledge sources during the generation process, specifically in the context of open-domain question answering. To address this issue, we introduce a novel approach integrating contrastive decoding with adversarial irrelevant passages as negative samples to enhance robust context grounding during generation. Notably, our method operates at inference time without requiring further training. We conduct comprehensive experiments to demonstrate its applicability and effectiveness, providing empirical evidence showcasing its superiority over existing methodologies.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.237",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Generating Attractive and Authentic Copywriting from Customer Reviews": {
        "type": "INPROCEEDINGS",
        "key": "lin-ma-2024-generating",
        "author": "Lin, Yu-Xiang and Ma, Wei-Yun",
        "booktitle": "NAACL2024",
        "title": "Generating Attractive and Authentic Copywriting from Customer Reviews",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The goal of product copywriting is to capture the interest of potential buyers by emphasizing the features of products through text descriptions. As e-commerce platforms offer a wide range of services, it\u2019s becoming essential to dynamically adjust the styles of these auto-generated descriptions. Typical approaches to copywriting generation often rely solely on specified product attributes, which may result in dull and repetitive content. To tackle this issue, we propose to generate copywriting based on customer reviews, as they provide firsthand practical experiences with products, offering a richer source of information than just product attributes. We have developed a sequence-to-sequence framework, enhanced with reinforcement learning, to produce copywriting that is attractive, authentic, and rich in information. Our framework outperforms all existing baseline and zero-shot large language models, including LLaMA-2-chat-7B and GPT-3.5, in terms of both attractiveness and faithfulness. Furthermore, this work features the use of LLMs for aspect-based summaries collection and argument allure assessment. Experiments demonstrate the effectiveness of using LLMs for marketing domain corpus construction. The code and the dataset is publicly available at: https://github.com/YuXiangLin1234/Copywriting-Generation.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.259",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Fake Alignment: Are LLMs Really Aligned Well?": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-fake",
        "author": "Wang, Yixu and Teng, Yan and Huang, Kexin and Lyu, Chengqi and Zhang, Songyang and Zhang, Wenwei and Ma, Xingjun and Jiang, Yu-Gang and Qiao, Yu and Wang, Yingchun",
        "booktitle": "NAACL2024",
        "title": "Fake Alignment: Are LLMs Really Aligned Well?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The growing awareness of safety concerns in large language models (LLMs) has sparked considerable interest in the evaluation of safety. This study investigates an under-explored issue about the evaluation of LLMs, namely the substantial discrepancy in performance between multiple-choice questions and open-ended questions. Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization. That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests. We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics\u2014\u2014Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment and obtain corrected performance estimation. Applying FINE to 14 widely-used LLMs reveals several models with purported safety are poorly aligned in practice. Subsequently, we found that multiple-choice format data can also be used as high-quality contrast distillation-based fine-tuning data, which can strongly improve the alignment consistency of LLMs with minimal fine-tuning overhead. For data and code, see https://github.com/AIFlames/Fake-Alignment.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.263",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax": {
        "type": "INPROCEEDINGS",
        "key": "mueller-etal-2024-context",
        "author": "Mueller, Aaron and Webson, Albert and Petty, Jackson and Linzen, Tal",
        "booktitle": "NAACL2024",
        "title": "In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In-context learning (ICL) is now a common method for teaching large language models (LLMs) new tasks: given labeled examples in the input context, the LLM learns to perform the task without weight updates. Do models guided via ICL infer the underlying structure of the task defined by the context, or do they rely on superficial heuristics that only generalize to identically distributed examples? We address this question using transformations tasks and an NLI task that assess sensitivity to syntax\u2014a requirement for robust language understanding. We further investigate whether out-of-distribution generalization can be improved via chain-of-thought prompting, where the model is provided with a sequence of intermediate computation steps that illustrate how the task ought to be performed. In experiments with models from the GPT, PaLM, and Llama 2 families, we find large variance across LMs. The variance is explained more by the composition of the pre-training corpus and supervision methods than by model size; in particular, models pre-trained on code generalize better, and benefit more from chain-of-thought prompting.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.267",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Leveraging Code to Improve In-Context Learning for Semantic Parsing": {
        "type": "INPROCEEDINGS",
        "key": "bogin-etal-2024-leveraging",
        "author": "Bogin, Ben and Gupta, Shivanshu and Clark, Peter and Sabharwal, Ashish",
        "booktitle": "NAACL2024",
        "title": "Leveraging Code to Improve In-Context Learning for Semantic Parsing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In-context learning (ICL) is an appealing approach for semantic parsing due to its few-shot nature and improved generalization. However, learning to parse to rare domain-specific languages (DSLs) from just a few demonstrations is challenging, limiting the performance of even the most capable LLMs.In this work, we show how pre-existing coding abilities of LLMs can be leveraged for semantic parsing by (1) using general-purpose programming languages such as Python instead of DSLs and (2) augmenting prompts with a structured domain description that includes, e.g., the available classes and functions. We show that both these changes significantly improve accuracy across three popular datasets; combined, they lead to dramatic improvements (e.g., 7.9% to 66.5% on SMCalFlow compositional split) and can substantially improve compositional generalization, nearly closing the performance gap between easier i.i.d. and harder compositional splits. Finally, comparisons across multiple PLs and DSL variations suggest that the similarity of a target language to general-purpose code is more important than prevalence in pretraining corpora. Our findings provide an improved methodology for building semantic parsers in the modern context of ICL with LLMs.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.279",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "CASA: Causality-driven Argument Sufficiency Assessment": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-casa",
        "author": "Liu, Xiao and Feng, Yansong and Chang, Kai-Wei",
        "booktitle": "NAACL2024",
        "title": "CASA: Causality-driven Argument Sufficiency Assessment",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion.To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the definition of probability of sufficiency (PS) in the causal literature, we proposeCASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion and revise them by injecting the premise event.Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance application, and find that suggestions generated by CASA enhance the sufficiency of student-written arguments. Code and data are available at https://github.com/xxxiaol/CASA.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.296",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer": {
        "type": "INPROCEEDINGS",
        "key": "zaratiana-etal-2024-gliner",
        "author": "Zaratiana, Urchade and Tomeh, Nadi and Holat, Pierre and Charnois, Thierry",
        "booktitle": "NAACL2024",
        "title": "GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Named Entity Recognition (NER) is essential in various Natural Language Processing (NLP) applications. Traditional NER models are effective but limited to a set of predefined entity types. In contrast, Large Language Models (LLMs) can extract arbitrary entities through natural language instructions, offering greater flexibility. However, their size and cost, particularly for those accessed via APIs like ChatGPT, make them impractical in resource-limited scenarios. In this paper, we introduce a compact NER model trained to identify any type of entity. Leveraging a bidirectional transformer encoder, our model, GLiNER, facilitates parallel entity extraction, an advantage over the slow sequential token generation of LLMs. Through comprehensive testing, GLiNER demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs in zero-shot evaluations on various NER benchmarks.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.300",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Carpe diem: On the Evaluation of World Knowledge in Lifelong Language Models": {
        "type": "INPROCEEDINGS",
        "key": "kim-etal-2024-carpe",
        "author": "Kim, Yujin and Yoon, Jaehong and Ye, Seonghyeon and Bae, Sangmin and Ho, Namgyu and Hwang, Sung Ju and Yun, Se-Young",
        "booktitle": "NAACL2024",
        "title": "Carpe diem: On the Evaluation of World Knowledge in Lifelong Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The dynamic nature of knowledge in an ever-changing world presents challenges for language models trained on static data; the model in the real world often requires not only acquiring new knowledge but also overwriting outdated information into updated ones. To study the ability of language models for these time-dependent dynamics in human language, we introduce a novel task, EvolvingQA, a temporally evolving question-answering benchmark designed for training and evaluating LMs on an evolving Wikipedia database. The construction of EvolvingQA is automated with our pipeline using large language models. We uncover that existing continual learning baselines suffer from updating and removing outdated knowledge. Our analysis suggests that models fail to rectify knowledge due to small weight gradients. In addition, we elucidate that language models particularly struggle to reflect the change of numerical or temporal information. Our work aims to model the dynamic nature of real-world information, suggesting faithful evaluations of the evolution-adaptability of language models. Our data construction code and dataset files are available at https://github.com/kimyuji/EvolvingQA_benchmark.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.302",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "CMB: A Comprehensive Medical Benchmark in Chinese": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-cmb",
        "author": "Wang, Xidong and Chen, Guiming and Dingjie, Song and Zhiyi, Zhang and Chen, Zhihong and Xiao, Qingying and Chen, Junying and Jiang, Feng and Li, Jianquan and Wan, Xiang and Wang, Benyou and Li, Haizhou",
        "booktitle": "NAACL2024",
        "title": "CMB: A Comprehensive Medical Benchmark in Chinese",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in contextual incongruities to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. Our data and code are publicly available at https://github.com/FreedomIntelligence/CMB.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.343",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-contradoc",
        "author": "Li, Jierui and Raheja, Vipul and Kumar, Dhruv",
        "booktitle": "NAACL2024",
        "title": "ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In recent times, large language models (LLMs) have shown impressive performance on various document-level tasks such as document classification, summarization, and question-answering. However, research on understanding their capabilities on the task of self-contradictions in long documents has been very limited. In this work, we introduce ContraDoc, the first human-annotated dataset to study self-contradictions in long documents across multiple domains, varying document lengths, self-contradiction types, and appearance scope. We then analyze the current capabilities of four state-of-the-art open-source and commercially available LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4 performs the best and can outperform humans on this task, we find that it is still unreliable and struggles with self-contradictions that require more nuance and context. We release the dataset and all the code associated with the experiments.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.362",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers": {
        "type": "INPROCEEDINGS",
        "key": "lee-etal-2024-planrag",
        "author": "Lee, Myeonghwa and An, Seonho and Kim, Min-Soo",
        "booktitle": "NAACL2024",
        "title": "PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we conduct a study to utilize LLMs as a solution for decision making that requires complex data analysis. We define **Decision QA** as the task of answering the best decision, d_best, for a decision-making question Q, business rules R and a database D. Since there is no benchmark that can examine Decision QA, we propose Decision QA benchmark, **DQA**. It has two scenarios, Locating and Building, constructed from two video games (Europa Universalis IV and Victoria 3) that have almost the same goal as Decision QA. To address Decision QA effectively, we also propose a new RAG technique called the *iterative plan-then-retrieval augmented generation* (**PlanRAG**). Our PlanRAG-based LM generates the plan for decision making as the first step, and the retriever generates the queries for data analysis as the second step. The proposed method outperforms the state-of-the-art iterative RAG method by 15.8% in the Locating scenario and by 7.4% in the Building scenario, respectively. We release our code and benchmark at https://github.com/myeon9h/PlanRAG.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.364",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References": {
        "type": "INPROCEEDINGS",
        "key": "tang-etal-2024-metrics",
        "author": "Tang, Tianyi and Lu, Hongyuan and Jiang, Yuchen and Huang, Haoyang and Zhang, Dongdong and Zhao, Xin and Kocmi, Tom and Wei, Furu",
        "booktitle": "NAACL2024",
        "title": "Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Most research about natural language generation (NLG) relies on evaluation benchmarks with limited references for a sample, which may result in poor correlations with human judgements. The underlying reason is that one semantic meaning can actually be expressed in different forms, and the evaluation with a single or few references may not accurately reflect the quality of the model\u2019s hypotheses. To address this issue, this paper presents a simple and effective method, named **Div-Ref**, to enhance existing evaluation benchmarks by enriching the number of references. We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible. We conduct comprehensive experiments to empirically demonstrate that diversifying the expression of reference can significantly enhance the correlation between automatic evaluation and human evaluation. This idea is compatible with recent LLM-based evaluation which can similarly derive advantages from incorporating multiple references. *We strongly encourage future generation benchmarks to include more references, even if they are generated by LLMs, which is once for all.* We release all the code and data at https://github.com/RUCAIBox/Div-Ref to facilitate research.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.367",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity": {
        "type": "INPROCEEDINGS",
        "key": "jeong-etal-2024-adaptive",
        "author": "Jeong, Soyeong and Baek, Jinheon and Cho, Sukmin and Hwang, Sung Ju and Park, Jong",
        "booktitle": "NAACL2024",
        "title": "Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-RAG.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.389",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "R-Tuning: Instructing Large Language Models to Say \u2018I Don\u2019t Know\u2019": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-r",
        "author": "Zhang, Hanning and Diao, Shizhe and Lin, Yong and Fung, Yi and Lian, Qing and Wang, Xingyao and Chen, Yangyi and Ji, Heng and Zhang, Tong",
        "booktitle": "NAACL2024",
        "title": "R-Tuning: Instructing Large Language Models to Say \u2018I Don\u2019t Know\u2019",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the disparity in knowledge encompassed by pre-trained parameters compared to that of instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate R-Tuning effectively improves a model\u2019s ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty results in better calibration and an improved ability to estimate the uncertainty than uncertainty-based testing. Our code is available at https://github.com/shizhediao/R-Tuning",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.394",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Analysis of State-Level Legislative Process in Enhanced Linguistic and Nationwide Network Contexts": {
        "type": "INPROCEEDINGS",
        "key": "davoodi-goldwasser-2024-analysis",
        "author": "Davoodi, Maryam and Goldwasser, Dan",
        "booktitle": "NAACL2024",
        "title": "Analysis of State-Level Legislative Process in Enhanced Linguistic and Nationwide Network Contexts",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "State bills have a significant impact on various aspects of society, including health, education, and the economy. Consequently, it is crucial to conduct systematic research on state bills before and after they are enacted to evaluate their benefits and drawbacks, thereby guiding future decision-making. In this work, we developed the first state-level deep learning framework that (1) handles the complex and inconsistent language of policies across US states using generative large language models and (2) decodes legislators\u2019 behavior and implications of state policies by establishing a shared nationwide network, enriched with diverse contexts, such as information on interest groups influencing public policy and legislators\u2019 courage test results, which reflect their political positions.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.411",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback": {
        "type": "INPROCEEDINGS",
        "key": "wu-2024-uicoder",
        "author": "Wu, Jason and Schoop, Eldon and Leung, Alan and Barik, Titus and Bigham, Jeffrey and Nichols, Jeffrey",
        "booktitle": "NAACL2024",
        "title": "UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Many large language models (LLMs) struggle to consistently generate UI code that compiles and produces visually relevant designs. Existing approaches to improve generation rely either on expensive human feedback or distilling a proprietary model. In this paper, we explore the use of automated feedback (compilers and multi-modal models) to guide LLMs to generate high-quality UI code. Our method starts with an existing LLM and iteratively produces improved models by self-generating a large synthetic dataset using an original model, applying automated tools to aggressively filter, score, and de-duplicate the data into a refined higher quality dataset, and producing a new LLM by finetuning the original on the refined dataset.We applied our approach to several open-source LLMs and compared the resulting performance to baseline models with both automated metrics and human preferences.Our results show the resulting models outperform all other downloadable baselines and approach the performance of larger proprietary models.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.417",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "MisgenderMender: A Community-Informed Approach to Interventions for Misgendering": {
        "type": "INPROCEEDINGS",
        "key": "hossain-etal-2024-misgendermender",
        "author": "Hossain, Tamanna and Dev, Sunipa and Singh, Sameer",
        "booktitle": "NAACL2024",
        "title": "MisgenderMender: A Community-Informed Approach to Interventions for Misgendering",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering.Misgendering, the act of incorrectly addressing someone\u2019s gender, inflicts serious harm and is pervasive in everyday technologies, yet there is a notable lack of research to combat it. We are the first to address this lack of research into interventions for misgendering by conducting a survey of gender-diverse individuals in the US to understand perspectives about automated interventions for text-based misgendering. Based on survey insights on the prevalence of misgendering, desired solutions, and associated concerns, we introduce a misgendering interventions task and evaluation dataset, MisgenderMender. We define the task with two sub-tasks: (i) detecting misgendering, followed by (ii) correcting misgendering where misgendering is present, in domains where editing is appropriate. MisgenderMender comprises 3790 instances of social media content and LLM-generations about non-cisgender public figures, annotated for the presence of misgendering, with additional annotations for correcting misgendering in LLM-generated text. Using this dataset, we set initial benchmarks by evaluating existing NLP systems and highlighting challenges for future models to address. We release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendermender/",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.419",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-quantity",
        "author": "Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing",
        "booktitle": "NAACL2024",
        "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In the realm of Large Language Models (LLMs), the balance between instruction data quality and quantity is a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to identify discrepancies between a model\u2019s expected responses and its intrinsic generation capability. Through the application of IFD, cherry samples can be pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of original data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the instruction tuning of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.421",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Naive Bayes-based Context Extension for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "su-etal-2024-naive",
        "author": "Su, Jianlin and Ahmed, Murtadha and Wen, Bo and Ao, Luo and Zhu, Mingren and Liu, Yunfeng",
        "booktitle": "NAACL2024",
        "title": "Naive Bayes-based Context Extension for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown promising in-context learning abilities. However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples. In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size. Importantly, this expansion does not require fine-tuning or dependence on particular model architectures, all the while preserving linear efficiency. NBCE initially splits the context into equal-sized windows fitting the target LLM\u2019s maximum length. Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context. Finally, it employs Bayes\u2019 theorem to generate the test task. Our experimental results demonstrate that NBCE substantially enhances performance, particularly as the number of demonstration examples increases, consistently outperforming alternative methods. The NBCE code will be made publicly accessible. The code NBCE is available at: https://github.com/amurtadha/NBCE-master",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.431",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "SuperGLEBer: German Language Understanding Evaluation Benchmark": {
        "type": "INPROCEEDINGS",
        "key": "pfister-hotho-2024-supergleber",
        "author": "Pfister, Jan and Hotho, Andreas",
        "booktitle": "NAACL2024",
        "title": "SuperGLEBer: German Language Understanding Evaluation Benchmark",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We assemble a broad Natural Language Understanding benchmark suite for the German language and consequently evaluate a wide array of existing German-capable models in order to create a better understanding of the current state of German LLMs. Our benchmark consists of 29 different tasks ranging over different types such as document classification, sequence tagging, sentence similarity, and question answering, on which we evaluate 10 different German-pretrained models, thereby charting the landscape of German LLMs. In our comprehensive evaluation we find that encoder models are a good choice for most tasks, but also that the largest encoder model does not necessarily perform best for all tasks. We make our benchmark suite and a leaderboard publically available at https://supergleber.professor-x.de and encourage the community to contribute new tasks and evaluate more models on it (https://github.com/LSX-UniWue/SuperGLEBer).",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.438",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation": {
        "type": "INPROCEEDINGS",
        "key": "ruan-etal-2024-defining",
        "author": "Ruan, Jie and Wang, Wenqing and Wan, Xiaojun",
        "booktitle": "NAACL2024",
        "title": "Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Human evaluation serves as the gold standard for assessing the quality of Natural Language Generation (NLG) systems. Nevertheless, the evaluation guideline, as a pivotal element ensuring reliable and reproducible human assessment, has received limited attention. Our investigation revealed that only 29.84% of recent papers involving human evaluation at top conferences release their evaluation guidelines, with vulnerabilities identified in 77.09% of these guidelines. Unreliable evaluation guidelines can yield inaccurate assessment outcomes, potentially impeding the advancement of NLG in the right direction. To address these challenges, we take an initial step towards reliable evaluation guidelines and propose the first human evaluation guideline dataset by collecting annotations of guidelines extracted from existing papers as well as generated via Large Language Models (LLMs). We then introduce a taxonomy of eight vulnerabilities and formulate a principle for composing evaluation guidelines. Furthermore, a method for detecting guideline vulnerabilities has been explored using LLMs, and we offer a set of recommendations to enhance reliability in human evaluation. The annotated human evaluation guideline dataset and code for the vulnerability detection method are publicly available online.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.441",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "AceGPT, Localizing Large Language Models in Arabic": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-acegpt",
        "author": "Huang, Huang and Yu, Fei and Zhu, Jianqing and Sun, Xuening and Cheng, Hao and Dingjie, Song and Chen, Zhihong and Alharthi, Mosen and An, Bang and He, Juncai and Liu, Ziche and Chen, Junying and Li, Jianquan and Wang, Benyou and Zhang, Lian and Sun, Ruoyu and Wan, Xiang and Li, Haizhou and Xu, Jinchao",
        "booktitle": "NAACL2024",
        "title": "AceGPT, Localizing Large Language Models in Arabic",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper is devoted to the development of a localized Large Language Model (LLM) specifically for Arabic, a language imbued with unique cultural characteristics inadequately addressed by current mainstream models. Significant concerns emerge when addressing cultural sensitivity and local values. To address this, the paper proposes a comprehensive solution that includes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT) utilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside Reinforcement Learning with AI Feedback (RLAIF) employing a reward model attuned to local culture and values. The goal is to cultivate culturally cognizant and value-aligned Arabic LLMs capable of accommodating the diverse, application-specific needs of Arabic-speaking communities. Comprehensive evaluations reveal that the resulting model, dubbed \u2018AceGPT\u2019, sets the state-of-the-art standard for open Arabic LLMs across various benchmarks. Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.450",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation": {
        "type": "INPROCEEDINGS",
        "key": "saha-etal-2024-branch",
        "author": "Saha, Swarnadeep and Levy, Omer and Celikyilmaz, Asli and Bansal, Mohit and Weston, Jason and Li, Xian",
        "booktitle": "NAACL2024",
        "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria. However, their performance can fall short, due to the model\u2019s lack of coherence and inability to plan and decompose the problem. We propose Branch-Solve-Merge (BSM), a Large Language Model program (Schlag et al., 2023) for tackling such challenging natural language tasks. It consists of branch, solve, and merge modules that are parameterized with specific prompts to the base LLM. These three modules plan a decomposition of the task into multiple parallel sub-tasks, independently solve them, and fuse the solutions to the sub-tasks. We apply our method to the tasks of LLM response evaluation and constrained text generation and evaluate its effectiveness with multiple LLMs, including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to 26%, reducing length and pairwise position biases by up to 50%, and allowing LLaMA-2-chat to match or outperform GPT-4 on most domains. On a constraint story generation task, BSM improves the coherence of stories while also improving constraint satisfaction by 12%.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.462",
        "doi": "",
        "ISSN": "",
        "month": "June"
    },
    "Table Question Answering for Low-resourced Indic Languages": {
        "type": "INPROCEEDINGS",
        "key": "pal-etal-2024-table",
        "author": "Pal, Vaishali and Kanoulas, Evangelos and Yates, Andrew and de Rijke, Maarten",
        "booktitle": "EMNLP-main2024",
        "title": "Table Question Answering for Low-resourced Indic Languages",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "TableQA is the task of answering questions over tables of structured information, returning individual cells or tables as output. TableQA research has focused primarily on high-resource languages, leaving medium- and low-resource languages with little progress due to scarcity of annotated data and neural models. We address this gap by introducing a fully automatic large-scale tableQA data generation process for low-resource languages with limited budget. We incorporate our data generation method on two Indic languages, Bengali and Hindi, which have no tableQA datasets or models. TableQA models trained on our large-scale datasets outperform state-of-the-art LLMs. We further study the trained models on different aspects, including mathematical reasoning capabilities and zero-shot cross-lingual transfer. Our work is the first on low-resource tableQA focusing on scalable data generation and evaluation procedures. Our proposed data generation method can be applied to any low-resource language with a web presence. We release datasets, models, and code (https://github.com/kolk/Low-Resource-TableQA-Indic-languages).",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.5",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning": {
        "type": "INPROCEEDINGS",
        "key": "ye-etal-2024-rotbench",
        "author": "Ye, Junjie and Wu, Yilong and Gao, Songyang and Huang, Caishuang and Li, Sixian and Li, Guanyu and Fan, Xiaoran and Zhang, Qi and Gui, Tao and Huang, Xuanjing",
        "booktitle": "EMNLP-main2024",
        "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs\u2019 capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce *RoTBench*, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model\u2019s resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.19",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation": {
        "type": "INPROCEEDINGS",
        "key": "salim-etal-2024-impeding",
        "author": "Salim, Saiful Islam and Yang, Rubin Yuchan and Cooper, Alexander and Ray, Suryashree and Debray, Saumya and Rahaman, Sazzadur",
        "booktitle": "EMNLP-main2024",
        "title": "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at measuring the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77%, ii) the drop in correctness caused by these perturbations was affected based on their detectability.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.27",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Mitigating the Alignment Tax of RLHF": {
        "type": "INPROCEEDINGS",
        "key": "lin-etal-2024-mitigating",
        "author": "Lin, Yong and Lin, Hangyu and Xiong, Wei and Diao, Shizhe and Liu, Jianmeng and Zhang, Jipeng and Pan, Rui and Wang, Haoxiang and Hu, Wenbin and Zhang, Hanning and Dong, Hanze and Pi, Renjie and Zhao, Han and Jiang, Nan and Ji, Heng and Yao, Yuan and Zhang, Tong",
        "booktitle": "EMNLP-main2024",
        "title": "Mitigating the Alignment Tax of RLHF",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "LLMs acquire a wide range of abilities during pre-training, but aligning LLMs under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting pretrained abilities, which is also known as the alignment tax. To investigate alignment tax, we conducted experiments with existing RLHF algorithms using OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. Whereas, despite various techniques to mitigate forgetting, they are often at odds with the RLHF performance, leading to a trade-off between alignment performance and forgetting mitigation, leading to an alignment-forgetting trade-off. In this paper we show that model averaging, which simply interpolates between pre and post RLHF model weights, surprisingly achieves the most strongest alignment-forgetting Pareto front among a wide range of competing methods. To understand its effectiveness, we offer theoretical insights into model averaging, revealing that it enhances performance Pareto front by increasing feature diversity on the layers where tasks share overlapped feature spaces. Empirical evidence corroborates our analysis by showing the benefits of averaging low-level transformer layers. Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different alignment-forgetting trade-offs, we propose Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination ratios of model layers. HMA seeks to maximize the alignment performance while incurring minimal alignment tax. Moreover, we validate HMA\u2019s performance across a range of RLHF algorithms over OpenLLaMA-3B and further extend our findings to Mistral-7B which is evaluated by open-sourced preference model and GPT4. Code available here.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.35",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks": {
        "type": "INPROCEEDINGS",
        "key": "wu-etal-2024-parameter",
        "author": "Wu, Haoyuan and Zheng, Haisheng and He, Zhuolun and Yu, Bei",
        "booktitle": "EMNLP-main2024",
        "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across general tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce parameter-efficient sparsity crafting (PESC), which crafts dense models into sparse models using the mixture-of-experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal parameter increase when guaranteeing the quality of approximation in function space compared to original sparse upcycling. Our empirical evaluation demonstrates the effectiveness of the PESC method. Using PESC during instruction tuning, our best sparse model outperforms other sparse and dense models and exhibits superior general capabilities compared to GPT-3.5.Our code is available at https://github.com/wuhy68/Parameter-Efficient-MoE.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.43",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence": {
        "type": "INPROCEEDINGS",
        "key": "lu-etal-2024-eliminating",
        "author": "Lu, Junru and Li, Jiazheng and An, Siyu and Zhao, Meng and He, Yulan and Yin, Di and Sun, Xing",
        "booktitle": "EMNLP-main2024",
        "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for the direct and robust alignment of Large Language Models (LLMs) with human preferences, offering a more straightforward alternative to the complex Reinforcement Learning from Human Feedback (RLHF). Despite its promising efficacy, DPO faces a notable drawback: \u201cverbosity\u201d, a common over-optimization phenomenon also observed in RLHF. While previous studies mainly attributed verbosity to biased labels within the data, we propose that the issue also stems from an inherent algorithmic length reliance in DPO. Specifically, we suggest that the discrepancy between sequence-level Kullback\u2013Leibler (KL) divergences between chosen and rejected sequences, used in DPO, results in overestimated or underestimated rewards due to varying token lengths. Empirically, we utilize datasets with different label lengths to demonstrate the presence of biased rewards. We then introduce an effective downsampling approach, named SamPO, to eliminate potential length reliance. Our experimental evaluations, conducted across three LLMs of varying scales and a diverse array of conditional and open-ended benchmarks, highlight the efficacy of SamPO in mitigating verbosity, achieving improvements of 5% to 12% over DPO through debaised rewards. Our code can be accessed at: https://github.com/LuJunru/SamPO/.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.60",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-cryptotrade",
        "author": "Li, Yuan and Luo, Bingqiao and Wang, Qian and Chen, Nuo and Liu, Xu and He, Bingsheng",
        "booktitle": "EMNLP-main2024",
        "title": "CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The utilization of Large Language Models (LLMs) in financial trading has primarily been concentrated within the stock market, aiding in economic and financial decisions. Yet, the unique opportunities presented by the cryptocurrency market, noted for its on-chain data\u2019s transparency and the critical influence of off-chain signals like news, remain largely untapped by LLMs. This work aims to bridge the gap by developing an LLM-based trading agent, CryptoTrade, which uniquely combines the analysis of on-chain and off-chain data. This approach leverages the transparency and immutability of on-chain data, as well as the timeliness and influence of off-chain signals, providing a comprehensive overview of the cryptocurrency market. CryptoTrade incorporates a reflective mechanism specifically engineered to refine its daily trading decisions by analyzing the outcomes of prior trading decisions. This research makes two significant contributions. Firstly, it broadens the applicability of LLMs to the domain of cryptocurrency trading. Secondly, it establishes a benchmark for cryptocurrency trading strategies. Through extensive experiments, CryptoTrade has demonstrated superior performance in maximizing returns compared to time-series baselines, but not compared to traditional trading signals, across various cryptocurrencies and market conditions. Our code and data are available at https://github.com/Xtra-Computing/CryptoTrade",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.63",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-amr",
        "author": "Luo, Ziyang and Li, Xin and Lin, Hongzhan and Ma, Jing and Bing, Lidong",
        "booktitle": "EMNLP-main2024",
        "title": "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The impressive performance of proprietary LLMs like GPT4 in code generation has led to a trend to replicate these capabilities in open-source models through knowledge distillation (e.g. Code Evol-Instruct). However, these efforts often neglect the crucial aspect of response quality, relying heavily on teacher models for direct response distillation. This paradigm, especially for complex instructions, can degrade the quality of synthesized data, compromising the knowledge distillation process. To this end, our study introduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which employs a two-stage process to refine response distillation. The first stage, modular decomposition, breaks down the direct response into more manageable sub-modules. The second stage, adaptive response evolution, automatically evolves the response with the related function modules. Our experiments with three popular code benchmarks\u2014HumanEval, MBPP, and EvalPlus\u2014attests to the superiority of the AMR-Evol framework over baseline response distillation methods. By comparing with the open-source Code LLMs trained on a similar scale of data, we observed performance enhancements: more than +3.0 points on HumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the effectiveness of our framework. Our codes are available at https://github.com/ChiYeungLaw/AMR-Evol.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.66",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "xing-etal-2024-efuf",
        "author": "Xing, Shangyu and Zhao, Fei and Wu, Zhen and An, Tuo and Chen, Weihao and Li, Chunhui and Zhang, Jianbing and Dai, Xinyu",
        "booktitle": "EMNLP-main2024",
        "title": "EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we propose an efficient fine-grained unlearning framework (EFUF), which performs gradient ascent utilizing three tailored losses to eliminate hallucinations without paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.67",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-large",
        "author": "Luo, Kun and Qin, Minghao and Liu, Zheng and Xiao, Shitao and Zhao, Jun and Liu, Kang",
        "booktitle": "EMNLP-main2024",
        "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Pre-trained language models like BERT and T5 serve as crucial backbone encoders for dense retrieval. However, these models often exhibit limited generalization capabilities and face challenges in improving in-domain accuracy. Recent research has explored using large language models (LLMs) as retrievers, achieving state-of-the-art performance across various tasks. Despite these advancements, the specific benefits of LLMs over traditional retrievers and the impact of different LLM configurations\u2014such as parameter sizes, pre-training duration, and alignment processes\u2014on retrieval tasks remain unclear. In this work, we conduct a comprehensive empirical study on a wide range of retrieval tasks, including in-domain accuracy, data efficiency, zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. We evaluate over 15 different backbone LLMs and non-LLMs. Our findings reveal that larger models and extensive pre-training consistently enhance in-domain accuracy and data efficiency. Additionally, larger models demonstrate significant potential in zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. These results underscore the advantages of LLMs as versatile and effective backbone encoders in dense retrieval, providing valuable insights for future research and development in this field.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.80",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "A New Pipeline for Knowledge Graph Reasoning Enhanced by Large Language Models Without Fine-Tuning": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-new",
        "author": "Chen, Zhongwu and Bai, Long and Li, Zixuan and Huang, Zhen and Jin, Xiaolong and Dou, Yong",
        "booktitle": "EMNLP-main2024",
        "title": "A New Pipeline for Knowledge Graph Reasoning Enhanced by Large Language Models Without Fine-Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Conventional Knowledge Graph Reasoning (KGR) models learn the embeddings of KG components over the structure of KGs, but their performances are limited when the KGs are severely incomplete. Recent LLM-enhanced KGR models input KG structural information into LLMs. However, they require fine-tuning on open-source LLMs and are not applicable to closed-source LLMs. Therefore, in this paper, to leverage the knowledge in LLMs without fine-tuning to assist and enhance conventional KGR models, we propose a new three-stage pipeline, including knowledge alignment, KG reasoning and entity reranking. Specifically, in the alignment stage, we propose three strategies to align the knowledge in LLMs to the KG schema by explicitly associating unconnected nodes with semantic relations. Based on the enriched KGs, we train structure-aware KGR models to integrate aligned knowledge to original knowledge existing in KGs. In the reranking stage, after obtaining the results of KGR models, we rerank the top-scored entities with LLMs to recall correct answers further. Experiments show our pipeline can enhance the KGR performance in both incomplete and general situations. Code and datasets are available.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.81",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Towards Tool Use Alignment of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-towards-tool",
        "author": "Chen, Zhi-Yuan and Shen, Shiqi and Shen, Guangyao and Zhi, Gong and Chen, Xu and Lin, Yankai",
        "booktitle": "EMNLP-main2024",
        "title": "Towards Tool Use Alignment of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, tool use with LLMs has become one of the primary research topics as it can help LLM generate truthful and helpful responses. Existing studies on tool use with LLMs primarily focus on enhancing the tool-calling ability of LLMs. In practice, like chat assistants, LLMs are also required to align with human values in the context of tool use. Specifically, LLMs should refuse to answer unsafe tool use relevant instructions and insecure tool responses to ensure their reliability and harmlessness. At the same time, LLMs should demonstrate autonomy in tool use to reduce the costs associated with tool calling. To tackle this issue, we first introduce the principle that LLMs should follow in tool use scenarios: H2A. The goal of H2A is to align LLMs with **helpfulness**, **harmlessness**, and **autonomy**. In addition, we propose ToolAlign, a dataset comprising instruction-tuning data and preference data to align LLMs with the H2A principle for tool use. Based on ToolAlign, we develop LLMs by supervised fine-tuning and preference learning, and experimental results demonstrate that the LLMs exhibit remarkable tool-calling capabilities, while also refusing to engage with harmful content, and displaying a high degree of autonomy in tool utilization. The code and datasets are available at: https://github.com/zhiyuanc2001/ToolAlign.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.82",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-mar",
        "author": "Zhang, Zhengxuan and Wu, Yin and Luo, Yuyu and Tang, Nan",
        "booktitle": "EMNLP-main2024",
        "title": "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A multimodal large language model MLLMs may struggle with answering visual-based (personal) entity questions (VEQA), such as \u201dwho is A?\u201d or \u201dwho is A that B is talking to?\u201d for various reasons, e.g., the absence of the name of A in the caption or the inability of MLLMs to recognize A, particularly for less common entities. Furthermore, even if the MLLMs can identify A, it may refrain from answering due to privacy concerns. In this paper, we introduce a novel method called Matching-Augmented Reasoning (MAR) to enhance VEQA. Given a collection of visual objects with captions, MAR preprocesses each object individually, identifying faces, names, and their alignments within the object. It encodes this information and stores their vector representations in vector databases. When handling VEQA, MAR retrieves matching faces and names and organizes these entities into a matching graph. MAR then derives the answer to the query by reasoning over this matching graph. Extensive experiments show that MAR significantly improves VEQA compared with the state-of-the-art methods using MLLMs.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.91",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2024-large-language-models-always",
        "author": "Yang, Zhe and Zhang, Yichang and Liu, Tianyu and Yang, Jian and Lin, Junyang and Zhou, Chang and Sui, Zhifang",
        "booktitle": "EMNLP-main2024",
        "title": "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities, but still suffer from inconsistency issues (e.g. LLMs can react differently to disturbances like rephrasing or inconsequential order change). In addition to these inconsistencies, we also observe that LLMs, while capable of solving hard problems, can paradoxically fail at easier ones. To evaluate this hard-to-easy inconsistency, we develop the ConsisEval benchmark, where each entry comprises a pair of questions with a strict order of difficulty. Furthermore, we introduce the concept of consistency score to quantitatively measure this inconsistency and analyze the potential for improvement in consistency by relative consistency score. Based on comprehensive experiments across a variety of existing models, we find: (1) GPT-4 achieves the highest consistency score of 92.2% but is still inconsistent to specific questions due to distraction by redundant information, misinterpretation of questions, etc.; (2) models with stronger capabilities typically exhibit higher consistency, but exceptions also exist; (3) hard data enhances consistency for both fine-tuning and in-context learning. Our data and code will be publicly available on GitHub.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.92",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models": {
        "type": "INPROCEEDINGS",
        "key": "tan-etal-2024-glue",
        "author": "Tan, Zhen and Zhao, Chengshuai and Moraffah, Raha and Li, Yifan and Wang, Song and Li, Jundong and Chen, Tianlong and Liu, Huan",
        "booktitle": "EMNLP-main2024",
        "title": "Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Retrieval-Augmented Generative (RAG) models enhance Large Language Models (LLMs) by integrating external knowledge bases, improving their performance in applications like fact-checking and information searching. In this paper, we demonstrate a security threat where adversaries can exploit the openness of these knowledge bases by injecting deceptive content into the retrieval database, intentionally changing the model\u2019s behavior. This threat is critical as it mirrors real-world usage scenarios where RAG systems interact with publicly accessible knowledge bases, such as web scrapings and user-contributed data pools. To be more realistic, we target a realistic setting where the adversary has no knowledge of users\u2019 queries, knowledge base data, and the LLM parameters. We demonstrate that it is possible to exploit the model successfully through crafted content uploads with access to the retriever. Our findings emphasize an urgent need for security measures in the design and deployment of RAG systems to prevent potential manipulation and ensure the integrity of machine-generated content.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.96",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "TinyChart: Efficient Chart Understanding with Program-of-Thoughts Learning and Visual Token Merging": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-tinychart",
        "author": "Zhang, Liang and Hu, Anwen and Xu, Haiyang and Yan, Ming and Xu, Yichen and Jin, Qin and Zhang, Ji and Huang, Fei",
        "booktitle": "EMNLP-main2024",
        "title": "TinyChart: Efficient Chart Understanding with Program-of-Thoughts Learning and Visual Token Merging",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Charts are important for presenting and explaining complex data relationships. Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in chart understanding. However, the sheer size of these models limits their use in resource-constrained environments. In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters. TinyChart overcomes two key challenges in efficient chart understanding: (1) reduce the burden of learning numerical computations through Program-of-Thoughts (PoT) learning, which trains the model to generate Python programs for numerical calculations, and (2) reduce lengthy vision feature sequences through Vision Token Merging, which gradually merges most similar vision tokens. Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on various chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart-understanding MLLMs with up to 13B parameters, and close-sourced MLLM GPT-4V on ChartQA, with higher throughput during inference due to a smaller model scale and more efficient vision encoding.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.112",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Aligning Language Models to Explicitly Handle Ambiguity": {
        "type": "INPROCEEDINGS",
        "key": "kim-etal-2024-aligning",
        "author": "Kim, Hyuhng Joon and Kim, Youna and Park, Cheonbok and Kim, Junyeob and Park, Choonghyun and Yoo, Kang Min and Lee, Sang-goo and Kim, Taeuk",
        "booktitle": "EMNLP-main2024",
        "title": "Aligning Language Models to Explicitly Handle Ambiguity",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In interactions between users and language model agents, user utterances frequently exhibit ellipsis (omission of words or phrases) or imprecision (lack of exactness) to prioritize efficiency. This can lead to varying interpretations of the same input based on different assumptions or background knowledge. It is thus crucial for agents to adeptly handle the inherent ambiguity in queries to ensure reliability. However, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge. To address these issues, we propose Alignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to manage ambiguous queries by leveraging their own assessment of ambiguity (i.e., perceived ambiguity). Experimental results on question-answering datasets demonstrate that APA empowers LLMs to explicitly detect and manage ambiguous queries while retaining the ability to answer clear questions. Furthermore, our finding proves that APA excels beyond training with gold-standard labels, especially in out-of-distribution scenarios. The data and code are available at https://github.com/heyjoonkim/APA.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.119",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-glape",
        "author": "Zhang, Xuanchang and Zhang, Zhuosheng and Zhao, Hai",
        "booktitle": "EMNLP-main2024",
        "title": "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders its generality. To overcome the limitation, this work proposes GLaPE, a gold label-agnostic prompt evaluation method to alleviate dependence on gold labels. GLaPE is composed of two critical aspects: self-consistency evaluation of a single prompt and mutual-consistency refinement across multiple prompts. Experimental results on 8 widely-recognized reasoning tasks demonstrate that GLaPE can produce more effective prompts, achieving performance comparable to those derived from manually annotated gold labels. Analysis shows that GLaPE provides reliable evaluations aligned with accuracy, even in the absence of gold labels. Code is publicly available at **Anonymous**.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.121",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Optimizing Code Retrieval: High-Quality and Scalable Dataset Annotation through Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-optimizing",
        "author": "Li, Rui and Liu, Qi and He, Liyang and Zhang, Zheng and Zhang, Hao and Ye, Shengyu and Lu, Junyu and Huang, Zhenya",
        "booktitle": "EMNLP-main2024",
        "title": "Optimizing Code Retrieval: High-Quality and Scalable Dataset Annotation through Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code retrieval aims to identify code from extensive codebases that semantically aligns with a given query code snippet. Collecting a broad and high-quality set of query and code pairs is crucial to the success of this task. However, existing data collection methods struggle to effectively balance scalability and annotation quality. In this paper, we first analyze the factors influencing the quality of function annotations generated by Large Language Models (LLMs). We find that the invocation of intra-repository functions and third-party APIs plays a significant role. Building on this insight, we propose a novel annotation method that enhances the annotation context by incorporating the content of functions called within the repository and information on third-party API functionalities. Additionally, we integrate LLMs with a novel sorting method to address the multi-level function call relationships within repositories. Furthermore, by applying our proposed method across a range of repositories, we have developed the Query4Code dataset. The quality of this synthesized dataset is validated through both model training and human evaluation, demonstrating high-quality annotations. Moreover, cost analysis confirms the scalability of our annotation method.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.123",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "An Inversion Attack Against Obfuscated Embedding Matrix in Language Model Inference": {
        "type": "INPROCEEDINGS",
        "key": "lin-etal-2024-inversion",
        "author": "Lin, Yu and Zhang, Qizhi and Cai, Quanwei and Hong, Jue and Ye, Wu and Liu, Huiqi and Duan, Bing",
        "booktitle": "EMNLP-main2024",
        "title": "An Inversion Attack Against Obfuscated Embedding Matrix in Language Model Inference",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the rapidly-growing deployment of large language model (LLM) inference services, privacy concerns have arisen regarding to the user input data. Recent studies are exploring transforming user inputs to obfuscated embedded vectors, so that the data will not be eavesdropped by service provides. However, in this paper we show that again, without a solid and deliberate security design and analysis, such embedded vector obfuscation failed to protect users\u2019 privacy. We demonstrate the conclusion via conducting a novel inversion attack called Element-wise Differential Nearest Neighbor (EDNN) on the glide-reflection proposed in (CITATION), and the result showed that the original user input text can be 100% recovered from the obfuscated embedded vectors. We further analyze security requirements on embedding obfuscation and present several remedies to our proposed attack.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.126",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "wan-etal-2024-logicasker",
        "author": "Wan, Yuxuan and Wang, Wenxuan and Yang, Yiliu and Yuan, Youliang and Huang, Jen-tse and He, Pinjia and Jiao, Wenxiang and Lyu, Michael",
        "booktitle": "EMNLP-main2024",
        "title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT-4. Despite LLMs\u2019 prowess in tasks like writing assistance, code generation, and machine translation, assessing their ability to reason has been challenging. Traditional evaluations often prioritize accuracy on downstream tasks over direct assessments of reasoning processes. LogicAsker addresses this gap by employing a set of atomic reasoning skills grounded in propositional and predicate logic to systematically examine and improve the reasoning prowess of LLMs. Our methodology reveals significant gaps in LLMs\u2019 learning of logical rules, with identified reasoning failures ranging from 29% to 90% across different models. Moreover, we leverage these findings to construct targeted demonstration examples and fine-tune data, notably enhancing logical reasoning in models like GPT-4o by up to 5%. To our knowledge, this is the first effort to utilize test case outcomes to effectively refine LLMs\u2019 formal reasoning capabilities. We make our code, data, and results publicly available(https://github.com/yxwan123/LogicAsker) to facilitate further research and replication of our findings.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.128",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "I Need Help! Evaluating LLM\u2019s Ability to Ask for Users\u2019 Support: A Case Study on Text-to-SQL Generation": {
        "type": "INPROCEEDINGS",
        "key": "wu-etal-2024-need",
        "author": "Wu, Cheng-Kuang and Tam, Zhi Rui and Wu, Chao-Chung and Lin, Chieh-Yen and Lee, Hung-yi and Chen, Yun-Nung",
        "booktitle": "EMNLP-main2024",
        "title": "I Need Help! Evaluating LLM\u2019s Ability to Ask for Users\u2019 Support: A Case Study on Text-to-SQL Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This study explores the proactive ability of LLMs to seek user support. We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability. Our experiments show that without external feedback, many LLMs struggle to recognize their need for user support. The findings highlight the importance of external signals and provide insights for future research on improving support-seeking strategies. Source code: https://github.com/appier-research/i-need-help",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.131",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting": {
        "type": "INPROCEEDINGS",
        "key": "yoon-etal-2024-eyes",
        "author": "Yoon, Hyungjun and Tolera, Biniyam Aschalew and Gong, Taesik and Lee, Kimin and Lee, Sung-Ju",
        "booktitle": "EMNLP-main2024",
        "title": "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated exceptional abilities across various domains. However, utilizing LLMs for ubiquitous sensing applications remains challenging as existing text-prompt methods show significant performance degradation when handling long sensor data sequences. In this paper, we propose a visual prompting approach for sensor data using multimodal LLMs (MLLMs). Specifically, we design a visual prompt that directs MLLMs to utilize visualized sensor data alongside descriptions of the target sensory task. Additionally, we introduce a visualization generator that automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge. We evaluated our approach on nine sensory tasks involving four sensing modalities, achieving an average of 10% higher accuracy compared to text-based prompts and reducing token costs by 15.8 times. Our findings highlight the effectiveness and cost-efficiency of using visual prompts with MLLMs for various sensory tasks. The source code is available at https://github.com/diamond264/ByMyEyes.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.133",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CHIQ: Contextual History Enhancement for Improving Query Rewriting in Conversational Search": {
        "type": "INPROCEEDINGS",
        "key": "mo-etal-2024-chiq",
        "author": "Mo, Fengran and Ghaddar, Abbas and Mao, Kelong and Rezagholizadeh, Mehdi and Chen, Boxing and Liu, Qun and Nie, Jian-Yun",
        "booktitle": "EMNLP-main2024",
        "title": "CHIQ: Contextual History Enhancement for Improving Query Rewriting in Conversational Search",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we study how open-source large language models (LLMs) can be effectively deployed for improving query rewriting in conversational search, especially for ambiguous queries. We introduce CHIQ, a two-step method that leverages the capabilities of LLMs to resolve ambiguities in the conversation history before query rewriting. This approach contrasts with prior studies that predominantly use closed-source LLMs to directly generate search queries from conversation history. We demonstrate on five well-established benchmarks that CHIQ leads to state-of-the-art results across most settings, showing highly competitive performances with systems leveraging closed-source LLMs. Our study provides a first step towards leveraging open-source LLMs in conversational search, as a competitive alternative to the prevailing reliance on commercial LLMs. Data, models, and source code will be publicly available upon acceptance at https://github.com/fengranMark/CHIQ.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.135",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?": {
        "type": "INPROCEEDINGS",
        "key": "bhuiya-etal-2024-seemingly",
        "author": "Bhuiya, Neeladri and Schlegel, Viktor and Winkler, Stefan",
        "booktitle": "EMNLP-main2024",
        "title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on multi-hop reasoning\u2014the ability to identify and integrate information from multiple textual sources.Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. We propose a challenging multi-hop reasoning benchmark by generating seemingly plausible multi-hop reasoning chains that ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs and show that their multi-hop reasoning performance is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives. We also find that\u2014while LLMs tend to ignore misleading lexical cues\u2014misleading reasoning paths indeed present a significant challenge. The code and data are made available at https://github.com/zawedcvg/Are-Large-Language-Models-Attentive-Readers",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.147",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Knowledge Verification to Nip Hallucination in the Bud": {
        "type": "INPROCEEDINGS",
        "key": "wan-etal-2024-knowledge",
        "author": "Wan, Fanqi and Huang, Xinting and Cui, Leyang and Quan, Xiaojun and Bi, Wei and Shi, Shuming",
        "booktitle": "EMNLP-main2024",
        "title": "Knowledge Verification to Nip Hallucination in the Bud",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation LLMs. Specifically, we propose a novel approach called Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs. To address knowledge inconsistencies in the alignment data, KCA implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of KCA in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data are openly accessible at https://github.com/fanqiwan/KCA.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.152",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios": {
        "type": "INPROCEEDINGS",
        "key": "schrader-etal-2024-quite",
        "author": "Schrader, Timo Pierre and Lange, Lukas and Razniewski, Simon and Friedrich, Annemarie",
        "booktitle": "EMNLP-main2024",
        "title": "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reasoning is key to many decision making processes. It requires consolidating a set of rule-like premises that are often associated with degrees of uncertainty and observations to draw conclusions. In this work, we address both the case where premises are specified as numeric probabilistic rules and situations in which humans state their estimates using words expressing degrees of certainty. Existing probabilistic reasoning datasets simplify the task, e.g., by requiring the model to only rank textual alternatives, by including only binary random variables, or by making use of a limited set of templates that result in less varied text.In this work, we present QUITE, a question answering dataset of real-world Bayesian reasoning scenarios with categorical random variables and complex relationships. QUITE provides high-quality natural language verbalizations of premises together with evidence statements and expects the answer to a question in the form of an estimated probability. We conduct an extensive set of experiments, finding that logic-based models outperform out-of-the-box large language models on all reasoning types (causal, evidential, and explaining-away). Our results provide evidence that neuro-symbolic models are a promising direction for improving complex reasoning. We release QUITE and code for training and experiments on Github.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.153",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification": {
        "type": "INPROCEEDINGS",
        "key": "geigle-etal-2024-african",
        "author": "Geigle, Gregor and Timofte, Radu and Glava\u0161, Goran",
        "booktitle": "EMNLP-main2024",
        "title": "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities on numerous image understanding and reasoning tasks. The task of fine-grained object classification (e.g., distinction between animal species), however, has been probed insufficiently, despite its downstream importance. We fill this evaluation gap by creating FOCI (Fine-grained Object ClassIfication), a difficult multiple-choice benchmark for fine-grained object classification, from existing object classification datasets: (1) multiple-choice avoids ambiguous answers associated with casting classification as open-ended QA task; (2) we retain classification difficulty by mining negative labels with a CLIP model. FOCI complements five popular classification datasets with four domain-specific subsets from ImageNet-21k. We benchmark 12 public LVLMs on and show that it tests for a complementary skill to established image understanding and reasoning benchmarks. Crucially, CLIP models exhibit dramatically better performance than LVLMs. Since the image encoders of LVLMs come from these CLIP models, this points to inadequate alignment for fine-grained object distinction between the encoder and the LLM and warrants (pre)training data with more fine-grained annotation. We release our code at ANONYMIZED.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.154",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-asetf",
        "author": "Wang, Hao and Li, Hao and Huang, Minlie and Sha, Lei",
        "booktitle": "EMNLP-main2024",
        "title": "ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The safety defense methods of Large language models (LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. However, similar to traditional text adversarial attacks, this approach, while effective, is limited by the challenge of the discrete tokens. This gradient based discrete optimization attack requires over 100,000 LLM calls, and due to the unreadable of adversarial suffixes, it can be relatively easily penetrated by common defense methods such as perplexity filters.To cope with this challenge, in this paper, we propose an Adversarial Suffix Embedding Translation Framework (ASETF), aimed at transforming continuous adversarial suffix embeddings into coherent and understandable text. This method greatly reduces the computational overhead during the attack process and helps to automatically generate multiple adversarial samples, which can be used as data to strengthen LLM\u2019s security defense. Experimental evaluations were conducted on Llama2, Vicuna, and other prominent LLMs, employing harmful directives sourced from the Advbench dataset.The results indicate that our method significantly reduces the computation time of adversarial suffixes and achieves a much better attack success rate than existing techniques, while significantly enhancing the textual fluency of the prompts. In addition, our approach can be generalized into a broader method for generating transferable adversarial suffixes that can successfully attack multiple LLMs, even black-box LLMs, such as ChatGPT and Gemini.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.157",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-kb",
        "author": "Zhang, Jiajie and Cao, Shulin and Hu, Linmei and Feng, Ling and Hou, Lei and Li, Juanzi",
        "booktitle": "EMNLP-main2024",
        "title": "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of a given KB, and is thus challenging for many low-resourced KBs that lack annotated data. To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB. Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin. Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize the information to induce programs over this KB. Experiments show that KB-Plugin outperforms SoTA low-resourced PI methods with 25x smaller backbone LLM on both large-scale and domain-specific KBs, and even approaches the performance of supervised methods.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.168",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "he-etal-2024-seekr",
        "author": "He, Jinghan and Guo, Haiyun and Zhu, Kuan and Zhao, Zihan and Tang, Ming and Wang, Jinqiao",
        "booktitle": "EMNLP-main2024",
        "title": "SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Continual learning (CL) is crucial for language models to dynamically adapt to the evolving real-world demands. To mitigate the catastrophic forgetting problem in CL, data replay has been proven a simple and effective strategy, and the subsequent data-replay-based distillation can further enhance the performance. However, existing methods fail to fully exploit the knowledge embedded in models from previous tasks, resulting in the need for a relatively large number of replay samples to achieve good results. In this work, we first explore and emphasize the importance of attention weights in knowledge retention, and then propose a SElective attEntion-guided Knowledge Retention method (SEEKR) for data-efficient replay-based continual learning of large language models (LLMs). Specifically, SEEKR performs attention distillation on the selected attention heads for finer-grained knowledge retention, where the proposed forgettability-based and task-sensitivity-based measures are used to identify the most valuable attention heads. Experimental results on two continual learning benchmarks for LLMs demonstrate the superiority of SEEKR over the existing methods on both performance and efficiency. Explicitly, SEEKR achieves comparable or even better performance with only 1/10 of the replayed data used by other methods, and reduces the proportion of replayed data to 1%. The code is available at https://github.com/jinghan1he/SEEKR.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.190",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Neuron-Level Knowledge Attribution in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "yu-ananiadou-2024-neuron",
        "author": "Yu, Zeping and Ananiadou, Sophia",
        "booktitle": "EMNLP-main2024",
        "title": "Neuron-Level Knowledge Attribution in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify \u201cvalue neurons\u201d directly contributing to the final prediction, we propose a method for identifying \u201cquery neurons\u201d which activate these \u201cvalue neurons\u201d. Finally, we apply our methods to analyze six types of knowledge across both attention and feed-forward network (FFN) layers. Our method and analysis are helpful for understanding the mechanisms of knowledge storage and set the stage for future research in knowledge editing. The code is available on https://github.com/zepingyu0512/neuron-attribution.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.191",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "GoldCoin: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory": {
        "type": "INPROCEEDINGS",
        "key": "fan-etal-2024-goldcoin",
        "author": "Fan, Wei and Li, Haoran and Deng, Zheye and Wang, Weiqi and Song, Yangqiu",
        "booktitle": "EMNLP-main2024",
        "title": "GoldCoin: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Privacy issues arise prominently during the inappropriate transmission of information between entities. Existing research primarily studies privacy by exploring various privacy attacks, defenses, and evaluations within narrowly predefined patterns, while neglecting that privacy is not an isolated, context-free concept limited to traditionally sensitive data (e.g., social security numbers), but intertwined with intricate social contexts that complicate the identification and analysis of potential privacy violations. The advent of Large Language Models (LLMs) offers unprecedented opportunities for incorporating the nuanced scenarios outlined in privacy laws to tackle these complex privacy issues. However, the scarcity of open-source relevant case studies restricts the efficiency of LLMs in aligning with specific legal statutes. To address this challenge, we introduce a novel framework, GoldCoin, designed to efficiently ground LLMs in privacy laws for judicial assessing privacy violations. Our framework leverages the theory of contextual integrity as a bridge, creating numerous synthetic scenarios grounded in relevant privacy statutes (e.g., HIPAA), to assist LLMs in comprehending the complex contexts for identifying privacy risks in the real world. Extensive experimental results demonstrate that GoldCoin markedly enhances LLMs\u2019 capabilities in recognizing privacy risks across real court cases, surpassing the baselines on different judicial tasks.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.195",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "zhuang-etal-2024-efficientrag",
        "author": "Zhuang, Ziyuan and Zhang, Zhiyang and Cheng, Sitao and Yang, Fangkai and Liu, Jia and Huang, Shujian and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei and Zhang, Qi",
        "booktitle": "EMNLP-main2024",
        "title": "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Retrieval-augmented generation (RAG) methods encounter difficulties when addressing complex questions like multi-hop queries.While iterative retrieval methods improve performance by gathering additional information, current approaches often rely on multiple calls of large language models (LLMs).In this paper, we introduce EfficientRAG, an efficient retriever for multi-hop question answering.EfficientRAG iteratively generates new queries without the need for LLM calls at each iteration and filters out irrelevant information.Experimental results demonstrate that EfficientRAG surpasses existing RAG methods on three open-domain multi-hop question-answering datasets.The code is available in [aka.ms/efficientrag](https://github.com/NIL-zhuang/EfficientRAG-official).",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.199",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "LLM4Decompile: Decompiling Binary Code with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "tan-etal-2024-llm4decompile",
        "author": "Tan, Hanzhuo and Luo, Qi and Li, Jing and Zhang, Yuqun",
        "booktitle": "EMNLP-main2024",
        "title": "LLM4Decompile: Decompiling Binary Code with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.203",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-user",
        "author": "Wang, Jiayin and Mo, Fengran and Ma, Weizhi and Sun, Peijie and Zhang, Min and Nie, Jian-Yun",
        "booktitle": "EMNLP-main2024",
        "title": "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) are essential tools that users employ across various scenarios, so evaluating their performance and guiding users in selecting the suitable service is important. Although many benchmarks exist, they mainly focus on specific predefined model abilities, such as world knowledge, reasoning, etc. Based on these ability scores, it is hard for users to determine which LLM best suits their particular needs. To address these issues, we propose to evaluate LLMs from a user-centric perspective and design this benchmark to measure their efficacy in satisfying user needs under distinct intents. Firstly, we collect 1,846 real-world use cases from a user study with 712 participants from 23 countries. This first-hand data helps us understand actual user intents and needs in LLM interactions, forming the User Reported Scenarios (URS) dataset, which is categorized with six types of user intents. Secondly, based on this authentic dataset, we benchmark 10 LLM services with GPT-4-as-Judge. Thirdly, we show that benchmark scores align well with human preference in both real-world experience and pair-wise annotations, achieving Pearson correlations of 0.95 and 0.94, respectively. This alignment confirms that the URS dataset and our evaluation method establish an effective user-centric benchmark. The dataset, code, and process data are publicly available at https://github.com/Alice1998/URS.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.210",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "What do Large Language Models Need for Machine Translation Evaluation?": {
        "type": "INPROCEEDINGS",
        "key": "qian-etal-2024-large",
        "author": "Qian, Shenbin and Sindhujan, Archchana and Kabra, Minnie and Kanojia, Diptesh and Orasan, Constantin and Ranasinghe, Tharindu and Blain, Fred",
        "booktitle": "EMNLP-main2024",
        "title": "What do Large Language Models Need for Machine Translation Evaluation?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Leveraging large language models (LLMs) for various natural language processing tasks has led to superlative claims about their performance. For the evaluation of machine translation (MT), existing research shows that LLMs are able to achieve results comparable to fine-tuned multilingual pre-trained language models. In this paper, we explore what translation information, such as the source, reference, translation errors and annotation guidelines, is needed for LLMs to evaluate MT quality. In addition, we investigate prompting techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for eight language pairs covering high-, medium- and low-resource languages, leveraging varying LLM variants. Our findings indicate the importance of reference translations for an LLM-based evaluation. While larger models do not necessarily fare better, they tend to benefit more from CoT prompting, than smaller models. We also observe that LLMs do not always provide a numerical score when generating evaluations, which poses a question on their reliability for the task. Our work presents a comprehensive analysis for resource-constrained and training-less LLM-based evaluation of machine translation. We release the accrued prompt templates, code and data publicly for reproducibility.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.214",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "M\u00b2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-m2pt",
        "author": "Wang, Taowen and Liu, Yiyang and Liang, James Chenhao and Zhao, Junhan and Cui, Yiming and Mao, Yuning and Nie, Shaoliang and Liu, Jiahao and Feng, Fuli and Xu, Zenglin and Han, Cheng and Huang, Lifu and Wang, Qifan and Liu, Dongfang",
        "booktitle": "EMNLP-main2024",
        "title": "M\u00b2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multimodal Large Language Models (MLLMs) demonstrate remarkable performance across a wide range of domains, with increasing emphasis on enhancing their zero-shot generalization capabilities for unseen tasks across various modalities. Instruction tuning has emerged as an effective strategy for achieving zero-shot generalization by finetuning pretrained models on diverse multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient finetuning becomes increasingly critical. However, most existing parameter-efficient approaches focus only on single modalities and often overlook the multimodal characteristics during finetuning. In this work, we introduce a novel Multimodal Prompt Tuning (M\u00b2PT) approach for efficient instruction tuning of MLLMs. M\u00b2PT effectively integrates visual and textual prompts into the vision encoder and language processor respectively during finetuning, facilitating the extraction and alignment of features across modalities. Empirical results on various multimodal evaluation datasets demonstrate the superior performance of our approach compared to several state-of-the-art baselines. A comprehensive set of ablation studies validates the effectiveness of our prompt design and the efficiency of our approach.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.218",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-ptd",
        "author": "Luo, Ruilin and Wang, Liyuan and Lin, Binghuai and Lin, Zicheng and Yang, Yujiu",
        "booktitle": "EMNLP-main2024",
        "title": "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problem and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-the-art (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress. Code is available at https://github.com/lrlbbzl/PTD-SQL.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.221",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Where is the signal in tokenization space?": {
        "type": "INPROCEEDINGS",
        "key": "geh-etal-2024-signal",
        "author": "Geh, Renato and Zhang, Honghua and Ahmed, Kareem and Wang, Benjie and Van Den Broeck, Guy",
        "booktitle": "EMNLP-main2024",
        "title": "Where is the signal in tokenization space?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) are typically shipped with tokenizers that *deterministically* encode text into so-called *canonical* token sequences, to which the LLMs assign probability values.One common assumption is that the probability of a piece of text is the probability of its canonical token sequence.However, the tokenization of a string is not unique: e.g., the Llama2 tokenizer encodes \u2018Tokens\u2018 as \u2018[Tok,ens]\u2018, but \u2018[Tok,en,s]\u2018 also represents the same text.In this paper, we study non-canonical tokenizations.We prove that, given a string, it is computationally hard to find the most likely tokenization for an autoregressive LLM, as well as to compute the marginal probability over all possible tokenizations.We then show how the marginal is, in most cases, indistinguishable from the canonical probability.Surprisingly, we then empirically demonstrate the existence of a significant amount of signal hidden within tokenization space.Notably, by simply aggregating the probabilities of non-canonical tokenizations, we achieve improvements across a range of LLM evaluation benchmarks for a variety of architectures, including transformers and state space models.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.230",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "I Could\u2019ve Asked That: Reformulating Unanswerable Questions": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-couldve",
        "author": "Zhao, Wenting and Gao, Ge and Cardie, Claire and Rush, Alexander M.",
        "booktitle": "EMNLP-main2024",
        "title": "I Could\u2019ve Asked That: Reformulating Unanswerable Questions",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "When seeking information from unfamiliar documents, users frequently pose questions that cannot be answered by the documents. While existing large language models (LLMs) identify these unanswerable questions, they do not assist users in reformulating their questions, thereby reducing their overall utility. We curate CouldAsk, an evaluation benchmark composed of existing and new datasets for document-grounded question answering, specifically designed to study reformulating unanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMs on CouldAsk. The results demonstrate the limited capabilities of these models in reformulating questions. Specifically, GPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the time, respectively. Error analysis shows that 62% of the unsuccessful reformulations stem from the models merely rephrasing the questions or even generating identical questions. We publicly release the benchmark and the code to reproduce the experiments.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.242",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "DVD: Dynamic Contrastive Decoding for Knowledge Amplification in Multi-Document Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "jin-etal-2024-dvd",
        "author": "Jin, Jing and Wang, Houfeng and Zhang, Hao and Li, Xiaoguang and Guo, Zhijiang",
        "booktitle": "EMNLP-main2024",
        "title": "DVD: Dynamic Contrastive Decoding for Knowledge Amplification in Multi-Document Question Answering",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) are widely used in question-answering (QA) systems but often generate information with hallucinations. Retrieval-augmented generation (RAG) offers a potential remedy, yet the uneven retrieval quality and irrelevant contents may distract LLMs.In this work, we address these issues at the generation phase by treating RAG as a multi-document QA task.We propose a novel decoding strategy, Dynamic Contrastive Decoding, which dynamically amplifies knowledge from selected documents during the generation phase. involves constructing inputs batchwise, designing new selection criteria to identify documents worth amplifying, and applying contrastive decoding with a specialized weight calculation to adjust the final logits used for sampling answer tokens. Zero-shot experimental results on ALCE-ASQA, NQ, TQA and PopQA benchmarks show that our method outperforms other decoding strategies. Additionally, we conduct experiments to validate the effectiveness of our selection criteria, weight calculation, and general multi-document scenarios. Our method requires no training and can be integrated with other methods to improve the RAG performance. Our codes will be publicly available at https://github.com/JulieJin-km/Dynamic_Contrastive_Decoding.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.266",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "How Do Humans Write Code? Large Models Do It the Same Way Too": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-humans",
        "author": "Li, Long and He, Xuzheng and Wang, Haozhe and Wang, Linlin and He, Liang",
        "booktitle": "EMNLP-main2024",
        "title": "How Do Humans Write Code? Large Models Do It the Same Way Too",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model\u2019s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.267",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "yin-etal-2024-mumath",
        "author": "Yin, Shuo and You, Weihao and Ji, Zhilong and Zhong, Guoqiang and Bai, Jinfeng",
        "booktitle": "EMNLP-main2024",
        "title": "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math reasoning data. However, a great method to integrate the above two research paths and combine their advantages remains to be explored. In this work, we firstly include new math questions via **mu**lti-perspective data augmenting methods and then synthesize **code**-nested solutions to them. The open LLMs (e.g., Llama-2) are finetuned on the augmented dataset to get the resulting models, **MuMath-Code** (\u03bc-Math-Code). During the inference phase, our MuMath-Code generates code and interacts with the external python interpreter to get the execution results. Therefore, MuMath-Code leverages the advantages of both the external tool and data augmentation. To fully leverage the advantages of our augmented data, we propose a two-stage training strategy: In Stage-1, we finetune Llama-2 on pure CoT data to get an intermediate model, which then is trained on the code-nested data in Stage-2 to get the resulting MuMath-Code.Our MuMath-Code-7B achieves 83.8% on GSM8K and 52.4% on MATH, while MuMath-Code-70B model achieves new state-of-the-art performance among open methods\u2014achieving 90.7% on GSM8K and 55.1% on MATH. Extensive experiments validate the combination of tool use and data augmentation, as well as our two-stage training strategy.We release the proposed dataset along with the associated code for public use: https://github.com/youweihao-tal/MuMath-Code.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.274",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "RWKV-CLIP: A Robust Vision-Language Representation Learner": {
        "type": "INPROCEEDINGS",
        "key": "gu-etal-2024-rwkv",
        "author": "Gu, Tiancheng and Yang, Kaicheng and An, Xiang and Feng, Ziyong and Liu, Dongnan and Cai, Weidong and Deng, Jiankang",
        "booktitle": "EMNLP-main2024",
        "title": "RWKV-CLIP: A Robust Vision-Language Representation Learner",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Contrastive Language-Image Pre-training (CLIP) has significantly improved performance in various vision-language tasks by expanding the dataset with image-text pairs obtained from the web. This paper further explores CLIP from the perspectives of data and model architecture. To mitigate the impact of the noise data and enhance the quality of large-scale image-text data crawled from the internet, we introduce a diverse description generation framework that can leverage Large Language Models (LLMs) to combine and refine information from web-based image-text pairs, synthetic captions, and detection tags. Additionally, we propose RWKV-CLIP, the first RWKV-driven vision-language representation learning model that combines the effective parallel training of transformers with the efficient inference of RNNs. Extensive experiments across different model scales and pre-training datasets demonstrate that RWKV-CLIP is a robust vision-language representation learner and it achieves state-of-the-art performance across multiple downstream tasks, including linear probing, zero-shot classification, and zero-shot image-text retrieval. To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/RWKV-CLIP.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.276",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-pretraining",
        "author": "Zhang, Weichao and Zhang, Ruqing and Guo, Jiafeng and de Rijke, Maarten and Fan, Yixing and Cheng, Xueqi",
        "booktitle": "EMNLP-main2024",
        "title": "Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "As the scale of training corpora for large language models (LLMs) grows, model developers become increasingly reluctant to disclose details on their data. This lack of transparency poses challenges to scientific evaluation and ethical deployment. Recently, pretraining data detection approaches, which infer whether a given text was part of an LLM\u2019s training data through black-box access, have been explored. The Min-K% Prob method, which has achieved state-of-the-art results, assumes that a non-training example tends to contain a few outlier words with low token probabilities. However, the effectiveness may be limited as it tends to misclassify non-training texts that contain many common words with high probabilities predicted by LLMs. To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score.We have developed a Chinese-language benchmark, PatentMIA, to assess the performance of detection approaches for LLMs on Chinese text. Experimental results on English-language benchmarks and PatentMIA demonstrate that our proposed method significantly outperforms existing methods. Our code and PatentMIA benchmark are available at https://github.com/zhang-wei-chao/DC-PDD.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.300",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model": {
        "type": "INPROCEEDINGS",
        "key": "zheng-etal-2024-safely",
        "author": "Zheng, Jia-Ying and Zhang, Hainan and Wang, Lingxiang and Qiu, Wangjie and Zheng, Hong-Wei and Zheng, Zhi-Ming",
        "booktitle": "EMNLP-main2024",
        "title": "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Private data, being larger and quality-higher than public data, can greatly improve large language models (LLM). However, due to privacy concerns, this data is often dispersed in multiple silos, making its secure utilization for LLM training a challenge. Federated learning (FL) is an ideal solution for training models with distributed private data, but traditional frameworks like FedAvg are unsuitable for LLM due to their high computational demands on clients. An alternative, split learning, offloads most training parameters to the server while training embedding and output layers locally, making it more suitable for LLM. Nonetheless, it faces significant challenges in security and efficiency. Firstly, the gradients of embeddings are prone to attacks, leading to potential reverse engineering of private data. Furthermore, the server\u2019s limitation of handling only one client\u2019s training request at a time hinders parallel training, severely impacting training efficiency. In this paper, we propose a Federated Learning framework for LLM, named FL-GLM, which prevents data leakage caused by both server-side and peer-client attacks while improving training efficiency. Specifically, we first place the input block and output block on local client to prevent embedding gradient attacks from server. Secondly, we employ key-encryption during client-server communication to prevent reverse engineering attacks from peer-clients. Lastly, we employ optimization methods like client-batching or server-hierarchical, adopting different acceleration methods based on the actual computational capabilities of the server. Experimental results on NLU and generation tasks demonstrate that FL-GLM achieves comparable metrics to centralized chatGLM model, validating the effectiveness of our federated learning framework.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.303",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-textual",
        "author": "Luo, Yang and Zheng, Zangwei and Zhu, Zirui and You, Yang",
        "booktitle": "EMNLP-main2024",
        "title": "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The increase in parameter size of multimodal large language models (MLLMs) introduces significant capabilities, particularly multimodal in-context learning, where MLLMs enhance task performance without updating pre-trained parameters. However, this effectiveness hinges on the appropriate selection of in-context examples, a process currently biased towards visual data, overlooking textual information. More importantly, the area of supervised retrievers for retrieval of multimodal in-context learning, crucial for optimal in-context example selection, continues to be investigated. Our study provides an in-depth evaluation of the impact of textual information on the unsupervised selection of in-context examples in multimodal contexts, uncovering a notable sensitivity of retriever performance to the employed modalities. Based on the above finding, we introduce a novel supervised MLLM prompt retriever MSIER that leverages a trained retriever based on MLLM\u2019s confidence to select examples, which enhances multimodal in-context learning efficiency. This approach is validated through extensive testing across three different tasks, demonstrating the method\u2019s effectiveness. Additionally, we investigate the influence of modalities on our supervised retrieval method\u2019s training and explore the transferability of the supervised prompt retriever. This exploration paves the way for future advancements, highlighting the potential for refined in-context learning in MLLMs through the strategic use of multimodal data. The public code is available at https://github.com/NUS-HPC-AI-Lab/Multimodal-ICL-Retriever.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.305",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "purohit-etal-2024-explora",
        "author": "Purohit, Kiran and V, Venktesh and Devalla, Raghuram and Yerragorla, Krishna Mohan and Bhattacharya, Sourangshu and Anand, Avishek",
        "booktitle": "EMNLP-main2024",
        "title": "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Answering reasoning-based complex questions over text and hybrid sources, including tables, is a challenging task. Recent advances in large language models (LLMs) have enabled in-context learning (ICL), allowing LLMs to acquire proficiency in a specific task using only a few demonstration samples (exemplars). A critical challenge in ICL is the selection of optimal exemplars, which can be either task-specific (static) or test-example-specific (dynamic). Static exemplars provide faster inference times and increased robustness across a distribution of test examples. In this paper, we propose an algorithm for static exemplar subset selection for complex reasoning tasks. We introduce EXPLORA, a novel exploration method designed to estimate the parameters of the scoring function, which evaluates exemplar subsets without incorporating confidence information. EXPLORA significantly reduces the number of LLM calls to ~11% of those required by state-of-the-art methods and achieves a substantial performance improvement of 12.24%. We open-source our code and data (https://github.com/kiranpurohit/EXPLORA).",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.307",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-adaption",
        "author": "Xu, Mayi and Li, Yongqi and Sun, Ke and Qian, Tieyun",
        "booktitle": "EMNLP-main2024",
        "title": "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have shown excellent capability for solving reasoning problems. Existing approaches do not differentiate the question difficulty when designing prompting methods for them. Clearly, a simple method cannot elicit sufficient knowledge from LLMs to answer a hard question. Meanwhile, a sophisticated one will force the LLM to generate redundant or even inaccurate intermediate steps toward a simple question. Consequently, the performance of existing methods fluctuates among various questions.In this work, we propose Adaption-of-Thought (AdoT), an adaptive method to improve LLMs for the reasoning problem, which first measures the question difficulty and then tailors demonstration set construction and difficulty-adapted retrieval strategies for the adaptive demonstration construction. Experimental results on three reasoning tasks prove the superiority of our proposed method, showing an absolute improvement of up to 5.5% on arithmetic reasoning, 7.4% on symbolic reasoning, and 2.3% on commonsense reasoning. Our codes and implementation details are available at: https://github.com/NLPGM/AdoT",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.313",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model": {
        "type": "INPROCEEDINGS",
        "key": "yuan-etal-2024-predicting",
        "author": "Yuan, Chenhan and Huang, Fei and Peng, Ru and Lu, Keming and Yu, Bowen and Zhou, Chang and Zhou, Jingren",
        "booktitle": "EMNLP-main2024",
        "title": "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Transformer-based large language models (LLMs) exhibit limitations such as generating unsafe responses, unreliable reasoning, etc. Existing inference intervention approaches attempt to mitigate these issues by finetuning additional models to produce calibration signals (such as rewards) that guide the LLM\u2019s decoding process. However, this solution introduces substantial time and space overhead due to the separate models required. This work proposes Non-disruptive parameters insertion (Otter), inserting extra parameters into the transformer architecture to predict calibration signals along with the original LLM output. Otter offers state-of-the-art performance on multiple demanding tasks while saving up to 86.5% extra space and 98.5% extra time. Furthermore, Otter seamlessly integrates with existing inference engines, requiring only a one-line code change, and the original model response remains accessible after the parameter insertion.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.316",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-nlebench",
        "author": "Liu, Peng and Zhang, Lemei and Farup, Terje and Lauvrak, Even W. and Ingvaldsen, Jon Espen and Eide, Simen and Gulla, Jon Atle and Yang, Zhirong",
        "booktitle": "EMNLP-main2024",
        "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Norwegian, spoken by only 5 million population, is under-representative within the most impressive breakthroughs in NLP tasks. To the best of our knowledge, there has not yet been a comprehensive evaluation of the existing language models (LMs) on Norwegian generation tasks during the article writing process. To fill this gap, we 1) compiled the existing Norwegian dataset and pre-trained 4 Norwegian Open Language Models varied from parameter scales and architectures, collectively called NorGLM; 2) introduced a comprehensive benchmark, NLEBench, for evaluating natural language generation capabilities in Norwegian, encompassing translation and human annotation. Based on the investigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5 has limited capability in understanding the Norwegian context; 2) the increase in model parameter scales demonstrates limited impact on the performance of downstream tasks when the pre-training dataset is constrained in size; 3) smaller models also demonstrate the reasoning capability through Chain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be used to verify the generalizability of LLMs on natural language understanding and, meanwhile, test the interconnectedness of these NLP tasks. We share our resources and code for reproducibility under a CC BY-NC 4.0 license.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.317",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-rear",
        "author": "Wang, Yuhao and Ren, Ruiyang and Li, Junyi and Zhao, Xin and Liu, Jing and Wen, Ji-Rong",
        "booktitle": "EMNLP-main2024",
        "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness regarding the reliability of external knowledge for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a novel architecture for LLM based RAG system, by incorporating a specially designed assessnent module that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our codes can be accessed at https://github.com/RUCAIBox/REAR.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.321",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse": {
        "type": "INPROCEEDINGS",
        "key": "guo-etal-2024-computational",
        "author": "Guo, Xiaobo and Potnis, Neil and Yu, Melody and Gillani, Nabeel and Vosoughi, Soroush",
        "booktitle": "EMNLP-main2024",
        "title": "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The ability for individuals to constructively engage with one another across lines of difference is a critical feature of a healthy pluralistic society. This is also true in online discussion spaces like social media platforms. To date, much social media research has focused on preventing ills\u2014like political polarization and the spread of misinformation. While this is important, enhancing the quality of online public discourse requires not just reducing ills, but also, promoting foundational human virtues. In this study, we focus on one particular virtue: \u201cintellectual humility\u201d (IH), or acknowledging the potential limitations in one\u2019s own beliefs. Specifically, we explore the development of computational methods for measuring IH at scale. We manually curate and validate an IH codebook on 350 posts about religion drawn from subreddits and use them to develop LLM-based models for automating this measurement. Our best model achieves a Macro-F1 score of 0.64 across labels (and 0.70 when predicting IH/IA/Neutral at the coarse level), higher than an expected naive baseline of 0.51 (0.32 for IH/IA/Neutral) but lower than a human annotator-informed upper bound of 0.85 (0.83 for IH/IA/Neutral). Our results both highlight the challenging nature of detecting IH online\u2014opening the door to new directions in NLP research\u2014and also lay a foundation for computational social science researchers interested in analyzing and fostering more IH in online public discourse.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.327",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts": {
        "type": "INPROCEEDINGS",
        "key": "lee-etal-2024-analyzing",
        "author": "Lee, Jaewook and Jang, Yeajin and Kim, Hongjin and Lee, Woojin and Kim, Harksoo",
        "booktitle": "EMNLP-main2024",
        "title": "Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Emotional intelligence (EI) in artificial intelligence (AI), which refers to the ability of an AI to understand and respond appropriately to human emotions, has emerged as a crucial research topic. Recent studies have shown that large language models (LLMs) and vision large language models (VLLMs) possess EI and the ability to understand emotional stimuli in the form of text and images, respectively. However, factors influencing the emotion prediction performance of VLLMs in real-world conversational contexts have not been sufficiently explored. This study aims to analyze the key elements affecting the emotion prediction performance of VLLMs in conversational contexts systematically. To achieve this, we reconstructed the MELD dataset, which is based on the popular TV series Friends, and conducted experiments through three sub-tasks: overall emotion tone prediction, character emotion prediction, and contextually appropriate emotion expression selection. We evaluated the performance differences based on various model architectures (e.g., image encoders, modality alignment, and LLMs) and image scopes (e.g., entire scene, person, and facial expression). In addition, we investigated the impact of providing persona information on the emotion prediction performance of the models and analyzed how personality traits and speaking styles influenced the emotion prediction process. We conducted an in-depth analysis of the impact of various other factors, such as gender and regional biases, on the emotion prediction performance of VLLMs. The results revealed that these factors significantly influenced the model performance.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.331",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling": {
        "type": "INPROCEEDINGS",
        "key": "bai-etal-2024-citrus",
        "author": "Bai, Yu and Zou, Xiyuan and Huang, Heyan and Chen, Sanxing and Rondeau, Marc-Antoine and Gao, Yang and Cheung, Jackie CK",
        "booktitle": "EMNLP-main2024",
        "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Long sequence modeling has gained broad interest as large language models (LLMs) continue to advance. Recent research has identified that a large portion of hidden states within the key-value caches of Transformer models can be discarded (also termed evicted) withoutaffecting the perplexity performance in generating long sequences. However, we show that these methods, despite preserving perplexity performance, often drop information that is important for solving downstream tasks, a problem which we call information neglect. To address this issue, we introduce Chunked Instruction-aware State Eviction (CItruS), a novel modeling technique that integrates the attention preferences useful for a downstream task into the eviction process of hidden states. In addition, we design a method for chunked sequence processing to further improve efficiency. Our training-free method exhibits superior performance on long sequence comprehension and retrieval tasks over several strong baselines under the same memory budget, while preserving language modeling perplexity. The code and data have been released at https://github.com/ybai-nlp/CItruS.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.338",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection": {
        "type": "INPROCEEDINGS",
        "key": "lin-etal-2024-video",
        "author": "Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li",
        "booktitle": "EMNLP-main2024",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers.In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks.Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.342",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-sayself",
        "author": "Xu, Tianyang and Wu, Shujin and Diao, Shizhe and Liu, Xiaoze and Wang, Xingyao and Chen, Yangyi and Gao, Jing",
        "booktitle": "EMNLP-main2024",
        "title": "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work has elicited confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present SaySelf, a novel training framework that teaches LLMs to express more fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. The generated self-reflective rationales are also reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.343",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation": {
        "type": "INPROCEEDINGS",
        "key": "qi-etal-2024-model",
        "author": "Qi, Jirui and Sarti, Gabriele and Fern\u00e1ndez, Raquel and Bisazza, Arianna",
        "booktitle": "EMNLP-main2024",
        "title": "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain. Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers. However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs\u2019 context usage throughout the generation. In this work, we present MIRAGE \u2013 Model Internals-based RAG Explanations \u2013 a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. We evaluate our proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. Our qualitative evaluation highlights the faithfulness of MIRAGE\u2019s attributions and underscores the promising application of model internals for RAG answer attribution. Code and data released at https://github.com/Betswish/MIRAGE.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.347",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities": {
        "type": "INPROCEEDINGS",
        "key": "ghosh-etal-2024-gama",
        "author": "Ghosh, Sreyan and Kumar, Sonal and Seth, Ashish and Evuru, Chandra Kiran Reddy and Tyagi, Utkarsh and Sakshi, S. and Nieto, Oriol and Duraiswami, Ramani and Manocha, Dinesh",
        "booktitle": "EMNLP-main2024",
        "title": "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Perceiving and understanding non-speech sounds and non-verbal speech is essential to making decisions that help us interact with our surroundings. In this paper, we propose GAMA, a novel General-purpose Large Audio-Language Model (LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We build GAMA by integrating an LLM with multiple types of audio representations, including features from a custom Audio Q-Former, a multi-layer aggregator that aggregates features from multiple layers of an audio encoder. We fine-tune GAMA on a large-scale audio-language dataset, which augments it with audio understanding capabilities. Next, we propose CompA-R (Instruction-Tuning for Complex Audio Reasoning), a synthetically generated instruction-tuning (IT) dataset with instructions that require the model to perform complex reasoning on the input audio. We instruction-tune GAMA with CompA-R to endow it with complex reasoning abilities, where we further add a soft prompt as input with high-level semantic evidence by leveraging event tags of the input audio. Finally, we also propose CompA-R-test, a human-labeled evaluation dataset for evaluating the capabilities of LALMs on open-ended audio question-answering that requires complex reasoning. Through automated and expert human evaluations, we show that GAMA outperforms all other LALMs in literature on diverse audio understanding tasks by margins of 1%-84% and demonstrates state-of-the-art performance on deductive reasoning and hallucination evaluation benchmarks. Further, GAMA IT-ed on CompA-R proves to be superior in its complex reasoning capabilities.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.361",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Satyrn: A Platform for Analytics Augmented Generation": {
        "type": "INPROCEEDINGS",
        "key": "sterbentz-etal-2024-satyrn",
        "author": "Sterbentz, Marko and Barrie, Cameron and Shahi, Shubham and Dutta, Abhratanu and Hooshmand, Donna and Pack, Harper and Hammond, Kristian J.",
        "booktitle": "EMNLP-main2024",
        "title": "Satyrn: A Platform for Analytics Augmented Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) are capable of producing documents, and retrieval augmented generation (RAG) has shown itself to be a powerful method for improving accuracy without sacrificing fluency. However, not all information can be retrieved from text. We propose an approach that uses the analysis of structured data to generate fact sets that are used to guide generation in much the same way that retrieved documents are used in RAG. This analytics augmented generation (AAG) approach supports the ability to utilize standard analytic techniques to generate facts that are then converted to text and passed to an LLM. We present a neurosymbolic platform, Satyrn, that leverages AAG to produce accurate, fluent, and coherent reports grounded in large scale databases. In our experiments, we find that Satyrn generates reports in which over 86% of claims are accurate while maintaining high levels of fluency and coherence, even when using smaller language models such as Mistral-7B, as compared to GPT-4 Code Interpreter in which just 57% of claims are accurate.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.365",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model": {
        "type": "INPROCEEDINGS",
        "key": "huo-etal-2024-mmneuron",
        "author": "Huo, Jiahao and Yan, Yibo and Hu, Boren and Yue, Yutao and Hu, Xuming",
        "booktitle": "EMNLP-main2024",
        "title": "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Projecting visual features into word embedding space has become a significant fusion strategy adopted by Multimodal Large Language Models (MLLMs). However, its internal mechanisms have yet to be explored. Inspired by multilingual research, we identify domain-specific neurons in multimodal large language models. Specifically, we investigate the distribution of domain-specific neurons and the mechanism of how MLLMs process features from diverse domains. Furthermore, we propose a three-stage framework for language model modules in MLLMs when handling projected image features, and verify this hypothesis using logit lens. Extensive experiments indicate that while current MLLMs exhibit Visual Question Answering (VQA) capability, they may not fully utilize domain-specific information. Manipulating domain-specific neurons properly will result in a 10% change of accuracy at most, shedding light on the development of cross-domain, all-encompassing MLLMs in the future. The source code is available at https://anonymous.4open.science/r/MMNeuron.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.387",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "zhong-etal-2024-beyond",
        "author": "Zhong, Yiwu and Hu, Zi-Yuan and Lyu, Michael and Wang, Liwei",
        "booktitle": "EMNLP-main2024",
        "title": "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Visual representation learning has been a cornerstone in computer vision, involving typical forms such as visual embeddings, structural symbols, and text-based representations. Despite the success of CLIP-type visual embeddings, they often lack access to world knowledge critical for visual reasoning. In this work, we propose Visual Table, a novel form of visual representation tailored for visual reasoning. Visual tables are constructed as hierarchical descriptions of visual scenes, featuring a scene description and multiple object-centric descriptions covering categories, attributes, and knowledge. Thanks to the structural and textual formats, visual tables offer unique properties over mere visual embeddings, such as explainability and controllable editing. Furthermore, they deliver instance-level world knowledge and detailed attributes that are essential for visual reasoning. To create visual tables, we develop a generator trained on the dataset with collected, small-scale annotations. Extensive results on 11 visual reasoning benchmarks demonstrate that the generated visual tables significantly outperform previous structural and text-based representations. Moreover, they consistently enhance state-of-the-art multi-modal large language models across diverse benchmarks, showcasing their potential for advancing visual reasoning tasks. Our code is available at https://github.com/LaVi-Lab/Visual-Table.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.391",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-taylor",
        "author": "Wang, Guanchu and Chuang, Yu-Neng and Tang, Ruixiang and Zhong, Shaochen and Yuan, Jiayi and Jin, Hongye and Liu, Zirui and Chaudhary, Vipin and Xu, Shuai and Caverlee, James and Hu, Xia",
        "booktitle": "EMNLP-main2024",
        "title": "Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Ensuring the security of released large language models (LLMs) poses a significant dilemma, as existing mechanisms either compromise ownership rights or raise data privacy concerns. To address this dilemma, we introduce TaylorMLP to protect the ownership of released LLMs and prevent their abuse. Specifically, TaylorMLP preserves the ownership of LLMs by transforming the weights of LLMs into parameters of Taylor-series. Instead of releasing the original weights, developers can release the Taylor-series parameters with users, thereby ensuring the security of LLMs. Moreover, TaylorMLP can prevent abuse of LLMs by adjusting the generation speed. It can induce low-speed token generation for the protected LLMs by increasing the terms in the Taylor-series. This intentional delay helps LLM developers prevent potential large-scale unauthorized uses of their models. Empirical experiments across five datasets and three LLM architectures demonstrate that TaylorMLP induces over increase in latency, producing the tokens precisely matched with original LLMs. Subsequent defensive experiments further confirm that TaylorMLP effectively prevents users from reconstructing the weight values based on downstream datasets.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.393",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "TimeR\u2074 : Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "qian-etal-2024-timer4",
        "author": "Qian, Xinying and Zhang, Ying and Zhao, Yu and Zhou, Baohang and Sui, Xuhui and Zhang, Li and Song, Kehui",
        "booktitle": "EMNLP-main2024",
        "title": "TimeR\u2074 : Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer temporal questions using knowledge in Temporal Knowledge Graphs (TKGs). Previous works employ pre-trained TKG embeddings or graph neural networks to incorporate the knowledge of TKGs. However, these methods fail to fully understand the complex semantic information of time constraints in questions.In contrast, Large Language Models (LLMs) have shown exceptional performance in knowledge graph reasoning, unifying both semantic understanding and structural reasoning. To further enhance LLMs\u2019 temporal reasoning ability, this paper aims to integrate relevant temporal knowledge from TKGs into LLMs through a Time-aware Retrieve-Rewrite-Retrieve-Rerank framework, which we named TimeR\u2074.Specifically, to reduce temporal hallucination in LLMs, we propose a retrieve-rewrite module to rewrite questions using background knowledge stored in the TKGs, thereby acquiring explicit time constraints. Then, we implement a retrieve-rerank module aimed at retrieving semantically and temporally relevant facts from the TKGs and reranking them according to the temporal constraints.To achieve this, we fine-tune a retriever using the contrastive time-aware learning framework.Our approach achieves great improvements, with relative gains of 47.8% and 22.5% on two datasets, underscoring its effectiveness in boosting the temporal reasoning abilities of LLMs. Our code is available at https://github.com/qianxinying/TimeR4.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.394",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Learning to Retrieve Iteratively for In-Context Learning": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-learning-retrieve",
        "author": "Chen, Yunmo and Chen, Tongfei and Jhamtani, Harsh and Xia, Patrick and Shin, Richard and Eisner, Jason and Van Durme, Benjamin",
        "booktitle": "EMNLP-main2024",
        "title": "Learning to Retrieve Iteratively for In-Context Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We introduce iterative retrieval, a novel framework that empowers retrievers to make iterative decisions through policy optimization. Finding an optimal portfolio of retrieved items is a combinatorial optimization problem, generally considered NP-hard. This approach provides a learned approximation to such a solution, meeting specific task requirements under a given family of large language models (LLMs). We propose a training procedure based on reinforcement learning, incorporating feedback from LLMs. We instantiate an iterative retriever for composing in-context learning (ICL) exemplars and apply it to various semantic parsing tasks that demand synthesized programs as outputs. By adding only 4M additional parameters for state encoding, we convert an off-the-shelf dense retriever into a stateful iterative retriever, outperforming previous methods in selecting ICL exemplars on semantic parsing datasets such as CalFlow, TreeDST, and MTOP. Additionally, the trained iterative retriever generalizes across different inference LLMs beyond the one used during training.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.406",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Aligning Large Language Models with Diverse Political Viewpoints": {
        "type": "INPROCEEDINGS",
        "key": "stammbach-etal-2024-aligning",
        "author": "Stammbach, Dominik and Widmer, Philine and Cho, Eunjung and Gulcehre, Caglar and Ash, Elliott",
        "booktitle": "EMNLP-main2024",
        "title": "Aligning Large Language Models with Diverse Political Viewpoints",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models such as ChatGPT exhibit striking political biases. If users query them about political information, they often take a normative stance. To overcome this, we align LLMs with diverse political viewpoints from 100,000 comments written by candidates running for national parliament in Switzerland. Models aligned with this data can generate more accurate political viewpoints from Swiss parties, compared to commercial models such as ChatGPT. We also propose a procedure to generate balanced overviews summarizing multiple viewpoints using such models. The replication package contains all code and data.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.412",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-magic",
        "author": "Xu, Lin and Hu, Zhiyuan and Zhou, Daquan and Ren, Hongyu and Dong, Zhen and Keutzer, Kurt and Ng, See-Kiong and Feng, Jiashi",
        "booktitle": "EMNLP-main2024",
        "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs\u2019 reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality.We utilize two social deduction games alongside three game-theory scenarios to create diverse environments.Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs\u2019 capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37%. Our data and code can be found here https://github.com/cathyxl/MAgIC.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.416",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-towards-injecting",
        "author": "Chen, Junying and Gui, Chi and Ouyang, Ruyi and Gao, Anningzhe and Chen, Shunian and Chen, Guiming Hardy and Wang, Xidong and Cai, Zhenyang and Ji, Ke and Wan, Xiang and Wang, Benyou",
        "booktitle": "EMNLP-main2024",
        "title": "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements. However, these models still face challenges in medical multimodal capabilities due to limitations in the quantity and quality of medical vision-text data, stemming from data privacy concerns and high annotation costs. While pioneering approaches utilize PubMed\u2019s large-scale, de-identified medical image-text pairs to address these limitations, they often fall short due to inherent data noise. To tackle this, we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in an \u2018unblinded\u2019 capacity to denoise and reformat the data, resulting in the creation of the **PubMedVision** dataset with 1.3 million medical VQA samples. Our validation demonstrates that: (1) PubMedVision can significantly enhance the medical multimodal capabilities of MLLMs, showing significant improvement in benchmarks including the MMMU Health &amp; Medicine track; (2) manual checks by medical experts and empirical results validate the superior data quality of our dataset compared to other data construction methods. Using PubMedVision, we train a 34B medical MLLM **HuatuoGPT-Vision**, which shows superior performance in medical multimodal scenarios among open-source MLLMs. Our code and data are available at https://github.com/FreedomIntelligence/HuatuoGPT-Vision.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.418",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-unveiling",
        "author": "Wang, Yifei and Chen, Yuheng and Wen, Wanting and Sheng, Yu and Li, Linjing and Zeng, Daniel Dajun",
        "booktitle": "EMNLP-main2024",
        "title": "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks. Through an analysis of LLMs\u2019 internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances. Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions. By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation. Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs. Code and data will be available soon.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.420",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-xplainllm",
        "author": "Chen, Zichen and Chen, Jianda and Singh, Ambuj and Sra, Misha",
        "booktitle": "EMNLP-main2024",
        "title": "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success in natural language tasks, yet understanding their reasoning processes remains a significant challenge. We address this by introducing XplainLLM, a dataset accompanying an explanation framework designed to enhance LLM transparency and reliability. Our dataset comprises 24,204 instances where each instance interprets the LLM\u2019s reasoning behavior using knowledge graphs (KGs) and graph attention networks (GAT), and includes explanations of LLMs such as the decoder-only Llama-3 and the encoder-only RoBERTa. XplainLLM also features a framework for generating grounded explanations and the debugger-scores for multidimensional quality analysis. Our explanations include why-choose and why-not-choose components, reason-elements, and debugger-scores that collectively illuminate the LLM\u2019s reasoning behavior. Our evaluations demonstrate XplainLLM\u2019s potential to reduce hallucinations and improve grounded explanation generation in LLMs. XplainLLM is a resource for researchers and practitioners to build trust and verify the reliability of LLM outputs. Our code and dataset are publicly available.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.432",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space": {
        "type": "INPROCEEDINGS",
        "key": "tang-etal-2024-morpheus",
        "author": "Tang, Yihong and Wang, Bo and Zhao, Dongming and Jinxiaojia, Jinxiaojia and Zhangjijun, Zhangjijun and He, Ruifang and Hou, Yuexian",
        "booktitle": "EMNLP-main2024",
        "title": "MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Personalized Dialogue Generation (PDG) aims to create coherent responses according to roles or personas. Traditional PDG relies on external role data, which can be scarce and raise privacy concerns. Approaches address these issues by extracting role information from dialogue history, which often fail to generically model roles in continuous space. To overcome these limitations, we introduce a novel framework Models Roles from Personalized Dialogue History by Exploring and Utilizing Latent Space (MORPHEUS) through a three-stage training process. Specifically, we create a persona codebook to represent roles in latent space compactly, and this codebook is used to construct a posterior distribution of role information. This method enables the model to generalize across roles, allowing the generation of personalized dialogues even for unseen roles. Experiments on both Chinese and English datasets demonstrate that MORPHEUS enhances the extraction of role information, and improves response generation without external role data. Additionally, MORPHEUS can be considered an efficient fine-tuning for large language models.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.437",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-knowledgesg",
        "author": "Wang, WenHao and Liang, Xiaoyu and Ye, Rui and Chai, Jingyi and Chen, Siheng and Wang, Yanfeng",
        "booktitle": "EMNLP-main2024",
        "title": "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The success of large language models (LLMs) facilitate many parties to fine-tune LLMs on their own private data. However, this practice raises privacy concerns due to the memorization of LLMs. Existing solutions, such as utilizing synthetic data for substitution, struggle to simultaneously improve performance and preserve privacy.They either rely on a local model for generation, resulting in a performance decline, or take advantage of APIs, directly exposing the data to API servers. To address this issue, we propose KnowledgeSG, a novel client-server framework which enhances synthetic data quality and improves model performance while ensuring privacy. We achieve this by learning local knowledge from the private data with differential privacy (DP) and distilling professional knowledge from the server. Additionally, inspired by federated learning, we transmit models rather than data between the client and server to prevent privacy leakage.Extensive experiments in medical and financial domains demonstrate the effectiveness of *KnowledgeSG*. Our code is now publicly available at https://github.com/wwh0411/KnowledgeSG.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.438",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination": {
        "type": "INPROCEEDINGS",
        "key": "gong-etal-2024-damro",
        "author": "Gong, Xuan and Ming, Tianshi and Wang, Xinpeng and Wei, Zhihua",
        "booktitle": "EMNLP-main2024",
        "title": "DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Despite the great success of Large Vision-Language Models (LVLMs), they inevitably suffer from hallucination. As we know, both the visual encoder and the Large Language Model (LLM) decoder in LVLMs are Transformer-based, allowing the model to extract visual information and generate text outputs via attention mechanisms. We find that the attention distribution of LLM decoder on image tokens is highly consistent with the visual encoder and both distributions tend to focus on particular background tokens rather than the referred objects in the image. We attribute to the unexpected attention distribution to an inherent flaw in the visual encoder itself, which misguides LLMs to over emphasize the redundant information and generate object hallucination. To address the issue, we propose DAMRO, a novel training-free strategy that **D**ive into **A**ttention **M**echanism of LVLM to **R**educe **O**bject Hallucination. Specifically, our approach employs classification token (CLS) of ViT to filter out high-attention tokens scattered in the background and then eliminate their influence during decoding stage. We evaluate our method on LVLMs including LLaVA-1.5, LLaVA-NeXT and InstructBLIP, using various benchmarks such as POPE, CHAIR, MME and GPT-4V Aided Evaluation. The results demonstrate that our approach significantly reduces the impact of these outlier tokens, thus effectively alleviating the hallucination of LVLMs.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.439",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "men-etal-2024-unlocking",
        "author": "Men, Tianyi and Cao, Pengfei and Jin, Zhuoran and Chen, Yubo and Liu, Kang and Zhao, Jun",
        "booktitle": "EMNLP-main2024",
        "title": "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Planning, as the core module of agents, is crucial in various fields such as embodied agents, web navigation, and tool using. With the development of large language models (LLMs), some researchers treat large language models as intelligent agents to stimulate and evaluate their planning capabilities. However, the planning mechanism is still unclear. In this work, we focus on exploring the look-ahead planning mechanism in large language models from the perspectives of information flow and internal representations. First, we study how planning is done internally by analyzing the multi-layer perception (MLP) and multi-head self-attention (MHSA) components at the last token. We find that the output of MHSA in the middle layers at the last token can directly decode the decision to some extent. Based on this discovery, we further trace the source of MHSA by information flow, and we reveal that MHSA extracts information from spans of the goal states and recent steps. According to information flow, we continue to study what information is encoded within it. Specifically, we explore whether future decisions have been considered in advance in the representation of flow. We demonstrate that the middle and upper layers encode a few short-term future decisions. Overall, our research analyzes the look-ahead planning mechanisms of LLMs, facilitating future research on LLMs performing planning tasks.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.440",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "LawBench: Benchmarking Legal Knowledge of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "fei-etal-2024-lawbench",
        "author": "Fei, Zhiwei and Shen, Xiaoyu and Zhu, Dawei and Zhou, Fengzhe and Han, Zhuo and Huang, Alan and Zhang, Songyang and Chen, Kai and Yin, Zhixin and Shen, Zongwen and Ge, Jidong and Ng, Vincent",
        "booktitle": "EMNLP-main2024",
        "title": "LawBench: Benchmarking Legal Knowledge of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present LawBench, the first evaluation benchmark composed of 20 tasks aimed to assess the ability of Large Language Models (LLMs) to perform Chinese legal-related tasks. LawBench is meticulously crafted to enable precise assessment of LLMs\u2019 legal capabilities from three cognitive levels that correspond to the widely accepted Bloom\u2019s cognitive taxonomy. Using LawBench, we present a comprehensive evaluation of 21 popular LLMs and the first comparative analysis of the empirical results in order to reveal their relative strengths and weaknesses. All data, model predictions and evaluation code are accessible from https://github.com/open-compass/LawBench.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.452",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards": {
        "type": "INPROCEEDINGS",
        "key": "sahinuc-etal-2024-efficient",
        "author": "\u015eahinu\u00e7, Furkan and Tran, Thy Thy and Grishina, Yulia and Hou, Yufang and Chen, Bei and Gurevych, Iryna",
        "booktitle": "EMNLP-main2024",
        "title": "Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Scientific leaderboards are standardized ranking systems that facilitate evaluating and comparing competitive methods. Typically, a leaderboard is defined by a task, dataset, and evaluation metric (TDM) triple, allowing objective performance assessment and fostering innovation through benchmarking. However, the exponential increase in publications has made it infeasible to construct and maintain these leaderboards manually. Automatic leaderboard construction has emerged as a solution to reduce manual labor. Existing datasets for this task are based on the community-contributed leaderboards without additional curation. Our analysis shows that a large portion of these leaderboards are incomplete, and some of them contain incorrect information. In this work, we present SciLead, a manually-curated Scientific Leaderboard dataset that overcomes the aforementioned problems. Building on this dataset, we propose three experimental settings that simulate real-world scenarios where TDM triples are fully defined, partially defined, or undefined during leaderboard construction. While previous research has only explored the first setting, the latter two are more representative of real-world applications. To address these diverse settings, we develop a comprehensive LLM-based framework for constructing leaderboards. Our experiments and analysis reveal that various LLMs often correctly identify TDM triples while struggling to extract result values from publications. We make our code and data publicly available.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.453",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Efficient Vision-Language pre-training via domain-specific learning for human activities": {
        "type": "INPROCEEDINGS",
        "key": "bulat-etal-2024-efficient",
        "author": "Bulat, Adrian and Ouali, Yassine and Guerrero, Ricardo and Martinez, Brais and Tzimiropoulos, Georgios",
        "booktitle": "EMNLP-main2024",
        "title": "Efficient Vision-Language pre-training via domain-specific learning for human activities",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Current Vision-Language (VL) models owe their success to large-scale pre-training on web-collected data, which in turn requires high-capacity architectures and large compute resources for training. We posit that when the downstream tasks are known in advance, which is in practice common, the pretraining process can be aligned to the downstream domain, leading to more efficient and accurate models, while shortening the pretraining step. To this end, we introduce a domain-aligned pretraining strategy that, without additional data collection, improves the accuracy on a domain of interest, herein, that of human activities, while largely preserving the generalist knowledge. At the core of our approach stands a new LLM-based method that, provided with a simple set of concept seeds, produces a concept hierarchy with high coverage of the target domain.The concept hierarchy is used to filter a large-scale web-crawled dataset and, then, enhance the resulting instances with targeted synthetic labels. We study in depth how to train such approaches and their resulting behavior. We further show generalization to video-based data by introducing a fast adaptation approach for transitioning from a static (image) model to a dynamic one (i.e. with temporal modeling). On the domain of interest, our approach significantly outperforms models trained on up to 60\\times more samples and between 10-100\\times shorter training schedules for image retrieval, video retrieval and action recognition. Code will be released.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.454",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "gong-etal-2024-coba",
        "author": "Gong, Zi and Yu, Hang and Liao, Cong and Liu, Bingchang and Chen, Chaoyu and Li, Jianguo",
        "booktitle": "EMNLP-main2024",
        "title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multi-task learning (MTL) benefits the fine-tuning of large language models (LLMs) by providing a single model with improved performance and generalization ability across tasks, presenting a resource-efficient alternative to developing separate models for each task. Yet, existing MTL strategies for LLMs often fall short by either being computationally intensive or failing to ensure simultaneous task convergence. This paper presents CoBa, a new MTL approach designed to effectively manage task convergence balance with minimal computational overhead. Utilizing Relative Convergence Scores (RCS), Absolute Convergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically adjusts task weights during the training process, ensuring that the validation loss of all tasks progress towards convergence at an even pace while mitigating the issue of individual task divergence. The results of our experiments involving three disparate datasets underscore that this approach not only fosters equilibrium in task improvement but enhances the LLMs\u2019 performance by up to 13% relative to the second-best baselines. Code is open-sourced at https://github.com/codefuse-ai/MFTCoder.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.459",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Language-to-Code Translation with a Single Labeled Example": {
        "type": "INPROCEEDINGS",
        "key": "bostrom-etal-2024-language",
        "author": "Bostrom, Kaj and Jhamtani, Harsh and Fang, Hao and Thomson, Sam and Shin, Richard and Xia, Patrick and Van Durme, Benjamin and Eisner, Jason and Andreas, Jacob",
        "booktitle": "EMNLP-main2024",
        "title": "Language-to-Code Translation with a Single Labeled Example",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Tools for translating natural language into code promise natural, open-ended interaction with databases, web APIs, and other software systems. However, this promise is complicated by the diversity and continual development of these systems, each with its own interface and distinct set of features. Building a new language-to-code translator, even starting with a large language model (LM), typically requires annotating a large set of natural language commands with their associated programs. In this paper, we describe ICIP (In-Context Inverse Programming), a method for bootstrapping a language-to-code system using mostly (or entirely) unlabeled programs written using a potentially unfamiliar (but human-readable) library or API. ICIP uses a pre-trained LM to assign candidate natural language descriptions to these programs, then iteratively refines the descriptions to ensure global consistency. Across nine different application domains from the Overnight and Spider benchmarks and text-davinci-003 and CodeLlama-7b-Instruct models, ICIP outperforms a number of prompting baselines. Indeed, in a \u201cnearly unsupervised\u201d setting with only a single annotated program and 100 unlabeled examples, it achieves up to 85% of the performance of a fully supervised system.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.462",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Attribute or Abstain: Large Language Models as Long Document Assistants": {
        "type": "INPROCEEDINGS",
        "key": "buchmann-etal-2024-attribute",
        "author": "Buchmann, Jan and Liu, Xiao and Gurevych, Iryna",
        "booktitle": "EMNLP-main2024",
        "title": "Attribute or Abstain: Large Language Models as Long Document Assistants",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "LLMs can help humans working with long documents, but are known to hallucinate. *Attribution* can increase trust in LLM responses: The LLM provides evidence that supports its response, which enhances verifiability. Existing approaches to attribution have only been evaluated in RAG settings, where the initial retrieval confounds LLM performance. This is crucially different from the long document setting, where retrieval is not needed, but could help. Thus, a long document specific evaluation of attribution is missing. To fill this gap, we present LAB, a benchmark of 6 diverse long document tasks with attribution, and experiments with different approaches to attribution on 5 LLMs of different sizes. We find that *citation*, i.e. response generation and evidence extraction in one step, performs best for large and fine-tuned models, while additional retrieval can help for small, prompted models. We investigate whether the \u201cLost in the Middle\u201d phenomenon exists for attribution, but do not find this. We also find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims. We release code and data for further investigation. [Link](https://github.com/UKPLab/arxiv2024-attribute-or-abstain)",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.463",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-vptq",
        "author": "Liu, Yifei and Wen, Jicheng and Wang, Yang and Ye, Shengyu and Zhang, Li Lyna and Cao, Ting and Li, Cheng and Yang, Mao",
        "booktitle": "EMNLP-main2024",
        "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit.Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables. In this paper, we introduce **Vector Post-Training Quantization (VPTQ)** for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization.We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ.In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model.Our experimental results show that VPTQ reduces model quantization perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, 11-22% on LLaMA-3 on QA tasks on average. We only utilize 10.4-18.6% of the quantization algorithm execution time, resulting in a 1.6-1.8\\times increase in inference throughput compared to SOTA.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.467",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification": {
        "type": "INPROCEEDINGS",
        "key": "sahu-etal-2024-pelican",
        "author": "Sahu, Pritish and Sikka, Karan and Divakaran, Ajay",
        "booktitle": "EMNLP-main2024",
        "title": "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Visual Language Models (LVLMs) struggle with hallucinations in visual instruction following task(s). These issues hinder their trustworthiness and real-world applicability. We propose Pelican \u2013 a novel framework designed to detect and mitigate hallucinations through claim verification. Pelican first decomposes the visual claim into a chain of sub-claims based on first-order predicates. These sub-claims consists of (predicate, question) pairs and can be conceptualized as nodes of a computational graph. We then use use Program-of-Thought prompting to generate Python code for answering these questions through flexible composition of external tools. Pelican improves over prior work by introducing (1) intermediate variables for precise grounding of object instances, and (2) shared computation for answering the sub-question to enable adaptive corrections and inconsistency identification. We finally use reasoning abilities of LLM to verify the correctness of the the claim by considering the consistency and confidence of the (question, answer) pairs from each sub-claim. Our experiments demonstrate consistent performance improvements over various baseline LVLMs and existing hallucination mitigation approaches across several benchmarks.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.470",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "RealVul: Can We Detect Vulnerabilities in Web Applications with LLM?": {
        "type": "INPROCEEDINGS",
        "key": "cao-etal-2024-realvul",
        "author": "Cao, Di and Liao, Yong and Shang, Xiuwei",
        "booktitle": "EMNLP-main2024",
        "title": "RealVul: Can We Detect Vulnerabilities in Web Applications with LLM?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The latest advancements in large language models (LLMs) have sparked interest in their potential for software vulnerability detection. However, there is currently a lack of research specifically focused on vulnerabilities in the PHP language, and challenges in data sampling and processing persist, hindering the model\u2019s ability to effectively capture the characteristics of specific vulnerabilities. In this paper, we present RealVul, the first LLM-based framework designed for PHP vulnerability detection, addressing these issues. By improving code sampling methods and employing normalization techniques, we can isolate potential vulnerability triggers while streamlining the code and eliminating unnecessary semantic information, enabling the model to better understand and learn from the generated vulnerability samples. We also address the issue of insufficient PHP vulnerability samples by improving data synthesis methods. To evaluate RealVul\u2019s performance, we conduct an extensive analysis using five distinct code LLMs on vulnerability data from 180 PHP projects. The results demonstrate a significant improvement in both effectiveness and generalization compared to existing methods, effectively boosting the vulnerability detection capabilities of these models.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.472",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "WPO: Enhancing RLHF with Weighted Preference Optimization": {
        "type": "INPROCEEDINGS",
        "key": "zhou-etal-2024-wpo",
        "author": "Zhou, Wenxuan and Agrawal, Ravi and Zhang, Shujian and Indurthi, Sathish Reddy and Zhao, Sanqiang and Song, Kaiqiang and Xu, Silei and Zhu, Chenguang",
        "booktitle": "EMNLP-main2024",
        "title": "WPO: Enhancing RLHF with Weighted Preference Optimization",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reinforcement learning from human feedback (RLHF) is a promising solution to align large language models (LLMs) more closely with human values. Off-policy preference optimization, where the preference data is obtained from other models, is widely adopted due to its cost efficiency and scalability. However, off-policy preference optimization often suffers from a distributional gap between the policy used for data collection and the target policy, leading to suboptimal optimization. In this paper, we propose a novel strategy to mitigate this problem by simulating on-policy learning with off-policy preference data. Our Weighted Preference Optimization (WPO) method adapts off-policy data to resemble on-policy data more closely by reweighting preference pairs according to their probability under the current policy. This method not only addresses the distributional gap problem but also enhances the optimization process without incurring additional costs. We validate our method on instruction following benchmarks including Alpaca Eval 2 and MT-bench. WPO not only outperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2 but also establishes a remarkable length-controlled winning rate against GPT-4-turbo of 76.7% based on Gemma-2-9b-it. We release the code and models at https://github.com/wzhouad/WPO.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.475",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MetaReflection: Learning Instructions for Language Agents using Past Reflections": {
        "type": "INPROCEEDINGS",
        "key": "gupta-etal-2024-metareflection",
        "author": "Gupta, Priyanshu and Kirtania, Shashank and Singha, Ananya and Gulwani, Sumit and Radhakrishna, Arjun and Soares, Gustavo and Shi, Sherry",
        "booktitle": "EMNLP-main2024",
        "title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The popularity of Large Language Models (LLMs) have unleashed a new age of Language Agents for solving a diverse range of tasks. While contemporary frontier LLMs are capable enough to power reasonably good Language agents, the closed-API model makes it hard to improve in cases they perform sub-optimally. To address this, recent works have explored techniques to improve their performance using self reflection and prompt optimization techniques. While techniques like self reflection work well in an online setup, contemporary prompt optimization techniques are designed to work on simpler tasks. To address this, we introduce METAREFLECTION, a novel offline reinforcement learning technique that enhances the performance of Language Agents by augmenting a semantic memory based on experiential learnings from past trials. We demonstrate the efficacy of METAREFLECTION by evaluating across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, with different agent design. METAREFLECTION boosts Language agents\u2019 performance by 4 % to 16.82 % over the raw GPT-4 baseline and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.477",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "FIRST: Faster Improved Listwise Reranking with Single Token Decoding": {
        "type": "INPROCEEDINGS",
        "key": "gangi-reddy-etal-2024-first",
        "author": "Gangi Reddy, Revanth and Doo, JaeHyeok and Xu, Yifei and Sultan, Md Arafat and Swain, Deevya and Sil, Avirup and Ji, Heng",
        "booktitle": "EMNLP-main2024",
        "title": "FIRST: Faster Improved Listwise Reranking with Single Token Decoding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have significantly advanced the field of information retrieval, particularly for reranking. Listwise LLM rerankers have showcased superior performance and generalizability compared to existing supervised approaches. However, conventional listwise LLM reranking methods lack efficiency as they provide ranking output in the form of a generated ordered sequence of candidate passage identifiers. Further, they are trained with the typical language modeling objective, which treats all ranking errors uniformly\u2013potentially at the cost of misranking highly relevant passages. Addressing these limitations, we introduce FIRST, a novel listwise LLM reranking approach leveraging the output logits of the first generated identifier to directly obtain a ranked ordering of the candidates. Further, we incorporate a learning-to-rank loss during training, prioritizing ranking accuracy for the more relevant passages. Empirical results demonstrate that FIRST accelerates inference by 50% while maintaining a robust ranking performance with gains across the BEIR benchmark. Finally, to illustrate the practical effectiveness of listwise LLM rerankers, we investigate their application in providing relevance feedback for retrievers during inference. Our results show that LLM rerankers can provide a stronger distillation signal compared to cross-encoders, yielding substantial improvements in retriever recall after relevance feedback.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.491",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-jellyfish",
        "author": "Zhang, Haochen and Dong, Yuyang and Xiao, Chuan and Oyamada, Masafumi",
        "booktitle": "EMNLP-main2024",
        "title": "Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format. We instruction-tune local LLMs as universal DP task solvers that operate on a local, single, and low-priced GPU, ensuring data security and enabling further customization. We select a collection of datasets across four representative DP tasks and construct instruction data using data configuration, knowledge injection, and reasoning data distillation techniques tailored to DP. By tuning Mistral-7B, Llama 3-8B, and OpenOrca-Platypus2-13B, our models, Jellyfish-7B/8B/13B, deliver competitiveness compared to GPT-3.5/4 models and strong generalizability to unseen tasks while barely compromising the base models\u2019 abilities in NLP tasks. Meanwhile, Jellyfish offers enhanced reasoning capabilities compared to GPT-3.5. Our models are available at: https://huggingface.co/NECOUDBFM/JellyfishOur instruction dataset is available at: https://huggingface.co/datasets/NECOUDBFM/Jellyfish-Instruct",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.497",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents": {
        "type": "INPROCEEDINGS",
        "key": "tang-etal-2024-minicheck",
        "author": "Tang, Liyan and Laban, Philippe and Durrett, Greg",
        "booktitle": "EMNLP-main2024",
        "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of fact-checking are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to a model to check a single response. In this work, we show how to build small fact-checking models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify datasets from recent work on fact-checking and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.499",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning": {
        "type": "INPROCEEDINGS",
        "key": "wu-etal-2024-beyond",
        "author": "Wu, John and Wu, David and Sun, Jimeng",
        "booktitle": "EMNLP-main2024",
        "title": "Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Medical coding, the translation of unstructured clinical text into standardized medical codes, is a crucial but time-consuming healthcare practice. Though large language models (LLM) could automate the coding process and improve the efficiency of such tasks, interpretability remains paramount for maintaining patient trust. Current efforts in interpretability of medical coding applications rely heavily on label attention mechanisms, which often leads to the highlighting of extraneous tokens irrelevant to the ICD code. To facilitate accurate interpretability in medical language models, this paper leverages dictionary learning that can efficiently extract sparsely activated representations from dense language model embeddings in superposition. Compared with common label attention mechanisms, our model goes beyond token-level representations by building an interpretable dictionary which enhances the mechanistic-based explanations for each ICD code prediction, even when the highlighted tokens are medically irrelevant. We show that dictionary features are human interpretable, can elucidate the hidden meanings of upwards of 90% of medically irrelevant tokens, and steer model behavior.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.500",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "PostMark: A Robust Blackbox Watermark for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chang-etal-2024-postmark",
        "author": "Chang, Yapei and Krishna, Kalpesh and Houmansadr, Amir and Wieting, John Frederick and Iyyer, Mohit",
        "booktitle": "EMNLP-main2024",
        "title": "PostMark: A Robust Blackbox Watermark for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The most effective techniques to detect LLM-generated text rely on inserting a detectable signature\u2014or watermark\u2014during the model\u2019s decoding process. Most existing watermarking methods require access to the underlying LLM\u2019s logits, which LLM API providers are loath to share due to fears of model distillation. As such, these watermarks must be implemented independently by each LLM provider. In this paper, we develop PostMark, a modular post-hoc watermarking procedure in which an input-dependent set of words (determined via a semantic embedding) is inserted into the text after the decoding process has completed. Critically, PostMark does not require logit access, which means it can be implemented by a third party. We also show that PostMark is more robust to paraphrasing attacks than existing watermarking methods: our experiments cover eight baseline algorithms, five base LLMs, and three datasets. Finally, we evaluate the impact of PostMark on text quality using both automated and human assessments, highlighting the trade-off between quality and robustness to paraphrasing. We release our code, outputs, and annotations at https://github.com/lilakk/PostMark.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.506",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction": {
        "type": "INPROCEEDINGS",
        "key": "deng-etal-2024-text",
        "author": "Deng, Zheye and Chan, Chunkit and Wang, Weiqi and Sun, Yuxi and Fan, Wei and Zheng, Tianshi and Yim, Yauwai and Song, Yangqiu",
        "booktitle": "EMNLP-main2024",
        "title": "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The task of condensing large chunks of textual information into concise and structured tables has gained attention recently due to the emergence of Large Language Models (LLMs) and their potential benefit for downstream tasks, such as text summarization and text mining. Previous approaches often generate tables that directly replicate information from the text, limiting their applicability in broader contexts, as text-to-table generation in real-life scenarios necessitates information extraction, reasoning, and integration. However, there is a lack of both datasets and methodologies towards this task. In this paper, we introduce LiveSum, a new benchmark dataset created for generating summary tables of competitions based on real-time commentary texts. We evaluate the performances of state-of-the-art LLMs on this task in both fine-tuning and zero-shot settings, and additionally propose a novel pipeline called T\u00b3(Text-Tuple-Table) to improve their performances. Extensive experimental results demonstrate that LLMs still struggle with this task even after fine-tuning, while our approach can offer substantial performance gains without explicit training. Further analyses demonstrate that our method exhibits strong generalization abilities, surpassing previous approaches on several other text-to-table datasets. Our codeand data can be found at https://github.com/HKUST-KnowComp/LiveSum.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.523",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding": {
        "type": "INPROCEEDINGS",
        "key": "fan-etal-2024-read",
        "author": "Fan, Yue and Ding, Lei and Kuo, Ching-Chen and Jiang, Shan and Zhao, Yang and Guan, Xinze and Yang, Jie and Zhang, Yi and Wang, Xin Eric",
        "booktitle": "EMNLP-main2024",
        "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Graphical User Interfaces (GUIs) are central to our interaction with digital devices and growing efforts have been made to build models for various GUI understanding tasks. However, these efforts largely overlook an important GUI-referring task: screen reading based on user-indicated points, which we name the Screen Point-and-Read (ScreenPR) task. Currently, this task is predominantly handled by rigid accessible screen reading tools, in great need of new models driven by advancements in Multimodal Large Language Models (MLLMs). In this paper, we propose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism, to address the ScreenPR task. Based on the input point coordinate and the corresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout Tree. Based on the tree, our ToL agent not only comprehends the content of the indicated area but also articulates the layout and spatial relationships between elements. Such layout information is crucial for accurately interpreting information on the screen, distinguishing our ToL agent from other screen reading tools. We also thoroughly evaluate the ToL agent against other baselines on a newly proposed ScreenPR benchmark, which includes GUIs from mobile, web, and operating systems. Last but not least, we test the ToL agent on mobile GUI navigation tasks, demonstrating its utility in identifying incorrect actions along the path of agent execution trajectories. Code and data: https://screen-point-and-read.github.io.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.533",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-unlocking",
        "author": "Wang, Zhepeng and Bao, Runxue and Wu, Yawen and Taylor, Jackson and Xiao, Cao and Zheng, Feng and Jiang, Weiwen and Gao, Shangqian and Zhang, Yanfu",
        "booktitle": "EMNLP-main2024",
        "title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Pretrained large language models (LLMs) have excelled in a variety of natural language processing (NLP) tasks, including summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize training data, leading to potential privacy breaches and copyright infringement. Therefore, accurate measurement of the memorization is essential to evaluate and mitigate these potential risks. However, previous attempts to characterize memorization are constrained by either using prefixes only or by prepending a constant soft prompt to the prefixes, which cannot react to changes in input. To address this challenge, we propose a novel method for estimating LLM memorization using dynamic, prefix-dependent soft prompts. Our approach involves training a transformer-based generator to produce soft prompts that adapt to changes in input, thereby enabling more accurate extraction of memorized data. Our method not only addresses the limitations of previous methods but also demonstrates superior performance in diverse experimental settings compared to state-of-the-art techniques. In particular, our method can achieve the maximum relative improvement of 135.3% and 39.8% over the vanilla baseline on average in terms of *discoverable memorization rate* for the text generation task and code generation task, respectively. Our code is available at https://github.com/wangger/llm-memorization-dsp.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.546",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction": {
        "type": "INPROCEEDINGS",
        "key": "zhang-soh-2024-extract",
        "author": "Zhang, Bowen and Soh, Harold",
        "booktitle": "EMNLP-main2024",
        "title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that, in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schemas easily exceed the LLMs\u2019 context window length. Furthermore, there are scenarios where a fixed pre-defined schema is not available and we would like the method to construct a high-quality KG with a succinct self-generated schema. To address these problems, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs\u2019 extraction performance in a retrieval-augmented generation-like manner. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works. Code for EDC is available at https://github.com/clear-nus/edc.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.548",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation": {
        "type": "INPROCEEDINGS",
        "key": "tang-etal-2024-scoi",
        "author": "Tang, Chenming and Wang, Zhixiang and Wu, Yunfang",
        "booktitle": "EMNLP-main2024",
        "title": "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In-context learning (ICL) greatly improves the performance of large language models (LLMs) on various down-stream tasks, where the improvement highly depends on the quality of demonstrations. In this work, we introduce syntactic knowledge to select better in-context examples for machine translation (MT). We propose a new strategy, namely Syntax-augmented COverage-based In-context example selection (SCOI), leveraging the deep syntactic structure beyond conventional word matching. Specifically, we measure the set-level syntactic coverage by computing the coverage of polynomial terms with the help of a simplified tree-to-polynomial algorithm, and lexical coverage using word overlap. Furthermore, we devise an alternate selection approach to combine both coverage measures, taking advantage of syntactic and lexical information. We conduct experiments with two multi-lingual LLMs on six translation directions. Empirical results show that our proposed SCOI obtains the highest average COMET score among all learning-free methods, indicating that combining syntactic and lexical coverage successfully helps to select better in-context examples for MT. Our code is available at https://github.com/JamyDon/SCOI.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.555",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-efficient",
        "author": "Wang, Yuxuan and Wang, Yueqian and Wu, Pengfei and Liang, Jianxin and Zhao, Dongyan and Liu, Yang and Zheng, Zilong",
        "booktitle": "EMNLP-main2024",
        "title": "Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Despite progress in multimodal large language models (MLLMs), the challenge of interpreting long-form videos in response to linguistic queries persists, largely due to the inefficiency in temporal grounding and limited pre-trained context window size. In this work, we introduce Temporal Grounding Bridge (TGB), a novel framework that bootstraps MLLMs with advanced temporal grounding capabilities and broadens their contextual scope. Our framework significantly enhances the temporal capabilities of current MLLMs through three key innovations: an efficient multi-span temporal grounding algorithm applied to low-dimension temporal features projected from flow; a multimodal length extrapolation training paradigm that utilizes low-dimension temporal features to extend the training context window size; and a bootstrapping framework that bridges our model with pluggable MLLMs without requiring annotation. We validate TGB across seven video benchmarks and demonstrate substantial performance improvements compared with prior MLLMs. Notably, our model, initially trained on sequences of four frames, effectively handles sequences up to 16 longer without sacrificing performance, highlighting its scalability and effectiveness in real-world applications. Our code is publicly available.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.556",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions": {
        "type": "INPROCEEDINGS",
        "key": "rao-etal-2024-commonit",
        "author": "Rao, Jun and Liu, Xuebo and Lian, Lian and Cheng, Shengjun and Liao, Yunjie and Zhang, Min",
        "booktitle": "EMNLP-main2024",
        "title": "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With instruction tuning, Large Language Models (LLMs) can enhance their ability to adhere to commands. Diverging from most works focusing on data mixing, our study concentrates on enhancing the model\u2019s capabilities from the perspective of data sampling during training. Drawing inspiration from the human learning process, where it is generally easier to master solutions to similar topics through focused practice on a single type of topic, we introduce a novel instruction tuning strategy termed CommonIT: Commonality-aware Instruction Tuning. Specifically, we cluster instruction datasets into distinct groups with three proposed metrics Task, Embedding and Length). We ensure each training mini-batch, or \u201cpartition\u201d, consists solely of data from a single group, which brings about both data randomness across mini-batches and intra-batch data similarity. Rigorous testing on LLaMa models demonstrates CommonIT\u2019s effectiveness in enhancing the instruction-following capabilities of LLMs through IT datasets (FLAN, CoT, and Alpaca) and models (LLaMa2-7B, Qwen2-7B, LLaMa 13B, and BLOOM 7B). CommonIT consistently boosts an average improvement of 2.1% on the general domain (i.e., the average score of Knowledge, Reasoning, Multilinguality and Coding) with the Length metric, and 5.2% on the special domain (i.e., GSM, Openfunctions and Code) with the Task metric, and 3.8% on the specific tasks (i.e., MMLU) with the Embedding metric. Code is available at https://github.com/raojay7/CommonIT.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.561",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance": {
        "type": "INPROCEEDINGS",
        "key": "khanuja-etal-2024-image",
        "author": "Khanuja, Simran and Ramamoorthy, Sathyanarayanan and Song, Yueqi and Neubig, Graham",
        "booktitle": "EMNLP-main2024",
        "title": "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we introduce a new task of translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset \u2013 (i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image; and (ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our project webpage is here: https://machine-transcreation.github.io/image-transcreation and our code, data and model outputs can be found here: https://github.com/simran-khanuja/image-transcreation.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.573",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "KNN-Instruct: Automatic Instruction Construction with K Nearest Neighbor Deduction": {
        "type": "INPROCEEDINGS",
        "key": "kou-etal-2024-knn",
        "author": "Kou, Jianshang and Xu, Benfeng and Zhu, Chiwei and Mao, Zhendong",
        "booktitle": "EMNLP-main2024",
        "title": "KNN-Instruct: Automatic Instruction Construction with K Nearest Neighbor Deduction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Supervised fine-tuning (SFT) is a critical procedure for aligning large language models. Despite its efficiency, the construction of SFT data often struggles with issues of quality, diversity, and scalability. Many existing methods, inspired by the Self-Instruct framework, typically generate synthetic instructions by prompting aligned proprietary models like ChatGPT. However, such process suffers from stale distribution, resulting in instructions that are merely trivial variations of existing ones. In this paper, we introduce a novel bootstrapping approach termed KNN-Instruct, which incorporates KNN deduction to produce meaningful new instructions by effectively summarizing and learning from similar existing ones. We conduct an economical controlled experiment to preliminarily validate its effectiveness. In the further experiment, we construct a high-quality SFT dataset named KNN-Inst-12k*. Applying the dataset to Qwen-2-7B, we get a MT-Bench score of 7.64, which outperforms all 7B models on the LMSYS leaderboard, including Starling-LM-7B (7.48), OpenChat-3.5 (7.06) and Zephyr-7B-beta (6.53). Our code and data are available at https://github.com/CrossmodalGroup/KNN-Instruct/.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.577",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CARER - ClinicAl Reasoning-Enhanced Representation for Temporal Health Risk Prediction": {
        "type": "INPROCEEDINGS",
        "key": "nguyen-etal-2024-carer",
        "author": "Nguyen, Tuan Dung and Huynh, Thanh Trung and Phan, Minh Hieu and Nguyen, Quoc Viet Hung and Nguyen, Phi Le",
        "booktitle": "EMNLP-main2024",
        "title": "CARER - ClinicAl Reasoning-Enhanced Representation for Temporal Health Risk Prediction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The increasing availability of multimodal data from electronic health records (EHR) has paved the way for deep learning methods to improve diagnosis accuracy. However, deep learning models are data-driven, requiring large-scale datasets to achieve high generalizability. Inspired by how human experts leverage reasoning for medical diagnosis, we propose CARER, a novel health risk prediction framework, that enhances deep learning models with clinical rationales derived from medically proficient Large Language Models (LLMs). In addition, we provide a cross-view alignment loss which aligns the \u201clocal\u201d view from the patient\u2019s health status with the \u201cglobal\u201d view from the external LLM\u2019s clinical reasoning to boost the mutual feature learning. Through extensive experiments on two predictive tasks using two popular EHR datasets, our CARER\u2019s significantly exceeds the performance of state-of-the-art models by up to 11.2%, especially in improving data efficiency and generalizability. Our code is available at https://github.com/tuandung2812/CARER-EMNLP-2024",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.580",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles": {
        "type": "INPROCEEDINGS",
        "key": "louie-etal-2024-roleplay",
        "author": "Louie, Ryan and Nandi, Ananjan and Fang, William and Chang, Cheng and Brunskill, Emma and Yang, Diyi",
        "booktitle": "EMNLP-main2024",
        "title": "Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent works leverage LLMs to roleplay realistic social scenarios, aiding novices in practicing their social skills. However, simulating sensitive interactions, such as in the domain of mental health, is challenging. Privacy concerns restrict data access, and collecting expert feedback, although vital, is laborious. To address this, we develop Roleplay-doh, a novel human-LLM collaboration pipeline that elicits qualitative feedback from a domain-expert, which is transformed into a set of principles, or natural language rules, that govern an LLM-prompted roleplay. We apply this pipeline to enable senior mental health supporters to create customized AI patients as simulated practice partners for novice counselors. After uncovering issues with basic GPT-4 simulations not adhering to expert-defined principles, we also introduce a novel principle-adherence prompting pipeline which shows a 30% improvement in response quality and principle following for the downstream task. Through a user study with 25 counseling experts, we demonstrate that the pipeline makes it easy and effective to create AI patients that more faithfully resemble real patients, as judged by both creators and third-party counselors. We provide access to the code and data on our project website: https://roleplay-doh.github.io/.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.591",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA": {
        "type": "INPROCEEDINGS",
        "key": "jian-etal-2024-large",
        "author": "Jian, Pu and Yu, Donglei and Zhang, Jiajun",
        "booktitle": "EMNLP-main2024",
        "title": "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Visual question answering (VQA) tasks, often performed by visual language model (VLM), face challenges with long-tail knowledge. Recent retrieval-augmented VQA (RA-VQA) systems address this by retrieving and integrating external knowledge sources. However, these systems still suffer from redundant visual information irrelevant to the question during retrieval. To address these issues, in this paper, we propose LLM-RA, a novel method leveraging the reasoning capability of a large language model (LLM) to identify key visual entities, thus minimizing the impact of irrelevant information in the query of retriever. Furthermore, key visual entities are independently encoded for multimodal joint retrieval, preventing cross-entity interference. Experimental results demonstrate that our method outperforms other strong RA-VQA systems. In two knowledge-intensive VQA benchmarks, our method achieves the new state-of-the-art performance among those with similar scale of parameters and even performs comparably to models with 1-2 orders larger parameters.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.613",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models": {
        "type": "INPROCEEDINGS",
        "key": "cao-etal-2024-vleu",
        "author": "Cao, Jingtao and Zheng, Zhang and Wang, Hongru and Wong, Kam-Fai",
        "booktitle": "EMNLP-main2024",
        "title": "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Progress in Text-to-Image (T2I) models has significantly advanced the generation of images from textual descriptions. Existing metrics, such as CLIP, effectively measure the semantic alignment between single prompts and their corresponding images. However, they fall short in evaluating a model\u2019s ability to generalize across a broad spectrum of textual inputs. To address this gap, we propose the VLEU (Visual Language Evaluation Understudy) metric. VLEU leverages the power of Large Language Models (LLMs) to sample from the visual text domain, encompassing the entire range of potential inputs for the T2I task, to generate a wide variety of visual text. The images generated by T2I models from these prompts are then assessed for their alignment with the input text using the CLIP model. VLEU quantitatively measures a model\u2019s generalizability by computing the Kullback-Leibler (KL) divergence between the visual text marginal distribution and the conditional distribution over the images generated by the model. This provides a comprehensive metric for comparing the overall generalizability of T2I models, beyond single-prompt evaluations, and offers valuable insights during the finetuning process. Our experimental results demonstrate VLEU\u2019s effectiveness in evaluating the generalizability of various T2I models, positioning it as an essential metric for future research and development in image synthesis from text prompts. Our code and data will be publicly available at https://github.com/mio7690/VLEU.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.618",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation": {
        "type": "INPROCEEDINGS",
        "key": "saha-srihari-2024-integrating",
        "author": "Saha, Sougata and Srihari, Rohini",
        "booktitle": "EMNLP-main2024",
        "title": "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The proliferation of online misinformation presents a significant challenge, requiring scalable strategies for effective mitigation. While detection methods exist, current reactive approaches, like content flagging and banning, are short-term and insufficient. Additionally, advancements like large language models (LLMs) exacerbate the issue by enabling large-scale creation and dissemination of misinformation. Thus, sustainable, scalable solutions that encourage behavior change and broaden perspectives by persuading misinformants against their viewpoints or broadening their perspectives are needed. To this end, we propose persuasive LLM-based dialogue systems to tackle misinformation. However, challenges arise due to the lack of suitable datasets and formal frameworks for generating persuasive responses. Inspired by existing methods for countering online hate speech, we explore adapting counter-hate response strategies for misinformation. Since misinformation and hate speech often coexist despite differing intentions, we develop classifiers to identify and annotate response strategies from hate-speech counter-responses for use in misinformation scenarios. Human evaluations show a 91% agreement on the applicability of these strategies to misinformation. Next, as a scalable counter-misinformation solution, we create an LLM-based argument graph framework that generates persuasive responses, using the strategies as control codes to adjust the style and content. Human evaluations and case studies demonstrate that our framework generates expert-like responses and is 14% more engaging, 21% more natural, and 18% more factual than the best available alternatives.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.622",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs": {
        "type": "INPROCEEDINGS",
        "key": "puerto-etal-2024-code",
        "author": "Puerto, Haritz and Tutek, Martin and Aditya, Somak and Zhu, Xiaodan and Gurevych, Iryna",
        "booktitle": "EMNLP-main2024",
        "title": "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reasoning is a fundamental component of language understanding. Recent prompting techniques, such as chain of thought, have consistently improved LLMs\u2019 performance on various reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs in the inference stage. In this paper, we investigate the effect of the input representation on the reasoning abilities of LLMs. We hypothesize that representing natural language tasks as code can enhance specific reasoning abilities such as entity tracking or logical reasoning. To study this, we propose code prompting, a methodology we operationalize as a chain of prompts that transforms a natural language problem into code and directly prompts the LLM using the generated code without resorting to external code execution. We find that code prompting exhibits a high-performance boost for multiple LLMs (up to 22.52 percentage points on GPT 3.5, 7.75 on Mixtral, and 16.78 on Mistral) across multiple conditional reasoning datasets. We then conduct comprehensive experiments to understand how the code representation triggers reasoning abilities and which capabilities are elicited in the underlying models. Our analysis on GPT 3.5 reveals that the code formatting of the input problem is essential for performance improvement. Furthermore, the code representation improves sample efficiency of in-context learning and facilitates state tracking of entities.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.629",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CodeAgent: Autonomous Communicative Agents for Code Review": {
        "type": "INPROCEEDINGS",
        "key": "tang-etal-2024-codeagent",
        "author": "Tang, Xunzhu and Kim, Kisub and Song, Yewei and Lothritz, Cedric and Li, Bei and Ezzini, Saad and Tian, Haoye and Klein, Jacques and Bissyand\u00e9, Tegawend\u00e9 F.",
        "booktitle": "EMNLP-main2024",
        "title": "CodeAgent: Autonomous Communicative Agents for Code Review",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code review, which aims at ensuring the overall quality and reliability of software, is a cornerstone of software development. Unfortunately, while crucial, Code review is a labor-intensive process that the research community is looking to automate. Existing automated methods rely on single input-output generative models and thus generally struggle to emulate the collaborative nature of code review. This work introduces CodeAgent, a novel multi-agent Large Language Model (LLM) system for code review automation. CodeAgent incorporates a supervisory agent, QA-Checker, to ensure that all the agents\u2019 contributions address the initial review question. We evaluated CodeAgent on critical code review tasks: (1) detect inconsistencies between code changes and commit messages, (2) identify vulnerability introductions, (3) validate code style adherence, and (4) suggest code revisions. The results demonstrate CodeAgent\u2019s effectiveness, contributing to a new state-of-the-art in code review automation. Our data and code are publicly available (https://github.com/Daniel4SE/codeagent).",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.632",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback": {
        "type": "INPROCEEDINGS",
        "key": "pesaran-zadeh-etal-2024-text2chart31",
        "author": "Pesaran Zadeh, Fatemeh and Kim, Juyeon and Kim, Jin-Hwa and Kim, Gunhee",
        "booktitle": "EMNLP-main2024",
        "title": "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated strong capabilities across various language tasks, notably through instruction-tuning methods. However, LLMs face challenges in visualizing complex, real-world data through charts and plots. Firstly, existing datasets rarely cover a full range of chart types, such as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning methods do not fully leverage the intricate relationships within rich datasets, including text, code, and figures. To address these challenges, we propose a hierarchical pipeline and a new dataset for chart generation. Our dataset, Text2Chart31, includes 31 unique plot types referring to the Matplotlib library, with 11.1K tuples of descriptions, code, data tables, and plots. Moreover, we introduce a reinforcement learning-based instruction tuning technique for chart generation tasks without requiring human feedback. Our experiments show that this approach significantly enhances the model performance, enabling smaller models to outperform larger open-source models and be comparable to state-of-the-art proprietary models in data visualization tasks.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.640",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-universal",
        "author": "Zhao, Shuai and Jia, Meihuizi and Luu, Anh Tuan and Pan, Fengjun and Wen, Jinming",
        "booktitle": "EMNLP-main2024",
        "title": "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we design a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which can make models behave in alignment with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model\u2019s generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 180B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.642",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Enhancing High-order Interaction Awareness in LLM-based Recommender Model": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-enhancing-high",
        "author": "Wang, Xinfeng and Cui, Jin and Fukumoto, Fumiyo and Suzuki, Yoshimi",
        "booktitle": "EMNLP-main2024",
        "title": "Enhancing High-order Interaction Awareness in LLM-based Recommender Model",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated prominent reasoning capabilities in recommendation tasks by transforming them into text-generation tasks. However, existing approaches either disregard or ineffectively model the user-item high-order interactions. To this end, this paper presents an enhanced LLM-based recommender (ELMRec). We enhance whole-word embeddings to substantially enhance LLMs\u2019 interpretation of graph-constructed interactions for recommendations, without requiring graph pre-training. This finding may inspire endeavors to incorporate rich knowledge graphs into LLM-based recommenders via whole-word embedding. We also found that LLMs often recommend items based on users\u2019 earlier interactions rather than recent ones, and present a reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods, especially achieving a 124.3% to 293.7% improvement over SOTA LLM-based methods in direct recommendations. Our code is available online.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.653",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation": {
        "type": "INPROCEEDINGS",
        "key": "frohmann-etal-2024-segment",
        "author": "Frohmann, Markus and Sterner, Igor and Vuli\u0107, Ivan and Minixhofer, Benjamin and Schedl, Markus",
        "booktitle": "EMNLP-main2024",
        "title": "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, we find that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. We introduce a new model \u2014 Segment any Text (SaT) \u2014 to solve this problem. To enhance robustness, we propose a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, we introduce an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, we introduce architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, we introduce a variant of our model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, our contributions provide a universal approach for segmenting any text. Our method outperforms all baselines \u2014 including strong LLMs \u2014 across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. Our models and code, including documentation, are readily available at https://github.com/segment-any-text/wtpsplit under the MIT license.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.665",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-theoremllama",
        "author": "Wang, Ruida and Zhang, Jipeng and Jia, Yizhen and Pan, Rui and Diao, Shizhe and Pi, Renjie and Zhang, Tong",
        "booktitle": "EMNLP-main2024",
        "title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Proving mathematical theorems using computer-verifiable formal languages like Lean significantly impacts mathematical reasoning. One approach to formal theorem proving involves generating complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs. However, due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data most modern LLMs exhibit suboptimal performance.This scarcity results in a paucity of methodologies for training LLMs and techniques to fully utilize their capabilities in composing formal proofs. To address these challenges, this paper proposes **TheoremLlama**, an end-to-end framework that trains a general-purpose LLM to be a Lean4 expert. **TheoremLlama** includes NL-FL dataset generation and bootstrapping method to obtain aligned dataset, curriculum learning and block training techniques to train the model, and iterative proof writing method to write Lean4 proofs that work together synergistically.Using the dataset generation method in **TheoremLlama**, we provide *Open Bootstrapped Theorems* (OBT), an NL-FL aligned and bootstrapped dataset. Our novel NL-FL bootstrapping method, where NL proofs are integrated into Lean4 code for training datasets, leverages the NL reasoning ability of LLMs for formal reasoning. The **TheoremLlama** framework achieves cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%. Our code, model checkpoints, and the generated dataset is published in GitHub",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.667",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "PANDA: Persona Attributes Navigation for Detecting and Alleviating Overuse Problem in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "kim-etal-2024-panda",
        "author": "Kim, Jinsung and Koo, Seonmin and Lim, Heuiseok",
        "booktitle": "EMNLP-main2024",
        "title": "PANDA: Persona Attributes Navigation for Detecting and Alleviating Overuse Problem in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In the persona-grounded dialogue (PGD) task, it is required not only to respond fluently, but also to ground the attributes according to the current conversation topic properly. However, due to their tendency to overly ground given attributes, LLMs often generate unnatural responses provoked by using attributes that deviate from the flow of the conversation or by exploiting too many attributes at once. We term this phenomenon the *overuse* problem of LLMs. Unfortunately, research devising precise criteria and frameworks to quantitatively verify LLMs\u2019 *overuse* problem is obviously insufficient. To address this issue, we propose **P**ersona **A**ttributes **N**avigation for **D**etecting and **A**lleviating the *overuse* problem (**PANDA**) framework. **PANDA** is the first study to quantify the persona *overuse* problem of LLMs by establishing clear standards of the problem and verifying various LLMs based on them. Moreover, this framework navigates us into understanding persona attributes by introducing diverse and detailed dialogue topics that consider practical conversation situations. We provide insights related to LLMs\u2019 persona attribute *overuse* problem through comprehensive verification and analysis with **PANDA** in the PGD task. Our code and resources can be found at http://github.com/jin62304/PANDA.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.670",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ranaldi-etal-2024-empowering-multi",
        "author": "Ranaldi, Leonardo and Pucci, Giulia and Haddow, Barry and Birch, Alexandra",
        "booktitle": "EMNLP-main2024",
        "title": "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In-context learning methods are popular inference strategies where Large Language Models (LLMs) are elicited to solve a task using provided demonstrations without parameter updates. Among these approaches are the reasoning methods, best exemplified by Chain-of-Thought (CoT) and Program-Aided Language Models (PAL), which elicit LLMs to generate reasoning paths, thus promoting accuracy and attracting increasing attention. However, despite the success of these methods, the ability to deliver multi-step reasoning remains limited to a single language, making it challenging to generalize to other languages and hindering global development.In this work, we propose Cross-lingual Program-Aided Language Models (CrossPAL), a method for aligning reasoning programs across languages. In particular, our method delivers programs as intermediate reasoning steps in different languages through a double-step cross-lingual prompting mechanism inspired by the Program-Aided approach. In addition, we introduce Self-consistent CrossPAL (SCrossPAL) to ensemble different reasoning paths across languages. Our experimental evaluations show that our method significantly outperforms existing prompting methods, reducing the number of interactions and achieving state-of-the-art performance.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.678",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Where Am I From? Identifying Origin of LLM-generated Content": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-identifying",
        "author": "Li, Liying and Bai, Yihan and Cheng, Minhao",
        "booktitle": "EMNLP-main2024",
        "title": "Where Am I From? Identifying Origin of LLM-generated Content",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Generative models, particularly large language models (LLMs), have achieved remarkable success in producing natural and high-quality content. However, their widespread adoption raises concerns regarding copyright infringement, privacy violations, and security risks associated with AI-generated content. To address these concerns, we propose a novel digital forensics framework for LLMs, enabling the tracing of AI-generated content back to its source. This framework embeds a secret watermark directly into the generated output, eliminating the need for model retraining. To enhance traceability, especially for short outputs, we introduce a \u201cdepth watermark\u201d that strengthens the link between content and generator. Our approach ensures accurate tracing while maintaining the quality of the generated content. Extensive experiments across various settings and datasets validate the effectiveness and robustness of our proposed framework.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.681",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories": {
        "type": "INPROCEEDINGS",
        "key": "bogin-etal-2024-super",
        "author": "Bogin, Ben and Yang, Kejuan and Gupta, Shashank and Richardson, Kyle and Bransom, Erin and Clark, Peter and Sabharwal, Ashish and Khot, Tushar",
        "booktitle": "EMNLP-main2024",
        "title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPER aims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub-problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.702",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "PATIENT-\u03c8: Using Large Language Models to Simulate Patients for Training Mental Health Professionals": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-patient",
        "author": "Wang, Ruiyi and Milani, Stephanie and Chiu, Jamie C. and Zhi, Jiayin and Eack, Shaun M. and Labrum, Travis and Murphy, Samuel M. and Jones, Nev and Hardy, Kate V. and Shen, Hong and Fang, Fei and Chen, Zhiyu",
        "booktitle": "EMNLP-main2024",
        "title": "PATIENT-\u03c8: Using Large Language Models to Simulate Patients for Training Mental Health Professionals",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Mental illness remains one of the most critical public health issues. Despite its importance, many mental health professionals highlight a disconnect between their training and actual real-world patient practice. To help bridge this gap, we propose PATIENT-\u03c8, a novel patient simulation framework for cognitive behavior therapy (CBT) training. To build PATIENT-\u03c8, we construct diverse patient cognitive models based on CBT principles and use large language models (LLMs) programmed with these cognitive models to act as a simulated therapy patient. We propose an interactive training scheme, PATIENT-\u03c8-TRAINER, for mental health trainees to practice a key skill in CBT \u2013 formulating the cognitive model of the patient \u2013 through role-playing a therapy session with PATIENT-\u03c8. To evaluate PATIENT-\u03c8, we conducted a comprehensive user study of 13 mental health trainees and 20 experts. The results demonstrate that practice using PATIENT-\u03c8-TRAINER enhances the perceived skill acquisition and confidence of the trainees beyond existing forms of training such as textbooks, videos, and role-play with non-patients. Based on the experts\u2019 perceptions, PATIENT-\u03c8 is perceived to be closer to real patient interactions than GPT-4, and PATIENT-\u03c8-TRAINER holds strong promise to improve trainee competencies. Our code and data are released at https://github.com/ruiyiw/patient-psi.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.711",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-make",
        "author": "Wang, Yixuan and Luo, Xianzhen and Wei, Fuxuan and Liu, Yijun and Zhu, Qingfu and Zhang, Xuanyu and Yang, Qing and Xu, Dongliang and Che, Wanxiang",
        "booktitle": "EMNLP-main2024",
        "title": "Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Existing speculative decoding methods typically require additional model structure and training processes to assist the model for draft token generation. This makes the migration of acceleration methods to the new model more costly and more demanding on device memory. To address this problem, we propose the Make Some Noise (MSN) training framework as a replacement for the supervised fine-tuning stage of the large language model. The training method simply introduces some noise at the input for the model to learn the denoising task. It significantly enhances the parallel decoding capability of the model without affecting the original task capability. In addition, we propose a tree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further improve the inference speed of MSN models. Experiments in both the general and code domains have shown that MSN can improve inference speed by 2.3-2.7x times without compromising model performance. The MSN model also achieves comparable acceleration ratios to the SOTA model with additional model structure on Spec-Bench.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.718",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "UNICORN: A Unified Causal Video-Oriented Language-Modeling Framework for Temporal Video-Language Tasks": {
        "type": "INPROCEEDINGS",
        "key": "xiong-etal-2024-unicorn",
        "author": "Xiong, Yuanhao and Nie, Yixin and Liu, Haotian and Wang, Boxin and Chen, Jun and Jin, Rong and Hsieh, Cho-Jui and Torresani, Lorenzo and Lei, Jie",
        "booktitle": "EMNLP-main2024",
        "title": "UNICORN: A Unified Causal Video-Oriented Language-Modeling Framework for Temporal Video-Language Tasks",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The great success of large language models has encouraged the development of large multimodal models, with a focus on image-language interaction. Despite promising results in various image-language downstream tasks, it is still challenging and unclear how to extend the capabilities of these models to the more complex video domain, especially when dealing with explicit temporal signals. To address the problem in existing large multimodal models, in this paper we adopt visual instruction tuning to build a unified causal video-oriented language modeling framework, named UNICORN. Specifically, we collect a comprehensive dataset under the instruction-following format, and instruction-tune the model accordingly. Experimental results demonstrate that without customized training objectives and intensive pre-training, UNICORN can achieve comparable or better performance on established temporal video-language tasks including moment retrieval, video paragraph captioning and dense video captioning. Moreover, the instruction-tuned model can be used to automatically annotate internet videos with temporally-aligned captions. Compared to commonly used ASR captions, we show that training on our generated captions improves the performance of video-language models on both zero-shot and fine-tuning settings. Source code can be found at https://github.com/xyh97/UNICORN.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.722",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Story Morals: Surfacing value-driven narrative schemas using large language models": {
        "type": "INPROCEEDINGS",
        "key": "hobson-etal-2024-story",
        "author": "Hobson, David G. and Zhou, Haiqi and Ruths, Derek and Piper, Andrew",
        "booktitle": "EMNLP-main2024",
        "title": "Story Morals: Surfacing value-driven narrative schemas using large language models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Stories are not only designed to entertain but encode lessons reflecting their authors\u2019 beliefs about the world. In this paper, we propose a new task of narrative schema labelling based on the concept of \u201cstory morals\u201d to identify the values and lessons conveyed in stories. Using large language models (LLMs) such as GPT-4, we develop methods to automatically extract and validate story morals across a diverse set of narrative genres, including folktales, novels, movies and TV, personal stories from social media and the news. Our approach involves a multi-step prompting sequence to derive morals and validate them through both automated metrics and human assessments. The findings suggest that LLMs can effectively approximate human story moral interpretations and offer a new avenue for computational narrative understanding. By clustering the extracted morals on a sample dataset of folktales from around the world, we highlight the commonalities and distinctiveness of narrative values, providing preliminary insights into the distribution of values across cultures. This work opens up new possibilities for studying narrative schemas and their role in shaping human beliefs and behaviors.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.723",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models": {
        "type": "INPROCEEDINGS",
        "key": "zeng-etal-2024-beear",
        "author": "Zeng, Yi and Sun, Weiyu and Huynh, Tran and Song, Dawn and Li, Bo and Jia, Ruoxi",
        "booktitle": "EMNLP-main2024",
        "title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Safety backdoor attacks in large language models (LLMs) enable harmful behaviors to be stealthily triggered while evading detection during normal interactions. The high dimensionality of the trigger search space and the diverse range of potential malicious behaviors in LLMs make this a critical open problem. This paper presents BEEAR, a novel mitigation method based on a key insight: backdoor triggers induce a uniform drift in the model\u2019s embedding space, irrespective of the trigger\u2019s form or targeted behavior. Leveraging this observation, we introduce a bi-level optimization approach. The inner level identifies universal perturbations to the decoder\u2019s embeddings that steer the model towards defender-defined unwanted behaviors; the outer level fine-tunes the model to reinforce safe behaviors against these perturbations. Our experiments demonstrate the effectiveness of this approach, reducing the success rate of safety backdoor attacks from over 95% to \\textless1% for general harmful behaviors and from 47% to 0% for Sleeper Agents, without compromising the model\u2019s helpfulness. Notably, our method relies only on defender-defined sets of safe and unwanted behaviors without any assumptions about the trigger location or attack mechanism. This work represents the first practical framework to counter safety backdoors in LLMs and provides a foundation for future advancements in AI safety and security.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.732",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "OpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation": {
        "type": "INPROCEEDINGS",
        "key": "mahmud-marculescu-2024-opensep",
        "author": "Mahmud, Tanvir and Marculescu, Diana",
        "booktitle": "EMNLP-main2024",
        "title": "OpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Audio separation in real-world scenarios, where mixtures contain a variable number of sources, presents significant challenges due to limitations of existing models, such as over-separation, under-separation, and dependence on predefined training sources. We propose OpenSep, a novel framework that leverages large language models (LLMs) for automated audio separation, eliminating the need for manual intervention and overcoming source limitations. OpenSep uses textual inversion to generate captions from audio mixtures with off-the-shelf audio captioning models, effectively parsing the sound sources present. It then employs few-shot LLM prompting to extract detailed audio properties of each parsed source, facilitating separation in unseen mixtures. Additionally, we introduce a multi-level extension of the mix-and-separate training framework to enhance modality alignment by separating single source sounds and mixtures simultaneously. Extensive experiments demonstrate OpenSep\u2019s superiority in precisely separating new, unseen, and variable sources in challenging mixtures, outperforming SOTA baseline methods. Code is released at https://github.com/tanvir-utexas/OpenSep.git.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.735",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-ouroboros",
        "author": "Zhao, Weilin and Huang, Yuxiang and Han, Xu and Xu, Wang and Xiao, Chaojun and Zhang, Xinrong and Fang, Yewei and Zhang, Kaihuo and Liu, Zhiyuan and Sun, Maosong",
        "booktitle": "EMNLP-main2024",
        "title": "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Speculative decoding is a widely used method that accelerates the generation process of large language models (LLMs) with no compromise in model performance. It achieves this goal by using an existing smaller model for drafting and then employing the target LLM to verify the draft in a low-cost parallel manner. Under such a drafting-verification framework, drafting efficiency has become a bottleneck in the final speedup of speculative decoding. Therefore, generating longer drafts at less cost can lead to better decoding speedup. To achieve this, we introduce Ouroboros, which can generate draft phrases to parallelize the drafting process and meanwhile lengthen drafts in a training-free manner. The experimental results on various typical text generation tasks show that Ouroboros can achieve speedups of up to 2.4\\times over speculative decoding and 3.9\\times over vanilla decoding, without fine-tuning draft and target models. Code available at https://github.com/thunlp/Ouroboros.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.742",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective": {
        "type": "INPROCEEDINGS",
        "key": "xiao-etal-2024-leverage",
        "author": "Xiao, Teng and Li, Mingxiao and Yuan, Yige and Zhu, Huaisheng and Cui, Chao and Honavar, Vasant G.",
        "booktitle": "EMNLP-main2024",
        "title": "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper introduces a novel generalized self-imitation learning GSIL framework, which effectively and efficiently aligns large language models with offline demonstration data. We develop GSIL by deriving a surrogate objective of imitation learning with density ratio estimates, facilitating the use of self-generated data and optimizing the imitation learning objective with simple classification losses. GSIL eliminates the need for complex adversarial training in standard imitation learning, achieving lightweight and efficient fine-tuning for large language models. In addition, GSIL encompasses a family of offline losses parameterized by a general class of convex functions for density ratio estimation and enables a unified view for alignment with demonstration data. Extensive experiments show that GSIL consistently and significantly outperforms baselines in many challenging benchmarks, such as coding (HuamnEval), mathematical reasoning (GSM8K) and instruction-following benchmark (MT-Bench). Code is public available at https://github.com/tengxiao1/GSIL.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.744",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-da",
        "author": "Huang, Yiming and Luo, Jianwen and Yu, Yan and Zhang, Yitong and Lei, Fangyu and Wei, Yifan and He, Shizhu and Huang, Lifu and Liu, Xiao and Zhao, Jun and Liu, Kang",
        "booktitle": "EMNLP-main2024",
        "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, including Python and SQL, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously designed the evaluation suite to ensure the accuracy and robustness of the evaluation. We developed the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at [link](https://github.com/yiyihum/dabench)",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.748",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Leveraging Context-Aware Prompting for Commit Message Generation": {
        "type": "INPROCEEDINGS",
        "key": "jiang-etal-2024-leveraging-context",
        "author": "Jiang, Zhihua and Chen, Jianwei and Rao, Dongning and Ye, Guanghui",
        "booktitle": "EMNLP-main2024",
        "title": "Leveraging Context-Aware Prompting for Commit Message Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Writing comprehensive commit messages is tedious yet important, because these messages describe changes of code, such as fixing bugs or adding new features. However, most existing methods focus on either only the changed lines or nearest context lines, without considering the effectiveness of selecting useful contexts. On the other hand, it is possible that introducing excessive contexts can lead to noise. To this end, we propose a code model COMMIT (Context-aware prOMpting based comMIt-message generaTion) in conjunction with a code dataset CODEC (COntext and metaData Enhanced Code dataset). Leveraging program slicing, CODEC consolidates code changes along with related contexts via property graph analysis. Further, utilizing CodeT5+ as the backbone model, we train COMMIT via context-aware prompt on CODEC. Experiments show that COMMIT can surpass all compared models including pre-trained language models for code (code-PLMs) such as CommitBART and large language models for code (code-LLMs) such as Code-LlaMa. Besides, we investigate several research questions (RQs), further verifying the effectiveness of our approach. We release the data and code at: https://github.com/Jnunlplab/COMMIT.git.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.749",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-lifelong",
        "author": "Chen, Qizhou and Zhang, Taolin and He, Xiaofeng and Li, Dongyang and Wang, Chengyu and Huang, Longtao and Xue\u2019, Hui",
        "booktitle": "EMNLP-main2024",
        "title": "Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Model editing aims to correct outdated or erroneous knowledge in large language models (LLMs) without the need for costly retraining. Lifelong model editing is the most challenging task that caters to the continuous editing requirements of LLMs. Prior works primarily focus on single or batch editing; nevertheless, these methods fall short in lifelong editing scenarios due to catastrophic knowledge forgetting and the degradation of model performance. Although retrieval-based methods alleviate these issues, they are impeded by slow and cumbersome processes of integrating the retrieved knowledge into the model. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous Prompt lEarning method, to boost editing efficacy and inference efficiency in lifelong learning. RECIPE first converts knowledge statements into short and informative continuous prompts, prefixed to the LLM\u2019s input query embedding, to efficiently refine the response grounded on the knowledge. It further integrates the Knowledge Sentinel (KS) that acts as an intermediary to calculate a dynamic threshold, determining whether the retrieval repository contains relevant knowledge. Our retriever and prompt encoder are jointly trained to achieve editing properties, i.e., reliability, generality, and locality. In our experiments, RECIPE is assessed extensively across multiple LLMs and editing datasets, where it achieves superior editing performance. RECIPE also demonstrates its capability to maintain the overall performance of LLMs alongside showcasing fast editing and inference speed.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.751",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Curriculum Consistency Learning for Conditional Sentence Generation": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-curriculum",
        "author": "Liu, Liangxin and Liu, Xuebo and Lian, Lian and Cheng, Shengjun and Rao, Jun and Yu, Tengfei and Deng, Hexuan and Zhang, Min",
        "booktitle": "EMNLP-main2024",
        "title": "Curriculum Consistency Learning for Conditional Sentence Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Consistency learning (CL) has proven to be a valuable technique for improving the robustness of models in conditional sentence generation (CSG) tasks by ensuring stable predictions across various input data forms. However, models augmented with CL often face challenges in optimizing consistency features, which can detract from their efficiency and effectiveness. To address these challenges, we introduce Curriculum Consistency Learning (CCL), a novel strategy that guides models to learn consistency in alignment with their current capacity to differentiate between features. CCL is designed around the inherent aspects of CL-related losses, promoting task independence and simplifying implementation. Implemented across four representative CSG tasks, including instruction tuning (IT) for large language models and machine translation (MT) in three modalities (text, speech, and vision), CCL demonstrates marked improvements. Specifically, it delivers +2.0 average accuracy point improvement compared with vanilla IT and an average increase of +0.7 in COMET scores over traditional CL methods in MT tasks. Our comprehensive analysis further indicates that models utilizing CCL are particularly adept at managing complex instances, showcasing the effectiveness and efficiency of CCL in improving CSG models. Code and scripts are available at https://github.com/xinxinxing/Curriculum-Consistency-Learning.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.768",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "How Do Your Code LLMs perform? Empowering Code Instruction Tuning with Really Good Data": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-code",
        "author": "Wang, Yejie and He, Keqing and Fu, Dayuan and GongQue, Zhuoma and Xu, Heyang and Chen, Yanxu and Wang, Zhexu and Fu, Yujia and Dong, Guanting and Diao, Muxi and Wang, Jingang and Zhang, Mengdi and Cai, Xunliang and Xu, Weiran",
        "booktitle": "EMNLP-main2024",
        "title": "How Do Your Code LLMs perform? Empowering Code Instruction Tuning with Really Good Data",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we observe Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we find that many datasets suffer from severe data leakage. After cleaning up most of the leaked data, some well-known high-quality datasets perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data pruning strategy for selecting good samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Our experiments show Xcoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.777",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries": {
        "type": "INPROCEEDINGS",
        "key": "biran-etal-2024-hopping",
        "author": "Biran, Eden and Gottesman, Daniela and Yang, Sohee and Geva, Mor and Globerson, Amir",
        "booktitle": "EMNLP-main2024",
        "title": "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) can solve complex multi-step problems, but little is known about how these computations are implemented internally. Motivated by this, we study how LLMs answer multi-hop queries such as \u201cThe spouse of the performer of Imagine is\u201d. These queries require two information extraction steps: a latent one for resolving the first hop (\u201cthe performer of Imagine\u201d) into the bridge entity (John Lennon), and another for resolving the second hop (\u201cthe spouse of John Lennon\u201d) into the target entity (Yoko Ono). Understanding how the latent step is computed internally is key to understanding the overall computation. By carefully analyzing the internal computations of transformer-based LLMs, we discover that the bridge entity is resolved in the early layers of the model. Then, only after this resolution, the two-hop query is solved in the later layers. Because the second hop commences in later layers, there could be cases where these layers no longer encode the necessary knowledge for correctly predicting the answer. Motivated by this, we propose a novel \u201cback-patching\u201d analysis method whereby a hidden representation from a later layer is patched back to an earlier layer. We find that in up to 66% of previously incorrect cases there exists a back-patch that results in the correct generation of the answer, showing that the later layers indeed sometimes lack the needed functionality. Overall our methods and findings open further opportunities for understanding and improving latent reasoning in transformer-based LLMs.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.781",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts": {
        "type": "INPROCEEDINGS",
        "key": "koo-etal-2024-large",
        "author": "Koo, Seonmin and Kim, Jinsung and Jang, YoungJoon and Park, Chanjun and Lim, Heuiseok",
        "booktitle": "EMNLP-main2024",
        "title": "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "As the utilization of Large Language Models (LLMs) becomes more widespread, there is a growing demand for their ability to handle more complex and longer external knowledge across various use cases. Most existing evaluations of the open-ended question answering (ODQA) task, which necessitates the use of external knowledge, focus solely on whether the model provides the correct answer. However, even when LLMs answer correctly, they often fail to provide an obvious source for their responses. Therefore, it is necessary to jointly evaluate and verify the correctness of the answers and the appropriateness of grounded evidence in complex external contexts. To address this issue, we examine the phenomenon of discrepancies in abilities across two distinct tasks\u2014QA and evidence selection\u2014when performed simultaneously, from the perspective of task alignment. To verify LLMs\u2019 task alignment, we introduce a verification framework and resources considering both semantic relevancy and structural diversity of the given long context knowledge. Through extensive experiments and detailed analysis, we provide insights into the task misalignment between QA and evidence selection. Our code and resources will be available upon acceptance.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.783",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Are LLMs Good Zero-Shot Fallacy Classifiers?": {
        "type": "INPROCEEDINGS",
        "key": "pan-etal-2024-llms",
        "author": "Pan, Fengjun and Wu, Xiaobao and Li, Zongrui and Luu, Anh Tuan",
        "booktitle": "EMNLP-main2024",
        "title": "Are LLMs Good Zero-Shot Fallacy Classifiers?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Fallacies are defective arguments with faulty reasoning. Detecting and classifying them is a crucial NLP task to prevent misinformation, manipulative claims, and biased decisions. However, existing fallacy classifiers are limited by the requirement for sufficient labeled data for training, which hinders their out-of-distribution (OOD) generalization abilities. In this paper, we focus on leveraging Large Language Models (LLMs) for zero-shot fallacy classification. To elicit fallacy-related knowledge and reasoning abilities of LLMs, we propose diverse single-round and multi-round prompting schemes, applying different taskspecific instructions such as extraction, summarization, and Chain-of-Thought reasoning. With comprehensive experiments on benchmark datasets, we suggest that LLMs could be potential zero-shot fallacy classifiers. In general, LLMs under single-round prompting schemes have achieved acceptable zeroshot performances compared to the best fullshot baselines and can outperform them in all OOD inference scenarios and some opendomain tasks. Our novel multi-round prompting schemes can effectively bring about more improvements, especially for small LLMs. Our analysis further underlines the future research on zero-shot fallacy classification. Codes and data are available at: https://github.com/panFJCharlotte98/Fallacy_Detection.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.794",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector": {
        "type": "INPROCEEDINGS",
        "key": "cheng-etal-2024-small",
        "author": "Cheng, Xiaoxue and Li, Junyi and Zhao, Xin and Zhang, Hongzhi and Zhang, Fuzheng and Zhang, Di and Gai, Kun and Wen, Ji-Rong",
        "booktitle": "EMNLP-main2024",
        "title": "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Hallucination detection is a challenging task for large language models (LLMs), and existing studies heavily rely on powerful closed-source LLMs such as GPT-4. In this paper, we propose an autonomous LLM-based agent framework, called HaluAgent, which enables relatively smaller LLMs (e.g. Baichuan2-Chat 7B) to actively select suitable tools for detecting multiple hallucination types such as text, code, and mathematical expression. In HaluAgent, we integrate the LLM, multi-functional toolbox, and design a fine-grained three-stage detection framework along with memory mechanism. To facilitate the effectiveness of HaluAgent, we leverage existing Chinese and English datasets to synthesize detection trajectories for fine-tuning, which endows HaluAgent with the capability for bilingual hallucination detection. Extensive experiments demonstrate that only using 2K samples for tuning LLMs, HaluAgent can perform hallucination detection on various types of tasks and datasets, achieving performance comparable to or even higher than GPT-4 without tool enhancements on both in-domain and out-of-domain datasets.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.809",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "pan-etal-2024-dynathink",
        "author": "Pan, Jiabao and Zhang, Yan and Zhang, Chen and Liu, Zuozhu and Wang, Hongwei and Li, Haizhou",
        "booktitle": "EMNLP-main2024",
        "title": "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with complicated problems, while a thorough method, which considers multiple reasoning pathways and verifies each step carefully, results in slower inference. This paper addresses the challenge of enabling LLMs to autonomously select between fast and slow inference methods, thereby optimizing both efficiency and effectiveness. We introduce a dynamic decision-making framework that categorizes tasks into two distinct pathways: \u2018Fast,\u2019 designated for tasks where the LLM quickly identifies a high-confidence solution, and \u2018Slow,\u2019 allocated for tasks that the LLM perceives as complex and for which it has low confidence in immediate solutions as well as requiring more reasoning paths to verify. Experiments on five popular reasoning benchmarks demonstrated the superiority of the DynaThink over baselines. For example, when we compared it to strong COT with self-consistency baseline on the complicated MATH dataset, DynaThink achieved more than 3% increase in accuracy with lower cost. The code will be made available upon publication.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.814",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs": {
        "type": "INPROCEEDINGS",
        "key": "fan-etal-2024-biasalert",
        "author": "Fan, Zhiting and Chen, Ruizhe and Xu, Ruiling and Liu, Zuozhu",
        "booktitle": "EMNLP-main2024",
        "title": "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Evaluating the bias of LLMs becomes more crucial with their rapid development. However, existing evaluation approaches rely on fixed-form outputs and cannot adapt to the flexible open-text generation scenarios of LLMs (e.g., sentence completion and question answering). To address this, we introduce BiasAlert, a plug-and-play tool designed to detect social bias in open-text generations of LLMs. BiasAlert integrates external human knowledge with its inherent reasoning capabilities to detect bias reliably. Extensive experiments demonstrate that BiasAlert significantly outperforms existing state-of-the-art methods like GPT-4-as-Judge in detecting bias. Furthermore, through application studies, we showcase the utility of BiasAlert in reliable LLM fairness evaluation and bias mitigation across various scenarios. Model and code will be publicly released.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.820",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-layer",
        "author": "Zhao, Zheng and Ziser, Yftah and Cohen, Shay B.",
        "booktitle": "EMNLP-main2024",
        "title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Fine-tuning pre-trained large language models (LLMs) on a diverse array of tasks has become a common approach for building models that can solve various natural language processing (NLP) tasks. However, where and to what extent these models retain task-specific knowledge remains largely unexplored. This study investigates the task-specific information encoded in pre-trained LLMs and the effects of instruction tuning on their representations across a diverse set of over 60 NLP tasks. We use a set of matrix analysis tools to examine the differences between the way pre-trained and instruction-tuned LLMs store task-specific information. Our findings reveal that while some tasks are already encoded within the pre-trained LLMs, others greatly benefit from instruction tuning. Additionally, we pinpointed the layers in which the model transitions from high-level general representations to more task-oriented representations. This finding extends our understanding of the governing mechanisms of LLMs and facilitates future research in the fields of parameter-efficient transfer learning and multi-task learning. Our code is available at: https://github.com/zsquaredz/layer_by_layer/",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.847",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "AppBench: Planning of Multiple APIs from Various APPs for Complex User Instruction": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-appbench",
        "author": "Wang, Hongru and Wang, Rui and Xue, Boyang and Xia, Heming and Cao, Jingtao and Liu, Zeming and Pan, Jeff Z. and Wong, Kam-Fai",
        "booktitle": "EMNLP-main2024",
        "title": "AppBench: Planning of Multiple APIs from Various APPs for Complex User Instruction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) can interact with the real world by connecting with versatile external APIs, resulting in better problem-solving and task automation capabilities. Previous research primarily either focuses on APIs with limited arguments from a single source or overlooks the complex dependency relationship between different APIs. However, it is essential to utilize multiple APIs collaboratively from various sources, especially for complex user instructions. In this paper, we introduce MetaBench, the first benchmark to evaluate LLMs\u2019 ability to plan and execute multiple APIs from various sources in order to complete the user\u2019s task. Specifically, we consider two significant challenges in multiple APIs: 1) graph structures: some APIs can be executed independently while others need to be executed one by one, resulting in graph-like execution order; and 2) permission constraints: which source is authorized to execute the API call. We have experimental results on 9 distinct LLMs; e.g., GPT-4o achieves only a 2.0% success rate at the most complex instruction, revealing that the existing state-of-the-art LLMs still cannot perform well in this situation even with the help of in-context learning and finetuning. Our code and data are publicly available at https://github.com/ruleGreen/AppBench.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.856",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-everything",
        "author": "Chen, Zhipeng and Zhou, Kun and Zhao, Xin and Wang, Jingyuan and Wen, Ji-Rong",
        "booktitle": "EMNLP-main2024",
        "title": "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) are still struggling in aligning with human preference in complex tasks and scenarios. They are prone to overfit into the unexpected patterns or superficial styles in the training data. We conduct an empirical study that only selects the top-10% most updated parameters in LLMs for alignment training, and see improvements in the convergence process and final performance. It indicates the existence of redundant neurons in LLMs for alignment training. To reduce its influence, we propose a low-redundant alignment method named **ALLO**, focusing on optimizing the most related neurons with the most useful supervised signals. Concretely, we first identify the neurons that are related to the human preference data by a gradient-based strategy, then identify the alignment-related key tokens by reward models for computing loss. Besides, we also decompose the alignment process into the forgetting and learning stages, where we first forget the tokens with unaligned knowledge and then learn aligned knowledge, by updating different ratios of neurons, respectively. Experimental results on 10 datasets have shown the effectiveness of ALLO. Our code and data will be publicly released.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.857",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?": {
        "type": "INPROCEEDINGS",
        "key": "waghjale-etal-2024-ecco",
        "author": "Waghjale, Siddhant and Veerendranath, Vishruth and Wang, Zhiruo and Fried, Daniel",
        "booktitle": "EMNLP-main2024",
        "title": "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although large language models (LLMs) have been largely successful in generating functionally correct programs, conditioning models to produce efficient solutions while ensuring correctness remains a challenge. Further, unreliability in benchmarking code efficiency is a hurdle across varying hardware specifications for popular interpreted languages such as Python. In this paper, we present ECCO, a reproducible benchmark for evaluating program efficiency via two paradigms: natural language (NL) based code generation and history-based code editing. On ECCO, we adapt and thoroughly investigate the three most promising existing LLM-based approaches: in-context learning, iterative refinement with execution or NL feedback, and fine-tuning conditioned on execution and editing history. While most methods degrade functional correctness and moderately increase program efficiency, we find that adding execution information often helps maintain functional correctness, and NL feedback enhances more on efficiency. We release our benchmark to support future work on LLM-based generation of efficient code.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.859",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Re-Reading Improves Reasoning in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-reading",
        "author": "Xu, Xiaohan and Tao, Chongyang and Shen, Tao and Xu, Can and Xu, Hongbo and Long, Guodong and Lou, Jian-Guang and Ma, Shuai",
        "booktitle": "EMNLP-main2024",
        "title": "Re-Reading Improves Reasoning in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, RE2, i.e., Re-Reading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, RE2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, RE2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, RE2 facilitates a \u201cbidirectional\u201d encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of RE2, illustrating its potential to enable \u201cbidirectional\u201d attention mechanisms. We then evaluate RE2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, RE2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal RE2\u2019s adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.871",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MLLM-Protector: Ensuring MLLM\u2019s Safety without Hurting Performance": {
        "type": "INPROCEEDINGS",
        "key": "pi-etal-2024-mllm",
        "author": "Pi, Renjie and Han, Tianyang and Zhang, Jianshu and Xie, Yueqi and Pan, Rui and Lian, Qing and Dong, Hanze and Zhang, Jipeng and Zhang, Tong",
        "booktitle": "EMNLP-main2024",
        "title": "MLLM-Protector: Ensuring MLLM\u2019s Safety without Hurting Performance",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. This paper investigates the novel challenge of defending MLLMs against such attacks. Compared to large language models (LLMs), MLLMs include an additional image modality. We discover that images act as a \u201cforeign language\u201d that is not considered during safety alignment, making MLLMs more prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover all possible scenarios. This vulnerability is exacerbated by the fact that most state-of-the-art MLLMs are fine-tuned on limited image-text pairs that are much fewer than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during safety fine-tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play strategy that solves two subtasks: 1) identifying harmful responses via a lightweight harm detector, and 2) transforming harmful responses into harmless ones via a detoxifier. This approach effectively mitigates the risks posed by malicious visual inputs without compromising the original performance of MLLMs. Our results demonstrate that MLLM-Protector offers a robust solution to a previously unaddressed aspect of MLLM security.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.895",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Free your mouse! Command Large Language Models to Generate Code to Format Word Documents": {
        "type": "INPROCEEDINGS",
        "key": "rao-etal-2024-free",
        "author": "Rao, Shihao and Li, Liang and Liu, Jiapeng and Weixin, Guan and Gao, Xiyan and Lim, Bing and Ma, Can",
        "booktitle": "EMNLP-main2024",
        "title": "Free your mouse! Command Large Language Models to Generate Code to Format Word Documents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, LLMs have significantly improved code generation, making it increasingly accessible to users. As a result, LLM-powered code generation applications have sprung up, vastly boosting user productivity. This paper mainly explores how to improve the efficiency and experience of users in formatting the document. Specifically, we propose an automatic document formatting method, Text-to-Format, which is driven by various prompting strategies. Text-to-Format takes the user\u2019s formatting instructions and then generates code that can be run in Microsoft Word to format the content in a document. Further, to evaluate automatic document formatting approaches and advance the document formatting task, we built an evaluation specification including a high-quality dataset DocFormEval data, a code runtime environment, and evaluation metrics. Extensive experimental results on data reveal that the prompting strategy\u2019s effect positively correlates with how much knowledge it introduces related to document formatting task. We believe the constructed DocFormEval data and the exploration about Text-to-Format can help developers build more intelligent tools for automatic document formatting, especially in offline scenarios, where the data privacy is the top priority.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.902",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs": {
        "type": "INPROCEEDINGS",
        "key": "han-etal-2024-instinctive",
        "author": "Han, Tianyang and Lian, Qing and Pan, Rui and Pi, Renjie and Zhang, Jipeng and Diao, Shizhe and Lin, Yong and Zhang, Tong",
        "booktitle": "EMNLP-main2024",
        "title": "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from visual illusion. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the visual illusion level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs\u2019 robustness in the presence of misleading images. The code and datasets are available at https://github.com/MasaiahHan/CorrelationQA.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.904",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "LM2: A Simple Society of Language Models Solves Complex Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "juneja-etal-2024-lm2",
        "author": "Juneja, Gurusha and Dutta, Subhabrata and Chakraborty, Tanmoy",
        "booktitle": "EMNLP-main2024",
        "title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in LLM reasoning \u2013 a decomposer generates the subproblems, and a solver solves each of these subproblems. However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) \u2013 the decomposer does not keep track of the ability of the solver to follow the decomposed reasoning. In this paper, we propose LM2 to address these challenges. LM2 modularizes the decomposition, solution, and verification into three different language models. The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the reasoning requirement. The solver model generates the solution to the subproblems that are then checked by the verifier module; depending upon the feedback from the verifier, the reasoning context is constructed using the subproblems and the solutions. These models are trained to coordinate using policy learning. Exhaustive experimentation suggests the superiority of LM2 over existing methods on in- and out-domain reasoning problems, outperforming the best baselines by 8.1% on MATH, 7.71% on JEEBench, and 9.7% on MedQA problems (code available at https://github.com/ LCS2-IIITD/Language_Model_Multiplex).",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.920",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?": {
        "type": "INPROCEEDINGS",
        "key": "he-etal-2024-trust",
        "author": "He, Jianfeng and Yang, Runing and Yu, Linlin and Li, Changbin and Jia, Ruoxi and Chen, Feng and Jin, Ming and Lu, Chang-Tien",
        "booktitle": "EMNLP-main2024",
        "title": "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Text summarization, a key natural language generation (NLG) task, is vital in various domains. However, the high cost of inaccurate summaries in risk-critical applications, particularly those involving human-in-the-loop decision-making, raises concerns about the reliability of uncertainty estimation on text summarization (UE-TS) evaluation methods. This concern stems from the dependency of uncertainty model metrics on diverse and potentially conflicting NLG metrics. To address this issue, we introduce a comprehensive UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The benchmark evaluates the uncertainty estimation capabilities of two large language models and one pre-trained language model on three datasets, with human-annotation analysis incorporated where applicable. We also assess the performance of 14 common uncertainty estimation methods within this benchmark. Our findings emphasize the importance of considering multiple uncorrelated NLG metrics and diverse uncertainty estimation methods to ensure reliable and efficient evaluation of UE-TS techniques. Our code and data are available: https://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.923",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "MedReadMe: A Systematic Study for Fine-grained Sentence Readability in Medical Domain": {
        "type": "INPROCEEDINGS",
        "key": "jiang-xu-2024-medreadme",
        "author": "Jiang, Chao and Xu, Wei",
        "booktitle": "EMNLP-main2024",
        "title": "MedReadMe: A Systematic Study for Fine-grained Sentence Readability in Medical Domain",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Medical texts are notoriously challenging to read. Properly measuring their readability is the first step towards making them more accessible. Here, we present the first systematic study on fine-grained readability measurements in the medical domain, at both sentence-level and span-level. We first introduce a new dataset MedReadMe, which consists of manually annotated readability ratings and fine-grained complex span annotation for 4,520 sentences, featuring two novel \u201cGoogle-Easy\u201d and \u201cGoogle-Hard\u201d categories. It supports our quantitative analysis, which covers 650 linguistic features and additional complex span features, to answer \u201cwhy medical sentences are so hard.\u201d Enabled by our high-quality annotation, we benchmark several state-of-the-art sentence-level readability metrics, including unsupervised, supervised, and prompting-based methods using recently developed large language models (LLMs). Informed by our fine-grained complex span annotation, we find that adding a single feature, capturing the number of jargon spans, into existing readability formulas can significantly improve their correlation with human judgments, and also make them more stable. We will publicly release data and code.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.958",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness": {
        "type": "INPROCEEDINGS",
        "key": "ma-wang-2024-zero",
        "author": "Ma, Shixuan and Wang, Quan",
        "booktitle": "EMNLP-main2024",
        "title": "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The increasing capability and widespread usage of large language models (LLMs) highlight the desirability of automatic detection of LLM-generated text. Zero-shot detectors, due to their training-free nature, have received considerable attention and notable success. In this paper, we identify a new feature, token cohesiveness, that is useful for zero-shot detection, and we demonstrate that LLM-generated text tends to exhibit higher token cohesiveness than human-written text. Based on this observation, we devise TOCSIN, a generic dual-channel detection paradigm that uses token cohesiveness as a plug-and-play module to improve existing zero-shot detectors. To calculate token cohesiveness, TOCSIN only requires a few rounds of random token deletion and semantic difference measurement, making it particularly suitable for a practical black-box setting where the source model used for generation is not accessible. Extensive experiments with four state-of-the-art base detectors on various datasets, source models, and evaluation settings demonstrate the effectiveness and generality of the proposed approach. Code available at: https://github.com/Shixuan-Ma/TOCSIN.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.971",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-llms-mllms",
        "author": "Wang, Siyuan and Long, Zhuohan and Fan, Zhihao and Wei, Zhongyu",
        "booktitle": "EMNLP-main2024",
        "title": "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The rapid development of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has exposed vulnerabilities to various adversarial attacks. This paper provides a comprehensive overview of jailbreaking research targeting both LLMs and MLLMs, highlighting recent advancements in evaluation benchmarks, attack techniques and defense strategies. Compared to the more advanced state of unimodal jailbreaking, multimodal domain remains underexplored. We summarize the limitations and potential research directions of multimodal jailbreaking, aiming to inspire future research and further enhance the robustness and security of MLLMs.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.973",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-pruning",
        "author": "Liu, Deyuan and Qin, Zhanyue and Wang, Hairu and Yang, Zhao and Wang, Zecheng and Rong, Fangying and Liu, Qingbin and Hao, Yanchao and Li, Bo and Chen, Xi and Fan, Cunhang and Lv, Zhao and Chu, Dianhui and Tu, Zhiying and Sui, Dianbo",
        "booktitle": "EMNLP-main2024",
        "title": "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) excel in many domains, their complexity and scale challenge deployment in resource-limited environments. Current compression techniques, such as parameter pruning, often fail to effectively utilize the knowledge from pruned parameters. To address these challenges, we propose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA), a novel approach that uses manifold learning and the Information Bottleneck (IB) measure to merge similar layers, reducing model size while preserving essential performance. We evaluate MKA on multiple benchmark datasets and various LLMs. Our findings show that MKA not only preserves model performance but also achieves substantial compression ratios, outperforming traditional pruning methods. Moreover, when coupled with quantization, MKA delivers even greater compression. Specifically, on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75% with a minimal performance decrease of only 2.82%. The proposed MKA method offers a resource-efficient and performance-preserving model compression technique for LLMs. We make our code available at https://github.com/SempraETY/Pruning-via-Merging",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.987",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Tree of Problems: Improving structured problem solving with compositionality": {
        "type": "INPROCEEDINGS",
        "key": "zebaze-etal-2024-tree",
        "author": "Zebaze, Armel Randy and Sagot, Beno\u00eet and Bawden, Rachel",
        "booktitle": "EMNLP-main2024",
        "title": "Tree of Problems: Improving structured problem solving with compositionality",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across multipletasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition per forms better than CoT on complex reasoning tasks. All code for this paper will be made available.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1001",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning": {
        "type": "INPROCEEDINGS",
        "key": "bandari-etal-2024-c4",
        "author": "Bandari, Abhinav and Yin, Lu and Hsieh, Cheng-Yu and Jaiswal, Ajay Kumar and Chen, Tianlong and Shen, Li and Krishna, Ranjay and Liu, Shiwei",
        "booktitle": "EMNLP-main2024",
        "title": "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Network pruning has emerged as a potential solution to make LLMs cheaper to deploy. However, existing LLM pruning approachesuniversally rely on the C4 dataset as the calibration data for calculating pruning scores, leaving its optimality unexplored. In this study, we evaluate the choice of calibration data on LLM pruning, across a wide range of datasets that are most commonly used in LLM training and evaluation, including four pertaining datasets as well as three categories of downstream tasks encompassing nine datasets. Each downstream dataset is prompted with In-Context Learning (ICL) and Chain-of-Thought (CoT), respectively. Besides the already intriguingobservation that the choice of calibration data significantly impacts the performance of pruned LLMs, our results also uncover several subtle and often unexpected findings, summarized as follows: (1) C4 is not the optimal choice for LLM pruning, even among commonly used pre-training datasets; (2) arithmetic datasets\u2014when used as calibration data\u2014performs on par or even better than pre-training datasets; (3) pruning with downstream datasets does not necessarily help the corresponding downstream task, compared to pre-training data; (4) ICL is widely beneficial to all data categories, whereas CoT is only useful on certain tasks. Our findings shed light on the importance of carefully selecting calibration data for LLM pruning and pave the way for more efficient deployment of these powerfulmodels in real-world applications. We release our code at: https://github.com/abx393/llm-pruning-calibration-data.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1004",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?": {
        "type": "INPROCEEDINGS",
        "key": "uchiyama-etal-2024-programming",
        "author": "Uchiyama, Fumiya and Kojima, Takeshi and Gambardella, Andrew and Cao, Qi and Iwasawa, Yusuke and Matsuo, Yutaka",
        "booktitle": "EMNLP-main2024",
        "title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent large language models (LLMs) have demonstrated remarkable generalization abilities in mathematics and logical reasoning tasks.Prior research indicates that LLMs pre-trained with programming language data exhibit high mathematical and reasoning abilities; however, this causal relationship has not been rigorously tested. Our research aims to verify which programming languages and features during pre-training affect logical inference performance. Specifically, we pre-trained decoder-based language models from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under identical conditions. Thereafter, we evaluated the trained models in a few-shot in-context learning setting on logical reasoning tasks: FLD and bAbi, which do not require commonsense or world knowledge. The results demonstrate that nearly all models trained with programming languages consistently outperform those trained with natural languages, indicating that programming languages contain factors that elicit logic inference performance. In addition, we found that models trained with programming languages exhibit a better ability to follow instructions compared to those trained with natural languages. Further analysis reveals that the depth of Abstract Syntax Trees representing parsed results of programs also affects logical reasoning performance. These findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1008",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "User Inference Attacks on Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "kandpal-etal-2024-user",
        "author": "Kandpal, Nikhil and Pillutla, Krishna and Oprea, Alina and Kairouz, Peter and Choquette-Choo, Christopher A. and Xu, Zheng",
        "booktitle": "EMNLP-main2024",
        "title": "User Inference Attacks on Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Text written by humans makes up the vast majority of the data used to pre-train and fine-tune large language models (LLMs). Many sources of this data\u2014like code, forum posts, personal websites, and books\u2014are easily attributed to one or a few \u201cusers\u201d. In this paper, we ask if it is possible to infer if any of a _user\u2019s_ data was used to train an LLM. Not only would this constitute a breach of privacy, but it would also enable users to detect when their data was used for training. We develop the first effective attacks for _user inference_\u2014at times, with near-perfect success\u2014against LLMs. Our attacks are easy to employ, requiring only black-box access to an LLM and a few samples from the user, which _need not be the ones that were trained on_. We find, both theoretically and empirically, that certain properties make users more susceptible to user inference: being an outlier, having highly correlated examples, and contributing a larger fraction of data. Based on these findings, we identify several methods for mitigating user inference including training with example-level differential privacy, removing within-user duplicate examples, and reducing a user\u2019s contribution to the training data. Though these provide partial mitigation, our work highlights the need to develop methods to fully protect LLMs from user inference.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1014",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback": {
        "type": "INPROCEEDINGS",
        "key": "wu-etal-2024-toolplanner",
        "author": "Wu, Qinzhuo and Liu, Wei and Luan, Jian and Wang, Bin",
        "booktitle": "EMNLP-main2024",
        "title": "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, tool-augmented LLMs have gained increasing attention. Given an instruction, tool-augmented LLMs can interact with various external tools in multiple rounds and provide a final answer. However, previous LLMs were trained on overly detailed instructions, which included API names or parameters, while real users would not explicitly mention these API details. This leads to a gap between trained LLMs and real-world scenarios. In addition, most works ignore whether the interaction process follows the instruction. To address these issues, we constructed a training dataset called MGToolBench, which contains statement and category-level instructions to better reflect real-world scenarios. In addition, we propose ToolPlanner, a two-stage reinforcement learning framework that utilizes path planning and two feedback mechanisms to enhance the LLM\u2019s task completion and instruction-following capabilities. Experimental results show that ToolPlanner significantly improves the Match Rate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA model. Human evaluation verifies that the multi-granularity instructions can better align with users\u2019 usage habits. Our data and code will be released upon acceptance.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1018",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "A Simple and Effective L_2 Norm-Based Strategy for KV Cache Compression": {
        "type": "INPROCEEDINGS",
        "key": "devoto-etal-2024-simple",
        "author": "Devoto, Alessio and Zhao, Yu and Scardapane, Simone and Minervini, Pasquale",
        "booktitle": "EMNLP-main2024",
        "title": "A Simple and Effective L_2 Norm-Based Strategy for KV Cache Compression",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The deployment of large language models (LLMs) is often hindered by the extensive memory requirements of the Key-Value (KV) cache, especially as context lengths increase. Existing approaches to reduce the KV cache size involve either fine-tuning the model to learn a compression strategy or leveraging attention scores to reduce the sequence length. We analyse the attention distributions in decoder-only Transformers-based models and observe that attention allocation patterns stay consistent across most layers. Surprisingly, we find a clear correlation between the L\u2082 norm and the attention scores over cached KV pairs, where a low L\u2082 norm of a key embedding usually leads to a high attention score during decoding. This finding indicates that the influence of a KV pair is potentially determined by the key embedding itself before being queried. Based on this observation, we compress the KV cache based on the L\u2082 norm of key embeddings. Our experimental results show that this simple strategy can reduce the KV cache size by 50% on language modelling and needle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing accuracy. Moreover, without relying on the attention scores, this approach remains compatible with FlashAttention, enabling broader applicability.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1027",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Recurrent Alignment with Hard Attention for Hierarchical Text Rating": {
        "type": "INPROCEEDINGS",
        "key": "lin-etal-2024-recurrent",
        "author": "Lin, Chenxi and Jiayu, Ren and He, Guoxiu and Jiang, Zhuoren and Yu, Haiyan and Zhu, Xiaomin",
        "booktitle": "EMNLP-main2024",
        "title": "Recurrent Alignment with Hard Attention for Hierarchical Text Rating",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) excel at understanding and generating plain text, they are not tailored to handle hierarchical text structures or directly predict task-specific properties such as text rating. In fact, selectively and repeatedly grasping the hierarchical structure of large-scale text is pivotal for deciphering its essence. To this end, we propose a novel framework for hierarchical text rating utilizing LLMs, which incorporates Recurrent Alignment with Hard Attention (RAHA). Particularly, hard attention mechanism prompts a frozen LLM to selectively focus on pertinent leaf texts associated with the root text and generate symbolic representations of their relationships. Inspired by the gradual stabilization of the Markov Chain, recurrent alignment strategy involves feeding predicted ratings iteratively back into the prompts of another trainable LLM, aligning it to progressively approximate the desired target. Experimental results demonstrate that RAHA outperforms existing state-of-the-art methods on three hierarchical text rating datasets. Theoretical and empirical analysis confirms RAHA\u2019s ability to gradually converge towards the underlying target through multiple inferences. Additional experiments on plain text rating datasets verify the effectiveness of this Markov-like alignment. Our data and code can be available in https://github.com/ECNU-Text-Computing/Markov-LLM.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1037",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "DocCGen: Document-based Controlled Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "pimparkhede-etal-2024-doccgen",
        "author": "Pimparkhede, Sameer and Kammakomati, Mehant and Tamilselvam, Srikanth G. and Kumar, Prince and Kumar, Ashok Pon and Bhattacharyya, Pushpak",
        "booktitle": "EMNLP-main2024",
        "title": "DocCGen: Document-based Controlled Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent developments show that Large Language Models (LLMs) produce state-of-the-art performance on natural language (NL) to code generation for resource-rich general-purpose languages like C++, Java, and Python. However, their practical usage for structured domain-specific languages (DSLs) such as YAML, JSON is limited due to domain-specific schema, grammar, and customizations generally unseen by LLMs during pre-training. Efforts have been made to mitigate this challenge via in-context learning through relevant examples or by fine-tuning. However, it suffers from problems, such as limited DSL samples and prompt sensitivity but enterprises maintain good documentation of the DSLs. Therefore, we propose DocCGen, a framework that can leverage such rich knowledge by breaking the NL-to-Code generation task for structured code languages into a two-step process. First, it detects the correct libraries using the library documentation that best matches the NL query. Then, it utilizes schema rules extracted from the documentation of these libraries to constrain the decoding. We evaluate our framework for two complex structured languages, Ansible YAML and Bash command, consisting of two settings: Out-of-domain (OOD) and In domain (ID). Our extensive experiments show that DocCGen consistently improves different sized language models across all six evaluation metrics, reducing syntactic and semantic errors in structured code.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1040",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Model-based Preference Optimization in Abstractive Summarization without Human Feedback": {
        "type": "INPROCEEDINGS",
        "key": "choi-etal-2024-model",
        "author": "Choi, Jaepill and Chae, Kyubyung and Song, Jiwoo and Jo, Yohan and Kim, Taesup",
        "booktitle": "EMNLP-main2024",
        "title": "Model-based Preference Optimization in Abstractive Summarization without Human Feedback",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In abstractive summarization, the challenge of producing concise and accurate summaries arises from the vast amount of information contained in the source document. Consequently, although Large Language Models (LLMs) can generate fluent text, they often introduce inaccuracies by hallucinating content not found in the original source. While supervised fine-tuning methods that maximize likelihood contribute to this issue, they do not consistently enhance the faithfulness of the summaries. Preference-based optimization methods, such as Direct Preference Optimization (DPO), can further refine the model to align with human preferences. However, these methods still heavily depend on costly human feedback. In this work, we introduce a novel and straightforward approach called Model-based Preference Optimization (MPO) to fine-tune LLMs for improved summarization abilities without any human feedback. By leveraging the model\u2019s inherent summarization capabilities, we create a preference dataset that is fully generated by the model using different decoding strategies. Our experiments on standard summarization datasets and various metrics demonstrate that our proposed MPO significantly enhances the quality of generated summaries without relying on human feedback. The code is publicly available at https://github.com/cjaep/MPO.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1048",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "akhauri-etal-2024-shadowllm",
        "author": "Akhauri, Yash and AbouElhamayed, Ahmed F. and Dotzel, Jordan and Zhang, Zhiru and Rush, Alexander M. and Huda, Safeen and Abdelfattah, Mohamed S.",
        "booktitle": "EMNLP-main2024",
        "title": "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated efficiency techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy. Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We develop a novel predictor called ShadowLLM, which can shadow the LLM behavior and enforce better sparsity patterns, resulting in over 15% improvement in end-to-end accuracy compared to prior methods. In addition, ShadowLLM achieves up to a 20% speed-up over the state-of-the-art DejaVu framework. These enhancements are validated on Llama-2 and OPT models with up to 30 billion parameters. Our code is available at https://github.com/abdelfattah-lab/shadow_llm/",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1068",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-multimodal",
        "author": "Zhang, Wenqi and Cheng, Zhenglin and He, Yuanyu and Wang, Mengna and Shen, Yongliang and Tan, Zeqi and Hou, Guiyang and He, Mingqian and Ma, Yanna and Lu, Weiming and Zhuang, Yueting",
        "booktitle": "EMNLP-main2024",
        "title": "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although most current large multimodal models (LMMs) can already understand photos of natural scenes and portraits, their understanding of abstract images, e.g., charts, maps, or layouts, and visual reasoning capabilities remains quite rudimentary. They often struggle with simple daily tasks, such as reading time from a clock, understanding a flowchart, or planning a route using a road map. In light of this, we design a multi-modal self-instruct, utilizing large language models and their code capabilities to synthesize massive abstract images and visual reasoning instructions across daily scenarios. Our strategy effortlessly creates a multimodal benchmark with 11,193 instructions for eight visual scenarios: charts, tables, simulated maps, dashboards, flowcharts, relation graphs, floor plans, and visual puzzles. This benchmark, constructed with simple lines and geometric elements, exposes the shortcomings of most advanced LMMs like GPT-4V and Llava in abstract image understanding, spatial relations reasoning, and visual element induction. Besides, to verify the quality of our synthetic data, we fine-tune an LMM using 62,476 synthetic chart, table and road map instructions. The results demonstrate improved chart understanding and map navigation performance, and also demonstrate potential benefits for other visual reasoning tasks.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1072",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing": {
        "type": "INPROCEEDINGS",
        "key": "he-etal-2024-cocost",
        "author": "He, Xinyi and Zou, Jiaru and Lin, Yun and Zhou, Mengyu and Han, Shi and Yuan, Zejian and Zhang, Dongmei",
        "booktitle": "EMNLP-main2024",
        "title": "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CoCoST framework, which enhances complex code generation by online searching for more information with planned queries and correctness testing for code refinement. Moreover, CoCoST serializes the complex inputs and outputs to improve comprehension and generates test cases to ensure the adaptability for real-world applications. CoCoST is validated through rigorous experiments on the DS-1000 and ClassEval datasets. Experimental results show that CoCoST substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality of LLMs in generating complex code.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1082",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "The Empirical Variability of Narrative Perceptions of Social Media Texts": {
        "type": "INPROCEEDINGS",
        "key": "mire-etal-2024-empirical",
        "author": "Mire, Joel and Antoniak, Maria and Ash, Elliott and Piper, Andrew and Sap, Maarten",
        "booktitle": "EMNLP-main2024",
        "title": "The Empirical Variability of Narrative Perceptions of Social Media Texts",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Most NLP work on narrative detection has focused on prescriptive definitions of stories crafted by researchers, leaving open the questions: how do crowd workers perceive texts to be a story, and why? We investigate this by building StoryPerceptions, a dataset of 2,496 perceptions of storytelling in 502 social media texts from 255 crowd workers, including categorical labels along with free-text storytelling rationales, authorial intent, and more. We construct a fine-grained bottom-up taxonomy of crowd workers\u2019 varied and nuanced perceptions of storytelling by open-coding their free-text rationales. Through comparative analyses at the label and code level, we illuminate patterns of disagreement among crowd workers and across other annotation contexts, including prescriptive labeling from researchers and LLM-based predictions. Notably, plot complexity, references to generalized or abstract actions, and holistic aesthetic judgments (such as a sense of cohesion) are especially important in disagreements. Our empirical findings broaden understanding of the types, relative importance, and contentiousness of features relevant to narrative detection, highlighting opportunities for future work on reader-contextualized models of narrative reception.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1113",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues": {
        "type": "INPROCEEDINGS",
        "key": "sun-etal-2024-revealing",
        "author": "Sun, Lei and Zhao, Jinming and Jin, Qin",
        "booktitle": "EMNLP-main2024",
        "title": "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Personality recognition aims to identify the personality traits implied in user data such as dialogues and social media posts. Current research predominantly treats personality recognition as a classification task, failing to reveal the supporting evidence for the recognized personality. In this paper, we propose a novel task named Explainable Personality Recognition, aiming to reveal the reasoning process as supporting evidence of the personality trait. Inspired by personality theories, personality traits are made up of stable patterns of personality state, where the states are short-term characteristic patterns of thoughts, feelings, and behaviors in a concrete situation at a specific moment in time. We propose an explainable personality recognition framework called Chain-of-Personality-Evidence (CoPE), which involves a reasoning process from specific contexts to short-term personality states to long-term personality traits. Furthermore, based on the CoPE framework, we construct an explainable personality recognition dataset from dialogues, PersonalityEvd. We introduce two explainable personality state recognition and explainable personality trait recognition tasks, which require models to recognize the personality state and trait labels and their corresponding support evidence. Our extensive experiments based on Large Language Models on the two tasks show that revealing personality traits is very challenging and we present some insights for future research. We will release our dataset and source code to facilitate further studies in this direction.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1115",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities": {
        "type": "INPROCEEDINGS",
        "key": "menon-etal-2024-whiteboard",
        "author": "Menon, Sachit and Zemel, Richard and Vondrick, Carl",
        "booktitle": "EMNLP-main2024",
        "title": "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "When presented with questions involving visual thinking, humans naturally switch reasoning modalities, often forming mental images or drawing visual aids. Large language models have shown promising results in arithmetic and symbolic reasoning by expressing intermediate reasoning in text as a chain of thought, yet struggle to extend this capability to answer text queries that are easily solved by visual reasoning, even with extensive multimodal pretraining. We introduce a simple method, whiteboard-of-thought prompting, to unlock the visual reasoning capabilities of multimodal large language models across modalities. Whiteboard-of-thought prompting provides multimodal large language models with a metaphorical \u2018whiteboard\u2019 to draw out reasoning steps as images, then returns these images back to the model for further processing. We find this can be accomplished with no demonstrations or specialized modules, instead leveraging models\u2019 existing ability to write code with libraries such as Matplotlib and Turtle. This simple approach shows state-of-the-art results on four difficult natural language tasks that involve visual and spatial reasoning. We identify multiple settings where GPT-4o using chain-of-thought fails dramatically, including more than one where it achieves 0% accuracy, while whiteboard-of-thought enables up to 92% accuracy in these same settings. We present a detailed exploration of where the technique succeeds as well as its sources of error.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1117",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CodeJudge: Evaluating Code Generation with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "tong-zhang-2024-codejudge",
        "author": "Tong, Weixi and Zhang, Tianyi",
        "booktitle": "EMNLP-main2024",
        "title": "CodeJudge: Evaluating Code Generation with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown promising performance in code generation. However, how to reliably evaluate code generated by LLMs remains an unresolved problem. This paper presents CodeJudge, a code evaluation framework that leverages LLMs to evaluate the semantic correctness of generated code without the need for test cases. We investigate different ways to guide the LLM in performing \u201cslow thinking\u201d to arrive at an in-depth and reliable evaluation. We experimented with four LLMs as evaluators on four code generation datasets and five programming languages. The results show that CodeJudge significantly outperformed existing methods in most settings. Furthermore, compared with a SOTA GPT-3.5-based code evaluation method, CodeJudge achieved better results even when using a much smaller model, Llama-3-8B-Instruct. Our code and datasets are available on GitHub https://github.com/VichyTong/CodeJudge.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1118",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Defending Jailbreak Prompts via In-Context Adversarial Game": {
        "type": "INPROCEEDINGS",
        "key": "zhou-etal-2024-defending",
        "author": "Zhou, Yujun and Han, Yufei and Zhuang, Haomin and Guo, Kehan and Liang, Zhenwen and Bao, Hongyan and Zhang, Xiangliang",
        "booktitle": "EMNLP-main2024",
        "title": "Defending Jailbreak Prompts via In-Context Adversarial Game",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG\u2019s efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover, ICAG demonstrates remarkable transferability to other LLMs, indicating its potential as a versatile defense mechanism. The code is available at https://github.com/YujunZhou/In-Context-Adversarial-Game.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1121",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-cmdcaliper",
        "author": "Huang, Sian-Yao and Yang, Cheng-Lin and Lin, Che-Yu and Huang, Chun-Ying",
        "booktitle": "EMNLP-main2024",
        "title": "CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This research addresses command-line embedding in cybersecurity, a field obstructed by the lack of comprehensive datasets due to privacy and regulation concerns. We propose the first dataset of similar command lines, named CyPHER, for training and unbiased evaluation. The training set is generated using a set of large language models (LLMs) comprising 28,520 similar command-line pairs. Our testing dataset consists of 2,807 similar command-line pairs sourced from authentic command-line data.In addition, we propose a command-line embedding model named CmdCaliper, enabling the computation of semantic similarity with command lines. Performance evaluations demonstrate that the smallest version of CmdCaliper (30 million parameters) suppresses state-of-the-art (SOTA) sentence embedding models with ten times more parameters across various tasks (e.g., malicious command-line detection and similar command-line retrieval).Our study explores the feasibility of data generation using LLMs in the cybersecurity domain. Furthermore, we release our proposed command-line dataset, embedding models\u2019 weights and all program codes to the public. This advancement paves the way for more effective command-line embedding for future researchers.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1126",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?": {
        "type": "INPROCEEDINGS",
        "key": "roccabruna-etal-2024-will",
        "author": "Roccabruna, Gabriel and Rizzoli, Massimo and Riccardi, Giuseppe",
        "booktitle": "EMNLP-main2024",
        "title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The automatic detection of temporal relations among events has been mainly investigated with encoder-only models such as RoBERTa. Large Language Models (LLM) have recently shown promising performance in temporal reasoning tasks such as temporal question answering. Nevertheless, recent studies have tested the LLMs\u2019 performance in detecting temporal relations of closed-source models only, limiting the interpretability of those results. In this work, we investigate LLMs\u2019 performance and decision process in the Temporal Relation Classification task. First, we assess the performance of seven open and closed-sourced LLMs experimenting with in-context learning and lightweight fine-tuning approaches. Results show that LLMs with in-context learning significantly underperform smaller encoder-only models based on RoBERTa. Then, we delve into the possible reasons for this gap by applying explainable methods. The outcome suggests a limitation of LLMs in this task due to their autoregressive nature, which causes them to focus only on the last part of the sequence. Additionally, we evaluate the word embeddings of these two models to better understand their pre-training differences. The code and the fine-tuned models can be found respectively on GitHub.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1136",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Waterfall: Scalable Framework for Robust Text Watermarking and Provenance for LLMs": {
        "type": "INPROCEEDINGS",
        "key": "lau-etal-2024-waterfall",
        "author": "Lau, Gregory Kang Ruey and Niu, Xinyuan and Dao, Hieu and Chen, Jiangwei and Foo, Chuan-Sheng and Low, Bryan Kian Hsiang",
        "booktitle": "EMNLP-main2024",
        "title": "Waterfall: Scalable Framework for Robust Text Watermarking and Provenance for LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Protecting intellectual property (IP) of text such as articles and code is increasingly important, especially as sophisticated attacks become possible, such as paraphrasing by large language models (LLMs) or even unauthorized training of LLMs on copyrighted text to infringe such IP. However, existing text watermarking methods are not robust enough against such attacks nor scalable to millions of users for practical implementation. In this paper, we propose Waterfall, the first training-free framework for robust and scalable text watermarking applicable across multiple text types (e.g., articles, code) and languages supportable by LLMs, for general text and LLM data provenance. Waterfall comprises several key innovations, such as being the first to use LLM as paraphrasers for watermarking along with a novel combination of techniques that are surprisingly effective in achieving robust verifiability and scalability. We empirically demonstrate that Waterfall achieves significantly better scalability, robust verifiability, and computational efficiency compared to SOTA article-text watermarking methods, and also showed how it could be directly applied to the watermarking of code.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1138",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality": {
        "type": "INPROCEEDINGS",
        "key": "qing-etal-2024-alphalora",
        "author": "Qing, Peijun and Gao, Chongyang and Zhou, Yefan and Diao, Xingjian and Yang, Yaoqing and Vosoughi, Soroush",
        "booktitle": "EMNLP-main2024",
        "title": "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), are known to enhance training efficiency in Large Language Models (LLMs). Due to the limited parameters of LoRA, recent studies seek to combine LoRA with Mixture-of-Experts (MoE) to boost performance across various tasks. However, inspired by the observed redundancy in traditional MoE structures, prior studies find that LoRA experts within the MoE architecture also exhibit redundancy, suggesting a need to vary the allocation of LoRA experts across different layers. In this paper, we leverage Heavy-Tailed Self-Regularization (HT-SR) Theory to design a fine-grained allocation strategy. Our analysis reveals that the number of experts per layer correlates with layer training quality, which exhibits significant variability across layers. Based on this, we introduce AlphaLoRA, a theoretically principled and training-free method for allocating LoRA experts to reduce redundancy further. Experiments on three models across ten language processing and reasoning benchmarks demonstrate that AlphaLoRA achieves comparable or superior performance over all baselines. Our code is available at https://github.com/morelife2017/alphalora.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1141",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding": {
        "type": "INPROCEEDINGS",
        "key": "sun-etal-2024-spechub",
        "author": "Sun, Ryan and Zhou, Tianyi and Chen, Xun and Sun, Lichao",
        "booktitle": "EMNLP-main2024",
        "title": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have become essential in advancing natural language processing (NLP) tasks, but their sequential token generation limits inference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising solution by using a smaller draft model to generate multiple token sequences, which the target LLM verifies in parallel.However, current heuristic approaches, such as Recursive Rejection Sampling (RRS), suffer from low acceptance rates in subsequent drafts, limiting the advantages of using multiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can theoretically improve acceptance rates, but its computational cost is too high for real-time use.We present SpecHub, a novel, efficient sampling-verification method for MDSD that improves acceptance rates with only linear computational overhead. By simplifying the OTM problem into a compact Linear Programming model, SpecHub significantly reduces computational complexity. It further accelerates sampling by leveraging a sparse joint distribution, focusing computation on high-probability token sequences.%It integrates seamlessly into existing MDSD frameworks.In extensive experiments, Spechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step than RRS and RRS without replacement. We attach our code at https://github.com/MasterGodzilla/Speculative_decoding_OT.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1148",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Encoding Spreadsheets for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "dong-etal-2024-encoding",
        "author": "Dong, Haoyu and Zhao, Jianbo and Tian, Yuzhang and Xiong, Junyu and Zhou, Mengyu and Lin, Yun and Cambronero, Jos\u00e9 and He, Yeye and Han, Shi and Zhang, Dongmei",
        "booktitle": "EMNLP-main2024",
        "title": "Encoding Spreadsheets for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SheetEncoder, pioneering an efficient encoding method designed to unleash and optimize LLMs\u2019 powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs\u2019 token constraints, making it impractical for most applications. To tackle this challenge, three innovative modules are proposed to compress spreadsheets effectively: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4\u2019s in-context learning setting. Moreover, fine-tuned LLM with SheetEncoder has an average compression ratio of 25\\times, but achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%, demonstrating that SheetEncoder greatly boosts LLMs\u2019s performance on spreadsheet data.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1154",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion": {
        "type": "INPROCEEDINGS",
        "key": "flet-berliac-etal-2024-contrastive",
        "author": "Flet-Berliac, Yannis and Grinsztajn, Nathan and Strub, Florian and Choi, Eugene and Wu, Bill and Cremer, Chris and Ahmadian, Arash and Chandak, Yash and Azar, Mohammad Gheshlaghi and Pietquin, Olivier and Geist, Matthieu",
        "booktitle": "EMNLP-main2024",
        "title": "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reinforcement Learning (RL) has been used to finetune Large Language Models (LLMs) using a reward model trained from preference data, to better align with human judgment. The recently introduced direct alignment methods, which are often simpler, more stable, and computationally lighter, can more directly achieve this. However, these approaches cannot optimize arbitrary rewards, and the preference-based ones are not the only rewards of interest for LLMs (eg, unit tests for code generation or textual entailment for summarization, among others). RL-finetuning is usually done with a variation of policy gradient, which calls for on-policy or near-on-policy samples, requiring costly generations. We introduce *Contrastive Policy Gradient*, or CoPG, a simple and mathematically principled new RL algorithm that can estimate the optimal policy even from off-policy data. It can be seen as an off-policy policy gradient approach that does not rely on important sampling techniques and highlights the importance of using (the right) state baseline. We show this approach to generalize the direct alignment method IPO (identity preference optimization) and classic policy gradient. We experiment with the proposed CoPGon a toy bandit problem to illustrate its properties, as well as for finetuning LLMs on a summarization task, using a learned reward function considered as ground truth for the purpose of the experiments.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1190",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Memory-Efficient Fine-Tuning of Transformers via Token Selection": {
        "type": "INPROCEEDINGS",
        "key": "simoulin-etal-2024-memory",
        "author": "Simoulin, Antoine and Park, Namyong and Liu, Xiaoyi and Yang, Grey",
        "booktitle": "EMNLP-main2024",
        "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Fine-tuning provides an effective means to specialize pre-trained models for various downstream tasks. However, fine-tuning often incurs high memory overhead, especially for large transformer-based models, such as LLMs. While existing methods may reduce certain parts of the memory required for fine-tuning, they still require caching all intermediate activations computed in the forward pass to update weights during the backward pass. In this work, we develop TokenTune, a method to reduce memory usage, specifically the memory to store intermediate activations, in the fine-tuning of transformer-based models. During the backward pass, TokenTune approximates the gradient computation by backpropagating through just a subset of input tokens. Thus, with TokenTune, only a subset of intermediate activations are cached during the forward pass. Also, TokenTune can be easily combined with existing methods like LoRA, further reducing the memory cost. We evaluate our approach on pre-trained transformer models with up to billions of parameters, considering the performance on multiple downstream tasks such as text classification and question answering in a few-shot learning setup. Overall, TokenTune achieves performance on par with full fine-tuning or representative memory-efficient fine-tuning methods, while greatly reducing the memory footprint, especially when combined with other methods with complementary memory reduction mechanisms. We hope that our approach will facilitate the fine-tuning of large transformers, in specializing them for specific domains or co-training them with other neural components from a larger system. Our code is available at https://github.com/facebookresearch/tokentune.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1202",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "A Simple LLM Framework for Long-Range Video Question-Answering": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-simple",
        "author": "Zhang, Ce and Lu, Taixi and Islam, Md Mohaiminul and Wang, Ziyang and Yu, Shoubin and Bansal, Mohit and Bertasius, Gedas",
        "booktitle": "EMNLP-main2024",
        "title": "A Simple LLM Framework for Long-Range Video Question-Answering",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present LLoVi, a simple yet effective **L**anguage-based **Lo**ng-range **Vi**deo question-answering (LVQA) framework. Our method decomposes the short- and long-range modeling aspects of LVQA into two stages. First, we use a short-term visual captioner to generate textual descriptions of short video clips (0.5-8 seconds in length) densely sampled from a long input video. Afterward, an LLM aggregates the densely extracted short-term captions to answer a given question. Furthermore, we propose a novel multi-round summarization prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question. To analyze what makes our simple framework so effective, we thoroughly evaluate various components of our framework. Our empirical analysis reveals that the choice of the visual captioner and LLM is critical for good LVQA performance. The proposed multi-round summarization prompt also leads to a significant LVQA performance boost. Our method achieves the best-reported results on the EgoSchema dataset, best known for very long-form video question-answering. LLoVi also outperforms the previous state-of-the-art by **10.2%** and **6.2%** on NExT-QA and IntentQA for LVQA. Finally, we extend LLoVi to grounded VideoQA, which requires both QA and temporal localization, and show that it outperforms all prior methods on NExT-GQA. Code is available at https://github.com/CeeZh/LLoVi.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1209",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Assessing and Verifying Task Utility in LLM-Powered Applications": {
        "type": "INPROCEEDINGS",
        "key": "arabzadeh-etal-2024-assessing",
        "author": "Arabzadeh, Negar and Huo, Siqing and Mehta, Nikhil and Wu, Qingyun and Wang, Chi and Awadallah, Ahmed Hassan and Clarke, Charles L. A. and Kiseleva, Julia",
        "booktitle": "EMNLP-main2024",
        "title": "Assessing and Verifying Task Utility in LLM-Powered Applications",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The rapid development of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents, assisting humans in their daily tasks. However, a significant gap remains in assessing to what extent LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the need to verify utility of LLM-powered applications, particularly by ensuring alignment between the application\u2019s functionality and end-user needs. We introduce AgentEval, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the effectiveness and robustness of AgentEval for two open source datasets including Math Problem solving and ALFWorld House-hold related tasks. For reproducibility purposes, we make the data, code and all the logs publicly available at https://github.com/Narabzad/AgentEval",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1219",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "The Greatest Good Benchmark: Measuring LLMs\u2019 Alignment with Utilitarian Moral Dilemmas": {
        "type": "INPROCEEDINGS",
        "key": "marraffini-etal-2024-greatest",
        "author": "Marraffini, Giovanni Franco Gabriel and Cotton, Andr\u00e9s and Hsueh, Noe Fabian and Fridman, Axel and Wisznia, Juan and Corro, Luciano Del",
        "booktitle": "EMNLP-main2024",
        "title": "The Greatest Good Benchmark: Measuring LLMs\u2019 Alignment with Utilitarian Moral Dilemmas",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The question of how to make decisions that maximise the well-being of all persons is very relevant to design language models that are beneficial to humanity and free from harm. We introduce the Greatest Good Benchmark to evaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis across 15 diverse LLMs reveals consistently encoded moral preferences that diverge from established moral theories and lay population moral standards. Most LLMs have a marked preference for impartial beneficence and rejection of instrumental harm. These findings showcase the \u2018artificial moral compass\u2019 of LLMs, offering insights into their moral alignment.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1224",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought": {
        "type": "INPROCEEDINGS",
        "key": "kumari-etal-2024-m3hop",
        "author": "Kumari, Gitanjali and Jain, Kirtan and Ekbal, Asif",
        "booktitle": "EMNLP-main2024",
        "title": "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In recent years, there has been a significant rise in the phenomenon of hate against women on social media platforms, particularly through the use of misogynous memes. These memes often target women with subtle and obscure cues, making their detection a challenging task for automated systems. Recently, Large Language Models (LLMs) have shown promising results in reasoning using Chain-of-Thought (CoT) prompting to generate the intermediate reasoning chains as the rationale to facilitate multimodal tasks, but often neglect cultural diversity and key aspects like emotion and contextual knowledge hidden in the visual modalities. To address this gap, we introduce a **M**ultimodal **M**ulti-hop CoT (M3Hop-CoT) framework for **M**isogynous meme identification, combining a CLIP-based classifier and a multimodal CoT module with entity-object-relationship integration. M3Hop-CoT employs a three-step multimodal prompting principle to induce emotions, target awareness, and contextual knowledge for meme analysis. Our empirical evaluation, including both qualitative and quantitative analysis, validates the efficacy of the M3Hop-CoT framework on the SemEval-2022 Task 5 (**MAMI task**) dataset, highlighting its strong performance in the macro-F1 score. Furthermore, we evaluate the model\u2019s generalizability by evaluating it on various benchmark meme datasets, offering a thorough insight into the effectiveness of our approach across different datasets. Codes are available at this link: https://github.com/Gitanjali1801/LLM_CoT",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1234",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation": {
        "type": "INPROCEEDINGS",
        "key": "ramesh-etal-2024-gpt",
        "author": "Ramesh, Govind and Dou, Yao and Xu, Wei",
        "booktitle": "EMNLP-main2024",
        "title": "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Research on jailbreaking has been valuable for testing and understanding the safety and security issues of large language models (LLMs). In this paper, we introduce Iterative Refinement Induced Self-Jailbreak (IRIS), a novel approach that leverages the reflective capabilities of LLMs for jailbreaking with only black-box access. Unlike previous methods, IRIS simplifies the jailbreaking process by using a single model as both the attacker and target. This method first iteratively refines adversarial prompts through self-explanation, which is crucial for ensuring that even well-aligned LLMs obey adversarial instructions. IRIS then rates and enhances the output given the refined prompt to increase its harmfulness. We find that IRIS achieves jailbreak success rates of 98% on GPT-4, 92% on GPT-4 Turbo, and 94% on Llama-3.1-70B in under 7 queries. It significantly outperforms prior approaches in automatic, black-box, and interpretable jailbreaking, while requiring substantially fewer queries, thereby establishing a new standard for interpretable jailbreaking methods.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1235",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records": {
        "type": "INPROCEEDINGS",
        "key": "shi-etal-2024-ehragent",
        "author": "Shi, Wenqi and Xu, Ran and Zhuang, Yuchen and Yu, Yue and Zhang, Jieyu and Wu, Hang and Zhu, Yuanda and Ho, Joyce C. and Yang, Carl and Wang, May Dongmei",
        "booktitle": "EMNLP-main2024",
        "title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Clinicians often rely on data engineers to retrieve complex patient information from electronic health record (EHR) systems, a process that is both inefficient and time-consuming. We propose EHRAgent, a large language model (LLM) agent empowered with accumulative domain knowledge and robust coding capability. EHRAgent enables autonomous code generation and execution to facilitate clinicians in directly interacting with EHRs using natural language. Specifically, we formulate a multi-tabular reasoning task based on EHRs as a tool-use planning process, efficiently decomposing a complex task into a sequence of manageable actions with external toolsets. We first inject relevant medical information to enable EHRAgent to effectively reason about the given query, identifying and extracting the required records from the appropriate tables. By integrating interactive coding and execution feedback, EHRAgent then effectively learns from error messages and iteratively improves its originally generated code. Experiments on three real-world EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate, verifying its strong capacity to tackle complex clinical tasks with minimal demonstrations.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1245",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chae-etal-2024-language",
        "author": "Chae, Hyungjoo and Kim, Yeonghyeon and Kim, Seungone and Ong, Kai Tzu-iunn and Kwak, Beong-woo and Kim, Moohyeon and Kim, Sunghwan and Kwon, Taeyoon and Chung, Jiwan and Yu, Youngjae and Yeo, Jinyoung",
        "booktitle": "EMNLP-main2024",
        "title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Algorithmic reasoning tasks that involve complex logical patterns, such as completing Dyck language, pose challenges for large language models (LLMs), despite their recent success. Prior work has used LLMs to generate programming language and applied external compilers for such tasks. Yet, when on the fly, it is hard to generate an executable code with the correct logic for the solution. Even so, code for one instance cannot be reused for others, although they might require the same logic to solve. We present Think-and-Execute, a novel framework that improves LLMs\u2019 algorithmic reasoning: (1) In Think, we discover task-level logic shared across all instances, and express such logic with pseudocode; (2) In Execute, we tailor the task-level pseudocode to each instance and simulate the execution of it. Think-and-Execute outperforms several strong baselines (including CoT and PoT) in diverse algorithmic reasoning tasks. We manifest the advantage of using task-level pseudocode over generating instance-specific solutions one by one. Also, we show that pseudocode can better improve LMs\u2019 reasoning than natural language (NL) guidance, even though they are trained with NL instructions.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1253",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code": {
        "type": "INPROCEEDINGS",
        "key": "chae-etal-2024-coffee",
        "author": "Chae, Hyungjoo and Kwon, Taeyoon and Moon, Seungjun and Song, Yongho and Kang, Dongjin and Ong, Kai Tzu-iunn and Kwak, Beong-woo and Bae, Seonghyeon and Hwang, Seung-won and Yeo, Jinyoung",
        "booktitle": "EMNLP-main2024",
        "title": "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper presents Coffee-Gym, a comprehensive RL environment for training models that provide feedback on code editing. Coffee-Gym includes two major components: (1) Coffee, a dataset containing humans\u2019 code edit traces for coding questions and human-written feedback for editing erroneous code; (2) CoffeeEval, a reward function that faithfully reflects the helpfulness of feedback by assessing the performance of the revised code in unit tests. With them, Coffee-Gym addresses the unavailability of high-quality datasets for training feedback models with RL, and provides more accurate rewards than the SOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback models that outperform baselines in enhancing open-source code LLMs\u2019 code editing, making them comparable with closed-source LLMs. We make the dataset and the model checkpoint publicly available in https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1254",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Improving Minimum Bayes Risk Decoding with Multi-Prompt": {
        "type": "INPROCEEDINGS",
        "key": "heineman-etal-2024-improving",
        "author": "Heineman, David and Dou, Yao and Xu, Wei",
        "booktitle": "EMNLP-main2024",
        "title": "Improving Minimum Bayes Risk Decoding with Multi-Prompt",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While instruction fine-tuned LLMs are effective text generators, sensitivity to prompt construction makes performance unstable and sub-optimal in practice. Relying on a single \u2018best\u2019 prompt cannot capture all differing approaches to a generation problem. Using this observation, we propose multi-prompt decoding, where many candidate generations are decoded from a prompt bank at inference-time. To ensemble candidates, we use Minimum Bayes Risk (MBR) decoding, which selects a final output using a trained value metric. We show multi-prompt improves MBR across a comprehensive set of conditional generation tasks, and show this is a result of estimating a more diverse and higher quality candidate space than that of a single prompt. Our experiments confirm multi-prompt improves generation across tasks, models and metrics.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1255",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Deciphering Cognitive Distortions in Patient-Doctor Mental Health Conversations: A Multimodal LLM-Based Detection and Reasoning Framework": {
        "type": "INPROCEEDINGS",
        "key": "singh-etal-2024-deciphering",
        "author": "Singh, Gopendra Vikram and Vemulapalli, Sai Vardhan and Firdaus, Mauajama and Ekbal, Asif",
        "booktitle": "EMNLP-main2024",
        "title": "Deciphering Cognitive Distortions in Patient-Doctor Mental Health Conversations: A Multimodal LLM-Based Detection and Reasoning Framework",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Cognitive distortion research holds increasing significance as it sheds light on pervasive errors in thinking patterns, providing crucial insights into mental health challenges and fostering the development of targeted interventions and therapies. This paper delves into the complex domain of cognitive distortions which are prevalent distortions in cognitive processes often associated with mental health issues. Focusing on patient-doctor dialogues, we introduce a pioneering method for detecting and reasoning about cognitive distortions utilizing Large Language Models (LLMs). Operating within a multimodal context encompassing audio, video, and textual data, our approach underscores the critical importance of integrating diverse modalities for a comprehensive understanding of cognitive distortions. By leveraging multimodal information, including audio, video, and textual data, our method offers a nuanced perspective that enhances the accuracy and depth of cognitive distortion detection and reasoning in a zero-shot manner. Our proposed hierarchical framework adeptly tackles both detection and reasoning tasks, showcasing significant performance enhancements compared to current methodologies. Through comprehensive analysis, we elucidate the efficacy of our approach, offering promising insights into the diagnosis and understanding of cognitive distortions in multimodal settings.The code and dataset can be found here: https://github.com/clang1234/ZS-CoDR.git",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1256",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-longrag",
        "author": "Zhao, Qingfei and Wang, Ruobing and Cen, Yukuo and Zha, Daren and Tan, Shicheng and Dong, Yuxiao and Tang, Jie",
        "booktitle": "EMNLP-main2024",
        "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Long-Context Question Answering (LCQA), a challenging task, aims to reason over long-context documents to yield accurate answers to questions. Existing long-context Large Language Models (LLMs) for LCQA often struggle with the \u201clost in the middle\u201d issue. Retrieval-Augmented Generation (RAG) mitigates this issue by providing external factual evidence. However, its chunking strategy disrupts the global long-context information, and its low-quality retrieval in long contexts hinders LLMs from identifying effective factual details due to substantial noise. To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG\u2019s understanding of complex long-context knowledge (i.e., global information and factual details). We design LongRAG as a plug-and-play paradigm, facilitating adaptation to various domains and LLMs. Extensive experiments on three multi-hop datasets demonstrate that LongRAG significantly outperforms long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%). Furthermore, we conduct quantitative ablation studies and multi-dimensional analyses, highlighting the effectiveness of the system\u2019s components and fine-tuning strategies.Data and code are available at [https://github.com/QingFei1/LongRAG](https://github.com/QingFei1/LongRAG).",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1259",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "\u2018Quis custodiet ipsos custodes?\u2019 Who will watch the watchmen? On Detecting AI-generated peer-reviews": {
        "type": "INPROCEEDINGS",
        "key": "kumar-etal-2024-quis",
        "author": "Kumar, Sandeep and Sahu, Mohit and Gacche, Vardhan and Ghosal, Tirthankar and Ekbal, Asif",
        "booktitle": "EMNLP-main2024",
        "title": "\u2018Quis custodiet ipsos custodes?\u2019 Who will watch the watchmen? On Detecting AI-generated peer-reviews",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The integrity of the peer-review process is vital for maintaining scientific rigor and trust within the academic community. With the steady increase in the usage of large language models (LLMs) like ChatGPT in academic writing, there is a growing concern that AI-generated texts could compromise the scientific publishing including peer-reviews. Previous works have focused on generic AI-generated text detection or have presented an approach for estimating the fraction of peer-reviews that can be AI-generated. Our focus here is to solve a real-world problem by assisting the editor or chair in determining whether a review is written by ChatGPT or not. To address this, we introduce the Term Frequency (TF) model, which posits that AI often repeats tokens, and the Review Regeneration (RR) model which is based on the idea that ChatGPT generates similar outputs upon re-prompting. We stress test these detectors against token attack and paraphrasing. Finally we propose an effective defensive strategy to reduce the effect of paraphrasing on our models. Our findings suggest both our proposed methods perform better than other AI text detectors. Our RR model is more robust, although our TF model performs better than the RR model without any attacks. We make our code, dataset, model public.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1262",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes": {
        "type": "INPROCEEDINGS",
        "key": "nishida-etal-2024-initialization",
        "author": "Nishida, Kosuke and Nishida, Kyosuke and Saito, Kuniko",
        "booktitle": "EMNLP-main2024",
        "title": "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Loss spikes, a phenomenon in which the loss value diverges suddenly, is a fundamental issue in the pre-training of large language models. This paper supposes that the non-uniformity of the norm of the parameters is one of the causes of loss spikes. Here, in training of neural networks, the scale of the gradients is required to be kept constant throughout the layers to avoid the vanishing and exploding gradients problem. However, to meet these requirements in the Transformer model, the norm of the model parameters must be non-uniform, and thus, parameters whose norm is smaller are more sensitive to the parameter update. To address this issue, we propose a novel technique, weight scaling as reparameterization (WeSaR). WeSaR introduces a gate parameter per parameter matrix and adjusts it to the value satisfying the requirements. Because of the gate parameter, WeSaR sets the norm of the original parameters uniformly, which results in stable training. Experimental results with the Transformer decoders consisting of 130 million, 1.3 billion, and 13 billion parameters showed that WeSaR stabilizes and accelerates training and that it outperformed compared methods including popular initialization methods.",
        "keywords": "",
        "url": "https://aclanthology.org/2024.emnlp-main.1264",
        "doi": "",
        "ISSN": "",
        "month": "November"
    },
    "Exploring Chain-of-Thought for Multi-modal Metaphor Detection": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-exploring",
        "author": "Xu, Yanzhi and Hua, Yueying and Li, Shichen and Wang, Zhongqing",
        "booktitle": "ACL2024",
        "title": "Exploring Chain-of-Thought for Multi-modal Metaphor Detection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Metaphors are commonly found in advertising and internet memes. However, the free form of internet memes often leads to a lack of high-quality textual data. Metaphor detection demands a deep interpretation of both textual and visual elements, requiring extensive common-sense knowledge, which poses a challenge to language models. To address these challenges, we propose a compact framework called C4MMD, which utilizes a Chain-of-Thought(CoT) method for Multi-modal Metaphor Detection. Specifically, our approach designs a three-step process inspired by CoT that extracts and integrates knowledge from Multi-modal Large Language Models(MLLMs) into smaller ones. We also developed a modality fusion architecture to transform knowledge from large models into metaphor features, supplemented by auxiliary tasks to improve model performance. Experimental results on the MET-MEME dataset demonstrate that our method not only effectively enhances the metaphor detection capabilities of small models but also outperforms existing models. To our knowledge, this is the first systematic study leveraging MLLMs in metaphor detection tasks. The code for our method is publicly available at https://github.com/xyz189411yt/C4MMD.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.6",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation": {
        "type": "INPROCEEDINGS",
        "key": "du-etal-2024-bitdistiller",
        "author": "Du, DaYou and Zhang, Yijia and Cao, Shijie and Guo, Jiaqi and Cao, Ting and Chu, Xiaowen and Xu, Ningyi",
        "booktitle": "ACL2024",
        "title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit configurations on general language understanding and complex reasoning benchmarks. Notably, BitDistiller is shown to be more cost-effective, demanding fewer data and training resources. The code is available at https://github.com/DD-DuDa/BitDistiller.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.7",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-unsupervised",
        "author": "Xu, Shicheng and Pang, Liang and Yu, Mo and Meng, Fandong and Shen, Huawei and Cheng, Xueqi and Zhou, Jie",
        "booktitle": "ACL2024",
        "title": "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignore it or be misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as \u201cInformation Refiner\u201d, which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named INFO-RAG that optimizes LLMs for RAG in an unsupervised manner. INFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that INFO-RAG improves the performance of LLaMA2 by an average of 9.39% relative points. INFO-RAG also shows advantages in in-context learning and robustness of RAG.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.9",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition": {
        "type": "INPROCEEDINGS",
        "key": "dong-etal-2024-abilities",
        "author": "Dong, Guanting and Yuan, Hongyi and Lu, Keming and Li, Chengpeng and Xue, Mingfeng and Liu, Dayiheng and Wang, Wei and Yuan, Zheng and Zhou, Chang and Zhou, Jingren",
        "booktitle": "ACL2024",
        "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, codegeneration, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specificially focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.12",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction": {
        "type": "INPROCEEDINGS",
        "key": "jian-etal-2024-expedited",
        "author": "Jian, Yiren and Liu, Tingkai and Tao, Yunzhe and Zhang, Chunhui and Vosoughi, Soroush and Yang, Hongxia",
        "booktitle": "ACL2024",
        "title": "Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We introduce \\textEVL_\\textGen, a streamlined framework designed for the pre-training of visually conditioned language generation models with high computational demands, utilizing frozen pre-trained large language models (LLMs). The conventional approach in vision-language pre-training (VLP) typically involves a two-stage optimization process: an initial resource-intensive phase dedicated to general-purpose vision-language representation learning, focused on extracting and consolidating relevant visual features. This is followed by a subsequent phase that emphasizes end-to-end alignment between visual and linguistic modalities. Our novel one-stage, single-loss framework bypasses the computationally demanding first training stage by gradually merging similar visual tokens during training, while avoiding model collapse caused by single-stage training of BLIP-2 type models. The gradual merging process effectively condenses visual information while preserving semantic richness, resulting in rapid convergence without compromising performance. Our experimental findings demonstrate that our approach accelerates the training of vision-language models by a factor of 5 without a noticeable impact on overall performance. Furthermore, we illustrate that our models significantly narrow the performance gap to current vision-language models using only 1/10 of the data. Finally, we showcase how our image-text models can seamlessly adapt to video-conditioned language generation tasks through novel soft attentive temporal token contextualizing modules. Code: https://github.com/yiren-jian/EVLGen",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.19",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Retrieval-Augmented Multilingual Knowledge Editing": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-retrieval",
        "author": "Wang, Weixuan and Haddow, Barry and Birch, Alexandra",
        "booktitle": "ACL2024",
        "title": "Retrieval-Augmented Multilingual Knowledge Editing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Knowledge represented in Large Language Models (LLMs) is quite often incorrect and can also become obsolete over time. Updating knowledge via fine-tuning is computationally resource-hungry and not reliable, and so knowledge editing (KE) has developed as an effective and economical alternative to inject new knowledge or to fix factual errors in LLMs. Although there has been considerable interest in this area, current KE research exclusively focuses on monolingual settings, typically in English. However, what happens if the new knowledge is supplied in one language, but we would like to query an LLM in a different language? To address the problem of multilingual knowledge editing, we propose Retrieval-Augmented Multilingual Knowledge Editor (ReMaKE) to update knowledge in LLMs. ReMaKE can be used to perform model-agnostic knowledge editing in a multilingual setting. ReMaKE concatenates the new knowledge retrieved from a multilingual knowledge base with users\u2019 prompts before querying an LLM. Our experimental results show that ReMaKE outperforms baseline knowledge editing methods by a significant margin and is scalable to real-word application scenarios. Our multilingual knowledge editing dataset (MzsRE) in 12 languages, the code, and additional project information are available at https://github.com/weixuan-wang123/ReMaKE.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.21",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Answer is All You Need: Instruction-following Text Embedding via Answering the Question": {
        "type": "INPROCEEDINGS",
        "key": "peng-etal-2024-answer",
        "author": "Peng, Letian and Zhang, Yuwei and Wang, Zilong and Srinivasa, Jayanth and Liu, Gaowen and Wang, Zihan and Shang, Jingbo",
        "booktitle": "ACL2024",
        "title": "Answer is All You Need: Instruction-following Text Embedding via Answering the Question",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This work aims to build a text embedder that can capture characteristics of texts specified by user instructions clarifying the similarity criterion. While previous methods improve general task awareness by injecting the instruction information into encoding, they fail to be sensitive to clearer criteria like \u201cevaluate similarity based on emotion\u201d. We instead propose a different viewpoint, which treats the instruction as a \u201cquestion\u201d about the input text and encodes the expected answers to obtain the representation accordingly. Intuitively, texts with the same (implicit) semantics would share similar answers following the instruction, thus leading to more similar representations. Specifically, we propose InBedder that instantiates this learning-to-answer idea by only fine-tuning language models via abstractive question answering tasks. Despite its simplicity, InBedder demonstrates significantly improved instruction-following capabilities according to our proposed instruction awareness tests and instruction robustness tests, when applied to language models with large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-based LMs (e.g., roberta-large). Additionally, our qualitative analysis of clustering outcomes, achieved by applying diverse instructions to the same unlabeled corpus, demonstrates a high degree of interpretability in the clusters formed.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.27",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis": {
        "type": "INPROCEEDINGS",
        "key": "xie-etal-2024-gradsafe",
        "author": "Xie, Yueqi and Fang, Minghong and Pi, Renjie and Gong, Neil",
        "booktitle": "ACL2024",
        "title": "GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing methods for detecting jailbreak prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects jailbreak prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our method is grounded in a pivotal observation: the gradients of an LLM\u2019s loss for jailbreak prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect jailbreak prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard\u2014despite its extensive finetuning with a large dataset\u2014in detecting jailbreak prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on ToxicChat and XSTest. The source code is available at https://github.com/xyq7/GradSafe.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.30",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "FineSurE: Fine-grained Summarization Evaluation using LLMs": {
        "type": "INPROCEEDINGS",
        "key": "song-etal-2024-finesure",
        "author": "Song, Hwanjun and Su, Hang and Shalyminov, Igor and Cai, Jason and Mansour, Saab",
        "booktitle": "ACL2024",
        "title": "FineSurE: Fine-grained Summarization Evaluation using LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Automated evaluation is crucial for streamlining text summarization benchmarking and model development, given the costly and time-consuming nature of human evaluation. Traditional methods like ROUGE do not correlate well with human judgment, while recently proposed LLM-based metrics provide only summary-level assessment using Likert-scale scores. This limits deeper model analysis, e.g., we can only assign one hallucination score at the summary level, while at the sentence level, we can count sentences containing hallucinations. To remedy those limitations, we propose FineSurE, a fine-grained evaluator specifically tailored for the summarization task using large language models (LLMs). It also employs completeness and conciseness criteria, in addition to faithfulness, enabling multi-dimensional assessment. We compare various open-source and proprietary LLMs as backbones for FineSurE. In addition, we conduct extensive benchmarking of FineSurE against SOTA methods including NLI-, QA-, and LLM-based methods, showing improved performance especially on the completeness and conciseness dimensions. The code is available at https://github.com/DISL-Lab/FineSurE.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.51",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback": {
        "type": "INPROCEEDINGS",
        "key": "ahn-etal-2024-tuning",
        "author": "Ahn, Daechul and Choi, Yura and Yu, Youngjae and Kang, Dongyeop and Choi, Jonghyun",
        "booktitle": "ACL2024",
        "title": "Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). Previous approaches for VLMMs involve Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and additional learnable parameters. Here, aligning video with text, and vice versa, remains a challenge, primarily due to the insufficient quality and quantity of multimodal instruction-tune data compared to that of text-only. This discrepancy often results in alignments that poorly ground the video content. To address this, we present a novel alignment strategy that employs a multimodal AI system equipped with Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. Our approach uniquely integrates detailed video descriptions as context into a multimodal AI system during the preference feedback generation to enrich the understanding of video content, a process we call context-aware reward modeling. Empirical evaluations on various video benchmarks demonstrate that our VLM-RLAIF outperforms existing approaches, including the SFT model. We commit to open-sourcing our code, models, and datasets to foster further research in this area.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.52",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Can ChatGPT\u2019s Performance be Improved on Verb Metaphor Detection Tasks? Bootstrapping and Combining Tacit Knowledge": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2024-chatgpts",
        "author": "Yang, Cheng and Chen, Puli and Huang, Qingbao",
        "booktitle": "ACL2024",
        "title": "Can ChatGPT\u2019s Performance be Improved on Verb Metaphor Detection Tasks? Bootstrapping and Combining Tacit Knowledge",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Metaphors detection, as an important task in the field of NLP, has been receiving sustained academic attention in recent years. Current researches focus supervised metaphors detection systems, which usually require large-scale, high-quality labeled data support. The emerge of large language models (e.g., ChatGPT) has made many NLP tasks (e.g., automatic summarization and dialogue systems) a qualitative leap. However, it is worth noting that the use of ChatGPT for unsupervised metaphors detection is often challenged with less-than-expected performance. Therefore, the aim of our work is to explore how to bootstrap and combine ChatGPT by detecting the most prevalent verb metaphors among metaphors. Our approach first utilizes ChatGPT to obtain literal collocations of target verbs and subject-object pairs of verbs in the text to be detected. Subsequently, these literal collocations and subject-object pairs are mapped to the same set of topics, and finally the verb metaphors are detected through the analysis of entailment relations. The experimental results show that our method achieves the best performance on the unsupervised verb metaphors detection task compared to existing unsupervised methods or direct prediction using ChatGPT. Our code is available at https://github.com/VILAN-Lab/Unsupervised-Metaphor-Detection.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.57",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2024-self",
        "author": "Yang, Zhaorui and Pang, Tianyu and Feng, Haozhe and Wang, Han and Chen, Wei and Zhu, Minfeng and Liu, Qian",
        "booktitle": "ACL2024",
        "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at https://github.com/sail-sg/sdft.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.58",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chu-etal-2024-timebench",
        "author": "Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and Wang, Haotian and Liu, Ming and Qin, Bing",
        "booktitle": "ACL2024",
        "title": "TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Grasping the concept of time is a fundamental facet of human cognition, indispensable for truly comprehending the intricacies of the world.Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark.To address this, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena.TimeBench provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models.We conduct extensive experiments on GPT-4, LLaMA2, and other popular LLMs under various settings.Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning.Besides, LLMs exhibit capability discrepancies across different reasoning categories.Furthermore, we thoroughly analyze the impact of multiple aspects on temporal reasoning and emphasize the associated challenges.We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning.Code and data are available at https://github.com/zchuz/TimeBench.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.66",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-easygen",
        "author": "Zhao, Xiangyu and Liu, Bo and Liu, Qijiong and Shi, Guangyuan and Wu, Xiao-Ming",
        "booktitle": "ACL2024",
        "title": "EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge modalities, EasyGen leverages BiDiffuser, a bidirectional conditional diffusion model, to foster more efficient modality interactions. EasyGen achieves text generation by training a projection layer linking BiDiffuser and an LLM, and facilities image generation by training an adapter to align the LLM\u2019s text space with the BiDiffuser\u2019s image space. Comprehensive quantitative and qualitative experiments show that EasyGen excels in data-efficient training, high-quality image generation, and extendibility, effectively addressing the challenges in multimodal generation.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.74",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-rewriting",
        "author": "Li, Haochen and Zhou, Xin and Shen, Zhiqi",
        "booktitle": "ACL2024",
        "title": "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%), and fine-tuned dense (up to 23.6%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances. The source code and data are available at https://github.com/Alex-HaochenLi/ReCo.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.75",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-enhancing",
        "author": "Huang, Baizhou and Lu, Shuai and Wan, Xiaojun and Duan, Nan",
        "booktitle": "ACL2024",
        "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs\u2019 reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph.MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.78",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Citation-Enhanced Generation for LLM-based Chatbots": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-citation",
        "author": "Li, Weitao and Li, Junkai and Ma, Weizhi and Liu, Yang",
        "booktitle": "ACL2024",
        "title": "Citation-Enhanced Generation for LLM-based Chatbots",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc Citation-Enhanced Generation (CEG) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various LLMs. Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks. Our code and datasets can be found at https://github.com/Tsinghua-dhy/CEG.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.79",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "tu-etal-2024-waterbench",
        "author": "Tu, Shangqing and Sun, Yuliang and Bai, Yushi and Yu, Jifan and Hou, Lei and Li, Juanzi",
        "booktitle": "ACL2024",
        "title": "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For benchmarking procedure, to ensure an apples-to-apples comparison, we first adjust each watermarking method\u2019s hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For task selection, we diversify the input and output length to form a five-category taxonomy, covering 9 tasks. (3) For evaluation metric, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate 4 open-source watermarks on 2 LLMs under 2 watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at https://github.com/THU-KEG/WaterBench.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.83",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model": {
        "type": "INPROCEEDINGS",
        "key": "jin-etal-2024-persuading",
        "author": "Jin, Chuhao and Ren, Kening and Kong, Lingzhen and Wang, Xiting and Song, Ruihua and Chen, Huan",
        "booktitle": "ACL2024",
        "title": "Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Persuasive dialogue requires multi-turn following and planning abilities to achieve the goal of persuading users, which is still challenging even for state-of-the-art large language models (LLMs). Previous works focus on retrieval-based models or generative models in a specific domain due to a lack of data across multiple domains. In this paper, we leverage GPT-4 to create the first multi-domain persuasive dialogue dataset DailyPersuasion. Then we propose a general method named PersuGPT to learn a persuasion model based on LLMs through intent-to-strategy reasoning, which summarizes the intent of user\u2019s utterance and reasons next strategy to respond. Moreover, we design a simulation-based preference optimization, which utilizes a learned user model and our model to simulate next turns and estimate their rewards more accurately. Experimental results on two datasets indicate that our proposed method outperforms all baselines in terms of automatic evaluation metric Win-Rate and human evaluation. The code and data are available at https://persugpt.github.io.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.92",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "UniCoder: Scaling Code Large Language Model via Universal Code": {
        "type": "INPROCEEDINGS",
        "key": "sun-etal-2024-unicoder",
        "author": "Sun, Tao and Chai, Linzheng and Yang, Jian and Yin, Yuwei and Guo, Hongcheng and Liu, Jiaheng and Wang, Bing and Yang, Liqun and Li, Zhoujun",
        "booktitle": "ACL2024",
        "title": "UniCoder: Scaling Code Large Language Model via Universal Code",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks.When applying LLMs for code generation, recent works mainly focus on directing the models to articulate intermediate natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and then output code with the natural language or other structured intermediate steps. However, such output is not suitable for code translation or generation tasks since the standard CoT has different logical structures and forms of expression with the code. In this work, we introduce the universal code (UniCode) as the intermediate representation. It is a description of algorithm steps using a mix of conventions of programming languages, such as assignment operator, conditional operator, and loop. Hence, we collect an instruction dataset UniCoder-Instruct to train our model UniCoder on multi-task learning objectives. UniCoder-Instruct comprises natural-language questions, code solutions, and the corresponding universal code. The alignment between the intermediate universal code representation and the final code solution significantly improves the quality of the generated code. The experimental results demonstrate that UniCoder with the universal code significantly outperforms the previous prompting methods by a large margin, showcasing the effectiveness of the structural clues in pseudo-code.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.100",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin": {
        "type": "INPROCEEDINGS",
        "key": "dou-etal-2024-loramoe",
        "author": "Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Shen, Wei and Xiong, Limao and Zhou, Yuhao and Wang, Xiao and Xi, Zhiheng and Fan, Xiaoran and Pu, Shiliang and Zhu, Jiang and Zheng, Rui and Gui, Tao and Zhang, Qi and Huang, Xuanjing",
        "booktitle": "ACL2024",
        "title": "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Substantially increasing instruction data is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can significantly improve the ability to process downstream tasks, while maintaining the world knowledge stored in the LLM. Our code is available at https://github.com/Ablustrund/LoRAMoE.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.106",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation": {
        "type": "INPROCEEDINGS",
        "key": "ma-etal-2024-mops",
        "author": "Ma, Yan and Qiao, Yu and Liu, Pengfei",
        "booktitle": "ACL2024",
        "title": "MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A story premise succinctly defines a story\u2019s main idea, foundation, and trajectory. It serves as the initial trigger in automatic story generation. Existing sources of story premises are limited by a lack of diversity, uneven quality, and high costs that make them difficult to scale. In response, we introduce Modular Story Premise Synthesis (MoPS) which breaks down story premises into modules like background and persona for automated design and generation. MoPS consists of three phases: (1) Pre-collect a consistent set of candidates for each module to form a nested dictionary. (2) Extract a key path from the nested dictionary as the premise design. (3) Instruct an LLM to integrate the design into a coherent premise sentence. Thorough evaluations demonstrate that our synthesized premises excel in diversity, fascination, completeness, and originality compared to those induced from large language models and captured from public story datasets. Similarly, the extended novels and scripts generated from our premises also exhibit higher quality. In supplementary materials, we provide the MoPS code suite, along with 7.5k generated premises and 1k extended stories.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.117",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "LRQuant: Learnable and Robust Post-Training Quantization for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-lrquant",
        "author": "Zhao, Jiaqi and Zhang, Miao and Zeng, Chao and Wang, Ming and Liu, Xuebo and Nie, Liqiang",
        "booktitle": "ACL2024",
        "title": "LRQuant: Learnable and Robust Post-Training Quantization for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Post-training quantization (PTQ) for large language models (LLMs) significantly accelerates model inference and relieves memory constraints, without incurring model training. A \u201csmoothing paradigm\u201d is commonly used in LLM quantization, which transfers the quantization difficulty of activation to weight quantization using mathematically equivalent transformations. However, existing methods face two issues: 1) Most smoothing parameters are hand-crafted defined which leads to suboptimal results; 2) There are significant performance degradations when tested on unseen datasets. To address these challenges, this paper introduces a robust learnable smooth-based PTQ framework, called LRQuant. Firstly, we consider a learnable paradigm to find optimal smoothing parameters which are initialized by logarithmic activation equivalent. In addition, we empirically found that only relying on MSE loss could hardly lead to optimal quantization results, and we then propose a novel loss function based on the negative logarithm of cosine similarity (NLC loss) between outputs of full-precision and quantized block. At last, we pioneeringly introduce Test-time adaptation (TTA) into LLM quantization, which allows for rapid model adaptation during testing to improve generalization performance. More surprisingly, we find that by using our TTA method, we can achieve better results on test sets than directly using test sets for calibration in some cases while avoiding catastrophic forgetting. Codes are available at https://github.com/zjq0455/RLQ.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.122",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks": {
        "type": "INPROCEEDINGS",
        "key": "moskvoretskii-etal-2024-taxollama",
        "author": "Moskvoretskii, Viktor and Neminova, Ekaterina and Lobanova, Alina and Panchenko, Alexander and Nikishina, Irina",
        "booktitle": "ACL2024",
        "title": "TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the \u201call-in-one\u201d model for taxonomy-related tasks, lightweight due to 4-bit quantization and LoRA. TaxoLLaMA achieves 11 SOTA results, and 4 top-2 results out of 16 tasks on the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates a very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and pre-trained models are available online (code: https://github.com/VityaVitalich/TaxoLLaMA)",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.127",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter": {
        "type": "INPROCEEDINGS",
        "key": "hao-etal-2024-meft",
        "author": "Hao, Jitai and Sun, Weiwei and Xin, Xin and Meng, Qi and Chen, Zhumin and Ren, Pengjie and Ren, Zhaochun",
        "booktitle": "ACL2024",
        "title": "MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Parameter-Efficient Fine-tuning (PEFT) facilitates the fine-tuning of Large Language Models (LLMs) under limited resources. However, the fine-tuning performance with PEFT on complex, knowledge-intensive tasks is limited due to the constrained model capacity, which originates from the limited number of additional trainable parameters. To overcome this limitation, we introduce a novel mechanism that fine-tunes LLMs with adapters of larger size yet memory-efficient. This is achieved by leveraging the inherent activation sparsity in the Feed-Forward Networks (FFNs) of LLMs and utilizing the larger capacity of Central Processing Unit (CPU) memory compared to Graphics Processing Unit (GPU). We store and update the parameters of larger adapters on the CPU. Moreover, we employ a Mixture of Experts (MoE)-like architecture to mitigate unnecessary CPU computations and reduce the communication volume between the GPU and CPU. This is particularly beneficial over the limited bandwidth of PCI Express (PCIe). Our method can achieve fine-tuning results comparable to those obtained with larger memory capacities, even when operating under more limited resources such as a 24GB memory single GPU setup, with acceptable loss in training efficiency. Our codes are available at https://github.com/CURRENTF/MEFT.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.129",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-rlhfpoison",
        "author": "Wang, Jiongxiao and Wu, Junlin and Chen, Muhao and Vorobeychik, Yevgeniy and Xiao, Chaowei",
        "booktitle": "ACL2024",
        "title": "RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences, playing an important role in LLMs alignment. Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially. To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates\u2019 selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost). With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance. Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our findings highlight critical security challenges in RLHF, underscoring the necessity for more robust alignment methods for LLMs.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.140",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Long-Context Language Modeling with Parallel Context Encoding": {
        "type": "INPROCEEDINGS",
        "key": "yen-etal-2024-long",
        "author": "Yen, Howard and Gao, Tianyu and Chen, Danqi",
        "booktitle": "ACL2024",
        "title": "Long-Context Language Modeling with Parallel Context Encoding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Extending large language models (LLMs) to process longer inputs is crucial for numerous applications. However, the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window. We introduce Cross-Attention to Parallel Encodings (CAPE), a framework that can be applied to any existing decoder-only LLMs for context expansion. CAPE leverages a small encoder to process a long input chunk by chunk and enables the frozen decoder to cross-attend to the additional contexts. CAPE is efficient, generalizable, and versatile: trained with 8K-token documents, CAPE extends the context window of LLaMA-2 to 128K tokens, offering 10\\times of the throughput with only 1/6 of the memory. CAPE yields strong performance on language modeling and in-context learning. CAPE also excels in retrieval-augmented applications, while existing long-context models degenerate with retrieved contexts. We further introduce a CAPE variant that can extend the context window of instruction-tuned models with only unlabeled data, and showcase its effectiveness on LLaMA-2-Chat, leading to a strong instruction-following model that can leverage very long context on downstream tasks.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.142",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "SirLLM: Streaming Infinite Retentive LLM": {
        "type": "INPROCEEDINGS",
        "key": "yao-etal-2024-sirllm",
        "author": "Yao, Yao and Li, Zuchao and Zhao, Hai",
        "booktitle": "ACL2024",
        "title": "SirLLM: Streaming Infinite Retentive LLM",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "As Large Language Models (LLMs) become increasingly prevalent in various domains, their ability to process inputs of any length and maintain a degree of memory becomes essential. However, the one-off input of overly long texts is limited, as studies have shown that when input lengths exceed the LLMs\u2019 pre-trained text length, there is a dramatic decline in text generation capabilities. Moreover, simply extending the length of pre-training texts is impractical due to the difficulty in obtaining long text data and the substantial memory consumption costs this would entail for LLMs. Recent efforts have employed streaming inputs to alleviate the pressure of excessively long text inputs, but this approach can significantly impair the model\u2019s long-term memory capabilities.Motivated by this challenge, we introduce Streaming Infinite Retentive LLM (SirLLM), which allows LLMs to maintain longer memory during infinite-length dialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy metric and a memory decay mechanism to filter key phrases, endowing LLMs with both long-lasting and flexible memory. We designed three distinct tasks and constructed three datasets to measure the effectiveness of SirLLM from various angles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our experimental results robustly demonstrate that SirLLM can achieve stable and significant improvements across different LLMs and tasks, compellingly proving its effectiveness. When having a coversation, \u201cA sir could forget himself,\u201d but SirLLM never does! Our code is publicly available at https://github.com/Zoeyyao27/SirLLMhttps://github.com/Zoeyyao27/SirLLM",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.143",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-generalizing",
        "author": "Chen, Haonan and Dou, Zhicheng and Mao, Kelong and Liu, Jiongnan and Zhao, Ziliang",
        "booktitle": "ACL2024",
        "title": "Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem \u2013 that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). We first generate multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware prompting process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space. A contrastive learning objective is then employed to train a better conversational context encoder. Extensive experiments conducted on four public datasets, under both normal and zero-shot settings, demonstrate the effectiveness, generalizability, and applicability of ConvAug. The code is released at https://github.com/haon-chen/ConvAug.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.149",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "ItD: Large Language Models Can Teach Themselves Induction through Deduction": {
        "type": "INPROCEEDINGS",
        "key": "sun-etal-2024-itd",
        "author": "Sun, Wangtao and Xu, Haotian and Yu, Xuanqing and Chen, Pei and He, Shizhu and Zhao, Jun and Liu, Kang",
        "booktitle": "ACL2024",
        "title": "ItD: Large Language Models Can Teach Themselves Induction through Deduction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt \u201cpost processes\u201d paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search &amp; refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36% and 10% compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different LLMs and deductors. The data and code of this paper can be found at https://github.com/forangel2014/ItD.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.150",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "On Context Utilization in Summarization with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ravaut-etal-2024-context",
        "author": "Ravaut, Mathieu and Sun, Aixin and Chen, Nancy and Joty, Shafiq",
        "booktitle": "ACL2024",
        "title": "On Context Utilization in Summarization with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) excel in abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, exceeding 100k tokens. However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source document(s). Besides, in summarization, mapping facts from the source to the summary is not trivial as salient content is usually re-phrased. In this paper, we conduct the first comprehensive study on context utilization and position bias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5 evaluation metrics. We introduce a new evaluation benchmark called MiddleSum on the which we benchmark two alternative inference methods to alleviate position bias: hierarchical summarization and incremental summarization. Our code and data can be found here: https://github.com/ntunlp/MiddleSum.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.153",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Virtual Compiler Is All You Need For Assembly Code Search": {
        "type": "INPROCEEDINGS",
        "key": "gao-etal-2024-virtual",
        "author": "Gao, Zeyu and Wang, Hao and Wang, Yuanda and Zhang, Chao",
        "booktitle": "ACL2024",
        "title": "Virtual Compiler Is All You Need For Assembly Code Search",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Assembly code search is vital for reducing the burden on reverse engineers, allowing them to quickly identify specific functions using natural language within vast binary programs.Despite its significance, this critical task is impeded by the complexities involved in building high-quality datasets. This paper explores training a Large Language Model (LLM) to emulate a general compiler. By leveraging Ubuntu packages to compile a dataset of 20 billion tokens, we further continue pre-train CodeLlama as a Virtual Compiler (ViC), capable of compiling any source code to assembly code. This approach allows for \u201cvirtual\u201d compilation across a wide range of programming languages without the need for a real compiler, preserving semantic equivalency and expanding the possibilities for assembly code dataset construction. Furthermore, we use ViC to construct a sufficiently large dataset for assembly code search. Employing this extensive dataset, we achieve a substantial improvement in assembly code search performance, with our model surpassing the leading baseline by 26%.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.167",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding": {
        "type": "INPROCEEDINGS",
        "key": "bai-etal-2024-longbench",
        "author": "Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and Dong, Yuxiao and Tang, Jie and Li, Juanzi",
        "booktitle": "ACL2024",
        "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs\u2019 long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.172",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Black-Box Prompt Optimization: Aligning Large Language Models without Model Training": {
        "type": "INPROCEEDINGS",
        "key": "cheng-etal-2024-black",
        "author": "Cheng, Jiale and Liu, Xiao and Zheng, Kehan and Ke, Pei and Wang, Hongning and Dong, Yuxiao and Tang, Jie and Huang, Minlie",
        "booktitle": "ACL2024",
        "title": "Black-Box Prompt Optimization: Aligning Large Language Models without Model Training",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them; that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods primarily focus on further training them. However, the extra training of LLMs is usually expensive in terms of GPU computing; even worse, some LLMs are not accessible for user-demanded training, such as GPTs. In this work, we take a different perspective\u2014Black-Box Prompt Optimization (BPO)\u2014to perform alignments. The idea is to optimize user prompts to suit LLMs\u2019 input understanding, so as to best realize users\u2019 intents without updating LLMs\u2019 parameters. BPO leverages human preferences to optimize prompts, thus making it superior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is model-agnostic, and the empirical results demonstrate that the BPO-aligned ChatGPT yields a 22% increase in the win rate against its original version and 10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO. Code and datasets are released at https://github.com/thu-coai/BPO.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.176",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-llama2vec",
        "author": "Li, Chaofan and Liu, Zheng and Xiao, Shitao and Shao, Yingxia and Lian, Defu",
        "booktitle": "ACL2024",
        "title": "Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Dense retrieval calls for discriminative embeddings to represent the semantic relationship between query and document. It may benefit from the using of large language models (LLMs), given LLMs\u2019 strong capability on semantic understanding. However, the LLMs are learned by auto-regression, whose working mechanism is completely different from representing whole text as one discriminative embedding. Thus, it is imperative to study how to adapt LLMs properly so that they can be effectively initialized as the backbone encoder for dense retrieval. In this paper, we propose a novel approach, called Llama2Vec, which performs unsupervised adaptation of LLM for its dense retrieval application. Llama2Vec consists of two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression), where the LLM is prompted to reconstruct the input sentence and predict the next sentence based on its text embeddings. Llama2Vec is simple, lightweight, but highly effective. It is used to adapt LLaMA-2-7B on the Wikipedia corpus. With a moderate steps of adaptation, it substantially improves the model\u2019s fine-tuned performances on a variety of dense retrieval benchmarks. Notably, it results in the new state-of-the-art performances on popular benchmarks, such as passage and document retrieval on MSMARCO, and zero-shot retrieval on BEIR. The model and source code will be made publicly available to facilitate the future research. Our model is available at https://github.com/FlagOpen/FlagEmbedding.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.191",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "A Multi-Task Embedder For Retrieval Augmented LLMs": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-multi-task",
        "author": "Zhang, Peitian and Liu, Zheng and Xiao, Shitao and Dou, Zhicheng and Nie, Jian-Yun",
        "booktitle": "ACL2024",
        "title": "A Multi-Task Embedder For Retrieval Augmented LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "LLMs confront inherent limitations in terms of its knowledge, memory, and action. The retrieval augmentation stands as a vital mechanism to address these limitations, which brings in useful information from external sources to augment the LLM. However, existing retrieval methods encounter two pressing issues. On one hand, the general retrievers are not properly optimized for retrieval augmentation hence exhibit limited effectiveness; on the other hand, the task-specific retrievers excel in the targeted retrieval augmentation scenario, while lack the versatility to handle diverse scenarios. In this work, we propose LLM-Embedder for the unified support of diverse retrieval augmentation scenarios. Our method presents three technical contributions. Firstly, we introduce a new reward formulation, namely rank-aware reward. It exploits the ranking position of the desired output among N sampled outputs from the LLM, which leads to fine-grained and robust computation of reward from the LLM\u2019s feedback. Secondly, we design a novel distillation objective, called graded distillation. It incorporates both the absolute value and the relative order of the reward for more sufficient utilization of the LLM\u2019s feedback. Thirdly, we systematically optimize the multi-task learning, which effectively unifies the multiple retrieval functionalities into one model. In our experiment, LLM-Embedder substantially improves the LLM\u2019s performances in various downstream tasks, while introducing superior retrieval augmentation\u2019s effect over both general and task-specifc retrievers. Our data, code, and model have been released at https://github.com/FlagOpen/FlagEmbedding.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.194",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Language Models Don\u2019t Learn the Physical Manifestation of Language": {
        "type": "INPROCEEDINGS",
        "key": "lee-lim-2024-language",
        "author": "Lee, Bruce and Lim, Jaehyuk",
        "booktitle": "ACL2024",
        "title": "Language Models Don\u2019t Learn the Physical Manifestation of Language",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We argue that language-only models don\u2019t learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test.These tasks highlight a fundamental gap between human linguistic understanding and the sensory-deprived linguistic understanding of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -\\textgreater LLaMA 2 70B) has no significant effect on H-Test performance. We bring in the philosophical case of Mary, who learns about the world in a sensory-deprived environment as a useful conceptual framework to understand how language-only models learn about the world (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limitations of linguistic knowledge acquired in the absence of sensory experience. Our code and data are available at \\textlessgithub.com/brucewlee/h-test\\textgreater.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.195",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning": {
        "type": "INPROCEEDINGS",
        "key": "dai-etal-2024-mpcoder",
        "author": "Dai, Zhenlong and Yao, Chang and Han, WenKang and Yuanying, Yuanying and Gao, Zhipeng and Chen, Jingyuan",
        "booktitle": "ACL2024",
        "title": "MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated great potential for assisting developers in their daily development. However, most research focuses on generating correct code, how to use LLMs to generate personalized code has seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user Personalized Code Generator) to generate personalized code for multiple users. To better learn coding style features, we utilize explicit coding style residual learning to capture the syntax code style standards and implicit style learning to capture the semantic code style conventions. We train a multi-user style adapter to better differentiate the implicit feature representations of different users through contrastive learning, ultimately enabling personalized code generation for multiple users. We further propose a novel evaluation metric for estimating similarities between codes of different coding styles. The experimental results show the effectiveness of our approach for this novel task.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.207",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows": {
        "type": "INPROCEEDINGS",
        "key": "patel-etal-2024-datadreamer",
        "author": "Patel, Ajay and Raffel, Colin and Callison-Burch, Chris",
        "booktitle": "ACL2024",
        "title": "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this ACL 2024 theme track paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at: https://github.com/datadreamer-dev/DataDreamer.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.208",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems": {
        "type": "INPROCEEDINGS",
        "key": "he-etal-2024-olympiadbench",
        "author": "He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and Liu, Jie and Qi, Lei and Liu, Zhiyuan and Sun, Maosong",
        "booktitle": "ACL2024",
        "title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97% on OlympiadBench, with a mere 10.74% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at https://github.com/OpenBMB/OlympiadBench",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.211",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Instruction Fusion: Advancing Prompt Evolution through Hybridization": {
        "type": "INPROCEEDINGS",
        "key": "guo-etal-2024-instruction",
        "author": "Guo, Weidong and Yang, Jiuding and Yang, Kaitong and Li, Xiangyang and Rao, Zhuwei and Xu, Yu and Niu, Di",
        "booktitle": "ACL2024",
        "title": "Instruction Fusion: Advancing Prompt Evolution through Hybridization",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The fine-tuning of Large Language Models (LLMs) specialized in code generation has seen notable advancements through the use of open-domain coding queries. Despite the successes, existing methodologies like Evol-Instruct encounter performance limitations, impeding further enhancements in code generation tasks. This paper examines the constraints of existing prompt evolution techniques and introduces a novel approach, Instruction Fusion (IF). IF innovatively combines two distinct prompts through a hybridization process, thereby enhancing the evolution of training prompts for code LLMs. Our experimental results reveal that the proposed novel method effectively addresses the shortcomings of prior methods, significantly improving the performance of Code LLMs across five code generation benchmarks, namely HumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the effectiveness of Instruction Fusion in advancing the capabilities of LLMs in code generation.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.214",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "MAVEN-ARG: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-maven",
        "author": "Wang, Xiaozhi and Peng, Hao and Guan, Yong and Zeng, Kaisheng and Chen, Jianhui and Hou, Lei and Han, Xu and Lin, Yankai and Liu, Zhiyuan and Xie, Ruobing and Zhou, Jie and Li, Juanzi",
        "booktitle": "ACL2024",
        "title": "MAVEN-ARG: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and codes can be obtained from https://github.com/THU-KEG/MAVEN-Argument.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.224",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes": {
        "type": "INPROCEEDINGS",
        "key": "fan-etal-2024-nphardeval",
        "author": "Fan, Lizhou and Hua, Wenyue and Li, Lingyao and Ling, Haoyang and Zhang, Yongfeng",
        "booktitle": "ACL2024",
        "title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Complex reasoning ability is one of the most important features of Large Language Models (LLMs). Numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, they are inadequate in offering a rigorous evaluation and prone to the risk of overfitting, as these publicly accessible and static benchmarks allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, we introduce a new benchmark NPHardEval. It contains a broad spectrum of 900 algorithmic questions belonging up to the NP-Hard complexity class, offering a rigorous measure of the reasoning ability of LLMs utilizing computational complexity. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https://github.com/casmlab/NPHardEval.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.225",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "In-context Mixing (ICM): Code-mixed Prompts for Multilingual LLMs": {
        "type": "INPROCEEDINGS",
        "key": "shankar-etal-2024-context",
        "author": "Shankar, Bhavani and Jyothi, Preethi and Bhattacharyya, Pushpak",
        "booktitle": "ACL2024",
        "title": "In-context Mixing (ICM): Code-mixed Prompts for Multilingual LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We introduce a simple and effective prompting technique called in-context mixing (ICM) for effective in-context learning (ICL) with multilingual large language models (MLLMs). With ICM, we modify the few-shot examples within ICL prompts to be intra-sententially code-mixed by randomly swapping content words in the target languages with their English translations. We observe that ICM prompts yield superior performance in NLP tasks such as disfluency correction, grammar error correction and text simplification that demand a close correspondence between the input and output sequences. Significant improvements are observed mainly for low-resource languages that are under-represented during the pretraining and finetuning of MLLMs. We present an extensive set of experiments to analyze when ICM is effective and what design choices contribute towards its effectiveness. ICM works consistently and significantly better than other prompting techniques across models of varying capacity such as mT0-XXL, BloomZ and GPT-4.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.228",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending": {
        "type": "INPROCEEDINGS",
        "key": "zhu-etal-2024-coca",
        "author": "Zhu, Shiyi and Ye, Jing and Jiang, Wei and Xue, Siqiao and Zhang, Qi and Wu, Yifan and Li, Jianguo",
        "booktitle": "ACL2024",
        "title": "CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Self-attention and position embedding are two crucial modules in transformer-based Large Language Models (LLMs). However, the potential relationship between them is far from well studied, especially for long context window extending. In fact, anomalous behaviors that hinder long context extrapolation exist between Rotary Position Embedding (RoPE) and vanilla self-attention.Incorrect initial angles between Q and K can cause misestimation in modeling rotary position embedding of the closest tokens.To address this issue, we propose Collinear Constrained Attention mechanism, namely CoCA. Specifically, we enforce a collinear constraint between Q and K to seamlessly integrate RoPE and self-attention.While only adding minimal computational and spatial complexity, this integration significantly enhances long context window extrapolation ability. We provide an optimized implementation, making it a drop-in replacement for any existing transformer-based models.Extensive experiments demonstrate that CoCA excels in extending context windows. A CoCA-based GPT model, trained with a context length of 512, can extend the context window up to 32K (60\\times) without any fine-tuning.Additionally, incorporating CoCA into LLaMA-7B achieves extrapolation up to 32K within a training length of only 2K.Our code is publicly available at: https://github.com/codefuse-ai/Collinear-Constrained-Attention",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.233",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback": {
        "type": "INPROCEEDINGS",
        "key": "dou-etal-2024-stepcoder",
        "author": "Dou, Shihan and Liu, Yan and Jia, Haoxiang and Zhou, Enyu and Xiong, Limao and Shan, Junjie and Huang, Caishuang and Wang, Xiao and Fan, Xiaoran and Xi, Zhiheng and Zhou, Yuhao and Ji, Tao and Zheng, Rui and Zhang, Qi and Gui, Tao and Huang, Xuanjing",
        "booktitle": "ACL2024",
        "title": "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. The code and dataset will be made available upon publication.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.251",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision": {
        "type": "INPROCEEDINGS",
        "key": "ruan-etal-2024-re3",
        "author": "Ruan, Qian and Kuznetsov, Ilia and Gurevych, Iryna",
        "booktitle": "ACL2024",
        "title": "Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Collaborative review and revision of textual documents is the core of knowledge work and a promising target for empirical analysis and NLP assistance. Yet, a holistic framework that would allow modeling complex relationships between document revisions, reviews and author responses is lacking. To address this gap, we introduce Re3, a framework for joint analysis of collaborative document revision. We instantiate this framework in the scholarly domain, and present Re3-Sci, a large corpus of aligned scientific paper revisions manually labeled according to their action and intent, and supplemented with the respective peer reviews and human-written edit summaries. We use the new data to provide first empirical insights into collaborative document revision in the academic domain, and to assess the capabilities of state-of-the-art LLMs at automating edit analysis and facilitating text-based collaboration. We make our annotation environment and protocols, the resulting data and experimental code publicly available.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.255",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "jiang-etal-2024-followbench",
        "author": "Jiang, Yuxin and Wang, Yufei and Zeng, Xingshan and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Shang, Lifeng and Jiang, Xin and Liu, Qun and Wang, Wei",
        "booktitle": "ACL2024",
        "title": "FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating pure response quality, rather than assessing whether the response follows constraints stated in the instruction. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Situation, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each increased level. To assess whether LLMs\u2019 outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths to handle challenging open-ended instructions. By evaluating 13 closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.257",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Learning to Edit: Aligning LLMs with Knowledge Editing": {
        "type": "INPROCEEDINGS",
        "key": "jiang-etal-2024-learning",
        "author": "Jiang, Yuxin and Wang, Yufei and Wu, Chuhan and Zhong, Wanjun and Zeng, Xingshan and Gao, Jiahui and Li, Liangyou and Jiang, Xin and Shang, Lifeng and Tang, Ruiming and Liu, Qun and Wang, Wei",
        "booktitle": "ACL2024",
        "title": "Learning to Edit: Aligning LLMs with Knowledge Editing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Knowledge editing techniques, aiming to efficiently modify a minor proportion of knowledge in large language models (LLMs) without negatively impacting performance across other inputs, have garnered widespread attention. However, existing methods predominantly rely on memorizing the updated knowledge, impeding LLMs from effectively combining the new knowledge with their inherent knowledge when answering questions. To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of \u201cTeach a man to fish.\u201d LTE features a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on a meticulously curated parallel dataset to make reliable, in-scope edits while preserving out-of-scope information and linguistic proficiency; and (ii) the Inference Phase, which employs a retrieval-based mechanism for real-time and mass knowledge editing. By comparing our approach with seven advanced baselines across four popular knowledge editing benchmarks and two LLM architectures, we demonstrate LTE\u2019s superiority in knowledge editing performance, robustness in both batch and sequential editing, minimal interference on general tasks, and rapid editing speeds. The data and code are publicly available at https://github.com/YJiangcm/LTE.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.258",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-dolphcoder",
        "author": "Wang, Yejie and He, Keqing and Dong, Guanting and Wang, Pei and Zeng, Weihao and Diao, Muxi and Xu, Weiran and Wang, Jingang and Zhang, Mengdi and Cai, Xunliang",
        "booktitle": "ACL2024",
        "title": "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Various instruction finetuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model DolphCoder with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with more distinct reasoning paths increases the code capability of LLMs. (2) Improving one\u2019s ability to evaluate the correctness of code also enhances their ability to create it.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.259",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Systematic Task Exploration with LLMs: A Study in Citation Text Generation": {
        "type": "INPROCEEDINGS",
        "key": "sahinuc-etal-2024-systematic",
        "author": "\u015eahinu\u00e7, Furkan and Kuznetsov, Ilia and Hou, Yufang and Gurevych, Iryna",
        "booktitle": "ACL2024",
        "title": "Systematic Task Exploration with LLMs: A Study in Citation Text Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) bring unprecedented flexibility in defining and executing complex, creative natural language generation (NLG) tasks. Yet, this flexibility brings new challenges, as it introduces new degrees of freedom in formulating the task inputs and instructions and in evaluating model performance. To facilitate the exploration of creative NLG tasks, we propose a three-component research framework that consists of systematic input manipulation, reference data, and output measurement. We use this framework to explore citation text generation \u2013 a popular scholarly NLP task that lacks consensus on the task definition and evaluation metric and has not yet been tackled within the LLM paradigm. Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs, and reveal non-trivial relationships between different evaluation metrics used for citation text generation. Additional human generation and human evaluation experiments provide new qualitative insights into the task to guide future research in citation text generation. We make our code and data publicly available.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.265",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Temporal Knowledge Question Answering via Abstract Reasoning Induction": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-temporal",
        "author": "Chen, Ziyang and Li, Dongfang and Zhao, Xiang and Hu, Baotian and Zhang, Min",
        "booktitle": "ACL2024",
        "title": "Temporal Knowledge Question Answering via Abstract Reasoning Induction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this study, we address the challenge of enhancing temporal knowledge reasoning in Large Language Models (LLMs). LLMs often struggle with this task, leading to the generation of inaccurate or misleading responses. This issue mainly arises from their limited ability to handle evolving factual knowledge and complex temporal logic. To overcome these limitations, we propose Abstract Reasoning Induction (ARI) framework, which divides temporal reasoning into two distinct phases: Knowledge agnostic and Knowledge-based. This framework offers factual knowledge support to LLMs while minimizing the incorporation of extraneous noisy data. Concurrently, informed by the principles of constructivism, ARI provides LLMs the capability to engage in proactive, self-directed learning from both correct and incorrect historical reasoning samples. By teaching LLMs to actively construct knowledge and methods, it can significantly boosting their temporal reasoning abilities. Our approach achieves significant improvements, with relative gains of 29.7% and 9.27% on two temporal QA datasets, underscoring its efficacy in advancing temporal reasoning in LLMs. The code can be found at https: //github.com/czy1999/ARI-QA.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.267",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Who Wrote this Code? Watermarking for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "lee-etal-2024-wrote",
        "author": "Lee, Taehyun and Hong, Seokhee and Ahn, Jaewoo and Hong, Ilgee and Lee, Hwaran and Yun, Sangdoo and Shin, Jamin and Kim, Gunhee",
        "booktitle": "ACL2024",
        "title": "Who Wrote this Code? Watermarking for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to detect machine-generated text by embedding watermarks are being developed.However, we discover that the existing works fail to function appropriately in code generation tasks due to the task\u2019s nature of having low entropy.Extending a logit-modifying watermark method, we propose Selective WatErmarking via Entropy Thresholding (SWEET), which enhances detection ability and mitigates code quality degeneration by removing low-entropy segments at generating and detecting watermarks.Our experiments show that SWEET significantly improves code quality preservation while outperforming all baselines, including post-hoc detection methods, in detecting machine-generated code text.Our code is available inhttps://github.com/hongcheki/sweet-watermark.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.268",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving": {
        "type": "INPROCEEDINGS",
        "key": "islam-etal-2024-mapcoder",
        "author": "Islam, Md. Ashraful and Ali, Mohammed Eunus and Parvez, Md Rizwan",
        "booktitle": "ACL2024",
        "title": "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code synthesis, which requires a deep understanding of complex natural language (NL) problem descriptions, generation of code instructions for complex algorithms and data structures, and the successful execution of comprehensive unit tests, presents a significant challenge. Thus, while large language models (LLMs) demonstrate impressive proficiency in natural language processing (NLP), their performance in code generation tasks remains limited. In this paper, we introduce a new approach to code generation tasks leveraging the multi-agent prompting that uniquely replicates the full cycle of program synthesis as observed in human developers. Our framework, MapCoder, consists of four LLM agents specifically designed to emulate the stages of this cycle: recalling relevant examples, planning, code generation, and debugging. After conducting thorough experiments, with multiple LLMs ablations and analyses across eight challenging competitive problem-solving and program synthesis benchmarks\u2014MapCoder showcases remarkable code generation capabilities, achieving their new state-of-the-art (pass@1) results\u2014(HumanEval 93.9%, MBPP 83.1%, APPS 22.0%, CodeContests 28.5%, and xCodeEval 45.3%). Moreover, our method consistently delivers superior performance across various programming languages and varying problem difficulties. We open-source our framework at https://github.com/Md-Ashraful-Pramanik/MapCoder.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.269",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "LLM Knows Body Language, Too: Translating Speech Voices into Human Gestures": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-llm",
        "author": "Xu, Chenghao and Lyu, Guangtao and Yan, Jiexi and Yang, Muli and Deng, Cheng",
        "booktitle": "ACL2024",
        "title": "LLM Knows Body Language, Too: Translating Speech Voices into Human Gestures",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In response to the escalating demand for digital human representations, progress has been made in the generation of realistic human gestures from given speeches. Despite the remarkable achievements of recent research, the generation process frequently includes unintended, meaningless, or non-realistic gestures. To address this challenge, we propose a gesture translation paradigm, GesTran, which leverages large language models (LLMs) to deepen the understanding of the connection between speech and gesture and sequentially generates human gestures by interpreting gestures as a unique form of body language. The primary stage of the proposed framework employs a transformer-based auto-encoder network to encode human gestures into discrete symbols. Following this, the subsequent stage utilizes a pre-trained LLM to decipher the relationship between speech and gesture, translating the speech into gesture by interpreting the gesture as unique language tokens within the LLM. Our method has demonstrated state-of-the-art performance improvement through extensive and impartial experiments conducted on public TED and TED-Expressive datasets.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.273",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "duan-etal-2024-shifting",
        "author": "Duan, Jinhao and Cheng, Hao and Wang, Shiqi and Zavalny, Alex and Wang, Chenan and Xu, Renjing and Kailkhura, Bhavya and Xu, Kaidi",
        "booktitle": "ACL2024",
        "title": "Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) show promising results in language generation and instruction following but frequently \u201challucinate\u201d, making their outputs less reliable. Despite Uncertainty Quantification\u2019s (UQ) potential solutions, implementing it accurately within LLMs is challenging. Our research introduces a simple heuristic: not all tokens in auto-regressive LLM text equally represent the underlying meaning, as \u201clinguistic redundancy\u201d often allows a few keywords to convey the essence of long sentences. However, current methods underestimate this inequality when assessing uncertainty, causing tokens with limited semantics to be equally or excessively weighted in UQ. To correct this, we propose Shifting Attention to more Relevant (SAR) components at both token- and sentence-levels for better UQ. We conduct extensive experiments involving a range of popular \u201coff-the-shelf\u201d LLMs, such as Vicuna, WizardLM, and LLaMA-2-chat, with model sizes extending up to 33B parameters. We evaluate various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&amp;A, and medical Q&amp;A. Our experimental results, coupled with a comprehensive demographic analysis, demonstrate the superior performance of SAR. The code is available at https://github.com/jinhaoduan/SAR.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.276",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning": {
        "type": "INPROCEEDINGS",
        "key": "yu-etal-2024-wavecoder",
        "author": "Yu, Zhaojian and Zhang, Xin and Shang, Ning and Huang, Yangyu and Xu, Can and Zhao, Yishujie and Hu, Wenxiang and Yin, Qiufeng",
        "booktitle": "ACL2024",
        "title": "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeOcean, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.280",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Eliciting Better Multilingual Structured Reasoning from LLMs through Code": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-eliciting-better",
        "author": "Li, Bryan and Alkhouli, Tamer and Bonadiman, Daniele and Pappas, Nikolaos and Mansour, Saab",
        "booktitle": "ACL2024",
        "title": "Eliciting Better Multilingual Structured Reasoning from LLMs through Code",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The development of large language models (LLM) has shown progress on reasoning, though studies have largely considered either English or simple reasoning tasks. To address this, we introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks.We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multilingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reasoning subtask. Furthermore, the models show no regression on non-reasoning tasks, thus demonstrating our techniques maintain general-purpose abilities.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.281",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs": {
        "type": "INPROCEEDINGS",
        "key": "kim-etal-2024-threads",
        "author": "Kim, Zae Myung and Lee, Kwang and Zhu, Preston and Raheja, Vipul and Kang, Dongyeop",
        "booktitle": "ACL2024",
        "title": "Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers\u2019 overall performance in distinguishing between human-written and machine-generated texts, even on out-of-distribution and paraphrased samples. This underscores the significance of incorporating hierarchical discourse features in the analysis of text patterns. The code and dataset will be available at [TBA].",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.298",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "NICE: To Optimize In-Context Examples or Not?": {
        "type": "INPROCEEDINGS",
        "key": "srivastava-etal-2024-nice",
        "author": "Srivastava, Pragya and Golechha, Satvik and Deshpande, Amit and Sharma, Amit",
        "booktitle": "ACL2024",
        "title": "NICE: To Optimize In-Context Examples or Not?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent work shows that in-context learning and optimization of in-context examples (ICE) can significantly improve the accuracy of large language models (LLMs) on a wide range of tasks, leading to an apparent consensus that ICE optimization is crucial for better performance. However, most of these studies assume a fixed or no instruction provided in the prompt. We challenge this consensus by investigating the necessity of optimizing ICE when task-specific instructions are provided and find that there are many tasks for which it yields diminishing returns. In particular, using a diverse set of tasks and a systematically created instruction set with gradually added details, we find that as the prompt instruction becomes more detailed, the returns on ICE optimization diminish. To characterize this behavior, we introduce a task-specific metric called Normalized Invariability to Choice of Examples (NICE) that quantifies the learnability of tasks from a given instruction, and provides a heuristic to help decide whether to optimize instructions or ICE for a new task. Given a task, the proposed metric can reliably predict the utility of optimizing ICE compared to using random ICE. Our code is available at [https://github.com/microsoft/nice-icl](https://github.com/microsoft/nice-icl).",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.300",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation": {
        "type": "INPROCEEDINGS",
        "key": "yan-etal-2024-codescope",
        "author": "Yan, Weixiang and Liu, Haitian and Wang, Yunkun and Li, Yunzhe and Chen, Qian and Wang, Wen and Lin, Tingyu and Zhao, Weishan and Zhu, Li and Sundaram, Hari and Deng, Shuiguang",
        "booktitle": "ACL2024",
        "title": "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce **CodeScope**, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers **43 programming languages** and **eight coding tasks**. It evaluates the coding performance of LLMs from three dimensions (perspectives): **length**, **difficulty**, and **efficiency**. To facilitate execution-based evaluations of code generation, we develop **MultiCodeEngine**, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at https://github.com/WeixiangYAN/CodeScope.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.301",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-safedecoding",
        "author": "Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Jia, Jinyuan and Lin, Bill Yuchen and Poovendran, Radha",
        "booktitle": "ACL2024",
        "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, which aim to provoke unintended and unsafe behaviors from LLMs, remain a significant LLM safety threat. We analyze tokens, which are the smallest unit of text that can be processed by LLMs and make the following observations: (1) probabilities of tokens representing harmful responses are higher than those of harmless responses, and (2) responses containing safety disclaimers appear among the top tokens when token probabilities are sorted in descending order. In this paper, we leverage (1) and (2) to develop SafeDecoding, a safety-aware decoding strategy for LLMs, to defend against jailbreak attacks. We perform extensive experiments to evaluate SafeDecoding against six SOTA jailbreak attacks (GCG, AutoDAN, PAIR, DeepInception, SAP30, and template based attack) on five LLMs (Vicuna, Llama2, Guanaco, falcon, and Dolphin) using four benchmark datasets (AdvBench, HEx-PHI, MT-Bench, and Just-Eval). Our results show that SafeDecoding significantly reduces attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries while outperforming six defense methods (Perpelexity, Paraphrase, Retokenization, Self-Reminder, ICD, and Self-Examination).",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.303",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?": {
        "type": "INPROCEEDINGS",
        "key": "son-etal-2024-multi-task",
        "author": "Son, Guijin and Baek, SangWon and Nam, Sangdae and Jeong, Ilgyun and Kim, Seungone",
        "booktitle": "ACL2024",
        "title": "Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench (Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by \\times 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench dataset and our code at this [link](https://anonymous.4open.science/r/MTI-Bench-6F01).",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.304",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Experiential Co-Learning of Software-Developing Agents": {
        "type": "INPROCEEDINGS",
        "key": "qian-etal-2024-experiential",
        "author": "Qian, Chen and Dang, Yufan and Li, Jiahao and Liu, Wei and Xie, Zihao and Wang, YiFei and Chen, Weize and Yang, Cheng and Cong, Xin and Che, Xiaoyin and Liu, Zhiyuan and Sun, Maosong",
        "booktitle": "ACL2024",
        "title": "Experiential Co-Learning of Software-Developing Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce Experiential Co-Learning, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at https://github.com/OpenBMB/ChatDev.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.305",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "PRP-Graph: Pairwise Ranking Prompting to LLMs with Graph Aggregation for Effective Text Re-ranking": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-prp",
        "author": "Luo, Jian and Chen, Xuanang and He, Ben and Sun, Le",
        "booktitle": "ACL2024",
        "title": "PRP-Graph: Pairwise Ranking Prompting to LLMs with Graph Aggregation for Effective Text Re-ranking",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Pairwise Ranking Prompting (PRP) demonstrates impressive effectiveness in zero-shot document re-ranking tasks with large language models (LLMs). However, in the existing methods, PRP only outputs the same label for the comparison results of different confidence intervals without considering the uncertainty of pairwise comparison, which implies an underutilization of the generation probability information of LLMs. To bridge this gap, we propose PRP-Graph, a novel pairwise re-ranking approach, based on a refined scoring PRP unit that exploits the output probabilities of target labels to capture the degree of certainty of the comparison results. Specifically, the PRP-Graph consists of two stages, namely ranking graph construction and ranking graph aggregation. Extensive experiments conducted on the BEIR benchmark demonstrate the superiority of our approach over existing PRP-based methods. Comprehensive analysis reveals that the PRP-Graph displays strong robustness towards the initial ranking order and delivers exceptional re-ranking results with acceptable efficiency. Our code and data are available at https://github.com/Memelank/PRP-Graph.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.313",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "RepCodec: A Speech Representation Codec for Speech Tokenization": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-repcodec",
        "author": "Huang, Zhichao and Meng, Chutong and Ko, Tom",
        "booktitle": "ACL2024",
        "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec.We believe our method can facilitate large language modeling research on speech processing.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.314",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Stealthy Attack on Large Language Model based Recommendation": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-stealthy",
        "author": "Zhang, Jinghao and Liu, Yuting and Liu, Qiang and Wu, Shu and Guo, Guibing and Wang, Liang",
        "booktitle": "ACL2024",
        "title": "Stealthy Attack on Large Language Model based Recommendation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item\u2019s exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model\u2019s training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior efficacy and stealthiness of our approach. Our work unveils a significant security gap in LLM-based recommendation systems and paves the way for future research on protecting these systems.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.318",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-masked",
        "author": "Chen, Changyu and Wang, Xiting and Lin, Ting-En and Lv, Ang and Wu, Yuchuan and Gao, Xin and Wen, Ji-Rong and Yan, Rui and Li, Yongbin",
        "booktitle": "ACL2024",
        "title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models insuch domains. Earlier fine-tuning approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a techniquewe found to be particularly effective for reasoning tasks. When applied to fine-tuning with GSM8K on Llama-2-7B, this method achieveda 5% improvement in GSM8K accuracy and a 10% improvement in GSM-IC accuracy over standard supervised fine-tuning with a few codes modified. Furthermore, it is complementary to existing methods. When integrated with related explicit data augmentation methods, it leads to improvements across five datasets of various augmentation methods, as well as two different base models. We further investigate the mechanisms behind this improvement through case studies and quantitative analysis, suggesting that our approach may provide superior support for the model in capturing long-distance dependencies, especially those related to questions. This enhancement could deepen understanding of the premises in questions and prior steps.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.320",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "EmoBench: Evaluating the Emotional Intelligence of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "sabour-etal-2024-emobench",
        "author": "Sabour, Sahand and Liu, Siyang and Zhang, Zheyuan and Liu, June and Zhou, Jinfeng and Sunaryo, Alvionna and Lee, Tatia and Mihalcea, Rada and Huang, Minlie",
        "booktitle": "ACL2024",
        "title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.326",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Are AI-Generated Text Detectors Robust to Adversarial Perturbations?": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-ai",
        "author": "Huang, Guanhua and Zhang, Yuchen and Li, Zhe and You, Yongjian and Wang, Mingze and Yang, Zhouwang",
        "booktitle": "ACL2024",
        "title": "Are AI-Generated Text Detectors Robust to Adversarial Perturbations?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The widespread use of large language models (LLMs) has sparked concerns about the potential misuse of AI-generated text, as these models can produce content that closely resembles human-generated text. Current detectors for AI-generated text (AIGT) lack robustness against adversarial perturbations, with even minor changes in characters or words causing a reversal in distinguishing between human-created and AI-generated text. This paper investigates the robustness of existing AIGT detection methods and introduces a novel detector, the Siamese Calibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction network to add and remove noise from text, extracting a semantic representation that is robust to local perturbations. We also propose a siamese calibration technique to train the model to make equally confident predictions under different noise, which improves the model\u2019s robustness against adversarial perturbations. Experiments on four publicly available datasets show that the SCRN outperforms all baseline methods, achieving 6.5%-18.25% absolute accuracy improvement over the best baseline method under adversarial attacks. Moreover, it exhibits superior generalizability in cross-domain, cross-genre, and mixed-source scenarios. The code is available at https://github.com/CarlanLark/Robust-AIGC-Detector.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.327",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-rethinking-bounds",
        "author": "Wang, Qineng and Wang, Zihao and Su, Ying and Tong, Hanghang and Song, Yangqiu",
        "booktitle": "ACL2024",
        "title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same best performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observed that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion. Our code can be found in https://github.com/HKUST-KnowComp/LLM-discussion.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.331",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "TasTe: Teaching Large Language Models to Translate through Self-Reflection": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-taste",
        "author": "Wang, Yutong and Zeng, Jiali and Liu, Xuebo and Meng, Fandong and Zhou, Jie and Zhang, Min",
        "booktitle": "ACL2024",
        "title": "TasTe: Teaching Large Language Models to Translate through Self-Reflection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have exhibited remarkable performance in various natural language processing tasks. Techniques like instruction tuning have effectively enhanced the proficiency of LLMs in the downstream task of machine translation. However, the existing approaches fail to yield satisfactory translation outputs that match the quality of supervised neural machine translation (NMT) systems. One plausible explanation for this discrepancy is that the straightforward prompts employed in these methodologies are unable to fully exploit the acquired instruction-following capabilities. To this end, we propose the TasTe framework, which stands for translating through self-reflection. The self-reflection process includes two stages of inference. In the first stage, LLMs are instructed to generate preliminary translations and conduct self-assessments on these translations simultaneously. In the second stage, LLMs are tasked to refine these preliminary translations according to the evaluation results. The evaluation results in four language directions on the WMT22 benchmark reveal the effectiveness of our approach compared to existing methods. Our work presents a promising approach to unleash the potential of LLMs and enhance their capabilities in MT. The codes and datasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.333",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "lu-etal-2024-experts",
        "author": "Lu, Xudong and Liu, Qi and Xu, Yuhui and Zhou, Aojun and Huang, Siyuan and Zhang, Bo and Yan, Junchi and Li, Hongsheng",
        "booktitle": "ACL2024",
        "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer active parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Code will be made available at https://github.com/Lucky-Lance/Expert_Sparsity.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.334",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-unimo",
        "author": "Li, Wei and Xu, Xue and Liu, Jiachen and Xiao, Xinyan",
        "booktitle": "ACL2024",
        "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.335",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models\u2019 Understanding of Discourse Relations": {
        "type": "INPROCEEDINGS",
        "key": "miao-etal-2024-discursive",
        "author": "Miao, Yisong and Liu, Hongfu and Lei, Wenqiang and Chen, Nancy and Kan, Min-Yen",
        "booktitle": "ACL2024",
        "title": "Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models\u2019 Understanding of Discourse Relations",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models have significantly enhanced the effectiveness of discourse relation classifications, it remains unclear whether their comprehension is faithful and reliable. We provide DiSQ, a new method for evaluating the faithfulness of understanding discourse based on question answering. We first employ in-context learning to annotate the reasoning for discourse comprehension, based on the connections among key events within the discourse. Following this, DiSQ interrogates the model with a sequence of questions to assess its grasp of core event relations, its resilience to counterfactual queries, as well as its consistency to its previous responses. then evaluate language models with different architectural designs using DiSQ, finding: (1) DiSQ presents a significant challenge for all models, with the top-performing GPT model attaining only 41% of the ideal performance in PDTB; (2) DiSQ is robust to domain shifts and paraphrase variations; (3) Open-source models generally lag behind their closed-source GPT counterparts, with notable exceptions being those enhanced with chat and code/math features; (4) Our analysis validates the effectiveness of explicitly signalled discourse connectives, the role of contextual information, and the benefits of using historical QA data.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.341",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Self-Evolving GPT: A Lifelong Autonomous Experiential Learner": {
        "type": "INPROCEEDINGS",
        "key": "gao-etal-2024-self-evolving",
        "author": "Gao, Jinglong and Ding, Xiao and Cui, Yiming and Zhao, Jianbai and Wang, Hepeng and Liu, Ting and Qin, Bing",
        "booktitle": "ACL2024",
        "title": "Self-Evolving GPT: A Lifelong Autonomous Experiential Learner",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "To improve the performance of large language models (LLMs), researchers have explored providing LLMs with textual task-solving experience via prompts. However, they rely on manual efforts to acquire and apply such experience for each task, which is not feasible for the growing demand for LLMs and the variety of user questions.To address this issue, we design a lifelong autonomous experiential learning framework based on LLMs to explore whether LLMs can imitate human ability for learning and utilizing experience. It autonomously learns and accumulates experience through experience transfer and induction, categorizing the types of input questions to select which accumulated experience to employ for them.Experimental results on six widely used NLP datasets show that our framework performs reliably in each intermediate step and effectively improves the performance of GPT-3.5 and GPT-4. This validates the feasibility of using LLMs to mimic human experiential learning and application capabilities, offering a new path worth further exploration for the evolution of machine intelligence. Additionally, we provide a detailed analysis of the behavior of our framework at each step.We will open source codes after the acceptance, fostering open research in the NLP community and beyond.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.346",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "WRP: Weight Recover Prune for Structured Sparsity": {
        "type": "INPROCEEDINGS",
        "key": "tan-etal-2024-wrp",
        "author": "Tan, Zhendong and Zhang, Xingjun and Wei, Zheng",
        "booktitle": "ACL2024",
        "title": "WRP: Weight Recover Prune for Structured Sparsity",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "As the scale of Large Language Models (LLMs) increases, it is necessary to compress the models to reduce the substantial demand on computational resources. Network pruning significantly reduces the model size by converting the weight matrix from dense to sparse data format. Current methodologies advocate for one-shot pruning to avoid the expense of retraining, ensuring the maintenance of model performance under conditions of 50%-60% unstructured pruning. Nevertheless, matrices characterized by this level of sparsity could not be treated as sparse matrices, because the indices would incur significant costs. To mitigate this problem, NVIDIA introduced the 2:4 structured sparsity. However, we observe a notable decline in model performance when adopting 2:4 structured sparsity due to group constraints. In this paper, we introduce the Weight Recover Prune (WRP) approach. By recovering a minimal set of critical weights, WRP aims to enhance model performance while maintaining the efficiency of the compression. Our evaluation of the WRP method on the LLAMA2 and OPT models shows that it outperforms other 2:4 pattern one-shot pruning methods. Meanwhile, WRP can guarantee that the size of the pruned model is about 60% of the dense model. Our code is available at: https://github.com/TanZhendong/WRP.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.347",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "LLaMA Pro: Progressive LLaMA with Block Expansion": {
        "type": "INPROCEEDINGS",
        "key": "wu-etal-2024-llama",
        "author": "Wu, Chengyue and Gan, Yukang and Ge, Yixiao and Lu, Zeyu and Wang, Jiahao and Feng, Ye and Shan, Ying and Luo, Ping",
        "booktitle": "ACL2024",
        "title": "LLaMA Pro: Progressive LLaMA with Block Expansion",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model\u2019s knowledge while mitigating forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro - Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.352",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "GroundingGPT: Language Enhanced Multi-modal Grounding Model": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-groundinggpt",
        "author": "Li, Zhaowei and Xu, Qi and Zhang, Dong and Song, Hang and Cai, YiQing and Qi, Qi and Zhou, Ran and Pan, Junting and Li, Zefeng and Tu, Vu and Huang, Zhida and Wang, Tao",
        "booktitle": "ACL2024",
        "title": "GroundingGPT: Language Enhanced Multi-modal Grounding Model",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multi-modal large language models (MLLMs) have demonstrated remarkable performance across various tasks. However, these models often prioritize capturing global information and overlook the importance of perceiving local information. This limitation hinders their ability to effectively understand fine-grained details and handle grounding tasks that necessitate nuanced comprehension. Although some recent works have made strides in this, they have primarily focused on single-modality inputs. Therefore, we propose GroundingGPT, an end-to-end language enhanced multi-modal grounding model. It is designed to perform fine-grained grounding tasks for three modalities: image, video and audio. To enhance the model\u2019s performance, we adopt a coarse-to-fine training strategy, utilizing a three-stage training approach to progressively enhance the model\u2019s semantic awareness and fine-grained understanding capabilities. Additionally, we employ a diversified stage-specific dataset construction pipeline, developing a multi-modal, multi-granularity dataset tailored for training the model in different stages. Extensive experiments conducted on multiple multi-modal benchmarks demonstrate that our model achieves impressive fine-grained understanding of multi-modal inputs on grounding tasks while maintaining or improving its global comprehension capabilities. Our code, model, and dataset are available at https://github.com/lzw-lzw/GroundingGPT.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.360",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval": {
        "type": "INPROCEEDINGS",
        "key": "khan-etal-2024-xcodeeval",
        "author": "Khan, Mohammad Abdullah Matin and Bari, M. Saiful and Long, Do and Wang, Weishi and Parvez, Md Rizwan and Joty, Shafiq",
        "booktitle": "ACL2024",
        "title": "XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce *xCodeEval*, the largest executable multilingual multitask benchmark to date consisting of 25 M document-level coding examples (16.5 B tokens) from about 7.5 K unique problems covering up to 11 programming languages with execution-level parallelism. It features a total of 7 tasks involving code understanding, generation, translation and retrieval. *xCodeEval* adopts an execution-based evaluation and offers a multilingual code execution engine, *ExecEval* that supports unit test based execution in all the 11 languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI\u2019s LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate to be quite challenging as per the current advancements in language models.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.367",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations": {
        "type": "INPROCEEDINGS",
        "key": "dutt-etal-2024-leveraging",
        "author": "Dutt, Ritam and Wu, Zhen and Shi, Jiaxin and Sheth, Divyanshu and Gupta, Prakhar and Rose, Carolyn",
        "booktitle": "ACL2024",
        "title": "Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present a generalizable classification approach that leverages Large Language Models (LLMs) to facilitate the detection of implicitly encoded social meaning in conversations. We design a multi-faceted prompt to extract a textual explanation of the reasoning that connects visible cues to underlying social meanings. These extracted explanations or rationales serve as augmentations to the conversational text to facilitate dialogue understanding and transfer. Our empirical results over 2,340 experimental settings demonstrate the significant positive impact of adding these rationales. Our findings hold true for in-domain classification, zero-shot, and few-shot domain transfer for two different social meaning detection tasks, each spanning two different corpora.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.373",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Lightweight reranking for language model generations": {
        "type": "INPROCEEDINGS",
        "key": "jain-etal-2024-lightweight",
        "author": "Jain, Siddhartha and Ma, Xiaofei and Deoras, Anoop and Xiang, Bing",
        "booktitle": "ACL2024",
        "title": "Lightweight reranking for language model generations",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) can exhibit considerable variation in the quality of their sampled outputs. Reranking and selecting the best generation from the sampled set is a popular way of obtaining strong gains in generation quality. In this paper, we present a novel approach for reranking LLM generations. Unlike other techniques that might involve additional inferences or training a specialized reranker, our approach relies on easy to compute pairwise statistics between the generations that have minimal compute overhead. We show that our approach can be formalized as an extension of self-consistency and analyze its performance in that framework, theoretically as well as via simulations. We show strong improvements for selecting the best k generations for code generation tasks as well as robust improvements for the best generation for the tasks of autoformalization, summarization, and translation. While our approach only assumes black-box access to LLMs, we show that additional access to token probabilities can improve performance even further.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.376",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-plug",
        "author": "Zhang, Zhihan and Lee, Dong-Ho and Fang, Yuwei and Yu, Wenhao and Jia, Mengzhao and Jiang, Meng and Barbieri, Francesco",
        "booktitle": "ACL2024",
        "title": "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following abilities of LLMs by 29% on average, compared to directly responding in the target language alone. Further experiments validate the versatility of our approach by employing alternative pivot languages beyond English to assist languages where LLMs exhibit lower proficiency. Code and data are available at https://github.com/ytyz1307zzh/PLUG.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.379",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-enhancing-eeg",
        "author": "Wang, Jiaqi and Song, Zhenxi and Ma, Zhengyu and Qiu, Xipeng and Zhang, Min and Zhang, Zhiguo",
        "booktitle": "ACL2024",
        "title": "Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modules alongside the EEG stream from CET-MAE and further enables an LLM (specifically BART) to decode text from EEG sequences. Comprehensive experiments conducted on the popular text-evoked EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms the baseline framework in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%, respectively. Our proposed pre-trained EEG-Text model shows the potential to improve downstream tasks involving EEG and text. This opens up promising avenues for its application in inner speech BCI paradigms, meriting further investigation.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.393",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues": {
        "type": "INPROCEEDINGS",
        "key": "bai-etal-2024-mt",
        "author": "Bai, Ge and Liu, Jie and Bu, Xingyuan and He, Yancheng and Liu, Jiaheng and Zhou, Zhanhui and Lin, Zhuoran and Su, Wenbo and Ge, Tiezheng and Zheng, Bo and Ouyang, Wanli",
        "booktitle": "ACL2024",
        "title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities. The data and code are available at https://github.com/mtbench101/mt-bench-101.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.401",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Unlocking the Power of Large Language Models for Entity Alignment": {
        "type": "INPROCEEDINGS",
        "key": "jiang-etal-2024-unlocking",
        "author": "Jiang, Xuhui and Shen, Yinghan and Shi, Zhichao and Xu, Chengjin and Li, Wei and Li, Zixuan and Guo, Jian and Shen, Huawei and Wang, Yuanzhuo",
        "booktitle": "ACL2024",
        "title": "Unlocking the Power of Large Language Models for Entity Alignment",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs\u2019 capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy while preserving efficiency. Our experimental results affirm ChatEA\u2019s superior performance, highlighting LLMs\u2019 potential in facilitating EA tasks.The source code is available at https://anonymous.4open.science/r/ChatEA/.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.408",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-cognitive",
        "author": "Li, Yunxin and Chen, Xinyu and Hu, Baotian and Shi, Haoyuan and Zhang, Min",
        "booktitle": "ACL2024",
        "title": "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.411",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs": {
        "type": "INPROCEEDINGS",
        "key": "bakman-etal-2024-mars",
        "author": "Bakman, Yavuz Faruk and Yaldiz, Duygu Nur and Buyukates, Baturalp and Tao, Chenyang and Dimitriadis, Dimitrios and Avestimehr, Salman",
        "booktitle": "ACL2024",
        "title": "MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical QA dataset. Code can be found here.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.419",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Text Embedding Inversion Security for Multilingual Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-text",
        "author": "Chen, Yiyi and Lent, Heather and Bjerva, Johannes",
        "booktitle": "ACL2024",
        "title": "Text Embedding Inversion Security for Multilingual Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Textual data is often represented as real-numbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be susceptible to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages potentially exposed to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and crosslingual inversion attacks, and explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English-based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for both monolingual and multilingual models. This study is the first to investigate multilingual inversion attacks, shedding light on the differences in attacks and defenses across monolingual and multilingual settings.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.422",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "LLMEmbed: Rethinking Lightweight LLM\u2019s Genuine Function in Text Classification": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-llmembed",
        "author": "Liu, Chun and Zhang, Hongguang and Zhao, Kainan and Ju, Xinghai and Yang, Lin",
        "booktitle": "ACL2024",
        "title": "LLMEmbed: Rethinking Lightweight LLM\u2019s Genuine Function in Text Classification",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas. Recently, many attempts based on prompt-learning have been made to improve the performance of text classification. However, most of these methods are based on heuristic Chain-of-Thought (CoT), and tend to be more complex but less efficient. In this paper, we rethink the LLM-based text classification methodology, propose a simple and effective transfer learning strategy, namely LLMEmbed, to address this classical but challenging task. To illustrate, we first study how to properly extract and fuse the text embeddings via various lightweight LLMs at different network depths to improve their robustness and discrimination, then adapt such embeddings to train the classifier. We perform extensive experiments on publicly available datasets, and the results show that LLMEmbed achieves strong performance while enjoys low training overhead using lightweight LLM backbones compared to recent methods based on larger LLMs, *i.e.* GPT-3, and sophisticated prompt-based strategies. Our LLMEmbed achieves adequate accuracy on publicly available benchmarks without any fine-tuning while merely use 4% model parameters, 1.8% electricity consumption and 1.5% runtime compared to its counterparts. Code is available at: https://github.com/ChunLiu-cs/LLMEmbed-ACL2024.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.433",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "PokeMQA: Programmable knowledge editing for Multi-hop Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "gu-etal-2024-pokemqa",
        "author": "Gu, Hengrui and Zhou, Kaixiong and Han, Xiaotian and Liu, Ninghao and Wang, Ruobing and Wang, Xin",
        "booktitle": "ACL2024",
        "title": "PokeMQA: Programmable knowledge editing for Multi-hop Question Answering",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multi-hop question answering (MQA) is one of the challenging tasks to evaluate machine\u2019s comprehension and reasoning abilities, where large language models (LLMs) have widely achieved the human-comparable performance. Due to the dynamics of knowledge facts in real world, knowledge editing has been explored to update model with the up-to-date facts while avoiding expensive re-training or fine-tuning. Starting from the edited fact, the updated model needs to provide cascading changes in the chain of MQA. The previous art simply adopts a mix-up prompt to instruct LLMs conducting multiple reasoning tasks sequentially, including question decomposition, answer generation, and conflict checking via comparing with edited facts. However, the coupling of these functionally-diverse reasoning tasks inhibits LLMs\u2019 advantages in comprehending and answering questions while disturbing them with the unskilled task of conflict checking. We thus propose a framework, Programmable knowledge editing for Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically, we prompt LLMs to decompose knowledge-augmented multi-hop question, while interacting with a detached trainable scope detector to modulate LLMs behavior depending on external conflict signal. The experiments on three LLM backbones and two benchmark datasets validate our superiority in knowledge editing of MQA, outperforming all competitors by a large margin in almost all settings and consistently producing reliable reasoning process.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.438",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Full Parameter Fine-tuning for Large Language Models with Limited Resources": {
        "type": "INPROCEEDINGS",
        "key": "lv-etal-2024-full",
        "author": "Lv, Kai and Yang, Yuqing and Liu, Tengxiao and Guo, Qipeng and Qiu, Xipeng",
        "booktitle": "ACL2024",
        "title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 \\times RTX 3090, each with 24GB memory. Code and data are available at https://github.com/OpenLMLab/LOMO.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.445",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "BizBench: A Quantitative Reasoning Benchmark for Business and Finance": {
        "type": "INPROCEEDINGS",
        "key": "krumdick-etal-2024-bizbench",
        "author": "Krumdick, Michael and Koncel-Kedziorski, Rik and Lai, Viet and Reddy, Varshini and Lovering, Charles and Tanner, Chris",
        "booktitle": "ACL2024",
        "title": "BizBench: A Quantitative Reasoning Benchmark for Business and Finance",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Answering questions within business and finance requires reasoning, precision, and a wide-breadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models\u2019 ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. We include three financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model\u2019s financial background knowledge, ability to parse financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, comparing and contrasting the behavior of code-focused and language-focused models. We demonstrate that the current bottleneck in performance is due to LLMs\u2019 limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.452",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-docllm",
        "author": "Wang, Dongsheng and Raman, Natraj and Sibue, Mathieu and Ma, Zhiqiang and Babkin, Petr and Kaur, Simerjot and Pei, Yulong and Nourbakhsh, Armineh and Liu, Xiaomo",
        "booktitle": "ACL2024",
        "title": "DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Enterprise documents such as forms, receipts, reports, and other such records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.463",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-large-language-models",
        "author": "Li, Zekun and Chen, Zhiyu and Ross, Mike and Huber, Patrick and Moon, Seungwhan and Lin, Zhaojiang and Dong, Xin and Sagar, Adithya and Yan, Xifeng and Crook, Paul",
        "booktitle": "ACL2024",
        "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT\u2019s performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available at https://github.com/facebookresearch/FnCTOD.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.471",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation": {
        "type": "INPROCEEDINGS",
        "key": "niu-etal-2024-enhancing",
        "author": "Niu, Cheng and Wang, Xingguang and Cheng, Xuxin and Song, Juntong and Zhang, Tong",
        "booktitle": "ACL2024",
        "title": "Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Dialogue State Tracking (DST) is designed to monitor the evolving dialogue state in the conversations and plays a pivotal role in developing task-oriented dialogue systems. However, obtaining the annotated data for the DST task is usually a costly endeavor. In this paper, we focus on employing LLMs to generate dialogue data to reduce dialogue collection and annotation costs. Specifically, GPT-4 is used to simulate the user and agent interaction, generating thousands of dialogues annotated with DST labels. Then a two-stage fine-tuning on LLaMA 2 is performed on the generated data and the real data for the DST prediction. Experimental results on two public DST benchmarks show that with the generated dialogue data, our model performs better than the baseline trained solely on real data. In addition, our approach is also capable of adapting to the dynamic demands in real-world scenarios, generating dialogues in new domains swiftly. After replacing dialogue segments in any domain with the corresponding generated ones, the model achieves comparable performance to the model trained on real data. The source code and generated dialogue data are available at https://github.com/ParticleMedia/LUAS.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.473",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-defending",
        "author": "Zhang, Zhexin and Yang, Junxiao and Ke, Pei and Mi, Fei and Wang, Hongning and Huang, Minlie",
        "booktitle": "ACL2024",
        "title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of effort in defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the intrinsic conflict between the goals of being helpful and ensuring safety. Accordingly, we propose to integrate goal prioritization at both training and inference stages to counteract. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And integrating goal prioritization into model training reduces the ASR from 71.0% to 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks, both because of their stronger ability in instruction following. Our work thus contributes to the comprehension of jailbreaking attacks and defenses, and sheds light on the relationship between LLMs\u2019 capability and safety. Our code is available at https://github.com/thu-coai/JailbreakDefense_GoalPriority.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.481",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-truthx",
        "author": "Zhang, Shaolei and Yu, Tian and Feng, Yang",
        "booktitle": "ACL2024",
        "title": "TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) sometimes suffer from producing hallucinations, especially LLMs may generate untruthful responses despite knowing the correct knowledge. Activating the truthfulness within LLM is the key to fully unlocking LLM\u2019s knowledge potential. In this paper, we propose TruthX, an inference-time intervention method to activate the truthfulness of LLM by identifying and editing the features within LLM\u2019s internal representations that govern the truthfulness. TruthX employs an auto-encoder to map LLM\u2019s representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM\u2019s internal representations in truthful space, TruthX effectively enhances the truthfulness of LLM. Experiments show that TruthX improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to produce truthful or hallucinatory responses via editing only one vector in LLM\u2019s internal representations.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.483",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ju-etal-2024-investigating",
        "author": "Ju, Tianjie and Chen, Yijin and Yuan, Xinwei and Zhang, Zhuosheng and Du, Wei and Zheng, Yubin and Liu, Gongshen",
        "booktitle": "ACL2024",
        "title": "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximately 20% of the failures are attributed to shortcuts, and the initial and terminal entities in these failure instances usually have higher co-occurrences in the pre-training corpus. Finally, we propose erasing shortcut neurons to mitigate the associated risks and find that this approach significantly reduces failures in multiple-hop knowledge editing caused by shortcuts. Code is publicly available at https://github.com/Jometeorie/MultiHopShortcuts.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.486",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Why Don\u2019t Prompt-Based Fairness Metrics Correlate?": {
        "type": "INPROCEEDINGS",
        "key": "zayed-etal-2024-dont",
        "author": "Zayed, Abdelrahman and Mordido, Goncalo and Baldini, Ioana and Chandar, Sarath",
        "booktitle": "ACL2024",
        "title": "Why Don\u2019t Prompt-Based Fairness Metrics Correlate?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The widespread use of large language models has brought up essential questions about the potential biases these models might learn. This led to the development of several metrics aimed at evaluating and mitigating these biases. In this paper, we first demonstrate that prompt-based fairness metrics exhibit poor agreement, as measured by correlation, raising important questions about the reliability of fairness assessment using prompts. Then, we outline six relevant reasons why such a low correlation is observed across existing metrics. Based on these insights, we propose a method called Correlated Fairness Output (CAIRO) to enhance the correlation between fairness metrics. CAIRO augments the original prompts of a given fairness metric by using several pre-trained language models and then selects the combination of the augmented prompts that achieves the highest correlation across metrics. We show a significant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and 0.98 across metrics for gender and religion biases, respectively. Our code is available at https://github.com/chandar-lab/CAIRO.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.487",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Text-like Encoding of Collaborative Information in Large Language Models for Recommendation": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-text",
        "author": "Zhang, Yang and Bao, Keqin and Yan, Ming and Wang, Wenjie and Feng, Fuli and He, Xiangnan",
        "booktitle": "ACL2024",
        "title": "Text-like Encoding of Collaborative Information in Large Language Models for Recommendation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "When adapting Large Language Models for Recommendation (LLMRec), it is crucial to integrate collaborative information. Existing methods achieve this by learning collaborative embeddings in LLMs\u2019 latent space from scratch or by mapping from external models. However, they fail to represent the information in a text-like format, which may not align optimally with LLMs. To bridge this gap, we introduce BinLLM, a novel LLMRec method that seamlessly integrates collaborative information through text-like encoding. BinLLM converts collaborative embeddings from external models into binary sequences \u2014 a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs. Additionally, BinLLM provides options to compress the binary sequence using dot-decimal notation to avoid excessively long lengths. Extensive experiments validate that BinLLM introduces collaborative information in a manner better aligned with LLMs, resulting in enhanced performance. We release our code at https://github.com/zyang1580/BinLLM.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.497",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-mm",
        "author": "Wang, Yuhao and Liao, Yusheng and Liu, Heyang and Liu, Hongcheng and Wang, Yanfeng and Wang, Yu",
        "booktitle": "ACL2024",
        "title": "MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models\u2019 struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been overlooked in prior studies. In this paper, we aim to define and evaluate the self-awareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of popular MLLMs, offering a comprehensive analysis of their self-awareness and providing detailed insights. The experiment results reveal that current MLLMs possess limited self-awareness capabilities, pointing to a crucial area for future advancement in the development of trustworthy MLLMs. Code and data are available at https://github.com/YHWmz/MM-SAP.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.498",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Improving Large Language Models in Event Relation Logical Prediction": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-improving-large",
        "author": "Chen, Meiqi and Ma, Yubo and Song, Kaitao and Cao, Yixin and Zhang, Yan and Li, Dongsheng",
        "booktitle": "ACL2024",
        "title": "Improving Large Language Models in Event Relation Logical Prediction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Event relations are crucial for narrative understanding and reasoning. Governed by nuanced logic, event relation extraction (ERE) is a challenging task that demands thorough semantic understanding and rigorous logical reasoning. In this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in understanding and applying event relation logic. More in detail, we first investigate the deficiencies of LLMs in logical reasoning across different tasks. Our study reveals that LLMs are not logically consistent reasoners, which results in their suboptimal performance on tasks that need rigorous reasoning. To address this, we explore three different approaches to endow LLMs with event relation logic, and thus enable them to generate more coherent answers across various scenarios. Based on our approach, we also contribute a synthesized dataset (LLM-ERL) involving high-order reasoning for evaluation and fine-tuning. Extensive quantitative and qualitative analyses on different tasks also validate the effectiveness of our approach and provide insights for solving practical tasks with LLMs in future work. Codes are available at https://github.com/chenmeiqii/Teach-LLM-LR.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.512",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Synchronized Video Storytelling: Generating Video Narrations with Structured Storyline": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2024-synchronized",
        "author": "Yang, Dingyi and Zhan, Chunru and Wang, Ziheng and Wang, Biao and Ge, Tiezheng and Zheng, Bo and Jin, Qin",
        "booktitle": "ACL2024",
        "title": "Synchronized Video Storytelling: Generating Video Narrations with Structured Storyline",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Video storytelling is engaging multimedia content that utilizes video and its accompanying narration to share a story and attract the audience, where a key challenge is creating narrations for recorded visual scenes. Previous studies on dense video captioning and video story generation have made some progress. However, in practical applications, we typically require synchronized narrations for ongoing visual scenes. In this work, we introduce a new task of Synchronized Video Storytelling, which aims to generate synchronous and informative narrations for videos. These narrations, associated with each video clip, should relate to the visual content, integrate relevant knowledge, and have an appropriate word count corresponding to the clip\u2019s duration. Specifically, a structured storyline is beneficial to guide the generation process, ensuring coherence and integrity. To support the exploration of this task, we introduce a new benchmark dataset E-SyncVidStory with rich annotations. Since existing Multimodal LLMs are not effective in addressing this task in one-shot or few-shot settings, we propose a framework named VideoNarrator that can generate a storyline for input videos and simultaneously generate narrations with the guidance of the generated or predefined storyline. We further introduce a set of evaluation metrics to thoroughly assess the generation. Both automatic and human evaluations validate the effectiveness of our approach. Our dataset, codes, and evaluations will be released.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.513",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-fine",
        "author": "Chen, Wenting and Shen, Linlin and Lin, Jingyang and Luo, Jiebo and Li, Xiang and Yuan, Yixuan",
        "booktitle": "ACL2024",
        "title": "Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Fine-grained vision-language models (VLM) have been widely used for inter-modality local alignment between the predefined fixed patches and textual words. However, in medical analysis, lesions exhibit varying sizes and positions, and using fixed patches may cause incomplete representations of lesions. Moreover, these methods provide explainability by using heatmaps to show the general image areas potentially associated with texts rather than specific regions, making their explanations not explicit and specific enough. To address these issues, we propose a novel Adaptive patch-word Matching (AdaMatch) model to correlate chest X-ray (CXR) image regions with words in medical reports and apply it to CXR-report generation to provide explainability for the generation process. AdaMatch exploits the fine-grained relation between adaptive patches and words to provide explanations of specific image regions with corresponding words. To capture the abnormal regions of varying sizes and positions, we introduce an Adaptive Patch extraction (AdaPatch) module to acquire adaptive patches for these regions adaptively. Aiming to provide explicit explainability for the CXR-report generation task, we propose an AdaMatch-based bidirectional LLM for Cyclic CXR-report generation (AdaMatch-Cyclic). It employs AdaMatch to obtain the keywords for CXR images and \u2018keypatches\u2019 for medical reports as hints to guide CXR-report generation. Extensive experiments on two publicly available CXR datasets validate the effectiveness of our method and its superior performance over existing methods. Source code will be released.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.514",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Synergistic Interplay between Search and Large Language Models for Information Retrieval": {
        "type": "INPROCEEDINGS",
        "key": "feng-etal-2024-synergistic",
        "author": "Feng, Jiazhan and Tao, Chongyang and Geng, Xiubo and Shen, Tao and Xu, Can and Long, Guodong and Zhao, Dongyan and Jiang, Daxin",
        "booktitle": "ACL2024",
        "title": "Synergistic Interplay between Search and Large Language Models for Information Retrieval",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern retrieval models (RMs). The emergence of large language models (LLMs) has further revolutionized the IR field by enabling users to interact with search systems in natural languages. In this paper, we explore the advantages and disadvantages of LLMs and RMs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose **InteR**, a novel framework that facilitates information refinement through synergy between RMs and LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using retrieved documents. This iterative refinement process augments the inputs of RMs and LLMs, leading to more accurate retrieval. Experiments on large-scale retrieval benchmarks involving web search and low-resource retrieval tasks show that InteR achieves overall superior **zero-shot** retrieval performance compared to state-of-the-art methods, even those using relevance judgment. Source code is available at https://github.com/Cyril-JZ/InteR.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.517",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "sun-etal-2024-parrot",
        "author": "Sun, Yuchong and Liu, Che and Zhou, Kun and Huang, Jinwen and Song, Ruihua and Zhao, Xin and Zhang, Fuzheng and Zhang, Di and Gai, Kun",
        "booktitle": "ACL2024",
        "title": "Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Humans often interact with large language models (LLMs) in multi-turn interaction to obtain desired answers or more information. However, most existing studies overlook the multi-turn instruction following ability of LLMs, in terms of training dataset, training method, and evaluation benchmark. In this paper, we introduce Parrot, a solution aiming to enhance multi-turn instruction following for LLMs. First, we introduce an efficient but effective method for collecting multi-turn instructions that feature human-like queries, such as anaphora and ellipsis. Second, we propose a context-aware preference optimization strategy to further enhance LLMs for complex queries in multi-turn interaction. Moreover, to quantitatively evaluate LLMs in multi-turn instruction following, we manually build a multi-turn benchmark derived from existing ones. Extensive experiments show that Parrot improves current LLMs by up to 7.2% in multi-turn instruction following. Our dataset and codes will be open-sourced to facilitate future research.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.525",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "VulLibGen: Generating Names of Vulnerability-Affected Packages via a Large Language Model": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-vullibgen",
        "author": "Chen, Tianyu and Li, Lin and ZhuLiuchuan, ZhuLiuchuan and Li, Zongyang and Liu, Xueqing and Liang, Guangtai and Wang, Qianxiang and Xie, Tao",
        "booktitle": "ACL2024",
        "title": "VulLibGen: Generating Names of Vulnerability-Affected Packages via a Large Language Model",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Security practitioners maintain vulnerability reports (e.g., GitHub Advisory) to help developers mitigate security risks. An important task for these databases is automatically extracting structured information mentioned in the report, e.g., the affected software packages, to accelerate the defense of the vulnerability ecosystem.However, it is challenging for existing work on affected package identification to achieve high precision. One reason is that all existing work focuses on relatively smaller models, thus they cannot harness the knowledge and semantic capabilities of large language models.To address this limitation, we propose VulLibGen, the first method to use LLM for affected package identification. In contrast to existing work, VulLibGen proposes the novel idea to directly generate the affected package. To improve the precision, VulLibGen employs supervised fine-tuning (SFT), retrieval augmented generation (RAG) and a local search algorithm. The local search algorithm is a novel post-processing algorithm we introduce for reducing the hallucination of the generated packages. Our evaluation results show that VulLibGen has an average precision of 0.806 for identifying vulnerable packages in the four most popular ecosystems in GitHub Advisory (Java, JS, Python, Go) while the best average precision in previous work is 0.721. Additionally, VulLibGen has high value to security practice: we submitted 60 \\textlessvulnerability, affected package\\textgreater pairs to GitHub Advisory (covers four ecosystems) and 34 of them have been accepted and merged.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.527",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-badagent",
        "author": "Wang, Yifei and Xue, Dizhan and Zhang, Shengjie and Qian, Shengsheng",
        "booktitle": "ACL2024",
        "title": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.530",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training": {
        "type": "INPROCEEDINGS",
        "key": "fang-etal-2024-enhancing",
        "author": "Fang, Feiteng and Bai, Yuelin and Ni, Shiwen and Yang, Min and Chen, Xiaojun and Xu, Ruifeng",
        "booktitle": "ACL2024",
        "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs\u2019 capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model\u2019s training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model\u2019s capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we will release our code and data upon acceptance.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.540",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning": {
        "type": "INPROCEEDINGS",
        "key": "pasewark-etal-2024-tuning",
        "author": "Pasewark, Eric and Montgomery, Kyle and Duan, Kefei and Song, Dawn and Wang, Chenguang",
        "booktitle": "ACL2024",
        "title": "Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present a new method for large language models to solve compositional tasks. Although they have shown strong performance on traditional language understanding tasks, large language models struggle to solve compositional tasks, where the solution depends on solving smaller instances of the same problem. We propose a natural approach to solve compositional tasks recursively. Our method, Re-Tuning, tunes models to break down a problem into subproblems, solve those subproblems, and combine the results. We show that our method significantly improves model performance on three representative compositional tasks: integer addition, dynamic programming, and parity. Compared to state-of-the-art methods that keep intermediate steps towards solving the problems, Re-Tuning achieves significantly higher accuracy and is more GPU memory efficient.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.561",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Prompted Aspect Key Point Analysis for Quantitative Review Summarization": {
        "type": "INPROCEEDINGS",
        "key": "tang-etal-2024-prompted",
        "author": "Tang, An and Zhang, Xiuzhen and Dinh, Minh and Cambria, Erik",
        "booktitle": "ACL2024",
        "title": "Prompted Aspect Key Point Analysis for Quantitative Review Summarization",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Key Point Analysis (KPA) aims for quantitative summarization that provides key points (KPs) as succinct textual summaries and quantities measuring their prevalence. KPA studies for arguments and reviews have been reported in the literature. A majority of KPA studies for reviews adopt supervised learning to extract short sentences as KPs before matching KPs to review comments for quantification of KP prevalence. Recent abstractive approaches still generate KPs based on sentences, often leading to KPs with overlapping and hallucinated opinions, and inaccurate quantification. In this paper, we propose Prompted Aspect Key Point Analysis (PAKPA) for quantitative review summarization. PAKPA employs aspect sentiment analysis and prompted in-context learning with Large Language Models (LLMs) to generate and quantify KPs grounded in aspects for business entities, which achieves faithful KPs with accurate quantification, and removes the need for large amounts of annotated data for supervised training. Experiments on the popular review dataset Yelp and the aspect-oriented review summarization dataset SPACE show that our framework achieves state-of-the-art performance. Source code and data are available at: https://github.com/antangrocket1312/PAKPA",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.576",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors": {
        "type": "INPROCEEDINGS",
        "key": "zhou-etal-2024-navigating",
        "author": "Zhou, Ying and He, Ben and Sun, Le",
        "booktitle": "ACL2024",
        "title": "Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the launch of ChatGPT, large language models (LLMs) have attracted global attention. In the realm of article writing, LLMs have witnessed extensive utilization, giving rise to concerns related to intellectual property protection, personal privacy, and academic integrity. In response, AI-text detection has emerged to distinguish between human and machine-generated content. However, recent research indicates that these detection systems often lack robustness and struggle to effectively differentiate perturbed texts. Currently, there is a lack of systematic evaluations regarding detection performance in real-world applications, and a comprehensive examination of perturbation techniques and detector robustness is also absent. To bridge this gap, our work simulates real-world scenarios in both informal and professional writing, exploring the out-of-the-box performance of current detectors. Additionally, we have constructed 12 black-box text perturbation methods to assess the robustness of current detection models across various perturbation granularities. Furthermore, through adversarial learning experiments, we investigate the impact of perturbation data augmentation on the robustness of AI-text detectors. We have released our code and data at https://github.com/zhouying20/ai-text-detector-evaluation.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.584",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-chat",
        "author": "Huang, Shih-Cheng and Li, Pin-Zu and Hsu, Yu-chi and Chen, Kuang-Ming and Lin, Yu Tung and Hsiao, Shih-Kai and Tsai, Richard and Lee, Hung-yi",
        "booktitle": "ACL2024",
        "title": "Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of chat vector to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model\u2019s weights, we can endow the model with chat capabilities in new languages without the need for further training.Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our approach, we extend our experiments to encompass various languages, base models, and chat vectors. The results underscore the chat vector\u2019s simplicity, effectiveness, and wide applicability, making it a compelling solution for efficiently enabling conversational capabilities in pre-trained language models. Our code is available at https://github.com/aqweteddy/ChatVector.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.590",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails": {
        "type": "INPROCEEDINGS",
        "key": "mangaokar-etal-2024-prp",
        "author": "Mangaokar, Neal and Hooda, Ashish and Choi, Jihye and Chandrashekaran, Shreyas and Fawaz, Kassem and Jha, Somesh and Prakash, Atul",
        "booktitle": "ACL2024",
        "title": "PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) are typically aligned to be harmless to humans. Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content. More recent LLMs often incorporate an additional layer of defense, a Guard Model, which is a second LLM that is designed to check and moderate the output response of the primary LLM. Our key contribution is to show a novel attack strategy, PRP, that is successful against several open-source (e.g., Llama 2) and closed-source (e.g., GPT 3.5) implementations of Guard Models. PRP leverages a two step prefix-based attack that operates by (a) constructing a universal adversarial prefix for the Guard Model, and (b) propagating this prefix to the response. We find that this procedure is effective across multiple threat models, including ones in which the adversary has no access to the Guard Model at all. Our work suggests that further advances are required on defenses and Guard Models before they can be considered effective. Code at https://github.com/AshishHoodaIITD/prp-llm-guard-rail-attack.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.591",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "CLOMO: Counterfactual Logical Modification with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-clomo",
        "author": "Huang, Yinya and Hong, Ruixin and Zhang, Hongming and Shao, Wei and Yang, Zhicheng and Yu, Dong and Zhang, Changshui and Liang, Xiaodan and Song, Linqi",
        "booktitle": "ACL2024",
        "title": "CLOMO: Counterfactual Logical Modification with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model\u2019s counterfactual capabilities, we propose an innovative evaluation metric, the decomposed Self-Evaluation Score (SES) to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance. Code and data are available at https://github.com/Eleanor-H/CLOMO.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.593",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Layer-Condensed KV Cache for Efficient Inference of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "wu-tu-2024-layer",
        "author": "Wu, Haoyi and Tu, Kewei",
        "booktitle": "ACL2024",
        "title": "Layer-Condensed KV Cache for Efficient Inference of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Huge memory consumption has been a major bottleneck for deploying high-throughput large language models in real-world applications. In addition to the large number of parameters, the key-value (KV) cache for the attention mechanism in the transformer architecture consumes a significant amount of memory, especially when the number of layers is large for deep language models. In this paper, we propose a novel method that only computes and caches the KVs of a small number of layers, thus significantly saving memory consumption and improving inference throughput. Our experiments on large language models show that our method achieves up to 26\\times higher throughput than standard transformers and competitive performance in language modeling and downstream tasks. In addition, our method is orthogonal to existing transformer memory-saving techniques, so it is straightforward to integrate them with our model, achieving further improvement in inference efficiency. Our code is available at https://github.com/whyNLP/LCKV.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.602",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Browse and Concentrate: Comprehending Multimodal Content via Prior-LLM Context Fusion": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-browse",
        "author": "Wang, Ziyue and Chen, Chi and Zhu, Yiqi and Luo, Fuwen and Li, Peng and Yan, Ming and Zhang, Ji and Huang, Fei and Sun, Maosong and Liu, Yang",
        "booktitle": "ACL2024",
        "title": "Browse and Concentrate: Comprehending Multimodal Content via Prior-LLM Context Fusion",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the bloom of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) that incorporate LLMs with pre-trained vision models have recently demonstrated impressive performance across diverse vision-language tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth multimodal context fusion prior to feeding the features into LLMs. This paradigm initially \u201cbrowses\u201d through the inputs for essential insights, and then revisits the inputs to \u201cconcentrate\u201d on crucial details, guided by these insights, to achieve a more comprehensive understanding of the multimodal inputs. Additionally, we develop training strategies specifically to enhance the understanding of multi-image inputs. Our method markedly boosts the performance on 7 multi-image scenarios, contributing to increments on average accuracy by 2.13% and 7.60% against strong MLLMs baselines with 3B and 11B LLMs, respectively.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.605",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Model Composition for Multimodal Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-model",
        "author": "Chen, Chi and Du, Yiyang and Fang, Zheng and Wang, Ziyue and Luo, Fuwen and Li, Peng and Yan, Ming and Zhang, Ji and Huang, Fei and Sun, Maosong and Liu, Yang",
        "booktitle": "ACL2024",
        "title": "Model Composition for Multimodal Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.606",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Self-chats from Large Language Models Make Small Emotional Support Chatbot Better": {
        "type": "INPROCEEDINGS",
        "key": "zheng-etal-2024-self",
        "author": "Zheng, Zhonghua and Liao, Lizi and Deng, Yang and Qin, Libo and Nie, Liqiang",
        "booktitle": "ACL2024",
        "title": "Self-chats from Large Language Models Make Small Emotional Support Chatbot Better",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown strong generalization abilities to excel in various tasks, including emotion support conversations. However, deploying such LLMs like GPT-3 (175B parameters) is resource-intensive and challenging at scale. In this study, we utilize LLMs as \u201cCounseling Teacher\u201d to enhance smaller models\u2019 emotion support response abilities, significantly reducing the necessity of scaling up model size. To this end, we first introduce an iterative expansion framework, aiming to prompt the large teacher model to curate an expansive emotion support dialogue dataset. This curated dataset, termed ExTES, encompasses a broad spectrum of scenarios and is crafted with meticulous strategies to ensure its quality and comprehensiveness. Based on this, we then devise a Diverse Response Inpainting (DRI) mechanism to harness the teacher model to produce multiple diverse responses by filling in the masked conversation context. This richness and variety serve as instructive examples, providing a robust foundation for fine-tuning smaller student models. Experiments across varied scenarios reveal that the teacher-student scheme with DRI notably improves the response abilities of smaller models, even outperforming the teacher model in some cases. The dataset and codes are available in https://github.com/pandazzh2020/ExTES.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.611",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "JumpCoder: Go Beyond Autoregressive Coder via Online Modification": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2024-jumpcoder",
        "author": "Chen, Mouxiang and Tian, Hao and Liu, Zhongxin and Ren, Xiaoxue and Sun, Jianling",
        "booktitle": "ACL2024",
        "title": "JumpCoder: Go Beyond Autoregressive Coder via Online Modification",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance. We introduce JumpCoder, a novel model-agnostic framework that enables human-like online modification and non-sequential generation to augment code LLMs. The key idea behind JumpCoder is to insert new code into the currently generated code when necessary during generation, which is achieved through an auxiliary infilling model that works in tandem with the code LLM. Since identifying the best infill position beforehand is intractable, we adopt an infill-first, judge-later strategy, which experiments with filling at the k most critical positions following the generation of each line, and uses an Abstract Syntax Tree (AST) parser alongside the Generation Model Scoring to effectively judge the validity of each potential infill. Extensive experiments using six state-of-the-art code LLMs across multiple and multilingual benchmarks consistently indicate significant improvements over all baselines. Our code is available in the uploaded attachment.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.619",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "AlignBench: Benchmarking Chinese Alignment of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-alignbench",
        "author": "Liu, Xiao and Lei, Xuanyu and Wang, Shengyuan and Huang, Yue and Feng, Andrew and Wen, Bosi and Cheng, Jiale and Ke, Pei and Xu, Yifan and Tam, Weng Lam and Zhang, Xiaohan and Sun, Lichao and Gu, Xiaotao and Wang, Hongning and Zhang, Jing and Huang, Minlie and Dong, Yuxiao and Tang, Jie",
        "booktitle": "ACL2024",
        "title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, effective evaluation of alignment for emerging Chinese LLMs is still significantly lacking, calling for real-scenario grounded, open-ended, challenging and automatic evaluations tailored for alignment. To fill in this gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark for evaluating LLMs\u2019 alignment in Chinese. We tailor a human-in-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references.To ensure references\u2019 correctness, each knowledge-intensive query is accompanied with evidences collected from reliable webpages (including the url and quotation) by our annotators.For automatic evaluation, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge (CITATION) with Chain-of-Thought to generate explanations and final ratings as evaluations, ensuring high reliability and interpretability.All evaluation codes and data are publicly available at https://github.com/THUDM/AlignBench",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.624",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "An Entropy-based Text Watermarking Detection Method": {
        "type": "INPROCEEDINGS",
        "key": "lu-etal-2024-entropy",
        "author": "Lu, Yijian and Liu, Aiwei and Yu, Dianzhi and Li, Jingjing and King, Irwin",
        "booktitle": "ACL2024",
        "title": "An Entropy-based Text Watermarking Detection Method",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Text watermarking algorithms for large language models (LLMs) can effectively identify machine-generated texts by embedding and detecting hidden features in the text. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we opine that the influence of token entropy should be fully considered in the watermark detection process, i.e., the weight of each token during watermark detection should be customized according to its entropy, rather than setting the weights of all tokens to the same value as in previous methods. Specifically, we propose Entropy-based Text Watermarking Detection (EWD) that gives higher-entropy tokens higher influence weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free and fully automated. From the experiments, we demonstrate that our EWD can achieve better detection performance in low-entropy scenarios, and our method is also general and can be applied to texts with different entropy distributions. Our code and data is available. Additionally, our algorithm could be accessed through MarkLLM (CITATION).",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.630",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-integrate",
        "author": "Wang, Xinglin and Li, Yiwei and Feng, Shaoxiong and Yuan, Peiwen and Pan, Boyuan and Wang, Heda and Hu, Yao and Li, Kan",
        "booktitle": "ACL2024",
        "title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample selection or voting mechanisms to improve output quality. These methods, however, face limitations due to their inability to fully utilize the nuanced consensus knowledge present within multiple candidate samples, often resulting in suboptimal outputs. We propose Fine-Grained Self-Consistency (FSC) to addresses these limitations by extracting and integrating segment-level commonalities from candidate samples, enhancing the performance of LLMs both in open-ended and reasoning tasks. Based on this, we present two additional strategies: candidate filtering, which enhances overall quality by identifying highly similar candidate sets, and merging, which reduces input token requirements by combining similar samples. The effectiveness of FSC is demonstrated through extensive experiments on various tasks, including summarization, code generation, and mathematical reasoning, using GPT-3.5-turbo and GPT-4. The results indicate significant improvements over baseline methods, showcasing the potential of FSC to optimize output quality by effectively synthesizing fine-grained consensus knowledge from multiple samples.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.634",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Quantifying Generalizations: Exploring the Divide Between Human and LLMs\u2019 Sensitivity to Quantification": {
        "type": "INPROCEEDINGS",
        "key": "collacciani-etal-2024-quantifying",
        "author": "Collacciani, Claudia and Rambelli, Giulia and Bolognesi, Marianna",
        "booktitle": "ACL2024",
        "title": "Quantifying Generalizations: Exploring the Divide Between Human and LLMs\u2019 Sensitivity to Quantification",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Generics are expressions used to communicate abstractions about categories. While conveying general truths (e.g., \u201cBirds fly\u201d), generics have the interesting property to admit exceptions (e.g., penguins do not fly). Statements of this type help us organizing our knowledge of the world, and form the basis of how we express it (Hampton, 2012; Leslie, 2014).This study investigates how Large Language Models (LLMs) interpret generics, drawing upon psycholinguistic experimental methodologies. Understanding how LLMs interpret generic statements serves not only as a measure of their ability to abstract but also arguably plays a role in their encoding of stereotypes. Given that generics interpretation necessitates a comparison with explicitly quantified sentences, we explored i.) whether LLMs can correctly associate a quantifier with the generic structure, and ii.) whether the presence of a generic sentence as context influences the outcomes of quantifiers. We evaluated LLMs using both Surprisal distributions and prompting techniques.The findings indicate that models do not exhibit a strong sensitivity to quantification. Nevertheless, they seem to encode a meaning linked with the generic structure, which leads them to adjust their answers accordingly when a generalization is provided as context.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.636",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-self-training",
        "author": "Zhang, Yice and Zeng, Jie and Hu, Weiming and Wang, Ziyi and Chen, Shiwei and Xu, Ruifeng",
        "booktitle": "ACL2024",
        "title": "Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect term, aspect category, opinion term, sentiment polarity) for a given review, which is the most representative and challenging task in aspect-based sentiment analysis. A key challenge in the ASQP task is the scarcity of labeled data, which limits the performance of existing methods. To tackle this issue, we propose a self-training framework with a pseudo-label scorer, wherein a scorer assesses the match between reviews and their pseudo-labels, aiming to filter out mismatches and thereby enhance the effectiveness of self-training. We highlight two critical aspects to ensure the scorer\u2019s effectiveness and reliability: the quality of the training dataset and its model architecture. To this end, we create a human-annotated comparison dataset and train a generative model on it using ranking-based objectives. Extensive experiments on public ASQP datasets reveal that using our scorer can greatly and consistently improve the effectiveness of self-training. Moreover, we explore the possibility of replacing humans with large language models for comparison dataset annotation, and experiments demonstrate its feasibility. We will release our code and data via GitHub.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.640",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Improving Text Embeddings with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-improving-text",
        "author": "Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu",
        "booktitle": "ACL2024",
        "title": "Improving Text Embeddings with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.642",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation": {
        "type": "INPROCEEDINGS",
        "key": "kasner-dusek-2024-beyond",
        "author": "Kasner, Zden\u011bk and Dusek, Ondrej",
        "booktitle": "ACL2024",
        "title": "Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We analyze the behaviors of open large language models (LLMs) on the task of data-to-text (D2T) generation, i.e., generating coherent and relevant text from structured data. To avoid the issue of LLM training data contamination with standard benchmarks, we design Quintd - a tool for collecting novel structured data records from public APIs. We find that open LLMs (Llama 2, Mistral, and Zephyr) can generate fluent and coherent texts in zero-shot settings from data in common formats collected with Quintd. However, we show that the semantic accuracy of the outputs is a major issue: both according to human annotators and our reference-free metric based on GPT-4, more than 80% of the outputs of open LLMs contain at least one semantic error. We publicly release the code, data, and model outputs.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.651",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs": {
        "type": "INPROCEEDINGS",
        "key": "markowitz-etal-2024-tree",
        "author": "Markowitz, Elan and Ramakrishna, Anil and Dhamala, Jwala and Mehrabi, Ninareh and Peris, Charith and Gupta, Rahul and Chang, Kai-Wei and Galstyan, Aram",
        "booktitle": "ACL2024",
        "title": "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing reliable, structured, domain-specific, and up-to-date external knowledge. However, KGs and LLMs are often developed separately and must be integrated after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning algorithm that enables augmentation of black-box LLMs with one or more KGs. The algorithm equips a LLM with actions for interfacing a KG and enables the LLM to perform tree search over possible thoughts and actions to find high confidence reasoning paths. Tree-of-Traversals significantly improves performance on question answering and KG question answering tasks. Code is available at https://github.com/amazon-science/tree-of-traversals",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.665",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Agent Lumos: Unified and Modular Training for Open-Source Language Agents": {
        "type": "INPROCEEDINGS",
        "key": "yin-etal-2024-agent",
        "author": "Yin, Da and Brahman, Faeze and Ravichander, Abhilasha and Chandu, Khyathi and Chang, Kai-Wei and Choi, Yejin and Lin, Bill Yuchen",
        "booktitle": "ACL2024",
        "title": "Agent Lumos: Unified and Modular Training for Open-Source Language Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Closed-source agents suffer from several issues such as a lack of affordability, transparency, and reproducibility, particularly on complex interactive tasks. This motivates the development of open-source alternatives. We introduce Lumos, one of the first frameworks for training open-source LLM-based agents. Lumos features a learnable, unified and modular architecture with a planning module that learns high-level subgoal generation, and a grounding module trained to translate these into the actions using various tools in the execution module. The design allows for modular upgrades and wider applicability to diverse interactive tasks. To foster generalizable agent learning, we collect large-scale, unified, and high-quality training annotations derived from diverse ground-truth reasoning rationales across various complex interactive tasks. On 9 datasets, Lumos exhibits several key advantages: (1) Lumos excels multiple larger open-source agents on the held-out datasets (unused for training) for each task type. Lumos even surpasses GPT agents on QA and web tasks; (2) Lumos outperforms open-source agents produced by chain-of-thoughts and unmodularized integrated training; and (3) Lumos effectively generalizes to unseen tasks, outperforming 33B-scale agents and domain-specific agents. Code and data will be released.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.670",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles": {
        "type": "INPROCEEDINGS",
        "key": "kruk-etal-2024-silent",
        "author": "Kruk, Julia and Marchini, Michela and Magu, Rijul and Ziems, Caleb and Muchlinski, David and Yang, Diyi",
        "booktitle": "ACL2024",
        "title": "Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A dog whistle is a form of coded communication that carries a secondary meaning to specific audiences and is often weaponized for racial and socioeconomic discrimination. Dog whistling historically originated from United States politics, but in recent years has taken root in social media as a means of evading hate speech detection systems and maintaining plausible deniability. In this paper, we present an approach for word-sense disambiguation of dog whistles from standard speech using Large Language Models (LLMs), and leverage this technique to create a dataset of 16,550 high-confidence coded examples of dog whistles used in formal and informal communication. Silent Signals is the largest dataset of disambiguated dog whistle usage, created for applications in hate speech detection, neology, and political science.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.675",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models": {
        "type": "INPROCEEDINGS",
        "key": "maaz-etal-2024-video",
        "author": "Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad",
        "booktitle": "ACL2024",
        "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of video-based conversation by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Code: https://github.com/mbzuai-oryx/Video-ChatGPT.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.679",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding": {
        "type": "INPROCEEDINGS",
        "key": "elhoushi-etal-2024-layerskip",
        "author": "Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich, Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and Roman, Ahmed and Aly, Ahmed and Chen, Beidi and Wu, Carole-Jean",
        "booktitle": "ACL2024",
        "title": "LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task. We open source our code at https://github.com/facebookresearch/LayerSkip.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.681",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "ActionIE: Action Extraction from Scientific Literature with Programming Languages": {
        "type": "INPROCEEDINGS",
        "key": "zhong-etal-2024-actionie",
        "author": "Zhong, Xianrui and Du, Yufeng and Ouyang, Siru and Zhong, Ming and Luo, Tingfeng and Ho, Qirong and Peng, Hao and Ji, Heng and Han, Jiawei",
        "booktitle": "ACL2024",
        "title": "ActionIE: Action Extraction from Scientific Literature with Programming Languages",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Extraction of experimental procedures from human language in scientific literature and patents into actionable sequences in robotics language holds immense significance in scientific domains. Such an action extraction task is particularly challenging given the intricate details and context-dependent nature of the instructions, especially in fields like chemistry where reproducibility is paramount. In this paper, we introduce ActionIE, a method that leverages Large Language Models (LLMs) to bridge this divide by converting actions written in natural language into executable Python code. This enables us to capture the entities of interest, and the relationship between each action, given the features of Programming Languages. Utilizing linguistic cues identified by frequent patterns, ActionIE provides an improved mechanism to discern entities of interest. While our method is broadly applicable, we exemplify its power in the domain of chemical literature, wherein we focus on extracting experimental procedures for chemical synthesis. The code generated by our method can be easily transformed into robotics language which is in high demand in scientific fields. Comprehensive experiments demonstrate the superiority of our method. In addition, we propose a graph-based metric to more accurately reflect the precision of extraction. We also develop a dataset to address the scarcity of scientific literature occurred in existing datasets.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.683",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "A Community-Centric Perspective for Characterizing and Detecting Anti-Asian Violence-Provoking Speech": {
        "type": "INPROCEEDINGS",
        "key": "verma-etal-2024-community",
        "author": "Verma, Gaurav and Grover, Rynaa and Zhou, Jiawei and Mathew, Binny and Kraemer, Jordan and Choudhury, Munmun and Kumar, Srijan",
        "booktitle": "ACL2024",
        "title": "A Community-Centric Perspective for Characterizing and Detecting Anti-Asian Violence-Provoking Speech",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Violence-provoking speech \u2013 speech that implicitly or explicitly promotes violence against the members of the targeted community, contributed to a massive surge in anti-Asian crimes during the COVID-19 pandemic. While previous works have characterized and built tools for detecting other forms of harmful speech, like fear speech and hate speech, our work takes a community-centric approach to studying anti-Asian violence-provoking speech. Using data from ~420k Twitter posts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we develop a codebook to characterize anti-Asian violence-provoking speech and collect a community-crowdsourced dataset to facilitate its large-scale detection using state-of-the-art classifiers. We contrast the capabilities of natural language processing classifiers, ranging from BERT-based to LLM-based classifiers, in detecting violence-provoking speech with their capabilities to detect anti-Asian hateful speech. In contrast to prior work that has demonstrated the effectiveness of such classifiers in detecting hateful speech (F\u2081 = 0.89), our work shows that accurate and reliable detection of violence-provoking speech is a challenging task (F\u2081 = 0.69). We discuss the implications of our findings, particularly the need for proactive interventions to support Asian communities during public health crises.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.684",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "TaPERA: Enhancing Faithfulness and Interpretability in Long-Form Table QA by Content Planning and Execution-based Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-tapera",
        "author": "Zhao, Yilun and Chen, Lyuhao and Cohan, Arman and Zhao, Chen",
        "booktitle": "ACL2024",
        "title": "TaPERA: Enhancing Faithfulness and Interpretability in Long-Form Table QA by Content Planning and Execution-based Reasoning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Long-form Table Question Answering (LFTQA) requires systems to generate paragraph long and complex answers to questions over tabular data. While Large language models based systems have made significant progress, it often hallucinates, especially when the task involves complex reasoning over tables. To tackle this issue, we propose a new LLM-based framework, TaPERA, for LFTQA tasks. Our framework uses a modular approach that decomposes the whole process into three sub-modules: 1) QA-based Content Planner that iteratively decomposes the input question into sub-questions; 2) Execution-based Table Reasoner that produces executable Python program for each sub-question; and 3) Answer Generator that generates long-form answer grounded on the program output. Human evaluation results on the FeTaQA and QTSumm datasets indicate that our framework significantly improves strong baselines on both accuracy and truthfulness, as our modular framework is better at table reasoning, and the long-form answer is always consistent with the program output. Our modular design further provides transparency as users are able to interact with our framework by manually changing the content plans.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.692",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "FinanceMATH: Knowledge-Intensive Math Reasoning in Finance Domains": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-knowledgefmath",
        "author": "Zhao, Yilun and Liu, Hongjun and Long, Yitao and Zhang, Rui and Zhao, Chen and Cohan, Arman",
        "booktitle": "ACL2024",
        "title": "FinanceMATH: Knowledge-Intensive Math Reasoning in Finance Domains",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We introduce FinanceMath, a novel benchmark designed to evaluate LLMs\u2019 capabilities in solving knowledge-intensive math reasoning problems. Compared to prior works, this study features three core advancements. First, FinanceMath includes 1,200 problems with a hybrid of textual and tabular content. These problems require college-level knowledge in the finance domain for effective resolution. Second, we provide expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment. We also construct a finance-domain knowledge bank and investigate various knowledge integration strategies. Finally, we evaluate a wide spectrum of 44 LLMs with both Chain-of-Thought and Program-of-Thought prompting methods. Our experimental results reveal that the current best-performing system (i.e., GPT-4o) achieves only 60.9% accuracy using CoT prompting, leaving substantial room for improvement. Moreover, while augmenting LLMs with external knowledge can improve model performance (e.g., from 47.5% to 54.5% for Gemini-1.5-Pro), their accuracy remains significantly lower than the estimated human expert performance of 92%. We believe that FinanceMath can advance future research in the area of domain-specific knowledge retrieval and integration, particularly within the context of solving reasoning-intensive tasks.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.693",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs": {
        "type": "INPROCEEDINGS",
        "key": "basu-etal-2024-api",
        "author": "Basu, Kinjal and Abdelaziz, Ibrahim and Chaudhury, Subhajit and Dan, Soham and Crouse, Maxwell and Munawar, Asim and Austel, Vernon and Kumaravel, Sadhana and Muthusamy, Vinod and Kapanipathi, Pavan and Lastras, Luis",
        "booktitle": "ACL2024",
        "title": "API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrate the utility of the API-BLEND dataset for both training and benchmarking purposes.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.694",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts": {
        "type": "INPROCEEDINGS",
        "key": "ding-etal-2024-mathcal",
        "author": "Ding, Yifeng and Liu, Jiawei and Wei, Yuxiang and Zhang, Lingming",
        "booktitle": "ACL2024",
        "title": "XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We introduce XFT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs). While vanilla sparse upcycling fails to improve instruction tuning, XFT introduces a shared expert mechanism with a novel routing weight normalization strategy into sparse upcycling, which significantly boosts instruction tuning. After fine-tuning the upcycled MoE model, XFT introduces a learnable model merging mechanism to compile the upcycled MoE model back to a dense model, achieving upcycled MoE-level performance with only dense-model compute. By applying XFT to a 1.3B model, we create a new state-of-the-art tiny code LLM with 67.1 and 64.6 pass@1 on HumanEval and HumanEval+ respectively. With the same data and model architecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+, along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and DS-1000, demonstrating its generalizability. XFT is fully orthogonal to existing techniques such as Evol-Instruct and OSS-Instruct, opening a new dimension for improving code instruction tuning. Codes are available at https://github.com/ise-uiuc/xft.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.699",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Word Matters: What Influences Domain Adaptation in Summarization?": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-word",
        "author": "Li, Yinghao and Miao, Siyu and Huang, Heyan and Gao, Yang",
        "booktitle": "ACL2024",
        "title": "Word Matters: What Influences Domain Adaptation in Summarization?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Domain adaptation aims to enable Large Language Models (LLMs) to generalize domain datasets unseen effectively during the training phase. However, factors such as the size of the model parameters and the scale of training data are general influencers and do not reflect the nuances of domain adaptation performance. This paper investigates the fine-grained factors affecting domain adaptation performance, analyzing the specific impact of \u2018words\u2019 in training data on summarization tasks. We propose quantifying dataset learning difficulty as the learning difficulty of generative summarization, which is determined by two indicators: word-based compression rate and abstraction level. Our experiments conclude that, when considering dataset learning difficulty, the cross-domain overlap and the performance gain in summarization tasks exhibit an approximate linear relationship, which is not directly related to the number of words. Based on this finding, predicting a model\u2019s performance on unknown domain datasets is possible without undergoing training. Source code and scripts are available at https://github.com/li-aolong/Word-Matters.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.715",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Visualization Recommendation with Prompt-based Reprogramming of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-visualization",
        "author": "Li, Xinhang and Zhou, Jingbo and Chen, Wei and Xu, Derong and Xu, Tong and Chen, Enhong",
        "booktitle": "ACL2024",
        "title": "Visualization Recommendation with Prompt-based Reprogramming of Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Visualization recommendations, which aim to automatically match proper visual charts for specific data tables, can significantly simplify the data analysis process. Traditional approaches in this domain have primarily relied on rule-based or machine learning-based methodologies. These methods often demand extensive manual maintenance and yet fail to fully comprehend the tabular data, leading to unsatisfactory performance. Recently, Large Language Models (LLMs) have emerged as powerful tools, exhibiting strong reasoning capabilities. This advancement suggests their substantial promise in addressing visualization recommendation challenges. However, effectively harnessing LLMs to discern and rationalize patterns in tabular data, and consequently deduce the essential information for chart generation, remains an unresolved challenge. To this end, we introduce a novel Hierarchical Table Prompt-based reprogramming framework, named HTP. This framework aims to integrate multi-dimensional tabular data into LLMs through a strategically crafted prompt learning method while keeping the LLMs\u2019 backbone and weights unaltered. The HTP framework uniquely incorporates a four-level prompt structure, encompassing general, instance, cluster, and column levels. This multi-level approach is engineered to provide a comprehensive understanding of both general distribution and multifaceted fine-grained features of tabular data, before inputting the tabular data into the frozen LLM. Our empirical studies confirm that the HTP framework achieves state-of-the-art performance, marking an advancement in the field of data visualization and analysis. The code and data will be made publicly available upon acceptance.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.716",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Faithful Logical Reasoning via Symbolic Chain-of-Thought": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-faithful",
        "author": "Xu, Jundong and Fei, Hao and Pan, Liangming and Liu, Qian and Lee, Mong-Li and Hsu, Wynne",
        "booktitle": "ACL2024",
        "title": "Faithful Logical Reasoning via Symbolic Chain-of-Thought",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first attempt at combining symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at https://github.com/Aiden0526/SymbCoT.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.720",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "han-etal-2024-archcode",
        "author": "Han, Hojae and Kim, Jaejin and Yoo, Jaeseok and Lee, Youngwon and Hwang, Seung-won",
        "booktitle": "ACL2024",
        "title": "ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (i.e. achieving expected behavior for inputs) and non-functional (e.g., time/space performance, robustness, maintainability) requirements. However, textual descriptions can either express requirements verbosely or may even omit some of them. We introduce ARCHCODE, a novel framework that leverages in-context learning to organize requirements observed in descriptions and to extrapolate unexpressed requirements from them. ARCHCODE generates requirements from given descriptions, conditioning them to produce code snippets and test cases. Each test case is tailored to one of the requirements, allowing for the ranking of code snippets based on the compliance of their execution results with the requirements. Public benchmarks show that ARCHCODE enhances to satisfy functional requirements, significantly improving Pass@k scores.Furthermore, we introduce HumanEval-NFR, the first evaluation of LLMs\u2019 non-functional requirements in code generation, demonstrating ARCHCODE\u2019s superiority over baseline methods. The implementation of ARCHCODE and the HumanEval-NFR benchmark are both publicly accessible.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.730",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-hirope",
        "author": "Zhang, Kechi and Li, Ge and Zhang, Huangzhao and Jin, Zhi",
        "booktitle": "ACL2024",
        "title": "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Addressing the limitation of context length in large language models for code-related tasks is the primary focus of this paper. Existing LLMs are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences. Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code. HiRoPE offers easy integration into existing LLMs without extra training costs. Our method is extensively evaluated with various LLMs, demonstrating stable performance in tasks such as language modeling and long code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. Theoretically and experimentally, we find that HiRoPE also addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.735",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training": {
        "type": "INPROCEEDINGS",
        "key": "he-etal-2024-never",
        "author": "He, Junqing and Pan, Kunhao and Dong, Xiaoqun and Song, Zhuoyang and LiuYiBo, LiuYiBo and Qianguosun, Qianguosun and Liang, Yuxin and Wang, Hao and Zhang, Enming and Zhang, Jiaxing",
        "booktitle": "ACL2024",
        "title": "Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) are equipped with longer text input capabilities than before, they are struggling to seek correct information in long contexts. The \u201clost in the middle\u201d problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called Position-Agnostic Multi-step QA (PAM QA). Trained in this task, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7% absolute gain in shuffled settings, by 21.5% in passage retrieval task. We release our model and code to promote related research in the community.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.736",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-codeagent",
        "author": "Zhang, Kechi and Li, Jia and Li, Ge and Shi, Xianjie and Jin, Zhi",
        "booktitle": "ACL2024",
        "title": "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. However, real-world software development often involves complex code repositories with complex dependencies and extensive documentation. To enable LLMs to handle these realworld repo-level code generation, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code implementation, and code testing. We implement four agent strategies to optimize these tools\u2019 usage. To the best of our knowledge, CodeAgent is the first agent tool framework specifically for repo-level code generation. In order to measure the effectiveness of our method at the repository level, we have introduced a benchmark dataset CodAgentBench. The performance on this dataset shows a significant improvement brought by our method, with improvements of pass rate ranging from 2.0 to 15.8. Further tests on the HumanEval benchmark confirm CodeAgent\u2019s adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent\u2019s robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.737",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Meta-Tuning LLMs to Leverage Lexical Knowledge for Generalizable Language Style Understanding": {
        "type": "INPROCEEDINGS",
        "key": "guo-etal-2024-meta",
        "author": "Guo, Ruohao and Xu, Wei and Ritter, Alan",
        "booktitle": "ACL2024",
        "title": "Meta-Tuning LLMs to Leverage Lexical Knowledge for Generalizable Language Style Understanding",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Language style is often used by writers to convey their intentions, identities, and mastery of language. In this paper, we show that current large language models struggle to capture some language styles without fine-tuning. To address this challenge, we investigate whether LLMs can be meta-trained based on representative lexicons to recognize new styles they have not been fine-tuned on. Experiments on 13 established style classification tasks, as well as 63 novel tasks generated using LLMs, demonstrate that meta-training with style lexicons consistently improves zero-shot transfer across styles. We release the code and data at https://github.com/octaviaguo/Style-LLM.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.740",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards": {
        "type": "INPROCEEDINGS",
        "key": "alzahrani-etal-2024-benchmarks",
        "author": "Alzahrani, Norah and Alyahya, Hisham and Alnumay, Yazeed and AlRashed, Sultan and Alsubaie, Shaykhah and Almushayqih, Yousef and Mirza, Faisal and Alotaibi, Nouf and Al-Twairesh, Nora and Alowisheq, Areeb and Bari, M. Saiful and Khan, Haidar",
        "booktitle": "ACL2024",
        "title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value \u2014 we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a *hybrid* scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at [https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness](https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness).",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.744",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms": {
        "type": "INPROCEEDINGS",
        "key": "zheng-etal-2024-neo",
        "author": "Zheng, Jonathan and Ritter, Alan and Xu, Wei",
        "booktitle": "ACL2024",
        "title": "NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms \u2013 new word forms \u2013 over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs\u2019 ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.749",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning": {
        "type": "INPROCEEDINGS",
        "key": "yu-etal-2024-explanation",
        "author": "Yu, Yue and Shen, Jiaming and Liu, Tianqi and Qin, Zhen and Yan, Jing Nathan and Liu, Jialu and Zhang, Chao and Bendersky, Michael",
        "booktitle": "ACL2024",
        "title": "Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have shown remarkable capabilities in various natural language understanding tasks with a few demonstration examples via in-context learning. Common strategies to boost such \u201cin-context\u201d learning ability are to ensemble multiple model decoded results and require the model to generate an explanation along with the prediction. However, these models often treat different class predictions equally and neglect the potential discrepancy between the explanations and predictions. To fully unleash the power of explanations, we propose EASE, an Explanation-Aware Soft Ensemble framework to empower in-context learning with LLMs. We design two techniques, explanation-guided ensemble, and soft probability aggregation, to mitigate the effect of unreliable explanations and improve the consistency between explanations and final predictions. Experiments on seven natural language understanding tasks and four varying-size LLMs demonstrate the effectiveness of our proposed framework.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.755",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models": {
        "type": "INPROCEEDINGS",
        "key": "riddell-etal-2024-quantifying",
        "author": "Riddell, Martin and Ni, Ansong and Cohan, Arman",
        "booktitle": "ACL2024",
        "title": "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. While recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of LLMs in programming contexts. In this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform significantly better on the subset of the benchmarks where similar solutions are seen during training. We also conduct extensive analysis on the factors that affect model memorization and generalization, such as model size, problem difficulty, and question length. We release all resulting files from our matching pipeline for future research.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.761",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic": {
        "type": "INPROCEEDINGS",
        "key": "bhardwaj-etal-2024-language",
        "author": "Bhardwaj, Rishabh and Do, Duc Anh and Poria, Soujanya",
        "booktitle": "ACL2024",
        "title": "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We propose RESTA to perform LLM realignment towards safety, which gets compromised due to downstream task fine-tuning. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in parameter-efficient and full fine-tuning, respectively, while maintaining most of the model\u2019s performance on the task. We release the source codes at: https://github.com/declare-lab/resta.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.762",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "DeVAn: Dense Video Annotation for Video-Language Models": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2024-devan",
        "author": "Liu, Tingkai and Tao, Yunzhe and Liu, Haogeng and Fang, Qihang and Zhou, Ding and Huang, Huaibo and He, Ran and Yang, Hongxia",
        "booktitle": "ACL2024",
        "title": "DeVAn: Dense Video Annotation for Video-Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present a novel human annotated dataset for evaluating the ability for visual-language models to generate both short and long descriptions for real-world video clips, termed DeVAn (Dense Video Annotation). The dataset contains 8.5K YouTube video clips of 20-60 seconds in duration and covers a wide range of topics and interests. Each video clip is independently annotated by 5 human annotators, producing both captions (1 sentence) and summaries (3-10 sentences). Given any video selected from the dataset and its corresponding ASR information, we evaluate visual-language models on either caption or summary generation that is grounded in both the visual and auditory content of the video. Additionally, models are also evaluated on caption- and summary-based retrieval tasks, where the summary-based retrieval task requires the identification of a target video given excerpts of a given summary. Given the novel nature of the paragraph-length video summarization task, we compared different existing evaluation metrics and their alignment with human preferences and found that model-based evaluation metrics provide more semantically-oriented and human-aligned evaluation. Finally, we benchmarked a wide range of current video-language models on DeVAn, and we aim for DeVAn to serve as a useful evaluation set in the age of large language models and complex multi-modal tasks. Code is available at https://github.com/TK-21st/DeVAn.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.772",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs": {
        "type": "INPROCEEDINGS",
        "key": "zeng-etal-2024-johnny",
        "author": "Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan",
        "booktitle": "ACL2024",
        "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Most traditional AI safety research views models as machines and centers on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. Observing this, we shift the perspective, by treating LLMs as human-like communicators to examine the interplay between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak risk across all risk categories: PAP consistently achieves an attack success rate of over 92% on Llama-2-7b-Chat, GPT-3.5, and GPT-4 in 10 trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for more fundamental solutions for AI safety.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.773",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-exploring",
        "author": "Zhang, Jintian and Xu, Xin and Zhang, Ningyu and Liu, Ruibo and Hooi, Bryan and Deng, Shumin",
        "booktitle": "ACL2024",
        "title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: *Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)?* This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique \u2018societies\u2019 comprised of LLM agents, where each agent is characterized by a specific \u2018trait\u2019 (easy-going or overconfident) and engages in collaboration with a distinct \u2018thinking pattern\u2019 (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets, hoping to catalyze further research in this promising avenue.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.782",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Speech vs. Transcript: Does It Matter for Human Annotators in Speech Summarization?": {
        "type": "INPROCEEDINGS",
        "key": "sharma-etal-2024-speech",
        "author": "Sharma, Roshan and Shon, Suwon and Lindsey, Mark and Dhamyal, Hira and Raj, Bhiksha",
        "booktitle": "ACL2024",
        "title": "Speech vs. Transcript: Does It Matter for Human Annotators in Speech Summarization?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reference summaries for abstractive speech summarization require human annotation, which can be performed by listening to an audio recording or by reading textual transcripts of the recording. In this paper, we examine whether summaries based on annotators listening to the recordings differ from those based on annotators reading transcripts. Using existing intrinsic evaluation based on human evaluation, automatic metrics, LLM-based evaluation, and a retrieval-based reference-free method, we find that summaries are indeed different based on the source modality, and that speech-based summaries are more factually consistent and information-selective than transcript-based summaries. Transcript-based summaries are impacted by recognition errors in the source, and expert-written summaries are more informative and reliable. We make all the collected data and analysis code public to facilitate the reproduction of our work and advance research in this area.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.790",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "D2LLM: Decomposed and Distilled Large Language Models for Semantic Search": {
        "type": "INPROCEEDINGS",
        "key": "liao-etal-2024-d2llm",
        "author": "Liao, Zihan and Yu, Hang and Li, Jianguo and Wang, Jun and Zhang, Wei",
        "booktitle": "ACL2024",
        "title": "D2LLM: Decomposed and Distilled Large Language Models for Semantic Search",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The key challenge in semantic search is to create models that are both accurate and efficient in pinpointing relevant sentences for queries. While BERT-style bi-encoders excel in efficiency with pre-computed embeddings, they often miss subtle nuances in search tasks. Conversely, GPT-style LLMs with cross-encoder designs capture these nuances but are computationally intensive, hindering real-time applications. In this paper, we present D2LLMs\u2014Decomposed and Distilled LLMs for semantic search\u2014that combines the best of both worlds. We decompose a cross-encoder into an efficient bi-encoder integrated with Pooling by Multihead Attention and an Interaction Emulation Module, achieving nuanced understanding and pre-computability. Knowledge from the LLM is distilled into this model using contrastive, rank, and feature imitation techniques. Our experiments show that D2LLM surpasses five leading baselines in terms of all metrics across three tasks, particularly improving NLI task performance by at least 6.45%",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.791",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages": {
        "type": "INPROCEEDINGS",
        "key": "cahyawijaya-etal-2024-cendol",
        "author": "Cahyawijaya, Samuel and Lovenia, Holy and Koto, Fajri and Putri, Rifki and Cenggoro, Wawan and Lee, Jhonson and Akbar, Salsabil and Dave, Emmanuel and Nuurshadieq, Nuurshadieq and Mahendra, Muhammad and Putri, Rr and Wilie, Bryan and Winata, Genta and Aji, Alham and Purwarianti, Ayu and Fung, Pascale",
        "booktitle": "ACL2024",
        "title": "Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) show remarkable human-like capability in various domains and languages. To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes. We highlight Cendol\u2019s effectiveness across a diverse array of tasks, attaining ~20% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia. Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia. In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation. Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.796",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction": {
        "type": "INPROCEEDINGS",
        "key": "yan-etal-2024-talk",
        "author": "Yan, Haoqiu and Zhu, Yongxin and Zheng, Kai and Liu, Bing and Cao, Haoyu and Jiang, Deqiang and Xu, Linli",
        "booktitle": "ACL2024",
        "title": "Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Model (LLM)-enhanced agents become increasingly prevalent in Human-AI communication, offering vast potential from entertainment to professional domains. However, current multi-modal dialogue systems overlook the acoustic information present in speech, which is crucial for understanding human communication nuances. This oversight can lead to misinterpretations of speakers\u2019 intentions, resulting in inconsistent or even contradictory responses within dialogues. To bridge this gap, in this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system designed to discern deeper or more subtle meanings beyond the literal interpretations of words through the integration of speech modality perception. Employing LLMs as a cognitive core, PerceptiveAgent perceives acoustic information from input speech and generates empathetic responses based on speaking styles described in natural language. Experimental results indicate that PerceptiveAgent excels in contextual understanding by accurately discerning the speakers\u2019 true intentions in scenarios where the linguistic meaning is either contrary to or inconsistent with the speaker\u2019s true feelings, producing more nuanced and expressive spoken dialogues. Code is publicly available at: https://github.com/Haoqiu-Yan/PerceptiveAgent.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.801",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "MultiLegalPile: A 689GB Multilingual Legal Corpus": {
        "type": "INPROCEEDINGS",
        "key": "niklaus-etal-2024-multilegalpile",
        "author": "Niklaus, Joel and Matoshi, Veton and St\u00fcrmer, Matthias and Chalkidis, Ilias and Ho, Daniel",
        "booktitle": "ACL2024",
        "title": "MultiLegalPile: A 689GB Multilingual Legal Corpus",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, few datasets are available for specialized critical domains such as law and the available ones are often small and only in English. To fill this gap, we curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. MultiLegalPile includes diverse legal data sources and allows for pretraining NLP models under fair use, with most of the dataset licensed very permissively. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, trained models, and all code under the most open licenses possible.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.805",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations": {
        "type": "INPROCEEDINGS",
        "key": "deng-etal-2024-webcites",
        "author": "Deng, Haolin and Wang, Chang and Xin, Li and Yuan, Dezhang and Zhan, Junlang and Zhou, Tian and Ma, Jin and Gao, Jun and Xu, Ruifeng",
        "booktitle": "ACL2024",
        "title": "WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Enhancing the attribution in large language models (LLMs) is a crucial task. One feasible approach is to enable LLMs to cite external sources that support their generations. However, existing datasets and evaluation methods in this domain still exhibit notable limitations. In this work, we formulate the task of attributed query-focused summarization (AQFS) and present WebCiteS, a Chinese dataset featuring 7k human-annotated summaries with citations. WebCiteS derives from real-world user queries and web search results, offering a valuable resource for model training and evaluation. Prior works in attribution evaluation do not differentiate between groundedness errors and citation errors. They also fall short in automatically verifying sentences that draw partial support from multiple sources. We tackle these issues by developing detailed metrics and enabling the automatic evaluator to decompose the sentences into sub-claims for fine-grained verification. Our comprehensive evaluation of both open-source and proprietary models on WebCiteS highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement. The dataset and code will be open-sourced to facilitate further research in this crucial field.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.806",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "ChatDev: Communicative Agents for Software Development": {
        "type": "INPROCEEDINGS",
        "key": "qian-etal-2024-chatdev",
        "author": "Qian, Chen and Liu, Wei and Liu, Hongzhang and Chen, Nuo and Dang, Yufan and Li, Jiahao and Yang, Cheng and Chen, Weize and Su, Yusheng and Cong, Xin and Xu, Juyuan and Li, Dahai and Liu, Zhiyuan and Sun, Maosong",
        "booktitle": "ACL2024",
        "title": "ChatDev: Communicative Agents for Software Development",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.810",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-psysafe",
        "author": "Zhang, Zaibin and Zhang, Yongting and Li, Lijun and Shao, Jing and Gao, Hongzhi and Qiao, Yu and Wang, Lijun and Lu, Huchuan and Zhao, Feng",
        "booktitle": "ACL2024",
        "title": "PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety.To tackle these concerns, we propose a comprehensive framework (PsySafe) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks.Our experiments reveal several intriguing phenomena, such as the collective dangerous behaviors among agents, agents\u2019 self-reflection when engaging in dangerous behavior, and the correlation between agents\u2019 psychological assessments and dangerous behaviors. We anticipate that our framework and observations will provide valuable insights for further research into the safety of multi-agent systems. We make our data and code publicly accessible at https://github.com/AI4Good24/PsySafe.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.812",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "\u0131nftyBench: Extending Long Context Evaluation Beyond 100K Tokens": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-bench",
        "author": "Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo and Han, Xu and Thai, Zhen and Wang, Shuo and Liu, Zhiyuan and Sun, Maosong",
        "booktitle": "ACL2024",
        "title": "\u0131nftyBench: Extending Long Context Evaluation Beyond 100K Tokens",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose , the first LLM benchmark featuring an average data length surpassing 100K tokens. comprises synthetic and realistic tasks spanning diverse domains in English and Chinese. The tasks in are designed to require an understanding of long dependencies in contexts and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. Based on , we evaluate several state-of-the-art LLMs tailored for processing long contexts. The experimental results indicate that existing long-context LLMs still require significant advancements to process 100K+ contexts effectively. Furthermore, we present three intriguing analyses regarding the behavior of LLMs processing long context. Our code and data is released.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.814",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-pride",
        "author": "Xu, Wenda and Zhu, Guanglei and Zhao, Xuandong and Pan, Liangming and Li, Lei and Wang, William",
        "booktitle": "ACL2024",
        "title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM\u2019s bias in evaluating their own output. In this paper, we formally define LLM\u2019s self-bias \u2013 the tendency to favor its own generation \u2013 using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks. The code and data are released at https://github.com/xu1998hz/llm_self_bias.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.826",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-econagent",
        "author": "Li, Nian and Gao, Chen and Li, Mingyu and Li, Yong and Liao, Qingmin",
        "booktitle": "ACL2024",
        "title": "EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The advent of artificial intelligence has led to a growing emphasis on data-driven modeling in macroeconomics, with agent-based modeling (ABM) emerging as a prominent bottom-up simulation paradigm. In ABM, agents (*e.g.*, households, firms) interact within a macroeconomic environment, collectively generating market dynamics. Existing agent modeling typically employs predetermined rules or learning-based neural networks for decision-making. However, customizing each agent presents significant challenges, complicating the modeling of agent heterogeneity. Additionally, the influence of multi-period market dynamics and multifaceted macroeconomic factors are often overlooked in decision-making processes.In this work, we introduce **EconAgent**, a large language model-empowered agent with human-like characteristics for macroeconomic simulation. We first construct a simulation environment that incorporates various market dynamics driven by agents\u2019 decisions regarding work and consumption. Through the perception module, we create heterogeneous agents with distinct decision-making mechanisms. Furthermore, we model the impact of macroeconomic trends using a memory module, which allows agents to reflect on past individual experiences and market dynamics.Simulation experiments show that EconAgent can make realistic decisions, leading to more reasonable macroeconomic phenomena compared to existing rule-based or learning-based agents. Our codes are released at https://github.com/tsinghua-fib-lab/ACL24-EconAgent.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.829",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!": {
        "type": "INPROCEEDINGS",
        "key": "zhou-etal-2024-emulated",
        "author": "Zhou, Zhanhui and Liu, Jie and Dong, Zhichen and Liu, Jiaheng and Yang, Chao and Ouyang, Wanli and Qiao, Yu",
        "booktitle": "ACL2024",
        "title": "Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) undergo safety alignment to ensure safe conversations with humans. However, this paper introduces a training-free attack method capable of reversing safety alignment, converting the outcomes of stronger alignment into greater potential for harm by accessing only LLM output token distributions. Specifically, our method achieves this reversal by contrasting the output token distribution of a safety-aligned language model (e.g., Llama-2-chat) against its pre-trained version (e.g., Llama-2), so that the token predictions are shifted towards the opposite direction of safety alignment.We name this method emulated disalignment (ED) because sampling from this contrastive distribution provably emulates the result of fine-tuning to minimize a safety reward.Our experiments with ED across three evaluation datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rates in 43 out of 48 evaluation subsets by a large margin.Eventually, given ED\u2019s reliance on language model output token distributions, which particularly compromises open-source models, our findings highlight the need to reassess the open accessibility of language models, even if they have been safety-aligned.Code is available at https://github.com/ZHZisZZ/emulated-disalignment.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.842",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents": {
        "type": "INPROCEEDINGS",
        "key": "trivedi-etal-2024-appworld",
        "author": "Trivedi, Harsh and Khot, Tushar and Hartmann, Mareike and Manku, Ruskin and Dong, Vinty and Li, Edward and Gupta, Shashank and Sabharwal, Ashish and Balasubramanian, Niranjan",
        "booktitle": "ACL2024",
        "title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only ~49% of our \u2018normal\u2019 tasks and ~30% of \u2018challenge\u2019 tasks, while other models solve at least 16% fewer. This highlights the benchmark\u2019s difficulty and AppWorld\u2019s potential to push the frontiers of interactive coding agents.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.850",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-docmath",
        "author": "Zhao, Yilun and Long, Yitao and Liu, Hongjun and Kamoi, Ryo and Nan, Linyong and Chen, Lyuhao and Liu, Yixin and Tang, Xiangru and Zhang, Rui and Cohan, Arman",
        "booktitle": "ACL2024",
        "title": "DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing specialized documents containing both text and tables. We conduct an extensive evaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting methods, aiming to comprehensively assess the capabilities and limitations of existing LLMs in DocMath-Eval. We found that even the current best-performing system (i.e., GPT-4o) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe that DocMath-Eval can serve as a valuable benchmark for evaluating LLMs\u2019 capabilities in solving challenging numerical reasoning problems within expert domains.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.852",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Unintended Impacts of LLM Alignment on Global Representation": {
        "type": "INPROCEEDINGS",
        "key": "ryan-etal-2024-unintended",
        "author": "Ryan, Michael J. and Held, William and Yang, Diyi",
        "booktitle": "ACL2024",
        "title": "Unintended Impacts of LLM Alignment on Global Representation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable preference tuning. We make our code and data publicly available on Github.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-long.853",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "RDRec: Rationale Distillation for LLM-based Recommendation": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-rdrec",
        "author": "Wang, Xinfeng and Cui, Jin and Suzuki, Yoshimi and Fukumoto, Fumiyo",
        "booktitle": "ACL2024",
        "title": "RDRec: Rationale Distillation for LLM-based Recommendation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language model (LLM)-based recommender models that bridge users and items through textual prompts for effective semantic reasoning have gained considerable attention. However, few methods consider the underlying rationales behind interactions, such as user preferences and item attributes, limiting the reasoning ability of LLMs for recommendations. This paper proposes a rationale distillation recommender (RDRec), a compact model designed to learn rationales generated by a larger language model (LM). By leveraging rationales from reviews related to users and items, RDRec remarkably specifies their profiles for recommendations. Experiments show that RDRec achieves state-of-the-art (SOTA) performance in both top-N and sequential recommendations. Our code is available online.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-short.6",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Soft Self-Consistency Improves Language Models Agents": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-soft",
        "author": "Wang, Han and Prasad, Archiki and Stengel-Eskin, Elias and Bansal, Mohit",
        "booktitle": "ACL2024",
        "title": "Soft Self-Consistency Improves Language Models Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current \u201csample and select\u201d methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (SOFT-SC), which replaces SC\u2019s discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. SOFT-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, SOFT-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that SOFT-SC can be applied to both open-source and black-box models.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-short.28",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "horvitz-etal-2024-getting",
        "author": "Horvitz, Zachary and Chen, Jingru and Aditya, Rahul and Srivastava, Harshvardhan and West, Robert and Yu, Zhou and McKeown, Kathleen",
        "booktitle": "ACL2024",
        "title": "Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. We investigate whether large language models (LLMs) can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to \u201cunfun\u201d jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset where we find that GPT-4\u2019s synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-short.76",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot": {
        "type": "INPROCEEDINGS",
        "key": "fei-etal-2024-empathyear",
        "author": "Fei, Hao and Zhang, Han and Wang, Bin and Liao, Lizi and Liu, Qian and Cambria, Erik",
        "booktitle": "ACL2024",
        "title": "EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper introduces EmpathyEar, a pioneering open-source, avatar-based multimodal empathetic chatbot, to fill the gap in traditional text-only empathetic response generation (ERG) systems. Leveraging the advancements of a large language model, combined with multimodal encoders and generators, EmpathyEar supports user inputs in any combination of text, sound, and vision, and produces multimodal empathetic responses, offering users, not just textual responses but also digital avatars with talking faces and synchronized speeches. A series of emotion-aware instruction-tuning is performed for comprehensive emotional understanding and generation capabilities. In this way, EmpathyEar provides users with responses that achieve a deeper emotional resonance, closely emulating human-like empathy. The system paves the way for the next emotional intelligence, for which we open-source the code for public access.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-demos.7",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-easyedit",
        "author": "Wang, Peng and Zhang, Ningyu and Tian, Bozhong and Xi, Zekun and Yao, Yunzhi and Xu, Ziwen and Wang, Mengru and Mao, Shengyu and Wang, Xiaohan and Cheng, Siyuan and Liu, Kangwei and Ni, Yuansheng and Zheng, Guozhou and Chen, Huajun",
        "booktitle": "ACL2024",
        "title": "EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged \u2013 aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub, along with Google Colab tutorials and comprehensive documentation for beginners to get started. Besides, we present an online system for real-time knowledge editing, and a demo video.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-demos.9",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "DocPilot: Copilot for Automating PDF Edit Workflows in Documents": {
        "type": "INPROCEEDINGS",
        "key": "mathur-etal-2024-docpilot",
        "author": "Mathur, Puneet and Siu, Alexa and Manjunatha, Varun and Sun, Tong",
        "booktitle": "ACL2024",
        "title": "DocPilot: Copilot for Automating PDF Edit Workflows in Documents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Digital documents, such as PDFs, are vital in business workflows, enabling communication, documentation, and collaboration. Handling PDFs can involve navigating complex workflows and numerous tools (e.g., comprehension, annotation, editing), which can be tedious and time-consuming for users. We introduce DocPilot, an AI-assisted document workflow Copilot system capable of understanding user intent and executing tasks accordingly to help users streamline their workflows. DocPilot undertakes intelligent orchestration of various tools through LLM prompting in four steps: (1) Task plan generation, (2) Task plan verification and self-correction, (3) Multi-turn User Feedback, and (4) Task Plan Execution via Code Generation and Error log-based Code Self-Revision. The primary goal of this system is to free the user from the intricacies of document editing, enabling them to focus on the creative aspects and enrich their document management experience.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-demos.22",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "An LLM-based Knowledge Synthesis and Scientific Reasoning Framework for Biomedical Discovery": {
        "type": "INPROCEEDINGS",
        "key": "wysocki-etal-2024-llm",
        "author": "Wysocki, Oskar and Magdalena.wysocka@cruk.manchester.ac.uk, Magdalena.wysocka@cruk.manchester.ac.uk and Carvalho, Danilo and Bogatu, Alex and Danilo.miranda@idiap.ch, Danilo.miranda@idiap.ch and Maxime.delmas@idiap.ch, Maxime.delmas@idiap.ch and Harriet.unsworth@cruk.manchester.ac.uk, Harriet.unsworth@cruk.manchester.ac.uk and Freitas, Andre",
        "booktitle": "ACL2024",
        "title": "An LLM-based Knowledge Synthesis and Scientific Reasoning Framework for Biomedical Discovery",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present BioLunar, developed using the Lunar framework, as a tool for supporting biological analyses, with a particular emphasis on molecular-level evidence enrichment for biomarker discovery in oncology. The platform integrates Large Language Models (LLMs) to facilitate complex scientific reasoning across distributed evidence spaces, enhancing the capability for harmonizing and reasoning over heterogeneous data sources. Demonstrating its utility in cancer research, BioLunar leverages modular design, reusable data access and data analysis components, and a low-code user interface, enabling researchers of all programming levels to construct LLM-enabled scientific workflows. By facilitating automatic scientific discovery and inference from heterogeneous evidence, BioLunar exemplifies the potential of the integration between LLMs, specialised databases and biomedical tools to support expert-level knowledge synthesis and discovery.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-demos.34",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph": {
        "type": "INPROCEEDINGS",
        "key": "zhou-etal-2024-cogmg",
        "author": "Zhou, Tong and Chen, Yubo and Liu, Kang and Zhao, Jun",
        "booktitle": "ACL2024",
        "title": "CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models have become integral to question-answering applications despite their propensity for generating hallucinations and factually inaccurate content. Querying knowledge graphs to reduce hallucinations in LLM meets the challenge of incomplete knowledge coverage in knowledge graphs. On the other hand, updating knowledge graphs by information extraction and knowledge graph completion faces the knowledge update misalignment issue. In this work, we introduce a collaborative augmentation framework, CogMG, leveraging knowledge graphs to address the limitations of LLMs in QA scenarios, explicitly targeting the problems of incomplete knowledge coverage and knowledge update misalignment. The LLMs identify and decompose required knowledge triples that are not present in the KG, enriching them and aligning updates with real-world demands. We demonstrate the efficacy of this approach through a supervised fine-tuned LLM within an agent framework, showing significant improvements in reducing hallucinations and enhancing factual accuracy in QA responses. Our code and video are publicly available.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-demos.35",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "ReMAG-KR: Retrieval and Medically Assisted Generation with Knowledge Reduction for Medical Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "murali-etal-2024-remag",
        "author": "Murali, Sidhaarth and S., Sowmya and R, Supreetha",
        "booktitle": "ACL2024",
        "title": "ReMAG-KR: Retrieval and Medically Assisted Generation with Knowledge Reduction for Medical Question Answering",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have significant potential for facilitating intelligent end-user applications in healthcare. However, hallucinations remain an inherent problem with LLMs, making it crucial to address this issue with extensive medical knowledge and data. In this work, we propose a Retrieve-and-Medically-Augmented-Generation with Knowledge Reduction (ReMAG-KR) pipeline, employing a carefully curated knowledge base using cross-encoder re-ranking strategies. The pipeline is tested on medical MCQ-based QA datasets as well as general QA datasets. It was observed that when the knowledge base is reduced, the model\u2019s performance decreases by 2-8%, while the inference time improves by 47%.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-srw.13",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Knowledge Editing of Large Language Models Unconstrained by Word Order": {
        "type": "INPROCEEDINGS",
        "key": "ishigaki-etal-2024-knowledge",
        "author": "Ishigaki, Ryoma and Suzuki, Jundai and Shuzo, Masaki and Maeda, Eisaku",
        "booktitle": "ACL2024",
        "title": "Knowledge Editing of Large Language Models Unconstrained by Word Order",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the LLMs themselves. To address this issue, a method called local modification-based knowledge editing has been developed. This method identifies the knowledge neurons that encode the target knowledge and adjusts the parameters associated with these neurons to update the knowledge. Knowledge neurons are identified by masking the o part from sentences representing relational triplets (s, r, o), having the LLM predict the masked part, and observing the LLM\ufffds activation during the prediction. When the architecture is decoder-based, the predicted o needs to be located at the end of the sentence. Previous local modification-based knowledge editing methods for decoder-based models have assumed SVO languages and faced challenges when applied to SOV languages such as Japanese. In this study, we propose a knowledge editing method that eliminates the need for word order constraints by converting the input for identifying knowledge neurons into a question where o is the answer. We conducted validation experiments on 500 examples and confirmed that the proposed method is effective for Japanese, a non-SVO language. We also applied this method to English, an SVO language, and demonstrated that it outperforms conventional methods.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-srw.23",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "On Improving Repository-Level Code QA for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "strich-etal-2024-improving",
        "author": "Strich, Jan and Schneider, Florian and Nikishina, Irina and Biemann, Chris",
        "booktitle": "ACL2024",
        "title": "On Improving Repository-Level Code QA for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) such as ChatGPT, GitHub Copilot, Llama, or Mistral assist programmers as copilots and knowledge sources to make the coding process faster and more efficient. This paper aims to improve the copilot performance by implementing different self-alignment processes and retrieval-augmented generation (RAG) pipelines, as well as their combination. To test the effectiveness of all approaches, we create a dataset and apply a model-based evaluation, using LLM as a judge. It is designed to check the model\u2019s abilities to understand the source code semantics, the dependency between files, and the overall meta-information about the repository. We also compare our approach with other existing solutions, e.g. ChatGPT-3.5, and evaluate on the existing benchmarks. Code and dataset are available online (https://anonymous.4open.science/r/ma_llm-382D).",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-srw.28",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Trace-of-Thought Prompting: Investigating Prompt-Based Knowledge Distillation Through Question Decomposition": {
        "type": "INPROCEEDINGS",
        "key": "mcdonald-emami-2024-trace",
        "author": "McDonald, Tyler and Emami, Ali",
        "booktitle": "ACL2024",
        "title": "Trace-of-Thought Prompting: Investigating Prompt-Based Knowledge Distillation Through Question Decomposition",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Knowledge distillation allows smaller neural networks to emulate the performance of larger, teacher models with reduced computational demands. Traditional methods for Large Language Models (LLMs) often necessitate extensive fine-tuning, which limits their accessibility. To address this, we introduce Trace-of-Thought Prompting, a novel framework designed to distill critical reasoning capabilities from large-scale teacher models (over 8 billion parameters) to small-scale student models (up to 8 billion parameters). This approach leverages problem decomposition to enhance interpretability and facilitate human-in-the-loop interventions. Empirical evaluations on the GSM8K and MATH datasets show that student models achieve accuracy gains of up to 113% on GSM8K and 20% on MATH, with significant improvements particularly notable in smaller models like Llama 2 and Zephyr. Our results suggest a promising pathway for open-source, small-scale models to eventually serve as both students and teachers, potentially reducing our reliance on large-scale, proprietary models. Our code, featuring data analytics and testing scripts, is provided here: https://github.com/traceofthought/trace-of-thought-prompting/tree/main.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-srw.35",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Improving Sentence Embeddings with Automatic Generation of Training Data Using Few-shot Examples": {
        "type": "INPROCEEDINGS",
        "key": "sato-etal-2024-improving",
        "author": "Sato, Soma and Tsukagoshi, Hayato and Sasano, Ryohei and Takeda, Koichi",
        "booktitle": "ACL2024",
        "title": "Improving Sentence Embeddings with Automatic Generation of Training Data Using Few-shot Examples",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL requires a manually annotated natural language inference (NLI) dataset for fine-tuning.We aim to improve sentence embeddings without using large manually annotated datasets by automatically generating an NLI dataset with an LLM and using it for fine-tuning of PromptEOL. To achieve this, we explore methods of data generation suitable for sentence embedding learning in this study. Specifically, we will focus on automatic dataset generation through few-shot learning and explore the appropriate methods to leverage few-shot examples. Experimental results on the STS tasks demonstrate that our approach outperforms existing models in settings without large manually annotated datasets.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-srw.43",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "CheckersGPT: Learning World Models through Language Modeling": {
        "type": "INPROCEEDINGS",
        "key": "joshi-etal-2024-checkersgpt",
        "author": "Joshi, Abhinav and Sharma, Vaibhav and Modi, Ashutosh",
        "booktitle": "ACL2024",
        "title": "CheckersGPT: Learning World Models through Language Modeling",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although Large Language Models (LLMs) have been trained using just the next token prediction objective, these have shown impressive performance on various tasks. Consequently, it has attracted research interests in this regard. While one line of work in the past has suggested that LLMs learn surface-level statistics from the dataset, another line of work emphasizes that the learned representations are effective for simulating the underlying world model, considering the causal relationship for the next token prediction. This phenomenon is often referred to as the emergence of a world model in sequence prediction tasks. Recent work has demonstrated this phenomenon in a simulated setting of board games like Othello and Chess. In this paper, we analyze the game of Checkers to find out the emergence of a world model in a language model. By training a GPT-style autoregressive language model using only the next character prediction objective, we find that the model does show a hint of learning a world model representation of the board positions. We perform our analysis on two datasets: 1) synthetic dataset, which comes from the checkers game tree, and 2) human gameplay dataset. With multiple models trained with different layer sizes, we find that increasing the parameter size does help learn better world model representation decoded by linear probes.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-srw.48",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "Vulnerabilities of Large Language Models to Adversarial Attacks": {
        "type": "INPROCEEDINGS",
        "key": "fu-etal-2024-vulnerabilities",
        "author": "Fu, Yu and Shayegan, Erfan and Abdullah, Md. Mamun Al and Zaree, Pedram and Abu-Ghazaleh, Nael and Dong, Yue",
        "booktitle": "ACL2024",
        "title": "Vulnerabilities of Large Language Models to Adversarial Attacks",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This tutorial serves as a comprehensive guide on the vulnerabilities of Large Language Models (LLMs) to adversarial attacks, an interdisciplinary field that blends perspectives from Natural Language Processing (NLP) and Cybersecurity. As LLMs become more complex and integrated into various systems, understanding their security attributes is crucial. However, current research indicates that even safety-aligned models are not impervious to adversarial attacks that can result in incorrect or harmful outputs. The tutorial first lays the foundation by explaining safety-aligned LLMs and concepts in cybersecurity. It then categorizes existing research based on different types of learning architectures and attack methods. We highlight the existing vulnerabilities of unimodal LLMs, multi-modal LLMs, and systems that integrate LLMs, focusing on adversarial attacks designed to exploit weaknesses and mislead AI systems. Finally, the tutorial delves into the potential causes of these vulnerabilities and discusses potential defense mechanisms.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2024.acl-tutorials.5",
        "doi": "",
        "ISSN": "",
        "month": "August"
    },
    "SrcMarker: Dual-Channel Source Code Watermarking via Scalable Code Transformations": {
        "type": "INPROCEEDINGS",
        "key": "10646683",
        "author": "Yang, Borui and Li, Wei and Xiang, Liyao and Li, Bo",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "SrcMarker: Dual-Channel Source Code Watermarking via Scalable Code Transformations",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "4088-4106",
        "abstract": "The expansion of the open source community and the rise of large language models have raised ethical and security concerns on the distribution of source code, such as misconduct on copyrighted code, distributions without proper licenses, or misuse of the code for malicious purposes. Hence it is important to track the ownership of source code, in which watermarking is a major technique. Yet, drastically different from natural languages, source code watermarking requires far stricter and more complicated rules to ensure the readability as well as the functionality of the source code. Hence we introduce SrcMarker, a watermarking system to unobtrusively encode ID bitstrings into source code, without affecting the usage and semantics of the code. To this end, SrcMarker performs transformations on an AST-based intermediate representation that enables unified transformations across different programming languages. The core of the system utilizes learning-based embedding and extraction modules to select rule-based transformations for watermarking. In addition, a novel feature-approximation technique is designed to tackle the inherent non-differentiability of rule selection, thus seamlessly integrating the rule-based transformations and learning-based networks into an interconnected system to enable end-to-end training. Extensive experiments demonstrate the superiority of SrcMarker over existing methods in various watermarking requirements.",
        "keywords": "Training;Computer languages;Privacy;Codes;Source coding;Semantics;Pipelines",
        "doi": "10.1109/SP54263.2024.00097",
        "ISSN": "2375-1207",
        "month": "May"
    },
    "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks": {
        "type": "INPROCEEDINGS",
        "key": "10646663",
        "author": "Ullah, Saad and Han, Mingji and Pujar, Saurabh and Pearce, Hammond and Coskun, Ayse and Stringhini, Gianluca",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "862-880",
        "abstract": "Large Language Models (LLMs) have been suggested for use in automated vulnerability repair, but benchmarks showing they can consistently identify security-related bugs are lacking. We thus develop SecLLMHolmes, a fully automated evaluation framework that performs the most detailed investigation to date on whether LLMs can reliably identify and reason about security-related bugs. We construct a set of 228 code scenarios and analyze eight of the most capable LLMs across eight different investigative dimensions using our framework. Our evaluation shows LLMs provide non-deterministic responses, incorrect and unfaithful reasoning, and perform poorly in real-world scenarios. Most importantly, our findings reveal significant non-robustness in even the most advanced models like \u2018PaLM2\u2019 and \u2018GPT-4\u2019: by merely changing function or variable names, or by the addition of library functions in the source code, these models can yield incorrect answers in 26% and 17% of cases, respectively. These findings demonstrate that further LLM advances are needed before LLMs can be used as general purpose security assistants.",
        "keywords": "Privacy;Source coding;Computer bugs;Benchmark testing;Maintenance engineering;Cognition;Libraries;Machine Learning;Computer Security",
        "doi": "10.1109/SP54263.2024.00210",
        "ISSN": "2375-1207",
        "month": "May"
    },
    "LLMIF: Augmented Large Language Model for Fuzzing IoT Devices": {
        "type": "INPROCEEDINGS",
        "key": "10646659",
        "author": "Wang, Jincheng and Yu, Le and Luo, Xiapu",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "LLMIF: Augmented Large Language Model for Fuzzing IoT Devices",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "881-896",
        "abstract": "Despite the efficacy of fuzzing in verifying the implementation correctness of network protocols, existing IoT protocol fuzzing approaches grapple with several limitations, including obfuscated message formats, unresolved message dependencies, and a lack of evaluations on the testing cases. These limitations significantly curtail the capabilities of IoT fuzzers in vulnerability identification. In this work, we show that the protocol specification contains fruitful descriptions of protocol messages, which can be used to overcome the above limitations and guide IoT protocol fuzzing. To automate the specification analysis, we augment the large language model with the specification contents, and drive it to perform two tasks (i.e., protocol information extraction, and device response reasoning). We further design and implement a fuzzing algorithm, LLMIF, which incorporates the LLM into IoT fuzzing. Finally, we select Zigbee as the target protocol and initiate comprehensive evaluations. The evaluation result shows that LLMIF successfully addressed the above limitations. Compared with the existing Zigbee fuzzers, it increases the protocol message coverage and code coverage by 55.2% and 53.9%, respectively. Besides the enhanced coverage, LLMIF unearthed 11 vulnerabilities on real-world Zigbee devices, which include eight previously unknown vulnerabilities. Seven of them are not covered by the existing Zigbee fuzzers.",
        "keywords": "Privacy;Protocols;Codes;Large language models;Zigbee;Fuzzing;Internet of Things;fuzzing;IoT device;large language model",
        "doi": "10.1109/SP54263.2024.00211",
        "ISSN": "2375-1207",
        "month": "May"
    },
    "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models": {
        "type": "INPROCEEDINGS",
        "key": "10646865",
        "author": "Aghakhani, Hojjat and Dai, Wei and Manoel, Andre and Fernandes, Xavier and Kharkar, Anant and Kruegel, Christopher and Vigna, Giovanni and Evans, David and Zorn, Ben and Sim, Robert",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "1122-1140",
        "abstract": "With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model\u2019s training by injecting malicious data. Poisoning attacks could be designed to influence the model\u2019s suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, Covert and TrojanPuzzle, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TrojanPuzzle, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TrojanPuzzle robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both Covert and TrojanPuzzle have significant implications for practitioners when selecting code used to train or tune code-suggestion models.",
        "keywords": "Training;Codes;Toxicology;Training data;Static analysis;Transformers;Data models;Large Language Models;Generative AI;Code Generation;Data Poisoning;Trustworthy AI",
        "doi": "10.1109/SP54263.2024.00140",
        "ISSN": "2375-1207",
        "month": "May"
    },
    "Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694987",
        "author": "Gao, Xinyu and Xiong, Yun and Wang, Deze and Guan, Zhenhan and Shi, Zejian and Wang, Haofen and Li, Shanshan",
        "title": "Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694987",
        "doi": "10.1145/3691620.3694987",
        "abstract": "Retrieval-augmented code generation utilizes Large Language Models as the generator and significantly expands their code generation capabilities by providing relevant code, documentation, and more via the retriever. The current approach suffers from two primary limitations: 1) information redundancy. The indiscriminate inclusion of redundant information can result in resource wastage and may misguide generators, affecting their effectiveness and efficiency. 2) preference gap. Due to different optimization objectives, the retriever strives to procure code with higher ground truth similarity, yet this effort does not substantially benefit the generator. The retriever and the generator may prefer different golden code, and this gap in preference results in a suboptimal design. Additionally, differences in parameterization knowledge acquired during pre-training result in varying preferences among different generators.To address these limitations, in this paper, we propose RRG (Retrieve, Refactor, Generate), a novel framework for effective and efficient code generation. This framework introduces a code refactorer module between the retriever and the generator to bridge them. The refactoring process transforms the raw retrieved code into a more concise, efficient, and model-friendly version. It eliminates redundant information and noise, reducing the input length. Consequently, the generator receives higher-quality context, enabling it to produce more accurate results with lower inference costs. We conducted comprehensive experiments on multiple datasets. In the experiments, we confirmed the existence of a preference gap between the retriever and the generator, and RRG effectively bridges this gap. Specifically, RRG achieved significant performance improvements, with increases of up to 28\\% on EM, 13\\% on BLEU, and 6.8\\% on CodeBLEU.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "65\u201377",
        "numpages": "13",
        "keywords": "retrieval-augmented code generation, preference-guided refactorer, deep reinforcement learning",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Instructive Code Retriever: Learn from Large Language Model's Feedback for Code Intelligence Tasks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694997",
        "author": "Lu, Jiawei and Wang, Haoye and Liu, Zhongxin and Liang, Keyu and Bao, Lingfeng and Yang, Xiaohu",
        "title": "Instructive Code Retriever: Learn from Large Language Model's Feedback for Code Intelligence Tasks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694997",
        "doi": "10.1145/3691620.3694997",
        "abstract": "Recent studies proposed to leverage large language models (LLMs) with In-Context Learning (ICL) to handle code intelligence tasks without fine-tuning. ICL employs task instructions and a set of examples as demonstrations to guide the model in generating accurate answers without updating its parameters. While ICL has proven effective for code intelligence tasks, its performance heavily relies on the selected examples. Previous work has achieved some success in using BM25 to retrieve examples for code intelligence tasks. However, existing approaches lack the ability to understand the semantic and structural information of queries, resulting in less helpful demonstrations. Moreover, they do not adapt well to the complex and dynamic nature of user queries in diverse domains. In this paper, we introduce a novel approach named Instructive Code Retriever (ICR), which is designed to retrieve examples that enhance model inference across various code intelligence tasks and datasets. We enable ICR to learn the semantic and structural information of the corpus by a tree-based loss function. To better understand the correlation between queries and examples, we incorporate the feedback from LLMs to guide the training of the retriever. Experimental results demonstrate that our retriever significantly outperforms state-of-the-art approaches. We evaluate our model's effectiveness on various tasks, i.e., code summarization, program synthesis, and bug fixing. Compared to previous state-of-the-art algorithms, our method achieved improvements of 50.0\\% and 90.0\\% in terms of BLEU-4 for two code summarization datasets, 74.6\\% CodeBLEU on program synthesis dataset, and increases of 3.6 and 3.2 BLEU-4 on two bug fixing datasets.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "191\u2013203",
        "numpages": "13",
        "keywords": "software engineering, large language models, in-context learning",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Understanding Code Changes Practically with Small-Scale Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694999",
        "author": "Li, Cong and Xu, Zhaogui and Di, Peng and Wang, Dongxia and Li, Zheng and Zheng, Qian",
        "title": "Understanding Code Changes Practically with Small-Scale Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694999",
        "doi": "10.1145/3691620.3694999",
        "abstract": "Recent studies indicate that traditional techniques for understanding code changes are not as effective as techniques that directly prompt language models (LMs). However, current LM-based techniques heavily rely on expensive, large LMs (LLMs) such as GPT-4 and Llama-13b, which are either commercial or prohibitively costly to deploy on a wide scale, thereby restricting their practical applicability. This paper explores the feasibility of deploying small LMs (SLMs) while maintaining comparable or superior performance to LLMs in code change understanding. To achieve this, we created a small yet high-quality dataset called HQCM which was meticulously reviewed, revised, and validated by five human experts. We fine-tuned state-of-the-art 7b and 220m SLMs using HQCM and compared them with traditional techniques and LLMs with \u226570b parameters. Our evaluation confirmed HQCM's benefits and demonstrated that SLMs, after finetuning by HQCM, can achieve superior performance in three change understanding tasks: change summarization, change classification, and code refinement. This study supports the use of SLMs in environments with security, computational, and financial constraints, such as in industry scenarios and on edge devices, distinguishing our work from the others.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "216\u2013228",
        "numpages": "13",
        "keywords": "code change, code review, language model, LLM, SLM",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695000",
        "author": "Sun, Zhihong and Wan, Yao and Li, Jia and Zhang, Hongyu and Jin, Zhi and Li, Ge and Lyu, Chen",
        "title": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695000",
        "doi": "10.1145/3691620.3695000",
        "abstract": "Large Language Models (LLMs), such as GPT-4, StarCoder, and Code Llama, are transforming the way developers approach programming by automatically generating code based on given contexts, such as natural language descriptions or incomplete surrounding code. Despite advancements, generating syntactically and semantically correct code remains challenging, especially for complex programming tasks. Existing approaches typically generate multiple candidate solutions using LLMs to increase the likelihood of producing correct code. However, selecting the correct code from these candidates --- a process known as code ranking --- remains a major challenge. Current research on code ranking can be categorized into execution-based and non-execution-based methods. Execution-based methods, although effective, encounter notable limitations, such as scarcity of quality unit tests and security risks. Non-execution-based methods like CodeRanker, which rely solely on classification labels to train a code ranker, struggle to capture subtle errors and provide detailed error insights. Recognizing the strengths and limitations of both approaches, we propose a new method that integrates the advantages of execution-based and non-execution-based techniques. The key insight of our work is that an effective code ranker is expected to truly comprehend the underlying causes of erroneous code, as relying solely on classification labels is insufficient. Inspired by this, this paper puts forward RankEF, an innovative approach for code ranking that leverages execution feedback. RankEF employs multi-task learning to integrate code classification with execution feedback generation. This approach enables the model to understand the reasons behind incorrect code, distinguishing between correct and incorrect solutions without the need to execute the code during the ranking phase. Experiments on three code generation benchmarks---APPS, MBPP, and HumanEval---demonstrate that RankEF significantly outperforms the state-of-the-art CodeRanker, achieving relative improvements of +30.97\\%, +31.43\\%, and +19.51\\% in Pass@1, Pass@2, and Pass@5 on APPS test, respectively.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "229\u2013241",
        "numpages": "13",
        "keywords": "code generation, code ranking, execution feedback",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Imperceptible Content Poisoning in LLM-Powered Applications": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695001",
        "author": "Zhang, Quan and Zhou, Chijin and Go, Gwihwan and Zeng, Binqi and Shi, Heyuan and Xu, Zichen and Jiang, Yu",
        "title": "Imperceptible Content Poisoning in LLM-Powered Applications",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695001",
        "doi": "10.1145/3691620.3695001",
        "abstract": "Large Language Models (LLMs) have shown their superior capability in natural language processing, promoting extensive LLM-powered applications to be the new portals for people to access various content on the Internet. However, LLM-powered applications do not have sufficient security considerations on untrusted content, leading to potential threats. In this paper, we reveal content poisoning, where attackers can tailor attack content that appears benign to humans but causes LLM-powered applications to generate malicious responses. To highlight the impact of content poisoning and inspire the development of effective defenses, we systematically analyze the attack, focusing on the attack modes in various content, exploitable design features of LLM application frameworks, and the generation of attack content. We carry out a comprehensive evaluation on five LLMs, where content poisoning achieves an average attack success rate of 89.60\\%. Additionally, we assess content poisoning on four popular LLM-powered applications, achieving the attack on 72.00\\% of the content. Our experimental results also show that existing defenses are ineffective against content poisoning. Finally, we discuss potential mitigations for LLM application frameworks to counter content poisoning.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "242\u2013254",
        "numpages": "13",
        "keywords": "LLM applications, content poisoning",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "What Is Wrong with My Model? Identifying Systematic Problems with Semantic Data Slicing": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695033",
        "author": "Yang, Chenyang and Hong, Yining and Lewis, Grace and Wu, Tongshuang and K\\\"{a}stner, Christian",
        "title": "What Is Wrong with My Model? Identifying Systematic Problems with Semantic Data Slicing",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695033",
        "doi": "10.1145/3691620.3695033",
        "abstract": "Machine learning models make mistakes, yet sometimes it is difficult to identify the systematic problems behind the mistakes. Practitioners engage in various activities, including error analysis, testing, auditing, and red-teaming, to form hypotheses of what can go (or has gone) wrong with their models. To validate these hypotheses, practitioners employ data slicing to identify relevant examples. However, traditional data slicing is limited by available features and programmatic slicing functions. In this work, we propose SemSlicer, a framework that supports semantic data slicing, which identifies a semantically coherent slice, without the need for existing features. SemSlicer uses Large Language Models to annotate datasets and generate slices from any user-defined slicing criteria. We show that SemSlicer generates accurate slices with low cost, allows flexible trade-offs between different design dimensions, reliably identifies under-performing data slices, and helps practitioners identify useful data slices that reflect systematic problems.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "306\u2013318",
        "numpages": "13",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695010",
        "author": "Zhang, Yichi and Liu, Zixi and Feng, Yang and Xu, Baowen",
        "title": "Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695010",
        "doi": "10.1145/3691620.3695010",
        "abstract": "Rust is renowned for its robust memory safety capabilities, yet its distinctive memory management model poses substantial challenges in both writing and understanding programs. Within Rust source code, comments are employed to clearly delineate conditions that might cause panic behavior, thereby warning developers about potential hazards associated with specific operations. Therefore, comments are particularly crucial for documenting Rust's program logic and design. Nevertheless, as modern software frequently undergoes updates and modifications, maintaining the accuracy and relevance of these comments becomes a labor-intensive endeavor.In this paper, inspired by the remarkable capabilities of Large Language Models (LLMs) in understanding software programs, we propose a code-comment inconsistency detection tool, namely RustC4, that combines program analysis and LLM-driven techniques to identify inconsistencies in code comments. RustC4 leverages LLMs' ability to interpret natural language descriptions within code comments, facilitating the extraction of design constraints. Program analysis techniques are then employed to accurately verify the implementation of these constraints. To evaluate the effectiveness of RustC4, we construct a dataset from 12 large-scale real-world Rust projects. The experiment results demonstrate that RustC4 is effective in detecting 176 real inconsistent cases and 23 of them have been confirmed and fixed by developers by the time this paper was submitted.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "356\u2013366",
        "numpages": "11",
        "keywords": "code comment inconsistency, program analysis, large language model, bug detection",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695013",
        "author": "Wu, Yulun and Wen, Ming and Yu, Zeliang and Guo, Xiaochen and Jin, Hai",
        "title": "Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695013",
        "doi": "10.1145/3691620.3695013",
        "abstract": "Open-source software (OSS) has profoundly transformed the software development paradigm by facilitating effortless code reuse. However, in recent years, there has been an alarming increase in disclosed vulnerabilities within OSS, posing significant security risks to downstream users. Therefore, analyzing existing vulnerabilities and precisely assessing their threats to downstream applications become pivotal. Plenty of efforts have been made recently towards this problem, such as vulnerability reachability analysis and vulnerability reproduction. The key to these tasks is identifying the vulnerable function (i.e., the function where the root cause of a vulnerability resides). However, public vulnerability datasets (e.g., NVD) rarely include this information as pinpointing the exact vulnerable functions remains to be a longstanding challenge.Existing methods mainly detect vulnerable functions based on vulnerability patches or Proof-of-Concept (PoC). However, such methods face significant limitations due to data availability and the requirement for extensive manual efforts, thus hindering scalability. To address this issue, we propose a novel approach VFFinder that localizes vulnerable functions based on Common Vulnerabilities and Exposures (CVE) descriptions and the corresponding source code utilizing Large Language Models (LLMs). Specifically, VFFinder adopts a customized in-context learning (ICL) approach based on CVE description patterns to enable LLM to extract key entities. It then performs priority matching with the source code to localize vulnerable functions. We assess the performance of VFFinder on 75 large open-source projects. The results demonstrate that VFFinder surpasses existing baselines significantly. Notably, the Top-1 and MRR metrics have been improved substantially, averaging 4.25X and 2.37X respectively. We also integrate VFFinder with Software Composition Analysis (SCA) tools, and the results show that our tool can reduce the false positive rates of existing SCA tools significantly.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "393\u2013405",
        "numpages": "13",
        "keywords": "vulnerability analysis, vulnerable function, large language model",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695014",
        "author": "Wu, Guangyuan and Cao, Weining and Yao, Yuan and Wei, Hengfeng and Chen, Taolue and Ma, Xiaoxing",
        "title": "LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695014",
        "doi": "10.1145/3691620.3695014",
        "abstract": "Loop invariant inference, a key component in program verification, is a challenging task due to the inherent undecidability and complex loop behaviors in practice. Recently, machine learning based techniques have demonstrated impressive performance in generating loop invariants automatically. However, these methods highly rely on the labeled training data, and are intrinsically random and uncertain, leading to unstable performance. In this paper, we investigate a synergy of large language models (LLMs) and bounded model checking (BMC) to address these issues. The key observation is that, although LLMs may not be able to return the correct loop invariant in one response, they usually can provide all individual predicates of the correct loop invariant in multiple responses. To this end, we propose a \"query-filter-reassemble\" strategy, namely, we first leverage the language generation power of LLMs to produce a set of candidate invariants, where training data is not needed. Then, we employ BMC to identify valid predicates from these candidate invariants, which are assembled to produce new candidate invariants and checked by off-the-shelf SMT solvers. The feedback is incorporated into the prompt for the next round of LLM querying. We expand the existing benchmark of 133 programs to 316 programs, providing a more comprehensive testing ground. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art techniques, successfully generating 309 loop invariants out of 316 cases, whereas the existing baseline methods are only able to tackle 219 programs at best. The code is publicly available at https://github.com/SoftWiser-group/LaM4Inv.git.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "406\u2013417",
        "numpages": "12",
        "keywords": "loop invariant, program verification, large language model",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Semantic-Enhanced Indirect Call Analysis with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695016",
        "author": "Cheng, Baijun and Zhang, Cen and Wang, Kailong and Shi, Ling and Liu, Yang and Wang, Haoyu and Guo, Yao and Li, Ding and Chen, Xiangqun",
        "title": "Semantic-Enhanced Indirect Call Analysis with Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695016",
        "doi": "10.1145/3691620.3695016",
        "abstract": "In contemporary software development, the widespread use of indirect calls to achieve dynamic features poses challenges in constructing precise control flow graphs (CFGs), which further impacts the performance of downstream static analysis tasks. To tackle this issue, various types of indirect call analyzers have been proposed. However, they do not fully leverage the semantic information of the program, limiting their effectiveness in real-world scenarios.To address these issues, this paper proposes Semantic-Enhanced Analysis (SEA), a new approach to enhance the effectiveness of indirect call analysis. Our fundamental insight is that for common programming practices, indirect calls often exhibit semantic similarity with their invoked targets. This semantic alignment serves as a supportive mechanism for static analysis techniques in filtering out false targets. Notably, contemporary large language models (LLMs) are trained on extensive code corpora, encompassing tasks such as code summarization, making them well-suited for semantic analysis. Specifically, SEA leverages LLMs to generate natural language summaries of both indirect calls and target functions from multiple perspectives. Through further analysis of these summaries, SEA can determine their suitability as caller-callee pairs. Experimental results demonstrate that SEA can significantly enhance existing static analysis methods by producing more precise target sets for indirect calls.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "430\u2013442",
        "numpages": "13",
        "keywords": "indirect-call analysis, semantic analysis, LLM",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "DRMiner: Extracting Latent Design Rationale from Jira Issue Logs": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695019",
        "author": "Zhao, Jiuang and Yang, Zitian and Zhang, Li and Lian, Xiaoli and Yang, Donghao and Tan, Xin",
        "title": "DRMiner: Extracting Latent Design Rationale from Jira Issue Logs",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695019",
        "doi": "10.1145/3691620.3695019",
        "abstract": "Software architectures are usually meticulously designed to address multiple quality concerns and support long-term maintenance. However, there may be a lack of motivation for developers to document design rationales (i.e., the design alternatives and the underlying arguments for making or rejecting decisions) when they will not gain immediate benefit, resulting in a lack of standard capture of these rationales. With the turnover of developers, the architecture inevitably becomes eroded. This issue has motivated a number of studies to extract design knowledge from open-source communities in recent years. Unfortunately, none of the existing research has successfully extracted solutions alone with their corresponding arguments due to challenges such as the intricate semantics of online discussions and the lack of benchmarks for design rationale extraction.In this paper, we propose a novel approach, named DRMiner, to automatically mine latent design rationales from developers' live discussion in open-source community (i.e., issue logs in Jira). To better identify solutions and their relevant arguments, DRMiner skillfully decomposes the problem into multiple text classification tasks and tackles them using prompt tuning of large language models (LLMs) and specific heuristic features. To evaluate DRMiner, we acquire issue logs from Cassandra, Flink, and Solr repositories in Jira and form a dataset for design rationale mining. Experimental results show that DRMiner outperforms all baselines and achieves F1 improvements of 24\\%, 22\\%, and 20\\% for mining design rationales, solutions, and arguments, respectively, compared to the best baseline. Furthermore, we investigate the usefulness of the design rationales mined by DRMiner for automated program repair (APR) and find that advanced LLMs, when prompted with these extracted rationales, generate 10\\texttimes{}-18\\texttimes{} more full-match patches and achieve a 10\\%-13\\% gain in CodeBLEU scores.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "468\u2013480",
        "numpages": "13",
        "keywords": "design rationale, issue logs, design discussion, design recovery, program maintenance",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "WaDec: Decompiling WebAssembly Using Large Language Model": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695020",
        "author": "She, Xinyu and Zhao, Yanjie and Wang, Haoyu",
        "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695020",
        "doi": "10.1145/3691620.3695020",
        "abstract": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34\\%, a dramatic 97\\% reduction compared to the state-of-the-art's 116.94\\%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11\\%, a re-execution rate of 43.55\\%, and an output consistency of 27.15\\%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185\\%, cyclomatic complexity by 8\\%, and cosine similarity by 41\\%, achieving an average code similarity above 50\\%. In summary, WaDec enhances understanding of the code's structure and execution flow, facilitating automated code analysis, optimization, and security auditing.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "481\u2013492",
        "numpages": "12",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "FAIL: Analyzing Software Failures from the News Using LLMs": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695022",
        "author": "Anandayuvaraj, Dharun and Campbell, Matthew and Tewari, Arav and Davis, James C",
        "title": "FAIL: Analyzing Software Failures from the News Using LLMs",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695022",
        "doi": "10.1145/3691620.3695022",
        "abstract": "Software failures inform engineering work, standards, regulations. For example, the Log4J vulnerability brought government and industry attention to evaluating and securing software supply chains. Retrospective failure analysis is thus a valuable line of software engineering research. Accessing private engineering records is difficult, so such analyses tend to use information reported by the news media. However, prior works in this direction have relied on manual analysis. That has limited the scale of their analyses. The community lacks automated support to enable such analyses to consider a wide range of news sources and incidents.To fill this gap, we propose the Failure Analysis Investigation with LLMs (FAIL) system. FAIL is a novel LLM-based pipeline that collects, analyzes, and summarizes software failures as reported in the news. FAIL groups articles that describe the same incidents. It then analyzes incidents using existing taxonomies for postmortems, faults, and system characteristics. To tune and evaluate FAIL, we followed the methods of prior works by manually analyzing 31 software failures. FAIL achieved an F1 score of 90\\% for collecting news about software failures, a V-measure of 0.98 for merging articles reporting on the same incident, and extracted 90\\% of the facts about failures. We then applied FAIL to a total of 137,427 news articles from 11 providers published between 2010 and 2022. FAIL identified and analyzed 2,457 distinct failures reported across 4,184 articles. Our findings include: (1) current generation of large language models are capable of identifying news articles that describe failures, and analyzing them according to structured taxonomies; (2) high recurrences of similar failures within organizations and across organizations; and (3) severity of the consequences of software failures have increased over the past decade. The full FAIL database is available so that researchers, engineers, and policymakers can learn from a diversity of software failures.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "506\u2013518",
        "numpages": "13",
        "keywords": "software failure analysis, news analysis, large language models, empirical software engineering",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "MR-Adopt: Automatic Deduction of Input Transformation Function for Metamorphic Testing": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3696020",
        "author": "Xu, Congying and Chen, Songqiang and Wu, Jiarong and Cheung, Shing-Chi and Terragni, Valerio and Zhu, Hengcheng and Cao, Jialun",
        "title": "MR-Adopt: Automatic Deduction of Input Transformation Function for Metamorphic Testing",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3696020",
        "doi": "10.1145/3691620.3696020",
        "abstract": "While a recent study reveals that many developer-written test cases can encode a reusable Metamorphic Relation (MR), over 70\\% of them directly hard-code the source input and follow-up input in the encoded relation. Such encoded MRs, which do not contain an explicit input transformation to transform the source inputs to corresponding follow-up inputs, cannot be reused with new source inputs to enhance test adequacy.In this paper, we propose MR-Adopt (Automatic Deduction Of inPut Transformation) to automatically deduce the input transformation from the hard-coded source and follow-up inputs, aiming to enable the encoded MRs to be reused with new source inputs. With typically only one pair of source and follow-up inputs available in an MR-encoded test case as the example, we leveraged LLMs to understand the intention of the test case and generate additional examples of source-followup input pairs. This helps to guide the generation of input transformations generalizable to multiple source inputs. Besides, to mitigate the issue that LLMs generate erroneous code, we refine LLM-generated transformations by removing MR-irrelevant code elements with data-flow analysis. Finally, we assess candidate transformations based on encoded output relations and select the best transformation as the result. Evaluation results show that MR-Adopt can generate input transformations applicable to all experimental source inputs for 72.00\\% of encoded MRs, which is 33.33\\% more than using vanilla GPT-3.5. By incorporating MR-Adopt-generated input transformations, encoded MR-based test cases can effectively enhance the test adequacy, increasing the line coverage and mutation score by 10.62\\% and 18.91\\%, respectively.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "557\u2013569",
        "numpages": "13",
        "keywords": "software testing, metamorphic testing, metamorphic relation, input transformation, code generation, large language models",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "GraphCoder: Enhancing Repository-Level Code Completion via Coarse-to-fine Retrieval Based on Code Context Graph": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695054",
        "author": "Liu, Wei and Yu, Ailun and Zan, Daoguang and Shen, Bo and Zhang, Wei and Zhao, Haiyan and Jin, Zhi and Wang, Qianxiang",
        "title": "GraphCoder: Enhancing Repository-Level Code Completion via Coarse-to-fine Retrieval Based on Code Context Graph",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695054",
        "doi": "10.1145/3691620.3695054",
        "abstract": "The performance of repository-level code completion depends upon the effective leverage of both general and repository-specific knowledge. Despite the impressive capability of code LLMs in general code completion tasks, they often exhibit less satisfactory performance on repository-level completion due to the lack of repository-specific knowledge in these LLMs. To address this problem, we propose GraphCoder, a retrieval-augmented code completion framework that leverages LLMs' general code knowledge and the repository-specific knowledge via a graph-based retrieval-generation process. In particular, GraphCoder captures the context of completion target more accurately through code context graph (CCG) that consists of control-flow, data- and control-dependence between code statements, a more structured way to capture the completion target context than the sequence-based context used in existing retrieval-augmented approaches; based on CCG, GraphCoder further employs a coarse-to-fine retrieval process to locate context-similar code snippets with the completion target from the current repository. Experimental results demonstrate both the effectiveness and efficiency of GraphCoder: Compared to baseline retrieval-augmented methods, GraphCoder achieves higher exact match (EM) on average, with increases of +6.06 in code match and +6.23 in identifier match, while using less time and space.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "570\u2013581",
        "numpages": "12",
        "keywords": "code completion, large language model, retrieval augmented generation, code graphs",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695055",
        "author": "Wu, Cong and Chen, Jing and Wang, Ziwei and Liang, Ruichao and Du, Ruiying",
        "title": "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695055",
        "doi": "10.1145/3691620.3695055",
        "abstract": "Smart contracts, self-executing agreements directly encoded in code, are fundamental to blockchain technology, especially in decentralized finance (DeFi) and Web3. However, the rise of Ponzi schemes in smart contracts poses significant risks, leading to substantial financial losses and eroding trust in blockchain systems. Existing detection methods, such as PonziGuard, depend on large amounts of labeled data and struggle to identify unseen Ponzi schemes, limiting their reliability and generalizability. In contrast, we introduce PonziSleuth, the first LLM-driven approach for detecting Ponzi smart contracts, which requires no labeled training data. PonziSleuth utilizes advanced language understanding capabilities of LLMs to analyze smart contract source code through a novel two-step zero-shot chain-of-thought prompting technique. Our extensive evaluation on benchmark datasets and real-world contracts demonstrates that PonziSleuth delivers comparable, and often superior, performance without the extensive data requirements, achieving a balanced detection accuracy of 96.06\\% with GPT-3.5-turbo, 93.91\\% with LLAMA3, and 94.27\\% with Mistral. In real-world detection, PonziSleuth successfully identified 15 new Ponzi schemes from 4,597 contracts verified by Etherscan in March 2024, with a false negative rate of 0\\% and a false positive rate of 0.29\\%. These results highlight PonziSleuth's capability to detect diverse and novel Ponzi schemes, marking a significant advancement in leveraging LLMs for enhancing blockchain security and mitigating financial scams.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "582\u2013593",
        "numpages": "12",
        "keywords": "smart contracts, large language model, ponzi contracts",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "GlitchProber: Advancing Effective Detection and Mitigation of Glitch Tokens in Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695060",
        "author": "Zhang, Zhibo and Bai, Wuxia and Li, Yuxi and Meng, Mark Huasong and Wang, Kailong and Shi, Ling and Li, Li and Wang, Jun and Wang, Haoyu",
        "title": "GlitchProber: Advancing Effective Detection and Mitigation of Glitch Tokens in Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695060",
        "doi": "10.1145/3691620.3695060",
        "abstract": "Large language models (LLMs) have achieved unprecedented success in the field of natural language processing. However, the black-box nature of their internal mechanisms has brought many concerns about their trustworthiness and interpretability. Recent research has discovered a class of abnormal tokens in the model's vocabulary space and named them \"glitch tokens\". Those tokens, once included in the input, may induce the model to produce incorrect, irrelevant, or even harmful results, drastically undermining the reliability and practicality of LLMs.In this work, we aim to enhance the understanding of glitch tokens and propose techniques for their detection and mitigation. We first reveal the characteristic features induced by glitch tokens on LLMs, which are evidenced by significant deviations in the distributions of attention patterns and dynamic information from intermediate model layers. Based on the insights, we develop GlitchProber, a tool for efficient glitch token detection and mitigation. GlitchProber utilizes small-scale sampling, principal component analysis for accelerated feature extraction, and a simple classifier for efficient vocabulary screening. Taking one step further, GlitchProber rectifies abnormal model intermediate layer values to mitigate the destructive effects of glitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber demonstrates higher efficiency, precision, and recall compared to existing approaches, with an average F1 score of 0.86 and an average repair rate of 50.06\\%. GlitchProber unveils a novel path to address the challenges posed by glitch tokens and inspires future research toward more robust and interpretable LLMs. Our code is available at https://github.com/LLM-Integrity-Guard/GlitchProber.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "643\u2013655",
        "numpages": "13",
        "keywords": "LLM security, glitch token, LLM analysis",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695062",
        "author": "Liu, Fang and Liu, Zhenwei and Zhao, Qianhui and Jiang, Jing and Zhang, Li and Sun, Zian and Li, Ge and Li, Zhongqi and Ma, Yuchi",
        "title": "FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695062",
        "doi": "10.1145/3691620.3695062",
        "abstract": "Providing personalized and timely feedback for student's programming assignments is useful for programming education. Automated program repair (APR) techniques have been used to fix the bugs in programming assignments, where the Large Language Models (LLMs) based approaches have shown promising results. Given the growing complexity of identifying and fixing bugs in advanced programming assignments, current fine-tuning strategies for APR are inadequate in guiding the LLM to identify bugs and make accurate edits during the generative repair process. Furthermore, the autoregressive decoding approach employed by the LLM could potentially impede the efficiency of the repair, thereby hindering the ability to provide timely feedback. To tackle these challenges, we propose FastFixer, an efficient and effective approach for programming assignment repair. To assist the LLM in accurately identifying and repairing bugs, we first propose a novel repair-oriented fine-tuning strategy, aiming to enhance the LLM's attention towards learning how to generate the necessary patch and its associated context. Furthermore, to speed up the patch generation, we propose an inference acceleration approach that is specifically tailored for the program repair task. The evaluation results demonstrate that FastFixer obtains an overall improvement of 20.46\\% in assignment fixing when compared to the state-of-the-art baseline. Considering the repair efficiency, FastFixer achieves a remarkable inference speedup of 16.67\\texttimes{} compared to the autoregressive decoding algorithm.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "669\u2013680",
        "numpages": "12",
        "keywords": "automated program repair, large language models, programming education, inference acceleration",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "DroidCoder: Enhanced Android Code Completion with Context-Enriched Retrieval-Augmented Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695063",
        "author": "Yu, Xinran and Li, Chun and Pan, Minxue and Li, Xuandong",
        "title": "DroidCoder: Enhanced Android Code Completion with Context-Enriched Retrieval-Augmented Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695063",
        "doi": "10.1145/3691620.3695063",
        "abstract": "Android is the most popular mobile operating system. However, Android development requires extensive coding, especially for unique features such as lifecycle callbacks and UI widgets. Existing code completion methods typically utilize Retrieval-Augmented Generation (RAG) to provide contextual information for pre-trained code large language models (Code LLMs) to perform completion. Despite considerable progress in these methods, their effectiveness in Android development remains limited. This is because the features of Android development make it challenging for existing retrieval mechanisms to extract sufficient context effectively. In response, we propose DroidCoder, a novel Android code completion framework that employs Android development features and contextual information of code snippets to enrich RAG. It also incorporates a specifically designed loss function to fine-tune the model, enabling it to better utilize context-enhanced RAG for Android code completion. We evaluated our method on three base models and different types of applications, comparing it with two state-of-the-art code completion methods. The experimental results demonstrate that our method significantly outperforms the baselines at line-level and multi-line-level code completion and improves the quality of the completed code.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "681\u2013693",
        "numpages": "13",
        "keywords": "code completion, retrieval-augmented generation, Android",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695066",
        "author": "Li, Guochang and Zhi, Chen and Chen, Jialiang and Han, Junxiao and Deng, Shuiguang",
        "title": "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695066",
        "doi": "10.1145/3691620.3695066",
        "abstract": "Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that \"pre-training and fine-tuning\" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-Instruction, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-Instruction. The best fine-tuned model fixes 58\\% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that (IA)3 improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-Instruction, PEFT weights, and the fine-tuning code are publicly available as open-source resources.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "719\u2013731",
        "numpages": "13",
        "keywords": "automated program repair, parameter-effective fine-tuning, large language model, execution-based evaluation",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695068",
        "author": "Cui, Di and Wang, Jiaqi and Wang, Qiangqiang and Ji, Peng and Qiao, Minglang and Zhao, Yutong and Hu, Jingzhao and Wang, Luqiao and Li, Qingshan",
        "title": "Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695068",
        "doi": "10.1145/3691620.3695068",
        "abstract": "Methods implemented in incorrect classes will cause excessive reliance on other classes than their own, known as a typical code smell symptom: feature envy, which makes it difficult to maintain increased coupling between classes. Addressing this issue, several Move Method refactoring tools have been proposed, employing a two-phase process: identifying misplaced methods to move and appropriate classes to receive, and implementing the mechanics of refactoring. These tools traditionally use hard-coded metrics to measure correlations between movable methods and target classes and apply heuristic thresholds or trained classifiers to unearth refactoring opportunities. Yet, these approaches predominantly illuminate pairwise correlations between methods and classes while overlooking the complex and complicated dependencies binding multiple code entities within these methods/classes that are prevalent in real-world cases. This narrow focus can lead to refactoring suggestions that may diverge from developers' actual needs. To bridge this gap, our paper leverages the concept of inter-class code entity dependency hypergraph to model complicated dependency relationships involving multiple code entities within various methods/classes and proposes a hypergraph learning-based approach to suggest Move Method refactoring opportunities named HMove. We first construct inter-class code entity dependency hypergraphs from training samples and assign attributes to entities with a pre-trained code model. All the attributed hypergraphs are fed into a hybrid hypergraph neural network for training. Utilizing this trained neural network alongside a large language model, we construct a refactoring suggestion system. We trained HMove on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 27.8\\% in precision, 2.5\\% in recall, and 18.5\\% in f1-measure compared to 9 state-of-the-art refactoring tools, which is more useful for 68\\% of participants. The results also unveil practical suggestions and new insights that benefit existing feature envy-related refactoring techniques.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "745\u2013757",
        "numpages": "13",
        "keywords": "move method refactoring, hypergraph neural network",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "An Empirical Study to Evaluate AIGC Detectors on Code Content": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695468",
        "author": "JianWang and Liu, Shangqing and Xie, Xiaofei and Li, Yi",
        "title": "An Empirical Study to Evaluate AIGC Detectors on Code Content",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695468",
        "doi": "10.1145/3691620.3695468",
        "abstract": "Artificial Intelligence Generated Content (AIGC) has garnered considerable attention for its impressive performance, with Large Language Models (LLMs), like ChatGPT, emerging as a leading AIGC model that produces high-quality responses across various applications, including software development and maintenance. Despite its potential, the misuse of LLMs, especially in security and safety-critical domains, such as academic integrity and answering questions on Stack Overflow, poses significant concerns. Numerous AIGC detectors have been developed and evaluated on natural language data. However, their performance on code-related content generated by LLMs remains unexplored.To fill this gap, in this paper, we present an empirical study evaluating existing AIGC detectors in the software domain. We select three state-of-the-art LLMs, i.e., GPT-3.5, WizardCoder and CodeLlama, for machine-content generation. We further created a comprehensive dataset including 2.23M samples comprising code-related content for each model, encompassing popular software activities like Q&amp;A (150K), code summarization (1M), and code generation (1.1M). We evaluated thirteen AIGC detectors, comprising six commercial and seven open-source solutions, assessing their performance on this dataset. Our results indicate that AIGC detectors perform less on code-related data than natural language data. Fine-tuning can enhance detector performance, especially for content within the same domain; but generalization remains a challenge.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "844\u2013856",
        "numpages": "13",
        "keywords": "AIGC detection, code generation, large language model",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695470",
        "author": "Cao, Jialun and Chen, Zhiyong and Wu, Jiarong and Cheung, Shing-Chi and Xu, Chang",
        "title": "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695470",
        "doi": "10.1145/3691620.3695470",
        "abstract": "Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8\\% of benchmarks involve Python, while only 5 benchmarks involve Java, resulting in an insufficient understanding of LLMs' capability to generate Java code. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3\\% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills (e.g., variables, operators, and control structures), while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism). Considering the prevalence of these advanced features in real-world Java project development, constructing benchmarks to test LLMs on handling OOP features is necessary.To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92\\%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 48.24\\% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench. We also release a leaderboard and invite model developers to participate and test their models against JavaBench at https://java-bench.github.io/leaderboard.html.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "870\u2013882",
        "numpages": "13",
        "keywords": "large language model, program synthesis, object-oriented programming",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695480",
        "author": "Chen, Jiachi and Zhong, Qingyuan and Wang, Yanlin and Ning, Kaiwen and Liu, Yongkun and Xu, Zenan and Zhao, Zhe and Chen, Ting and Zheng, Zibin",
        "title": "RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695480",
        "doi": "10.1145/3691620.3695480",
        "abstract": "Warning: Please note that this article contains potential harmful or offensive content. This content is only for the evaluating and analysis of LLMs and does not imply any intention to promote criminal activities.The emergence of Large Language Models (LLMs) has significantly influenced various aspects of software development activities. Despite their benefits, LLMs also pose notable risks, including the potential to generate harmful content and being abused by malicious developers to create malicious code. Several previous studies have focused on the ability of LLMs to resist the generation of harmful content that violates human ethical standards, such as biased or offensive content. However, there is no research evaluating the ability of LLMs to resist malicious code generation. To fill this gap, we propose RMCBench, the first benchmark comprising 473 prompts designed to assess the ability of LLMs to resist malicious code generation. This benchmark employs two scenarios: a text-to-code scenario, where LLMs are prompted with descriptions to generate code, and a code-to-code scenario, where LLMs translate or complete existing malicious code. Based on RMCBench, we conduct an empirical study on the 11 representative LLMs to assess their ability to resist malicious code generation. Our findings indicate that current LLMs have a limited ability to resist malicious code generation with an average refusal rate of 40.36\\% in text-to-code scenario and 11.52\\% in code-to-code scenario. The average refusal rate of all LLMs in RMCBench is only 28.71\\%; ChatGPT-4 has a refusal rate of only 35.73\\%. We also analyze the factors that affect LLM's ability to resist malicious code generation and provide implications for developers to enhance model robustness.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "995\u20131006",
        "numpages": "12",
        "keywords": "large language models, malicious code, code generation",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "SpiderScan: Practical Detection of Malicious NPM Packages Based on Graph-Based Behavior Modeling and Matching": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695492",
        "author": "Huang, Yiheng and Wang, Ruisi and Zheng, Wen and Zhou, Zhuotong and Wu, Susheng and Ke, Shulin and Chen, Bihuan and Gao, Shan and Peng, Xin",
        "title": "SpiderScan: Practical Detection of Malicious NPM Packages Based on Graph-Based Behavior Modeling and Matching",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695492",
        "doi": "10.1145/3691620.3695492",
        "abstract": "Open source software (OSS) supply chains have been attractive targets for attacks. One of the significant, popular attacks is realized by malicious packages on package registries. NPM, as the largest package registry, has been recently flooded with malicious packages. In response to this severe security risk, many detection tools have been proposed. However, these tools do not model malicious behavior in a holistic way; only consider a predefined set of sensitive APIs; and require huge manual confirmation effort due to high false positives and binary detection results. Thus, their practical usefulness is hindered.To address these limitations, we propose a practical tool, named SpiderScan, to identify malicious NPM packages based on graph-based behavior modeling and matching. In the offline phase, given a set of malicious packages, SpiderScan models each malicious behavior in a graph that considers control flows and data dependencies across sensitive API calls, while leveraging LLM to recognize sensitive APIs in both built-in modules and third-party dependencies. In the online phase, given a target package, SpiderScan constructs its suspicious behavior graphs and matches them with malicious behavior graphs, and uses dynamic analysis and LLM to confirm the maliciousness only for certain malicious behaviors. Our extensive evaluation has demonstrated the effectiveness of SpiderScan over the state-of-the-art. SpiderScan has detected 249 new malicious packages in NPM, and received 70 thank letters from the official team of NPM.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1146\u20131158",
        "numpages": "13",
        "keywords": "software supply chain, malware detection, behavior modeling",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Contextualized Data-Wrangling Code Generation in Computational Notebooks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695503",
        "author": "Huang, Junjie and Guo, Daya and Wang, Chenglong and Gu, Jiazhen and Lu, Shuai and Inala, Jeevana Priya and Yan, Cong and Gao, Jianfeng and Duan, Nan and Lyu, Michael R.",
        "title": "Contextualized Data-Wrangling Code Generation in Computational Notebooks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695503",
        "doi": "10.1145/3691620.3695503",
        "abstract": "Data wrangling, the process of preparing raw data for further analysis in computational notebooks, is a crucial yet time-consuming step in data science. Code generation has the potential to automate the data wrangling process to reduce analysts' overhead by translating user intents into executable code. Precisely generating data wrangling code necessitates a comprehensive consideration of the rich context present in notebooks, including textual context, code context and data context. However, notebooks often interleave multiple non-linear analysis tasks into linear sequence of code blocks, where the contextual dependencies are not clearly reflected. Directly training models with source code blocks fails to fully exploit the contexts for accurate wrangling code generation.To bridge the gap, we aim to construct a high quality datasets with clear and rich contexts to help training models for data wrangling code generation tasks. In this work, we first propose an automated approach, CoCoMine to mine data-wrangling code generation examples with clear multi-modal contextual dependency. It first adopts data flow analysis to identify the code blocks containing data wrangling codes. Then, CoCoMine extracts the contextualized data-wrangling code examples through tracing and replaying notebooks. With CoCoMine, we construct CoCoNote, a dataset containing 58,221 examples for Contextualized Data-wrangling Code generation in Notebooks. To demonstrate the effectiveness of our dataset, we finetune a range of pretrained code models and prompt various large language models on our task. Furthermore, we also propose Data-Coder, which encodes data context and code&amp;textual contexts separately to enhance code generation. Experiment results demonstrate the significance of incorporating data context in data-wrangling code generation and the effectiveness of our model. We release code and data at https://github.com/Jun-jie-Huang/CoCoNote.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1282\u20131294",
        "numpages": "13",
        "keywords": "code generation, data wrangling, computational notebooks, large language models",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Dynamic Scoring Code Token Tree: A Novel Decoding Strategy for Generating High-Performance Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695505",
        "author": "Qu, Muzi and Liu, Jie and Kang, Liangyi and Wang, Shuai and Ye, Dan and Huang, Tao",
        "title": "Dynamic Scoring Code Token Tree: A Novel Decoding Strategy for Generating High-Performance Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695505",
        "doi": "10.1145/3691620.3695505",
        "abstract": "Within the realms of scientific computing, large-scale data processing, and artificial intelligence-powered computation, disparities in performance, which originate from differing code implementations, directly influence the practicality of the code. Although existing works tried to utilize code knowledge to enhance the execution performance of codes generated by large language models, they neglect code evaluation outcomes which directly refer to the code execution details, resulting in inefficient computation. To address this issue, we propose DSCT-Decode, an innovative adaptive decoding strategy for large language models, that employs a data structure named 'Code Token Tree' (CTT), which guides token selection based on code evaluation outcomes. DSCT-Decode assesses generated code across three dimensions---correctness, performance, and similarity---and utilizes a dynamic penalty-based boundary intersection method to compute multi-objective scores, which are then used to adjust the scores of nodes in the CTT during backpropagation. By maintaining a balance between exploration, through token selection probabilities, and exploitation, through multi-objective scoring, DSCT-Decode effectively navigates the code space to swiftly identify high-performance code solutions. To substantiate our framework, we developed a new benchmark, big-DS-1000, which is an extension of DS-1000. This benchmark is the first of its kind to specifically evaluate code generation methods based on execution performance. Comparative evaluations with leading large language models, such as CodeLlama and GPT-4, show that our framework achieves an average performance enhancement of nearly 30\\%. Furthermore, 30\\% of the codes exhibited a performance improvement of more than 20\\%, underscoring the effectiveness and potential of our framework for practical applications.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1308\u20131318",
        "numpages": "11",
        "keywords": "code generation, large language model, performance optimization",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695506",
        "author": "Zhang, Huan and Cheng, Wei and Wu, Yuhan and Hu, Wei",
        "title": "A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695506",
        "doi": "10.1145/3691620.3695506",
        "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation. The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback. The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement. This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers. We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks. Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00\\%--162.43\\% compared to prompting LLMs directly.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1319\u20131331",
        "numpages": "13",
        "keywords": "code generation, large language model, agent, pair programming",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "iSMELL: Assembling LLMs with Expert Toolsets for Code Smell Detection and Refactoring": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695508",
        "author": "Wu, Di and Mu, Fangwen and Shi, Lin and Guo, Zhaoqiang and Liu, Kui and Zhuang, Weiguang and Zhong, Yuqi and Zhang, Li",
        "title": "iSMELL: Assembling LLMs with Expert Toolsets for Code Smell Detection and Refactoring",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695508",
        "doi": "10.1145/3691620.3695508",
        "abstract": "Detecting and refactoring code smells is challenging, laborious, and sustaining. Although large language models have demonstrated potential in identifying various types of code smells, they also have limitations such as input-output token restrictions, difficulty in accessing repository-level knowledge, and performing dynamic source code analysis. Existing learning-based methods or commercial expert toolsets have advantages in handling complex smells. They can analyze project structures and contextual information in-depth, access global code repositories, and utilize advanced code analysis techniques. However, these toolsets are often designed for specific types and patterns of code smells and can only address fixed smells, lacking flexibility and scalability. To resolve that problem, we propose iSMELL, an ensemble approach that employs various code smell detection toolsets via Mixture of Experts (MoE) architecture for comprehensive code smell detection, and enhances the LLMs with the detection results from expert toolsets for refactoring those identified code smells. First, we train a MoE model that, based on input code vectors, outputs the most suitable expert tool for identifying each type of smell. Then, we select the recommended toolsets for code smell detection and obtain their results. Finally, we equip the prompts with the detection results from the expert toolsets, thereby enhancing the refactoring capability of LLMs for code with existing smells, enabling them to provide different solutions based on the type of smell. We evaluate our approach on detecting and refactoring three classical and complex code smells, i.e., Refused Bequest, God Class, and Feature Envy. The results show that, by adopting seven expert code smell toolsets, iSMELL achieved an average F1 score of 75.17\\% on code smell detection, outperforming LLMs baselines by an increase of 35.05\\% in F1 score. We further evaluate the code refactored by the enhanced LLM. The quantitative and human evaluation results show that iSMELL could improve code quality metrics and conduct satisfactory refactoring toward the identified code smells. We believe that our proposed solution could provide new insights into better leveraging LLMs and existing approaches to resolving complex software tasks.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1345\u20131357",
        "numpages": "13",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695510",
        "author": "Yan, Chuan and Ren, Ruomai and Meng, Mark Huasong and Wan, Liuhuo and Ooi, Tian Yang and Bai, Guangdong",
        "title": "Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695510",
        "doi": "10.1145/3691620.3695510",
        "abstract": "ChatGPT has enabled third-party developers to create plugins to expand ChatGPT's capabilities. These plugins are distributed through OpenAI's plugin store, making them easily accessible to users. With ChatGPT as the backbone, this app ecosystem has illustrated great business potential by offering users personalized services in a conversational manner. Nonetheless, many crucial aspects regarding app development, deployment, and security of this ecosystem have yet to be thoroughly studied in the research community, potentially hindering a broader adoption by both developers and users. In this work, we conduct the first comprehensive study of the ChatGPT app ecosystem, aiming to illuminate its landscape for our research community. Our study examines the distribution and deployment models in the integration of LLMs and third-party apps, and assesses their security and privacy implications. We uncover an uneven distribution of functionality among ChatGPT plugins, highlighting prevalent and emerging topics. We also identify severe flaws in the authentication and user data protection for third-party app APIs integrated within LLMs, revealing a concerning status quo of security and privacy in this app ecosystem. Our work provides insights for the secure and sustainable development of this rapidly evolving ecosystem.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1370\u20131382",
        "numpages": "13",
        "keywords": "large language model, testing, security, deployment",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "LLM-Generated Invariants for Bounded Model Checking Without Loop Unrolling": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695512",
        "author": "Pirzada, Muhammad A. A. and Reger, Giles and Bhayat, Ahmed and Cordeiro, Lucas C.",
        "title": "LLM-Generated Invariants for Bounded Model Checking Without Loop Unrolling",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695512",
        "doi": "10.1145/3691620.3695512",
        "abstract": "We investigate a modification of the classical Bounded Model Checking (BMC) procedure that does not handle loops through unrolling but via modifications to the control flow graph (CFG). A portion of the CFG representing a loop is replaced by a node asserting invariants of the loop. We generate these invariants using Large Language Models (LLMs) and use a first-order theorem prover to ensure the correctness of the generated statements. We thus transform programs to loop-free variants in a sound manner. Our experimental results show that the resulting tool, ESBMC ibmc, is competitive with state-of-the-art formal verifiers for programs with unbounded loops, significantly improving the number of programs verified by the industrial-strength software verifier ESBMC and verifying programs that state-of-the-art software verifiers such as SeaHorn and VeriAbs could not.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1395\u20131407",
        "numpages": "13",
        "keywords": "program verification, large language models, bounded model checking, invariant generation",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Semi-Supervised Code Translation Overcoming the Scarcity of Parallel Code Data": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695524",
        "author": "Zhu, Ming and Karim, Mohimenul and Lourentzou, Ismini and Yao, Daphne",
        "title": "Semi-Supervised Code Translation Overcoming the Scarcity of Parallel Code Data",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695524",
        "doi": "10.1145/3691620.3695524",
        "abstract": "Neural code translation is the task of converting source code from one programming language to another. One of the main challenges is the scarcity of parallel code data, which hinders the ability of translation models to learn accurate cross-language alignments. In this paper, we introduce MIRACLE, a semi-supervised approach that improves code translation through synthesizing high-quality parallel code data and curriculum learning on code data with ascending alignment levels. MIRACLE leverages static analysis and compilation to generate synthetic parallel code datasets with enhanced quality and alignment to address the challenge of data scarcity. We evaluate the proposed method along with strong baselines including instruction-tuned Large Language Models (LLMs) for code. Our analysis reveals that LLMs pre-trained on open-source code data, regardless of their size, suffer from the \"shallow translation\" problem. This issue arises when translated code copies keywords, statements, and even code blocks from the source language, leading to compilation and runtime errors. Extensive experiments demonstrate that our method significantly mitigates this issue, enhancing code translation performance across multiple models in C++, Java, Python, and C. Remarkably, MIRACLE outperforms code LLMs that are ten times larger in size. MIRACLE also achieves up to a 43\\% improvement in C code translation with fewer than 150 annotated examples.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1545\u20131556",
        "numpages": "12",
        "keywords": "neural code translation, cross-language code alignment, semi-supervised learning, curriculum learning, static analysis",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Test-Driven Development and LLM-based Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695527",
        "author": "Mathews, Noble Saji and Nagappan, Meiyappan",
        "title": "Test-Driven Development and LLM-based Code Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695527",
        "doi": "10.1145/3691620.3695527",
        "abstract": "Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1583\u20131594",
        "numpages": "12",
        "keywords": "code generation, LLM, TDD, testing, software engineering",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "On the Evaluation of Large Language Models in Unit Test Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695529",
        "author": "Yang, Lin and Yang, Chen and Gao, Shutao and Wang, Weijing and Wang, Bo and Zhu, Qihao and Chu, Xiao and Zhou, Jianyi and Liang, Guangtai and Wang, Qianxiang and Chen, Junjie",
        "title": "On the Evaluation of Large Language Models in Unit Test Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695529",
        "doi": "10.1145/3691620.3695529",
        "abstract": "Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs' capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1607\u20131619",
        "numpages": "13",
        "keywords": "large language model, unit test generation, empirical study",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Magneto: A Step-Wise Approach to Exploit Vulnerabilities in Dependent Libraries via LLM-Empowered Directed Fuzzing": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695531",
        "author": "Zhou, Zhuotong and Yang, Yongzhuo and Wu, Susheng and Huang, Yiheng and Chen, Bihuan and Peng, Xin",
        "title": "Magneto: A Step-Wise Approach to Exploit Vulnerabilities in Dependent Libraries via LLM-Empowered Directed Fuzzing",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695531",
        "doi": "10.1145/3691620.3695531",
        "abstract": "The wide adoption of open source third-party libraries can propagate vulnerabilities that originally exist in third-party libraries through dependency chains to downstream projects. To mitigate this security risk, vulnerability exploitation analysis has been proposed to further reduce false positives of vulnerability reachability analysis. However, existing approaches work less effectively when the vulnerable function of the vulnerable library is indirectly invoked by a client project through a call chain of multiple steps.To address this problem, we propose a step-wise approach, named Magneto, to exploit vulnerabilities in dependent libraries of a client project through LLM-empowered directed fuzzing. Its core idea is to decompose the directed fuzzing for the whole call chain (from the client project to the vulnerable function) into a series of step-wise directed fuzzing for each step of the call chain. To empower directed fuzzing, it leverages LLM to facilitate the initial seed generation. Our evaluation has demonstrated the effectiveness of Magneto over the state-of-the-art; i.e., Magneto achieves an improvement of at least 75.6\\% in successfully exploiting the vulnerability.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1633\u20131644",
        "numpages": "12",
        "keywords": "library vulnerabilities, exploit generation, directed fuzzing",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695536",
        "author": "Chen, Mouxiang and Liu, Zhongxin and Tao, He and Hong, Yusu and Lo, David and Xia, Xin and Sun, Jianling",
        "title": "B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695536",
        "doi": "10.1145/3691620.3695536",
        "abstract": "Selecting the best code solution from multiple generated ones is an essential task in code generation, which can be achieved by using some reliable validators (e.g., developer-written test cases) for assistance. Since reliable test cases are not always available and can be expensive to build in practice, researchers propose to automatically generate test cases to assess code solutions. However, when both code solutions and test cases are plausible and not reliable, selecting the best solution becomes challenging. Although some heuristic strategies have been proposed to tackle this problem, they lack a strong theoretical guarantee and it is still an open question whether an optimal selection strategy exists. Our work contributes in two ways. First, we show that within a Bayesian framework, the optimal selection strategy can be defined based on the posterior probability of the observed passing states between solutions and tests. The problem of identifying the best solution is then framed as an integer programming problem. Second, we propose an efficient approach for approximating this optimal (yet uncomputable) strategy, where the approximation error is bounded by the correctness of prior knowledge. We then incorporate effective prior knowledge to tailor code generation tasks. Both theoretical and empirical studies confirm that existing heuristics are limited in selecting the best solutions with plausible test cases. Our proposed approximated optimal strategy \u212c4 significantly surpasses existing heuristics in selecting code solutions generated by large language models (LLMs) with LLM-generated tests, achieving a relative performance improvement by up to 50\\% over the strongest heuristic and 246\\% over the random selection in the most challenging scenarios. Our code is publicly available at https://github.com/ZJU-CTAG/B4.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1693\u20131705",
        "numpages": "13",
        "keywords": "code generation, software engineering, large language models",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695552",
        "author": "Feng, Jia and Liu, Jiachen and Gao, Cuiyun and Chong, Chun Yong and Wang, Chaozheng and Gao, Shan and Xia, Xin",
        "title": "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695552",
        "doi": "10.1145/3691620.3695552",
        "abstract": "In recent years, with the widespread attention of academia and industry on the application of large language models (LLMs) to code-related tasks, an increasing number of large code models (LCMs) have been proposed and corresponding evaluation benchmarks have continually emerged. Although existing evaluation benchmarks are helpful for comparing different LCMs, they may not reflect the performance of LCMs in various development scenarios. Specifically, they might evaluate model performance in only one type of scenario (e.g., code generation or code completion), whereas real development contexts are diverse and may involve multiple tasks such as code generation, code completion, API recommendation, and test function generation. Additionally, the questions may not originate from actual development practices, failing to capture the programming challenges faced by developers during the development process.To address the aforementioned issues, we propose Complex-CodeEval, a new benchmark for evaluating the performance of LCMs in various development scenarios. ComplexCodeEval includes 3,897 Java samples from 1,055 high-star GitHub repositories and 7,184 Python samples from 2,107 high-star repositories. Each function sample in ComplexCodeEval contains multiple annotations (e.g., function signatures, docstrings and reference APIs) to accommodate various downstream tasks. Furthermore, to better reflect diverse development scenarios, each function sample is required to originate from a repository that depends on at least one selected library (based on popularity), and each function sample must invoke at least one API from the selected library. Additionally, each function sample has multiple timestamps to avoid data leakage. Based on ComplexCodeEval, we evaluate the performance of ten LCMs across four tasks (i.e., code generation, code completion, API recommendation, and test case generation) to explore their performance in complex development environments. Furthermore, we conduct an in-depth analysis of the impact of context and data leakage on model performance. Our experimental results reveal several key findings. For instance, LCMs exhibit varying performance across different coding tasks. Additionally, rich contextual information can greatly enhance the performance of LCMs. Moreover, using leaked data for evaluation may lead to an overestimation of model performance, resulting in inaccurate evaluation outcomes that deviate from the performance in practice.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1895\u20131906",
        "numpages": "12",
        "keywords": "large language models, code intelligence, benchmark",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Understanding Developer-Analyzer Interactions in Code Reviews": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695257",
        "author": "Schaef, Martin and Cirisci, Berk and Luo, Linghui and Mansur, Muhammad Numair and Tripp, Omer and Sanchez, Daniel and Zhou, Qiang and Zafar, Muhammad Bilal",
        "title": "Understanding Developer-Analyzer Interactions in Code Reviews",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695257",
        "doi": "10.1145/3691620.3695257",
        "abstract": "Static code analyzers are now a common part of the codereview process. These automated tools integrate into the code review process by commenting on code changes and suggesting improvements, in the same way as human reviewers. The comments made by static analyzers often trigger a conversation between developers to align on if and how the issue should be fixed. Because developers rarely give feedback directly to the tool, understanding the sentiment and intent in the conversation triggered by the tool comments can be used to measure the usefulness of the static analyzer.In this paper, we report on an experiment where we use large language models to automatically label and categorize the sentiment and intent of such conversations triggered by static analyzer comments. Our experiment demonstrates that LLMs not only classify and interpret complex developer-analyzer conversations, but can be more accurate than human experts.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1945\u20131955",
        "numpages": "11",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "AutoDW: Automatic Data Wrangling Leveraging Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695267",
        "author": "Liu, Lei and Hasegawa, So and Sampat, Shailaja Keyur and Xenochristou, Maria and Chen, Wei-Peng and Kato, Takashi and Kakibuchi, Taisei and Asai, Tatsuya",
        "title": "AutoDW: Automatic Data Wrangling Leveraging Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695267",
        "doi": "10.1145/3691620.3695267",
        "abstract": "Data wrangling is a critical yet often labor-intensive process, essential for transforming raw data into formats suitable for downstream tasks such as machine learning or data analysis. Traditional data wrangling methods can be time-consuming, resource-intensive, and prone to errors, limiting the efficiency and effectiveness of subsequent downstream tasks. In this paper, we introduce AutoDW: an end-to-end solution for automatic data wrangling that leverages the power of Large Language Models (LLMs) to enhance automation and intelligence in data preparation. AutoDW distinguishes itself through several innovative features, including comprehensive automation that minimizes human intervention, the integration of LLMs to enable advanced data processing capabilities, and the generation of source code for the entire wrangling process, ensuring transparency and reproducibility. These advancements position AuoDW as a superior alternative to existing data wrangling tools, offering significant improvements in efficiency, accuracy, and flexibility. Through detailed performance evaluations, we demonstrate the effectiveness of AutoDW for data wrangling. We also discuss our experience and lessons learned from the industrial deployment of AutoDW, showcasing its potential to transform the landscape of automated data preparation.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2041\u20132052",
        "numpages": "12",
        "keywords": "data wrangling, machine learning, large language models",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Ansible Lightspeed: A Code Generation Service for IT Automation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695277",
        "author": "Sahoo, Priyam and Pujar, Saurabh and Nalawade, Ganesh and Genhardt, Richard and Mandel, Louis and Buratti, Luca",
        "title": "Ansible Lightspeed: A Code Generation Service for IT Automation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695277",
        "doi": "10.1145/3691620.3695277",
        "abstract": "The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for Information Technology (IT) automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Ansible Lightspeed is an LLM-based service designed explicitly to generate Ansible YAML, given natural language prompt.In this paper, we present the design and implementation of the Ansible Lightspeed service. We then evaluate its utility to developers using diverse indicators, including extended utilization, analysis of user edited suggestions, as well as user sentiments analysis. The evaluation is based on data collected for 10,696 real users including 3,910 returning users. The code for Ansible Lightspeed service and the analysis framework is made available for others to use.To our knowledge, our study is the first to involve thousands of users of code assistants for domain-specific languages. We are also the first code completion tool to present N-Day user retention figures, which is 13.66\\% on Day 30. We propose an improved version of user acceptance rate, called Strong Acceptance rate, where a suggestion is considered accepted only if less than 50\\% of it is edited and these edits do not change critical parts of the suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong acceptance rate of 49.08\\% for multi-line Ansible task suggestions. With our findings we provide insights into the effectiveness of small, dedicated models in a domain-specific context. We hope this work serves as a reference for software engineering and machine learning researchers exploring code completion.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2148\u20132158",
        "numpages": "11",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Towards Leveraging LLMs for Reducing Open Source Onboarding Information Overload": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695286",
        "author": "Adejumo, Elijah Kayode and Johnson, Brittany",
        "title": "Towards Leveraging LLMs for Reducing Open Source Onboarding Information Overload",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695286",
        "doi": "10.1145/3691620.3695286",
        "abstract": "Consistent, diverse, and quality contributions are essential to the sustainability of the open source community. Therefore, it is important that there is infrastructure for effectively onboarding and retaining diverse newcomers to open source software projects. Most often, open source projects rely on onboarding documentation to support newcomers in making their first contributions. Unfortunately, prior studies suggest that information overload from available documentation, along with the predominantly monolingual nature of repositories, can have negative effects on the newcomer experiences and onboarding process. This, coupled with the effort involved in creating and maintaining onboarding documentation, suggest a need for support in creating more accessible documentation. Large language models (LLMs) have shown great potential in providing text transformation support in other domains, and even shown promise in simplifying or generating other kinds of computing artifacts, such as source code and technical documentation. We contend that LLMs can also help make software onboarding documentation more accessible, thereby reducing the potential for information overload. Using ChatGPT (GPT-3.5 Turbo) and Gemini Pro as case studies, we assessed the effectiveness of LLMs for simplifying software onboarding documentation, one method for reducing information overload. We discuss a broader vision for using LLMs to support the creation of more accessible documentation and outline future research directions toward this vision.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2210\u20132214",
        "numpages": "5",
        "keywords": "open-source, software, on-boarding, generative AI, documentation, ChatGPT, LLMs",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695290",
        "author": "Zhang, Beiqi and Liang, Peng and Feng, Qiong and Fu, Yujia and Li, Zengyang",
        "title": "Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695290",
        "doi": "10.1145/3691620.3695290",
        "abstract": "As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released in September 2023, functions as an interactive tool aimed at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot Chat's ability to fix the code smells. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot Chat in fixing these code smells employing different prompts. The results show that 8 out of 10 types of code smells can be detected in Copilot-generated Python code, among which Multiply-Nested Container is the most common one. For these code smells, Copilot Chat achieves a highest fixing rate of 87.1\\%, showing promise in fixing Python code smells generated by Copilot itself. In addition, the effectiveness of Copilot Chat in fixing these smells can be improved by providing more detailed prompts.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2230\u20132234",
        "numpages": "5",
        "keywords": "code smell, code quality, code refactoring, GitHub copilot",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer Recommendation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695291",
        "author": "Wang, Luqiao and Zhou, Yangtao and Zhuang, Huiying and Li, Qingshan and Cui, Di and Zhao, Yutong and Wang, Lu",
        "title": "Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer Recommendation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695291",
        "doi": "10.1145/3691620.3695291",
        "abstract": "Assigning pull requests to appropriate code reviewers can accelerate the review process and help uncover potential bugs. However, the inherent complexities in pull requests and code reviewers present challenges in making suitable matches between them. Prior studies focus on mining rich semantic information from pull requests or profile information from code reviewers to improve efficiency. These approaches often overlook the intrinsic relationships between pull requests and code reviewers, which can be represented by a combination of multiple factors and strategies, resulting in suboptimal recommendation accuracy.To address this issue, we propose CoRe, a collaborative agent-based code reviewer recommendation approach that emphasizes flexibility and adaptability. We leverage Large Language Models (LLMs) to precisely capture the rich textual semantics of both pull requests and reviewers. Additionally, we integrate various factors into the recommendation process through the robust planning, collaboration, and decision-making capabilities of multi-agent systems. This integration significantly enhances the performance of LLM-based code reviewer recommendations. We evaluate the effectiveness of our approach on four widely used projects. The results demonstrate that CoRe outperforms state-of-the-art methods in both performance and interpretability.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2235\u20132239",
        "numpages": "5",
        "keywords": "code reviewer recommendation, large language model",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "CoDefeater: Using LLMs To Find Defeaters in Assurance Cases": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695296",
        "author": "Gohar, Usman and Hunter, Michael C. and Lutz, Robyn R. and Cohen, Myra B.",
        "title": "CoDefeater: Using LLMs To Find Defeaters in Assurance Cases",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695296",
        "doi": "10.1145/3691620.3695296",
        "abstract": "Constructing assurance cases is a widely used and sometimes required process toward demonstrating that safety-critical systems will operate safely in their planned environment. To mitigate the risk of errors and missing edge cases, the concept of defeaters - challenges to claims in an assurance case - has been introduced. Defeaters can detect weaknesses in the arguments, prompting further investigation and timely mitigations. However, capturing defeaters relies on expert judgment, experience, and creativity and must be done iteratively due to evolving requirements and regulations. In this paper, we propose CoDefeater, an automated process to leverage large language models (LLMs) for finding defeaters. Initial results on two systems show that LLMs can efficiently find known and unforeseen feasible defeaters to support safety analysts in enhancing the completeness and confidence of assurance cases.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2262\u20132267",
        "numpages": "6",
        "keywords": "assurance case, large language models, assurance defeaters, sUAS",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Attacks and Defenses for Large Language Models on Coding Tasks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695297",
        "author": "Zhang, Chi and Wang, Zifan and Zhao, Ruoshi and Mangal, Ravi and Fredrikson, Matt and Jia, Limin and Pasareanu, Corina",
        "title": "Attacks and Defenses for Large Language Models on Coding Tasks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695297",
        "doi": "10.1145/3691620.3695297",
        "abstract": "Modern large language models (LLMs), such as ChatGPT, have demonstrated impressive capabilities for coding tasks, including writing and reasoning about code. They improve upon previous neural network models of code, such as code2seq or seq2seq, that already demonstrated competitive results when performing tasks such as code summarization and identifying code vulnerabilities. However, these previous code models were shown vulnerable to adversarial examples, i.e., small syntactic perturbations designed to \"fool\" the models. In this paper, we first aim to study the transferability of adversarial examples, generated through white-box attacks on smaller code models, to LLMs. We also propose a new attack using an LLM to generate the perturbations. Further, we propose novel cost-effective techniques to defend LLMs against such adversaries via prompting, without incurring the cost of retraining. These prompt-based defenses involve modifying the prompt to include additional information, such as examples of adversarially perturbed code and explicit instructions for reversing adversarial perturbations. Our preliminary experiments show the effectiveness of the attacks and the proposed defenses on popular LLMs such as GPT-3.5 and GPT-4.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2268\u20132272",
        "numpages": "5",
        "keywords": "LLMs, code models, adversarial attacks, robustness",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "RepoSim: Evaluating Prompt Strategies for Code Completion via User Behavior Simulation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695299",
        "author": "Peng, Chao and Wu, Qinyun and Liu, Jiangchao and Liu, Jierui and Jiang, Bo and Xu, Mengqian and Wang, Yinghao and Liu, Xia and Yang, Ping",
        "title": "RepoSim: Evaluating Prompt Strategies for Code Completion via User Behavior Simulation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695299",
        "doi": "10.1145/3691620.3695299",
        "abstract": "Large language models (LLMs) have revolutionized code completion tasks. IDE plugins such as MarsCode can generate code recommendations, saving developers significant time and effort. However, current evaluation methods for code completion are limited by their reliance on static code benchmarks, which do not consider human interactions and evolving repositories. This paper proposes RepoSim, a novel benchmark designed to evaluate code completion tasks by simulating the evolving process of repositories and incorporating user behaviors. RepoSim leverages data from an IDE plugin, by recording and replaying user behaviors to provide a realistic programming context for evaluation. This allows for the assessment of more complex prompt strategies, such as utilizing recently visited files and incorporating user editing history. Additionally, RepoSim proposes a new metric based on users' acceptance or rejection of predictions, offering a user-centric evaluation criterion. Our preliminary evaluation demonstrates that incorporating users' recent edit history into prompts significantly improves the quality of LLM-generated code, highlighting the importance of temporal context in code completion. RepoSim represents a significant advancement in benchmarking tools, offering a realistic and user-focused framework for evaluating code completion performance.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2279\u20132283",
        "numpages": "5",
        "keywords": "code completion, prompt engineering, benchmark, large language model",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "The Software Genome Project: Unraveling Software Through Genetic Principles": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695307",
        "author": "Wu, Yueming and Liu, Chengwei and Xu, Zhengzi and Zhang, Lyuye and Zhang, Yiran and Zhu, Zhiling and Liu, Yang",
        "title": "The Software Genome Project: Unraveling Software Through Genetic Principles",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695307",
        "doi": "10.1145/3691620.3695307",
        "abstract": "Open-source software is crucial to modern development, but its complexity creates challenges in quality, security, and management. Current governance approaches excel at collaboration but struggle with decentralized management and security. With the rise of large language models (LLM)-based software engineering, the need for a finer-grained understanding of software composition is more urgent than ever. To address these challenges, inspired by the Human Genome Project, we treat the software source code as software DNA and propose the Software Genome Project (SGP), which is geared towards the secure monitoring and exploitation of open-source software. By identifying and labeling integrated and classified code features at a fine-grained level, and effectively identifying safeguards for functional implementations and nonfunctional requirements at different levels of granularity, the SGP could build a comprehensive set of software genome maps to help developers and managers gain a deeper understanding of software complexity and diversity. By dissecting and summarizing functional and undesirable genes, SGP could help facilitate targeted software optimization, provide valuable insight and understanding of the entire software ecosystem, and support critical development tasks such as open source governance. SGP could also serve as a comprehensive dataset with abundant semantic labeling to enhance the training of LLMs for code. Based on these, we expect SGP to drive the evolution of software development towards more efficient, reliable, and sustainable software solutions.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2319\u20132323",
        "numpages": "5",
        "keywords": "software genes, software composition, OSS governance",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "PACGBI: A Pipeline for Automated Code Generation from Backlog Items": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695346",
        "author": "Sarschar, Mahja and Zhang, Gefei and Nowak, Annika",
        "title": "PACGBI: A Pipeline for Automated Code Generation from Backlog Items",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695346",
        "doi": "10.1145/3691620.3695346",
        "abstract": "While there exist several tools to leverage Large Language Models (LLMs) for code generation, their capabilities are limited to the source code editor and are disconnected from the overall software development process. These tools typically generate standalone code snippets that still require manual integration into the codebase. There is still a lack of integrated solutions that seamlessly automate the entire development cycle, from backlog items to code generation and merge requests. We present the Pipeline for Automated Code Generation from Backlog Items (PACGBI), an LLM-assisted pipeline integrated into GitLab CI. PACGBI reads backlog items in the code repository, automatically generates the corresponding code, and creates merge requests for the generated changes. Our case study demonstrates the potential of PACGBI in automating agile software development processes, allowing parallelization of development and reduction of development costs. PACGBI can be utilized by software developers and enables nontechnical stakeholders and designers by providing a holistic solution for using LLMs in software development. A screencast of this tool is available at https://youtu.be/TI53m-fIoyc, its source code at https://github.com/Masa-99/pacgbi.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2338\u20132341",
        "numpages": "4",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695349",
        "author": "Wang, Che and Zhang, Jiashuo and Gao, Jianbo and Xia, Libin and Guan, Zhi and Chen, Zhong",
        "title": "ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695349",
        "doi": "10.1145/3691620.3695349",
        "abstract": "Smart contracts are susceptible to being exploited by attackers, especially when facing real-world vulnerabilities. To mitigate this risk, developers often rely on third-party audit services to identify potential vulnerabilities before project deployment. Nevertheless, repairing the identified vulnerabilities is still complex and laborintensive, particularly for developers lacking security expertise. Moreover, existing pattern-based repair tools mostly fail to address real-world vulnerabilities due to their lack of high-level semantic understanding. To fill this gap, we propose ContractTinker, a Large Language Models (LLMs)-empowered tool for real-world vulnerability repair. The key insight is our adoption of the Chain-of-Thought approach to break down the entire generation task into subtasks. Additionally, to reduce hallucination, we integrate program static analysis to guide the LLM. We evaluate ContractTinker on 48 high-risk vulnerabilities. The experimental results show that among the patches generated by ContractTinker, 23 (48\\%) are valid patches that fix the vulnerabilities, while 10 (21\\%) require only minor modifications. A video of ContractTinker is available at https://youtu.be/HWFVi-YHcPE.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2350\u20132353",
        "numpages": "4",
        "keywords": "program repair, smart contract, large language model",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "CoqPilot, a plugin for LLM-based generation of proofs": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695357",
        "author": "Kozyrev, Andrei and Solovev, Gleb and Khramov, Nikita and Podkopaev, Anton",
        "title": "CoqPilot, a plugin for LLM-based generation of proofs",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695357",
        "doi": "10.1145/3691620.3695357",
        "abstract": "We present CoqPilot, a VS Code extension designed to help automate writing of Coq proofs. The plugin collects the parts of proofs marked with the admit tactic in a Coq file, i.e., proof holes, and combines LLMs along with non-machine-learning methods to generate proof candidates for the holes. Then, CoqPilot checks if each proof candidate solves the given subgoal and, if successful, replaces the hole with it. The focus of CoqPilot is twofold. Firstly, we want to allow users to seamlessly combine multiple Coq generation approaches and provide a zero-setup experience for our tool. Secondly, we want to deliver a platform for LLM-based experiments on Coq proof generation. We developed a benchmarking system for Coq generation methods, available in the plugin, and conducted an experiment using it, showcasing the framework's possibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo. Code at: https://github.com/JetBrains-Research/coqpilot",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2382\u20132385",
        "numpages": "4",
        "keywords": "LLM, coq, code generation",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "LLM4Workflow: An LLM-based Automated Workflow Model Generation Tool": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695360",
        "author": "Xu, Jia and Du, Weilin and Liu, Xiao and Li, Xuejun",
        "title": "LLM4Workflow: An LLM-based Automated Workflow Model Generation Tool",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695360",
        "doi": "10.1145/3691620.3695360",
        "abstract": "Workflows are pervasive in software systems where business processes and scientific methods are implemented as workflow models to achieve automated process execution. However, despite the benefit of no/low-code workflow automation, creating workflow models requires in-depth domain knowledge and nontrivial workflow modeling skills, which becomes a hurdle for the proliferation of workflow applications. Recently, Large language models (LLMs) have been widely applied in software code generation given their outstanding ability to understand complex instructions and generate accurate, context-aware code. Inspired by the success of LLMs in code generation, this paper aims to investigate how to use LLMs to automate workflow model generation. We present LLM4Workflow, an LLM-based automated workflow model generation tool. Using workflow descriptions as the input, LLM4Workflow can automatically embed relevant API knowledge and leverage LLM's powerful contextual learning abilities to generate correct and executable workflow models. Its effectiveness was validated through functional verification and simulation tests on a real-world workflow system. LLM4Workflow is open sourced at https://github.com/ISEC-AHU/LLM4Workflow, and the demo video is provided at https://youtu.be/XRQ0saKkuxY.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2394\u20132398",
        "numpages": "5",
        "keywords": "automated workflow model generation, large language models, low code development",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "LLM-Based Java Concurrent Program to ArkTS Converter": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695362",
        "author": "Liu, Runlin and Lin, Yuhang and Hu, Yunge and Zhang, Zhe and Gao, Xiang",
        "title": "LLM-Based Java Concurrent Program to ArkTS Converter",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695362",
        "doi": "10.1145/3691620.3695362",
        "abstract": "HarmonyOS NEXT is a distributed operating system developed to support HarmonyOS native apps. To support the new and independent Harmony ecosystem, developers are required to migrate their applications from Android to HarmonyOS. However, HarmonyOS utilizes ArkTS, a superset of TypeScript, as the programming language for application development. Hence, migrating applications to HarmonyOS requires translating programs across different program languages, e.g., Java, which is known to be very challenging, especially for concurrency programs. Java utilizes shared memory to implement concurrency programs, while ArkTS relies on message passing (i.e., Actor model). This paper presents an LLM-based concurrent Java program to ArkTS converter.Our converter utilizes large language models (LLMs) for efficient code translation, integrating ArkTS's SharedArrayBuffer API to create ThreadBridge, a library that replicates Java's shared memory model. Using LLM's Chain-of-Thought mechanism, the translation process is divided into specialized chains: the TS chain, concurrency chain, and synchronization chain, each handling TypeScript syntax, concurrency patterns, and synchronization logic with precision.This study offers solutions to bridge concurrency model differences between Java and ArkTS, reducing manual code rewriting and speeding up adaptation for HarmonyOS NEXT. Experiments show the converter successfully compiles 66\\% of 53 test samples, with 69\\% accuracy for compiled results. Overall, the approach shows promise in converting concurrent Java programs to ArkTS, laying the foundation for future improvements.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2403\u20132406",
        "numpages": "4",
        "keywords": "source code translations, HarmonyOS NEXT, ArkTS",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Automated Validation of COBOL to Java Transformation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695365",
        "author": "Kumar, Atul and Saha, Diptikalyan and Yasue, Toshiaki and Ono, Kohichi and Krishnan, Saravanan and Hans, Sandeep and Satoh, Fumiko and Mitchell, Gerald and Kumar, Sachin",
        "title": "Automated Validation of COBOL to Java Transformation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695365",
        "doi": "10.1145/3691620.3695365",
        "abstract": "Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterpriselevel code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code. We propose a framework and a tool to help validate the equivalence of COBOL and translated Java. The results can also help repair the code if there are some issues and provide feedback to the AI model to improve. We have developed a symbolic-execution-based test generation to automatically generate unit tests for the source COBOL programs which also mocks the external resource calls. We generate equivalent JUnit test cases with equivalent mocking as COBOL and run them to check semantic equivalence between original and translated programs. Demo Video: https://youtu.be/aqF_agNP-lU",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2415\u20132418",
        "numpages": "4",
        "keywords": "automatic validation, COBOL to Java, external resource testing",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Can Large Language Models Comprehend Code Stylometry?": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695370",
        "author": "Dipongkor, Atish",
        "title": "Can Large Language Models Comprehend Code Stylometry?",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695370",
        "doi": "10.1145/3691620.3695370",
        "abstract": "Code Authorship Attribution (CAA) has several applications such as copyright disputes, plagiarism detection and criminal prosecution. Existing studies mainly focused on CAA by proposing machine learning (ML) and Deep Learning (DL) based techniques. The main limitations of ML-based techniques are (a) manual feature engineering is required to train these models and (b) they are vulnerable to adversarial attack. In this study, we initially fine-tune five Large Language Models (LLMs) for CAA and evaluate their performance. Our results show that LLMs are robust and less vulnerable compared to existing techniques in CAA task.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2429\u20132431",
        "numpages": "3",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695322",
        "author": "Luo, Yang and Yu, Richard and Zhang, Fajun and Liang, Ling and Xiong, Yongqiang",
        "title": "Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695322",
        "doi": "10.1145/3691620.3695322",
        "abstract": "When using large language models (LLMs) for code translation of complex software, numerous compilation and runtime errors can occur due to insufficient context awareness. To address this issue, this paper presents a code translation method based on call graphs and bridged debuggers: TransGraph. TransGraph first obtains the call graph of the entire code project using the Language Server Protocol, which provides a detailed description of the function call relationships in the program. Through this structured view of the code, LLMs can more effectively handle large-scale and complex codebases, significantly reducing compilation errors. Furthermore, TransGraph, combined with bridged debuggers and dynamic test case generation, significantly reduces runtime errors, overcoming the limitations of insufficient test case coverage in traditional methods. In experiments on six datasets including CodeNet and Avatar, TransGraph outperformed existing code translation methods and LLMs in terms of translation accuracy, with improvements of up to 10.2\\%.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2448\u20132449",
        "numpages": "2",
        "keywords": "code translation, large language model, call graph, bridged debugger, language server protocol, runtime error, compilation error",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Cross-lingual Code Clone Detection: When LLMs Fail Short Against Embedding-based Classifier": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695335",
        "author": "Moumoula, Micheline Benedicte and Kabore, Abdoul Kader and Klein, Jacques and Bissyande, Tegawende F.",
        "title": "Cross-lingual Code Clone Detection: When LLMs Fail Short Against Embedding-based Classifier",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695335",
        "doi": "10.1145/3691620.3695335",
        "abstract": "Cross-lingual code clone detection has gained attention in software development due to the use of multiple programming languages. Recent advances in machine learning, particularly Large Language Models (LLMs), have motivated a reexamination of this problem.This paper evaluates the performance of four LLMs and eight prompts for detecting cross-lingual code clones, as well as a pretrained embedding model for classifying clone pairs. Both approaches are tested on the XLCoST and CodeNet datasets.Our findings show that while LLMs achieve high F1 scores (up to 0.98) on straightforward programming examples, they struggle with complex cases and cross-lingual understanding. In contrast, embedding models, which map code fragments from different languages into a common representation space, allow for the training of a basic classifier that outperforms LLMs by approximately 2 and 24 percentage points on the XLCoST and CodeNet datasets, respectively. This suggests that embedding models provide more robust representations, enabling state-of-the-art performance in cross-lingual code clone detection.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2474\u20132475",
        "numpages": "2",
        "keywords": "cross-language pairs, code clone detection, large language model, prompt engineering, embedding model",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24"
    },
    "Who Judges the Judge: An Empirical Study on Online Judge Tests": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598060",
        "author": "Liu, Kaibo and Han, Yudong and Zhang, Jie M. and Chen, Zhenpeng and Sarro, Federica and Harman, Mark and Huang, Gang and Ma, Yun",
        "title": "Who Judges the Judge: An Empirical Study on Online Judge Tests",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598060",
        "doi": "10.1145/3597926.3598060",
        "abstract": "Online Judge platforms play a pivotal role in education, competitive programming, recruitment, career training, and large language model training. They rely on predefined test suites to judge the correctness of submitted solutions. It is therefore important that the solution judgement is reliable and free from potentially misleading false positives (i.e., incorrect solutions that are judged as correct). In this paper, we conduct an empirical study of 939 coding problems with 541,552 solutions, all of which are judged to be correct according to the test suites used by the platform, finding that 43.4\\% of the problems include false positive solutions (3,440 bugs are revealed in total). We also find that test suites are, nevertheless, of high quality according to widely-studied test effectiveness measurements: 88.2\\% of false positives have perfect (100\\%) line coverage, 78.9\\% have perfect branch coverage, and 32.5\\% have a perfect mutation score. Our findings indicate that more work is required to weed out false positive solutions and to further improve test suite effectiveness. We have released the detected false positive solutions and the generated test inputs to facilitate future research.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "334\u2013346",
        "numpages": "13",
        "keywords": "test assessment, software testing, Online judge platform",
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023"
    },
    "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598067",
        "author": "Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming",
        "title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598067",
        "doi": "10.1145/3597926.3598067",
        "abstract": "Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations.  To address these limitations, we propose TitanFuzz \u2013 the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38\\%/50.84\\% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs.  This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "423\u2013435",
        "numpages": "13",
        "keywords": "Test Generation, Large Language Model, Fuzz Testing",
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023"
    },
    "ISSTA2023 Artifact for \"Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models": {
        "type": "software",
        "key": "10.5281/zenodo.7980923",
        "author": "Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming",
        "title": "ISSTA2023 Artifact for \"Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models",
        "year": "2023",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.7980923",
        "abstract": "    <p>The artifact provides the source code of the ISSTA\u20192023 paper \u201cLarge Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models\u201d. Specifically, it contains TitanFuzz\u2019s implementation for fuzzing PyTorch and TensorFlow.</p>",
        "keywords": "Fuzz Testing, Large Language Model, Test Generation"
    },
    "How Effective Are Neural Networks for Fixing Security Vulnerabilities": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598135",
        "author": "Wu, Yi and Jiang, Nan and Pham, Hung Viet and Lutellier, Thibaud and Davis, Jordan and Tan, Lin and Babkin, Petr and Shah, Sameena",
        "title": "How Effective Are Neural Networks for Fixing Security Vulnerabilities",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598135",
        "doi": "10.1145/3597926.3598135",
        "abstract": "Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans, to better evaluate LLMs and APR techniques, and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4\\%), the most number of vulnerabilities. Many of the generated patches are uncompilable patches. (2) Fine-tuning with general APR data improves LLMs\u2019 vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes 8.7 transformed vulnerabilities, outperforming all the other LLMs and APR models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning LLMs with such data, and applying code simplification transformation to facilitate vulnerability repair.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1282\u20131294",
        "numpages": "13",
        "keywords": "Vulnerability, Language Model, Automated Program Repair, AI and Software Engineering",
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023"
    },
    "CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00085",
        "author": "Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K. and Sen, Siddhartha",
        "title": "CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00085",
        "doi": "10.1109/ICSE48619.2023.00085",
        "abstract": "Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "919\u2013931",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23"
    },
    "CCTest: Testing and Repairing Code Completion Systems": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00110",
        "author": "Li, Zongjie and Wang, Chaozheng and Liu, Zhibo and Wang, Haoxuan and Chen, Dong and Wang, Shuai and Gao, Cuiyun",
        "title": "CCTest: Testing and Repairing Code Completion Systems",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00110",
        "doi": "10.1109/ICSE48619.2023.00110",
        "abstract": "Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems.In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in black-box settings. CCTest features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTest repairs the code completion outputs by selecting the output that mostly reflects the \"average\" appearance of all output cases, as the final output of the code completion systems. With around 18K test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86\\%) from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40\\% and 67\\% with respect to BLEU score and Levenshtein edit similarity.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1238\u20131250",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23"
    },
    "Automated Repair of Programs from Large Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00128",
        "author": "Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei",
        "title": "Automated Repair of Programs from Large Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00128",
        "doi": "10.1109/ICSE48619.2023.00128",
        "abstract": "Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1469\u20131481",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23"
    },
    "Automated Program Repair in the Era of Large Pre-Trained Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00129",
        "author": "Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming",
        "title": "Automated Program Repair in the Era of Large Pre-Trained Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00129",
        "doi": "10.1109/ICSE48619.2023.00129",
        "abstract": "Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1482\u20131494",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23"
    },
    "Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00194",
        "author": "Kang, Sungmin and Yoon, Juyeon and Yoo, Shin",
        "title": "Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00194",
        "doi": "10.1109/ICSE48619.2023.00194",
        "abstract": "Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28\\% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose LIBRO, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of LIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate failure reproducing test cases for 33\\% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate LIBRO against 31 bug reports submitted after the collection of the LLM training data terminated: LIBRO produces bug reproducing tests for 32\\% of the studied bug reports. Overall, our results show LIBRO has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "2312\u20132323",
        "numpages": "12",
        "keywords": "software engineering, natural language processing, test generation",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23"
    },
    "COMEX: A Tool for Generating Customized Source Code Representations": {
        "type": "INPROCEEDINGS",
        "key": "10298568",
        "author": "Das, Debeshee and Mathews, Noble Saji and Mathai, Alex and Tamilselvam, Srikanth and Sedamaki, Kranthi and Chimalakonda, Sridhar and Kumar, Atul",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "COMEX: A Tool for Generating Customized Source Code Representations",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "2054-2057",
        "abstract": "Learning effective representations of source code is critical for any Machine Learning for Software Engineering (ML4SE) system. Inspired by natural language processing, large language models (LLMs) like Codex and CodeGen treat code as generic sequences of text and are trained on huge corpora of code data, achieving state of the art performance on several software engineering (SE) tasks. However, valid source code, unlike natural language, follows a strict structure and pattern governed by the underlying grammar of the programming language. Current LLMs do not exploit this property of the source code as they treat code like a sequence of tokens and overlook key structural and semantic properties of code that can be extracted from code-views like the Control Flow Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc. Unfortunately, the process of generating and integrating code-views for every programming language is cumbersome and time consuming. To overcome this barrier, we propose our tool COMEX - a framework that allows researchers and developers to create and combine multiple code-views which can be used by machine learning (ML) models for various SE tasks. Some salient features of our tool are: (i) it works directly on source code (which need not be compilable), (ii) it currently supports Java and C#, (iii) it can analyze both method-level snippets and program-level snippets by using both intra-procedural and inter-procedural analysis, and (iv) it is easily extendable to other languages as it is built on tree-sitter - a widely used incremental parser that supports over 40 languages. We believe this easy-to-use code-view generation and customization tool will give impetus to research in source code representation learning methods and ML4SE. The source code and demonstration of our tool can be found at https://github.com/IBM/tree-sitter-codeviews and https://youtu.be/GER6U87FVbU, respectively.",
        "keywords": "Representation learning;Computer languages;Codes;Source coding;Semantics;Process control;Syntactics;Representation Learning;Static Analysis",
        "doi": "10.1109/ASE56229.2023.00010",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair": {
        "type": "INPROCEEDINGS",
        "key": "10298532",
        "author": "Huang, Kai and Meng, Xiangxin and Zhang, Jian and Liu, Yang and Wang, Wenjie and Li, Shuhao and Zhang, Yuqing",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1162-1174",
        "abstract": "The advent of large language models (LLMs) has opened up new opportunities for automated program repair (APR). In particular, some recent studies have explored how to leverage large language models of code (LLMCs) for program repair tasks and show promising results. However, most of them adopt the zero/few-shot learning paradigm for APR, which directly use LLMCs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMCs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMCs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in this work, we conduct a comprehensive study on the program repair capability of LLMCs in the fine-tuning paradigm. We select 5 popular LLMCs with representative pre-training architectures, including CodeBERT, GraphCode-BERT, PLBART, CodeT5, and UniX coder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, $\\mathrm{C}/\\mathrm{C}++$, and JavaScript). Notably, we take both single-hunk and multi-hunk bugs/vulnerabilities into account. We then fine-tune them on widely-used datasets and compare them with existing state-of-the-art APR tools. We also investigate the impact of different design choices, which include code abstractions, code representations, and model evaluation metrics. Our experimental results show that LLMCs in the fine-tuning paradigm can significantly outperform previous state-of-the-art APR tools. Through in-depth analysis, we provide insights into choosing appropriate strategies to guide LLMCs for better performance. Lastly, we reveal several limitations of LLMCs for APR and make suggestions for future research on LLMC-based APR.",
        "keywords": "Measurement;Java;Computer languages;Codes;Computer bugs;Maintenance engineering;Benchmark testing;Automated Program Repair;Large Language Models of Code;Neural Machine Translation;Fine-Tuning",
        "doi": "10.1109/ASE56229.2023.00181",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "SMT Solver Validation Empowered by Large Pre-Trained Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10298442",
        "author": "Sun, Maolin and Yang, Yibiao and Wang, Yang and Wen, Ming and Jia, Haoxiang and Zhou, Yuming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "SMT Solver Validation Empowered by Large Pre-Trained Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1288-1300",
        "abstract": "SMT solvers are utilized to check the satisfiability of logic formulas and have been applied in various crucial domains, including software verification, test case generation, and program synthesis. However, bugs hidden in SMT solvers can lead to severe consequences, causing erroneous results in these domains. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Despite several testing approaches proposed for SMT solvers, generating effective test formulas to comprehensively test SMT solvers remains a challenge. To address this challenge, in this study, we propose to port large language models (LLMs) to generate SMT formulas for fuzzing solvers. Specifically, the study presents a novel retrain-finetune pipeline to unleash the potential of language models to generate effective SMT formulas and improve their generation performance through data augmentation. We implemented our approach as a practical fuzzing tool, named LasT,and then extensively tested the state-of-the-art SMT solvers, namely Z3, cvc5, and Bitwuzla. To date, Last has successfully uncovered 65 genuine bugs for the solvers, of which 45 have been fixed by the developers.",
        "keywords": "Adaptation models;Computer bugs;Pipelines;Fuzzing;Data augmentation;Data models;Software;SMT solver;fuzzing;large language model;retrain-finetune;data augmentation",
        "doi": "10.1109/ASE56229.2023.00180",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "Improving Code Extraction from Coding Screencasts Using a Code-Aware Encoder-Decoder Model": {
        "type": "INPROCEEDINGS",
        "key": "10298433",
        "author": "Malkadi, Abdulkarim and Tayeb, Ahmad and Haiduc, Sonia",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Improving Code Extraction from Coding Screencasts Using a Code-Aware Encoder-Decoder Model",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1492-1504",
        "abstract": "Accurate automatic code extraction from tutorial videos is crucial for software developers seeking to reuse the code contained in these videos. Current methods using optical character recognition (OCR) often yield inaccurate results due to code complexity and variations in screencast formats. To address this issue, we introduce CodeT5-OCRfix, an approach that leverages the pre-trained code-aware large language model CodeT5 to enhance code extraction accuracy by post-processing OCRed code. We first collect a large and diverse dataset of source code screenshots captured from more than 10K Java projects from GitHub. We then apply the most widely used OCR engine for the task of code extraction from videos, Tesseract, on these screenshots and collect the OCRed code along with the ground truth code extracted from the Java files. We built a training dataset of more than 585K pairs of OCRed and ground truth code pairs, which we then used to fine-tune CodeT5, obtaining our model CodeT5-OCRfix. An empirical evaluation on both screenshots and screencast frames shows that CodeT5-OCRfix outperforms baseline code extraction models and is also more time-efficient. Our approach therefore improves the state-of-the-art in code extraction techniques from screencasts and images.",
        "keywords": "Training;Java;Codes;Source coding;Optical character recognition;Tutorials;Task analysis;code extraction;coding screencasts;code-aware model;optical character recognition",
        "doi": "10.1109/ASE56229.2023.00184",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "Modeling Programmer Attention as Scanpath Prediction": {
        "type": "INPROCEEDINGS",
        "key": "10298441",
        "author": "Bansal, Aakash and Su, Chia-Yi and Karas, Zachary and Zhang, Yifan and Huang, Yu and Li, Toby Jia-Jun and McMillan, Collin",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Modeling Programmer Attention as Scanpath Prediction",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1732-1736",
        "abstract": "This paper launches a new effort at modeling programmer attention by predicting eye movement scanpaths. Programmer attention refers to what information people intake when performing programming tasks. Models of programmer attention refer to machine prediction of what information is important to people. Models of programmer attention are important because they help researchers build better interfaces, assistive technologies, and more human-like AI. For many years, researchers in SE have built these models based on features such as mouse clicks, key logging, and IDE interactions. Yet the holy grail in this area is scanpath prediction - the prediction of the sequence of eye fixations a person would take over a visual stimulus. A person's eye movements are considered the most concrete evidence that a person is taking in a piece of information. Scanpath prediction is a notoriously difficult problem, but we believe that the emergence of lower-cost, higheraccuracy eye tracking equipment and better large language models of source code brings a solution within grasp. We present an eye tracking experiment with 27 programmers and a prototype scanpath predictor to present preliminary results and obtain early community feedback.",
        "keywords": "Visualization;Virtual assistants;Source coding;Neural networks;Prototypes;Gaze tracking;Predictive models;scanpath prediction;human attention;eye tracking;neural networks;artificial intelligence",
        "doi": "10.1109/ASE56229.2023.00092",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "On Automated Assistants for Software Development: The Role of LLMs": {
        "type": "INPROCEEDINGS",
        "key": "10298445",
        "author": "Leung, Mira and Murphy, Gail",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "On Automated Assistants for Software Development: The Role of LLMs",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1737-1741",
        "abstract": "Software developers handle many complex tasks that include gathering and applying domain knowledge, coordinating subtasks, designing interfaces, turning ideas into elegant code, and more. They must switch contexts between these tasks, incurring more cognitive costs. Recent advances in large language models (LLMs) open up new possibilities for moving beyond the support provided by automated assistants (AAs) available today. In this paper, we explore if a human memory model can provide a framework for the systematic investigation of AAs for software development based on LLMs and other new technologies.",
        "keywords": "Productivity;Systematics;Costs;Codes;Automation;Switches;Turning;automation;machine learning;artificial intelligence;large language models;software development productivity",
        "doi": "10.1109/ASE56229.2023.00035",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "Better Patching Using LLM Prompting, via Self-Consistency": {
        "type": "INPROCEEDINGS",
        "key": "10298561",
        "author": "Ahmed, Toufique and Devanbu, Premkumar",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Better Patching Using LLM Prompting, via Self-Consistency",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1742-1746",
        "abstract": "Large Language models (LLMs) can be induced to solve non-trivial problems with \u201cfew-shot\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \u201cchain of thought\u201d ($\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \u201cexplained\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\mathcal{S}-C$ (or even $\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.",
        "keywords": "Art;Maintenance engineering;Software;Cognition;Software engineering;LLMs;Self-consistency;Program Repair",
        "doi": "10.1109/ASE56229.2023.00065",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "Smart Prompt Advisor: Multi-Objective Prompt Framework for Consistency and Best Practices": {
        "type": "INPROCEEDINGS",
        "key": "10298429",
        "author": "Phokela, Kanchanjot Kaur and Sikand, Samarth and Singi, Kapil and Dey, Kuntal and Sharma, Vibhu Saujanya and Kaulgud, Vikrant",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Smart Prompt Advisor: Multi-Objective Prompt Framework for Consistency and Best Practices",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1846-1848",
        "abstract": "Recent breakthroughs in Large Language Models (LLM), comprised of billions of parameters, have achieved the ability to unveil exceptional insight into a wide range of Natural Language Processing (NLP) tasks. The onus of the performance of these models lies in the sophistication and completeness of the input prompt. Minimizing the enhancement cycles of prompt with improvised keywords becomes critically important as it directly affects the time to market and cost of the developing solution. However, this process inevitably has a trade-off between the learning curve/proficiency of the user and completeness of the prompt, as generating such a solutions is an incremental process. In this paper, we have designed a novel solution and implemented it in the form of a plugin for Visual Studio Code IDE, which can optimize this trade-off, by learning the underlying prompt intent to enhance with keywords. This will tend to align with developers' collection of semantics while developing a secure code, ensuring parameter and local variable names, return expressions, simple pre and post-conditions. and basic control and data flow are met.",
        "keywords": "Visualization;Codes;Costs;Semantics;Time to market;Natural language processing;Task analysis;Prompt Engineering;Artificial Intelligence;Deep Learning;LLM;Ontology",
        "doi": "10.1109/ASE56229.2023.00019",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT": {
        "type": "INPROCEEDINGS",
        "key": "10298505",
        "author": "Yan, Dapeng and Gao, Zhipeng and Liu, Zhiming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1887-1898",
        "abstract": "Code generation aims to generate source code implementing human requirements illustrated with natural language specifications. With the rapid development of intelligent software engineering, automated code generation has become a hot research topic in both artificial intelligence and software engineering, and researchers have made significant achievements on code generation. More recently, large language models (LLMs) have demonstrated outstanding performance on code generation tasks, such as ChatGPT released by OpenAI presents the fantastic potential on automated code generation. However, the existing studies are limited to exploring LLMs' ability for generating code snippets to solve simple programming problems, the task of competition-level code generation has never been investigated. The specifications of the programming competition are always complicated and require the specific input/output format as well as the high-level algorithmic reasoning ability. In this study, we conduct the first large empirical study to investigate the zero-shot learning ability of ChatGPT for solving competition programming problems. Specifically, we warm up the design of prompts by using the Human-Eval dataset. Then, we apply the well-designed prompt to the competition-level code generation dataset, namely APPS, to further explore the effectiveness of using ChatGPT for solving competition problems. We collect ChatGPT's outputs on 5,000 code competition problems, the evaluation results show that it can successfully pass 25.4% test cases. By further feeding extra information (e.g, test failed information) to ChatGPT, we observe that ChatGPT has the potential to fix partial pass into a fully pass program. Moreover, we investigate the solutions generated by LLMs and the existing solutions, we find that it prefers to directly copy the code instead of re-write when facing more difficult problems. Finally, we evaluate the code quality generated by ChatGPT in terms of \u201ccode cleanness\u201d, we observe that the generated codes are with small functions and file sizes, which are in line with the standard of clean code.",
        "keywords": "Codes;Source coding;Natural languages;Programming;Chatbots;Cognition;Task analysis;code generation;program competition;Chat-GPT;large language model;clean code",
        "doi": "10.1109/ASE56229.2023.00096",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting": {
        "type": "INPROCEEDINGS",
        "key": "10298538",
        "author": "Li, Tsz-On and Zong, Wenxi and Wang, Yibo and Tian, Haoye and Wang, Ying and Cheung, Shing-Chi and Kramer, Jeff",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "14-26",
        "abstract": "Automated detection of software failures is an important but challenging software engineering task. It involves finding in a vast search space the failure-inducing test cases that contain an input triggering the software fault and an oracle asserting the incorrect execution. We are motivated to study how far this outstanding challenge can be solved by recent advances in large language models (LLMs) such as ChatGPT. However, our study reveals that ChatGPT has a relatively low success rate (28.8%) in finding correct failure-inducing test cases for buggy programs. A possible conjecture is that finding failure-inducing test cases requires analyzing the subtle differences (nuances) between the tokens of a program's correct version and those for its buggy version. When these two versions have similar sets of tokens and attentions, ChatGPT is weak in distinguishing their differences. We find that ChatGPT can successfully generate failure-inducing test cases when it is guided to focus on the nuances. Our solution is inspired by an interesting observation that ChatGPT could infer the intended functionality of buggy code if it is similar to the correct version. Driven by the inspiration, we develop a novel technique, called Differential Prompting, to effectively find failure-inducing test cases with the help of the compilable code synthesized by the inferred intention. Prompts are constructed based on the nuances between the given version and the synthesized code. We evaluate Differential Prompting on Quixbugs (a popular benchmark of buggy programs) and recent programs published at Codeforces (a popular programming contest portal, which is also an official benchmark of ChatGPT). We compare Differential Prompting with two baselines constructed using conventional ChatGPT prompting and Pynguin (the state-of-the-art unit test generation tool for Python programs). Our evaluation results show that for programs of Quixbugs, Differential Prompting can achieve a success rate of 75.0% in finding failure-inducing test cases, outperforming the best baseline by 2.6X. For programs of Codeforces, Differential Prompting's success rate is 66.7%, outperforming the best baseline by 4.0X.",
        "keywords": "Codes;Benchmark testing;Programming;Chatbots;Software;Test pattern generators;Task analysis;failure-inducing test cases;large language models;program intention inference;program generation",
        "doi": "10.1109/ASE56229.2023.00089",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "The Devil is in the Tails: How Long-Tailed Code Distributions Impact Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10298393",
        "author": "Zhou, Xin and Kim, Kisub and Xu, Bowen and Liu, Jiakun and Han, DongGyun and Lo, David",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "The Devil is in the Tails: How Long-Tailed Code Distributions Impact Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "40-52",
        "abstract": "Learning-based techniques, especially advanced Large Language Models (LLMs) for code, have gained considerable popularity in various software engineering (SE) tasks. However, most existing works focus on designing better learning-based models and pay less attention to the properties of datasets. Learning-based models, including popular LLMs for code, heavily rely on data, and the data's properties (e.g., data distribution) could significantly affect their behavior. We conducted an exploratory study on the distribution of SE data and found that such data usually follows a skewed distribution (i.e., long-tailed distribution) where a small number of classes have an extensive collection of samples, while a large number of classes have very few samples. We investigate three distinct SE tasks and analyze the impacts of long-tailed distribution on the performance of LLMs for code. Our experimental results reveal that the long-tailed distribution has a substantial impact on the effectiveness of LLMs for code. Specifically, LLMs for code perform between 30.0% and 254.0% worse on data samples associated with infrequent labels compared to data samples of frequent labels. Our study provides a better understanding of the effects of long-tailed distributions on popular LLMs for code and insights for the future development of SE automation.",
        "keywords": "Codes;Automation;Tail;Data models;Behavioral sciences;Task analysis;Software engineering",
        "doi": "10.1109/ASE56229.2023.00157",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "SCPatcher: Mining Crowd Security Discussions to Enrich Secure Coding Practices": {
        "type": "INPROCEEDINGS",
        "key": "10298463",
        "author": "Jiang, Ziyou and Shi, Lin and Yang, Guowei and Wang, Qing",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "SCPatcher: Mining Crowd Security Discussions to Enrich Secure Coding Practices",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "358-370",
        "abstract": "Secure coding practices (SCPs) have been proposed to guide software developers to write code securely to prevent potential security vulnerabilities. Yet, they are typically one-sentence principles without detailed specifications, e.g., \u201cProperly free allocated memory upon the completion of functions and at all exit points.\u201d, which makes them difficult to follow in practice, especially for software developers who are not yet experienced in secure programming. To address this problem, this paper proposes SCPatcher, an automated approach to enrich secure coding practices by mining crowd security discussions on online knowledge-sharing platforms, such as Stack Overflow. In particular, for each security post, SCPatcher first extracts the area of coding examples and coding explanations with a fix-prompt tuned Large Language Model (LLM) via Prompt Learning. Then, it hierarchically slices the lengthy code into coding examples and summarizes the coding explanations with the areas. Finally, SCPatcher matches the CWE and Public SCP, integrating them with extracted coding examples and explanations to form the SCP specifications, which are the wild SCPs with details, proposed by the developers. To evaluate the performance of SCPatcher, we conduct experiments on 3,907 security posts from Stack Overflow. The experimental results show that SCPatcher outperforms all baselines in extracting the coding examples with 2.73 % MLine on average, as well as coding explanations with 3.97 % F1 on average. Moreover, we apply SCPatcher on 447 new security posts to further evaluate its practicality, and the extracted SCP specifications enrich the public SCPs with 3,074 lines of code and 1,967 sentences.",
        "keywords": "Codes;Programming;Encoding;Software;Security;Data mining;Software engineering;Software Security;Secure Coding Practice;Artificial Intelligence;Large Language Model",
        "doi": "10.1109/ASE56229.2023.00040",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "The Plastic Surgery Hypothesis in the Era of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10298499",
        "author": "Xia, Chunqiu Steven and Ding, Yifeng and Zhang, Lingming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "The Plastic Surgery Hypothesis in the Era of Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "522-534",
        "abstract": "Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names. The plastic surgery hypothesis is a well-known insight for APR, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional APR tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based approaches to exploit such existing code ingredients. However, as recent APR research starts focusing on LLM-based approaches, the plastic surgery hypothesis has been largely ignored. In this paper, we ask the following question: How useful is the plastic surgery hypothesis in the era of LLMs? Interestingly, LLM-based APR presents a unique opportunity to fully automate the plastic surgery hypothesis via fine-tuning (training on the buggy project) and prompting (directly providing valuable code ingredients as hints to the LLM). To this end, we propose FitRepair, which combines the direct usage of LLMs with two domain-specific fine-tuning strategies and one prompting strategy (via information retrieval and static analysis) for more powerful APR. While traditional APR techniques require intensive manual efforts in both generating patches based on the plastic surgery hypothesis and guaranteeing patch validity, our approach is fully automated and general. Moreover, while it is very challenging to manually design heuristics/patterns for effectively leveraging the hypothesis, due to the power of LLMs in code vectorization/understanding, even partial/imprecise project-specific information can still guide LLMs in generating correct patches! Our experiments on the widely studied Defects4j 1.2 and 2.0 datasets show that FitRepair fixes 89 and 44 bugs (substantially outperforming baseline techniques by 15 and 8), respectively, demonstrating a promising future of the plastic surgery hypothesis in the era of LLMs.",
        "keywords": "Training;Codes;Computer bugs;Surgery;Manuals;Static analysis;Maintenance engineering",
        "doi": "10.1109/ASE56229.2023.00047",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "Towards Automatically Addressing Self-Admitted Technical Debt: How Far Are We?": {
        "type": "INPROCEEDINGS",
        "key": "10298547",
        "author": "Mastropaolo, Antonio and Di Penta, Massimiliano and Bavota, Gabriele",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Towards Automatically Addressing Self-Admitted Technical Debt: How Far Are We?",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "585-597",
        "abstract": "Upon evolving their software, organizations and individual developers have to spend a substantial effort to pay back technical debt, i.e, the fact that software is released in a shape not as good as it should be, e.g, in terms of functionality, reliability, or maintainability. This paper empirically investigates the extent to which technical debt can be automatically paid back by neural-based generative models, and in particular models exploiting different strategies for pre-training and fine-tuning. We start by extracting a dateset of 5,039 Self-Admitted Technical Debt (SATD) removals from 595 open-source projects. SATD refers to technical debt instances documented (e.g, via code comments) by developers. We use this dataset to experiment with seven different generative deep learning (DL) model configurations. Specifically, we compare transformers pre-trained and fine-tuned with different combinations of training objectives, including the fixing of generic code changes, SATD removals, and SATD-comment prompt tuning. Also, we investigate the applicability in this context of a recently-available Large Language Model (LLM)-based chat bot. Results of our study indicate that the automated repayment of SATD is a challenging task, with the best model we experimented with able to automatically fix \u223c2% to 8% of test instances, depending on the number of attempts it is allowed to make. Given the limited size of the fine-tuning dataset (\u223c5k instances), the model's pre-training plays a fundamental role in boosting performance. Also, the ability to remove SATD steadily drops if the comment documenting the SATD is not provided as input to the model. Finally, we found general-purpose LLMs to not be a competitive approach for addressing SATD.",
        "keywords": "Training;Codes;Shape;Organizations;Transformers;Software;Software reliability;Self-Admitted Technical Debt;Pre-trained models;Machine Learning for Code",
        "doi": "10.1109/ASE56229.2023.00103",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?": {
        "type": "INPROCEEDINGS",
        "key": "10298329",
        "author": "Gao, Shuzheng and Wen, Xin-Cheng and Gao, Cuiyun and Wang, Wenxuan and Zhang, Hongyu and Lyu, Michael R.",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "761-773",
        "abstract": "Pre-trained models of source code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning (ICL). ICL employs task instructions and a few examples as demonstrations, and then inputs the demonstrations to the language models for making predictions. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of ICL heavily relies on the quality of demonstrations, e.g., the selected examples. It is important to systematically investigate how to construct a good demonstration for code-related tasks. In this paper, we empirically explore the impact of three key factors on the performance of ICL in code intelligence tasks: the selection, order, and number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including code summarization, bug fixing, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of ICL in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We also show that a carefully-designed demonstration based on our findings can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90%, 175.96%, and 50.81% on code summarization, bug fixing, and program synthesis, respectively.",
        "keywords": "Codes;Source coding;Computer bugs;Predictive models;Natural language processing;Task analysis;Software engineering",
        "doi": "10.1109/ASE56229.2023.00109",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining": {
        "type": "INPROCEEDINGS",
        "key": "10298349",
        "author": "Ren, Xiaoxue and Ye, Xinyuan and Zhao, Dehai and Xing, Zhenchang and Yang, Xiaohu",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "976-987",
        "abstract": "Large Language Models (LLMs) have shown promising results in automatic code generation by improving coding efficiency to a certain extent. However, generating high-quality and reliable code remains a formidable task because of LLMs' lack of good programming practice, especially in exception handling. In this paper, we first conduct an empirical study and summarize three crucial challenges of LLMs in exception handling, i.e., incomplete exception handling, incorrect exception handling and abuse of try-catch. We then try prompts with different granularities to address such challenges, finding fine-grained knowledge-driven prompts works best. Based on our empirical study, we propose a novel Knowledge-driven Prompt Chaining-based code generation approach, name KPC, which decomposes code generation into an AI chain with iterative check-rewrite steps and chains fine-grained knowledge-driven prompts to assist LLMs in considering exception-handling specifications. We evaluate our KPC-based approach with 3,079 code generation tasks extracted from the Java official API documentation. Extensive experimental results demonstrate that the KPC-based approach has considerable potential to ameliorate the quality of code generated by LLMs. It achieves this through proficiently managing exceptions and obtaining remarkable enhancements of 109.86% and 578.57% with static evaluation methods, as well as a reduction of 18 runtime bugs in the sampled dataset with dynamic validation.",
        "keywords": "Java;Codes;Runtime;Knowledge based systems;Documentation;Programming;Encoding;Large Language Model;Code Generation;Knowledge-driven Prompt;API Misuse",
        "doi": "10.1109/ASE56229.2023.00143",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "Generative Type Inference for Python": {
        "type": "INPROCEEDINGS",
        "key": "10298512",
        "author": "Peng, Yun and Wang, Chaozheng and Wang, Wenxuan and Gao, Cuiyun and Lyu, Michael R.",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Generative Type Inference for Python",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "988-999",
        "abstract": "Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub. However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs. Existing type inference approaches can be generally grouped into three categories, i.e., rule-based, supervised, and cloze- style approaches. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems caused by dynamic features and external calls. Supervised type inference approaches, while feature-agnostic and able to mitigate the low coverage problem, require large, high- quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem by leveraging the general knowledge in powerful pre-trained code models. However, their performance is limited since they ignore the domain knowledge from static typing rules which reflect the inference logic. What is more, their predictions are not interpretable, hindering developers' understanding and verification of the results. This paper introduces Typegen, a few-shot generative type inference approach that incorporates static domain knowledge from static analysis. Typegen creates chain-of-thought (COT) prompts by translating the type inference steps of static analysis into prompts based on the type dependency graphs (TDGs), enabling language models to learn from how static analysis infers types. By combining COT prompts with code slices and type hints, TypegEnconstructs example prompts from human annotations. Typeg Enonly requires very few annotated examples to teach language models to generate similar COT prompts via in-context learning. Moreover, Typeg Enenhances the interpretability of results through the use of the input- explanation-output strategy, which generates both explanations and type predictions in COT prompts. Experiments show that Typegen outperforms the best baseline Type4Py by 10.0% for argument type prediction and 22.5 % in return value type prediction in terms of top-l Exact Match by using only five examples. Furthermore, Typeg Enachieves substantial improvements of 27 % to 84 % compared to the zero-shot performance of large language models with parameter sizes ranging from 1.3B to 175B in terms of top-I Exact Match.",
        "keywords": "Analytical models;Codes;Annotations;Static analysis;Distance measurement;Dynamic programming;Python;type inference;chain-of-thought;generative model",
        "doi": "10.1109/ASE56229.2023.00031",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "A Comparative Study of Transformer-Based Neural Text Representation Techniques on Bug Triaging": {
        "type": "INPROCEEDINGS",
        "key": "10298494",
        "author": "Dipongkor, Atish Kumar and Moran, Kevin",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "A Comparative Study of Transformer-Based Neural Text Representation Techniques on Bug Triaging",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1012-1023",
        "abstract": "Bug report management has been shown to be an important and time consuming software maintenance task. Often, the first step in managing bug reports is related to triaging a bug to the appropriate developer who is best suited to understand, localize, and fix the target bug. Additionally, assigning a given bug to a particular part of a software project can help to expedite the fixing process. However, despite the importance of these activities, they are quite challenging, where days can be spent on the manual triaging process. Past studies have attempted to leverage the limited textual data of bug reports to train text classification models that automate this process - to varying degrees of success. However, the textual representations and machine learning models used in prior work are limited by their expressiveness, often failing to capture nuanced textual patterns that might otherwise aid in the triaging process. Recently, large, transformer-based, pre-tained neural text representation techniques (i.e., large language models or LLMs) such as BERT and CodeBERT have achieved greater performance with simplified training procedures in several natural language processing tasks, including text classification. However, the potential for using these techniques to improve upon prior approaches for automated bug triaging is not well studied or understood. Therefore, in this paper we offer one of the first investigations that fine-tunes transformer-based language models for the task of bug triaging on four open source datasets, spanning a collective 53 years of development history with over 400 developers and over 150 software project components. Our study includes both a quantitative and qualitative analysis of effectiveness. Our findings illustrate that DeBERTa is the most effective technique across the triaging tasks of developer and component assignment, and the measured performance delta is statistically significant compared to other techniques. However, through our qualitative analysis, we also observe that each technique possesses unique abilities best suited to certain types of bug reports.",
        "keywords": "Training;Software maintenance;Computer bugs;Text categorization;Manuals;Machine learning;Transformers;Bug Triaging;Transformer;LLMs;Text-Embedding;DL4SE",
        "doi": "10.1109/ASE56229.2023.00217",
        "ISSN": "2643-1572",
        "month": "Sep."
    },
    "Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks": {
        "type": "inproceedings",
        "key": "10.1145/3576915.3623120",
        "author": "Li, Zongjie and Wang, Chaozheng and Wang, Shuai and Gao, Cuiyun",
        "title": "Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks",
        "year": "2023",
        "isbn": "9798400700507",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3576915.3623120",
        "doi": "10.1145/3576915.3623120",
        "abstract": "The rise of large language model-based code generation (LLCG) has enabled various commercial services and APIs. Training LLCG models is often expensive and time-consuming, and the training data are often large-scale and even inaccessible to the public. As a result, the risk of intellectual property (IP) theft over the LLCG models (e.g., via imitation attacks) has been a serious concern. In this paper, we propose the first watermark (WM) technique to protect LLCG APIs from remote imitation attacks. Our proposed technique is based on replacing tokens in an LLCG output with their \"synonyms\" available in the programming language. A WM is thus defined as the stealthily tweaked distribution among token synonyms in LLCG outputs. We design six WM schemes (instantiated into over 30 WM passes) which rely on conceptually distinct token synonyms available in programming languages. Moreover, to check the IP of a suspicious model (decide if it is stolen from our protected LLCG API), we propose a statistical tests-based procedure that can directly check a remote, suspicious LLCG API.We evaluate our WM technique on LLCG models fine-tuned from two popular large language models, CodeT5 and CodeBERT. The evaluation shows that our approach is effective in both WM injection and IP check. The inserted WMs do not undermine the usage of normal users (i.e., high fidelity) and incur negligible extra cost. Moreover, our injected WMs exhibit high stealthiness and robustness against powerful attackers; even if they know all WM schemes, they can hardly remove WMs without largely undermining the accuracy of their stolen models.",
        "booktitle": "Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "2336\u20132350",
        "numpages": "15",
        "keywords": "code generation, large language model, watermark",
        "location": "Copenhagen, Denmark",
        "series": "CCS '23"
    },
    "Lost at C: a user study on the security implications of large language model code assistants": {
        "type": "inproceedings",
        "key": "10.5555/3620237.3620361",
        "author": "Sandoval, Gustavo and Pearce, Hammond and Nys, Teo and Karri, Ramesh and Garg, Siddharth and Dolan-Gavitt, Brendan",
        "title": "Lost at C: a user study on the security implications of large language model code assistants",
        "year": "2023",
        "isbn": "978-1-939133-37-3",
        "publisher": "USENIX Association",
        "address": "USA",
        "abstract": "Large Language Models (LLMs) such as OpenAI Codex are increasingly being used as AI-based coding assistants. Understanding the impact of these tools on developers' code is paramount, especially as recent work showed that LLMs may suggest cybersecurity vulnerabilities. We conduct a security-driven user study (N=58) to assess code written by student programmers when assisted by LLMs. Given the potential severity of low-level bugs as well as their relative frequency in real-world projects, we tasked participants with implementing a singly-linked 'shopping list' structure in C. Our results indicate that the security impact in this setting (low-level C with pointer and array manipulations) is small: AI-assisted users produce critical security bugs at a rate no greater than 10\\% more than the control, indicating the use of LLMs does not introduce new security risks.",
        "booktitle": "Proceedings of the 32nd USENIX Conference on Security Symposium",
        "articleno": "124",
        "numpages": "18",
        "location": "Anaheim, CA, USA",
        "series": "SEC '23"
    },
    "Examining Zero-Shot Vulnerability Repair with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10179420",
        "author": "Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan",
        "booktitle": "2023 IEEE Symposium on Security and Privacy (SP)",
        "title": "Examining Zero-Shot Vulnerability Repair with Large Language Models",
        "year": "2023",
        "volume": "",
        "ISSN": "",
        "pages": "2339-2356",
        "abstract": "Human developers can produce code with cybersecurity bugs. Can emerging \u2018smart\u2019 code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI\u2019s Codex and AI21\u2019s Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information\u2014 both semantically and syntactically\u2014with natural languages. We perform a large scale study of five commercially available, black-box, \"off-the-shelf\" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model\u2019s performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.",
        "keywords": "Privacy;Codes;Computer bugs;Natural languages;Closed box;Maintenance engineering;Computer crime",
        "doi": "10.1109/SP46215.2023.10179420",
        "url": "https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.10179420",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "month": "May"
    },
    "Primacy Effect of ChatGPT": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-primacy",
        "author": "Wang, Yiwei and Cai, Yujun and Chen, Muhao and Liang, Yuxuan and Hooi, Bryan",
        "booktitle": "EMNLP2023",
        "title": "Primacy Effect of ChatGPT",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherit humans\u2019 cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT\u2019s decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the source code at https://github.com/wangywUST/PrimacyEffectGPT.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.8",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2023-enhance",
        "author": "Chen, Hang and Yang, Xinyu and Luo, Jing and Zhu, Wenjing",
        "booktitle": "EMNLP2023",
        "title": "How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Our investigation into the Affective Reasoning in Conversation (ARC) task highlights the challenge of causal discrimination. Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships. To overcome this limitation, we propose the incorporation of i.i.d. noise terms into the conversation process, thereby constructing a structural causal model (SCM). It explores how distinct causal relationships of fitted embeddings can be discerned through independent conditions. To facilitate the implementation of deep learning, we introduce the cogn frameworks to handle unstructured conversation data, and employ an autoencoder architecture to regard the unobservable noise as learnable \u201cimplicit causes.\u201d Moreover, we curate a synthetic dataset that includes i.i.d. noise. Through comprehensive experiments, we validate the effectiveness and interpretability of our approach. Our code is available in https://github.com/Zodiark-ch/mater-of-our-EMNLP2023-paper.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.33",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "LLM-FP4: 4-Bit Floating-Point Quantized Transformers": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2023-llm",
        "author": "Liu, Shih-yang and Liu, Zechun and Huang, Xijie and Dong, Pingcheng and Cheng, Kwang-Ting",
        "booktitle": "EMNLP2023",
        "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.39",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Towards LLM-driven Dialogue State Tracking": {
        "type": "INPROCEEDINGS",
        "key": "feng-etal-2023-towards",
        "author": "Feng, Yujie and Lu, Zexin and Liu, Bo and Zhan, Liming and Wu, Xiao-Ming",
        "booktitle": "EMNLP2023",
        "title": "Towards LLM-driven Dialogue State Tracking",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within task-oriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across diverse applications. In this study, we conduct an initial examination of ChatGPT\u2019s capabilities in DST. Our evaluation uncovers the exceptional performance of ChatGPT in this task, offering valuable insights to researchers regarding its capabilities and providing useful directions for designing and enhancing dialogue systems. Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities. To address these concerns, we present LDST, an LLM-driven DST framework based on smaller, open-source foundation models. By utilizing a novel domain-slot instruction tuning method, LDST achieves performance on par with ChatGPT. Comprehensive evaluations across three distinct experimental settings, we find that LDST exhibits remarkable performance improvements in both zero-shot and few-shot setting compared to previous SOTA methods. The source code is provided for reproducibility.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.48",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms": {
        "type": "INPROCEEDINGS",
        "key": "han-etal-2023-reading",
        "author": "Han, Seungju and Kim, Junhyeok and Hessel, Jack and Jiang, Liwei and Chung, Jiwan and Son, Yejin and Choi, Yejin and Yu, Youngjae",
        "booktitle": "EMNLP2023",
        "title": "Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying commonsense norms: NormLens. NormLens consists of 10K human judgments accompanied by free-form explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation. Additionally, we present a simple yet effective approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code will be released.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.57",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "CodeT5+: Open Code Large Language Models for Code Understanding and Generation": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-codet5",
        "author": "Wang, Yue and Le, Hung and Gotmare, Akhilesh and Bui, Nghi and Li, Junnan and Hoi, Steven",
        "booktitle": "EMNLP2023",
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks, lacking the flexibility to operate in the optimal architecture for a specific task. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some tasks and hence result in substantial performance degrade. To address these limitations, we propose \u201cCodeT5+\u201d, a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives, which cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) performance on various code-related tasks, and our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.68",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings": {
        "type": "INPROCEEDINGS",
        "key": "wen-yi-mimno-2023-hyperpolyglot",
        "author": "Wen-Yi, Andrea W. and Mimno, David",
        "booktitle": "EMNLP2023",
        "title": "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Cross-lingual transfer learning is an important property of multilingual large language models (LLMs). But how do LLMs represent relationships between languages? Every language model has an input layer that maps tokens to vectors. This ubiquitous layer of language models is often overlooked. We find that similarities between these input embeddings are highly interpretable and that the geometry of these embeddings differs between model families. In one case (XLM-RoBERTa), embeddings encode language: tokens in different writing systems can be linearly separated with an average of 99.2% accuracy. Another family (mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors for any token represent an average of 7.61 writing systems, and are frequently translations. This result is surprising given that there is no explicit parallel cross-lingual training corpora and no explicit incentive for translations in pre-training objectives. Our research opens the door for investigations in 1) The effect of pre-training and model architectures on representations of languages and 2) The applications of cross-lingual representations embedded in language models.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.71",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "QTSumm: Query-Focused Summarization over Tabular Data": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2023-qtsumm",
        "author": "Zhao, Yilun and Qi, Zhenting and Nan, Linyong and Mi, Boyu and Liu, Yixin and Zou, Weijin and Han, Simeng and Chen, Ruizhe and Tang, Xiangru and Xu, Yumo and Radev, Dragomir and Cohan, Arman",
        "booktitle": "EMNLP2023",
        "title": "QTSumm: Query-Focused Summarization over Tabular Data",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users\u2019 information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring effective improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.74",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-coannotating",
        "author": "Li, Minzhi and Shi, Taiwei and Ziems, Caleb and Kan, Min-Yen and Chen, Nancy and Liu, Zhengyuan and Yang, Diyi",
        "booktitle": "EMNLP2023",
        "title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs\u2019 annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.92",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction": {
        "type": "INPROCEEDINGS",
        "key": "josifoski-etal-2023-exploiting",
        "author": "Josifoski, Martin and Sakota, Marija and Peyrard, Maxime and West, Robert",
        "booktitle": "EMNLP2023",
        "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data, and models are available at anonymous.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.96",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling": {
        "type": "INPROCEEDINGS",
        "key": "wei-etal-2023-outlier",
        "author": "Wei, Xiuying and Zhang, Yunchen and Li, Yuhang and Zhang, Xiangguo and Gong, Ruihao and Guo, Jinyang and Liu, Xianglong",
        "booktitle": "EMNLP2023",
        "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Post-training quantization (PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are concentrated in specific channels and are asymmetric across channels. To address this issue, we propose the Outlier Suppression+ (OS+) framework, which contains the channel-wise shifting for asymmetry and channel-wise scaling for concentration. We show that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we propose a fast and stable scheme to calculate effective shifting and scaling values. The channel-wise shifting aligns the center of each channel for removal of outlier asymmetry. The channel-wise scaling quantitatively evaluates changes brought by migration and quantization for better quantization burden balance. We validate our OS+ under both standard and fine-grained quantization settings with models including BERT, OPT, BLOOM, BLOOMZ, and LLaMA. Comprehensive results across various tasks demonstrate the superiority of our approach. Especially, with standard quantization, OS+ can achieve near-floating-point performance on both small models and large language models on 8-bit and 6-bit. Besides, we establish a new state-of-the-art for 4-bit BERT with 15.5% improvement. Our code is available at https://github.com/ModelTC/Outlier_Suppression_Plus.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.102",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Interpreting Embedding Spaces by Conceptualization": {
        "type": "INPROCEEDINGS",
        "key": "simhi-markovitch-2023-interpreting",
        "author": "Simhi, Adi and Markovitch, Shaul",
        "booktitle": "EMNLP2023",
        "title": "Interpreting Embedding Spaces by Conceptualization",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "One of the main methods for computational interpretation of a text is mapping it into a vector in some embedding space. Such vectors can then be used for a variety of textual processing tasks. Recently, most embedding spaces are a product of training large language models (LLMs). One major drawback of this type of representation is their incomprehensibility to humans. Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model. In this paper, we present a novel method of understanding embeddings by transforming a latent embedding space into a comprehensible conceptual space. We present an algorithm for deriving a conceptual space with dynamic on-demand granularity. We devise a new evaluation method, using either human rater or LLM-based raters, to show that the conceptualized vectors indeed represent the semantics of the original latent ones. We show the use of our method for various tasks, including comparing the semantics of alternative models and tracing the layers of the LLM. The code is available online https://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.106",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Democratizing Reasoning Ability: Tailored Learning from Large Language Model": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-democratizing",
        "author": "Wang, Zhaoyang and Huang, Shaohan and Liu, Yuxuan and Wang, Jiahai and Song, Minghui and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi",
        "booktitle": "EMNLP2023",
        "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instruction-following ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a tailored learning approach to distill such reasoning ability to smaller LMs to facilitate the democratization of the exclusive reasoning ability. In contrast to merely employing LLM as a data annotator, we exploit the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm. This paradigm enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return. Further, to exploit the reasoning potential of the smaller LM, we propose self-reflection learning to motivate the student to learn from self-made mistakes. The learning from self-reflection and LLM are all tailored to the student\u2019s learning status, thanks to the seamless integration with the multi-round learning paradigm. Comprehensive experiments and analysis on mathematical and commonsense reasoning tasks demonstrate the effectiveness of our method. The code will be available at https://github.com/Raibows/Learn-to-Reason.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.120",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-crt",
        "author": "Zhang, Zhehao and Li, Xitao and Gao, Yan and Lou, Jian-Guang",
        "booktitle": "EMNLP2023",
        "title": "CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) show powerful reasoning abilities on various text-based tasks. However, their reasoning capability on structured data such as tables has not been systematically explored. In this work, we first establish a comprehensive taxonomy of reasoning and operation types for tabular data analysis. Then, we construct a complex reasoning QA dataset over tabular data, named CRT-QA dataset (Complex Reasoning QA over Tabular data), with the following unique features: (1) it is the first Table QA dataset with multi-step operation and informal reasoning; (2) it contains fine-grained annotations on questions\u2019 directness, composition types of sub-questions, and human reasoning paths which can be used to conduct a thorough investigation on LLMs\u2019 reasoning ability; (3) it contains a collection of unanswerable and indeterminate questions that commonly arise in real-world situations. We further introduce an efficient and effective tool-augmented method, named ARC (Auto-exemplar-guided Reasoning with Code), to use external tools such as Pandas to solve table reasoning tasks without handcrafted demonstrations. The experiment results show that CRT-QA presents a strong challenge for baseline methods and ARC achieves the best result.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.132",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders": {
        "type": "INPROCEEDINGS",
        "key": "soares-etal-2023-nail",
        "author": "Soares, Livio and Gillick, Daniel and Cole, Jeremy and Kwiatkowski, Tom",
        "booktitle": "EMNLP2023",
        "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this servingtime requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10\u207b6% of the Transformer\u2019s FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-of-the art dual encoder retriever, that still requires an accelerator for query encoding. We introduce nail (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require neural processing of queries.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.156",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Analyzing Modular Approaches for Visual Question Decomposition": {
        "type": "INPROCEEDINGS",
        "key": "khandelwal-etal-2023-analyzing",
        "author": "Khandelwal, Apoorv and Pavlick, Ellie and Sun, Chen",
        "booktitle": "EMNLP2023",
        "title": "Analyzing Modular Approaches for Visual Question Decomposition",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Modular neural networks without additional training have recently been shown to surpass end-to-end neural networks on challenging vision\u2013language tasks. The latest such methods simultaneously introduce LLM-based code generation to build programs and a number of skill-specific, task-oriented modules to execute them. In this paper, we focus on ViperGPT and ask where its additional performance comes from and how much is due to the (state-of-art, end-to-end) BLIP-2 model it subsumes vs. additional symbolic components. To do so, we conduct a controlled study (comparing end-to-end, modular, and prompting-based methods across several VQA benchmarks). We find that ViperGPT\u2019s reported gains over BLIP-2 can be attributed to its selection of task-specific modules, and when we run ViperGPT using a more task-agnostic selection of modules, these gains go away. ViperGPT retains much of its performance if we make prominent alterations to its selection of modules: e.g. removing or retaining only BLIP-2. We also compare ViperGPT against a prompting-based decomposition strategy and find that, on some benchmarks, modular approaches significantly benefit by representing subtasks with natural language, instead of code. Our code is fully available at https://github.com/brown-palm/visual-question-decomposition.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.157",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology": {
        "type": "INPROCEEDINGS",
        "key": "odonoghue-etal-2023-bioplanner",
        "author": "O\u2019Donoghue, Odhran and Shtedritski, Aleksandar and Ginger, John and Abboud, Ralph and Ghareeb, Ali and Rodriques, Samuel",
        "booktitle": "EMNLP2023",
        "title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols is challenging, because experiments can be described correctly in many different ways, require expert knowledge to evaluate, and cannot usually be executed automatically. Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BioProt: a dataset of biology protocols with corresponding pseudocode representations. To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM\u2019s ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions. We evaluate GPT-3 and GPT-4 on this task and explore their robustness. We externally validate the utility of pseudocode representations of text by generating accurate novel protocols using retrieved pseudocode, and we run a generated protocol successfully in our biological laboratory. Our framework is extensible to the evaluation and improvement of language model",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.162",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts": {
        "type": "INPROCEEDINGS",
        "key": "liu-etal-2023-plan",
        "author": "Liu, Tengxiao and Guo, Qipeng and Yang, Yuqing and Hu, Xiangkun and Zhang, Yue and Qiu, Xipeng and Zhang, Zheng",
        "booktitle": "EMNLP2023",
        "title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "As large language models (LLMs) have shown effectiveness with different prompting methods, such as Chain of Thought, Program of Thought, we find that these methods have formed a great complementarity to each other on math reasoning tasks. In this work, we propose XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts. For each question, XoT always begins with selecting the most suitable method then executes each method iteratively. Within each iteration, XoT actively checks the validity of the generated answer and incorporates the feedback from external executors, allowing it to dynamically switch among different prompting methods. Through extensive experiments on 10 popular math reasoning datasets, we demonstrate the effectiveness of our proposed approach and thoroughly analyze the strengths of each module. Moreover, empirical results suggest that our framework is orthogonal to recent work that makes improvements on single reasoning methods and can further generalise to logical reasoning domain. By allowing method switching, XoT provides a fresh perspective on the collaborative integration of diverse reasoning thoughts in a unified framework.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.169",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Can LLMs Facilitate Interpretation of Pre-trained Language Models?": {
        "type": "INPROCEEDINGS",
        "key": "mousi-etal-2023-llms",
        "author": "Mousi, Basel and Durrani, Nadir and Dalvi, Fahim",
        "booktitle": "EMNLP2023",
        "title": "Can LLMs Facilitate Interpretation of Pre-trained Language Models?",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying agglomerative hierarchical clustering over contextualized representations and then annotate these concepts using ChatGPT. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts. Additionally, we showcase how GPT-based annotations empower interpretation analysis methodologies of which we demonstrate two: probing frameworks and neuron interpretation. To facilitate further exploration and experimentation in the field, we make available a substantial ConceptNet dataset (TCN) comprising 39,000 annotated concepts.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.196",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "slobodkin-etal-2023-curious",
        "author": "Slobodkin, Aviv and Goldman, Omer and Caciularu, Avi and Dagan, Ido and Ravfogel, Shauli",
        "booktitle": "EMNLP2023",
        "title": "The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence. In this paper, we explore the behavior of LLMs when presented with (un)answerable queries. We ask: do models represent the fact that the question is (un)answerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particularly in scenarios where query (un)answerability is a concern.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.220",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "ROBBIE: Robust Bias Evaluation of Large Generative Language Models": {
        "type": "INPROCEEDINGS",
        "key": "esiobu-etal-2023-robbie",
        "author": "Esiobu, David and Tan, Xiaoqing and Hosseini, Saghar and Ung, Megan and Zhang, Yuchen and Fernandes, Jude and Dwivedi-Yu, Jane and Presani, Eleonora and Williams, Adina and Smith, Eric",
        "booktitle": "EMNLP2023",
        "title": "ROBBIE: Robust Bias Evaluation of Large Generative Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.230",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media": {
        "type": "INPROCEEDINGS",
        "key": "mittal-etal-2023-lost",
        "author": "Mittal, Shubham and Sundriyal, Megha and Nakov, Preslav",
        "booktitle": "EMNLP2023",
        "title": "Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Claim span identification (CSI) is an important step in fact-checking pipelines, aiming to identify text segments that contain a check-worthy claim or assertion in a social media post. Despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the scarce research on this topic so far has only focused on English. Here we aim to bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K real-world claims collected from numerous social media platforms in five Indian languages and English. We report strong baselines with state-of-the-art encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of training on multiple languages over alternative cross-lingual transfer methods such as zero-shot transfer, or training on translated data, from a high-resource language such as English. We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they underperform the smaller encoder-only language models for low-resource languages.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.236",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation": {
        "type": "INPROCEEDINGS",
        "key": "yin-etal-2023-dynosaur",
        "author": "Yin, Da and Liu, Xiao and Yin, Fan and Zhong, Ming and Bansal, Hritik and Han, Jiawei and Chang, Kai-Wei",
        "booktitle": "EMNLP2023",
        "title": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions. By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform with comparable data sizes); and 3) it supports the continuous improvement of models by generating instruction-tuning data when a new annotated dataset becomes available. We further investigate a continual learning scheme for learning with the ever-growing instruction-tuning dataset, and demonstrate that replaying tasks with diverse instruction embeddings not only helps mitigate forgetting issues but generalizes to unseen tasks better. Code and data are available at https://github.com/WadeYin9712/Dynosaur.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.245",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Large Language Models are Temporal and Causal Reasoners for Video Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "ko-etal-2023-large",
        "author": "Ko, Dohwan and Lee, Ji and Kang, Woo-Young and Roh, Byungseok and Kim, Hyunwoo",
        "booktitle": "EMNLP2023",
        "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting linguistic shortcuts for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, i.e., linguistic bias, while ignoring visual content. This is also known as \u2018ungrounded guesses\u2019 or \u2018hallucinations\u2019. To address this problem while leveraging LLMs\u2019 prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of \u0142angleV, Q, A\u00e5ngle triplet by flipping the source pair and the target label to understand their complex relationships, i.e., predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https://github.com/mlvlab/Flipped-VQA.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.261",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "TrojanSQL: SQL Injection against Natural Language Interface to Database": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-trojansql",
        "author": "Zhang, Jinchuan and Zhou, Yan and Hui, Binyuan and Liu, Yaxin and Li, Ziming and Hu, Songlin",
        "booktitle": "EMNLP2023",
        "title": "TrojanSQL: SQL Injection against Natural Language Interface to Database",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The technology of text-to-SQL has significantly enhanced the efficiency of accessing and manipulating databases. However, limited research has been conducted to study its vulnerabilities emerging from malicious user interaction. By proposing TrojanSQL, a backdoor-based SQL injection framework for text-to-SQL systems, we show how state-of-the-art text-to-SQL parsers can be easily misled to produce harmful SQL statements that can invalidate user queries or compromise sensitive information about the database. The study explores two specific injection attacks, namely boolean-based injection and union-based injection, which use different types of triggers to achieve distinct goals in compromising the parser. Experimental results demonstrate that both medium-sized models based on fine-tuning and LLM-based parsers using prompting techniques are vulnerable to this type of attack, with attack success rates as high as 99% and 89%, respectively. We hope that this study will raise more concerns about the potential security risks of building natural language interfaces to databases.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.264",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Merging Generated and Retrieved Knowledge for Open-Domain QA": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-merging",
        "author": "Zhang, Yunxiang and Khalifa, Muhammad and Logeswaran, Lajanugen and Lee, Moontae and Lee, Honglak and Wang, Lu",
        "booktitle": "EMNLP2023",
        "title": "Merging Generated and Retrieved Knowledge for Open-Domain QA",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Open-domain question answering (QA) systems are often built with retrieval modules. However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage. Alternatively, prompting large language models (LLMs) to generate contextual passages based on their parametric knowledge has been shown to improve QA performance. Yet, LLMs tend to \u201challucinate\u201d content that conflicts with the retrieved knowledge. Based on the intuition that answers supported by both sources are more likely to be correct, we propose COMBO, a Compatibility-Oriented knowledge Merging for Better Open-domain QA framework, to effectively leverage the two sources of information. Concretely, we match LLM-generated passages with retrieved counterparts into compatible pairs, based on discriminators trained with silver compatibility labels. Then a Fusion-in-Decoder-based reader model handles passage pairs to arrive at the final answer. Experiments show that COMBO outperforms competitive baselines on three out of four tested open-domain QA benchmarks. Further analysis reveals that our proposed framework demonstrates greater efficacy in scenarios with a higher degree of knowledge conflicts.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.286",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Can We Edit Factual Knowledge by In-Context Learning?": {
        "type": "INPROCEEDINGS",
        "key": "zheng-etal-2023-edit",
        "author": "Zheng, Ce and Li, Lei and Dong, Qingxiu and Fan, Yuxuan and Wu, Zhiyong and Xu, Jingjing and Chang, Baobao",
        "booktitle": "EMNLP2023",
        "title": "Can We Edit Factual Knowledge by In-Context Learning?",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or outdated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/pkunlp-icler/IKE.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.296",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition": {
        "type": "INPROCEEDINGS",
        "key": "schulhoff-etal-2023-ignore",
        "author": "Schulhoff, Sander and Pinto, Jeremy and Khan, Anaum and Bouchard, Louis-Fran\u00e7ois and Si, Chenglei and Anati, Svetlina and Tagliabue, Valen and Kost, Anson and Carnahan, Christopher and Boyd-Graber, Jordan",
        "booktitle": "EMNLP2023",
        "title": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) are increasingly being deployed in interactive contexts that involve direct user engagement, such as chatbots and writing assistants. These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of a large-scale resource and quantitative study on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive ontology of the types of adversarial prompts.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.302",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers": {
        "type": "INPROCEEDINGS",
        "key": "olausson-etal-2023-linc",
        "author": "Olausson, Theo and Gu, Alex and Lipkin, Ben and Zhang, Cedegao and Solar-Lezama, Armando and Tenenbaum, Joshua and Levy, Roger",
        "booktitle": "EMNLP2023",
        "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.313",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation": {
        "type": "INPROCEEDINGS",
        "key": "sahu-etal-2023-promptmix",
        "author": "Sahu, Gaurav and Vechtomova, Olga and Bahdanau, Dzmitry and Laradji, Issam",
        "booktitle": "EMNLP2023",
        "title": "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data. Recent work often tackles this problem using large language models (LLMs) like GPT3 that can generate new examples given already available ones. In this work, we propose a method to generate more helpful augmented data by utilizing the LLM\u2019s abilities to follow instructions and perform few-shot classifications. Our specific PromptMix method consists of two steps: 1) generate challenging text augmentations near class boundaries; however, generating borderline examples increases the risk of false positives in the dataset, so we 2) relabel the text augmentations using a prompting-based LLM classifier to enhance the correctness of labels in the generated data. We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERT-base and BERT-base. Furthermore, 2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the four datasets. Our code is available at https://github.com/ServiceNow/PromptMix-EMNLP-2023.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.323",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "mekala-etal-2023-zerotop",
        "author": "Mekala, Dheeraj and Wolfe, Jason and Roy, Subhro",
        "booktitle": "EMNLP2023",
        "title": "ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We explore the use of large language models (LLMs) for zero-shot semantic parsing. Semantic parsing involves mapping natural language utterances to task-specific meaning representations. LLMs are generally trained on publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting. In this work, we propose ZEROTOP, a zero-shot task-oriented parsing method that decomposes semantic parsing problem into a set of abstractive and extractive question-answering (QA) problems. For each utterance, we prompt the LLM with questions corresponding to its top-level intent and a set of slots and use the LLM generations to construct the target meaning representation. We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots. We address this by fine-tuning a language model on public QA datasets using synthetic negative samples. Experimental results show that our QA-based decomposition paired with the fine-tuned LLM can zero-shot parse \\approx 16% of utterances in the MTOP dataset.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.354",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction": {
        "type": "INPROCEEDINGS",
        "key": "qi-etal-2023-preserving",
        "author": "Qi, Ji and Zhang, Chuchun and Wang, Xiaozhi and Zeng, Kaisheng and Yu, Jifan and Liu, Jinxin and Hou, Lei and Li, Juanzi and Bin, Xu",
        "booktitle": "EMNLP2023",
        "title": "Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial validation of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a representative large language model, and the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F\u2081 score. Our resources and code will be publicly available.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.360",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2023-beyond",
        "author": "Chen, Liang and Deng, Yang and Bian, Yatao and Qin, Zeyu and Wu, Bingzhe and Chua, Tat-Seng and Wong, Kam-Fai",
        "booktitle": "EMNLP2023",
        "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives \u2013 Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.390",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Can You Follow Me? Testing Situational Understanding for ChatGPT": {
        "type": "INPROCEEDINGS",
        "key": "yang-ettinger-2023-follow",
        "author": "Yang, Chenghao and Ettinger, Allyson",
        "booktitle": "EMNLP2023",
        "title": "Can You Follow Me? Testing Situational Understanding for ChatGPT",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Understanding sentence meanings and updating information states appropriately across time\u2014what we call \u201csituational understanding\u201d (SU)\u2014is a critical ability for human-like AI agents. SU is essential in particular for chat models, such as ChatGPT, to enable consistent, coherent, and effective dialogue between humans and AI. Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs), but the extent and causes of these limitations are not well understood, and capabilities of current chat-based models in this domain have not been explored. In this work we tackle these questions, proposing a novel synthetic environment for SU testing which allows us to do controlled and systematic testing of SU in chat-oriented models, through assessment of models\u2019 ability to track and enumerate environment states. Our environment also allows for close analysis of dynamics of model performance, to better understand underlying causes for performance patterns. We apply our test to ChatGPT, the state-of-the-art chatbot, and find that despite the fundamental simplicity of the task, the model\u2019s performance reflects an inability to retain correct environment states across time. Our follow-up analyses suggest that performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to hallucinated updates\u2014including updates that artificially inflate accuracies. Our findings suggest overall that ChatGPT is not currently equipped for robust tracking of situation states, and that trust in the impressive dialogue performance of ChatGPT comes with risks. We release the codebase for reproducing our test environment, as well as all prompts and API responses from ChatGPT, at https://github.com/yangalan123/SituationalTesting.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.394",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2023-personalized",
        "author": "Chen, Hailin and Saha, Amrita and Hoi, Steven and Joty, Shafiq",
        "booktitle": "EMNLP2023",
        "title": "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher\u2019s prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K personalised examples that incur a data-collection cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to achieve 45.8% pass@1 on HumanEval.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.417",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "jain-etal-2023-language-models",
        "author": "Jain, Raghav and Sojitra, Daivik and Acharya, Arkadeep and Saha, Sriparna and Jatowt, Adam and Dandapat, Sandipan",
        "booktitle": "EMNLP2023",
        "title": "Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Temporal reasoning represents a vital component of human communication and understanding, yet remains an underexplored area within the context of Large Language Models (LLMs). Despite LLMs demonstrating significant proficiency in a range of tasks, a comprehensive, large-scale analysis of their temporal reasoning capabilities is missing. Our paper addresses this gap, presenting the first extensive benchmarking of LLMs on temporal reasoning tasks. We critically evaluate 8 different LLMs across 6 datasets using 3 distinct prompting strategies. Additionally, we broaden the scope of our evaluation by including in our analysis 2 Code Generation LMs. Beyond broad benchmarking of models and prompts, we also conduct a fine-grained investigation of performance across different categories of temporal tasks. We further analyze the LLMs on varying temporal aspects, offering insights into their proficiency in understanding and predicting the continuity, sequence, and progression of events over time. Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning, offering a comprehensive reference for future research in this pivotal domain.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.418",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning": {
        "type": "INPROCEEDINGS",
        "key": "das-etal-2023-unified",
        "author": "Das, Sarkar Snigdha Sarathi and Zhang, Ranran Haoran and Shi, Peng and Yin, Wenpeng and Zhang, Rui",
        "booktitle": "EMNLP2023",
        "title": "Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Unified Sequence Labeling that articulates different sequence labeling problems such as Named Entity Recognition, Relation Extraction, Semantic Role Labeling, etc. in a generalized sequence-to-sequence format opens up the opportunity to make the maximum utilization of large language model knowledge toward structured prediction. Unfortunately, this requires formatting them into specialized augmented format unknown to the base pretrained language model (PLMs) necessitating finetuning to the target format. This significantly bounds its usefulness in data-limited settings where finetuning large models cannot properly generalize to the target format. To address this challenge and leverage PLM knowledge effectively, we propose FISH-DIP, a sample-aware dynamic sparse finetuning strategy that selectively focuses on a fraction of parameters, informed by feedback from highly regressing examples, during the fine-tuning process. By leveraging the dynamism of sparsity, our approach mitigates the impact of well-learned samples and prioritizes underperforming instances for improvement in generalization. Across five tasks of sequence labeling, we demonstrate that FISH-DIP can smoothly optimize the model in low resource settings offering upto 40% performance improvements over full fine-tuning depending on target evaluation settings. Also, compared to in-context learning and other parameter-efficient fine-tuning approaches, FISH-DIP performs comparably or better, notably in extreme low-resource settings. The source code of FISH-DIP will be available at [this URL](https://github.com/psunlpgroup/FISH-DIP)",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.433",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Benchmarking and Improving Text-to-SQL Generation under Ambiguity": {
        "type": "INPROCEEDINGS",
        "key": "bhaskar-etal-2023-benchmarking",
        "author": "Bhaskar, Adithya and Tomar, Tushar and Sathe, Ashutosh and Sarawagi, Sunita",
        "booktitle": "EMNLP2023",
        "title": "Benchmarking and Improving Text-to-SQL Generation under Ambiguity",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or structural ambiguity. When faced with ambiguity, an ideal top-k decoder should generate all valid interpretations for possible disambiguation by the user. We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal. The primary reason is that the prevalent beam search algorithm and its variants, treat SQL queries as a string and produce unhelpful token-level diversity in the top-k. We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic space using a blend of plan-based template generation and constrained infilling. Counterfactually generated plans diversify templates while in-filling with a beam-search that branches solely on schema names provides value diversity. LogicalBeam is up to 2.5 times more effective than state-of-the-art models at generating all candidate SQLs in the top-k ranked outputs. It also enhances the top-5 Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.436",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies": {
        "type": "INPROCEEDINGS",
        "key": "watson-etal-2023-hiddentables",
        "author": "Watson, William and Cho, Nicole and Balch, Tucker and Veloso, Manuela",
        "booktitle": "EMNLP2023",
        "title": "HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell boundaries, and (3) various limitations stemming from data confidentiality in the process of using external models such as gpt-35-turbo. We propose a cooperative game dubbed \u201cHiddenTables\u201d as a potential resolution to this challenge. In essence, \u201cHiddenTables\u201d is played between the code-generating LLM \u201cSolver\u201d and the \u201cOracle\u201d which evaluates the ability of the LLM agents to solve TableQA tasks. This game is based on natural language schemas and importantly, ensures the security of the underlying data. We provide evidential experiments on a diverse set of tables that demonstrate an LLM\u2019s collective inability to generalize and perform on complex queries, handle compositional dependencies, and align natural language to programmatic commands when concrete table schemas are provided. Unlike encoder-based models, we have pushed the boundaries of \u201cHiddenTables\u201d to not be limited by the number of rows - therefore we exhibit improved efficiency in prompt and completion tokens. Our infrastructure has spawned a new dataset \u201cPyQTax\u201d that spans across 116,671 question-table-answer triplets and provides additional fine-grained breakdowns and labels for varying question taxonomies. Therefore, in tandem with our academic contributions regarding LLMs\u2019 deficiency in TableQA tasks, \u201cHiddenTables\u201d is a tactile manifestation of how LLMs can interact with massive datasets while ensuring data security and minimizing generation costs.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.442",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Copyright Violations and Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "karamolegkou-etal-2023-copyright",
        "author": "Karamolegkou, Antonia and Li, Jiaang and Zhou, Li and S\u00f8gaard, Anders",
        "booktitle": "EMNLP2023",
        "title": "Copyright Violations and Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Language models may memorize more than just facts, including entire chunks of texts seen during training. Fair use exemptions to copyright laws typically allow for limited use of copyrighted material without permission from the copyright holder, but typically for extraction of information from copyrighted materials, rather than verbatim reproduction. This work explores the issue of copyright violations and large language models through the lens of verbatim memorization, focusing on possible redistribution of copyrighted text. We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials. Overall, this research highlights the need for further examination and the potential impact on future developments in natural language processing to ensure adherence to copyright regulations. Code is at https://github.com/coastalcph/CopyrightLLMs.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.458",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Symbolic Planning and Code Generation for Grounded Dialogue": {
        "type": "INPROCEEDINGS",
        "key": "chiu-etal-2023-symbolic-planning",
        "author": "Chiu, Justin and Zhao, Wenting and Chen, Derek and Vaduguru, Saujas and Rush, Alexander and Fried, Daniel",
        "booktitle": "EMNLP2023",
        "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) excel at processing and generating text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system, consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code\u2019s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system\u2019s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.460",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT": {
        "type": "INPROCEEDINGS",
        "key": "zhu-etal-2023-beat",
        "author": "Zhu, Biru and Yuan, Lifan and Cui, Ganqu and Chen, Yangyi and Fu, Chong and He, Bingxiang and Deng, Yangdong and Liu, Zhiyuan and Sun, Maosong and Gu, Ming",
        "booktitle": "EMNLP2023",
        "title": "Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs), e.g., ChatGPT, have revolutionized the domain of natural language processing because of their excellent performance on various tasks. Despite their great potential, LLMs also incur serious concerns as they are likely to be misused. There are already reported cases of academic cheating by using LLMs. Thus, it is a pressing problem to identify LLM-generated texts. In this work, we design a zero-shot black-box method for detecting LLM-generated texts. The key idea is to revise the text to be detected using the ChatGPT model. Our method is based on the intuition that the ChatGPT model will make fewer revisions to LLM-generated texts than it does to human-written texts, because the texts generated by LLMs are more in accord with the generation logic and statistical patterns learned by LLMs like ChatGPT. Thus, if the text to be detected and its ChatGPT-revised version have a higher degree of similarity, the text is more likely to be LLM-generated. Extensive experiments on various datasets and tasks show that our method can effectively detect LLM-generated texts. Moreover, compared with other detection methods, our method has better generalization ability and is more stable across various datasets. The codes are publicly available at https://github.com/thunlp/LLM-generated-text-detection.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.463",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables": {
        "type": "INPROCEEDINGS",
        "key": "lu-etal-2023-scitab",
        "author": "Lu, Xinyuan and Pan, Liangming and Liu, Qian and Nakov, Preslav and Kan, Min-Yen",
        "booktitle": "EMNLP2023",
        "title": "SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.483",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization": {
        "type": "INPROCEEDINGS",
        "key": "che-etal-2023-federated",
        "author": "Che, Tianshi and Liu, Ji and Zhou, Yang and Ren, Jiaxiang and Zhou, Jiwen and Sheng, Victor and Dai, Huaiyu and Dou, Dejing",
        "booktitle": "EMNLP2023",
        "title": "Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improve performance and efficiency simultaneously. Second, a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further. Extensive experiments based on 10 datasets demonstrate the superb performance (up to 60.8% in terms of accuracy) and efficiency (up to 97.59% in terms of training time) of FedPepTAO compared with 9 baseline approaches. Our code is available at https://github.com/llm-eff/FedPepTAO.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.488",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "TheoremQA: A Theorem-driven Question Answering Dataset": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2023-theoremqa",
        "author": "Chen, Wenhu and Yin, Ming and Ku, Max and Lu, Pan and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Wang, Xinyi and Xia, Tony",
        "booktitle": "EMNLP2023",
        "title": "TheoremQA: A Theorem-driven Question Answering Dataset",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models\u2019 capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&amp;CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4\u2019s capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15%, barely surpassing the random-guess baseline. Given the diversity and broad coverage of TheoremQA, we believe it can be used as a better benchmark to evaluate LLMs\u2019 capabilities to solve challenging science problems.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.489",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Generating Data for Symbolic Language with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ye-etal-2023-generating",
        "author": "Ye, Jiacheng and Li, Chengzu and Kong, Lingpeng and Yu, Tao",
        "booktitle": "EMNLP2023",
        "title": "Generating Data for Symbolic Language with Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks, and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort. SymGen takes a step toward data generation for annotation-expensive complex tasks, and we release the code at URL.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.523",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "DALE: Generative Data Augmentation for Low-Resource Legal NLP": {
        "type": "INPROCEEDINGS",
        "key": "ghosh-etal-2023-dale",
        "author": "Ghosh, Sreyan and Evuru, Chandra Kiran Reddy and Kumar, Sonal and Ramaneswaran, S. and Sakshi, S. and Tyagi, Utkarsh and Manocha, Dinesh",
        "booktitle": "EMNLP2023",
        "title": "DALE: Generative Data Augmentation for Low-Resource Legal NLP",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present DALE, a novel and effective generative Data Augmentation framework for low-resource LEgal NLP. DALE addresses the challenges existing frameworks pose in generating effective data augmentations of legal documents - legal language, with its specialized vocabulary and complex semantics, morphology, and syntax, does not benefit from data augmentations that merely rephrase the source sentence. To address this, DALE, built on an Encoder-Decoder Language Model, is pre-trained on a novel unsupervised text denoising objective based on selective masking - our masking strategy exploits the domain-specific language characteristics of templatized legal documents to mask collocated spans of text. Denoising these spans help DALE acquire broad legal knowledge and develop the ability to generate coherent and diverse augmentations with novel contexts. Finally, DALE performs conditional generation to generate synthetic augmentations for low-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13 datasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our baselines, including LLMs, qualitatively and quantitatively, with absolute improvements of 1%-50%.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.528",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "garcia-ferrero-etal-2023-dataset",
        "author": "Garc\u00eda-Ferrero, Iker and Altuna, Bego\u00f1a and Alvez, Javier and Gonzalez-Dios, Itziar and Rigau, German",
        "booktitle": "EMNLP2023",
        "title": "This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.531",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "SOUL: Towards Sentiment and Opinion Understanding of Language": {
        "type": "INPROCEEDINGS",
        "key": "deng-etal-2023-soul",
        "author": "Deng, Yue and Zhang, Wenxuan and Pan, Sinno and Bing, Lidong",
        "booktitle": "EMNLP2023",
        "title": "SOUL: Towards Sentiment and Opinion Understanding of Language",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Sentiment analysis is a well-established natural language processing task, with sentiment polarity classification being one of its most popular and representative tasks. However, despite the success of pre-trained language models in this area, they often fall short of capturing the broader complexities of sentiment analysis. To address this issue, we propose a new task called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims to evaluate sentiment understanding through two subtasks: Review Comprehension (RC) and Justification Generation (JG). RC seeks to validate statements that focus on subjective information based on a review text, while JG requires models to provide explanations for their sentiment predictions. To enable comprehensive evaluation, we annotate a new dataset comprising 15,028 statements from 3,638 reviews. Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications. These findings underscore the challenging nature of the SOUL task for existing models, emphasizing the need for further advancements in sentiment analysis to address its complexities. The new dataset and code are available at https://github.com/DAMO-NLP-SG/SOUL.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.538",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Detecting and Mitigating Hallucinations in Multilingual Summarisation": {
        "type": "INPROCEEDINGS",
        "key": "qiu-etal-2023-detecting",
        "author": "Qiu, Yifu and Ziser, Yftah and Korhonen, Anna and Ponti, Edoardo and Cohen, Shay",
        "booktitle": "EMNLP2023",
        "title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource languages, where summarisation requires cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. Through extensive experiments in multiple languages, we demonstrate that mFACT is best suited to detect hallucinations compared to alternative metrics. With mFACT, we assess a broad range of multilingual large language models, and find that they all tend to hallucinate often in languages different from English. We then propose a simple but effective method to reduce hallucinations in cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. This method drastically increases both performance and faithfulness according to both automatic and human evaluation when compared to strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset are available at https://github.com/yfqiu-nlp/mfact-summ.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.551",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration": {
        "type": "INPROCEEDINGS",
        "key": "wan-etal-2023-explore",
        "author": "Wan, Fanqi and Huang, Xinting and Yang, Tao and Quan, Xiaojun and Bi, Wei and Shi, Shuming",
        "booktitle": "EMNLP2023",
        "title": "Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas. To address this deficiency, we propose Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs). Built upon representative domain use cases, Explore-Instruct explores a multitude of variations or possibilities by implementing a search algorithm to obtain diversified and domain-focused instruction-tuning data. Our data-centric analysis validates the effectiveness of this proposed approach in improving domain-specific instruction coverage. Moreover, our model\u2019s performance demonstrates considerable advancements over multiple baselines, including those utilizing domain-specific data enhancement. Our findings offer a promising opportunity to improve instruction coverage, especially in domain-specific contexts, thereby advancing the development of adaptable language models. Our code, model weights, and data are public at https://github.com/fanqiwan/Explore-Instruct.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.587",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Just Adjust One Prompt: Enhancing In-Context Dialogue Scoring via Constructing the Optimal Subgraph of Demonstrations and Prompts": {
        "type": "INPROCEEDINGS",
        "key": "pu-etal-2023-just",
        "author": "Pu, Jiashu and Cheng, Ling and Fan, Lu and Lv, Tangjie and Zhang, Rongsheng",
        "booktitle": "EMNLP2023",
        "title": "Just Adjust One Prompt: Enhancing In-Context Dialogue Scoring via Constructing the Optimal Subgraph of Demonstrations and Prompts",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The use of modern Large Language Models (LLMs) as chatbots still has some problems such as hallucinations and lack of empathy. Identifying these issues can help improve chatbot performance. The community has been continually iterating on reference-free dialogue evaluation methods based on large language models (LLMs) that can be readily applied. However, many of these LLM-based metrics require selecting specific datasets and developing specialized training tasks for different evaluation dimensions (e.g., coherence, informative). The developing step can be time-consuming and may need to be repeated for new evaluation dimensions. To enable efficient and flexible adaptation to diverse needs of dialogue evaluation, we propose a dimension-agnostic scoring method that leverages the in-context learning (ICL) capability of LLMs to learn from human scoring to the fullest extent. Our method has three key features. To begin with, rather than manual prompt crafting, we propose automatically generating prompts, allowing the LLM to observe human labels and summarize the most suitable prompt. Additionally, since the LLM has a token limit and ICL is sensitive to demonstration variations, we train a selector to finely customize demonstrations and prompts for each dialogue input. Finally, during inference, we propose to request the LLM multiple times with a subgraph of demonstrations and prompts that are diverse and suitable to maximize ICL from various human scoring. We validate the efficacy of our method on five datasets, even with a small amount of annotated data, our method outperforms all strong baselines. Code is available at https://github.com/iamlxb3/EMNLP2023-ADOROR.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.590",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "EpiK-Eval: Evaluation for Language Models as Epistemic Models": {
        "type": "INPROCEEDINGS",
        "key": "prato-etal-2023-epik",
        "author": "Prato, Gabriele and Huang, Jerry and Parthasarathi, Prasanna and Sodhani, Shagun and Chandar, Sarath",
        "booktitle": "EMNLP2023",
        "title": "EpiK-Eval: Evaluation for Language Models as Epistemic Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents\u2014a crucial ability in numerous applications\u2014remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs\u2019 proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from this study offer insights for developing more robust and reliable LLMs. Our code and benchmark are available at https://github.com/chandar-lab/EpiK-Eval",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.593",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-revisiting",
        "author": "Zhang, Cheng and Cheng, Jianyi and Shumailov, Ilia and Constantinides, George and Zhao, Yiren",
        "booktitle": "EMNLP2023",
        "title": "Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The inference of Large language models (LLMs) requires immense computation and memory resources. To curtail these costs, quantisation has emerged as a promising solution, but existing LLM quantisation mainly focuses on 8-bit. In this work, we explore the statistical and learning properties of the LLM layer and attribute the bottleneck of LLM quantisation to numerical scaling offsets. To address this, we adapt block quantisations for LLMs, a family of methods that share scaling factors across packed numbers. Block quantisations efficiently reduce the numerical scaling offsets solely from an arithmetic perspective, without additional treatments in the computational path. Our nearly-lossless quantised 6-bit LLMs achieve a 19\\times higher arithmetic density and 5\\times memory density than the float32 baseline, surpassing the prior art 8-bit quantisation by 2.5\\times in arithmetic density and 1.2\\times in memory density, without requiring any data calibration or re-training. We also share our insights into sub-8-bit LLM quantisation, including the mismatch between activation and weight distributions, optimal fine-tuning strategies, and a lower quantisation granularity inherent in the statistical properties of LLMs. The latter two tricks enable nearly-lossless 4-bit LLMs on downstream tasks. Our code is open-sourced.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.617",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Expand, Highlight, Generate: RL-driven Document Generation for Passage Reranking": {
        "type": "INPROCEEDINGS",
        "key": "askari-etal-2023-expand",
        "author": "Askari, Arian and Aliannejadi, Mohammad and Meng, Chuan and Kanoulas, Evangelos and Verberne, Suzan",
        "booktitle": "EMNLP2023",
        "title": "Expand, Highlight, Generate: RL-driven Document Generation for Passage Reranking",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Generating synthetic training data based on large language models (LLMs) for ranking models has gained attention recently. Prior studies use LLMs to build pseudo query-document pairs by generating synthetic queries from documents in a corpus. In this paper, we propose a new perspective of data augmentation: generating synthetic documents from queries. To achieve this, we propose DocGen, that consists of a three-step pipeline that utilizes the few-shot capabilities of LLMs. DocGen pipeline performs synthetic document generation by (i) expanding, (ii) highlighting the original query, and then (iii) generating a synthetic document that is likely to be relevant to the query. To further improve the relevance between generated synthetic documents and their corresponding queries, we propose DocGen-RL, which regards the estimated relevance of the document as a reward and leverages reinforcement learning (RL) to optimize DocGen pipeline. Extensive experiments demonstrate that DocGen pipeline and DocGen-RL significantly outperform existing state-of-theart data augmentation methods, such as InPars, indicating that our new perspective of generating documents leverages the capacity of LLMs in generating synthetic data more effectively. We release the code, generated data, and model checkpoints to foster research in this area.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.623",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning": {
        "type": "INPROCEEDINGS",
        "key": "geng-etal-2023-grammar",
        "author": "Geng, Saibo and Josifoski, Martin and Peyrard, Maxime and West, Robert",
        "booktitle": "EMNLP2023",
        "title": "Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly. To address this issue, grammar-constrained decoding (GCD) can be used to control the generation of LMs, guaranteeing that the output follows a given structure. Most existing GCD methods are, however, limited to specific tasks, such as parsing or code generation. In this work, we demonstrate that formal grammars can describe the output space for a much wider range of tasks and argue that GCD can serve as a unified framework for structured NLP tasks in general. For increased flexibility, we introduce input-dependent grammars, which allow the grammar to depend on the input and thus enable the generation of different output structures for different inputs. We then empirically demonstrate the power and flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity disambiguation, and (3) constituency parsing. Our results indicate that grammar-constrained LMs substantially outperform unconstrained LMs or even beat task-specific finetuned models. Grammar constraints thus hold great promise for harnessing off-the-shelf LMs for a wide range of structured NLP tasks, especially where training data is scarce or finetuning is expensive. Code and data: https://github.com/epfl-dlab/GCD.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.674",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Axiomatic Preference Modeling for Longform Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "rosset-etal-2023-axiomatic",
        "author": "Rosset, Corby and Zheng, Guoqing and Dibia, Victor and Awadallah, Ahmed and Bennett, Paul",
        "booktitle": "EMNLP2023",
        "title": "Axiomatic Preference Modeling for Longform Question Answering",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The remarkable abilities of large language models (LLMs) like ChatGPT and GPT-4 partially stem from the post-training processes involving human preferences encoded within a reward model as part of a Reinforcement Learning from Human Feedback (RLHF) regimen. These reward models (RMs) often lack direct knowledge of why, or under what principles, the preferences annotations were made. In this study, we identify principles that guide RMs to better align with human preferences, and then develop an axiomatic framework to generate a rich variety of preference signals to uphold them. We use these axiomatic signals to train a model for the scoring answers to longform questions. Our approach yields a Preference Model with only about 220M parameters that agrees with gold human-annotated preference labels more often than GPT-4. The contributions of this work include: training a standalone preference model that can score human- and LLM-generated answers on the same scale; developing an axiomatic framework for generating training data pairs tailored to certain principles; and showing that a small amount of axiomatic signals can help small models outperform GPT-4 in preference scoring. We intend to release our axiomatic data and model.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.702",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs": {
        "type": "INPROCEEDINGS",
        "key": "aksu-etal-2023-cesar",
        "author": "Aksu, Taha and Hazarika, Devamanyu and Mehri, Shikib and Kim, Seokhwan and Hakkani-Tur, Dilek and Liu, Yang and Namazifar, Mahdi",
        "booktitle": "EMNLP2023",
        "title": "CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Instruction-based multitasking has played a critical role in the success of large language models (LLMs) in multi-turn dialog applications. While publicly available LLMs have shown promising performance, when exposed to complex instructions with multiple constraints, they lag against state-of-the-art models like ChatGPT. In this work, we hypothesize that the availability of large-scale complex demonstrations is crucial in bridging this gap. Focusing on dialog applications, we propose a novel framework, CESAR, that unifies a large number of dialog tasks in the same format and allows programmatic induction of complex instructions without any manual effort. We apply CESAR on InstructDial, a benchmark for instruction-based dialog tasks. We further enhance InstructDial with new datasets and tasks and utilize CESAR to induce complex tasks with compositional instructions. This results in a new benchmark called InstructDial++, which includes 63 datasets with 86 basic tasks and 68 composite tasks. Through rigorous experiments, we demonstrate the scalability of CESAR in providing rich instructions. Models trained on InstructDial++ can follow compositional prompts, such as prompts that ask for multiple stylistic constraints.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.717",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "kim-etal-2023-visually",
        "author": "Kim, Geewook and Lee, Hodong and Kim, Daehee and Jung, Haeji and Park, Sanghee and Kim, Yoonsik and Yun, Sangdoo and Kil, Taeho and Lee, Bado and Park, Seunghyun",
        "booktitle": "EMNLP2023",
        "title": "Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-situated language understanding tasks that demand reasoning capabilities, we demonstrate the compelling performance of Cream, positioning it as a prominent model in the field of visual document understanding. We provide our codebase and newly-generated datasets at https://github.com/naver-ai/cream.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.735",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "jeoung-etal-2023-stereomap",
        "author": "Jeoung, Sullam and Ge, Yubin and Diesner, Jana",
        "booktitle": "EMNLP2023",
        "title": "StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as the factors that delineate the nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs\u2019 perceptions of social groups (defined by socio-demographic features) using the dimensions of Warmth and Competence. Furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of LLMs\u2019 judgments to uncover underlying factors influencing their perceptions. Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of Warmth and Competence. Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning. This study contributes to the understanding of how LLMs perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.752",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation": {
        "type": "INPROCEEDINGS",
        "key": "cheng-etal-2023-uprise",
        "author": "Cheng, Daixuan and Huang, Shaohan and Bi, Junyu and Zhan, Yuefeng and Liu, Jianfeng and Wang, Yujing and Sun, Hao and Wei, Furu and Deng, Weiwei and Zhang, Qi",
        "booktitle": "EMNLP2023",
        "title": "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on diverse tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs. Our model and code are available at https://github.com/microsoft/LMOps.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.758",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Let\u2019s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs": {
        "type": "INPROCEEDINGS",
        "key": "aggarwal-etal-2023-lets",
        "author": "Aggarwal, Pranjal and Madaan, Aman and Yang, Yiming and Mausam",
        "booktitle": "EMNLP2023",
        "title": "Let\u2019s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.761",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Multilingual Large Language Models Are Not (Yet) Code-Switchers": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-multilingual",
        "author": "Zhang, Ruochen and Cahyawijaya, Samuel and Cruz, Jan Christian Blaise and Winata, Genta and Aji, Alham Fikri",
        "booktitle": "EMNLP2023",
        "title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current \u201cmultilingualism\u2019 in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.774",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Explaining Interactions Between Text Spans": {
        "type": "INPROCEEDINGS",
        "key": "choudhury-etal-2023-explaining",
        "author": "Ray Choudhury, Sagnik and Atanasova, Pepa and Augenstein, Isabelle",
        "booktitle": "EMNLP2023",
        "title": "Explaining Interactions Between Text Spans",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on identifying individual important features or interactions only between adjacent tokens or tuples of tokens. Most notably, there is a lack of annotations capturing the human decision-making process with respect to the necessary interactions for informed decision-making in such tasks. To bridge this gap, we introduce SpanEx, a multi-annotator dataset of human span interaction explanations for two NLU tasks: NLI and FC. We then investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes. Finally, we present a novel community detection based unsupervised method to extract such interaction explanations. We make the code and the dataset available on [Github](https://github.com/copenlu/spanex). The dataset is also available on [Huggingface datasets](https://huggingface.co/datasets/copenlu/spanex).",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.783",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Question Answering as Programming for Solving Time-Sensitive Questions": {
        "type": "INPROCEEDINGS",
        "key": "zhu-etal-2023-question",
        "author": "Zhu, Xinyu and Yang, Cheng and Chen, Bei and Li, Siheng and Lou, Jian-Guang and Yang, Yujiu",
        "booktitle": "EMNLP2023",
        "title": "Question Answering as Programming for Solving Time-Sensitive Questions",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs\u2019 inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the Question Answering task as Programming (QAaP). Concretely, by leveraging modern LLMs\u2019 superior capability in understanding both natural language and programming language, we endeavor to harness LLMs to represent diversely expressed text as well-structured code and select the best matching answer from multiple candidates through programming. We evaluate our QAaP framework on several time-sensitive question answering datasets and achieve decent improvement, up to 14.5% over strong baselines.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.787",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Context Compression for Auto-regressive Transformers with Sentinel Tokens": {
        "type": "INPROCEEDINGS",
        "key": "ren-etal-2023-context",
        "author": "Ren, Siyu and Jia, Qi and Zhu, Kenny",
        "booktitle": "EMNLP2023",
        "title": "Context Compression for Auto-regressive Transformers with Sentinel Tokens",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.794",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization": {
        "type": "INPROCEEDINGS",
        "key": "kim-etal-2023-soda",
        "author": "Kim, Hyunwoo and Hessel, Jack and Jiang, Liwei and West, Peter and Lu, Ximing and Yu, Youngjae and Zhou, Pei and Bras, Ronan and Alikhani, Malihe and Kim, Gunhee and Sap, Maarten and Choi, Yejin",
        "booktitle": "EMNLP2023",
        "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We plan to make our data, model, and code public.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.799",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "NameGuess: Column Name Expansion for Tabular Data": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-nameguess",
        "author": "Zhang, Jiani and Shen, Zhengyuan and Srinivasan, Balasubramaniam and Wang, Shen and Rangwala, Huzefa and Karypis, George",
        "booktitle": "EMNLP2023",
        "title": "NameGuess: Column Name Expansion for Tabular Data",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advances in large language models have revolutionized many sectors, including the database industry. One common challenge when dealing with large volumes of tabular data is the pervasive use of abbreviated column names, which can negatively impact performance on various data search, access, and understanding tasks. To address this issue, we introduce a new task, called NameGuess, to expand column names (used in database schema) as a natural language generation problem. We create a training dataset of 384K abbreviated-expanded column pairs using a new data fabrication method and a human-annotated evaluation benchmark that includes 9.2K examples from real-world tables. To tackle the complexities associated with polysemy and ambiguity in NameGuess, we enhance auto-regressive language models by conditioning on table content and column header names \u2013 yielding a fine-tuned model (with 2.7B parameters) that matches human performance. Furthermore, we conduct a comprehensive analysis (on multiple LLMs) to validate the effectiveness of table content in NameGuess and identify promising future opportunities. Code has been made available at https://github.com/amazon-science/nameguess.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.820",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA": {
        "type": "INPROCEEDINGS",
        "key": "tonglet-etal-2023-seer",
        "author": "Tonglet, Jonathan and Reusens, Manon and Borchert, Philipp and Baesens, Bart",
        "booktitle": "EMNLP2023",
        "title": "SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Question answering over hybrid contexts is a complex task, which requires the combination of information extracted from unstructured texts and structured tables in various ways. Recently, In-Context Learning demonstrated significant performance advances for reasoning tasks. In this paradigm, a large language model performs predictions based on a small set of supporting exemplars. The performance of In-Context Learning depends heavily on the selection procedure of the supporting exemplars, particularly in the case of HybridQA, where considering the diversity of reasoning chains and the large size of the hybrid contexts becomes crucial. In this work, we present Selection of ExEmplars for hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that is both representative and diverse. The key novelty of SEER is that it formulates exemplar selection as a Knapsack Integer Linear Program. The Knapsack framework provides the flexibility to incorporate diversity constraints that prioritize exemplars with desirable attributes, and capacity constraints that ensure that the prompt size respects the provided capacity budgets. The effectiveness of SEER is demonstrated on FinQA and TAT-QA, two real-world benchmarks for HybridQA, where it outperforms previous exemplar selection methods.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.837",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Empower Nested Boolean Logic via Self-Supervised Curriculum Learning": {
        "type": "INPROCEEDINGS",
        "key": "wu-etal-2023-empower",
        "author": "Wu, Hongqiu and Liu, Linfeng and Zhao, Hai and Zhang, Min",
        "booktitle": "EMNLP2023",
        "title": "Empower Nested Boolean Logic via Self-Supervised Curriculum Learning",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Beyond the great cognitive powers showcased by language models, it is crucial to scrutinize whether their reasoning capabilities stem from strong generalization or merely exposure to relevant data. As opposed to constructing increasingly complex logic, this paper probes into the boolean logic, the root capability of a logical reasoner. We find that any pre-trained language models even including large language models only behave like a random selector in the face of multi-nested boolean logic, a task that humans can handle with ease. To empower language models with this fundamental capability, this paper proposes a new self-supervised learning method Curriculum Logical Reasoning (Clr), where we augment the training data with nested boolean logic chain step-by-step, and program the training from simpler logical patterns gradually to harder ones. This new training paradigm allows language models to effectively generalize to much harder and longer-hop logic, which can hardly be learned through naive training. Furthermore, we show that boolean logic is a great foundation for improving the subsequent general logical tasks.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.847",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL": {
        "type": "INPROCEEDINGS",
        "key": "kothyari-etal-2023-crush4sql",
        "author": "Kothyari, Mayank and Dhingra, Dhruva and Sarawagi, Sunita and Chakrabarti, Soumen",
        "booktitle": "EMNLP2023",
        "title": "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Existing Text-to-SQL generators require the entire schema to be encoded with the user text. This is expensive or impractical for large databases with tens of thousands of columns. Standard dense retrieval techniques are inadequate for schema subsetting of a large structured database, where the correct semantics of retrieval demands that we rank sets of schema elements rather than individual documents. In response, we propose a two-stage process for effective coverage during retrieval. First, we use an LLM to hallucinate a minimal DB schema that it deems adequate to answer the query. We use the hallucinated schema to retrieve a subset of the actual schema, by composing the results from multiple dense retrievals. Remarkably, hallucination \u2014 generally considered a nuisance \u2014 turns out to be actually useful as a bridging mechanism. Since no existing benchmarks exist for schema subsetting on large databases, we introduce two benchmarks: (1) A semi-synthetic dataset of 4502 schema elements, by taking a union of schema on the well-known SPIDER dataset, and (2) A real-life benchmark called SocialDB sourced from an actual large data warehouse comprising of 17844 schema elements. We show that our method leads to significantly higher recall than SOTA retrieval-based augmentation methods.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.868",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation": {
        "type": "INPROCEEDINGS",
        "key": "lucas-etal-2023-fighting",
        "author": "Lucas, Jason and Uchendu, Adaku and Yamashita, Michiharu and Lee, Jooyoung and Rohatgi, Shaurya and Lee, Dongwon",
        "booktitle": "EMNLP2023",
        "title": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (*.i.e, generating large-scale harmful and misleading content*). To combat this emerging risk of LLMs, we propose a novel \u201c***Fighting Fire with Fire***\u201d (F3) strategy that harnesses modern LLMs\u2019 generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. In our extensive experiments, we observe GPT-3.5-turbo\u2019s zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. Our codebase and dataset are available at https://github.com/mickeymst/F3.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.883",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "API-Assisted Code Generation for Question Answering on Varied Table Structures": {
        "type": "INPROCEEDINGS",
        "key": "cao-etal-2023-api",
        "author": "Cao, Yihan and Chen, Shuyi and Liu, Ryan and Wang, Zhiruo and Fried, Daniel",
        "booktitle": "EMNLP2023",
        "title": "API-Assisted Code Generation for Question Answering on Varied Table Structures",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures \u2014 relational, multi-table, and hierarchical matrix shapes \u2014 and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.897",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Outlier Dimensions Encode Task Specific Knowledge": {
        "type": "INPROCEEDINGS",
        "key": "rudman-etal-2023-outlier",
        "author": "Rudman, William and Chen, Catherine and Eickhoff, Carsten",
        "booktitle": "EMNLP2023",
        "title": "Outlier Dimensions Encode Task Specific Knowledge",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in fine-tuned models and 2) a single outlier dimension can complete downstream tasks with a minimal error rate. Our results suggest that outlier dimensions can encode crucial task-specific knowledge and that the value of a representation in a single outlier dimension drives downstream model decisions.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.901",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "A Simple Baseline for Knowledge-Based Visual Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "xenos-etal-2023-simple",
        "author": "Xenos, Alexandros and Stafylakis, Themos and Patras, Ioannis and Tzimiropoulos, Georgios",
        "booktitle": "EMNLP2023",
        "title": "A Simple Baseline for Knowledge-Based Visual Question Answering",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper is on the problem of Knowledge-Based Visual Question Answering (KB-VQA). Recent works have emphasized the significance of incorporating both explicit (through external databases) and implicit (through LLMs) knowledge to answer questions requiring external knowledge effectively. A common limitation of such approaches is that they consist of relatively complicated pipelines and often heavily rely on accessing GPT-3 API. Our main contribution in this paper is to propose a much simpler and readily reproducible pipeline which, in a nutshell, is based on efficient in-context learning by prompting LLaMA (1 and 2) using question-informative captions as contextual information. Contrary to recent approaches, our method is training-free, does not require access to external databases or APIs, and yet achieves state-of-the-art accuracy on the OK-VQA and A-OK-VQA datasets. Finally, we perform several ablation studies to understand important aspects of our method. Our code is publicly available at https://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.919",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents": {
        "type": "INPROCEEDINGS",
        "key": "sun-etal-2023-chatgpt",
        "author": "Sun, Weiwei and Yan, Lingyong and Ma, Xinyu and Wang, Shuaiqiang and Ren, Pengjie and Chen, Zhumin and Yin, Dawei and Ren, Zhaochun",
        "booktitle": "EMNLP2023",
        "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model\u2019s ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.923",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "It Ain\u2019t Over: A Multi-aspect Diverse Math Word Problem Dataset": {
        "type": "INPROCEEDINGS",
        "key": "kim-etal-2023-aint",
        "author": "Kim, Jiwoo and Kim, Youngbin and Baek, Ilwoong and Bak, JinYeong and Lee, Jongwuk",
        "booktitle": "EMNLP2023",
        "title": "It Ain\u2019t Over: A Multi-aspect Diverse Math Word Problem Dataset",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The math word problem (MWP) is a complex task that requires natural language understanding and logical reasoning to extract key knowledge from natural language narratives. Previous studies have provided various MWP datasets but lack diversity in problem types, lexical usage patterns, languages, and annotations for intermediate solutions. To address these limitations, we introduce a new MWP dataset, named DMath (Diverse Math Word Problems), offering a wide range of diversity in problem types, lexical usage patterns, languages, and intermediate solutions. The problems are available in English and Korean and include an expression tree and Python code as intermediate solutions. Through extensive experiments, we demonstrate that the DMath dataset provides a new opportunity to evaluate the capability of large language models, i.e., GPT-4 only achieves about 75% accuracy on the DMath dataset.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.927",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Prompting with Pseudo-Code Instructions": {
        "type": "INPROCEEDINGS",
        "key": "mishra-etal-2023-prompting",
        "author": "Mishra, Mayank and Kumar, Prince and Bhat, Riyaz and Murthy, Rudra and Contractor, Danish and Tamilselvam, Srikanth",
        "booktitle": "EMNLP2023",
        "title": "Prompting with Pseudo-Code Instructions",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models (LLM). Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, like pseudo-code. In this paper, we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models. We manually create a dataset of pseudo-code prompts for 132 different tasks spanning classification, QA, and generative language tasks, sourced from the Super-NaturalInstructions dataset. Using these prompts along with their counterparts in natural language, we study their performance on two LLM families - BLOOM, CodeGen. Our experiments show that using pseudo-code instructions leads to better results, with an average increase (absolute) of 7-16 points in F1 scores for classification tasks and an improvement (relative) of 12-38% in aggregate ROUGE-L scores across all tasks. We include detailed ablation studies which indicate that code comments, docstrings, and the structural clues encoded in pseudo-code all contribute towards the improvement in performance. To the best of our knowledge, our work is the first to demonstrate how pseudo-code prompts can be helpful in improving the performance of pre-trained LMs.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.939",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "CRAB: Assessing the Strength of Causal Relationships Between Real-world Events": {
        "type": "INPROCEEDINGS",
        "key": "romanou-etal-2023-crab",
        "author": "Romanou, Angelika and Montariol, Syrielle and Paul, Debjit and Laugier, Leo and Aberer, Karl and Bosselut, Antoine",
        "booktitle": "EMNLP2023",
        "title": "CRAB: Assessing the Strength of Causal Relationships Between Real-world Events",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Understanding narratives requires reasoning about the cause-and-effect relationships between events mentioned in the text. While existing foundation models yield impressive results in many NLP tasks requiring reasoning, it is unclear whether they understand the complexity of the underlying network of causal relationships of events in narratives. In this work, we present CRAB, a new Causal Reasoning Assessment Benchmark designed to evaluate causal understanding of events in real-world narratives. CRAB contains fine-grained, contextual causality annotations for ~2.7K pairs of real-world events that describe various newsworthy event timelines (e.g., the acquisition of Twitter by Elon Musk). Using CRAB, we measure the performance of several large language models, demonstrating that most systems achieve poor performance on the task. Motivated by classical causal principles, we also analyze the causal structures of groups of events in CRAB, and find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains. We make our dataset and code available to the research community.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.940",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Making Large Language Models Better Data Creators": {
        "type": "INPROCEEDINGS",
        "key": "lee-etal-2023-making",
        "author": "Lee, Dong-Ho and Pujara, Jay and Sewak, Mohit and White, Ryen and Jauhar, Sujay",
        "booktitle": "EMNLP2023",
        "title": "Making Large Language Models Better Data Creators",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. In our experiments we demonstrate that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.948",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Zero-Shot Multi-Label Topic Inference with Sentence Encoders and LLMs": {
        "type": "INPROCEEDINGS",
        "key": "sarkar-etal-2023-zero",
        "author": "Sarkar, Souvika and Feng, Dongji and Karmaker Santu, Shubhra Kanti",
        "booktitle": "EMNLP2023",
        "title": "Zero-Shot Multi-Label Topic Inference with Sentence Encoders and LLMs",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we conducted a comprehensive study with the latest Sentence Encoders and Large Language Models (LLMs) on the challenging task of \u201cdefinition-wild zero-shot topic inference\u201d, where users define or provide the topics of interest in real-time. Through extensive experimentation on seven diverse data sets, we observed that LLMs, such as ChatGPT-3.5 and PaLM, demonstrated superior generality compared to other LLMs, e.g., BLOOM and GPT-NeoX. Furthermore, Sentence-BERT, a BERT-based classical sentence encoder, outperformed PaLM and achieved performance comparable to ChatGPT-3.5.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.1008",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Exploring Distributional Shifts in Large Language Models for Code Analysis": {
        "type": "INPROCEEDINGS",
        "key": "arakelyan-etal-2023-exploring",
        "author": "Arakelyan, Shushan and Das, Rocktim and Mao, Yi and Ren, Xiang",
        "booktitle": "EMNLP2023",
        "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. We establish that samples from each new domain present all the models with a significant challenge of distribution shift. We study how established methods adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. Moreover, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.1013",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Document-Level Machine Translation with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-document-level",
        "author": "Wang, Longyue and Lyu, Chenyang and Ji, Tianbo and Zhang, Zhirui and Yu, Dian and Shi, Shuming and Tu, Zhaopeng",
        "booktitle": "EMNLP2023",
        "title": "Document-Level Machine Translation with Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs\u2019 ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of ChatGPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs (We release our data and annotations at https://github.com/longyuewangdcu/Document-MT-LLM).",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.1036",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "LLM-driven Instruction Following: Progresses and Concerns": {
        "type": "INPROCEEDINGS",
        "key": "yin-etal-2023-llm",
        "author": "Yin, Wenpeng and Ye, Qinyuan and Liu, Pengfei and Ren, Xiang and Sch\u00fctze, Hinrich",
        "booktitle": "EMNLP2023",
        "title": "LLM-driven Instruction Following: Progresses and Concerns",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The progress of natural language processing (NLP) is primarily driven by machine learning that optimizes a system on a large-scale set of task-specific labeled examples. This learning paradigm limits the ability of machines to have the same capabilities as humans in handling new tasks since humans can often solve unseen tasks with a couple of examples accompanied by task instructions. In addition, we may not have a chance to prepare task-specific examples of large-volume for new tasks because we cannot foresee what task needs to be addressed next and how complex to annotate for it. Therefore, task instructions act as a novel and promising resource for supervision. This tutorial targets researchers and practitioners who are interested in AI and ML technologies for NLP generalization in a low-shot scenario. In particular, we will present a diverse thread of instruction-driven NLP studies that try to answer the following questions: (i) What is task instruction? (ii) How is the process of creating datasets and evaluating systems conducted? (iii) How to encode task instructions? (iv) When and why do some instructions work better? (v) What concerns remain in LLM-driven instruction following? We will discuss several lines of frontier research that tackle those challenges and will conclude the tutorial by outlining directions for further investigation.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-tutorial.4",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "H2O Open Ecosystem for State-of-the-art Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "candel-etal-2023-h2o",
        "author": "Candel, Arno and McKinney, Jon and Singer, Philipp and Pfeiffer, Pascal and Jeblick, Maximilian and Lee, Chun Ming and Conde, Marcos",
        "booktitle": "EMNLP2023",
        "title": "H2O Open Ecosystem for State-of-the-art Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. Our demo is available at: https://gpt.h2o.ai/",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-demo.6",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "yu-etal-2023-musicagent",
        "author": "Yu, Dingyao and Song, Kaitao and Lu, Peiling and He, Tianyu and Tan, Xu and Ye, Wei and Zhang, Shikun and Bian, Jiang",
        "booktitle": "EMNLP2023",
        "title": "MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "AI-empowered music processing is a diverse feld that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classifcation). For developers and amateurs, it is very diffcult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience. The code is available on GitHub along with a brief instructional video.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-demo.21",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "MiniChain: A Small Library for Coding with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "rush-2023-minichain",
        "author": "Rush, Alexander",
        "booktitle": "EMNLP2023",
        "title": "MiniChain: A Small Library for Coding with Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Programming augmented by large language models (LLMs) opens up many new application areas, but also requires care. LLMs are accurate enough, on average, to replace core functionality, yet make basic mistakes that demonstrate a lack of robustness. An ecosystem of prompting tools, from intelligent agents to new programming languages, have emerged with different solutions for patching LLMs with other tools. In this work, we introduce MiniChain, an opinionated tool for LLM augmented programming, with the design goals of ease-of-use of prototyping, transparency through automatic visualization, and a minimalistic approach to advanced features. The MiniChain library provides core primitives for coding LLM calls, separating out prompt templates, and capturing program structure. The library includes demo implementations of the main applications papers in the area, including chat-bots, code generation, retrieval-based question answering, and complex information extraction. The library is open-source and available at https://github.com/srush/MiniChain, with code demos available at https://srush-minichain.hf.space/, and video demo at https://www.youtube.com/watch?v=VszZ1VnO7sk.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-demo.27",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails": {
        "type": "INPROCEEDINGS",
        "key": "rebedea-etal-2023-nemo",
        "author": "Rebedea, Traian and Dinu, Razvan and Sreedhar, Makesh Narsimhan and Parisien, Christopher and Cohen, Jonathan",
        "booktitle": "EMNLP2023",
        "title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Using a runtime inspired from dialogue management, NeMo Guardrails provides a different approach by allowing developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-demo.40",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "LM-Polygraph: Uncertainty Estimation for Language Models": {
        "type": "INPROCEEDINGS",
        "key": "fadeeva-etal-2023-lm",
        "author": "Fadeeva, Ekaterina and Vashurin, Roman and Tsvigun, Akim and Vazhentsev, Artem and Petrakov, Sergey and Fedyanin, Kirill and Vasilev, Daniil and Goncharova, Elizaveta and Panchenko, Alexander and Panov, Maxim and Baldwin, Timothy and Shelmanov, Artem",
        "booktitle": "EMNLP2023",
        "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often \u201challucinate\u201d, i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-demo.41",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-video",
        "author": "Zhang, Hang and Li, Xin and Bing, Lidong",
        "booktitle": "EMNLP2023",
        "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual &amp; audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual &amp; audio encoders with LLM\u2019s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-demo.49",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Text2Topic: Multi-Label Text Classification System for Efficient Topic Detection in User Generated Content with Zero-Shot Capabilities": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-text2topic",
        "author": "Wang, Fengjun and Beladev, Moran and Kleinfeld, Ofri and Frayerman, Elina and Shachar, Tal and Fainman, Eran and Lastmann Assaraf, Karen and Mizrachi, Sarai and Wang, Benjamin",
        "booktitle": "EMNLP2023",
        "title": "Text2Topic: Multi-Label Text Classification System for Efficient Topic Detection in User Generated Content with Zero-Shot Capabilities",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multi-label text classification is a critical task in the industry. It helps to extract structured information from large amount of textual data. We propose Text to Topic (Text2Topic), which achieves high multi-label classification performance by employing a Bi-Encoder Transformer architecture that utilizes concatenation, subtraction, and multiplication of embeddings on both text and topic. Text2Topic also supports zero-shot predictions, produces domain-specific text embeddings, and enables production-scale batch-inference with high throughput. The final model achieves accurate and comprehensive results compared to state-of-the-art baselines, including large language models (LLMs). In this study, a total of 239 topics are defined, and around 1.6 million text-topic pairs annotations (in which 200K are positive) are collected on approximately 120K texts from 3 main data sources on Booking.com. The data is collected with optimized smart sampling and partial labeling. The final Text2Topic model is deployed on a real-world stream processing platform, and it outperforms other models with 92.9% micro mAP, as well as a 75.8% macro mAP score. We summarize the modeling choices which are extensively tested through ablation studies, and share detailed in-production decision-making steps.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-industry.10",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Investigating Table-to-Text Generation Capabilities of Large Language Models in Real-World Information Seeking Scenarios": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2023-investigating",
        "author": "Zhao, Yilun and Zhang, Haowei and Si, Shengyun and Nan, Linyong and Tang, Xiangru and Cohan, Arman",
        "booktitle": "EMNLP2023",
        "title": "Investigating Table-to-Text Generation Capabilities of Large Language Models in Real-World Information Seeking Scenarios",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored. In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within two real-world information seeking scenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets for data insight generation, along with the FeTaQA and our newly-constructed F2WTQ datasets for query-based generation. We structure our investigation around three research questions, evaluating the performance of LLMs in table-to-text generation, automated evaluation, and feedback generation, respectively. Experimental results indicate that the current high-performing LLM, specifically GPT-4, can effectively serve as a table-to-text generator, evaluator, and feedback generator, facilitating users\u2019 information seeking purposes in real-world scenarios. However, a significant performance gap still exists between other open-sourced LLMs (e.g., Vicuna and LLaMA-2) and GPT-4 models. Our data and code are publicly available at https://github.com/yale-nlp/LLM-T2T.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-industry.17",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2023-empower",
        "author": "Yang, Fangkai and Zhao, Pu and Wang, Zezhong and Wang, Lu and Qiao, Bo and Zhang, Jue and Garg, Mohit and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei",
        "booktitle": "EMNLP2023",
        "title": "Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average due to its lack of specific domain knowledge. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, centered around Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, an area not extensively covered in general LLMs, making it well-suited for evaluating methods aiming to enhance LLMs\u2019 domain-specific capabilities. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our method outperforms the commonly used LLM with retrieval methods. We make our source code and sample data available at: https://aka.ms/Microsoft_QA.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-industry.29",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Batch Prompting: Efficient Inference with Large Language Model APIs": {
        "type": "INPROCEEDINGS",
        "key": "cheng-etal-2023-batch",
        "author": "Cheng, Zhoujun and Kasai, Jungo and Yu, Tao",
        "booktitle": "EMNLP2023",
        "title": "Batch Prompting: Efficient Inference with Large Language Model APIs",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly (up to 5\\times with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Further analysis shows that the number of samples in each batch and the complexity of tasks affect its performance. Moreover, batch prompting can be applied across different reasoning methods using LLMs. Our code is released at the site https://github.com/xlang-ai/batch-prompting.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.emnlp-industry.74",
        "doi": "",
        "ISSN": "",
        "month": "December"
    },
    "Self-Edit: Fault-Aware Code Editor for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-self",
        "author": "Zhang, Kechi and Li, Zhuo and Li, Jia and Li, Ge and Jin, Zhi",
        "booktitle": "ACL2023",
        "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.45",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages": {
        "type": "INPROCEEDINGS",
        "key": "imanigooghari-etal-2023-glot500",
        "author": "Imani, Ayyoob and Lin, Peiqin and Kargaran, Amir Hossein and Severini, Silvia and Jalili Sabet, Masoud and Kassner, Nora and Ma, Chunlan and Schmid, Helmut and Martins, Andr\u00e9 and Yvon, Fran\u00e7ois and Sch\u00fctze, Hinrich",
        "booktitle": "ACL2023",
        "title": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, \u201chelp\u201d from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should notlimit NLP to a small fraction of the world\u2019s languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https://github.com/cisnlp/Glot500.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.61",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Robust Multi-bit Natural Language Watermarking through Invariant Features": {
        "type": "INPROCEEDINGS",
        "key": "yoo-etal-2023-robust",
        "author": "Yoo, KiYoon and Ahn, Wonhyuk and Jang, Jiho and Kwak, Nojun",
        "booktitle": "ACL2023",
        "title": "Robust Multi-bit Natural Language Watermarking through Invariant Features",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.117",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-plan",
        "author": "Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng",
        "booktitle": "ACL2023",
        "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.147",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Symbolic Chain-of-Thought Distillation: Small Models Can Also \u201cThink\u201d Step-by-Step": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-symbolic",
        "author": "Li, Liunian Harold and Hessel, Jack and Yu, Youngjae and Ren, Xiang and Chang, Kai-Wei and Choi, Yejin",
        "booktitle": "ACL2023",
        "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \u201cThink\u201d Step-by-Step",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Chain-of-thought prompting (e.g., \u201cLet\u2019s think step-by-ste\u201d) primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M\u20141.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.150",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Entity Tracking in Language Models": {
        "type": "INPROCEEDINGS",
        "key": "kim-schuster-2023-entity",
        "author": "Kim, Najoung and Schuster, Sebastian",
        "booktitle": "ACL2023",
        "title": "Entity Tracking in Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even when evaluated on a different set of entities from training or longer operation sequences, a finetuned model can perform non-trivial entity tracking. Taken together, these results suggest that language models can learn to track entities but pretraining on text corpora alone does not make this capacity surface.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.213",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Making Language Models Better Reasoners with Step-Aware Verifier": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-making",
        "author": "Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu",
        "booktitle": "ACL2023",
        "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.291",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "MISGENDERED: Limits of Large Language Models in Understanding Pronouns": {
        "type": "INPROCEEDINGS",
        "key": "hossain-etal-2023-misgendered",
        "author": "Hossain, Tamanna and Dev, Sunipa and Singh, Sameer",
        "booktitle": "ACL2023",
        "title": "MISGENDERED: Limits of Large Language Models in Understanding Pronouns",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering. Gender bias in language technologies has been widely studied, but research has mostly been restricted to a binary paradigm of gender. It is essential also to consider non-binary gender identities, as excluding them can cause further harm to an already marginalized group. In this paper, we comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns. We introduce Misgendered, a framework for evaluating large language models\u2019 ability to correctly use preferred pronouns, consisting of (i) instances declaring an individual\u2019s pronoun, followed by a sentence with a missing pronoun, and (ii) an experimental setup for evaluating masked and auto-regressive language models using a unified method. When prompted out-of-the-box, language models perform poorly at correctly predicting neo-pronouns (averaging 7.6% accuracy) and gender-neutral pronouns (averaging 31.0% accuracy). This inability to generalize results from a lack of representation of non-binary pronouns in training data and memorized associations. Few-shot adaptation with explicit examples in the prompt improves the performance but plateaus at only 45.4% for neo-pronouns. We release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendered/.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.293",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Parallel Context Windows for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ratner-etal-2023-parallel",
        "author": "Ratner, Nir and Levine, Yoav and Belinkov, Yonatan and Ram, Ori and Magar, Inbal and Abend, Omri and Karpas, Ehud and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav",
        "booktitle": "ACL2023",
        "title": "Parallel Context Windows for Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (\u201cwindows\u201d), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ai21labs/parallel-context-windows.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.352",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Few-shot In-context Learning on Knowledge Base Question Answering": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-shot",
        "author": "Li, Tianle and Ma, Xueguang and Zhuang, Alex and Gu, Yu and Su, Yu and Chen, Wenhu",
        "booktitle": "ACL2023",
        "title": "Few-shot In-context Learning on Knowledge Base Question Answering",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. We plan to release all the code and data. Our code is available at https://github.com/ltl3A87/KB-BINDER.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.385",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Fact-Checking Complex Claims with Program-Guided Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "pan-etal-2023-fact",
        "author": "Pan, Liangming and Wu, Xiaobao and Lu, Xinyuan and Luu, Anh Tuan and Wang, William Yang and Kan, Min-Yen and Nakov, Preslav",
        "booktitle": "ACL2023",
        "title": "Fact-Checking Complex Claims with Program-Guided Reasoning",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/ProgramFC.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.386",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales": {
        "type": "INPROCEEDINGS",
        "key": "joshi-etal-2023-machine",
        "author": "Joshi, Brihi and Liu, Ziyi and Ramnath, Sahana and Chan, Aaron and Tong, Zhewei and Nie, Shaoliang and Wang, Qifan and Choi, Yejin and Ren, Xiang",
        "booktitle": "ACL2023",
        "title": "Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale\u2019s helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs\u2019 ability to generate rationales with better human utility, while maintaining most of its task performance. Lastly, we release all code and collected data with this project.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.392",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Large Language Models Meet NL2Code: A Survey": {
        "type": "INPROCEEDINGS",
        "key": "zan-etal-2023-large",
        "author": "Zan, Daoguang and Chen, Bei and Zhang, Fengji and Lu, Dianjie and Wu, Bingchao and Guan, Bei and Yongji, Wang and Lou, Jian-Guang",
        "booktitle": "ACL2023",
        "title": "Large Language Models Meet NL2Code: A Survey",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \u201cLarge Size, Premium Data, Expert Tuning\u201d. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.411",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark": {
        "type": "INPROCEEDINGS",
        "key": "peng-etal-2023-copying",
        "author": "Peng, Wenjun and Yi, Jingwei and Wu, Fangzhao and Wu, Shangxi and Bin Zhu, Bin and Lyu, Lingjuan and Jiao, Binxing and Xu, Tong and Sun, Guangzhong and Xie, Xing",
        "booktitle": "ACL2023",
        "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called pasted macro \u2018METHOD\u2019 that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer\u2019s model for copyright verification while minimizing the adverse impact on the original embeddings\u2019 utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality. Our code is available at https://github.com/yjw1029/EmbMarker.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.423",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-element",
        "author": "Wang, Yiming and Zhang, Zhuosheng and Wang, Rui",
        "booktitle": "ACL2023",
        "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the \u201cLasswell Communication Model\u201d proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs\u2019 zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.482",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "felkner-etal-2023-winoqueer",
        "author": "Felkner, Virginia and Chang, Ho-Chun Herbert and Jang, Eugene and May, Jonathan",
        "booktitle": "ACL2023",
        "title": "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.507",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Introducing Semantics into Speech Encoders": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2023-introducing",
        "author": "Xu, Derek and Dong, Shuyan and Wang, Changhan and Kim, Suyoun and Lin, Zhaojiang and Liu, Bing and Shrivastava, Akshat and Li, Shang-Wen and Tseng, Liang-Hsuan and Lin, Guan-Ting and Baevski, Alexei and Lee, Hung-yi and Sun, Yizhou and Wang, Wei",
        "booktitle": "ACL2023",
        "title": "Introducing Semantics into Speech Encoders",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent studies find existing self-supervised speech encoders contain primarily acoustic rather than semantic information. As a result, pipelined supervised automatic speech recognition (ASR) to large language model (LLM) systems achieve state-of-the-art results on semantic spoken language tasks by utilizing rich semantic representations from the LLM. These systems come at the cost of labeled audio transcriptions, which is expensive and time-consuming to obtain. We propose a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions. By introducing semantics, we improve existing speech encoder spoken language understanding (SLU) performance by over 5% on intent classification (IC), with modest gains in named entity resolution (NER) and slot filling (SF), and spoken question answering (SQA) FF1 score by over 2%. Our approach, which uses no ASR data, achieves similar performance as methods trained on over 100 hours of labeled audio transcripts, demonstrating the feasibility of unsupervised semantic augmentations to existing speech encoders.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.639",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion": {
        "type": "INPROCEEDINGS",
        "key": "jiang-etal-2023-llm",
        "author": "Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen",
        "booktitle": "ACL2023",
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.792",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "FERMAT: An Alternative to Accuracy for Numerical Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "sivakumar-moosavi-2023-fermat",
        "author": "Sivakumar, Jasivan and Moosavi, Nafise Sadat",
        "booktitle": "ACL2023",
        "title": "FERMAT: An Alternative to Accuracy for Numerical Reasoning",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and are not accessible to everyone. In addition, numerical reasoning is measured using a single score on existing datasets. As a result, we do not have a clear understanding of the strengths and shortcomings of existing models on different numerical reasoning aspects and therefore, potential ways to improve them apart from scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we introduce a multi-view evaluation set for numerical reasoning in English, called FERMAT. Instead of reporting a single score on a whole dataset, FERMAT evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency. Apart from providing a comprehensive evaluation of models on different numerical reasoning aspects, FERMAT enables a systematic and automated generation of an arbitrarily large training or evaluation set for each aspect. The datasets and codes are publicly available to generate further multi-view data for ulterior tasks and languages.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.838",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models": {
        "type": "INPROCEEDINGS",
        "key": "mendelsohn-etal-2023-dogwhistles",
        "author": "Mendelsohn, Julia and Le Bras, Ronan and Choi, Yejin and Sap, Maarten",
        "booktitle": "ACL2023",
        "title": "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word \u201ccosmopolitan\u201d in a sentence such as \u201cwe need to end the cosmopolitan experiment\u201d can mean \u201cworldly\u201d to many but also secretly mean \u201cJewish\u201d to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians\u2019 speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3\u2019s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources to facilitate future research in modeling dogwhistles and mitigating their online harms.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.845",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-codeie",
        "author": "Li, Peng and Sun, Tianxiang and Tang, Qiong and Yan, Hang and Wu, Yuanbin and Huang, Xuanjing and Qiu, Xipeng",
        "booktitle": "ACL2023",
        "title": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.855",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-xsemplr",
        "author": "Zhang, Yusen and Wang, Jun and Wang, Zhiguo and Zhang, Rui",
        "booktitle": "ACL2023",
        "title": "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (full dataset, few-shot, and zero-shot). Our experiments show that encoder-decoder models (mT5) achieve the highest performance compared with other popular models, and multilingual training can further improve the average performance. Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks. We also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training. Our dataset and code are available at https://github.com/psunlpgroup/XSemPLR.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.887",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Crosslingual Generalization through Multitask Finetuning": {
        "type": "INPROCEEDINGS",
        "key": "muennighoff-etal-2023-crosslingual",
        "author": "Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Le Scao, Teven and Bari, M. Saiful and Shen, Sheng and Yong, Zheng Xin and Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Albert and Raff, Edward and Raffel, Colin",
        "booktitle": "ACL2023",
        "title": "Crosslingual Generalization through Multitask Finetuning",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-long.891",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Probing Physical Reasoning with Counter-Commonsense Context": {
        "type": "INPROCEEDINGS",
        "key": "kondo-etal-2023-probing",
        "author": "Kondo, Kazushi and Sugawara, Saku and Aizawa, Akiko",
        "booktitle": "ACL2023",
        "title": "Probing Physical Reasoning with Counter-Commonsense Context",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this study, we create a CConS (Counter-commonsense Contextual Size comparison) dataset to investigate how physical commonsense affects the contextualized size comparison task; the proposed dataset consists of both contexts that fit physical commonsense and those that do not. This dataset tests the ability of language models to predict the size relationship between objects under various contexts generated from our curated noun list and templates. We measure the ability of several masked language models and encoder-decoder models. The results show that while large language models can use prepositions such as \u201cin\u201d and \u201cinto\u201d in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-short.53",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Black-box language model explanation by context length probing": {
        "type": "INPROCEEDINGS",
        "key": "cifka-liutkus-2023-black",
        "author": "C\u00edfka, Ond\u0159ej and Liutkus, Antoine",
        "booktitle": "ACL2023",
        "title": "Black-box language model explanation by context length probing",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present *context length probing*, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign *differential importance scores* to different contexts. The technique is model-agnostic and does not rely on access to model internals beyond computing token-level probabilities. We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies. The [source code](https://github.com/cifkao/context-probing/) and an [interactive demo](https://cifkao.github.io/context-probing/) of the method are available.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-short.92",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "dibia-2023-lida",
        "author": "Dibia, Victor",
        "booktitle": "ACL2023",
        "title": "LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) and image generation models (IGMs) are suitable to addressing these tasks. We present LIDA, a novel tool for generating grammar-agnostic visualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER that converts data into a rich but compact natural language summary, a GOAL EXPLORER that enumerates visualization goals given the data, a VISGENERATOR that generates, refines, executes and filters visualization code and an INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA provides a python api, and a hybrid user interface (direct manipulation and multilingual natural language) for interactive chart, infographics and data story generation. Code and demo are available at this url - https://microsoft.github.io/lida/",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-demo.11",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Petals: Collaborative Inference and Fine-tuning of Large Models": {
        "type": "INPROCEEDINGS",
        "key": "borzunov-etal-2023-petals",
        "author": "Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and Riabinin, Maksim and Belkada, Younes and Chumachenko, Artem and Samygin, Pavel and Raffel, Colin",
        "booktitle": "ACL2023",
        "title": "Petals: Collaborative Inference and Fine-tuning of Large Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers. In some cases, LLMs can be used more affordably via RAM offloading or hosted APIs. However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are not flexible enough for research that requires access to weights, attention or logits. In this work, we propose Petals - a system for inference and fine-tuning of large models collaboratively by joining the resources of multiple parties. We demonstrate that this strategy outperforms offloading for very large models, running inference of BLOOM-176B on consumer GPUs with \\approx1 step per second, which is enough for many interactive LLM applications. Unlike most inference APIs, Petals also natively exposes hidden states of served models, allowing to train and share custom model extensions based on efficient fine-tuning methods. The system, its source code, and documentation are available at https://petals.mlVideo (2 min): https://youtu.be/F4muLI-0hTE",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-demo.54",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "Combining Tradition with Modernness: Exploring Event Representations in Vision-and-Language Models for Visual Goal-Step Inference": {
        "type": "INPROCEEDINGS",
        "key": "shen-silberer-2023-combining",
        "author": "Shen, Chong and Silberer, Carina",
        "booktitle": "ACL2023",
        "title": "Combining Tradition with Modernness: Exploring Event Representations in Vision-and-Language Models for Visual Goal-Step Inference",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Procedural knowledge understanding (PKU) underlies the ability to infer goal-step relations. The task of Visual Goal\u2013Step Inference addresses this ability in the multimodal domain. It requires to identify images that represent the steps towards achieving a textually expressed goal. The best existing methods encode texts and images either with independent encoders, or with object-level multimodal encoders using blackbox transformers. This stands in contrast to early, linguistically inspired methods for event representations, which focus on capturing the most crucial information, namely actions and the participants, to learn stereotypical event sequences and hence procedural knowledge. In this work, we study various methods and their effects on PKU of injecting the early shallow event representations to nowadays multimodal deep learning-based models. We find that the early, linguistically inspired methods for representing event knowledge does contribute to understand procedures in combination with modern vision-and-language models. In the future, we are going to explore more complex structure of events and study how to exploit it on top of large language models.",
        "keywords": "",
        "url": "https://doi.org/10.18653/v1/2023.acl-srw.36",
        "doi": "",
        "ISSN": "",
        "month": "July"
    },
    "LMQL as described in Prompting Is Programming: A Query Language for Large Language Models": {
        "type": "software",
        "key": "10.5281/zenodo.7711823",
        "author": "Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin",
        "title": "LMQL as described in Prompting Is Programming: A Query Language for Large Language Models",
        "year": "2023",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.7711823",
        "abstract": "    <p>LMQL is a query language for large language models (LLMs). It facilitates LLM interaction by combining the benefits of natural language prompting with the expressiveness of Python. With only a few lines of LMQL code, users can express advanced, multi-part and tool-augmented LM queries, which then are optimized by the LMQL runtime to run efficiently as part of the LM decoding loop.</p><p>An up to date version can be found at https://github.com/eth-sri/lmql</p>",
        "keywords": "language model programming, prompt programming"
    },
    "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616271",
        "author": "Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming",
        "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616271",
        "doi": "10.1145/3611643.3616271",
        "abstract": "During Automated Program Repair (APR), it can be challenging&nbsp;to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models&nbsp;(LLMs) have been shown to be helpful \u201ccopilots\u201d in assisting developers with various coding tasks, and have also been directly&nbsp;applied for patch synthesis. However, most LLMs treat programs as&nbsp;sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This&nbsp;results in plenty of statically invalid generated patches, impeding&nbsp;the practicality of the technique. Therefore, we propose Repilot,&nbsp;a framework to further copilot the AI \u201ccopilots\u201d (i.e., LLMs) by&nbsp;synthesizing more valid patches during the repair process. Our key&nbsp;insight is that many LLMs produce outputs autoregressively (i.e.,&nbsp;token by token), resembling human writing programs, which can&nbsp;be significantly boosted and guided through a Completion Engine.&nbsp;Repilot synergistically synthesizes a candidate patch through the&nbsp;interaction between an LLM and a Completion Engine, which 1)&nbsp;prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the&nbsp;Completion Engine. Our evaluation on a subset of the widely-used&nbsp;Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50&nbsp;bugs, respectively, surpassing the best-performing baseline by 14&nbsp;and 16 bugs fixed. More&nbsp;importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given&nbsp;the same generation budget.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "172\u2013184",
        "numpages": "13",
        "keywords": "Completion Engine, Large Language Model, Program Repair",
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023"
    },
    "Reproduction Package (Docker Image) for the ESEC/FSE 2023 Paper \"Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair": {
        "type": "software",
        "key": "10.5281/zenodo.8281250",
        "author": "Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming",
        "title": "Reproduction Package (Docker Image) for the ESEC/FSE 2023 Paper \"Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
        "year": "2023",
        "publisher": "Association for Computing Machinery",
        "url": "https://doi.org/10.5281/zenodo.8281250",
        "abstract": "    <p>This is the artifact accompanying our ESEC/FSE\u201923 paper \u201cCopiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair\u201d. For user convenience, we deliver our artifact in the form of a Docker image that has resolved all the software dependencies beforehand. The Docker image comprises (1) the source code of <strong>Repilot</strong>, the patch generation tool introduced in the paper, (2) all the data needed to reproduce the experiments done for the paper, (3) a detailed documentation on how to achieve the experimental results step-by-step, and (4) the <code>Dockerfile</code> we use to create this image.</p>",
        "keywords": "Artifact, Docker Image, Repilot"
    },
    "Multilingual Code Co-evolution using Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616350",
        "author": "Zhang, Jiyang and Nie, Pengyu and Li, Junyi Jessy and Gligoric, Milos",
        "title": "Multilingual Code Co-evolution using Large Language Models",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616350",
        "doi": "10.1145/3611643.3616350",
        "abstract": "Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#). Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics. Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "695\u2013707",
        "numpages": "13",
        "keywords": "Language model, code translation, software evolution",
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023"
    },
    "Baldur: Whole-Proof Generation and Repair with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616243",
        "author": "First, Emily and Rabe, Markus N. and Ringer, Talia and Brun, Yuriy",
        "title": "Baldur: Whole-Proof Generation and Repair with Large Language Models",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616243",
        "doi": "10.1145/3611643.3616243",
        "abstract": "Formally verifying software is a highly desirable but labor-intensive task.  Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time and using that model to search through the space of possible proofs.  This paper introduces a new method to automate formal verification: We use large language models, trained on natural language and code and fine-tuned on proofs, to generate whole proofs at once.  We then demonstrate that a model fine-tuned to repair generated proofs further increasing proving power.  This paper:  (1) Demonstrates that whole-proof generation using transformers is possible and is as effective but more efficient than search-based techniques.  (2) Demonstrates that giving the learned model additional context, such as a prior failed proof attempt and the ensuing error message, results in proof repair that further improves automated proof generation.  (3) Establishes, together with prior work, a new state of the art for fully automated proof synthesis.  We reify our method in a prototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL theorems and their proofs,  empirically showing the effectiveness of whole-proof generation, repair, and added context. We also show that Baldur complements the state-of-the-art tool, Thor, by automatically generating proofs for an additional 8.7\\% of the theorems. Together, Baldur and Thor can prove 65.7\\% of the theorems fully automatically. This paper paves the way for new research into using large language models for automating formal verification.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1229\u20131241",
        "numpages": "13",
        "keywords": "Proof assistants, automated formal verification, large language models, machine learning, proof repair, proof synthesis",
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023"
    },
    "Grace: Language Models Meet Code Edits": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616253",
        "author": "Gupta, Priyanshu and Khare, Avishree and Bajpai, Yasharth and Chakraborty, Saikat and Gulwani, Sumit and Kanade, Aditya and Radhakrishna, Arjun and Soares, Gustavo and Tiwari, Ashish",
        "title": "Grace: Language Models Meet Code Edits",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616253",
        "doi": "10.1145/3611643.3616253",
        "abstract": "Developers spend a significant amount of time in editing code for a variety of reasons such as bug fixing or adding new features. Designing effective methods to predict code edits has been an active yet challenging area of research due to the diversity of code edits and the difficulty of capturing the developer intent. In this work, we address these challenges by endowing pre-trained large language models (LLMs) with the knowledge of relevant prior associated edits, which we call the Grace (Generation conditioned on Associated Code Edits) method. The generative capability of the LLMs helps address the diversity in code changes and conditioning code generation on prior edits helps capture the latent developer intent. We evaluate two well-known LLMs, codex and CodeT5, in zero-shot and fine-tuning settings respectively. In our experiments with two datasets, Grace boosts the performance of the LLMs significantly, enabling them to generate 29\\% and 54\\% more correctly edited code in top-1 suggestions relative to the current state-of-the-art symbolic and neural approaches, respectively.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1483\u20131495",
        "numpages": "13",
        "keywords": "Associated edits, Code editing, Large language models, Pre-trained model, Programming language processing",
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023"
    },
    "InferFix: End-to-End Program Repair with LLMs": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3613892",
        "author": "Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey",
        "title": "InferFix: End-to-End Program Repair with LLMs",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3613892",
        "doi": "10.1145/3611643.3613892",
        "abstract": "Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose\u202f: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever \u2013 transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator \u2013 an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6\\% for generating fixes in C# and 76.8\\% in Java. We discuss the deployment of  alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1646\u20131656",
        "numpages": "11",
        "keywords": "Program repair, finetuning, prompt augmentation, static analyses",
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023"
    },
    "Getting pwn\u2019d by AI: Penetration Testing with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3613083",
        "author": "Happe, Andreas and Cito, J\\\"{u}rgen",
        "title": "Getting pwn\u2019d by AI: Penetration Testing with Large Language Models",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3613083",
        "doi": "10.1145/3611643.3613083",
        "abstract": "The field of software security testing, more specifically penetration testing, requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential use of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of AI sparring partners.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "2082\u20132086",
        "numpages": "5",
        "keywords": "large language models, penetration testing, security testing",
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023"
    },
    "Assisting Static Analysis with Large Language Models: A ChatGPT Experiment": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3613078",
        "author": "Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun",
        "title": "Assisting Static Analysis with Large Language Models: A ChatGPT Experiment",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3613078",
        "doi": "10.1145/3611643.3613078",
        "abstract": "Recent advances of Large Language Models (LLMs), e.g., ChatGPT, exhibited strong capabilities of comprehending and responding to questions across a variety of domains. Surprisingly, ChatGPT even possesses a strong understanding of program code. In this paper, we investigate where and how LLMs can assist static analysis by asking appropriate questions. In particular, we target a specific bug-finding tool, which produces many false positives from the static analysis. In our evaluation, we find that these false positives can be effectively pruned by asking carefully constructed questions about function-level behaviors or function summaries. Specifically, with a pilot study of 20 false positives, we can successfully prune 8 out of 20 based on GPT-3.5, whereas GPT-4 had a near-perfect result of 16 out of 20, where the four failed ones are not currently considered/supported by our questions, e.g., involving concurrency. Additionally, it also identified one false negative case (a missed bug). We find LLMs a promising tool that can enable a more effective and efficient program analysis.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "2107\u20132111",
        "numpages": "5",
        "keywords": "bug detection, large language model, static analysis",
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023"
    },
    "LLM-Based Code Generation Method for Golang Compiler Testing": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3617850",
        "author": "Gu, Qiuhan",
        "title": "LLM-Based Code Generation Method for Golang Compiler Testing",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3617850",
        "doi": "10.1145/3611643.3617850",
        "abstract": "Modern optimizing compilers are among the most complex software systems humans build. One way to identify subtle compiler bugs is fuzzing. Both the quantity and the quality of testcases are crucial to the performance of fuzzing. Traditional testcase-generation methods, such as Csmith and YARPGen, have been proven successful at discovering compiler bugs. However, such generated testcases have limited coverage and quantity. In this paper, we present a code generation method for compiler testing based on LLM to maximize the quality and quantity of the generated code. In particular, to avoid undefined behavior and syntax errors in generated testcases, we design a filter strategy to clean the source code, preparing a high-quality dataset for the model training. Besides, we present a seed schedule strategy to improve code generation. We apply the method to test the Golang compiler and the result shows that our pipeline outperforms previous methods both qualitatively and quantitatively. It produces testcases with an average coverage of 3.38\\%, in contrast to the testcases generated by GoFuzz, which have an average coverage of 0.44\\%. Moreover, among all the generated testcases, only 2.79\\% exhibited syntax errors, and none displayed undefined behavior.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "2201\u20132203",
        "numpages": "3",
        "keywords": "Code generation, Compiler testing, Go language, Large model",
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023"
    }
}