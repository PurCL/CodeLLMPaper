{
    "ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "han-etal-2024-archcode",
        "author": "Han, Hojae and Kim, Jaejin and Yoo, Jaeseok and Lee, Youngwon and Hwang, Seung-won",
        "booktitle": "ACL2024",
        "title": "ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (i.e. achieving expected behavior for inputs) and non-functional (e.g., time/space performance, robustness, maintainability) requirements. However, textual descriptions can either express requirements verbosely or may even omit some of them. We introduce ARCHCODE, a novel framework that leverages in-context learning to organize requirements observed in descriptions and to extrapolate unexpressed requirements from them. ARCHCODE generates requirements from given descriptions, conditioning them to produce code snippets and test cases. Each test case is tailored to one of the requirements, allowing for the ranking of code snippets based on the compliance of their execution results with the requirements. Public benchmarks show that ARCHCODE enhances to satisfy functional requirements, significantly improving Pass@k scores.Furthermore, we introduce HumanEval-NFR, the first evaluation of LLMs\u2019 non-functional requirements in code generation, demonstrating ARCHCODE\u2019s superiority over baseline methods. The implementation of ARCHCODE and the HumanEval-NFR benchmark are both publicly accessible.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.730",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-hirope",
        "author": "Zhang, Kechi and Li, Ge and Zhang, Huangzhao and Jin, Zhi",
        "booktitle": "ACL2024",
        "title": "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Addressing the limitation of context length in large language models for code-related tasks is the primary focus of this paper. Existing LLMs are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences. Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code. HiRoPE offers easy integration into existing LLMs without extra training costs. Our method is extensively evaluated with various LLMs, demonstrating stable performance in tasks such as language modeling and long code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. Theoretically and experimentally, we find that HiRoPE also addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length.",
        "keywords": [
            "code generation",
            "code completion",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.735",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-codeagent",
        "author": "Zhang, Kechi and Li, Jia and Li, Ge and Shi, Xianjie and Jin, Zhi",
        "booktitle": "ACL2024",
        "title": "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. However, real-world software development often involves complex code repositories with complex dependencies and extensive documentation. To enable LLMs to handle these realworld repo-level code generation, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code implementation, and code testing. We implement four agent strategies to optimize these tools\u2019 usage. To the best of our knowledge, CodeAgent is the first agent tool framework specifically for repo-level code generation. In order to measure the effectiveness of our method at the repository level, we have introduced a benchmark dataset CodAgentBench. The performance on this dataset shows a significant improvement brought by our method, with improvements of pass rate ranging from 2.0 to 15.8. Further tests on the HumanEval benchmark confirm CodeAgent\u2019s adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent\u2019s robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.737",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models": {
        "type": "INPROCEEDINGS",
        "key": "riddell-etal-2024-quantifying",
        "author": "Riddell, Martin and Ni, Ansong and Cohan, Arman",
        "booktitle": "ACL2024",
        "title": "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. While recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of LLMs in programming contexts. In this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform significantly better on the subset of the benchmarks where similar solutions are seen during training. We also conduct extensive analysis on the factors that affect model memorization and generalization, such as model size, problem difficulty, and question length. We release all resulting files from our matching pipeline for future research.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.761",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "ChatDev: Communicative Agents for Software Development": {
        "type": "INPROCEEDINGS",
        "key": "qian-etal-2024-chatdev",
        "author": "Qian, Chen and Liu, Wei and Liu, Hongzhang and Chen, Nuo and Dang, Yufan and Li, Jiahao and Yang, Cheng and Chen, Weize and Su, Yusheng and Cong, Xin and Xu, Juyuan and Li, Dahai and Liu, Zhiyuan and Sun, Maosong",
        "booktitle": "ACL2024",
        "title": "ChatDev: Communicative Agents for Software Development",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev.",
        "keywords": [
            "general coding task"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.810",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents": {
        "type": "INPROCEEDINGS",
        "key": "trivedi-etal-2024-appworld",
        "author": "Trivedi, Harsh and Khot, Tushar and Hartmann, Mareike and Manku, Ruskin and Dong, Vinty and Li, Edward and Gupta, Shashank and Sabharwal, Ashish and Balasubramanian, Niranjan",
        "booktitle": "ACL2024",
        "title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only ~49% of our \u2018normal\u2019 tasks and ~30% of \u2018challenge\u2019 tasks, while other models solve at least 16% fewer. This highlights the benchmark\u2019s difficulty and AppWorld\u2019s potential to push the frontiers of interactive coding agents.",
        "keywords": [
            "benchmark",
            "agent design"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.850",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "On Improving Repository-Level Code QA for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "strich-etal-2024-improving",
        "author": "Strich, Jan and Schneider, Florian and Nikishina, Irina and Biemann, Chris",
        "booktitle": "ACL2024",
        "title": "On Improving Repository-Level Code QA for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) such as ChatGPT, GitHub Copilot, Llama, or Mistral assist programmers as copilots and knowledge sources to make the coding process faster and more efficient. This paper aims to improve the copilot performance by implementing different self-alignment processes and retrieval-augmented generation (RAG) pipelines, as well as their combination. To test the effectiveness of all approaches, we create a dataset and apply a model-based evaluation, using LLM as a judge. It is designed to check the model\u2019s abilities to understand the source code semantics, the dependency between files, and the overall meta-information about the repository. We also compare our approach with other existing solutions, e.g. ChatGPT-3.5, and evaluate on the existing benchmarks. Code and dataset are available online (https://anonymous.4open.science/r/ma_llm-382D).",
        "keywords": [
            "general coding task",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-srw.28",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks": {
        "type": "INPROCEEDINGS",
        "key": "10646663",
        "author": "Ullah, Saad and Han, Mingji and Pujar, Saurabh and Pearce, Hammond and Coskun, Ayse and Stringhini, Gianluca",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "862-880",
        "abstract": "Large Language Models (LLMs) have been suggested for use in automated vulnerability repair, but benchmarks showing they can consistently identify security-related bugs are lacking. We thus develop SecLLMHolmes, a fully automated evaluation framework that performs the most detailed investigation to date on whether LLMs can reliably identify and reason about security-related bugs. We construct a set of 228 code scenarios and analyze eight of the most capable LLMs across eight different investigative dimensions using our framework. Our evaluation shows LLMs provide non-deterministic responses, incorrect and unfaithful reasoning, and perform poorly in real-world scenarios. Most importantly, our findings reveal significant non-robustness in even the most advanced models like \u2018PaLM2\u2019 and \u2018GPT-4\u2019: by merely changing function or variable names, or by the addition of library functions in the source code, these models can yield incorrect answers in 26% and 17% of cases, respectively. These findings demonstrate that further LLM advances are needed before LLMs can be used as general purpose security assistants.",
        "keywords": [
            "static analysis",
            "bug detection",
            "code generation",
            "program repair",
            "empirical study"
        ],
        "doi": "10.1109/SP54263.2024.00210",
        "ISSN": "2375-1207",
        "month": "May",
        "venue": "S&P2024"
    },
    "LLMIF: Augmented Large Language Model for Fuzzing IoT Devices": {
        "type": "INPROCEEDINGS",
        "key": "10646659",
        "author": "Wang, Jincheng and Yu, Le and Luo, Xiapu",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "LLMIF: Augmented Large Language Model for Fuzzing IoT Devices",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "881-896",
        "abstract": "Despite the efficacy of fuzzing in verifying the implementation correctness of network protocols, existing IoT protocol fuzzing approaches grapple with several limitations, including obfuscated message formats, unresolved message dependencies, and a lack of evaluations on the testing cases. These limitations significantly curtail the capabilities of IoT fuzzers in vulnerability identification. In this work, we show that the protocol specification contains fruitful descriptions of protocol messages, which can be used to overcome the above limitations and guide IoT protocol fuzzing. To automate the specification analysis, we augment the large language model with the specification contents, and drive it to perform two tasks (i.e., protocol information extraction, and device response reasoning). We further design and implement a fuzzing algorithm, LLMIF, which incorporates the LLM into IoT fuzzing. Finally, we select Zigbee as the target protocol and initiate comprehensive evaluations. The evaluation result shows that LLMIF successfully addressed the above limitations. Compared with the existing Zigbee fuzzers, it increases the protocol message coverage and code coverage by 55.2% and 53.9%, respectively. Besides the enhanced coverage, LLMIF unearthed 11 vulnerabilities on real-world Zigbee devices, which include eight previously unknown vulnerabilities. Seven of them are not covered by the existing Zigbee fuzzers.",
        "keywords": [
            "program testing",
            "fuzzing"
        ],
        "doi": "10.1109/SP54263.2024.00211",
        "ISSN": "2375-1207",
        "month": "May",
        "venue": "S&P2024"
    },
    "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models": {
        "type": "INPROCEEDINGS",
        "key": "10646865",
        "author": "Aghakhani, Hojjat and Dai, Wei and Manoel, Andre and Fernandes, Xavier and Kharkar, Anant and Kruegel, Christopher and Vigna, Giovanni and Evans, David and Zorn, Ben and Sim, Robert",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "1122-1140",
        "abstract": "With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model\u2019s training by injecting malicious data. Poisoning attacks could be designed to influence the model\u2019s suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, Covert and TrojanPuzzle, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TrojanPuzzle, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TrojanPuzzle robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both Covert and TrojanPuzzle have significant implications for practitioners when selecting code used to train or tune code-suggestion models.",
        "keywords": [
            "code model security",
            "attack",
            "code generation",
            "code completion"
        ],
        "doi": "10.1109/SP54263.2024.00140",
        "ISSN": "2375-1207",
        "month": "May",
        "venue": "S&P2024"
    },
    "Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694987",
        "author": "Gao, Xinyu and Xiong, Yun and Wang, Deze and Guan, Zhenhan and Shi, Zejian and Wang, Haofen and Li, Shanshan",
        "title": "Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694987",
        "doi": "10.1145/3691620.3694987",
        "abstract": "Retrieval-augmented code generation utilizes Large Language Models as the generator and significantly expands their code generation capabilities by providing relevant code, documentation, and more via the retriever. The current approach suffers from two primary limitations: 1) information redundancy. The indiscriminate inclusion of redundant information can result in resource wastage and may misguide generators, affecting their effectiveness and efficiency. 2) preference gap. Due to different optimization objectives, the retriever strives to procure code with higher ground truth similarity, yet this effort does not substantially benefit the generator. The retriever and the generator may prefer different golden code, and this gap in preference results in a suboptimal design. Additionally, differences in parameterization knowledge acquired during pre-training result in varying preferences among different generators.To address these limitations, in this paper, we propose RRG (Retrieve, Refactor, Generate), a novel framework for effective and efficient code generation. This framework introduces a code refactorer module between the retriever and the generator to bridge them. The refactoring process transforms the raw retrieved code into a more concise, efficient, and model-friendly version. It eliminates redundant information and noise, reducing the input length. Consequently, the generator receives higher-quality context, enabling it to produce more accurate results with lower inference costs. We conducted comprehensive experiments on multiple datasets. In the experiments, we confirmed the existence of a preference gap between the retriever and the generator, and RRG effectively bridges this gap. Specifically, RRG achieved significant performance improvements, with increases of up to 28\\% on EM, 13\\% on BLEU, and 6.8\\% on CodeBLEU.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "65\u201377",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Instructive Code Retriever: Learn from Large Language Model's Feedback for Code Intelligence Tasks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694997",
        "author": "Lu, Jiawei and Wang, Haoye and Liu, Zhongxin and Liang, Keyu and Bao, Lingfeng and Yang, Xiaohu",
        "title": "Instructive Code Retriever: Learn from Large Language Model's Feedback for Code Intelligence Tasks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694997",
        "doi": "10.1145/3691620.3694997",
        "abstract": "Recent studies proposed to leverage large language models (LLMs) with In-Context Learning (ICL) to handle code intelligence tasks without fine-tuning. ICL employs task instructions and a set of examples as demonstrations to guide the model in generating accurate answers without updating its parameters. While ICL has proven effective for code intelligence tasks, its performance heavily relies on the selected examples. Previous work has achieved some success in using BM25 to retrieve examples for code intelligence tasks. However, existing approaches lack the ability to understand the semantic and structural information of queries, resulting in less helpful demonstrations. Moreover, they do not adapt well to the complex and dynamic nature of user queries in diverse domains. In this paper, we introduce a novel approach named Instructive Code Retriever (ICR), which is designed to retrieve examples that enhance model inference across various code intelligence tasks and datasets. We enable ICR to learn the semantic and structural information of the corpus by a tree-based loss function. To better understand the correlation between queries and examples, we incorporate the feedback from LLMs to guide the training of the retriever. Experimental results demonstrate that our retriever significantly outperforms state-of-the-art approaches. We evaluate our model's effectiveness on various tasks, i.e., code summarization, program synthesis, and bug fixing. Compared to previous state-of-the-art algorithms, our method achieved improvements of 50.0\\% and 90.0\\% in terms of BLEU-4 for two code summarization datasets, 74.6\\% CodeBLEU on program synthesis dataset, and increases of 3.6 and 3.2 BLEU-4 on two bug fixing datasets.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "191\u2013203",
        "numpages": "13",
        "keywords": [
            "prompt strategy",
            "RAG"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Understanding Code Changes Practically with Small-Scale Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694999",
        "author": "Li, Cong and Xu, Zhaogui and Di, Peng and Wang, Dongxia and Li, Zheng and Zheng, Qian",
        "title": "Understanding Code Changes Practically with Small-Scale Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694999",
        "doi": "10.1145/3691620.3694999",
        "abstract": "Recent studies indicate that traditional techniques for understanding code changes are not as effective as techniques that directly prompt language models (LMs). However, current LM-based techniques heavily rely on expensive, large LMs (LLMs) such as GPT-4 and Llama-13b, which are either commercial or prohibitively costly to deploy on a wide scale, thereby restricting their practical applicability. This paper explores the feasibility of deploying small LMs (SLMs) while maintaining comparable or superior performance to LLMs in code change understanding. To achieve this, we created a small yet high-quality dataset called HQCM which was meticulously reviewed, revised, and validated by five human experts. We fine-tuned state-of-the-art 7b and 220m SLMs using HQCM and compared them with traditional techniques and LLMs with \u226570b parameters. Our evaluation confirmed HQCM's benefits and demonstrated that SLMs, after finetuning by HQCM, can achieve superior performance in three change understanding tasks: change summarization, change classification, and code refinement. This study supports the use of SLMs in environments with security, computational, and financial constraints, such as in industry scenarios and on edge devices, distinguishing our work from the others.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "216\u2013228",
        "numpages": "13",
        "keywords": [
            "code summarization",
            "code model training",
            "source code model"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695000",
        "author": "Sun, Zhihong and Wan, Yao and Li, Jia and Zhang, Hongyu and Jin, Zhi and Li, Ge and Lyu, Chen",
        "title": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695000",
        "doi": "10.1145/3691620.3695000",
        "abstract": "Large Language Models (LLMs), such as GPT-4, StarCoder, and Code Llama, are transforming the way developers approach programming by automatically generating code based on given contexts, such as natural language descriptions or incomplete surrounding code. Despite advancements, generating syntactically and semantically correct code remains challenging, especially for complex programming tasks. Existing approaches typically generate multiple candidate solutions using LLMs to increase the likelihood of producing correct code. However, selecting the correct code from these candidates --- a process known as code ranking --- remains a major challenge. Current research on code ranking can be categorized into execution-based and non-execution-based methods. Execution-based methods, although effective, encounter notable limitations, such as scarcity of quality unit tests and security risks. Non-execution-based methods like CodeRanker, which rely solely on classification labels to train a code ranker, struggle to capture subtle errors and provide detailed error insights. Recognizing the strengths and limitations of both approaches, we propose a new method that integrates the advantages of execution-based and non-execution-based techniques. The key insight of our work is that an effective code ranker is expected to truly comprehend the underlying causes of erroneous code, as relying solely on classification labels is insufficient. Inspired by this, this paper puts forward RankEF, an innovative approach for code ranking that leverages execution feedback. RankEF employs multi-task learning to integrate code classification with execution feedback generation. This approach enables the model to understand the reasons behind incorrect code, distinguishing between correct and incorrect solutions without the need to execute the code during the ranking phase. Experiments on three code generation benchmarks---APPS, MBPP, and HumanEval---demonstrate that RankEF significantly outperforms the state-of-the-art CodeRanker, achieving relative improvements of +30.97\\%, +31.43\\%, and +19.51\\% in Pass@1, Pass@2, and Pass@5 on APPS test, respectively.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "229\u2013241",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695010",
        "author": "Zhang, Yichi and Liu, Zixi and Feng, Yang and Xu, Baowen",
        "title": "Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695010",
        "doi": "10.1145/3691620.3695010",
        "abstract": "Rust is renowned for its robust memory safety capabilities, yet its distinctive memory management model poses substantial challenges in both writing and understanding programs. Within Rust source code, comments are employed to clearly delineate conditions that might cause panic behavior, thereby warning developers about potential hazards associated with specific operations. Therefore, comments are particularly crucial for documenting Rust's program logic and design. Nevertheless, as modern software frequently undergoes updates and modifications, maintaining the accuracy and relevance of these comments becomes a labor-intensive endeavor.In this paper, inspired by the remarkable capabilities of Large Language Models (LLMs) in understanding software programs, we propose a code-comment inconsistency detection tool, namely RustC4, that combines program analysis and LLM-driven techniques to identify inconsistencies in code comments. RustC4 leverages LLMs' ability to interpret natural language descriptions within code comments, facilitating the extraction of design constraints. Program analysis techniques are then employed to accurately verify the implementation of these constraints. To evaluate the effectiveness of RustC4, we construct a dataset from 12 large-scale real-world Rust projects. The experiment results demonstrate that RustC4 is effective in detecting 176 real inconsistent cases and 23 of them have been confirmed and fixed by developers by the time this paper was submitted.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "356\u2013366",
        "numpages": "11",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695013",
        "author": "Wu, Yulun and Wen, Ming and Yu, Zeliang and Guo, Xiaochen and Jin, Hai",
        "title": "Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695013",
        "doi": "10.1145/3691620.3695013",
        "abstract": "Open-source software (OSS) has profoundly transformed the software development paradigm by facilitating effortless code reuse. However, in recent years, there has been an alarming increase in disclosed vulnerabilities within OSS, posing significant security risks to downstream users. Therefore, analyzing existing vulnerabilities and precisely assessing their threats to downstream applications become pivotal. Plenty of efforts have been made recently towards this problem, such as vulnerability reachability analysis and vulnerability reproduction. The key to these tasks is identifying the vulnerable function (i.e., the function where the root cause of a vulnerability resides). However, public vulnerability datasets (e.g., NVD) rarely include this information as pinpointing the exact vulnerable functions remains to be a longstanding challenge.Existing methods mainly detect vulnerable functions based on vulnerability patches or Proof-of-Concept (PoC). However, such methods face significant limitations due to data availability and the requirement for extensive manual efforts, thus hindering scalability. To address this issue, we propose a novel approach VFFinder that localizes vulnerable functions based on Common Vulnerabilities and Exposures (CVE) descriptions and the corresponding source code utilizing Large Language Models (LLMs). Specifically, VFFinder adopts a customized in-context learning (ICL) approach based on CVE description patterns to enable LLM to extract key entities. It then performs priority matching with the source code to localize vulnerable functions. We assess the performance of VFFinder on 75 large open-source projects. The results demonstrate that VFFinder surpasses existing baselines significantly. Notably, the Top-1 and MRR metrics have been improved substantially, averaging 4.25X and 2.37X respectively. We also integrate VFFinder with Software Composition Analysis (SCA) tools, and the results show that our tool can reduce the false positive rates of existing SCA tools significantly.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "393\u2013405",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695014",
        "author": "Wu, Guangyuan and Cao, Weining and Yao, Yuan and Wei, Hengfeng and Chen, Taolue and Ma, Xiaoxing",
        "title": "LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695014",
        "doi": "10.1145/3691620.3695014",
        "abstract": "Loop invariant inference, a key component in program verification, is a challenging task due to the inherent undecidability and complex loop behaviors in practice. Recently, machine learning based techniques have demonstrated impressive performance in generating loop invariants automatically. However, these methods highly rely on the labeled training data, and are intrinsically random and uncertain, leading to unstable performance. In this paper, we investigate a synergy of large language models (LLMs) and bounded model checking (BMC) to address these issues. The key observation is that, although LLMs may not be able to return the correct loop invariant in one response, they usually can provide all individual predicates of the correct loop invariant in multiple responses. To this end, we propose a \"query-filter-reassemble\" strategy, namely, we first leverage the language generation power of LLMs to produce a set of candidate invariants, where training data is not needed. Then, we employ BMC to identify valid predicates from these candidate invariants, which are assembled to produce new candidate invariants and checked by off-the-shelf SMT solvers. The feedback is incorporated into the prompt for the next round of LLM querying. We expand the existing benchmark of 133 programs to 316 programs, providing a more comprehensive testing ground. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art techniques, successfully generating 309 loop invariants out of 316 cases, whereas the existing baseline methods are only able to tackle 219 programs at best. The code is publicly available at https://github.com/SoftWiser-group/LaM4Inv.git.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "406\u2013417",
        "numpages": "12",
        "keywords": [
            "static analysis",
            "program verification"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Semantic-Enhanced Indirect Call Analysis with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695016",
        "author": "Cheng, Baijun and Zhang, Cen and Wang, Kailong and Shi, Ling and Liu, Yang and Wang, Haoyu and Guo, Yao and Li, Ding and Chen, Xiangqun",
        "title": "Semantic-Enhanced Indirect Call Analysis with Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695016",
        "doi": "10.1145/3691620.3695016",
        "abstract": "In contemporary software development, the widespread use of indirect calls to achieve dynamic features poses challenges in constructing precise control flow graphs (CFGs), which further impacts the performance of downstream static analysis tasks. To tackle this issue, various types of indirect call analyzers have been proposed. However, they do not fully leverage the semantic information of the program, limiting their effectiveness in real-world scenarios.To address these issues, this paper proposes Semantic-Enhanced Analysis (SEA), a new approach to enhance the effectiveness of indirect call analysis. Our fundamental insight is that for common programming practices, indirect calls often exhibit semantic similarity with their invoked targets. This semantic alignment serves as a supportive mechanism for static analysis techniques in filtering out false targets. Notably, contemporary large language models (LLMs) are trained on extensive code corpora, encompassing tasks such as code summarization, making them well-suited for semantic analysis. Specifically, SEA leverages LLMs to generate natural language summaries of both indirect calls and target functions from multiple perspectives. Through further analysis of these summaries, SEA can determine their suitability as caller-callee pairs. Experimental results demonstrate that SEA can significantly enhance existing static analysis methods by producing more precise target sets for indirect calls.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "430\u2013442",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "fundamental analysis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "WaDec: Decompiling WebAssembly Using Large Language Model": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695020",
        "author": "She, Xinyu and Zhao, Yanjie and Wang, Haoyu",
        "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695020",
        "doi": "10.1145/3691620.3695020",
        "abstract": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34\\%, a dramatic 97\\% reduction compared to the state-of-the-art's 116.94\\%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11\\%, a re-execution rate of 43.55\\%, and an output consistency of 27.15\\%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185\\%, cyclomatic complexity by 8\\%, and cosine similarity by 41\\%, achieving an average code similarity above 50\\%. In summary, WaDec enhances understanding of the code's structure and execution flow, facilitating automated code analysis, optimization, and security auditing.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "481\u2013492",
        "numpages": "12",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "code generation",
            "program decompilation",
            "code model training",
            "source code model"
        ]
    },
    "GraphCoder: Enhancing Repository-Level Code Completion via Coarse-to-fine Retrieval Based on Code Context Graph": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695054",
        "author": "Liu, Wei and Yu, Ailun and Zan, Daoguang and Shen, Bo and Zhang, Wei and Zhao, Haiyan and Jin, Zhi and Wang, Qianxiang",
        "title": "GraphCoder: Enhancing Repository-Level Code Completion via Coarse-to-fine Retrieval Based on Code Context Graph",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695054",
        "doi": "10.1145/3691620.3695054",
        "abstract": "The performance of repository-level code completion depends upon the effective leverage of both general and repository-specific knowledge. Despite the impressive capability of code LLMs in general code completion tasks, they often exhibit less satisfactory performance on repository-level completion due to the lack of repository-specific knowledge in these LLMs. To address this problem, we propose GraphCoder, a retrieval-augmented code completion framework that leverages LLMs' general code knowledge and the repository-specific knowledge via a graph-based retrieval-generation process. In particular, GraphCoder captures the context of completion target more accurately through code context graph (CCG) that consists of control-flow, data- and control-dependence between code statements, a more structured way to capture the completion target context than the sequence-based context used in existing retrieval-augmented approaches; based on CCG, GraphCoder further employs a coarse-to-fine retrieval process to locate context-similar code snippets with the completion target from the current repository. Experimental results demonstrate both the effectiveness and efficiency of GraphCoder: Compared to baseline retrieval-augmented methods, GraphCoder achieves higher exact match (EM) on average, with increases of +6.06 in code match and +6.23 in identifier match, while using less time and space.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "570\u2013581",
        "numpages": "12",
        "keywords": [
            "code generation",
            "code completion"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695055",
        "author": "Wu, Cong and Chen, Jing and Wang, Ziwei and Liang, Ruichao and Du, Ruiying",
        "title": "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695055",
        "doi": "10.1145/3691620.3695055",
        "abstract": "Smart contracts, self-executing agreements directly encoded in code, are fundamental to blockchain technology, especially in decentralized finance (DeFi) and Web3. However, the rise of Ponzi schemes in smart contracts poses significant risks, leading to substantial financial losses and eroding trust in blockchain systems. Existing detection methods, such as PonziGuard, depend on large amounts of labeled data and struggle to identify unseen Ponzi schemes, limiting their reliability and generalizability. In contrast, we introduce PonziSleuth, the first LLM-driven approach for detecting Ponzi smart contracts, which requires no labeled training data. PonziSleuth utilizes advanced language understanding capabilities of LLMs to analyze smart contract source code through a novel two-step zero-shot chain-of-thought prompting technique. Our extensive evaluation on benchmark datasets and real-world contracts demonstrates that PonziSleuth delivers comparable, and often superior, performance without the extensive data requirements, achieving a balanced detection accuracy of 96.06\\% with GPT-3.5-turbo, 93.91\\% with LLAMA3, and 94.27\\% with Mistral. In real-world detection, PonziSleuth successfully identified 15 new Ponzi schemes from 4,597 contracts verified by Etherscan in March 2024, with a false negative rate of 0\\% and a false positive rate of 0.29\\%. These results highlight PonziSleuth's capability to detect diverse and novel Ponzi schemes, marking a significant advancement in leveraging LLMs for enhancing blockchain security and mitigating financial scams.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "582\u2013593",
        "numpages": "12",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695062",
        "author": "Liu, Fang and Liu, Zhenwei and Zhao, Qianhui and Jiang, Jing and Zhang, Li and Sun, Zian and Li, Ge and Li, Zhongqi and Ma, Yuchi",
        "title": "FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695062",
        "doi": "10.1145/3691620.3695062",
        "abstract": "Providing personalized and timely feedback for student's programming assignments is useful for programming education. Automated program repair (APR) techniques have been used to fix the bugs in programming assignments, where the Large Language Models (LLMs) based approaches have shown promising results. Given the growing complexity of identifying and fixing bugs in advanced programming assignments, current fine-tuning strategies for APR are inadequate in guiding the LLM to identify bugs and make accurate edits during the generative repair process. Furthermore, the autoregressive decoding approach employed by the LLM could potentially impede the efficiency of the repair, thereby hindering the ability to provide timely feedback. To tackle these challenges, we propose FastFixer, an efficient and effective approach for programming assignment repair. To assist the LLM in accurately identifying and repairing bugs, we first propose a novel repair-oriented fine-tuning strategy, aiming to enhance the LLM's attention towards learning how to generate the necessary patch and its associated context. Furthermore, to speed up the patch generation, we propose an inference acceleration approach that is specifically tailored for the program repair task. The evaluation results demonstrate that FastFixer obtains an overall improvement of 20.46\\% in assignment fixing when compared to the state-of-the-art baseline. Considering the repair efficiency, FastFixer achieves a remarkable inference speedup of 16.67\\texttimes{} compared to the autoregressive decoding algorithm.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "669\u2013680",
        "numpages": "12",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "DroidCoder: Enhanced Android Code Completion with Context-Enriched Retrieval-Augmented Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695063",
        "author": "Yu, Xinran and Li, Chun and Pan, Minxue and Li, Xuandong",
        "title": "DroidCoder: Enhanced Android Code Completion with Context-Enriched Retrieval-Augmented Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695063",
        "doi": "10.1145/3691620.3695063",
        "abstract": "Android is the most popular mobile operating system. However, Android development requires extensive coding, especially for unique features such as lifecycle callbacks and UI widgets. Existing code completion methods typically utilize Retrieval-Augmented Generation (RAG) to provide contextual information for pre-trained code large language models (Code LLMs) to perform completion. Despite considerable progress in these methods, their effectiveness in Android development remains limited. This is because the features of Android development make it challenging for existing retrieval mechanisms to extract sufficient context effectively. In response, we propose DroidCoder, a novel Android code completion framework that employs Android development features and contextual information of code snippets to enrich RAG. It also incorporates a specifically designed loss function to fine-tune the model, enabling it to better utilize context-enhanced RAG for Android code completion. We evaluated our method on three base models and different types of applications, comparing it with two state-of-the-art code completion methods. The experimental results demonstrate that our method significantly outperforms the baselines at line-level and multi-line-level code completion and improves the quality of the completed code.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "681\u2013693",
        "numpages": "13",
        "keywords": [
            "code generation",
            "code completion",
            "prompting strategy",
            "RAG"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695066",
        "author": "Li, Guochang and Zhi, Chen and Chen, Jialiang and Han, Junxiao and Deng, Shuiguang",
        "title": "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695066",
        "doi": "10.1145/3691620.3695066",
        "abstract": "Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that \"pre-training and fine-tuning\" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-Instruction, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-Instruction. The best fine-tuned model fixes 58\\% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that (IA)3 improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-Instruction, PEFT weights, and the fine-tuning code are publicly available as open-source resources.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "719\u2013731",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair",
            "code model training",
            "source code model"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695068",
        "author": "Cui, Di and Wang, Jiaqi and Wang, Qiangqiang and Ji, Peng and Qiao, Minglang and Zhao, Yutong and Hu, Jingzhao and Wang, Luqiao and Li, Qingshan",
        "title": "Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695068",
        "doi": "10.1145/3691620.3695068",
        "abstract": "Methods implemented in incorrect classes will cause excessive reliance on other classes than their own, known as a typical code smell symptom: feature envy, which makes it difficult to maintain increased coupling between classes. Addressing this issue, several Move Method refactoring tools have been proposed, employing a two-phase process: identifying misplaced methods to move and appropriate classes to receive, and implementing the mechanics of refactoring. These tools traditionally use hard-coded metrics to measure correlations between movable methods and target classes and apply heuristic thresholds or trained classifiers to unearth refactoring opportunities. Yet, these approaches predominantly illuminate pairwise correlations between methods and classes while overlooking the complex and complicated dependencies binding multiple code entities within these methods/classes that are prevalent in real-world cases. This narrow focus can lead to refactoring suggestions that may diverge from developers' actual needs. To bridge this gap, our paper leverages the concept of inter-class code entity dependency hypergraph to model complicated dependency relationships involving multiple code entities within various methods/classes and proposes a hypergraph learning-based approach to suggest Move Method refactoring opportunities named HMove. We first construct inter-class code entity dependency hypergraphs from training samples and assign attributes to entities with a pre-trained code model. All the attributed hypergraphs are fed into a hybrid hypergraph neural network for training. Utilizing this trained neural network alongside a large language model, we construct a refactoring suggestion system. We trained HMove on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 27.8\\% in precision, 2.5\\% in recall, and 18.5\\% in f1-measure compared to 9 state-of-the-art refactoring tools, which is more useful for 68\\% of participants. The results also unveil practical suggestions and new insights that benefit existing feature envy-related refactoring techniques.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "745\u2013757",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "An Empirical Study to Evaluate AIGC Detectors on Code Content": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695468",
        "author": "JianWang and Liu, Shangqing and Xie, Xiaofei and Li, Yi",
        "title": "An Empirical Study to Evaluate AIGC Detectors on Code Content",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695468",
        "doi": "10.1145/3691620.3695468",
        "abstract": "Artificial Intelligence Generated Content (AIGC) has garnered considerable attention for its impressive performance, with Large Language Models (LLMs), like ChatGPT, emerging as a leading AIGC model that produces high-quality responses across various applications, including software development and maintenance. Despite its potential, the misuse of LLMs, especially in security and safety-critical domains, such as academic integrity and answering questions on Stack Overflow, poses significant concerns. Numerous AIGC detectors have been developed and evaluated on natural language data. However, their performance on code-related content generated by LLMs remains unexplored.To fill this gap, in this paper, we present an empirical study evaluating existing AIGC detectors in the software domain. We select three state-of-the-art LLMs, i.e., GPT-3.5, WizardCoder and CodeLlama, for machine-content generation. We further created a comprehensive dataset including 2.23M samples comprising code-related content for each model, encompassing popular software activities like Q&amp;A (150K), code summarization (1M), and code generation (1.1M). We evaluated thirteen AIGC detectors, comprising six commercial and seven open-source solutions, assessing their performance on this dataset. Our results indicate that AIGC detectors perform less on code-related data than natural language data. Fine-tuning can enhance detector performance, especially for content within the same domain; but generalization remains a challenge.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "844\u2013856",
        "numpages": "13",
        "keywords": [
            "general coding task",
            "benchmark"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695470",
        "author": "Cao, Jialun and Chen, Zhiyong and Wu, Jiarong and Cheung, Shing-Chi and Xu, Chang",
        "title": "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695470",
        "doi": "10.1145/3691620.3695470",
        "abstract": "Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8\\% of benchmarks involve Python, while only 5 benchmarks involve Java, resulting in an insufficient understanding of LLMs' capability to generate Java code. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3\\% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills (e.g., variables, operators, and control structures), while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism). Considering the prevalence of these advanced features in real-world Java project development, constructing benchmarks to test LLMs on handling OOP features is necessary.To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92\\%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 48.24\\% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench. We also release a leaderboard and invite model developers to participate and test their models against JavaBench at https://java-bench.github.io/leaderboard.html.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "870\u2013882",
        "numpages": "13",
        "keywords": [
            "benchmark",
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695480",
        "author": "Chen, Jiachi and Zhong, Qingyuan and Wang, Yanlin and Ning, Kaiwen and Liu, Yongkun and Xu, Zenan and Zhao, Zhe and Chen, Ting and Zheng, Zibin",
        "title": "RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695480",
        "doi": "10.1145/3691620.3695480",
        "abstract": "Warning: Please note that this article contains potential harmful or offensive content. This content is only for the evaluating and analysis of LLMs and does not imply any intention to promote criminal activities.The emergence of Large Language Models (LLMs) has significantly influenced various aspects of software development activities. Despite their benefits, LLMs also pose notable risks, including the potential to generate harmful content and being abused by malicious developers to create malicious code. Several previous studies have focused on the ability of LLMs to resist the generation of harmful content that violates human ethical standards, such as biased or offensive content. However, there is no research evaluating the ability of LLMs to resist malicious code generation. To fill this gap, we propose RMCBench, the first benchmark comprising 473 prompts designed to assess the ability of LLMs to resist malicious code generation. This benchmark employs two scenarios: a text-to-code scenario, where LLMs are prompted with descriptions to generate code, and a code-to-code scenario, where LLMs translate or complete existing malicious code. Based on RMCBench, we conduct an empirical study on the 11 representative LLMs to assess their ability to resist malicious code generation. Our findings indicate that current LLMs have a limited ability to resist malicious code generation with an average refusal rate of 40.36\\% in text-to-code scenario and 11.52\\% in code-to-code scenario. The average refusal rate of all LLMs in RMCBench is only 28.71\\%; ChatGPT-4 has a refusal rate of only 35.73\\%. We also analyze the factors that affect LLM's ability to resist malicious code generation and provide implications for developers to enhance model robustness.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "995\u20131006",
        "numpages": "12",
        "keywords": [
            "benchmark",
            "code model security",
            "attack"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Contextualized Data-Wrangling Code Generation in Computational Notebooks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695503",
        "author": "Huang, Junjie and Guo, Daya and Wang, Chenglong and Gu, Jiazhen and Lu, Shuai and Inala, Jeevana Priya and Yan, Cong and Gao, Jianfeng and Duan, Nan and Lyu, Michael R.",
        "title": "Contextualized Data-Wrangling Code Generation in Computational Notebooks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695503",
        "doi": "10.1145/3691620.3695503",
        "abstract": "Data wrangling, the process of preparing raw data for further analysis in computational notebooks, is a crucial yet time-consuming step in data science. Code generation has the potential to automate the data wrangling process to reduce analysts' overhead by translating user intents into executable code. Precisely generating data wrangling code necessitates a comprehensive consideration of the rich context present in notebooks, including textual context, code context and data context. However, notebooks often interleave multiple non-linear analysis tasks into linear sequence of code blocks, where the contextual dependencies are not clearly reflected. Directly training models with source code blocks fails to fully exploit the contexts for accurate wrangling code generation.To bridge the gap, we aim to construct a high quality datasets with clear and rich contexts to help training models for data wrangling code generation tasks. In this work, we first propose an automated approach, CoCoMine to mine data-wrangling code generation examples with clear multi-modal contextual dependency. It first adopts data flow analysis to identify the code blocks containing data wrangling codes. Then, CoCoMine extracts the contextualized data-wrangling code examples through tracing and replaying notebooks. With CoCoMine, we construct CoCoNote, a dataset containing 58,221 examples for Contextualized Data-wrangling Code generation in Notebooks. To demonstrate the effectiveness of our dataset, we finetune a range of pretrained code models and prompt various large language models on our task. Furthermore, we also propose Data-Coder, which encodes data context and code&amp;textual contexts separately to enhance code generation. Experiment results demonstrate the significance of incorporating data context in data-wrangling code generation and the effectiveness of our model. We release code and data at https://github.com/Jun-jie-Huang/CoCoNote.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1282\u20131294",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Dynamic Scoring Code Token Tree: A Novel Decoding Strategy for Generating High-Performance Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695505",
        "author": "Qu, Muzi and Liu, Jie and Kang, Liangyi and Wang, Shuai and Ye, Dan and Huang, Tao",
        "title": "Dynamic Scoring Code Token Tree: A Novel Decoding Strategy for Generating High-Performance Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695505",
        "doi": "10.1145/3691620.3695505",
        "abstract": "Within the realms of scientific computing, large-scale data processing, and artificial intelligence-powered computation, disparities in performance, which originate from differing code implementations, directly influence the practicality of the code. Although existing works tried to utilize code knowledge to enhance the execution performance of codes generated by large language models, they neglect code evaluation outcomes which directly refer to the code execution details, resulting in inefficient computation. To address this issue, we propose DSCT-Decode, an innovative adaptive decoding strategy for large language models, that employs a data structure named 'Code Token Tree' (CTT), which guides token selection based on code evaluation outcomes. DSCT-Decode assesses generated code across three dimensions---correctness, performance, and similarity---and utilizes a dynamic penalty-based boundary intersection method to compute multi-objective scores, which are then used to adjust the scores of nodes in the CTT during backpropagation. By maintaining a balance between exploration, through token selection probabilities, and exploitation, through multi-objective scoring, DSCT-Decode effectively navigates the code space to swiftly identify high-performance code solutions. To substantiate our framework, we developed a new benchmark, big-DS-1000, which is an extension of DS-1000. This benchmark is the first of its kind to specifically evaluate code generation methods based on execution performance. Comparative evaluations with leading large language models, such as CodeLlama and GPT-4, show that our framework achieves an average performance enhancement of nearly 30\\%. Furthermore, 30\\% of the codes exhibited a performance improvement of more than 20\\%, underscoring the effectiveness and potential of our framework for practical applications.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1308\u20131318",
        "numpages": "11",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model training",
            "source code model"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695506",
        "author": "Zhang, Huan and Cheng, Wei and Wu, Yuhan and Hu, Wei",
        "title": "A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695506",
        "doi": "10.1145/3691620.3695506",
        "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation. The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback. The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement. This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers. We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks. Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00\\%--162.43\\% compared to prompting LLMs directly.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1319\u20131331",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis",
            "agent design",
            "planning"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "iSMELL: Assembling LLMs with Expert Toolsets for Code Smell Detection and Refactoring": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695508",
        "author": "Wu, Di and Mu, Fangwen and Shi, Lin and Guo, Zhaoqiang and Liu, Kui and Zhuang, Weiguang and Zhong, Yuqi and Zhang, Li",
        "title": "iSMELL: Assembling LLMs with Expert Toolsets for Code Smell Detection and Refactoring",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695508",
        "doi": "10.1145/3691620.3695508",
        "abstract": "Detecting and refactoring code smells is challenging, laborious, and sustaining. Although large language models have demonstrated potential in identifying various types of code smells, they also have limitations such as input-output token restrictions, difficulty in accessing repository-level knowledge, and performing dynamic source code analysis. Existing learning-based methods or commercial expert toolsets have advantages in handling complex smells. They can analyze project structures and contextual information in-depth, access global code repositories, and utilize advanced code analysis techniques. However, these toolsets are often designed for specific types and patterns of code smells and can only address fixed smells, lacking flexibility and scalability. To resolve that problem, we propose iSMELL, an ensemble approach that employs various code smell detection toolsets via Mixture of Experts (MoE) architecture for comprehensive code smell detection, and enhances the LLMs with the detection results from expert toolsets for refactoring those identified code smells. First, we train a MoE model that, based on input code vectors, outputs the most suitable expert tool for identifying each type of smell. Then, we select the recommended toolsets for code smell detection and obtain their results. Finally, we equip the prompts with the detection results from the expert toolsets, thereby enhancing the refactoring capability of LLMs for code with existing smells, enabling them to provide different solutions based on the type of smell. We evaluate our approach on detecting and refactoring three classical and complex code smells, i.e., Refused Bequest, God Class, and Feature Envy. The results show that, by adopting seven expert code smell toolsets, iSMELL achieved an average F1 score of 75.17\\% on code smell detection, outperforming LLMs baselines by an increase of 35.05\\% in F1 score. We further evaluate the code refactored by the enhanced LLM. The quantitative and human evaluation results show that iSMELL could improve code quality metrics and conduct satisfactory refactoring toward the identified code smells. We believe that our proposed solution could provide new insights into better leveraging LLMs and existing approaches to resolving complex software tasks.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1345\u20131357",
        "numpages": "13",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "code generation",
            "program repair",
            "static analysis",
            "bug detection"
        ]
    },
    "LLM-Generated Invariants for Bounded Model Checking Without Loop Unrolling": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695512",
        "author": "Pirzada, Muhammad A. A. and Reger, Giles and Bhayat, Ahmed and Cordeiro, Lucas C.",
        "title": "LLM-Generated Invariants for Bounded Model Checking Without Loop Unrolling",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695512",
        "doi": "10.1145/3691620.3695512",
        "abstract": "We investigate a modification of the classical Bounded Model Checking (BMC) procedure that does not handle loops through unrolling but via modifications to the control flow graph (CFG). A portion of the CFG representing a loop is replaced by a node asserting invariants of the loop. We generate these invariants using Large Language Models (LLMs) and use a first-order theorem prover to ensure the correctness of the generated statements. We thus transform programs to loop-free variants in a sound manner. Our experimental results show that the resulting tool, ESBMC ibmc, is competitive with state-of-the-art formal verifiers for programs with unbounded loops, significantly improving the number of programs verified by the industrial-strength software verifier ESBMC and verifying programs that state-of-the-art software verifiers such as SeaHorn and VeriAbs could not.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1395\u20131407",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "program verification",
            "invariant generation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Semi-Supervised Code Translation Overcoming the Scarcity of Parallel Code Data": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695524",
        "author": "Zhu, Ming and Karim, Mohimenul and Lourentzou, Ismini and Yao, Daphne",
        "title": "Semi-Supervised Code Translation Overcoming the Scarcity of Parallel Code Data",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695524",
        "doi": "10.1145/3691620.3695524",
        "abstract": "Neural code translation is the task of converting source code from one programming language to another. One of the main challenges is the scarcity of parallel code data, which hinders the ability of translation models to learn accurate cross-language alignments. In this paper, we introduce MIRACLE, a semi-supervised approach that improves code translation through synthesizing high-quality parallel code data and curriculum learning on code data with ascending alignment levels. MIRACLE leverages static analysis and compilation to generate synthetic parallel code datasets with enhanced quality and alignment to address the challenge of data scarcity. We evaluate the proposed method along with strong baselines including instruction-tuned Large Language Models (LLMs) for code. Our analysis reveals that LLMs pre-trained on open-source code data, regardless of their size, suffer from the \"shallow translation\" problem. This issue arises when translated code copies keywords, statements, and even code blocks from the source language, leading to compilation and runtime errors. Extensive experiments demonstrate that our method significantly mitigates this issue, enhancing code translation performance across multiple models in C++, Java, Python, and C. Remarkably, MIRACLE outperforms code LLMs that are ten times larger in size. MIRACLE also achieves up to a 43\\% improvement in C code translation with fewer than 150 annotated examples.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1545\u20131556",
        "numpages": "12",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Test-Driven Development and LLM-based Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695527",
        "author": "Mathews, Noble Saji and Nagappan, Meiyappan",
        "title": "Test-Driven Development and LLM-based Code Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695527",
        "doi": "10.1145/3691620.3695527",
        "abstract": "Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1583\u20131594",
        "numpages": "12",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "On the Evaluation of Large Language Models in Unit Test Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695529",
        "author": "Yang, Lin and Yang, Chen and Gao, Shutao and Wang, Weijing and Wang, Bo and Zhu, Qihao and Chu, Xiao and Zhou, Jianyi and Liang, Guangtai and Wang, Qianxiang and Chen, Junjie",
        "title": "On the Evaluation of Large Language Models in Unit Test Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695529",
        "doi": "10.1145/3691620.3695529",
        "abstract": "Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs' capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1607\u20131619",
        "numpages": "13",
        "keywords": [
            "program testing",
            "unit testing",
            "empirical study"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695536",
        "author": "Chen, Mouxiang and Liu, Zhongxin and Tao, He and Hong, Yusu and Lo, David and Xia, Xin and Sun, Jianling",
        "title": "B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695536",
        "doi": "10.1145/3691620.3695536",
        "abstract": "Selecting the best code solution from multiple generated ones is an essential task in code generation, which can be achieved by using some reliable validators (e.g., developer-written test cases) for assistance. Since reliable test cases are not always available and can be expensive to build in practice, researchers propose to automatically generate test cases to assess code solutions. However, when both code solutions and test cases are plausible and not reliable, selecting the best solution becomes challenging. Although some heuristic strategies have been proposed to tackle this problem, they lack a strong theoretical guarantee and it is still an open question whether an optimal selection strategy exists. Our work contributes in two ways. First, we show that within a Bayesian framework, the optimal selection strategy can be defined based on the posterior probability of the observed passing states between solutions and tests. The problem of identifying the best solution is then framed as an integer programming problem. Second, we propose an efficient approach for approximating this optimal (yet uncomputable) strategy, where the approximation error is bounded by the correctness of prior knowledge. We then incorporate effective prior knowledge to tailor code generation tasks. Both theoretical and empirical studies confirm that existing heuristics are limited in selecting the best solutions with plausible test cases. Our proposed approximated optimal strategy \u212c4 significantly surpasses existing heuristics in selecting code solutions generated by large language models (LLMs) with LLM-generated tests, achieving a relative performance improvement by up to 50\\% over the strongest heuristic and 246\\% over the random selection in the most challenging scenarios. Our code is publicly available at https://github.com/ZJU-CTAG/B4.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1693\u20131705",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695552",
        "author": "Feng, Jia and Liu, Jiachen and Gao, Cuiyun and Chong, Chun Yong and Wang, Chaozheng and Gao, Shan and Xia, Xin",
        "title": "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695552",
        "doi": "10.1145/3691620.3695552",
        "abstract": "In recent years, with the widespread attention of academia and industry on the application of large language models (LLMs) to code-related tasks, an increasing number of large code models (LCMs) have been proposed and corresponding evaluation benchmarks have continually emerged. Although existing evaluation benchmarks are helpful for comparing different LCMs, they may not reflect the performance of LCMs in various development scenarios. Specifically, they might evaluate model performance in only one type of scenario (e.g., code generation or code completion), whereas real development contexts are diverse and may involve multiple tasks such as code generation, code completion, API recommendation, and test function generation. Additionally, the questions may not originate from actual development practices, failing to capture the programming challenges faced by developers during the development process.To address the aforementioned issues, we propose Complex-CodeEval, a new benchmark for evaluating the performance of LCMs in various development scenarios. ComplexCodeEval includes 3,897 Java samples from 1,055 high-star GitHub repositories and 7,184 Python samples from 2,107 high-star repositories. Each function sample in ComplexCodeEval contains multiple annotations (e.g., function signatures, docstrings and reference APIs) to accommodate various downstream tasks. Furthermore, to better reflect diverse development scenarios, each function sample is required to originate from a repository that depends on at least one selected library (based on popularity), and each function sample must invoke at least one API from the selected library. Additionally, each function sample has multiple timestamps to avoid data leakage. Based on ComplexCodeEval, we evaluate the performance of ten LCMs across four tasks (i.e., code generation, code completion, API recommendation, and test case generation) to explore their performance in complex development environments. Furthermore, we conduct an in-depth analysis of the impact of context and data leakage on model performance. Our experimental results reveal several key findings. For instance, LCMs exhibit varying performance across different coding tasks. Additionally, rich contextual information can greatly enhance the performance of LCMs. Moreover, using leaked data for evaluation may lead to an overestimation of model performance, resulting in inaccurate evaluation outcomes that deviate from the performance in practice.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1895\u20131906",
        "numpages": "12",
        "keywords": [
            "general coding task",
            "benchmark"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Understanding Developer-Analyzer Interactions in Code Reviews": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695257",
        "author": "Schaef, Martin and Cirisci, Berk and Luo, Linghui and Mansur, Muhammad Numair and Tripp, Omer and Sanchez, Daniel and Zhou, Qiang and Zafar, Muhammad Bilal",
        "title": "Understanding Developer-Analyzer Interactions in Code Reviews",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695257",
        "doi": "10.1145/3691620.3695257",
        "abstract": "Static code analyzers are now a common part of the codereview process. These automated tools integrate into the code review process by commenting on code changes and suggesting improvements, in the same way as human reviewers. The comments made by static analyzers often trigger a conversation between developers to align on if and how the issue should be fixed. Because developers rarely give feedback directly to the tool, understanding the sentiment and intent in the conversation triggered by the tool comments can be used to measure the usefulness of the static analyzer.In this paper, we report on an experiment where we use large language models to automatically label and categorize the sentiment and intent of such conversations triggered by static analyzer comments. Our experiment demonstrates that LLMs not only classify and interpret complex developer-analyzer conversations, but can be more accurate than human experts.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1945\u20131955",
        "numpages": "11",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "software maintenance and deployment",
            "code review",
            "empirical study"
        ]
    },
    "Ansible Lightspeed: A Code Generation Service for IT Automation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695277",
        "author": "Sahoo, Priyam and Pujar, Saurabh and Nalawade, Ganesh and Genhardt, Richard and Mandel, Louis and Buratti, Luca",
        "title": "Ansible Lightspeed: A Code Generation Service for IT Automation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695277",
        "doi": "10.1145/3691620.3695277",
        "abstract": "The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for Information Technology (IT) automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Ansible Lightspeed is an LLM-based service designed explicitly to generate Ansible YAML, given natural language prompt.In this paper, we present the design and implementation of the Ansible Lightspeed service. We then evaluate its utility to developers using diverse indicators, including extended utilization, analysis of user edited suggestions, as well as user sentiments analysis. The evaluation is based on data collected for 10,696 real users including 3,910 returning users. The code for Ansible Lightspeed service and the analysis framework is made available for others to use.To our knowledge, our study is the first to involve thousands of users of code assistants for domain-specific languages. We are also the first code completion tool to present N-Day user retention figures, which is 13.66\\% on Day 30. We propose an improved version of user acceptance rate, called Strong Acceptance rate, where a suggestion is considered accepted only if less than 50\\% of it is edited and these edits do not change critical parts of the suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong acceptance rate of 49.08\\% for multi-line Ansible task suggestions. With our findings we provide insights into the effectiveness of small, dedicated models in a domain-specific context. We hope this work serves as a reference for software engineering and machine learning researchers exploring code completion.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2148\u20132158",
        "numpages": "11",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695290",
        "author": "Zhang, Beiqi and Liang, Peng and Feng, Qiong and Fu, Yujia and Li, Zengyang",
        "title": "Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695290",
        "doi": "10.1145/3691620.3695290",
        "abstract": "As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released in September 2023, functions as an interactive tool aimed at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot Chat's ability to fix the code smells. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot Chat in fixing these code smells employing different prompts. The results show that 8 out of 10 types of code smells can be detected in Copilot-generated Python code, among which Multiply-Nested Container is the most common one. For these code smells, Copilot Chat achieves a highest fixing rate of 87.1\\%, showing promise in fixing Python code smells generated by Copilot itself. In addition, the effectiveness of Copilot Chat in fixing these smells can be improved by providing more detailed prompts.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2230\u20132234",
        "numpages": "5",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Attacks and Defenses for Large Language Models on Coding Tasks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695297",
        "author": "Zhang, Chi and Wang, Zifan and Zhao, Ruoshi and Mangal, Ravi and Fredrikson, Matt and Jia, Limin and Pasareanu, Corina",
        "title": "Attacks and Defenses for Large Language Models on Coding Tasks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695297",
        "doi": "10.1145/3691620.3695297",
        "abstract": "Modern large language models (LLMs), such as ChatGPT, have demonstrated impressive capabilities for coding tasks, including writing and reasoning about code. They improve upon previous neural network models of code, such as code2seq or seq2seq, that already demonstrated competitive results when performing tasks such as code summarization and identifying code vulnerabilities. However, these previous code models were shown vulnerable to adversarial examples, i.e., small syntactic perturbations designed to \"fool\" the models. In this paper, we first aim to study the transferability of adversarial examples, generated through white-box attacks on smaller code models, to LLMs. We also propose a new attack using an LLM to generate the perturbations. Further, we propose novel cost-effective techniques to defend LLMs against such adversaries via prompting, without incurring the cost of retraining. These prompt-based defenses involve modifying the prompt to include additional information, such as examples of adversarially perturbed code and explicit instructions for reversing adversarial perturbations. Our preliminary experiments show the effectiveness of the attacks and the proposed defenses on popular LLMs such as GPT-3.5 and GPT-4.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2268\u20132272",
        "numpages": "5",
        "keywords": [
            "code model security",
            "attack",
            "defense"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "RepoSim: Evaluating Prompt Strategies for Code Completion via User Behavior Simulation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695299",
        "author": "Peng, Chao and Wu, Qinyun and Liu, Jiangchao and Liu, Jierui and Jiang, Bo and Xu, Mengqian and Wang, Yinghao and Liu, Xia and Yang, Ping",
        "title": "RepoSim: Evaluating Prompt Strategies for Code Completion via User Behavior Simulation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695299",
        "doi": "10.1145/3691620.3695299",
        "abstract": "Large language models (LLMs) have revolutionized code completion tasks. IDE plugins such as MarsCode can generate code recommendations, saving developers significant time and effort. However, current evaluation methods for code completion are limited by their reliance on static code benchmarks, which do not consider human interactions and evolving repositories. This paper proposes RepoSim, a novel benchmark designed to evaluate code completion tasks by simulating the evolving process of repositories and incorporating user behaviors. RepoSim leverages data from an IDE plugin, by recording and replaying user behaviors to provide a realistic programming context for evaluation. This allows for the assessment of more complex prompt strategies, such as utilizing recently visited files and incorporating user editing history. Additionally, RepoSim proposes a new metric based on users' acceptance or rejection of predictions, offering a user-centric evaluation criterion. Our preliminary evaluation demonstrates that incorporating users' recent edit history into prompts significantly improves the quality of LLM-generated code, highlighting the importance of temporal context in code completion. RepoSim represents a significant advancement in benchmarking tools, offering a realistic and user-focused framework for evaluating code completion performance.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2279\u20132283",
        "numpages": "5",
        "keywords": [
            "code generation",
            "code completion"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "PACGBI: A Pipeline for Automated Code Generation from Backlog Items": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695346",
        "author": "Sarschar, Mahja and Zhang, Gefei and Nowak, Annika",
        "title": "PACGBI: A Pipeline for Automated Code Generation from Backlog Items",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695346",
        "doi": "10.1145/3691620.3695346",
        "abstract": "While there exist several tools to leverage Large Language Models (LLMs) for code generation, their capabilities are limited to the source code editor and are disconnected from the overall software development process. These tools typically generate standalone code snippets that still require manual integration into the codebase. There is still a lack of integrated solutions that seamlessly automate the entire development cycle, from backlog items to code generation and merge requests. We present the Pipeline for Automated Code Generation from Backlog Items (PACGBI), an LLM-assisted pipeline integrated into GitLab CI. PACGBI reads backlog items in the code repository, automatically generates the corresponding code, and creates merge requests for the generated changes. Our case study demonstrates the potential of PACGBI in automating agile software development processes, allowing parallelization of development and reduction of development costs. PACGBI can be utilized by software developers and enables nontechnical stakeholders and designers by providing a holistic solution for using LLMs in software development. A screencast of this tool is available at https://youtu.be/TI53m-fIoyc, its source code at https://github.com/Masa-99/pacgbi.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2338\u20132341",
        "numpages": "4",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695349",
        "author": "Wang, Che and Zhang, Jiashuo and Gao, Jianbo and Xia, Libin and Guan, Zhi and Chen, Zhong",
        "title": "ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695349",
        "doi": "10.1145/3691620.3695349",
        "abstract": "Smart contracts are susceptible to being exploited by attackers, especially when facing real-world vulnerabilities. To mitigate this risk, developers often rely on third-party audit services to identify potential vulnerabilities before project deployment. Nevertheless, repairing the identified vulnerabilities is still complex and laborintensive, particularly for developers lacking security expertise. Moreover, existing pattern-based repair tools mostly fail to address real-world vulnerabilities due to their lack of high-level semantic understanding. To fill this gap, we propose ContractTinker, a Large Language Models (LLMs)-empowered tool for real-world vulnerability repair. The key insight is our adoption of the Chain-of-Thought approach to break down the entire generation task into subtasks. Additionally, to reduce hallucination, we integrate program static analysis to guide the LLM. We evaluate ContractTinker on 48 high-risk vulnerabilities. The experimental results show that among the patches generated by ContractTinker, 23 (48\\%) are valid patches that fix the vulnerabilities, while 10 (21\\%) require only minor modifications. A video of ContractTinker is available at https://youtu.be/HWFVi-YHcPE.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2350\u20132353",
        "numpages": "4",
        "keywords": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "CoqPilot, a plugin for LLM-based generation of proofs": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695357",
        "author": "Kozyrev, Andrei and Solovev, Gleb and Khramov, Nikita and Podkopaev, Anton",
        "title": "CoqPilot, a plugin for LLM-based generation of proofs",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695357",
        "doi": "10.1145/3691620.3695357",
        "abstract": "We present CoqPilot, a VS Code extension designed to help automate writing of Coq proofs. The plugin collects the parts of proofs marked with the admit tactic in a Coq file, i.e., proof holes, and combines LLMs along with non-machine-learning methods to generate proof candidates for the holes. Then, CoqPilot checks if each proof candidate solves the given subgoal and, if successful, replaces the hole with it. The focus of CoqPilot is twofold. Firstly, we want to allow users to seamlessly combine multiple Coq generation approaches and provide a zero-setup experience for our tool. Secondly, we want to deliver a platform for LLM-based experiments on Coq proof generation. We developed a benchmarking system for Coq generation methods, available in the plugin, and conducted an experiment using it, showcasing the framework's possibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo. Code at: https://github.com/JetBrains-Research/coqpilot",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2382\u20132385",
        "numpages": "4",
        "keywords": [
            "code generation",
            "program synthesis",
            "static analysis",
            "program verification"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "LLM-Based Java Concurrent Program to ArkTS Converter": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695362",
        "author": "Liu, Runlin and Lin, Yuhang and Hu, Yunge and Zhang, Zhe and Gao, Xiang",
        "title": "LLM-Based Java Concurrent Program to ArkTS Converter",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695362",
        "doi": "10.1145/3691620.3695362",
        "abstract": "HarmonyOS NEXT is a distributed operating system developed to support HarmonyOS native apps. To support the new and independent Harmony ecosystem, developers are required to migrate their applications from Android to HarmonyOS. However, HarmonyOS utilizes ArkTS, a superset of TypeScript, as the programming language for application development. Hence, migrating applications to HarmonyOS requires translating programs across different program languages, e.g., Java, which is known to be very challenging, especially for concurrency programs. Java utilizes shared memory to implement concurrency programs, while ArkTS relies on message passing (i.e., Actor model). This paper presents an LLM-based concurrent Java program to ArkTS converter.Our converter utilizes large language models (LLMs) for efficient code translation, integrating ArkTS's SharedArrayBuffer API to create ThreadBridge, a library that replicates Java's shared memory model. Using LLM's Chain-of-Thought mechanism, the translation process is divided into specialized chains: the TS chain, concurrency chain, and synchronization chain, each handling TypeScript syntax, concurrency patterns, and synchronization logic with precision.This study offers solutions to bridge concurrency model differences between Java and ArkTS, reducing manual code rewriting and speeding up adaptation for HarmonyOS NEXT. Experiments show the converter successfully compiles 66\\% of 53 test samples, with 69\\% accuracy for compiled results. Overall, the approach shows promise in converting concurrent Java programs to ArkTS, laying the foundation for future improvements.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2403\u20132406",
        "numpages": "4",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Automated Validation of COBOL to Java Transformation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695365",
        "author": "Kumar, Atul and Saha, Diptikalyan and Yasue, Toshiaki and Ono, Kohichi and Krishnan, Saravanan and Hans, Sandeep and Satoh, Fumiko and Mitchell, Gerald and Kumar, Sachin",
        "title": "Automated Validation of COBOL to Java Transformation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695365",
        "doi": "10.1145/3691620.3695365",
        "abstract": "Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterpriselevel code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code. We propose a framework and a tool to help validate the equivalence of COBOL and translated Java. The results can also help repair the code if there are some issues and provide feedback to the AI model to improve. We have developed a symbolic-execution-based test generation to automatically generate unit tests for the source COBOL programs which also mocks the external resource calls. We generate equivalent JUnit test cases with equivalent mocking as COBOL and run them to check semantic equivalence between original and translated programs. Demo Video: https://youtu.be/aqF_agNP-lU",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2415\u20132418",
        "numpages": "4",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Can Large Language Models Comprehend Code Stylometry?": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695370",
        "author": "Dipongkor, Atish",
        "title": "Can Large Language Models Comprehend Code Stylometry?",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695370",
        "doi": "10.1145/3691620.3695370",
        "abstract": "Code Authorship Attribution (CAA) has several applications such as copyright disputes, plagiarism detection and criminal prosecution. Existing studies mainly focused on CAA by proposing machine learning (ML) and Deep Learning (DL) based techniques. The main limitations of ML-based techniques are (a) manual feature engineering is required to train these models and (b) they are vulnerable to adversarial attack. In this study, we initially fine-tune five Large Language Models (LLMs) for CAA and evaluate their performance. Our results show that LLMs are robust and less vulnerable compared to existing techniques in CAA task.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2429\u20132431",
        "numpages": "3",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "software maintenance and deployment",
            "supply chain"
        ]
    },
    "Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695322",
        "author": "Luo, Yang and Yu, Richard and Zhang, Fajun and Liang, Ling and Xiong, Yongqiang",
        "title": "Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695322",
        "doi": "10.1145/3691620.3695322",
        "abstract": "When using large language models (LLMs) for code translation of complex software, numerous compilation and runtime errors can occur due to insufficient context awareness. To address this issue, this paper presents a code translation method based on call graphs and bridged debuggers: TransGraph. TransGraph first obtains the call graph of the entire code project using the Language Server Protocol, which provides a detailed description of the function call relationships in the program. Through this structured view of the code, LLMs can more effectively handle large-scale and complex codebases, significantly reducing compilation errors. Furthermore, TransGraph, combined with bridged debuggers and dynamic test case generation, significantly reduces runtime errors, overcoming the limitations of insufficient test case coverage in traditional methods. In experiments on six datasets including CodeNet and Avatar, TransGraph outperformed existing code translation methods and LLMs in terms of translation accuracy, with improvements of up to 10.2\\%.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2448\u20132449",
        "numpages": "2",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Cross-lingual Code Clone Detection: When LLMs Fail Short Against Embedding-based Classifier": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695335",
        "author": "Moumoula, Micheline Benedicte and Kabore, Abdoul Kader and Klein, Jacques and Bissyande, Tegawende F.",
        "title": "Cross-lingual Code Clone Detection: When LLMs Fail Short Against Embedding-based Classifier",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695335",
        "doi": "10.1145/3691620.3695335",
        "abstract": "Cross-lingual code clone detection has gained attention in software development due to the use of multiple programming languages. Recent advances in machine learning, particularly Large Language Models (LLMs), have motivated a reexamination of this problem.This paper evaluates the performance of four LLMs and eight prompts for detecting cross-lingual code clones, as well as a pretrained embedding model for classifying clone pairs. Both approaches are tested on the XLCoST and CodeNet datasets.Our findings show that while LLMs achieve high F1 scores (up to 0.98) on straightforward programming examples, they struggle with complex cases and cross-lingual understanding. In contrast, embedding models, which map code fragments from different languages into a common representation space, allow for the training of a basic classifier that outperforms LLMs by approximately 2 and 24 percentage points on the XLCoST and CodeNet datasets, respectively. This suggests that embedding models provide more robust representations, enabling state-of-the-art performance in cross-lingual code clone detection.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2474\u20132475",
        "numpages": "2",
        "keywords": [
            "static analysis",
            "code clone detection"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Who Judges the Judge: An Empirical Study on Online Judge Tests": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598060",
        "author": "Liu, Kaibo and Han, Yudong and Zhang, Jie M. and Chen, Zhenpeng and Sarro, Federica and Harman, Mark and Huang, Gang and Ma, Yun",
        "title": "Who Judges the Judge: An Empirical Study on Online Judge Tests",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598060",
        "doi": "10.1145/3597926.3598060",
        "abstract": "Online Judge platforms play a pivotal role in education, competitive programming, recruitment, career training, and large language model training. They rely on predefined test suites to judge the correctness of submitted solutions. It is therefore important that the solution judgement is reliable and free from potentially misleading false positives (i.e., incorrect solutions that are judged as correct). In this paper, we conduct an empirical study of 939 coding problems with 541,552 solutions, all of which are judged to be correct according to the test suites used by the platform, finding that 43.4\\% of the problems include false positive solutions (3,440 bugs are revealed in total). We also find that test suites are, nevertheless, of high quality according to widely-studied test effectiveness measurements: 88.2\\% of false positives have perfect (100\\%) line coverage, 78.9\\% have perfect branch coverage, and 32.5\\% have a perfect mutation score. Our findings indicate that more work is required to weed out false positive solutions and to further improve test suite effectiveness. We have released the detected false positive solutions and the generated test inputs to facilitate future research.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "334\u2013346",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023",
        "venue": "ISSTA2023"
    },
    "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598067",
        "author": "Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming",
        "title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598067",
        "doi": "10.1145/3597926.3598067",
        "abstract": "Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations.  To address these limitations, we propose TitanFuzz \u2013 the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38\\%/50.84\\% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs.  This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "423\u2013435",
        "numpages": "13",
        "keywords": [
            "program testing",
            "fuzzing",
            "library testing"
        ],
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023",
        "venue": "ISSTA2023"
    },
    "How Effective Are Neural Networks for Fixing Security Vulnerabilities": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598135",
        "author": "Wu, Yi and Jiang, Nan and Pham, Hung Viet and Lutellier, Thibaud and Davis, Jordan and Tan, Lin and Babkin, Petr and Shah, Sameena",
        "title": "How Effective Are Neural Networks for Fixing Security Vulnerabilities",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598135",
        "doi": "10.1145/3597926.3598135",
        "abstract": "Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans, to better evaluate LLMs and APR techniques, and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4\\%), the most number of vulnerabilities. Many of the generated patches are uncompilable patches. (2) Fine-tuning with general APR data improves LLMs\u2019 vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes 8.7 transformed vulnerabilities, outperforming all the other LLMs and APR models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning LLMs with such data, and applying code simplification transformation to facilitate vulnerability repair.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1282\u20131294",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair",
            "benchmark"
        ],
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023",
        "venue": "ISSTA2023"
    },
    "CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00085",
        "author": "Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K. and Sen, Siddhartha",
        "title": "CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00085",
        "doi": "10.1109/ICSE48619.2023.00085",
        "abstract": "Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "919\u2013931",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "keywords": [
            "program testing",
            "fuzzing"
        ]
    },
    "CCTest: Testing and Repairing Code Completion Systems": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00110",
        "author": "Li, Zongjie and Wang, Chaozheng and Liu, Zhibo and Wang, Haoxuan and Chen, Dong and Wang, Shuai and Gao, Cuiyun",
        "title": "CCTest: Testing and Repairing Code Completion Systems",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00110",
        "doi": "10.1109/ICSE48619.2023.00110",
        "abstract": "Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems.In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in black-box settings. CCTest features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTest repairs the code completion outputs by selecting the output that mostly reflects the \"average\" appearance of all output cases, as the final output of the code completion systems. With around 18K test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86\\%) from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40\\% and 67\\% with respect to BLEU score and Levenshtein edit similarity.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1238\u20131250",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "keywords": [
            "code generation",
            "code completion"
        ]
    },
    "Automated Repair of Programs from Large Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00128",
        "author": "Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei",
        "title": "Automated Repair of Programs from Large Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00128",
        "doi": "10.1109/ICSE48619.2023.00128",
        "abstract": "Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1469\u20131481",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "keywords": [
            "code generation",
            "program repair"
        ]
    },
    "Automated Program Repair in the Era of Large Pre-Trained Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00129",
        "author": "Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming",
        "title": "Automated Program Repair in the Era of Large Pre-Trained Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00129",
        "doi": "10.1109/ICSE48619.2023.00129",
        "abstract": "Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1482\u20131494",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "keywords": [
            "code generation",
            "program repair"
        ]
    },
    "Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00194",
        "author": "Kang, Sungmin and Yoon, Juyeon and Yoo, Shin",
        "title": "Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00194",
        "doi": "10.1109/ICSE48619.2023.00194",
        "abstract": "Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28\\% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose LIBRO, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of LIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate failure reproducing test cases for 33\\% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate LIBRO against 31 bug reports submitted after the collection of the LLM training data terminated: LIBRO produces bug reproducing tests for 32\\% of the studied bug reports. Overall, our results show LIBRO has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "2312\u20132323",
        "numpages": "12",
        "keywords": [
            "program testing",
            "bug reproduction"
        ],
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023"
    },
    "COMEX: A Tool for Generating Customized Source Code Representations": {
        "type": "INPROCEEDINGS",
        "key": "10298568",
        "author": "Das, Debeshee and Mathews, Noble Saji and Mathai, Alex and Tamilselvam, Srikanth and Sedamaki, Kranthi and Chimalakonda, Sridhar and Kumar, Atul",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "COMEX: A Tool for Generating Customized Source Code Representations",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "2054-2057",
        "abstract": "Learning effective representations of source code is critical for any Machine Learning for Software Engineering (ML4SE) system. Inspired by natural language processing, large language models (LLMs) like Codex and CodeGen treat code as generic sequences of text and are trained on huge corpora of code data, achieving state of the art performance on several software engineering (SE) tasks. However, valid source code, unlike natural language, follows a strict structure and pattern governed by the underlying grammar of the programming language. Current LLMs do not exploit this property of the source code as they treat code like a sequence of tokens and overlook key structural and semantic properties of code that can be extracted from code-views like the Control Flow Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc. Unfortunately, the process of generating and integrating code-views for every programming language is cumbersome and time consuming. To overcome this barrier, we propose our tool COMEX - a framework that allows researchers and developers to create and combine multiple code-views which can be used by machine learning (ML) models for various SE tasks. Some salient features of our tool are: (i) it works directly on source code (which need not be compilable), (ii) it currently supports Java and C#, (iii) it can analyze both method-level snippets and program-level snippets by using both intra-procedural and inter-procedural analysis, and (iv) it is easily extendable to other languages as it is built on tree-sitter - a widely used incremental parser that supports over 40 languages. We believe this easy-to-use code-view generation and customization tool will give impetus to research in source code representation learning methods and ML4SE. The source code and demonstration of our tool can be found at https://github.com/IBM/tree-sitter-codeviews and https://youtu.be/GER6U87FVbU, respectively.",
        "keywords": [
            "code generation",
            "source code model"
        ],
        "doi": "10.1109/ASE56229.2023.00010",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair": {
        "type": "INPROCEEDINGS",
        "key": "10298532",
        "author": "Huang, Kai and Meng, Xiangxin and Zhang, Jian and Liu, Yang and Wang, Wenjie and Li, Shuhao and Zhang, Yuqing",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1162-1174",
        "abstract": "The advent of large language models (LLMs) has opened up new opportunities for automated program repair (APR). In particular, some recent studies have explored how to leverage large language models of code (LLMCs) for program repair tasks and show promising results. However, most of them adopt the zero/few-shot learning paradigm for APR, which directly use LLMCs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMCs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMCs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in this work, we conduct a comprehensive study on the program repair capability of LLMCs in the fine-tuning paradigm. We select 5 popular LLMCs with representative pre-training architectures, including CodeBERT, GraphCode-BERT, PLBART, CodeT5, and UniX coder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, $\\mathrm{C}/\\mathrm{C}++$, and JavaScript). Notably, we take both single-hunk and multi-hunk bugs/vulnerabilities into account. We then fine-tune them on widely-used datasets and compare them with existing state-of-the-art APR tools. We also investigate the impact of different design choices, which include code abstractions, code representations, and model evaluation metrics. Our experimental results show that LLMCs in the fine-tuning paradigm can significantly outperform previous state-of-the-art APR tools. Through in-depth analysis, we provide insights into choosing appropriate strategies to guide LLMCs for better performance. Lastly, we reveal several limitations of LLMCs for APR and make suggestions for future research on LLMC-based APR.",
        "keywords": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "doi": "10.1109/ASE56229.2023.00181",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "SMT Solver Validation Empowered by Large Pre-Trained Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10298442",
        "author": "Sun, Maolin and Yang, Yibiao and Wang, Yang and Wen, Ming and Jia, Haoxiang and Zhou, Yuming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "SMT Solver Validation Empowered by Large Pre-Trained Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1288-1300",
        "abstract": "SMT solvers are utilized to check the satisfiability of logic formulas and have been applied in various crucial domains, including software verification, test case generation, and program synthesis. However, bugs hidden in SMT solvers can lead to severe consequences, causing erroneous results in these domains. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Despite several testing approaches proposed for SMT solvers, generating effective test formulas to comprehensively test SMT solvers remains a challenge. To address this challenge, in this study, we propose to port large language models (LLMs) to generate SMT formulas for fuzzing solvers. Specifically, the study presents a novel retrain-finetune pipeline to unleash the potential of language models to generate effective SMT formulas and improve their generation performance through data augmentation. We implemented our approach as a practical fuzzing tool, named LasT,and then extensively tested the state-of-the-art SMT solvers, namely Z3, cvc5, and Bitwuzla. To date, Last has successfully uncovered 65 genuine bugs for the solvers, of which 45 have been fixed by the developers.",
        "keywords": [
            "program testing",
            "fuzzing"
        ],
        "doi": "10.1109/ASE56229.2023.00180",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "Improving Code Extraction from Coding Screencasts Using a Code-Aware Encoder-Decoder Model": {
        "type": "INPROCEEDINGS",
        "key": "10298433",
        "author": "Malkadi, Abdulkarim and Tayeb, Ahmad and Haiduc, Sonia",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Improving Code Extraction from Coding Screencasts Using a Code-Aware Encoder-Decoder Model",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1492-1504",
        "abstract": "Accurate automatic code extraction from tutorial videos is crucial for software developers seeking to reuse the code contained in these videos. Current methods using optical character recognition (OCR) often yield inaccurate results due to code complexity and variations in screencast formats. To address this issue, we introduce CodeT5-OCRfix, an approach that leverages the pre-trained code-aware large language model CodeT5 to enhance code extraction accuracy by post-processing OCRed code. We first collect a large and diverse dataset of source code screenshots captured from more than 10K Java projects from GitHub. We then apply the most widely used OCR engine for the task of code extraction from videos, Tesseract, on these screenshots and collect the OCRed code along with the ground truth code extracted from the Java files. We built a training dataset of more than 585K pairs of OCRed and ground truth code pairs, which we then used to fine-tune CodeT5, obtaining our model CodeT5-OCRfix. An empirical evaluation on both screenshots and screencast frames shows that CodeT5-OCRfix outperforms baseline code extraction models and is also more time-efficient. Our approach therefore improves the state-of-the-art in code extraction techniques from screencasts and images.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model training",
            "source code model"
        ],
        "doi": "10.1109/ASE56229.2023.00184",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "Better Patching Using LLM Prompting, via Self-Consistency": {
        "type": "INPROCEEDINGS",
        "key": "10298561",
        "author": "Ahmed, Toufique and Devanbu, Premkumar",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Better Patching Using LLM Prompting, via Self-Consistency",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1742-1746",
        "abstract": "Large Language models (LLMs) can be induced to solve non-trivial problems with \u201cfew-shot\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \u201cchain of thought\u201d ($\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \u201cexplained\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\mathcal{S}-C$ (or even $\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.",
        "keywords": [
            "code generation",
            "program repair",
            "prompting strategy"
        ],
        "doi": "10.1109/ASE56229.2023.00065",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT": {
        "type": "INPROCEEDINGS",
        "key": "10298505",
        "author": "Yan, Dapeng and Gao, Zhipeng and Liu, Zhiming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1887-1898",
        "abstract": "Code generation aims to generate source code implementing human requirements illustrated with natural language specifications. With the rapid development of intelligent software engineering, automated code generation has become a hot research topic in both artificial intelligence and software engineering, and researchers have made significant achievements on code generation. More recently, large language models (LLMs) have demonstrated outstanding performance on code generation tasks, such as ChatGPT released by OpenAI presents the fantastic potential on automated code generation. However, the existing studies are limited to exploring LLMs' ability for generating code snippets to solve simple programming problems, the task of competition-level code generation has never been investigated. The specifications of the programming competition are always complicated and require the specific input/output format as well as the high-level algorithmic reasoning ability. In this study, we conduct the first large empirical study to investigate the zero-shot learning ability of ChatGPT for solving competition programming problems. Specifically, we warm up the design of prompts by using the Human-Eval dataset. Then, we apply the well-designed prompt to the competition-level code generation dataset, namely APPS, to further explore the effectiveness of using ChatGPT for solving competition problems. We collect ChatGPT's outputs on 5,000 code competition problems, the evaluation results show that it can successfully pass 25.4% test cases. By further feeding extra information (e.g, test failed information) to ChatGPT, we observe that ChatGPT has the potential to fix partial pass into a fully pass program. Moreover, we investigate the solutions generated by LLMs and the existing solutions, we find that it prefers to directly copy the code instead of re-write when facing more difficult problems. Finally, we evaluate the code quality generated by ChatGPT in terms of \u201ccode cleanness\u201d, we observe that the generated codes are with small functions and file sizes, which are in line with the standard of clean code.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "doi": "10.1109/ASE56229.2023.00096",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting": {
        "type": "INPROCEEDINGS",
        "key": "10298538",
        "author": "Li, Tsz-On and Zong, Wenxi and Wang, Yibo and Tian, Haoye and Wang, Ying and Cheung, Shing-Chi and Kramer, Jeff",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "14-26",
        "abstract": "Automated detection of software failures is an important but challenging software engineering task. It involves finding in a vast search space the failure-inducing test cases that contain an input triggering the software fault and an oracle asserting the incorrect execution. We are motivated to study how far this outstanding challenge can be solved by recent advances in large language models (LLMs) such as ChatGPT. However, our study reveals that ChatGPT has a relatively low success rate (28.8%) in finding correct failure-inducing test cases for buggy programs. A possible conjecture is that finding failure-inducing test cases requires analyzing the subtle differences (nuances) between the tokens of a program's correct version and those for its buggy version. When these two versions have similar sets of tokens and attentions, ChatGPT is weak in distinguishing their differences. We find that ChatGPT can successfully generate failure-inducing test cases when it is guided to focus on the nuances. Our solution is inspired by an interesting observation that ChatGPT could infer the intended functionality of buggy code if it is similar to the correct version. Driven by the inspiration, we develop a novel technique, called Differential Prompting, to effectively find failure-inducing test cases with the help of the compilable code synthesized by the inferred intention. Prompts are constructed based on the nuances between the given version and the synthesized code. We evaluate Differential Prompting on Quixbugs (a popular benchmark of buggy programs) and recent programs published at Codeforces (a popular programming contest portal, which is also an official benchmark of ChatGPT). We compare Differential Prompting with two baselines constructed using conventional ChatGPT prompting and Pynguin (the state-of-the-art unit test generation tool for Python programs). Our evaluation results show that for programs of Quixbugs, Differential Prompting can achieve a success rate of 75.0% in finding failure-inducing test cases, outperforming the best baseline by 2.6X. For programs of Codeforces, Differential Prompting's success rate is 66.7%, outperforming the best baseline by 4.0X.",
        "keywords": [
            "program testing",
            "differential testing"
        ],
        "doi": "10.1109/ASE56229.2023.00089",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "SCPatcher: Mining Crowd Security Discussions to Enrich Secure Coding Practices": {
        "type": "INPROCEEDINGS",
        "key": "10298463",
        "author": "Jiang, Ziyou and Shi, Lin and Yang, Guowei and Wang, Qing",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "SCPatcher: Mining Crowd Security Discussions to Enrich Secure Coding Practices",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "358-370",
        "abstract": "Secure coding practices (SCPs) have been proposed to guide software developers to write code securely to prevent potential security vulnerabilities. Yet, they are typically one-sentence principles without detailed specifications, e.g., \u201cProperly free allocated memory upon the completion of functions and at all exit points.\u201d, which makes them difficult to follow in practice, especially for software developers who are not yet experienced in secure programming. To address this problem, this paper proposes SCPatcher, an automated approach to enrich secure coding practices by mining crowd security discussions on online knowledge-sharing platforms, such as Stack Overflow. In particular, for each security post, SCPatcher first extracts the area of coding examples and coding explanations with a fix-prompt tuned Large Language Model (LLM) via Prompt Learning. Then, it hierarchically slices the lengthy code into coding examples and summarizes the coding explanations with the areas. Finally, SCPatcher matches the CWE and Public SCP, integrating them with extracted coding examples and explanations to form the SCP specifications, which are the wild SCPs with details, proposed by the developers. To evaluate the performance of SCPatcher, we conduct experiments on 3,907 security posts from Stack Overflow. The experimental results show that SCPatcher outperforms all baselines in extracting the coding examples with 2.73 % MLine on average, as well as coding explanations with 3.97 % F1 on average. Moreover, we apply SCPatcher on 447 new security posts to further evaluate its practicality, and the extracted SCP specifications enrich the public SCPs with 3,074 lines of code and 1,967 sentences.",
        "keywords": [
            "software maintenance and deployment",
            "code review"
        ],
        "doi": "10.1109/ASE56229.2023.00040",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "The Plastic Surgery Hypothesis in the Era of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10298499",
        "author": "Xia, Chunqiu Steven and Ding, Yifeng and Zhang, Lingming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "The Plastic Surgery Hypothesis in the Era of Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "522-534",
        "abstract": "Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names. The plastic surgery hypothesis is a well-known insight for APR, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional APR tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based approaches to exploit such existing code ingredients. However, as recent APR research starts focusing on LLM-based approaches, the plastic surgery hypothesis has been largely ignored. In this paper, we ask the following question: How useful is the plastic surgery hypothesis in the era of LLMs? Interestingly, LLM-based APR presents a unique opportunity to fully automate the plastic surgery hypothesis via fine-tuning (training on the buggy project) and prompting (directly providing valuable code ingredients as hints to the LLM). To this end, we propose FitRepair, which combines the direct usage of LLMs with two domain-specific fine-tuning strategies and one prompting strategy (via information retrieval and static analysis) for more powerful APR. While traditional APR techniques require intensive manual efforts in both generating patches based on the plastic surgery hypothesis and guaranteeing patch validity, our approach is fully automated and general. Moreover, while it is very challenging to manually design heuristics/patterns for effectively leveraging the hypothesis, due to the power of LLMs in code vectorization/understanding, even partial/imprecise project-specific information can still guide LLMs in generating correct patches! Our experiments on the widely studied Defects4j 1.2 and 2.0 datasets show that FitRepair fixes 89 and 44 bugs (substantially outperforming baseline techniques by 15 and 8), respectively, demonstrating a promising future of the plastic surgery hypothesis in the era of LLMs.",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "doi": "10.1109/ASE56229.2023.00047",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?": {
        "type": "INPROCEEDINGS",
        "key": "10298329",
        "author": "Gao, Shuzheng and Wen, Xin-Cheng and Gao, Cuiyun and Wang, Wenxuan and Zhang, Hongyu and Lyu, Michael R.",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "761-773",
        "abstract": "Pre-trained models of source code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning (ICL). ICL employs task instructions and a few examples as demonstrations, and then inputs the demonstrations to the language models for making predictions. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of ICL heavily relies on the quality of demonstrations, e.g., the selected examples. It is important to systematically investigate how to construct a good demonstration for code-related tasks. In this paper, we empirically explore the impact of three key factors on the performance of ICL in code intelligence tasks: the selection, order, and number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including code summarization, bug fixing, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of ICL in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We also show that a carefully-designed demonstration based on our findings can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90%, 175.96%, and 50.81% on code summarization, bug fixing, and program synthesis, respectively.",
        "keywords": [
            "general coding task"
        ],
        "doi": "10.1109/ASE56229.2023.00109",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining": {
        "type": "INPROCEEDINGS",
        "key": "10298349",
        "author": "Ren, Xiaoxue and Ye, Xinyuan and Zhao, Dehai and Xing, Zhenchang and Yang, Xiaohu",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "976-987",
        "abstract": "Large Language Models (LLMs) have shown promising results in automatic code generation by improving coding efficiency to a certain extent. However, generating high-quality and reliable code remains a formidable task because of LLMs' lack of good programming practice, especially in exception handling. In this paper, we first conduct an empirical study and summarize three crucial challenges of LLMs in exception handling, i.e., incomplete exception handling, incorrect exception handling and abuse of try-catch. We then try prompts with different granularities to address such challenges, finding fine-grained knowledge-driven prompts works best. Based on our empirical study, we propose a novel Knowledge-driven Prompt Chaining-based code generation approach, name KPC, which decomposes code generation into an AI chain with iterative check-rewrite steps and chains fine-grained knowledge-driven prompts to assist LLMs in considering exception-handling specifications. We evaluate our KPC-based approach with 3,079 code generation tasks extracted from the Java official API documentation. Extensive experimental results demonstrate that the KPC-based approach has considerable potential to ameliorate the quality of code generated by LLMs. It achieves this through proficiently managing exceptions and obtaining remarkable enhancements of 109.86% and 578.57% with static evaluation methods, as well as a reduction of 18 runtime bugs in the sampled dataset with dynamic validation.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "doi": "10.1109/ASE56229.2023.00143",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "Generative Type Inference for Python": {
        "type": "INPROCEEDINGS",
        "key": "10298512",
        "author": "Peng, Yun and Wang, Chaozheng and Wang, Wenxuan and Gao, Cuiyun and Lyu, Michael R.",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Generative Type Inference for Python",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "988-999",
        "abstract": "Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub. However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs. Existing type inference approaches can be generally grouped into three categories, i.e., rule-based, supervised, and cloze- style approaches. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems caused by dynamic features and external calls. Supervised type inference approaches, while feature-agnostic and able to mitigate the low coverage problem, require large, high- quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem by leveraging the general knowledge in powerful pre-trained code models. However, their performance is limited since they ignore the domain knowledge from static typing rules which reflect the inference logic. What is more, their predictions are not interpretable, hindering developers' understanding and verification of the results. This paper introduces Typegen, a few-shot generative type inference approach that incorporates static domain knowledge from static analysis. Typegen creates chain-of-thought (COT) prompts by translating the type inference steps of static analysis into prompts based on the type dependency graphs (TDGs), enabling language models to learn from how static analysis infers types. By combining COT prompts with code slices and type hints, TypegEnconstructs example prompts from human annotations. Typeg Enonly requires very few annotated examples to teach language models to generate similar COT prompts via in-context learning. Moreover, Typeg Enenhances the interpretability of results through the use of the input- explanation-output strategy, which generates both explanations and type predictions in COT prompts. Experiments show that Typegen outperforms the best baseline Type4Py by 10.0% for argument type prediction and 22.5 % in return value type prediction in terms of top-l Exact Match by using only five examples. Furthermore, Typeg Enachieves substantial improvements of 27 % to 84 % compared to the zero-shot performance of large language models with parameter sizes ranging from 1.3B to 175B in terms of top-I Exact Match.",
        "keywords": [
            "static analysis",
            "type inference"
        ],
        "doi": "10.1109/ASE56229.2023.00031",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "Lost at C: a user study on the security implications of large language model code assistants": {
        "type": "inproceedings",
        "key": "10.5555/3620237.3620361",
        "author": "Sandoval, Gustavo and Pearce, Hammond and Nys, Teo and Karri, Ramesh and Garg, Siddharth and Dolan-Gavitt, Brendan",
        "title": "Lost at C: a user study on the security implications of large language model code assistants",
        "year": "2023",
        "isbn": "978-1-939133-37-3",
        "publisher": "USENIX Association",
        "address": "USA",
        "abstract": "Large Language Models (LLMs) such as OpenAI Codex are increasingly being used as AI-based coding assistants. Understanding the impact of these tools on developers' code is paramount, especially as recent work showed that LLMs may suggest cybersecurity vulnerabilities. We conduct a security-driven user study (N=58) to assess code written by student programmers when assisted by LLMs. Given the potential severity of low-level bugs as well as their relative frequency in real-world projects, we tasked participants with implementing a singly-linked 'shopping list' structure in C. Our results indicate that the security impact in this setting (low-level C with pointer and array manipulations) is small: AI-assisted users produce critical security bugs at a rate no greater than 10\\% more than the control, indicating the use of LLMs does not introduce new security risks.",
        "booktitle": "Proceedings of the 32nd USENIX Conference on Security Symposium",
        "articleno": "124",
        "numpages": "18",
        "location": "Anaheim, CA, USA",
        "series": "SEC '23",
        "venue": "USENIXSec2023",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ]
    },
    "Examining Zero-Shot Vulnerability Repair with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10179420",
        "author": "Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan",
        "booktitle": "2023 IEEE Symposium on Security and Privacy (SP)",
        "title": "Examining Zero-Shot Vulnerability Repair with Large Language Models",
        "year": "2023",
        "volume": "",
        "ISSN": "",
        "pages": "2339-2356",
        "abstract": "Human developers can produce code with cybersecurity bugs. Can emerging \u2018smart\u2019 code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI\u2019s Codex and AI21\u2019s Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information\u2014 both semantically and syntactically\u2014with natural languages. We perform a large scale study of five commercially available, black-box, \"off-the-shelf\" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model\u2019s performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.",
        "keywords": [
            "static analysis",
            "bug detection",
            "empirical study"
        ],
        "doi": "10.1109/SP46215.2023.10179420",
        "url": "https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.10179420",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "month": "May",
        "venue": "S&P2023"
    },
    "CodeT5+: Open Code Large Language Models for Code Understanding and Generation": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-codet5",
        "author": "Wang, Yue and Le, Hung and Gotmare, Akhilesh and Bui, Nghi and Li, Junnan and Hoi, Steven",
        "booktitle": "EMNLP2023",
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks, lacking the flexibility to operate in the optimal architecture for a specific task. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some tasks and hence result in substantial performance degrade. To address these limitations, we propose \u201cCodeT5+\u201d, a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives, which cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) performance on various code-related tasks, and our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model.",
        "keywords": [
            "general coding task",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.68",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2023-personalized",
        "author": "Chen, Hailin and Saha, Amrita and Hoi, Steven and Joty, Shafiq",
        "booktitle": "EMNLP2023",
        "title": "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher\u2019s prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K personalised examples that incur a data-collection cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to achieve 45.8% pass@1 on HumanEval.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.417",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Benchmarking and Improving Text-to-SQL Generation under Ambiguity": {
        "type": "INPROCEEDINGS",
        "key": "bhaskar-etal-2023-benchmarking",
        "author": "Bhaskar, Adithya and Tomar, Tushar and Sathe, Ashutosh and Sarawagi, Sunita",
        "booktitle": "EMNLP2023",
        "title": "Benchmarking and Improving Text-to-SQL Generation under Ambiguity",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or structural ambiguity. When faced with ambiguity, an ideal top-k decoder should generate all valid interpretations for possible disambiguation by the user. We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal. The primary reason is that the prevalent beam search algorithm and its variants, treat SQL queries as a string and produce unhelpful token-level diversity in the top-k. We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic space using a blend of plan-based template generation and constrained infilling. Counterfactually generated plans diversify templates while in-filling with a beam-search that branches solely on schema names provides value diversity. LogicalBeam is up to 2.5 times more effective than state-of-the-art models at generating all candidate SQLs in the top-k ranked outputs. It also enhances the top-5 Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA.",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.436",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Symbolic Planning and Code Generation for Grounded Dialogue": {
        "type": "INPROCEEDINGS",
        "key": "chiu-etal-2023-symbolic-planning",
        "author": "Chiu, Justin and Zhao, Wenting and Chen, Derek and Vaduguru, Saujas and Rush, Alexander and Fried, Daniel",
        "booktitle": "EMNLP2023",
        "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) excel at processing and generating text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system, consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code\u2019s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system\u2019s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.",
        "keywords": [
            "code generation",
            "program synthesis",
            "agent design",
            "planning"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.460",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Generating Data for Symbolic Language with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ye-etal-2023-generating",
        "author": "Ye, Jiacheng and Li, Chengzu and Kong, Lingpeng and Yu, Tao",
        "booktitle": "EMNLP2023",
        "title": "Generating Data for Symbolic Language with Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks, and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort. SymGen takes a step toward data generation for annotation-expensive complex tasks, and we release the code at URL.",
        "keywords": [
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.523",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Let\u2019s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs": {
        "type": "INPROCEEDINGS",
        "key": "aggarwal-etal-2023-lets",
        "author": "Aggarwal, Pranjal and Madaan, Aman and Yang, Yiming and Mausam",
        "booktitle": "EMNLP2023",
        "title": "Let\u2019s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%",
        "keywords": [
            "prompt strategy",
            "sampling and ranking"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.761",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Question Answering as Programming for Solving Time-Sensitive Questions": {
        "type": "INPROCEEDINGS",
        "key": "zhu-etal-2023-question",
        "author": "Zhu, Xinyu and Yang, Cheng and Chen, Bei and Li, Siheng and Lou, Jian-Guang and Yang, Yujiu",
        "booktitle": "EMNLP2023",
        "title": "Question Answering as Programming for Solving Time-Sensitive Questions",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs\u2019 inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the Question Answering task as Programming (QAaP). Concretely, by leveraging modern LLMs\u2019 superior capability in understanding both natural language and programming language, we endeavor to harness LLMs to represent diversely expressed text as well-structured code and select the best matching answer from multiple candidates through programming. We evaluate our QAaP framework on several time-sensitive question answering datasets and achieve decent improvement, up to 14.5% over strong baselines.",
        "keywords": [
            "prompting strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.787",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL": {
        "type": "INPROCEEDINGS",
        "key": "kothyari-etal-2023-crush4sql",
        "author": "Kothyari, Mayank and Dhingra, Dhruva and Sarawagi, Sunita and Chakrabarti, Soumen",
        "booktitle": "EMNLP2023",
        "title": "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Existing Text-to-SQL generators require the entire schema to be encoded with the user text. This is expensive or impractical for large databases with tens of thousands of columns. Standard dense retrieval techniques are inadequate for schema subsetting of a large structured database, where the correct semantics of retrieval demands that we rank sets of schema elements rather than individual documents. In response, we propose a two-stage process for effective coverage during retrieval. First, we use an LLM to hallucinate a minimal DB schema that it deems adequate to answer the query. We use the hallucinated schema to retrieve a subset of the actual schema, by composing the results from multiple dense retrievals. Remarkably, hallucination \u2014 generally considered a nuisance \u2014 turns out to be actually useful as a bridging mechanism. Since no existing benchmarks exist for schema subsetting on large databases, we introduce two benchmarks: (1) A semi-synthetic dataset of 4502 schema elements, by taking a union of schema on the well-known SPIDER dataset, and (2) A real-life benchmark called SocialDB sourced from an actual large data warehouse comprising of 17844 schema elements. We show that our method leads to significantly higher recall than SOTA retrieval-based augmentation methods.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.868",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "API-Assisted Code Generation for Question Answering on Varied Table Structures": {
        "type": "INPROCEEDINGS",
        "key": "cao-etal-2023-api",
        "author": "Cao, Yihan and Chen, Shuyi and Liu, Ryan and Wang, Zhiruo and Fried, Daniel",
        "booktitle": "EMNLP2023",
        "title": "API-Assisted Code Generation for Question Answering on Varied Table Structures",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures \u2014 relational, multi-table, and hierarchical matrix shapes \u2014 and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.897",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Prompting with Pseudo-Code Instructions": {
        "type": "INPROCEEDINGS",
        "key": "mishra-etal-2023-prompting",
        "author": "Mishra, Mayank and Kumar, Prince and Bhat, Riyaz and Murthy, Rudra and Contractor, Danish and Tamilselvam, Srikanth",
        "booktitle": "EMNLP2023",
        "title": "Prompting with Pseudo-Code Instructions",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models (LLM). Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, like pseudo-code. In this paper, we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models. We manually create a dataset of pseudo-code prompts for 132 different tasks spanning classification, QA, and generative language tasks, sourced from the Super-NaturalInstructions dataset. Using these prompts along with their counterparts in natural language, we study their performance on two LLM families - BLOOM, CodeGen. Our experiments show that using pseudo-code instructions leads to better results, with an average increase (absolute) of 7-16 points in F1 scores for classification tasks and an improvement (relative) of 12-38% in aggregate ROUGE-L scores across all tasks. We include detailed ablation studies which indicate that code comments, docstrings, and the structural clues encoded in pseudo-code all contribute towards the improvement in performance. To the best of our knowledge, our work is the first to demonstrate how pseudo-code prompts can be helpful in improving the performance of pre-trained LMs.",
        "keywords": [
            "prompting strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.939",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Exploring Distributional Shifts in Large Language Models for Code Analysis": {
        "type": "INPROCEEDINGS",
        "key": "arakelyan-etal-2023-exploring",
        "author": "Arakelyan, Shushan and Das, Rocktim and Mao, Yi and Ren, Xiang",
        "booktitle": "EMNLP2023",
        "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. We establish that samples from each new domain present all the models with a significant challenge of distribution shift. We study how established methods adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. Moreover, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain.",
        "keywords": [
            "general coding task",
            "code model training",
            "source code model",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.1013",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "MiniChain: A Small Library for Coding with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "rush-2023-minichain",
        "author": "Rush, Alexander",
        "booktitle": "EMNLP2023",
        "title": "MiniChain: A Small Library for Coding with Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Programming augmented by large language models (LLMs) opens up many new application areas, but also requires care. LLMs are accurate enough, on average, to replace core functionality, yet make basic mistakes that demonstrate a lack of robustness. An ecosystem of prompting tools, from intelligent agents to new programming languages, have emerged with different solutions for patching LLMs with other tools. In this work, we introduce MiniChain, an opinionated tool for LLM augmented programming, with the design goals of ease-of-use of prototyping, transparency through automatic visualization, and a minimalistic approach to advanced features. The MiniChain library provides core primitives for coding LLM calls, separating out prompt templates, and capturing program structure. The library includes demo implementations of the main applications papers in the area, including chat-bots, code generation, retrieval-based question answering, and complex information extraction. The library is open-source and available at https://github.com/srush/MiniChain, with code demos available at https://srush-minichain.hf.space/, and video demo at https://www.youtube.com/watch?v=VszZ1VnO7sk.",
        "keywords": [
            "general coding task"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-demo.27",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Self-Edit: Fault-Aware Code Editor for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-self",
        "author": "Zhang, Kechi and Li, Zhuo and Li, Jia and Li, Ge and Jin, Zhi",
        "booktitle": "ACL2023",
        "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.",
        "keywords": [
            "code generation",
            "program repair",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.45",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Making Language Models Better Reasoners with Step-Aware Verifier": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-making",
        "author": "Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu",
        "booktitle": "ACL2023",
        "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",
        "keywords": [
            "prompt strategy",
            "sampling and ranking"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.291",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Fact-Checking Complex Claims with Program-Guided Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "pan-etal-2023-fact",
        "author": "Pan, Liangming and Wu, Xiaobao and Lu, Xinyuan and Luu, Anh Tuan and Wang, William Yang and Kan, Min-Yen and Nakov, Preslav",
        "booktitle": "ACL2023",
        "title": "Fact-Checking Complex Claims with Program-Guided Reasoning",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/ProgramFC.",
        "keywords": [
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.386",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Large Language Models Meet NL2Code: A Survey": {
        "type": "INPROCEEDINGS",
        "key": "zan-etal-2023-large",
        "author": "Zan, Daoguang and Chen, Bei and Zhang, Fengji and Lu, Dianjie and Wu, Bingchao and Guan, Bei and Yongji, Wang and Lou, Jian-Guang",
        "booktitle": "ACL2023",
        "title": "Large Language Models Meet NL2Code: A Survey",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \u201cLarge Size, Premium Data, Expert Tuning\u201d. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.",
        "keywords": [
            "survey",
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.411",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-codeie",
        "author": "Li, Peng and Sun, Tianxiang and Tang, Qiong and Yan, Hang and Wu, Yuanbin and Huang, Xuanjing and Qiu, Xipeng",
        "booktitle": "ACL2023",
        "title": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.855",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-xsemplr",
        "author": "Zhang, Yusen and Wang, Jun and Wang, Zhiguo and Zhang, Rui",
        "booktitle": "ACL2023",
        "title": "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (full dataset, few-shot, and zero-shot). Our experiments show that encoder-decoder models (mT5) achieve the highest performance compared with other popular models, and multilingual training can further improve the average performance. Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks. We also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training. Our dataset and code are available at https://github.com/psunlpgroup/XSemPLR.",
        "keywords": [
            "code model training",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.887",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616271",
        "author": "Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming",
        "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616271",
        "doi": "10.1145/3611643.3616271",
        "abstract": "During Automated Program Repair (APR), it can be challenging&nbsp;to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models&nbsp;(LLMs) have been shown to be helpful \u201ccopilots\u201d in assisting developers with various coding tasks, and have also been directly&nbsp;applied for patch synthesis. However, most LLMs treat programs as&nbsp;sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This&nbsp;results in plenty of statically invalid generated patches, impeding&nbsp;the practicality of the technique. Therefore, we propose Repilot,&nbsp;a framework to further copilot the AI \u201ccopilots\u201d (i.e., LLMs) by&nbsp;synthesizing more valid patches during the repair process. Our key&nbsp;insight is that many LLMs produce outputs autoregressively (i.e.,&nbsp;token by token), resembling human writing programs, which can&nbsp;be significantly boosted and guided through a Completion Engine.&nbsp;Repilot synergistically synthesizes a candidate patch through the&nbsp;interaction between an LLM and a Completion Engine, which 1)&nbsp;prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the&nbsp;Completion Engine. Our evaluation on a subset of the widely-used&nbsp;Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50&nbsp;bugs, respectively, surpassing the best-performing baseline by 14&nbsp;and 16 bugs fixed. More&nbsp;importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given&nbsp;the same generation budget.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "172\u2013184",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Multilingual Code Co-evolution using Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616350",
        "author": "Zhang, Jiyang and Nie, Pengyu and Li, Junyi Jessy and Gligoric, Milos",
        "title": "Multilingual Code Co-evolution using Large Language Models",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616350",
        "doi": "10.1145/3611643.3616350",
        "abstract": "Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#). Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics. Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "695\u2013707",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program transformation",
            "code model training",
            "source code model"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Baldur: Whole-Proof Generation and Repair with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616243",
        "author": "First, Emily and Rabe, Markus N. and Ringer, Talia and Brun, Yuriy",
        "title": "Baldur: Whole-Proof Generation and Repair with Large Language Models",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616243",
        "doi": "10.1145/3611643.3616243",
        "abstract": "Formally verifying software is a highly desirable but labor-intensive task.  Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time and using that model to search through the space of possible proofs.  This paper introduces a new method to automate formal verification: We use large language models, trained on natural language and code and fine-tuned on proofs, to generate whole proofs at once.  We then demonstrate that a model fine-tuned to repair generated proofs further increasing proving power.  This paper:  (1) Demonstrates that whole-proof generation using transformers is possible and is as effective but more efficient than search-based techniques.  (2) Demonstrates that giving the learned model additional context, such as a prior failed proof attempt and the ensuing error message, results in proof repair that further improves automated proof generation.  (3) Establishes, together with prior work, a new state of the art for fully automated proof synthesis.  We reify our method in a prototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL theorems and their proofs,  empirically showing the effectiveness of whole-proof generation, repair, and added context. We also show that Baldur complements the state-of-the-art tool, Thor, by automatically generating proofs for an additional 8.7\\% of the theorems. Together, Baldur and Thor can prove 65.7\\% of the theorems fully automatically. This paper paves the way for new research into using large language models for automating formal verification.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1229\u20131241",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "program verification"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Grace: Language Models Meet Code Edits": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616253",
        "author": "Gupta, Priyanshu and Khare, Avishree and Bajpai, Yasharth and Chakraborty, Saikat and Gulwani, Sumit and Kanade, Aditya and Radhakrishna, Arjun and Soares, Gustavo and Tiwari, Ashish",
        "title": "Grace: Language Models Meet Code Edits",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616253",
        "doi": "10.1145/3611643.3616253",
        "abstract": "Developers spend a significant amount of time in editing code for a variety of reasons such as bug fixing or adding new features. Designing effective methods to predict code edits has been an active yet challenging area of research due to the diversity of code edits and the difficulty of capturing the developer intent. In this work, we address these challenges by endowing pre-trained large language models (LLMs) with the knowledge of relevant prior associated edits, which we call the Grace (Generation conditioned on Associated Code Edits) method. The generative capability of the LLMs helps address the diversity in code changes and conditioning code generation on prior edits helps capture the latent developer intent. We evaluate two well-known LLMs, codex and CodeT5, in zero-shot and fine-tuning settings respectively. In our experiments with two datasets, Grace boosts the performance of the LLMs significantly, enabling them to generate 29\\% and 54\\% more correctly edited code in top-1 suggestions relative to the current state-of-the-art symbolic and neural approaches, respectively.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1483\u20131495",
        "numpages": "13",
        "keywords": [
            "code generation",
            "code completion",
            "code model training",
            "source code model"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "InferFix: End-to-End Program Repair with LLMs": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3613892",
        "author": "Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey",
        "title": "InferFix: End-to-End Program Repair with LLMs",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3613892",
        "doi": "10.1145/3611643.3613892",
        "abstract": "Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose\u202f: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever \u2013 transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator \u2013 an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6\\% for generating fixes in C# and 76.8\\% in Java. We discuss the deployment of  alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1646\u20131656",
        "numpages": "11",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Assisting Static Analysis with Large Language Models: A ChatGPT Experiment": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3613078",
        "author": "Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun",
        "title": "Assisting Static Analysis with Large Language Models: A ChatGPT Experiment",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3613078",
        "doi": "10.1145/3611643.3613078",
        "abstract": "Recent advances of Large Language Models (LLMs), e.g., ChatGPT, exhibited strong capabilities of comprehending and responding to questions across a variety of domains. Surprisingly, ChatGPT even possesses a strong understanding of program code. In this paper, we investigate where and how LLMs can assist static analysis by asking appropriate questions. In particular, we target a specific bug-finding tool, which produces many false positives from the static analysis. In our evaluation, we find that these false positives can be effectively pruned by asking carefully constructed questions about function-level behaviors or function summaries. Specifically, with a pilot study of 20 false positives, we can successfully prune 8 out of 20 based on GPT-3.5, whereas GPT-4 had a near-perfect result of 16 out of 20, where the four failed ones are not currently considered/supported by our questions, e.g., involving concurrency. Additionally, it also identified one false negative case (a missed bug). We find LLMs a promising tool that can enable a more effective and efficient program analysis.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "2107\u20132111",
        "numpages": "5",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "LLM-Based Code Generation Method for Golang Compiler Testing": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3617850",
        "author": "Gu, Qiuhan",
        "title": "LLM-Based Code Generation Method for Golang Compiler Testing",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3617850",
        "doi": "10.1145/3611643.3617850",
        "abstract": "Modern optimizing compilers are among the most complex software systems humans build. One way to identify subtle compiler bugs is fuzzing. Both the quantity and the quality of testcases are crucial to the performance of fuzzing. Traditional testcase-generation methods, such as Csmith and YARPGen, have been proven successful at discovering compiler bugs. However, such generated testcases have limited coverage and quantity. In this paper, we present a code generation method for compiler testing based on LLM to maximize the quality and quantity of the generated code. In particular, to avoid undefined behavior and syntax errors in generated testcases, we design a filter strategy to clean the source code, preparing a high-quality dataset for the model training. Besides, we present a seed schedule strategy to improve code generation. We apply the method to test the Golang compiler and the result shows that our pipeline outperforms previous methods both qualitatively and quantitatively. It produces testcases with an average coverage of 3.38\\%, in contrast to the testcases generated by GoFuzz, which have an average coverage of 0.44\\%. Moreover, among all the generated testcases, only 2.79\\% exhibited syntax errors, and none displayed undefined behavior.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "2201\u20132203",
        "numpages": "3",
        "keywords": [
            "program testing",
            "fuzzing",
            "compiler testing"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    }
}