{
    "Security of Language Models for Code: A Systematic Literature Review": {
        "type": "article",
        "key": "chen2024security",
        "title": "Security of Language Models for Code: A Systematic Literature Review",
        "author": "Chen, Yuchen and Sun, Weisong and Fang, Chunrong and Chen, Zhenpeng and Ge, Yifei and Han, Tingxu and Zhang, Quanjun and Liu, Yang and Chen, Zhenyu and Xu, Baowen",
        "journal": "arXiv preprint arXiv:2410.15631",
        "year": "2024",
        "volume": "",
        "number": "",
        "abstract": "Language models for code (CodeLMs) have emerged as powerful tools for code-related tasks, outperforming traditional methods and standard machine learning approaches. However, these models are susceptible to security vulnerabilities, drawing increasing research attention from domains such as software engineering, artificial intelligence, and cybersecurity. Despite the growing body of research focused on the security of CodeLMs, a comprehensive survey in this area remains absent. To address this gap, we systematically review 67 relevant papers, organizing them based on attack and defense strategies. Furthermore, we provide an overview of commonly used language models, datasets, and evaluation metrics, and highlight open-source tools and promising directions for future research in securing CodeLMs.",
        "keywords": [
            "code model security",
            "survey"
        ]
    },
    "LLMs: Understanding Code Syntax and Semantics for Code Analysis": {
        "type": "article",
        "key": "ma2023lms",
        "title": "LLMs: Understanding Code Syntax and Semantics for Code Analysis",
        "author": "Ma, Wei and Liu, Shangqing and Lin, Zhihao and Wang, Wenhan and Hu, Qiang and Liu, Ye and Zhang, Cen and Nie, Liming and Li, Li and Liu, Yang",
        "journal": "arXiv preprint arXiv:2305.12138",
        "year": "2023",
        "abstract": "Large language models~(LLMs) demonstrate significant potential to revolutionize software engineering (SE) by exhibiting outstanding performance in SE tasks such as code and document generation. However, the high reliability and risk control requirements in software engineering raise concerns about the lack of interpretability of LLMs. To address this concern, we conducted a study to evaluate the capabilities of LLMs and their limitations for code analysis in SE. We break down the abilities needed for artificial intelligence~(AI) models to address SE tasks related to code analysis into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on the ability of LLMs to comprehend code syntax and semantic structures, which include abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We employed four state-of-the-art foundational models, GPT4, GPT3.5, StarCoder and CodeLlama-13b-instruct. We assessed the performance of LLMs on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while LLMs have a talent for understanding code syntax, they struggle with comprehending code semantics, particularly dynamic semantics. We conclude that LLMs possess capabilities similar to an Abstract Syntax Tree (AST) parser, demonstrating initial competencies in static code analysis. Furthermore, our study highlights that LLMs are susceptible to hallucinations when interpreting code semantic structures and fabricating nonexistent facts. These results indicate the need to explore methods to verify the correctness of LLM output to ensure its dependability in SE. More importantly, our study provides an initial answer to why the codes generated by LLM are usually syntax-correct but vulnerable.",
        "keywords": [
            "code model",
            "source code model",
            "empirical study"
        ]
    },
    "Codemind: A framework to challenge large language models for code reasoning": {
        "type": "article",
        "key": "liu2024codemind",
        "title": "Codemind: A framework to challenge large language models for code reasoning",
        "author": "Liu, Changshu and Zhang, Shizhuo Dylan and Ibrahimzada, Ali Reza and Jabbarvand, Reyhaneh",
        "journal": "arXiv preprint arXiv:2402.09664",
        "year": "2024",
        "abstract": "Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly follow control flow constructs and, in general, explain how inputs evolve to output, specifically for simple programs and the ones they can correctly synthesize. However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls. Furthermore, we observe that, while correlated, specification reasoning (essential for code synthesis) does not imply execution reasoning (essential for broader programming tasks such as testing and debugging): ranking LLMs based on test passing can be different compared to code reasoning.",
        "keywords": [
            "general coding task",
            "empirical study"
        ]
    },
    "Which Syntactic Capabilities Are Statistically Learned by Masked Language Models for Code?": {
        "type": "inproceedings",
        "key": "velasco2024syntactic",
        "title": "Which Syntactic Capabilities Are Statistically Learned by Masked Language Models for Code?",
        "author": "Velasco, Alejandro and Palacio, David N and Rodriguez-Cardenas, Daniel and Poshyvanyk, Denys",
        "booktitle": "Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results",
        "pages": "72--76",
        "year": "2024",
        "abstract": "This paper discusses the limitations of evaluating Masked Language Models (MLMs) in code completion tasks. We highlight that relying on accuracy-based measurements may lead to an overestimation of models' capabilities by neglecting the syntax rules of programming languages. To address these issues, we introduce a technique called SyntaxEval in which Syntactic Capabilities are used to enhance the evaluation of MLMs. SyntaxEval automates the process of masking elements in the model input based on their Abstract Syntax Trees (ASTs). We conducted a case study on two popular MLMs using data from GitHub repositories. Our results showed negative causal effects between the node types and MLMs' accuracy. We conclude that MLMs under study fail to predict some syntactic capabilities.",
        "venue": "ICSE2024",
        "keywords": [
            "static analysis",
            "fundamental analysis",
            "empirical study"
        ]
    },
    "Grounded Copilot: How Programmers Interact with Code-Generating Models": {
        "type": "article",
        "key": "10.1145/3586030",
        "author": "Barke, Shraddha and James, Michael B. and Polikarpova, Nadia",
        "title": "Grounded Copilot: How Programmers Interact with Code-Generating Models",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "7",
        "number": "OOPSLA1",
        "url": "https://doi.org/10.1145/3586030",
        "doi": "10.1145/3586030",
        "abstract": "Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants\u2014with a range of prior experience using the assistant\u2014as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "apr",
        "articleno": "78",
        "numpages": "27",
        "keywords": [
            "code generation",
            "code completion",
            "empirical study"
        ],
        "venue": "OOPLSA2023"
    },
    "SemCoder: Training Code Language Models with Comprehensive Semantics": {
        "type": "article",
        "key": "ding2024semcoder",
        "title": "SemCoder: Training Code Language Models with Comprehensive Semantics",
        "author": "Ding, Yangruibo and Peng, Jinjun and Min, Marcus J and Kaiser, Gail and Yang, Junfeng and Ray, Baishakhi",
        "journal": "arXiv preprint arXiv:2406.01006",
        "year": "2024",
        "abstract": "Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states. This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for semantic understanding for complex tasks like debugging and program repair. We introduce a novel strategy, monologue reasoning, to train Code LLMs to reason comprehensive semantics, encompassing high-level functional descriptions, local execution effects of individual statements, and overall input/output behavior, thereby linking static code text with dynamic execution states. We begin by collecting PyX, a clean Python corpus of fully executable code samples with functional descriptions and test cases. We propose training Code LLMs not only to write code but also to understand code semantics by reasoning about key properties, constraints, and execution behaviors using natural language, mimicking human verbal debugging, i.e., rubber-duck debugging. This approach led to the development of SemCoder, a Code LLM with only 6.7B parameters, which shows competitive performance with GPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder achieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I (GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5-turbo: 59.0%). We also study the effectiveness of SemCoder's monologue-style execution reasoning compared to concrete scratchpad reasoning, showing that our approach integrates semantics from multiple dimensions more smoothly. Finally, we demonstrate the potential of applying learned semantics to improve Code LLMs' debugging and self-refining capabilities.",
        "venue": "NeurIPS2024",
        "keywords": [
            "general coding task",
            "code model",
            "source code model"
        ]
    },
    "CodeFort: Robust Training for Code Generation Models": {
        "type": "article",
        "key": "zhang2024codefort",
        "title": "CodeFort: Robust Training for Code Generation Models",
        "author": "Zhang, Yuhao and Wang, Shiqi and Qian, Haifeng and Wang, Zijian and Shang, Mingyue and Liu, Linbo and Gouda, Sanjay Krishna and Ray, Baishakhi and Ramanathan, Murali Krishna and Ma, Xiaofei and others",
        "journal": "arXiv preprint arXiv:2405.01567",
        "year": "2024",
        "abstract": "Code generation models are not robust to small perturbations, which often lead to inconsistent and incorrect generations and significantly degrade the performance of these models. Improving the robustness of code generation models is crucial to better user experience when these models are deployed in real-world applications. However, existing efforts have not addressed this issue for code generation models. To fill this gap, we propose CodeFort, a framework to improve the robustness of code generation models, generalizing a large variety of code perturbations to enrich the training data and enabling various robust training strategies, mixing data augmentation, batch augmentation, adversarial logits pairing, and contrastive learning, all carefully designed to support high-throughput training. Extensive evaluations show that we improve the average robust pass rates of baseline CodeGen models from 14.79 to 21.74. Notably, the improvement in robustness against code-syntax perturbations is evidenced by a significant decrease in pass rate drop from 95.04% to 53.35%",
        "venue": "EMNLP2024",
        "keywords": [
            "code generation",
            "code model",
            "attack",
            "defense"
        ]
    },
    "Constrained Decoding for Secure Code Generation": {
        "type": "article",
        "key": "fu2024constrained",
        "title": "Constrained Decoding for Secure Code Generation",
        "author": "Fu, Yanjun and Baker, Ethan and Ding, Yu and Chen, Yizheng",
        "journal": "arXiv preprint arXiv:2405.00218",
        "year": "2024",
        "abstract": "Code Large Language Models (Code LLMs) have been increasingly used by developers to boost productivity, but they often generate vulnerable code. Thus, there is an urgent need to ensure that code generated by Code LLMs is correct and secure. Previous research has primarily focused on generating secure code, overlooking the fact that secure code also needs to be correct. This oversight can lead to a false sense of security. Currently, the community lacks a method to measure actual progress in this area, and we need solutions that address both security and correctness of code generation. This paper introduces a new benchmark, CodeGuard+, along with two new metrics, to measure Code LLMs' ability to generate both secure and correct code. Using our new evaluation methods, we show that the state-of-the-art defense technique, prefix tuning, may not be as strong as previously believed, since it generates secure code but sacrifices functional correctness. We also demonstrate that different decoding methods significantly affect the security of Code LLMs. Furthermore, we explore a new defense direction: constrained decoding for secure code generation. We propose new constrained decoding techniques to generate secure code. Our results reveal that constrained decoding is more effective than prefix tuning to improve the security of Code LLMs, without requiring a specialized training dataset. Moreover, our evaluations over eight state-of-the-art Code LLMs show that constrained decoding has strong performance to improve the security of Code LLMs, and our technique outperforms GPT-4.",
        "keywords": [
            "code generation",
            "code model security",
            "defense"
        ]
    },
    "Instruction tuning for secure code generation": {
        "type": "article",
        "key": "he2024instruction",
        "title": "Instruction tuning for secure code generation",
        "author": "He, Jingxuan and Vero, Mark and Krasnopolska, Gabriela and Vechev, Martin",
        "journal": "arXiv preprint arXiv:2402.09497",
        "year": "2024",
        "abstract": "Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets. It is able to drastically improve security (by about 30%), while preserving utility.",
        "venue": "ICML2024",
        "keywords": [
            "code generation",
            "code model security",
            "defense"
        ]
    },
    "Large language models for code: Security hardening and adversarial testing": {
        "type": "inproceedings",
        "key": "he2023large",
        "title": "Large language models for code: Security hardening and adversarial testing",
        "author": "He, Jingxuan and Vechev, Martin",
        "booktitle": "Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "1865--1879",
        "year": "2023",
        "abstract": "Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN closely matches the original LMs in functional correctness.",
        "venue": "CCS2023",
        "keywords": [
            "code generation",
            "code model security",
            "defense",
            "attack"
        ]
    },
    "Graphcodebert: Pre-training code representations with data flow": {
        "type": "article",
        "key": "guo2020graphcodebert",
        "title": "Graphcodebert: Pre-training code representations with data flow",
        "author": "Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others",
        "journal": "arXiv preprint arXiv:2009.08366",
        "year": "2020",
        "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.",
        "venue": "ICLR2021",
        "keywords": [
            "code model",
            "source code model"
        ]
    },
    "Codebert: A pre-trained model for programming and natural languages": {
        "type": "article",
        "key": "feng2020codebert",
        "title": "Codebert: A pre-trained model for programming and natural languages",
        "author": "Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others",
        "journal": "arXiv preprint arXiv:2002.08155",
        "year": "2020",
        "abstract": "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and nat-ural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language codesearch, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both bimodal data of NL-PL pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation tasks. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing.",
        "venue": "EMNLP2020",
        "keywords": [
            "code model",
            "source code model"
        ]
    },
    "Neural code comprehension: A learnable representation of code semantics": {
        "type": "article",
        "key": "ben2018neural",
        "title": "Neural code comprehension: A learnable representation of code semantics",
        "author": "Ben-Nun, Tal and Jakobovits, Alice Shoshana and Hoefler, Torsten",
        "journal": "Advances in neural information processing systems",
        "volume": "31",
        "year": "2018",
        "abstract": "With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human-and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data-and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.",
        "venue": "NeurIPS2018",
        "keywords": [
            "code model",
            "source code model"
        ]
    },
    "Meta large language model compiler: Foundation models of compiler optimization": {
        "type": "article",
        "key": "cummins2024meta",
        "title": "Meta large language model compiler: Foundation models of compiler optimization",
        "author": "Cummins, Chris and Seeker, Volker and Grubisic, Dejan and Roziere, Baptiste and Gehring, Jonas and Synnaeve, Gabriel and Leather, Hugh",
        "journal": "arXiv preprint arXiv:2407.02524",
        "year": "2024",
        "venue": "Meta2024",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.",
        "keywords": [
            "code model",
            "IR code model",
            "code generation",
            "compiler optimization"
        ]
    },
    "Symmetry-Preserving Program Representations for Learning Code Semantics": {
        "type": "article",
        "key": "pei2023symmetry",
        "title": "Symmetry-Preserving Program Representations for Learning Code Semantics",
        "author": "Pei, Kexin and Li, Weichen and Jin, Qirui and Liu, Shuyang and Geng, Scott and Cavallaro, Lorenzo and Yang, Junfeng and Jana, Suman",
        "journal": "arXiv preprint arXiv:2308.03312",
        "year": "2023",
        "venue": "ICML2024",
        "abstract": "Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures. Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiveness in generalization and robustness through detailed experimental evaluations across different binary and source code analysis tasks. Overall, our code symmetry framework offers rigorous and powerful reasoning techniques that can guide the future development of specialized LLMs for code and advance LLM-guided program reasoning tasks.",
        "keywords": [
            "code model",
            "source code model"
        ]
    },
    "Fair: Flow type-aware pre-training of compiler intermediate representations": {
        "type": "inproceedings",
        "key": "niu2024fair",
        "title": "Fair: Flow type-aware pre-training of compiler intermediate representations",
        "author": "Niu, Changan and Li, Chuanyi and Ng, Vincent and Lo, David and Luo, Bin",
        "booktitle": "Proceedings of the 46th IEEE/ACM International Conference on Software Engineering",
        "pages": "1--12",
        "year": "2024",
        "abstract": "While the majority of existing pre-trained models from code learn source code features such as code tokens and abstract syntax trees, there are some other works that focus on learning from compiler intermediate representations (IRs). Existing IR-based models typically utilize IR features such as instructions, control and data flow graphs (CDFGs), call graphs, etc. However, these methods confuse variable nodes and instruction nodes in a CDFG and fail to distinguish different types of flows, and the neural networks they use fail to capture long-distance dependencies and have over-smoothing and over-squashing problems. To address these weaknesses, we propose FAIR, a Flow type-Aware pre-trained model for IR that involves employing (1) a novel input representation of IR programs; (2) Graph Transformer to address over-smoothing, over-squashing and long-dependencies problems; and (3) five pre-training tasks that we specifically propose to enable FAIR to learn the semantics of IR tokens, flow type information, and the overall representation of IR. Experimental results show that FAIR can achieve state-of-the-art results on four code-related downstream tasks.",
        "venue": "ICSE2024",
        "keywords": [
            "code model",
            "IR code model"
        ]
    },
    "How could neural networks understand programs?": {
        "type": "inproceedings",
        "key": "peng2021could",
        "title": "How could neural networks understand programs?",
        "author": "Peng, Dinglan and Zheng, Shuxin and Li, Yatao and Ke, Guolin and He, Di and Liu, Tie-Yan",
        "booktitle": "International Conference on Machine Learning",
        "pages": "8476--8486",
        "year": "2021",
        "organization": "PMLR",
        "venue": "ICML2021",
        "abstract": "Semantic understanding of programs is a fundamental problem for programming language processing (PLP). Recent works that learn representations of code based on pre-training techniques in NLP have pushed the frontiers in this direction. However, the semantics of PL and NL have essential differences. These being ignored, we believe it is difficult to build a model to better understand programs, by either directly applying off-the-shelf NLP pre-training techniques to the source code, or adding features to the model by the heuristic. In fact, the semantics of a program can be rigorously defined by formal semantics in PL theory. For example, the operational semantics, describes the meaning of a valid program as updating the environment (ie, the memory address-value function) through fundamental operations, such as memory I/O and conditional branching. Inspired by this, we propose a novel program semantics learning paradigm, that the model should learn from information composed of (1) the representations which align well with the fundamental operations in operational semantics, and (2) the information of environment transition, which is indispensable for program understanding. To validate our proposal, we present a hierarchical Transformer-based pre-training model called OSCAR to better facilitate the understanding of programs. OSCAR learns from intermediate representation (IR) and an encoded representation derived from static analysis, which are used for representing the fundamental operations and approximating the environment transitions respectively. OSCAR empirically shows the outstanding capability of program semantics understanding on many practical software engineering tasks. Code and models are released at:\\url {https://github. com/pdlan/OSCAR}.",
        "keywords": [
            "code model",
            "IR code model"
        ]
    },
    "Programl: A graph-based program representation for data flow analysis and compiler optimizations": {
        "type": "inproceedings",
        "key": "cummins2021programl",
        "title": "Programl: A graph-based program representation for data flow analysis and compiler optimizations",
        "author": "Cummins, Chris and Fisches, Zacharias V and Ben-Nun, Tal and Hoefler, Torsten and O\u2019Boyle, Michael FP and Leather, Hugh",
        "booktitle": "International Conference on Machine Learning",
        "pages": "2244--2253",
        "year": "2021",
        "organization": "PMLR",
        "venue": "ICML2021",
        "abstract": "Machine learning (ML) is increasingly seen as a viable approach for building compiler optimization heuristics, but many ML methods cannot replicate even the simplest of the data flow analyses that are critical to making good optimization decisions. We posit that if ML cannot do that, then it is insufficiently able to reason about programs. We formulate data flow analyses as supervised learning tasks and introduce a large open dataset of programs and their corresponding labels from several analyses. We use this dataset to benchmark ML methods and show that they struggle on these fundamental program reasoning tasks. We propose ProGraML-Program Graphs for Machine Learning-a language-independent, portable representation of program semantics. ProGraML overcomes the limitations of prior works and yields improved performance on downstream optimization tasks.",
        "keywords": [
            "static analysis",
            "fundamental analysis",
            "compiler optimization",
            "code model",
            "IR code model"
        ]
    },
    "ReSym: Harnessing LLMs to Recover Variable and Data Structure Symbols from Stripped Binaries": {
        "type": "inproceedings",
        "key": "xie2024resym",
        "title": "ReSym: Harnessing LLMs to Recover Variable and Data Structure Symbols from Stripped Binaries",
        "author": "Xie, Danning and Zhang, Zhuo and Jiang, Nan and Xu, Xiangzhe and Tan, Lin and Zhang, Xiangyu",
        "booktitle": "Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security (CCS)",
        "year": "2024",
        "venue": "CCS2024",
        "abstract": "Decompilation aims to recover a binary executable to the source code form and hence has a wide range of applications in cyber security, such as malware analysis and legacy code hardening. A prominent challenge is to recover variable symbols, including both primitive and complex types such as user-defined data structures, along with their symbol information such as names and types. Existing efforts focus on solving parts of the problem, eg, recovering only types (without names) or only local variables (without user-defined structures). In this paper, we propose ReSym, a novel hybrid technique that combines Large Language Models (LLMs) and program analysis to recover both names and types for local variables and user-defined data structures. Our method encompasses fine-tuning two LLMs to handle local variables and structures, respectively. To overcome the inherent token limitations in current LLMs, we devise a novel Prolog-based algorithm to aggregate and cross-check results from multiple LLM queries, suppressing uncertainty and hallucinations. Our experiments show that ReSym is effective in recovering variable information and user-defined data structures, substantially outperforming the state-of-the-art methods.",
        "keywords": [
            "code model",
            "binary code model",
            "code generation",
            "program decompilation"
        ]
    },
    "Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases": {
        "type": "article",
        "key": "su2024source",
        "title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases",
        "author": "Su, Zian and Xu, Xiangzhe and Huang, Ziyang and Zhang, Kaiyuan and Zhang, Xiangyu",
        "journal": "arXiv preprint arXiv:2405.19581",
        "year": "2024",
        "venue": "NeurIPS2024",
        "abstract": "Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a GPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.",
        "keywords": [
            "code model",
            "binary code model"
        ]
    },
    "Codeart: Better code models by attention regularization when symbols are lacking": {
        "type": "article",
        "key": "su2024codeart",
        "title": "Codeart: Better code models by attention regularization when symbols are lacking",
        "author": "Su, Zian and Xu, Xiangzhe and Huang, Ziyang and Zhang, Zhuo and Ye, Yapeng and Huang, Jianjun and Zhang, Xiangyu",
        "journal": "Proceedings of the ACM on Software Engineering",
        "volume": "1",
        "number": "FSE",
        "pages": "562--585",
        "year": "2024",
        "publisher": "ACM New York, NY, USA",
        "venue": "FSE2024",
        "abstract": "Transformer based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more important compared to others. To realize the idea, we enhance the vanilla tokenization and model architecture of a BERT model, construct and utilize attention masks, and introduce a new pre-training algorithm. We pre-train this BERT-like model from scratch, using a dataset of 26 million stripped binary functions with explicit program dependence information extracted by our tool. We apply the model in three downstream tasks: binary similarity, type inference, and malware family classification. Our pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49% to 60%, and 74% to 94%, respectively. It also substantially outperforms other general pre-training techniques of code understanding models.",
        "keywords": [
            "code model",
            "binary code model"
        ]
    },
    "Lmpa: Improving decompilation by synergy of large language model and program analysis": {
        "type": "article",
        "key": "xu2023lmpa",
        "title": "Lmpa: Improving decompilation by synergy of large language model and program analysis",
        "author": "Xu, Xiangzhe and Zhang, Zhuo and Feng, Shiwei and Ye, Yapeng and Su, Zian and Jiang, Nan and Cheng, Siyuan and Tan, Lin and Zhang, Xiangyu",
        "journal": "arXiv preprint arXiv:2306.02546",
        "year": "2023",
        "abstract": "Decompilation aims to recover the source code form of a binary executable. It has many applications in security and software engineering such as malware analysis, vulnerability detection and code reuse. A prominent challenge in decompilation is to recover variable names. We propose a novel method that leverages the synergy of large language model (LLM) and program analysis. Language models encode rich multi-modal knowledge, but its limited input size prevents providing sufficient global context for name recovery. We propose to divide the task to many LLM queries and use program analysis to correlate and propagate the query results, which in turn improves the performance of LLM by providing additional contextual information. Our results show that 75% of the recovered names are considered good by users and our technique outperforms the state-of-the-art technique by 16.5% and 20.23% in precision and recall, respectively.",
        "keywords": [
            "code model",
            "binary code model"
        ]
    },
    "Jtrans: Jump-aware transformer for binary code similarity detection": {
        "type": "inproceedings",
        "key": "wang2022jtrans",
        "title": "Jtrans: Jump-aware transformer for binary code similarity detection",
        "author": "Wang, Hao and Qu, Wenjie and Katz, Gilad and Zhu, Wenyu and Gao, Zeyu and Qiu, Han and Zhuge, Jianwei and Zhang, Chao",
        "booktitle": "Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1--13",
        "year": "2022",
        "venue": "ISSTA2022",
        "abstract": "Binary code similarity detection (BCSD) has important applications in various fields such as vulnerabilities detection, software component analysis, and reverse engineering. Recent studies have shown that deep neural networks (DNNs) can comprehend instructions or control-flow graphs (CFG) of binary code and support BCSD. In this study, we propose a novel Transformer-based approach, namely jTrans, to learn representations of binary code. It is the first solution that embeds control flow information of binary code into Transformer-based language models, by using a novel jump-aware representation of the analyzed binaries and a newly-designed pre-training task. Additionally, we release to the community a newly-created large dataset of binaries, BinaryCorp, which is the most diverse to date. Evaluation results show that jTrans outperforms state-of-the-art (SOTA) approaches on this more challenging dataset by 30.5% (i.e., from 32.0% to 62.5%). In a real-world task of known vulnerability searching, jTrans achieves a recall that is 2X higher than existing SOTA baselines.",
        "keywords": [
            "static analysis",
            "binary code model",
            "code model",
            "binary code model"
        ]
    },
    "Swe-bench: Can language models resolve real-world github issues?": {
        "type": "article",
        "key": "jimenez2023swe",
        "title": "Swe-bench: Can language models resolve real-world github issues?",
        "author": "Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik",
        "journal": "arXiv preprint arXiv:2310.06770",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",
        "keywords": [
            "benchmark",
            "code generation",
            "program repair"
        ]
    },
    "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories": {
        "type": "article",
        "key": "li2024evocodebench",
        "title": "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories",
        "author": "Li, Jia and Li, Ge and Zhang, Xuanming and Dong, Yihong and Jin, Zhi",
        "journal": "arXiv preprint arXiv:2404.00599",
        "year": "2024",
        "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repository-level code generation and evaluate 10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa, Gemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 only is 20.73% in our experiments. We also analyze failed cases and summarize the shortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.",
        "keywords": [
            "benchmark",
            "code generation"
        ]
    },
    "CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks": {
        "type": "article",
        "key": "xie2024codebenchgen",
        "title": "CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks",
        "author": "Xie, Yiqing and Xie, Alex and Sheth, Divyanshu and Liu, Pengfei and Fried, Daniel and Rose, Carolyn",
        "journal": "arXiv preprint arXiv:2404.00566",
        "year": "2024",
        "abstract": "To facilitate evaluation of code generation systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based benchmarks that only requires light guidance from humans. Specifically, we leverage a large language model (LLM) to convert an arbitrary piece of code into an evaluation example, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries revised from code in 367 GitHub repositories taken from the CodeSearchNet dataset. To demonstrate the complexity and solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as ``requires effort to solve''. We conduct code generation experiments on open-source and proprietary models and analyze the performance of both humans and models. We will release the code of both the framework and the dataset upon acceptance.",
        "keywords": [
            "code generation",
            "benchmark"
        ]
    },
    "A Survey on Large Language Models for Code Generation": {
        "type": "article",
        "key": "jiang2024survey",
        "title": "A Survey on Large Language Models for Code Generation",
        "author": "Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun",
        "journal": "arXiv preprint arXiv:2406.00515",
        "year": "2024",
        "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels of difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (this https URL) to continuously document and disseminate the most recent advances in the field.",
        "keywords": [
            "survey",
            "code generation"
        ]
    },
    "Is Self-Repair a Silver Bullet for Code Generation?": {
        "type": "inproceedings",
        "key": "olausson2023self",
        "title": "Is Self-Repair a Silver Bullet for Code Generation?",
        "author": "Olausson, Theo X and Inala, Jeevana Priya and Wang, Chenglong and Gao, Jianfeng and Solar-Lezama, Armando",
        "booktitle": "The Twelfth International Conference on Learning Representations",
        "year": "2023",
        "venue": "ICLR2023",
        "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair---in which the model debugs and repairs its own code---has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.",
        "keywords": [
            "code generation",
            "program repair"
        ]
    },
    "Repairagent: An autonomous, llm-based agent for program repair": {
        "type": "article",
        "key": "bouzenia2024repairagent",
        "title": "Repairagent: An autonomous, llm-based agent for program repair",
        "author": "Bouzenia, Islem and Devanbu, Premkumar and Pradel, Michael",
        "journal": "arXiv preprint arXiv:2403.17134",
        "year": "2024",
        "abstract": "Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.",
        "keywords": [
            "code generation",
            "program repair",
            "agent design",
            "planning"
        ]
    },
    "Natural Language Commanding via Program Synthesis": {
        "type": "article",
        "key": "gandhi2023natural",
        "title": "Natural Language Commanding via Program Synthesis",
        "author": "Gandhi, Apurva and Nguyen, Thong Q and Jiao, Huitian and Steen, Robert and Bhatawdekar, Ameya",
        "journal": "arXiv preprint arXiv:2306.03460",
        "year": "2023",
        "venue": "Microsoft2023",
        "abstract": "We present Semantic Interpreter, a natural language-friendly AI system for productivity software such as Microsoft Office that leverages large language models (LLMs) to execute user intent across application features. While LLMs are excellent at understanding user intent expressed as natural language, they are not sufficient for fulfilling application-specific user intent that requires more than text-to-text transformations. We therefore introduce the Office Domain Specific Language (ODSL), a concise, high-level language specialized for performing actions in and interacting with entities in Office applications. Semantic Interpreter leverages an Analysis-Retrieval prompt construction method with LLMs for program synthesis, translating natural language user utterances to ODSL programs that can be transpiled to application APIs and then executed. We focus our discussion primarily on a research exploration for Microsoft PowerPoint.",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "Effective Large Language Model Debugging with Best-first Tree Search": {
        "type": "article",
        "key": "song2024effective",
        "title": "Effective Large Language Model Debugging with Best-first Tree Search",
        "author": "Song, Jialin and Raiman, Jonathan and Catanzaro, Bryan",
        "journal": "arXiv preprint arXiv:2407.19055",
        "year": "2024",
        "venue": "NVDIA2024",
        "abstract": "Large Language Models (LLMs) show promise in code generation tasks. However, their code-writing abilities are often limited in scope: while they can successfully implement simple functions, they struggle with more complex tasks. A fundamental difference with how an LLM writes code, compared to a human programmer, is that it cannot consistently spot and fix bugs. Debugging is a crucial skill for programmers and it enables iterative code refinement towards a correct implementation. In this work, we propose a novel algorithm to enable LLMs to debug their code via self-reflection and search where a model attempts to identify its previous mistakes. Our key contributions are 1) a best-first tree search algorithm with self-reflections (BESTER) that achieves state-of-the-art Pass@1 in three code generation benchmarks. BESTER maintains its superiority when we measure pass rates taking into account additional inference costs incurred by tree search. 2) A novel interpretability study on what self-reflections attend to in buggy programs and how they impact bug fixes, which provides a deeper understanding of the debugging process. 3) An extensive study on when self-reflections are effective in finding bugs.",
        "keywords": [
            "code generation",
            "debugging"
        ]
    },
    "Automatic Programming: Large Language Models and Beyond": {
        "type": "article",
        "key": "lyu2024automatic",
        "title": "Automatic Programming: Large Language Models and Beyond",
        "author": "Lyu, Michael R and Ray, Baishakhi and Roychoudhury, Abhik and Tan, Shin Hwei and Thongtanunam, Patanamon",
        "journal": "arXiv preprint arXiv:2405.02213",
        "year": "2024",
        "abstract": "Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs, can help produce higher assurance code from LLMs, along with evidence of assurance.",
        "keywords": [
            "general coding task",
            "empirical study"
        ]
    },
    "Verified multi-step synthesis using large language models and monte carlo tree search": {
        "type": "article",
        "key": "brandfonbrener2024verified",
        "title": "Verified multi-step synthesis using large language models and monte carlo tree search",
        "author": "Brandfonbrener, David and Raja, Sibi and Prasad, Tarun and Loughridge, Chloe and Yang, Jianang and Henniger, Simon and Byrd, William E and Zinkov, Robert and Amin, Nada",
        "journal": "arXiv preprint arXiv:2402.08147",
        "year": "2024",
        "venue": "NeurIPS2024",
        "abstract": "We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq. Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step. In combination with the LLM prior, the verifier feedback raises the synthesis capabilities of open source models. On a set of five verified programming problems, we find that in four problems where the base model cannot solve the question even when re-sampling solutions for one hour, VMCTS can solve the problems within 6 minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented with plugins and multiple re-tries on these problems. Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search.",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "Hypothesis search: Inductive reasoning with language models": {
        "type": "article",
        "key": "wang2023hypothesis",
        "title": "Hypothesis search: Inductive reasoning with language models",
        "author": "Wang, Ruocheng and Zelikman, Eric and Poesia, Gabriel and Pu, Yewen and Haber, Nick and Goodman, Noah D",
        "journal": "arXiv preprint arXiv:2309.05660",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which can then be robustly generalized to novel scenarios. Recent work has evaluated large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding \"in context learning.\" This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be directly verified by running on the observed examples and generalized to novel inputs. Because of the prohibitive cost of generation with state-of-the-art LLMs, we consider a middle step to filter the set of hypotheses that will be implemented into programs: we either ask the LLM to summarize into a smaller set of hypotheses, or ask human annotators to select a subset of the hypotheses. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. On a random 40-problem subset of ARC, our automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted to 37.5%. (And we argue this is a lower bound on the performance of our approach without filtering.) Our ablation studies show that abstract hypothesis generation and concrete program representations are both beneficial for LLMs to perform inductive reasoning tasks.",
        "keywords": [
            "code generation",
            "program synthesis",
            "static analysis",
            "program verification"
        ]
    },
    "Guess \\& Sketch: Language Model Guided Transpilation": {
        "type": "article",
        "key": "lee2023guess",
        "title": "Guess \\& Sketch: Language Model Guided Transpilation",
        "author": "Lee, Celine and Mahmoud, Abdulrahman and Kurek, Michal and Campanoni, Simone and Brooks, David and Chong, Stephen and Wei, Gu-Yeon and Rush, Alexander M",
        "journal": "arXiv preprint arXiv:2309.14396",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods. Guess & Sketch extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test Guess & Sketch on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler. We also share a training and evaluation dataset for this task.",
        "keywords": [
            "code generation",
            "program transformation"
        ]
    },
    "Verified Code Transpilation with LLMs": {
        "type": "article",
        "key": "bhatia2024verified",
        "title": "Verified Code Transpilation with LLMs",
        "author": "Bhatia, Sahil and Qiu, Jie and Hasabnis, Niranjan and Seshia, Sanjit A and Cheung, Alvin",
        "journal": "arXiv preprint arXiv:2406.03003",
        "year": "2024",
        "venue": "NeurIPS2024",
        "abstract": "Domain-specific languages (DSLs) are integral to various software workflows. Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability. However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API. While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the transpiled code. Another approach for automating this task is verified lifting, which relies on program synthesis to find programs in the target language that are functionally equivalent to the source language program. While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient. In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools. We use the LLM's capabilities to reason about programs to translate a given program into its corresponding equivalent in the target language. Additionally, we use LLMs to generate proofs for functional equivalence. We develop lifting-based compilers for {\\em four different} DSLs targeting different application domains. Our approach not only outperforms previous symbolic-based tools in both the number of benchmarks transpiled and transpilation time, but also requires significantly less effort to build.",
        "keywords": [
            "code generation",
            "program synthesis",
            "static analysis",
            "program verification"
        ]
    },
    "Rectifier: Code translation with corrector via llms": {
        "type": "article",
        "key": "yin2024rectifier",
        "title": "Rectifier: Code translation with corrector via llms",
        "author": "Yin, Xin and Ni, Chao and Nguyen, Tien N and Wang, Shaohua and Yang, Xiaohu",
        "journal": "arXiv preprint arXiv:2407.07472",
        "year": "2024",
        "abstract": "Software migration is garnering increasing attention with the evolution of software and society. Early studies mainly relied on handcrafted translation rules to translate between two languages, the translation process is error-prone and time-consuming. In recent years, researchers have begun to explore the use of pre-trained large language models (LLMs) in code translation. However, code translation is a complex task that LLMs would generate mistakes during code translation, they all produce certain types of errors when performing code translation tasks, which include (1) compilation error, (2) runtime error, (3) functional error, and (4) non-terminating execution. We found that the root causes of these errors are very similar (e.g. failure to import packages, errors in loop boundaries, operator errors, and more). In this paper, we propose a general corrector, namely Rectifier, which is a micro and universal model for repairing translation errors. It learns from errors generated by existing LLMs and can be widely applied to correct errors generated by any LLM. The experimental results on translation tasks between C++, Java, and Python show that our model has effective repair ability, and cross experiments also demonstrate the robustness of our method.",
        "keywords": [
            "code generation",
            "program transformation"
        ]
    },
    "Learning performance-improving code edits": {
        "type": "article",
        "key": "shypula2023learning",
        "title": "Learning performance-improving code edits",
        "author": "Shypula, Alexander and Madaan, Aman and Zeng, Yimeng and Alon, Uri and Gardner, Jacob and Hashemi, Milad and Neubig, Graham and Ranganathan, Parthasarathy and Bastani, Osbert and Yazdanbakhsh, Amir",
        "journal": "arXiv preprint arXiv:2302.07867",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "With the waning of Moore's law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code. Simultaneously, pretrained large language models (LLMs) have demonstrated strong capabilities at solving a wide range of programming tasks. To that end, we introduce a framework for adapting LLMs to high-level program optimization. First, we curate a dataset of performance-improving edits made by human programmers of over 77K competitive C++ programming submission pairs, accompanied by extensive unit tests. A major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious \"improvements\". To isolate and reliably evaluate the impact of program optimizations, we design an environment based on the gem5 full system simulator, the de facto simulator used in academia and industry. Next, we propose a broad range of adaptation strategies for code optimization; for prompting, these include retrieval-based few-shot prompting and chain-of-thought, and for finetuning, these include performance-conditioned generation and synthetic data augmentation based on self-play. A combination of these techniques achieves an average speedup of 5.65X on CodeLlama-13B and 6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our proposed performance-conditioned generation is particularly effective at improving performance as well as increasing the fraction of optimized programs.",
        "keywords": [
            "code generation",
            "program transformation"
        ]
    },
    "Enabling Memory Safety of C Programs using LLMs": {
        "type": "article",
        "key": "mohammed2024enabling",
        "title": "Enabling Memory Safety of C Programs using LLMs",
        "author": "Mohammed, Nausheen and Lal, Akash and Rastogi, Aseem and Roy, Subhajit and Sharma, Rahul",
        "journal": "arXiv preprint arXiv:2404.01096",
        "year": "2024",
        "abstract": "Memory safety violations in low-level code, written in languages like C, continues to remain one of the major sources of software vulnerabilities. One method of removing such violations by construction is to port C code to a safe C dialect. Such dialects rely on programmer-supplied annotations to guarantee safety with minimal runtime overhead. This porting, however, is a manual process that imposes significant burden on the programmer and, hence, there has been limited adoption of this technique. The task of porting not only requires inferring annotations, but may also need refactoring/rewriting of the code to make it amenable to such annotations. In this paper, we use Large Language Models (LLMs) towards addressing both these concerns. We show how to harness LLM capabilities to do complex code reasoning as well as rewriting of large codebases. We also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an LLM. We implement our ideas in a tool called MSA that targets the CheckedC dialect. We evaluate MSA on several micro-benchmarks, as well as real-world code ranging up to 20K lines of code. We showcase superior performance compared to a vanilla LLM baseline, as well as demonstrate improvement over a state-of-the-art symbolic (non-LLM) technique.",
        "keywords": [
            "code generation",
            "program transformation"
        ]
    },
    "Refactoring programs using large language models with few-shot examples": {
        "type": "inproceedings",
        "key": "shirafuji2023refactoring",
        "title": "Refactoring programs using large language models with few-shot examples",
        "author": "Shirafuji, Atsushi and Oda, Yusuke and Suzuki, Jun and Morishita, Makoto and Watanobe, Yutaka",
        "booktitle": "2023 30th Asia-Pacific Software Engineering Conference (APSEC)",
        "pages": "151--160",
        "year": "2023",
        "organization": "IEEE",
        "abstract": "A less complex and more straightforward program is a crucial factor that enhances its maintainability and makes writing secure and bug-free programs easier. However, due to its heavy workload and the risks of breaking the working programs, programmers are reluctant to do code refactoring, and thus, it also causes the loss of potential learning experiences. To mitigate this, we demonstrate the application of using a large language model (LLM), GPT-3.5, to suggest less complex versions of the user-written Python program, aiming to encourage users to learn how to write better programs. We propose a method to leverage the prompting with few-shot examples of the LLM by selecting the best-suited code refactoring examples for each target programming problem based on the prior evaluation of prompting with the one-shot example. The quantitative evaluation shows that 95.68% of programs can be refactored by generating 10 candidates each, resulting in a 17.35% reduction in the average cyclomatic complexity and a 25.84% decrease in the average number of lines after filtering only generated programs that are semantically correct. Further-more, the qualitative evaluation shows outstanding capability in code formatting, while unnecessary behaviors such as deleting or translating comments are also observed.",
        "venue": "APSEC2023",
        "keywords": [
            "code generation",
            "program transformation"
        ]
    },
    "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2405-15383",
        "author": "Nicola Dainese and Matteo Merler and Minttu Alakuijala and Pekka Marttinen",
        "title": "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search",
        "journal": "CoRR",
        "volume": "abs/2405.15383",
        "year": "2024",
        "url": "https://doi.org/10.48550/arXiv.2405.15383",
        "doi": "10.48550/ARXIV.2405.15383",
        "eprinttype": "arXiv",
        "eprint": "2405.15383",
        "timestamp": "Wed, 19 Jun 2024 08:52:55 +0200",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2405-15383.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "NeurIPS2024",
        "abstract": "In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL). Calling code instead of LLMs for planning has the advantages of being precise, reliable, interpretable, and extremely efficient. However, writing appropriate Code World Models requires the ability to understand complex instructions, to generate exact code with non-trivial logic and to self-debug a long program with feedback from unit tests and environment trajectories. To address these challenges, we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for LLMs. To test our approach, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories. GIF-MCTS surpasses all baselines on the CWMB and two other benchmarks, and we show that the Code World Models synthesized with it can be successfully used for planning, resulting in model-based RL agents with greatly improved sample efficiency and inference speed.",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules": {
        "type": "inproceedings",
        "key": "DBLP:conf/iclr/LeCSGSJ24",
        "author": "Hung Le and Hailin Chen and Amrita Saha and Akash Gokul and Doyen Sahoo and Shafiq Joty",
        "title": "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules",
        "booktitle": "The Twelfth International Conference on Learning Representations",
        "publisher": "OpenReview.net",
        "year": "2024",
        "url": "https://openreview.net/forum?id=vYhglxSj8j",
        "timestamp": "Wed, 07 Aug 2024 17:11:53 +0200",
        "biburl": "https://dblp.org/rec/conf/iclr/LeCSGSJ24.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "ICLR2024",
        "abstract": "Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extracting and clustering the generated sub-modules and selecting the cluster representatives as the more generic and re-usable implementations, and 2) augmenting the original chain-of-thought prompt with these selected module-implementations and instructing the LLM to re-generate new modularized solutions. We find that by naturally encouraging the LLM to reuse the previously developed and verified sub-modules, CodeChain can significantly boost both modularity as well as correctness of the generated solutions, achieving relative pass@1 improvements of 35% on APPS and 76% on CodeContests. It is shown to be effective on both OpenAI LLMs as well as open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation studies with different methods of prompting, number of clusters, model sizes, program qualities, etc., to provide useful insights that underpin CodeChain's success.",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "LongCoder: {A} Long-Range Pre-trained Language Model for Code Completion": {
        "type": "inproceedings",
        "key": "DBLP:conf/icml/GuoXD0M23",
        "author": "Daya Guo and Canwen Xu and Nan Duan and Jian Yin and Julian J. McAuley",
        "title": "LongCoder: {A} Long-Range Pre-trained Language Model for Code Completion",
        "booktitle": "International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA",
        "series": "Proceedings of Machine Learning Research",
        "volume": "202",
        "pages": "12098--12107",
        "publisher": "PMLR",
        "year": "2023",
        "url": "https://proceedings.mlr.press/v202/guo23j.html",
        "timestamp": "Mon, 28 Aug 2023 17:23:08 +0200",
        "biburl": "https://dblp.org/rec/conf/icml/GuoXD0M23.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "ICML2023",
        "abstract": "In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens-bridge tokens and memory tokens-to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference.",
        "keywords": [
            "code generation",
            "code completion",
            "code model",
            "source code model"
        ]
    },
    "Repository-Level Prompt Generation for Large Language Models of Code": {
        "type": "inproceedings",
        "key": "DBLP:conf/icml/ShrivastavaLT23",
        "author": "Disha Shrivastava and Hugo Larochelle and Daniel Tarlow",
        "title": "Repository-Level Prompt Generation for Large Language Models of Code",
        "booktitle": "International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA",
        "series": "Proceedings of Machine Learning Research",
        "volume": "202",
        "pages": "31693--31715",
        "publisher": "PMLR",
        "year": "2023",
        "url": "https://proceedings.mlr.press/v202/shrivastava23a.html",
        "timestamp": "Mon, 28 Aug 2023 17:23:09 +0200",
        "biburl": "https://dblp.org/rec/conf/icml/ShrivastavaLT23.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "ICML2023",
        "abstract": "With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex used in GitHub Copilot), techniques for introducing domain-specific knowledge in the prompt design process become important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn't require any access to the weights of the LLM, making it applicable in cases where we only have black-box access to the LLM. We conduct experiments on the task of single-line code-autocompletion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our prompt proposals gives a remarkably high relative improvement of 36% over Codex, showing the quality of these proposals. Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines.",
        "keywords": [
            "code generation",
            "code completion",
            "prompting strategies",
            "RAG"
        ]
    }
}