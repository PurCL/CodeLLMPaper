{
    "Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities": {
        "type": "article",
        "key": "10.1145/3664606",
        "author": "Ma, Wei and Liu, Shangqing and Zhao, Mengjie and Xie, Xiaofei and Wang, Wenhang and Hu, Qiang and Zhang, Jie and Liu, Yang",
        "title": "Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664606",
        "doi": "10.1145/3664606",
        "abstract": "Code models have made significant advancements in code intelligence by encoding knowledge about programming languages. While previous studies have explored the capabilities of these models in learning code syntax, there has been limited investigation on their ability to understand code semantics. Additionally, existing analyses assume that the number of edges between nodes at the abstract syntax tree&nbsp;(AST) is related to syntax distance, and also often require transforming the high-dimensional space of deep learning models to a low-dimensional one, which may introduce inaccuracies. To study how code models represent code syntax and semantics, we conduct a comprehensive analysis of seven code models, including four representative code pre-trained models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) and three large language models (LLMs) (StarCoder, CodeLlama and CodeT5+). We design four probing tasks to assess the models\u2019 capacities in learning both code syntax and semantics. These probing tasks reconstruct code syntax and semantics structures (AST, control dependence graph (CDG), data dependence graph (DDG), and control flow graph (CFG)) in the representation space. These structures are core concepts for code understanding. We also investigate the syntax token role in each token representation and the long dependency between the code tokens. Additionally, we analyze the distribution of attention weights related to code semantic structures. Through extensive analysis, our findings highlight the strengths and limitations of different code models in learning code syntax and semantics. The results demonstrate that these models excel in learning code syntax, successfully capturing the syntax relationships between tokens and the syntax roles of individual tokens. However, their performance in encoding code semantics varies. CodeT5 and CodeBERT demonstrate proficiency in capturing control and data dependencies, whereas UnixCoder shows weaker performance in this aspect. We do not observe LLMs generally performing much better than pre-trained models. The shallow layers of LLMs perform better than their deep layers. The investigation of attention weights reveals that different attention heads play distinct roles in encoding code semantics. Our research findings emphasize the need for further enhancements in code models to better learn code semantics. This study contributes to the understanding of code models\u2019 abilities in syntax and semantics analysis. Our findings provide guidance for future improvements in code models, facilitating their effective application in various code-related tasks.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "169",
        "numpages": "29",
        "keywords": [
            "static analysis",
            "foundamental analysis",
            "empirical study"
        ],
        "venue": "TOSEM2024"
    },
    "Self-Planning Code Generation with Large Language Models": {
        "type": "article",
        "key": "10.1145/3672456",
        "author": "Jiang, Xue and Dong, Yihong and Wang, Lecheng and Fang, Zheng and Shang, Qiwei and Li, Ge and Jin, Zhi and Jiao, Wenpin",
        "title": "Self-Planning Code Generation with Large Language Models",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672456",
        "doi": "10.1145/3672456",
        "abstract": "Although large language models (LLMs) have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule solution steps prior to implementation. To this end, we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem-solving. This paper proposes a self-planning code generation approach with large language models, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, LLM plans out concise solution steps from the intent combined with few-shot prompting. Subsequently, in the implementation phase, the model generates code step by step, guided by the preceding solution steps. We conduct extensive experiments on various code-generation benchmarks across multiple programming languages. Experimental results show that self-planning code generation achieves a relative improvement of up to 25.4\\% in Pass@1 compared to direct code generation, and up to 11.9\\% compared to Chain-of-Thought of code generation. Moreover, our self-planning approach also enhances the quality of the generated code with respect to correctness, readability, and robustness, as assessed by humans.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "182",
        "numpages": "30",
        "keywords": [
            "code generation",
            "program synthesis",
            "agent design",
            "planning",
            "empirical study"
        ],
        "venue": "TOSEM2024"
    },
    "Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Dataflows to Generate Dataflow Graphs in Dynamically Typed Code": {
        "type": "article",
        "key": "10.1145/3672458",
        "author": "Huang, Qing and Luo, Zhiwen and Xing, Zhenchang and Zeng, Jinshan and Chen, Jieshan and Xu, Xiwei and Chen, Yong",
        "title": "Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Dataflows to Generate Dataflow Graphs in Dynamically Typed Code",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672458",
        "doi": "10.1145/3672458",
        "abstract": "Dataflow graphs (DFGs) capture definitions (defs) and uses across program blocks, which is a fundamental program representation for program analysis, testing and maintenance. However, dynamically typed programming languages like Python present implicit dataflow issues that make it challenging to determine def-use flow information at compile time. Static analysis methods like Soot and WALA are inadequate for handling these issues, and manually enumerating comprehensive heuristic rules is impractical. Large pre-trained language models (LLMs) offer a potential solution, as they have powerful language understanding and pattern matching abilities, allowing them to predict implicit dataflow by analyzing code context and relationships between variables, functions, and statements in code. We propose leveraging LLMs\u2019 in-context learning ability to learn implicit rules and patterns from code representation and contextual information to solve implicit dataflow problems. To further enhance the accuracy of LLMs, we design a five-step chain of thought (CoT) and break it down into an Artificial Intelligence (AI) chain, with each step corresponding to a separate AI unit to generate accurate DFGs for Python code. Our approach\u2019s performance is thoroughly assessed, demonstrating the effectiveness of each AI unit in the AI Chain. Compared to static analysis, our method achieves 82\\% higher def coverage and 58\\% higher use coverage in DFG generation on implicit dataflow. We also prove the indispensability of each unit in the AI Chain. Overall, our approach offers a promising direction for building software engineering tools by utilizing foundation models, eliminating significant engineering and maintenance effort, but focusing on identifying problems for AI to solve.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "183",
        "numpages": "29",
        "keywords": [
            "static analysis",
            "fundamental analysis"
        ],
        "venue": "TOSEM2024"
    },
    "LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models": {
        "type": "article",
        "key": "10.1145/3664812",
        "author": "Feng, Xiaoning and Han, Xiaohong and Chen, Simin and Yang, Wei",
        "title": "LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664812",
        "doi": "10.1145/3664812",
        "abstract": "Large Language Models (LLMs) have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of LLMs, which is of paramount importance due to often vast generation demands and real-time requirements, has surprisingly received little attention. In this article, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art LLMs. By analyzing the working mechanism and implementation of 20,543 public-accessible LLMs, we observe a fundamental property in LLMs that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of LLMs instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime-generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that LLMs would have to go through enough iterations to satisfy the pre-configured threshold. We present LLMEffiChecker, which can work under both white-box setting and black-box setting. In the white-box scenario, LLMEffiChecker develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level. In the black-box scenario, LLMEffiChecker employs a causal inference-based approach to find critical tokens and similarly applies three levels of imperceptible perturbation to them. Both the white-box and black-box settings effectively delay the appearance of EOS, compelling these inputs to reach the naturally unreachable threshold. To demonstrate the effectiveness of LLMEffiChecker, we conduct a systematic evaluation on nine publicly available LLMs: Google T5, AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT, and Salesforce CodeGen. Experimental results show that LLMEffiChecker can increase on average LLMs\u2019 response latency and energy consumption by 325\\% to 3,244\\% and 344\\% to 3,616\\%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by LLMEffiChecker significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "186",
        "numpages": "38",
        "keywords": [
            "code model security",
            "attack"
        ],
        "venue": "TOSEM2024"
    },
    "Self-Collaboration Code Generation via ChatGPT": {
        "type": "article",
        "key": "10.1145/3672459",
        "author": "Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge",
        "title": "Self-Collaboration Code Generation via ChatGPT",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672459",
        "doi": "10.1145/3672459",
        "abstract": "Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct \u201cexperts,\u201d each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other\u2019s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development\u2019s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9\u201347.1\\% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "189",
        "numpages": "38",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "venue": "TOSEM2024"
    },
    "Risky Dynamic Typing-related Practices in Python: An Empirical Study": {
        "type": "article",
        "key": "10.1145/3649593",
        "author": "Chen, Zhifei and Chen, Lin and Yang, Yibiao and Feng, Qiong and Li, Xuansong and Song, Wei",
        "title": "Risky Dynamic Typing-related Practices in Python: An Empirical Study",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649593",
        "doi": "10.1145/3649593",
        "abstract": "Python\u2019s dynamic typing nature provides developers with powerful programming abstractions. However, many type-related bugs are accumulated in code bases of Python due to the misuse of dynamic typing. The goal of this article is to aid in the understanding of developers\u2019 high-risk practices toward dynamic typing and the early detection of type-related bugs. We first formulate the rules of six types of risky dynamic typing-related practices (type smells for short) in Python. We then develop a rule-based tool named RUPOR, which builds an accurate type base to detect type smells. Our evaluation shows that RUPOR outperforms the existing type smell detection techniques (including the Large Language Models\u2013based approaches, Mypy, and PYDYPE) on a benchmark of 900 Python methods. Based on RUPOR, we conduct an empirical study on 25 real-world projects. We find that type smells are significantly related to the occurrence of post-release faults. The fault-proneness prediction model built with type smell features slightly outperforms the model built without them. We also summarize the common patterns, including inserting type check to fix type smell bugs. These findings provide valuable insights for preventing and fixing type-related bugs in the programs written in dynamic-typed languages.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "140",
        "numpages": "35",
        "keywords": [
            "static analysis",
            "type inference",
            "bug detection",
            "empirical study"
        ],
        "venue": "TOSEM2024"
    },
    "Statically Contextualizing Large Language Models with Typed Holes": {
        "type": "article",
        "key": "10.1145/3689728",
        "author": "Blinn, Andrew and Li, Xiang and Kim, June Hyung and Omar, Cyrus",
        "title": "Statically Contextualizing Large Language Models with Typed Holes",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689728",
        "doi": "10.1145/3689728",
        "abstract": "Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate code context, particularly when working with definitions that are neither in the training data nor near the cursor. This paper demonstrates that tighter integration with the type and binding structure of the programming language in use, as exposed by its language server, can help address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server is able to identify the type and typing context of the hole that the programmer is filling, with Hazel's total syntax and type error correction ensuring that a meaningful program sketch is available whenever the developer requests a completion. This allows the system to prompt the LLM with codebase-wide contextual information that is not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server, which provides error localization and error messages. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications with accompanying unit tests that have been written from scratch to avoid data contamination, and that can easily be ported to new languages because they do not have large external library dependencies. These applications serve as challenge problems due to their extensive reliance on application-specific data structures. Through an ablation study, we examine the impact of contextualization with type definitions, function headers, and errors messages, individually and in combination. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel, a low-resource language, we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource mainstream languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "288",
        "numpages": "31",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark",
            "empirical study"
        ],
        "venue": "OOPSLA2024"
    },
    "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs": {
        "type": "article",
        "key": "10.1145/3689735",
        "author": "Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Freeman, Anders and Anderson, Carolyn Jane and Feldman, Molly Q and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun",
        "title": "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689735",
        "doi": "10.1145/3689735",
        "abstract": "Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as building blocks for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming language. Code LLMs produce impressive results on high-resource programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages that have limited training data available (e.g., OCaml, Racket, and several others).                                                                                                                                                                                                                                                              This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach, called MultiPL-T, generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. MultiPL-T translates training data from high-resource languages into training data for low-resource languages in the following way. 1) We use a Code LLM to synthesize unit tests for commented code from a high-resource source language, filtering out faulty tests and code with low test coverage. 2) We use a Code LLM to translate the code from the high-resource source language to a target low-resource language. This gives us a corpus of candidate training data in the target language, but many of these translations are wrong. 3) We use a lightweight compiler to compile the test cases generated in (1) from the source language to the target language, which allows us to filter our obviously wrong translations. The result is a training corpus in the target low-resource language where all items have been validated with test cases. We apply this approach to generate tens of thousands of new, validated training items for five low-resource languages: Julia, Lua, OCaml, R, and Racket, using Python as the source high-resource language. Furthermore, we use an open Code LLM (StarCoderBase) with open training data (The Stack), which allows us to decontaminate benchmarks, train models without violating licenses, and run experiments that could not otherwise be done.                                                                                                                                                                                                                                                              Using datasets generated with MultiPL-T, we present fine-tuned versions of StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket that outperform other fine-tunes of these base models on the natural language to code task. We also present Racket fine-tunes for two very recent models, DeepSeek Coder and StarCoder2, to show that MultiPL-T continues to outperform other fine-tuning approaches for low-resource languages. The MultiPL-T approach is easy to apply to new languages, and is significantly more efficient and effective than alternatives such as training longer.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "295",
        "numpages": "32",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model training",
            "source code model"
        ],
        "venue": "OOPSLA2024"
    },
    "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models": {
        "type": "article",
        "key": "10.1145/3689736",
        "author": "Yang, Chenyuan and Deng, Yinlin and Lu, Runyu and Yao, Jiayi and Liu, Jiawei and Jabbarvand, Reyhaneh and Zhang, Lingming",
        "title": "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689736",
        "doi": "10.1145/3689736",
        "abstract": "Compiler correctness is crucial, as miscompilation can falsify program behaviors, leading to serious consequences over the software supply chain. In the literature, fuzzing has been extensively studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates test programs without sufficient understanding of internal compiler behaviors. As such, they often fail to construct test programs to exercise intricate optimizations. Meanwhile, traditional white-box techniques, such as symbolic execution, are computationally inapplicable to the giant codebase of compiler systems. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks and even have achieved state-of-the-art performance in black-box fuzzing. Nonetheless, guiding LLMs with compiler source-code information remains a missing piece of research in compiler testing.              To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization, with a spotlight on detecting deep logic bugs in the emerging deep learning (DL) compilers. WhiteFox adopts a multi-agent framework: (i) an LLM-based analysis agent examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; (ii) an LLM-based generation agent produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are also used as feedback to further enhance the test generation prompt on the fly. Our evaluation on the three most popular DL compilers (i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows that WhiteFox can generate high-quality test programs to exercise deep optimizations requiring intricate conditions, practicing up to 8 times more optimizations than state-of-the-art fuzzers. To date, WhiteFox has found in total 101 bugs for the compilers under test, with 92 confirmed as previously unknown and 70 already fixed. Notably, WhiteFox has been recently acknowledged by the PyTorch team, and is in the process of being incorporated into its development workflow. Finally, beyond DL compilers, WhiteFox can also be adapted for compilers in different domains, such as LLVM, where WhiteFox has already found multiple bugs.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "296",
        "numpages": "27",
        "keywords": [
            "program testing",
            "fuzzing",
            "code model training",
            "source code model"
        ],
        "venue": "OOPSLA2024"
    },
    "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models": {
        "type": "article",
        "key": "10.1145/3689776",
        "author": "Li, Ningke and Li, Yuekang and Liu, Yi and Shi, Ling and Wang, Kailong and Wang, Haoyu",
        "title": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689776",
        "doi": "10.1145/3689776",
        "abstract": "Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations \u2014 coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges: 1) Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases. 2) Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations.    To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth.    Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7\\% to 59.8\\%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations.    To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "336",
        "numpages": "30",
        "keywords": [
            "program testing",
            "fuzzing"
        ],
        "venue": "OOPSLA2024"
    },
    "Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach": {
        "type": "article",
        "key": "10.1145/3649828",
        "author": "Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun",
        "title": "Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA1",
        "url": "https://doi.org/10.1145/3649828",
        "doi": "10.1145/3649828",
        "abstract": "While static analysis is instrumental in uncovering software bugs, its precision in analyzing large and intricate codebases remains challenging. The emerging prowess of Large Language Models (LLMs) offers a promising avenue to address these complexities. In this paper, we present LLift, a pioneering framework that synergizes static analysis and LLMs, with a spotlight on identifying use-before-initialization (UBI) bugs within the Linux kernel. Drawing from our insights into variable usage conventions in Linux, we enhance path analysis using post-constraint guidance. This approach, combined with our methodically crafted procedures, empowers LLift to adeptly handle the challenges of bug-specific modeling, extensive codebases, and the unpredictable nature of LLMs. Our real-world evaluations identified four previously undiscovered UBI bugs in the mainstream Linux kernel, which the Linux community has acknowledged. This study reaffirms the potential of marrying static analysis with LLMs, setting a compelling direction for future research in this area.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "apr",
        "articleno": "111",
        "numpages": "26",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "venue": "OOPSLA2024"
    },
    "PyDex: Repairing Bugs in Introductory Python Assignments using LLMs": {
        "type": "article",
        "key": "10.1145/3649850",
        "author": "Zhang, Jialu and Cambronero, Jos\\'{e} Pablo and Gulwani, Sumit and Le, Vu and Piskac, Ruzica and Soares, Gustavo and Verbruggen, Gust",
        "title": "PyDex: Repairing Bugs in Introductory Python Assignments using LLMs",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA1",
        "url": "https://doi.org/10.1145/3649850",
        "doi": "10.1145/3649850",
        "abstract": "Students often make mistakes in their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex (a version of GPT), to build an APR system -- PyDex -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate PyDex on 286 real student programs and compare to three baselines, including one that combines a state-of-the-art Python syntax repair engine, BIFI, and a state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that PyDex can fix more programs and produce smaller patches on average.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "apr",
        "articleno": "133",
        "numpages": "25",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "venue": "OOPSLA2024"
    },
    "TransLLaMa: LLM-based Simultaneous Translation System": {
        "type": "INPROCEEDINGS",
        "key": "koshkin-etal-2024-transllama",
        "author": "Koshkin, Roman and Sudoh, Katsuhito and Nakamura, Satoshi",
        "booktitle": "EMNLP-findings2024",
        "title": "TransLLaMa: LLM-based Simultaneous Translation System",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers. This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special \u201cwait\u201d token. This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems.",
        "keywords": [
            "code generation",
            "program transformation",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.27",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-introducing",
        "author": "Zhang, Shuoming and Zhao, Jiacheng and Xia, Chunwei and Wang, Zheng and Chen, Yunji and Cui, Huimin",
        "booktitle": "EMNLP-findings2024",
        "title": "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Compilers are complex software containing millions of lines of code, taking years to develop. This paper investigates to what extent Large Language Models (LLMs) can replace hand-crafted compilers in translating high-level programming languages to machine instructions, using C to x86 assembly as a case study. We identify two challenges of using LLMs for code translation and introduce two novel data pre-processing techniques to address the challenges: numerical value conversion and training data resampling. While only using a 13B model, our approach achieves a behavioral accuracy of over 91%, outperforming the much larger GPT-4 Turbo model by over 50%. Our results are encouraging, showing that LLMs have the potential to transform how compilation tools are constructed.",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.55",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "EvoR: Evolving Retrieval for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "su-etal-2024-evor",
        "author": "Su, Hongjin and Jiang, Shuyang and Lai, Yuhang and Wu, Haoyuan and Shi, Boao and Liu, Che and Liu, Qian and Yu, Tao",
        "booktitle": "EMNLP-findings2024",
        "title": "EvoR: Evolving Retrieval for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently the retrieval-augmented generation (RAG) has been successfully applied in code generation. However, existing pipelines for retrieval-augmented code generation (RACG) employ static knowledge bases with a single source, limiting the adaptation capabilities of Large Language Models (LLMs) to domains they have insufficient knowledge of. In this work, we develop a novel pipeline, EVOR, that employs the synchronous evolution of both queries and diverse knowledge bases. On two realistic settings where the external knowledge is required to solve code generation tasks, we compile four new datasets associated with frequently updated libraries and long-tail programming languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR achieves two to four times of execution accuracy compared to other methods such as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We demonstrate that EVOR is flexible and can be easily combined with them to achieve further improvement. Further analysis reveals that EVOR benefits from the synchronous evolution of queries and documents and the diverse information sources in the knowledge base. We hope that our studies will inspire more insights into the design of advanced RACG pipelines in future research.",
        "keywords": [
            "code generation",
            "code completion",
            "source code model",
            "prompting strategy",
            "RAG"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.143",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization": {
        "type": "INPROCEEDINGS",
        "key": "nahid-rafiei-2024-normtab",
        "author": "Nahid, Md and Rafiei, Davood",
        "booktitle": "EMNLP-findings2024",
        "title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in parsing textual data and generating code. However, their performance in tasks involving tabular data, especially those requiring symbolic reasoning, faces challenges due to the structural variance and inconsistency in table cell values often found in web tables. In this paper, we introduce NormTab, a novel framework aimed at enhancing the symbolic reasoning performance of LLMs by normalizing web tables. We study table normalization as a stand-alone, one-time preprocessing step using LLMs to support symbolic reasoning on tabular data. Our experimental evaluation, conducted on challenging web table datasets such as WikiTableQuestion and TabFact, demonstrates that leveraging NormTab significantly improves symbolic reasoning performance, showcasing the importance and effectiveness of web table normalization for enhancing LLM-based symbolic reasoning tasks.",
        "keywords": [
            "static analysis",
            "fundamental analysis"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.203",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Sanitizing Large Language Models in Bug Detection with Data-Flow": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-sanitizing",
        "author": "Wang, Chengpeng and Zhang, Wuqi and Su, Zian and Xu, Xiangzhe and Zhang, Xiangyu",
        "booktitle": "EMNLP-findings2024",
        "title": "Sanitizing Large Language Models in Bug Detection with Data-Flow",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) show potential in code reasoning tasks, facilitating the customization of detecting bugs in software development. However, the hallucination effect can significantly compromise the reliability of bug reports. This work formulates a new schema of bug detection and presents a novel sanitization technique that detects false positives for hallucination mitigation. Our key idea is to enforce LLMs to emit data-flow paths in few-shot chain-of-thought prompting and validate them via the program-property decomposition. Specifically, we dissect data-flow paths into basic properties upon concise code snippets and leverage parsing-based analysis and LLMs for validation. Our approach averagely achieves 91.03% precision and 74.00% recall upon synthetic benchmarks and boosts the precision by 21.99% with the sanitization. The evaluation upon real-world Android malware applications also demonstrates the superiority over an industrial analyzer, surpassing the precision and recall by 15.36% and 3.61%, respectively.",
        "keywords": [
            "static analysis",
            "bug detection",
            "fundamental analysis"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.217",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Stanceformer: Target-Aware Transformer for Stance Detection": {
        "type": "INPROCEEDINGS",
        "key": "garg-caragea-2024-stanceformer",
        "author": "Garg, Krishna and Caragea, Cornelia",
        "booktitle": "EMNLP-findings2024",
        "title": "Stanceformer: Target-Aware Transformer for Stance Detection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The task of Stance Detection involves discerning the stance expressed in a text towards a specific subject or target. Prior works have relied on existing transformer models that lack the capability to prioritize targets effectively. Consequently, these models yield similar performance regardless of whether we utilize or disregard target information, undermining the task\u2019s significance. To address this challenge, we introduce Stanceformer, a target-aware transformer model that incorporates enhanced attention towards the targets during both training and inference. Specifically, we design a Target Awareness matrix that increases the self-attention scores assigned to the targets. We demonstrate the efficacy of the Stanceformer with various BERT-based models, including state-of-the-art models and Large Language Models (LLMs), and evaluate its performance across three stance detection datasets, alongside a zero-shot dataset. Our approach Stanceformer not only provides superior performance but also generalizes even to other domains, such as Aspect-based Sentiment Analysis. We make the code publicly available.",
        "keywords": [
            "static analysis",
            "bug detection",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.286",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-defending-large",
        "author": "Zhao, Wei and Li, Zhe and Li, Yige and Zhang, Ye and Sun, Jun",
        "booktitle": "EMNLP-findings2024",
        "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs respond to harmful prompts and propose a novel defense method termed Layer-specific Editing (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical safety layers exist among the early layers of LLMs. We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from identified toxic layers can significantly improve the alignment of LLMs against jailbreak attacks. Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. Our code is available at https://github.com/ledllm/ledllm.",
        "keywords": [
            "code model security",
            "defense"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.293",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement": {
        "type": "INPROCEEDINGS",
        "key": "feng-etal-2024-self",
        "author": "Feng, Yunlong and Teng, Dechuan and Xu, Yang and Mu, Honglin and Xu, Xiao and Qin, Libo and Zhu, Qingfu and Che, Wanxiang",
        "booktitle": "EMNLP-findings2024",
        "title": "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decompilation transforms compiled code back into a high-level programming language for analysis when source code is unavailable. Previous work has primarily focused on enhancing decompilation performance by increasing the scale of model parameters or training data for pre-training. Based on the characteristics of the decompilation task, we propose two methods: (1) Without fine-tuning, the Self-Constructed Context Decompilation (sc\u00b2dec) method recompiles the LLM\u2019s decompilation results to construct pairs for in-context learning, helping the model improve decompilation performance. (2) Fine-grained Alignment Enhancement (FAE), which meticulously aligns assembly code with source code at the statement level by leveraging debugging information, is employed during the fine-tuning phase to achieve further improvements in decompilation. By integrating these two methods, we achieved a Re-Executability performance improvement of approximately 3.90% on the Decompile-Eval benchmark, establishing a new state-of-the-art performance of 52.41%. The code, data, and models are available at https://github.com/AlongWY/sccdec.",
        "keywords": [
            "code generation",
            "program decompilation",
            "code model training",
            "source code model",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.385",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "cheng-etal-2024-autodetect",
        "author": "Cheng, Jiale and Lu, Yida and Gu, Xiaotao and Ke, Pei and Liu, Xiao and Dong, Yuxiao and Wang, Hongning and Tang, Jie and Huang, Minlie",
        "booktitle": "EMNLP-findings2024",
        "title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction-following or coding tasks.As these unexpected errors could lead to severe consequences in practical deployments, it is crucial to investigate the limitations within LLMs systematically.Traditional benchmarking approaches cannot thoroughly pinpoint specific model deficiencies, while manual inspections are costly and not scalable. In this paper, we introduce a unified framework, AutoDetect, to automatically expose weaknesses in LLMs across various tasks. Inspired by the educational assessment process that measures students\u2019 learning outcomes, AutoDetect consists of three LLM-powered agents: Examiner, Questioner, and Assessor.The collaboration among these three agents is designed to realize comprehensive and in-depth weakness identification. Our framework demonstrates significant success in uncovering flaws, with an identification success rate exceeding 30% in prominent models such as ChatGPT and Claude.More importantly, these identified weaknesses can guide specific model improvements, proving more effective than untargeted data augmentation methods like Self-Instruct. Our approach has led to substantial enhancements in popular LLMs, including the Llama series and Mistral-7b, boosting their performance by over 10% across several benchmarks.Code and data are publicly available at https://github.com/thu-coai/AutoDetect.",
        "keywords": [
            "code model security",
            "attack",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.397",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code": {
        "type": "INPROCEEDINGS",
        "key": "guan-etal-2024-codeip",
        "author": "Guan, Batu and Wan, Yao and Bi, Zhangqian and Wang, Zheng and Zhang, Hongyu and Zhou, Pan and Sun, Lichao",
        "booktitle": "EMNLP-findings2024",
        "title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress in code generation. It now becomes crucial to identify whether the code is AI-generated and to determine the specific model used, particularly for purposes such as protecting Intellectual Property (IP) in industry and preventing cheating in programming exercises. To this end, several attempts have been made to insert watermarks into machine-generated code. However, existing approaches are limited to inserting only a single bit of information. In this paper, we introduce CodeIP, a novel multi-bit watermarking technique that embeds additional information to preserve crucial provenance details, such as the vendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation. Furthermore, to ensure the syntactical correctness of the generated code, we propose constraining the sampling process for predicting the next token by training a type predictor. Experiments conducted on a real-world dataset across five programming languages demonstrate the effectiveness of CodeIP in watermarking LLMs for code generation while maintaining the syntactical correctness of code.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model security",
            "defense"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.541",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging": {
        "type": "INPROCEEDINGS",
        "key": "kargupta-etal-2024-instruct",
        "author": "Kargupta, Priyanka and Agarwal, Ishika and Tur, Dilek Hakkani and Han, Jiawei",
        "booktitle": "EMNLP-findings2024",
        "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student\u2019s conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes\u2013 all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct\u2019s state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct\u2019s ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning.",
        "keywords": [
            "program testing",
            "debugging",
            "agent design",
            "planning"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.553",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Can LLMs Reason in the Wild with Programs?": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2024-llms",
        "author": "Yang, Yuan and Xiong, Siheng and Payani, Ali and Shareghi, Ehsan and Fekri, Faramarz",
        "booktitle": "EMNLP-findings2024",
        "title": "Can LLMs Reason in the Wild with Programs?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown superior capability to solve reasoning problems with programs. While being a promising direction, most of such frameworks are trained and evaluated in settings with a prior knowledge of task requirements. However, as LLMs become more capable, it is necessary to assess their reasoning abilities in more realistic scenarios where many real-world problems are open-ended with ambiguous scope, and often require multiple formalisms to solve. To investigate this, we introduce the task of reasoning in the wild, where an LLM is tasked to solve a reasoning problem of unknown type by identifying the sub-problems and their corresponding formalisms, and writing a program to solve each sub-problem, guided by a tactic. We create a large tactic-guided trajectory dataset containing detailed solutions to a diverse set of reasoning problems, ranging from well-defined single-form reasoning (e.g., math, logic), to ambiguous and hybrid ones (e.g., commonsense, combined math and logic). This allows us to test various aspects of LLMs reasoning at the fine-grained level such as the selection and execution of tactics, and the tendency to take undesired shortcuts. In experiments, we highlight that existing LLMs fail significantly on problems with ambiguous and mixed scope, revealing critical limitations and overfitting issues (e.g. accuracy on GSM8K drops by at least 50%). We further show the potential of finetuning a local LLM on the tactic-guided trajectories in achieving better performance. Project repo is available at https://github.com/gblackout/Reason-in-the-Wild.",
        "keywords": [
            "prompting strategy",
            "reasoning with code"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.573",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Rethinking Code Refinement: Learning to Judge Code Efficiency": {
        "type": "INPROCEEDINGS",
        "key": "seo-etal-2024-rethinking",
        "author": "Seo, Minju and Baek, Jinheon and Hwang, Sung Ju",
        "booktitle": "EMNLP-findings2024",
        "title": "Rethinking Code Refinement: Learning to Judge Code Efficiency",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating codes. Due to these capabilities, many recent methods are proposed to automatically refine the codes with LLMs. However, we should rethink that the refined codes (from LLMs and even humans) are not always more efficient than their original versions. On the other hand, running two different versions of codes and comparing them every time is not ideal and time-consuming. Therefore, in this work, we propose a novel method based on the code language model that is trained to judge the efficiency between two different codes (generated across humans and machines) by either classifying the superior one or predicting the relative improvement. We validate our method on multiple programming languages with multiple refinement steps, demonstrating that the proposed method can effectively distinguish between more and less efficient versions of code.",
        "keywords": [
            "code generation",
            "program transformation",
            "code model training",
            "source code model"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.645",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Revisiting the Impact of Pursuing Modularity for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "kang-etal-2024-revisiting",
        "author": "Kang, Deokyeong and Seo, KiJung and Kim, Taeuk",
        "booktitle": "EMNLP-findings2024",
        "title": "Revisiting the Impact of Pursuing Modularity for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Modular programming, which aims to construct the final program by integrating smaller, independent building blocks, has been regarded as a desirable practice in software development. However, with the rise of recent code generation agents built upon large language models (LLMs), a question emerges: is this traditional practice equally effective for these new tools? In this work, we assess the impact of modularity in code generation by introducing a novel metric for its quantitative measurement. Surprisingly, unlike conventional wisdom on the topic, we find that modularity is not a core factor for improving the performance of code generation models. We also explore potential explanations for why LLMs do not exhibit a preference for modular code compared to non-modular code.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.676",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation": {
        "type": "INPROCEEDINGS",
        "key": "shen-etal-2024-enhancing",
        "author": "Shen, Zizhuo and Shao, Yanqiu and Li, Wei",
        "booktitle": "EMNLP-findings2024",
        "title": "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Due to the high complexity of Discourse Dependency Parsing (DDP) tasks, their existing annotation resources are relatively scarce compared to other NLP tasks, and different DDP tasks also have significant differences in annotation schema. These issues have led to the dilemma of low resources for DDP tasks. Thanks to the powerful capabilities of Large Language Models (LLMs) in cross-task learning, we can use LLMs to model dependency parsing under different annotation schema in an unified manner, in order to alleviate the dilemma of low resources for DDP tasks. However, enabling LLMs to deeply comprehend dependency parsing tasks is a challenge that remains underexplored. Inspired by the application of code-based methods in complex tasks, we propose a code-based unified dependency parsing method. We treat the process of dependency parsing as a search process of dependency paths and use code to represent this search process. Furthermore, we use a curriculum-learning based instruction tuning strategy for joint training of multiple dependency parsing tasks. The experimental results show that our proposed code-based DDP system has achieved good performance on two Chinese DDP tasks (especially significant improvement on the DDP task with relatively less training data).",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model training",
            "source code model"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.729",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "On Leakage of Code Generation Evaluation Datasets": {
        "type": "INPROCEEDINGS",
        "key": "matton-etal-2024-leakage",
        "author": "Matton, Alexandre and Sherborne, Tom and Aumiller, Dennis and Tommasone, Elena and Alizadeh, Milad and He, Jingyi and Ma, Raymond and Voisin, Maxime and Gilsenan-McMahon, Ellen and Gall\u00e9, Matthias",
        "booktitle": "EMNLP-findings2024",
        "title": "On Leakage of Code Generation Evaluation Datasets",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we consider contamination by code generation test sets, in particular in their use in modern large language models.We discuss three possible sources of such contamination and show findings supporting each of them: (i) direct data leakage, (ii) indirect data leakage through the use of synthetic data and (iii) overfitting to evaluation sets during model selection.To address this, we release Less Basic Python Problems (LBPP): an uncontaminated new benchmark of 161 prompts with their associated Python solutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.772",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    }
}